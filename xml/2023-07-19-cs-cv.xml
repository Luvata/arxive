<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-18T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08716" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08717" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08723" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08727" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08758" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08763" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08771" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08779" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08781" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08789" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08809" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08837" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08850" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08880" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08908" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08913" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08919" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08924" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08930" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08939" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08950" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08978" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08984" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08988" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08991" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08992" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08994" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08995" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08996" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09000" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09004" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09005" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09008" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09019" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09020" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09023" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09026" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09027" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09039" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09050" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09052" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09055" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09059" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09065" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09066" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09067" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09070" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09099" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09104" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09109" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09112" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09120" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09136" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09143" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09146" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09153" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09155" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09158" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09160" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09161" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09165" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09172" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09183" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09184" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09218" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09220" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09233" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09238" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09239" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09259" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09262" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09264" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09267" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09279" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09283" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09302" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09306" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09316" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09323" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09329" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09330" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09351" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09356" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09361" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09362" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09365" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09367" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09368" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09402" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09416" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09420" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09437" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09456" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09465" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09472" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09474" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09480" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09481" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2011.04408" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.02159" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.03824" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.15304" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.13108" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.03507" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.14581" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.02401" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.02885" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.05118" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.15932" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.01580" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.02689" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.03209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.08101" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.10769" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.11862" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.12306" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.04422" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.07848" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.08546" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.09211" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13399" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16656" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05439" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06849" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07615" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.13074" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15548" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01740" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02347" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07250" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07483" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07754" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07813" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07916" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08397" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08466" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08535" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.08716">
<title>Enforcing Topological Interaction between Implicit Surfaces via Uniform Sampling. (arXiv:2307.08716v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.08716</link>
<description rdf:parseType="Literal">&lt;p&gt;Objects interact with each other in various ways, including containment,
contact, or maintaining fixed distances. Ensuring these topological
interactions is crucial for accurate modeling in many scenarios. In this paper,
we propose a novel method to refine 3D object representations, ensuring that
their surfaces adhere to a topological prior. Our key observation is that the
object interaction can be observed via a stochastic approximation method: the
statistic of signed distances between a large number of random points to the
object surfaces reflect the interaction between them. Thus, the object
interaction can be indirectly manipulated by using choosing a set of points as
anchors to refine the object surfaces. In particular, we show that our method
can be used to enforce two objects to have a specific contact ratio while
having no surface intersection. The conducted experiments show that our
proposed method enables accurate 3D reconstruction of human hearts, ensuring
proper topological connectivity between components. Further, we show that our
proposed method can be used to simulate various ways a hand can interact with
an arbitrary object.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1&quot;&gt;Hieu Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talabot_N/0/1/0/all/0/1&quot;&gt;Nicolas Talabot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jiancheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1&quot;&gt;Pascal Fua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08717">
<title>Untrained neural network embedded Fourier phase retrieval from few measurements. (arXiv:2307.08717v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.08717</link>
<description rdf:parseType="Literal">&lt;p&gt;Fourier phase retrieval (FPR) is a challenging task widely used in various
applications. It involves recovering an unknown signal from its Fourier
phaseless measurements. FPR with few measurements is important for reducing
time and hardware costs, but it suffers from serious ill-posedness. Recently,
untrained neural networks have offered new approaches by introducing learned
priors to alleviate the ill-posedness without requiring any external data.
However, they may not be ideal for reconstructing fine details in images and
can be computationally expensive. This paper proposes an untrained neural
network (NN) embedded algorithm based on the alternating direction method of
multipliers (ADMM) framework to solve FPR with few measurements. Specifically,
we use a generative network to represent the image to be recovered, which
confines the image to the space defined by the network structure. To improve
the ability to represent high-frequency information, total variation (TV)
regularization is imposed to facilitate the recovery of local structures in the
image. Furthermore, to reduce the computational cost mainly caused by the
parameter updates of the untrained NN, we develop an accelerated algorithm that
adaptively trades off between explicit and implicit regularization.
Experimental results indicate that the proposed algorithm outperforms existing
untrained NN-based algorithms with fewer computational resources and even
performs competitively against trained NN-based algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Liyuan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongxia Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Leng_N/0/1/0/all/0/1&quot;&gt;Ningyi Leng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yuan_Z/0/1/0/all/0/1&quot;&gt;Ziyang Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08723">
<title>Revisiting Scene Text Recognition: A Data Perspective. (arXiv:2307.08723v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.08723</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper aims to re-assess scene text recognition (STR) from a
data-oriented perspective. We begin by revisiting the six commonly used
benchmarks in STR and observe a trend of performance saturation, whereby only
2.91% of the benchmark images cannot be accurately recognized by an ensemble of
13 representative models. While these results are impressive and suggest that
STR could be considered solved, however, we argue that this is primarily due to
the less challenging nature of the common benchmarks, thus concealing the
underlying issues that STR faces. To this end, we consolidate a large-scale
real STR dataset, namely Union14M, which comprises 4 million labeled images and
10 million unlabeled images, to assess the performance of STR models in more
complex real-world scenarios. Our experiments demonstrate that the 13 models
can only achieve an average accuracy of 66.53% on the 4 million labeled images,
indicating that STR still faces numerous challenges in the real world. By
analyzing the error patterns of the 13 models, we identify seven open
challenges in STR and develop a challenge-driven benchmark consisting of eight
distinct subsets to facilitate further progress in the field. Our exploration
demonstrates that STR is far from being solved and leveraging data may be a
promising solution. In this regard, we find that utilizing the 10 million
unlabeled images through self-supervised pre-training can significantly improve
the robustness of STR model in real-world scenarios and leads to
state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1&quot;&gt;Qing Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiapeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1&quot;&gt;Dezhi Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chongyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1&quot;&gt;Lianwen Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08727">
<title>Semantic Counting from Self-Collages. (arXiv:2307.08727v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.08727</link>
<description rdf:parseType="Literal">&lt;p&gt;While recent supervised methods for reference-based object counting continue
to improve the performance on benchmark datasets, they have to rely on small
datasets due to the cost associated with manually annotating dozens of objects
in images. We propose Unsupervised Counter (UnCo), a model that can learn this
task without requiring any manual annotations. To this end, we construct
&quot;SelfCollages&quot;, images with various pasted objects as training samples, that
provide a rich learning signal covering arbitrary object types and counts. Our
method builds on existing unsupervised representations and segmentation
techniques to successfully demonstrate the ability to count objects without
manual supervision. Our experiments show that our method not only outperforms
simple baselines and generic models such as FasterRCNN, but also matches the
performance of supervised counting models in some domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knobel_L/0/1/0/all/0/1&quot;&gt;Lukas Knobel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1&quot;&gt;Tengda Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1&quot;&gt;Yuki M. Asano&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08758">
<title>Linking Physics and Psychology of Bistable Perception Using an Eye Blink Inspired Quantum Harmonic Oscillator Model. (arXiv:2307.08758v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/2307.08758</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a novel quantum-mechanical model that describes
psychological phenomena using the analogy of a harmonic oscillator represented
by an electron trapped in a potential well. Study~1 demonstrates the
application of the proposed model to bistable perception of ambiguous figures
(i.e., optical illusions), exemplified by the Necker cube. While prior research
has theoretically linked quantum mechanics to psychological phenomena, in
Study~2 we demonstrate a viable physiological connection between physics and
bistable perception. To that end, the model draws parallels between quantum
tunneling of an electron through a potential energy barrier and an eye blink,
an action known to trigger perceptual reversals. Finally, we discuss the
ability of the model to capture diverse optical illusions and other
psychological phenomena, including cognitive dissonance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Maksymov_I/0/1/0/all/0/1&quot;&gt;Ivan S. Maksymov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Pogrebna_G/0/1/0/all/0/1&quot;&gt;Ganna Pogrebna&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08763">
<title>Video-Mined Task Graphs for Keystep Recognition in Instructional Videos. (arXiv:2307.08763v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.08763</link>
<description rdf:parseType="Literal">&lt;p&gt;Procedural activity understanding requires perceiving human actions in terms
of a broader task, where multiple keysteps are performed in sequence across a
long video to reach a final goal state -- such as the steps of a recipe or a
DIY fix-it task. Prior work largely treats keystep recognition in isolation of
this broader structure, or else rigidly confines keysteps to align with a
predefined sequential script. We propose discovering a task graph automatically
from how-to videos to represent probabilistically how people tend to execute
keysteps, and then leverage this graph to regularize keystep recognition in
novel videos. On multiple datasets of real-world instructional videos, we show
the impact: more reliable zero-shot keystep localization and improved video
representation learning, exceeding the state of the art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ashutosh_K/0/1/0/all/0/1&quot;&gt;Kumar Ashutosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramakrishnan_S/0/1/0/all/0/1&quot;&gt;Santhosh Kumar Ramakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Afouras_T/0/1/0/all/0/1&quot;&gt;Triantafyllos Afouras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1&quot;&gt;Kristen Grauman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08771">
<title>UPSCALE: Unconstrained Channel Pruning. (arXiv:2307.08771v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.08771</link>
<description rdf:parseType="Literal">&lt;p&gt;As neural networks grow in size and complexity, inference speeds decline. To
combat this, one of the most effective compression techniques -- channel
pruning -- removes channels from weights. However, for multi-branch segments of
a model, channel removal can introduce inference-time memory copies. In turn,
these copies increase inference latency -- so much so that the pruned model can
be slower than the unpruned model. As a workaround, pruners conventionally
constrain certain channels to be pruned together. This fully eliminates memory
copies but, as we show, significantly impairs accuracy. We now have a dilemma:
Remove constraints but increase latency, or add constraints and impair
accuracy. In response, our insight is to reorder channels at export time, (1)
reducing latency by reducing memory copies and (2) improving accuracy by
removing constraints. Using this insight, we design a generic algorithm UPSCALE
to prune models with any pruning pattern. By removing constraints from existing
pruners, we improve ImageNet accuracy for post-training pruned models by 2.1
points on average -- benefiting DenseNet (+16.9), EfficientNetV2 (+7.9), and
ResNet (+6.2). Furthermore, by reordering channels, UPSCALE improves inference
speeds by up to 2x over a baseline export.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_A/0/1/0/all/0/1&quot;&gt;Alvin Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_H/0/1/0/all/0/1&quot;&gt;Hanxiang Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patnaik_K/0/1/0/all/0/1&quot;&gt;Kaushik Patnaik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yueyang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hadad_O/0/1/0/all/0/1&quot;&gt;Omer Hadad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guera_D/0/1/0/all/0/1&quot;&gt;David G&amp;#xfc;era&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1&quot;&gt;Zhile Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_Q/0/1/0/all/0/1&quot;&gt;Qi Shan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08779">
<title>Similarity Min-Max: Zero-Shot Day-Night Domain Adaptation. (arXiv:2307.08779v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.08779</link>
<description rdf:parseType="Literal">&lt;p&gt;Low-light conditions not only hamper human visual experience but also degrade
the model&apos;s performance on downstream vision tasks. While existing works make
remarkable progress on day-night domain adaptation, they rely heavily on domain
knowledge derived from the task-specific nighttime dataset. This paper
challenges a more complicated scenario with border applicability, i.e.,
zero-shot day-night domain adaptation, which eliminates reliance on any
nighttime data. Unlike prior zero-shot adaptation approaches emphasizing either
image-level translation or model-level adaptation, we propose a similarity
min-max paradigm that considers them under a unified framework. On the image
level, we darken images towards minimum feature similarity to enlarge the
domain gap. Then on the model level, we maximize the feature similarity between
the darkened images and their normal-light counterparts for better model
adaptation. To the best of our knowledge, this work represents the pioneering
effort in jointly optimizing both aspects, resulting in a significant
improvement of model generalizability. Extensive experiments demonstrate our
method&apos;s effectiveness and broad applicability on various nighttime vision
tasks, including classification, semantic segmentation, visual place
recognition, and video action recognition. Code and pre-trained models are
available at https://red-fairy.github.io/ZeroShotDayNightDA-Webpage/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1&quot;&gt;Rundong Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenjing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wenhan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiaying Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08781">
<title>The FathomNet2023 Competition Dataset. (arXiv:2307.08781v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.08781</link>
<description rdf:parseType="Literal">&lt;p&gt;Ocean scientists have been collecting visual data to study marine organisms
for decades. These images and videos are extremely valuable both for basic
science and environmental monitoring tasks. There are tools for automatically
processing these data, but none that are capable of handling the extreme
variability in sample populations, image quality, and habitat characteristics
that are common in visual sampling of the ocean. Such distribution shifts can
occur over very short physical distances and in narrow time windows. Creating
models that are able to recognize when an image or video sequence contains a
new organism, an unusual collection of animals, or is otherwise out-of-sample
is critical to fully leverage visual data in the ocean. The FathomNet2023
competition dataset presents a realistic scenario where the set of animals in
the target data differs from the training data. The challenge is both to
identify the organisms in a target image and assess whether it is
out-of-sample.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orenstein_E/0/1/0/all/0/1&quot;&gt;Eric Orenstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barnard_K/0/1/0/all/0/1&quot;&gt;Kevin Barnard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lundsten_L/0/1/0/all/0/1&quot;&gt;Lonny Lundsten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patterson_G/0/1/0/all/0/1&quot;&gt;Genevi&amp;#xe8;ve Patterson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woodward_B/0/1/0/all/0/1&quot;&gt;Benjamin Woodward&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katija_K/0/1/0/all/0/1&quot;&gt;Kakani Katija&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08789">
<title>Harnessing the Power of AI based Image Generation Model DALLE 2 in Agricultural Settings. (arXiv:2307.08789v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.08789</link>
<description rdf:parseType="Literal">&lt;p&gt;This study investigates the potential impact of artificial intelligence (AI)
on the enhancement of visualization processes in the agricultural sector, using
the advanced AI image generator, DALLE 2, developed by OpenAI. By
synergistically utilizing the natural language processing proficiency of
chatGPT and the generative prowess of the DALLE 2 model, which employs a
Generative Adversarial Networks (GANs) framework, our research offers an
innovative method to transform textual descriptors into realistic visual
content. Our rigorously assembled datasets include a broad spectrum of
agricultural elements such as fruits, plants, and scenarios differentiating
crops from weeds, maintained for AI-generated versus original images. The
quality and accuracy of the AI-generated images were evaluated via established
metrics including mean squared error (MSE), peak signal-to-noise ratio (PSNR),
and feature similarity index (FSIM). The results underline the significant role
of the DALLE 2 model in enhancing visualization processes in agriculture,
aiding in more informed decision-making, and improving resource distribution.
The outcomes of this research highlight the imminent rise of an AI-led
transformation in the realm of precision agriculture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sapkota_R/0/1/0/all/0/1&quot;&gt;Ranjan Sapkota&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08809">
<title>Local or Global: Selective Knowledge Assimilation for Federated Learning with Limited Labels. (arXiv:2307.08809v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08809</link>
<description rdf:parseType="Literal">&lt;p&gt;Many existing FL methods assume clients with fully-labeled data, while in
realistic settings, clients have limited labels due to the expensive and
laborious process of labeling. Limited labeled local data of the clients often
leads to their local model having poor generalization abilities to their larger
unlabeled local data, such as having class-distribution mismatch with the
unlabeled data. As a result, clients may instead look to benefit from the
global model trained across clients to leverage their unlabeled data, but this
also becomes difficult due to data heterogeneity across clients. In our work,
we propose FedLabel where clients selectively choose the local or global model
to pseudo-label their unlabeled data depending on which is more of an expert of
the data. We further utilize both the local and global models&apos; knowledge via
global-local consistency regularization which minimizes the divergence between
the two models&apos; outputs when they have identical pseudo-labels for the
unlabeled data. Unlike other semi-supervised FL baselines, our method does not
require additional experts other than the local or global model, nor require
additional parameters to be communicated. We also do not assume any
server-labeled data or fully labeled clients. For both cross-device and
cross-silo settings, we show that FedLabel outperforms other semi-supervised FL
baselines by $8$-$24\%$, and even outperforms standard fully supervised FL
baselines ($100\%$ labeled data) with only $5$-$20\%$ of labeled data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1&quot;&gt;Yae Jee Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_G/0/1/0/all/0/1&quot;&gt;Gauri Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dimitriadis_D/0/1/0/all/0/1&quot;&gt;Dimitrios Dimitriadis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08837">
<title>DARTS: Double Attention Reference-based Transformer for Super-resolution. (arXiv:2307.08837v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.08837</link>
<description rdf:parseType="Literal">&lt;p&gt;We present DARTS, a transformer model for reference-based image
super-resolution. DARTS learns joint representations of two image distributions
to enhance the content of low-resolution input images through matching
correspondences learned from high-resolution reference images. Current
state-of-the-art techniques in reference-based image super-resolution are based
on a multi-network, multi-stage architecture. In this work, we adapt the double
attention block from the GAN literature, processing the two visual streams
separately and combining self-attention and cross-attention blocks through a
gating attention strategy. Our work demonstrates how the attention mechanism
can be adapted for the particular requirements of reference-based image
super-resolution, significantly simplifying the architecture and training
pipeline. We show that our transformer-based model performs competitively with
state-of-the-art models, while maintaining a simpler overall architecture and
training process. In particular, we obtain state-of-the-art on the SUN80
dataset, with a PSNR/SSIM of 29.83 / .809. These results show that attention
alone is sufficient for the RSR task, without multiple purpose-built
subnetworks, knowledge distillation, or multi-stage training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aslahishahri_M/0/1/0/all/0/1&quot;&gt;Masoomeh Aslahishahri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ubbens_J/0/1/0/all/0/1&quot;&gt;Jordan Ubbens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stavness_I/0/1/0/all/0/1&quot;&gt;Ian Stavness&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08850">
<title>LiDAR-BEVMTN: Real-Time LiDAR Bird&apos;s-Eye View Multi-Task Perception Network for Autonomous Driving. (arXiv:2307.08850v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.08850</link>
<description rdf:parseType="Literal">&lt;p&gt;LiDAR is crucial for robust 3D scene perception in autonomous driving. LiDAR
perception has the largest body of literature after camera perception. However,
multi-task learning across tasks like detection, segmentation, and motion
estimation using LiDAR remains relatively unexplored, especially on
automotive-grade embedded platforms. We present a real-time multi-task
convolutional neural network for LiDAR-based object detection, semantics, and
motion segmentation. The unified architecture comprises a shared encoder and
task-specific decoders, enabling joint representation learning. We propose a
novel Semantic Weighting and Guidance (SWAG) module to transfer semantic
features for improved object detection selectively. Our heterogeneous training
scheme combines diverse datasets and exploits complementary cues between tasks.
The work provides the first embedded implementation unifying these key
perception tasks from LiDAR point clouds achieving 3ms latency on the embedded
NVIDIA Xavier platform. We achieve state-of-the-art results for two tasks,
semantic and motion segmentation, and close to state-of-the-art performance for
3D object detection. By maximizing hardware efficiency and leveraging
multi-task synergies, our method delivers an accurate and efficient solution
tailored for real-world automated driving deployment. Qualitative results can
be seen at https://youtu.be/H-hWRzv2lIY.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohapatra_S/0/1/0/all/0/1&quot;&gt;Sambit Mohapatra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1&quot;&gt;Senthil Yogamani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1&quot;&gt;Varun Ravi Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milz_S/0/1/0/all/0/1&quot;&gt;Stefan Milz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gotzig_H/0/1/0/all/0/1&quot;&gt;Heinrich Gotzig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mader_P/0/1/0/all/0/1&quot;&gt;Patrick M&amp;#xe4;der&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08880">
<title>Modular Neural Network Approaches for Surgical Image Recognition. (arXiv:2307.08880v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.08880</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning-based applications have seen a lot of success in recent years.
Text, audio, image, and video have all been explored with great success using
deep learning approaches. The use of convolutional neural networks (CNN) in
computer vision, in particular, has yielded reliable results. In order to
achieve these results, a large amount of data is required. However, the dataset
cannot always be accessible. Moreover, annotating data can be difficult and
time-consuming. Self-training is a semi-supervised approach that managed to
alleviate this problem and achieve state-of-the-art performances. Theoretical
analysis even proved that it may result in a better generalization than a
normal classifier. Another problem neural networks can face is the increasing
complexity of modern problems, requiring a high computational and storage cost.
One way to mitigate this issue, a strategy that has been inspired by human
cognition known as modular learning, can be employed. The principle of the
approach is to decompose a complex problem into simpler sub-tasks. This
approach has several advantages, including faster learning, better
generalization, and enables interpretability.
&lt;/p&gt;
&lt;p&gt;In the first part of this paper, we introduce and evaluate different
architectures of modular learning for Dorsal Capsulo-Scapholunate Septum (DCSS)
instability classification. Our experiments have shown that modular learning
improves performances compared to non-modular systems. Moreover, we found that
weighted modular, that is to weight the output using the probabilities from the
gating module, achieved an almost perfect classification. In the second part,
we present our approach for data labeling and segmentation with self-training
applied on shoulder arthroscopy images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salem_N/0/1/0/all/0/1&quot;&gt;Nosseiba Ben Salem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bennani_Y/0/1/0/all/0/1&quot;&gt;Younes Bennani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karkazan_J/0/1/0/all/0/1&quot;&gt;Joseph Karkazan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barbara_A/0/1/0/all/0/1&quot;&gt;Abir Barbara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dacheux_C/0/1/0/all/0/1&quot;&gt;Charles Dacheux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gregory_T/0/1/0/all/0/1&quot;&gt;Thomas Gregory&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08908">
<title>What Can Simple Arithmetic Operations Do for Temporal Modeling?. (arXiv:2307.08908v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.08908</link>
<description rdf:parseType="Literal">&lt;p&gt;Temporal modeling plays a crucial role in understanding video content. To
tackle this problem, previous studies built complicated temporal relations
through time sequence thanks to the development of computationally powerful
devices. In this work, we explore the potential of four simple arithmetic
operations for temporal modeling. Specifically, we first capture auxiliary
temporal cues by computing addition, subtraction, multiplication, and division
between pairs of extracted frame features. Then, we extract corresponding
features from these cues to benefit the original temporal-irrespective domain.
We term such a simple pipeline as an Arithmetic Temporal Module (ATM), which
operates on the stem of a visual backbone with a plug-andplay style. We conduct
comprehensive ablation studies on the instantiation of ATMs and demonstrate
that this module provides powerful temporal modeling capability at a low
computational cost. Moreover, the ATM is compatible with both CNNs- and
ViTs-based architectures. Our results show that ATM achieves superior
performance over several popular video benchmarks. Specifically, on
Something-Something V1, V2 and Kinetics-400, we reach top-1 accuracy of 65.6%,
74.6%, and 89.4% respectively. The code is available at
https://github.com/whwu95/ATM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wenhao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yuxin Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhun Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingdong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1&quot;&gt;Wanli Ouyang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08913">
<title>Towards the Sparseness of Projection Head in Self-Supervised Learning. (arXiv:2307.08913v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08913</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, self-supervised learning (SSL) has emerged as a promising
approach for extracting valuable representations from unlabeled data. One
successful SSL method is contrastive learning, which aims to bring positive
examples closer while pushing negative examples apart. Many current contrastive
learning approaches utilize a parameterized projection head. Through a
combination of empirical analysis and theoretical investigation, we provide
insights into the internal mechanisms of the projection head and its
relationship with the phenomenon of dimensional collapse. Our findings
demonstrate that the projection head enhances the quality of representations by
performing contrastive loss in a projected subspace. Therefore, we propose an
assumption that only a subset of features is necessary when minimizing the
contrastive loss of a mini-batch of data. Theoretical analysis further suggests
that a sparse projection head can enhance generalization, leading us to
introduce SparseHead - a regularization term that effectively constrains the
sparsity of the projection head, and can be seamlessly integrated with any
self-supervised learning (SSL) approaches. Our experimental results validate
the effectiveness of SparseHead, demonstrating its ability to improve the
performance of existing contrastive methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zeen Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1&quot;&gt;Xingzhe Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingyao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiang_W/0/1/0/all/0/1&quot;&gt;Wenwen Qiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Changwen Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1&quot;&gt;Fuchun Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08919">
<title>Accuracy versus time frontiers of semi-supervised and self-supervised learning on medical images. (arXiv:2307.08919v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.08919</link>
<description rdf:parseType="Literal">&lt;p&gt;For many applications of classifiers to medical images, a trustworthy label
for each image can be difficult or expensive to obtain. In contrast, images
without labels are more readily available. Two major research directions both
promise that additional unlabeled data can improve classifier performance:
self-supervised learning pretrains useful representations on unlabeled data
only, then fine-tunes a classifier on these representations via the labeled
set; semi-supervised learning directly trains a classifier on labeled and
unlabeled data simultaneously. Recent methods from both directions have claimed
significant gains on non-medical tasks, but do not systematically assess
medical images and mostly compare only to methods in the same direction. This
study contributes a carefully-designed benchmark to help answer a
practitioner&apos;s key question: given a small labeled dataset and a limited budget
of hours to spend on training, what gains from additional unlabeled images are
possible and which methods best achieve them? Unlike previous benchmarks, ours
uses realistic-sized validation sets to select hyperparameters, assesses
runtime-performance tradeoffs, and bridges two research fields. By comparing 6
semi-supervised methods and 5 self-supervised methods to strong labeled-only
baselines on 3 medical datasets with 30-1000 labels per class, we offer
insights to resource-constrained, results-focused practitioners: MixMatch,
SimCLR, and BYOL represent strong choices that were not surpassed by more
recent methods. After much effort selecting hyperparameters on one dataset, we
publish settings that enable strong methods to perform well on new medical
tasks within a few hours, with further search over dozens of hours delivering
modest additional gains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhe Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1&quot;&gt;Ruijie Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aeron_S/0/1/0/all/0/1&quot;&gt;Shuchin Aeron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1&quot;&gt;Michael C. Hughes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08924">
<title>Learning to Sample Tasks for Meta Learning. (arXiv:2307.08924v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08924</link>
<description rdf:parseType="Literal">&lt;p&gt;Through experiments on various meta-learning methods, task samplers, and
few-shot learning tasks, this paper arrives at three conclusions. Firstly,
there are no universal task sampling strategies to guarantee the performance of
meta-learning models. Secondly, task diversity can cause the models to either
underfit or overfit during training. Lastly, the generalization performance of
the models are influenced by task divergence, task entropy, and task
difficulty. In response to these findings, we propose a novel task sampler
called Adaptive Sampler (ASr). ASr is a plug-and-play task sampler that takes
task divergence, task entropy, and task difficulty to sample tasks. To optimize
ASr, we rethink and propose a simple and general meta-learning algorithm.
Finally, a large number of empirical experiments demonstrate the effectiveness
of the proposed ASr.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingyao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zeen Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1&quot;&gt;Xingzhe Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1&quot;&gt;Lingyu Si&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hongwei Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiang_W/0/1/0/all/0/1&quot;&gt;Wenwen Qiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Changwen Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08930">
<title>Unsupervised Deep Graph Matching Based on Cycle Consistency. (arXiv:2307.08930v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.08930</link>
<description rdf:parseType="Literal">&lt;p&gt;We contribute to the sparsely populated area of unsupervised deep graph
matching with application to keypoint matching in images. Contrary to the
standard \emph{supervised} approach, our method does not require ground truth
correspondences between keypoint pairs. Instead, it is self-supervised by
enforcing consistency of matchings between images of the same object category.
As the matching and the consistency loss are discrete, their derivatives cannot
be straightforwardly used for learning. We address this issue in a principled
way by building our method upon the recent results on black-box differentiation
of combinatorial solvers. This makes our method exceptionally flexible, as it
is compatible with arbitrary network architectures and combinatorial solvers.
Our experimental evaluation suggests that our technique sets a new
state-of-the-art for unsupervised graph matching.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tourani_S/0/1/0/all/0/1&quot;&gt;Siddharth Tourani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rother_C/0/1/0/all/0/1&quot;&gt;Carsten Rother&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Muhammad Haris Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savchynskkyy_B/0/1/0/all/0/1&quot;&gt;Bogdan Savchynskkyy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08939">
<title>Experimental Security Analysis of DNN-based Adaptive Cruise Control under Context-Aware Perception Attacks. (arXiv:2307.08939v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2307.08939</link>
<description rdf:parseType="Literal">&lt;p&gt;Adaptive Cruise Control (ACC) is a widely used driver assistance feature for
maintaining desired speed and safe distance to the leading vehicles. This paper
evaluates the security of the deep neural network (DNN) based ACC systems under
stealthy perception attacks that strategically inject perturbations into camera
data to cause forward collisions. We present a combined
knowledge-and-data-driven approach to design a context-aware strategy for the
selection of the most critical times for triggering the attacks and a novel
optimization-based method for the adaptive generation of image perturbations at
run-time. We evaluate the effectiveness of the proposed attack using an actual
driving dataset and a realistic simulation platform with the control software
from a production ACC system and a physical-world driving simulator while
considering interventions by the driver and safety features such as Automatic
Emergency Braking (AEB) and Forward Collision Warning (FCW). Experimental
results show that the proposed attack achieves 142.9x higher success rate in
causing accidents than random attacks and is mitigated 89.6% less by the safety
features while being stealthy and robust to real-world factors and dynamic
changes in the environment. This study provides insights into the role of human
operators and basic safety interventions in preventing attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xugui Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1&quot;&gt;Anqi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kouzel_M/0/1/0/all/0/1&quot;&gt;Maxfield Kouzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1&quot;&gt;Haotian Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCarty_M/0/1/0/all/0/1&quot;&gt;Morgan McCarty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nita_Rotaru_C/0/1/0/all/0/1&quot;&gt;Cristina Nita-Rotaru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alemzadeh_H/0/1/0/all/0/1&quot;&gt;Homa Alemzadeh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08950">
<title>Deep Physics-Guided Unrolling Generalization for Compressed Sensing. (arXiv:2307.08950v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.08950</link>
<description rdf:parseType="Literal">&lt;p&gt;By absorbing the merits of both the model- and data-driven methods, deep
physics-engaged learning scheme achieves high-accuracy and interpretable image
reconstruction. It has attracted growing attention and become the mainstream
for inverse imaging tasks. Focusing on the image compressed sensing (CS)
problem, we find the intrinsic defect of this emerging paradigm, widely
implemented by deep algorithm-unrolled networks, in which more plain iterations
involving real physics will bring enormous computation cost and long inference
time, hindering their practical application. A novel deep
$\textbf{P}$hysics-guided un$\textbf{R}$olled recovery $\textbf{L}$earning
($\textbf{PRL}$) framework is proposed by generalizing the traditional
iterative recovery model from image domain (ID) to the high-dimensional feature
domain (FD). A compact multiscale unrolling architecture is then developed to
enhance the network capacity and keep real-time inference speeds. Taking two
different perspectives of optimization and range-nullspace decomposition,
instead of building an algorithm-specific unrolled network, we provide two
implementations: $\textbf{PRL-PGD}$ and $\textbf{PRL-RND}$. Experiments exhibit
the significant performance and efficiency leading of PRL networks over other
state-of-the-art methods with a large potential for further improvement and
real application to other inverse imaging problems or optimization models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jiechong Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1&quot;&gt;Jingfen Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jian Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08978">
<title>Learned Scalable Video Coding For Humans and Machines. (arXiv:2307.08978v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.08978</link>
<description rdf:parseType="Literal">&lt;p&gt;Video coding has traditionally been developed to support services such as
video streaming, videoconferencing, digital TV, and so on. The main intent was
to enable human viewing of the encoded content. However, with the advances in
deep neural networks (DNNs), encoded video is increasingly being used for
automatic video analytics performed by machines. In applications such as
automatic traffic monitoring, analytics such as vehicle detection, tracking and
counting, would run continuously, while human viewing could be required
occasionally to review potential incidents. To support such applications, a new
paradigm for video coding is needed that will facilitate efficient
representation and compression of video for both machine and human use in a
scalable manner. In this manuscript, we introduce the first end-to-end
learnable video codec that supports a machine vision task in its base layer,
while its enhancement layer supports input reconstruction for human viewing.
The proposed system is constructed based on the concept of conditional coding
to achieve better compression gains. Comprehensive experimental evaluations
conducted on four standard video datasets demonstrate that our framework
outperforms both state-of-the-art learned and conventional video codecs in its
base layer, while maintaining comparable performance on the human vision task
in its enhancement layer. We will provide the implementation of the proposed
system at www.github.com upon completion of the review process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hadizadeh_H/0/1/0/all/0/1&quot;&gt;Hadi Hadizadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bajic_I/0/1/0/all/0/1&quot;&gt;Ivan V. Baji&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08984">
<title>In Defense of Clip-based Video Relation Detection. (arXiv:2307.08984v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.08984</link>
<description rdf:parseType="Literal">&lt;p&gt;Video Visual Relation Detection (VidVRD) aims to detect visual relationship
triplets in videos using spatial bounding boxes and temporal boundaries.
Existing VidVRD methods can be broadly categorized into bottom-up and top-down
paradigms, depending on their approach to classifying relations. Bottom-up
methods follow a clip-based approach where they classify relations of short
clip tubelet pairs and then merge them into long video relations. On the other
hand, top-down methods directly classify long video tubelet pairs. While recent
video-based methods utilizing video tubelets have shown promising results, we
argue that the effective modeling of spatial and temporal context plays a more
significant role than the choice between clip tubelets and video tubelets. This
motivates us to revisit the clip-based paradigm and explore the key success
factors in VidVRD. In this paper, we propose a Hierarchical Context Model (HCM)
that enriches the object-based spatial context and relation-based temporal
context based on clips. We demonstrate that using clip tubelets can achieve
superior performance compared to most video-based methods. Additionally, using
clip tubelets offers more flexibility in model designs and helps alleviate the
limitations associated with video tubelets, such as the challenging long-term
object tracking problem and the loss of temporal information in long-term
tubelet feature compression. Extensive experiments conducted on two challenging
VidVRD benchmarks validate that our HCM achieves a new state-of-the-art
performance, highlighting the effectiveness of incorporating advanced spatial
and temporal context modeling within the clip-based paradigm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1&quot;&gt;Meng Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Long Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1&quot;&gt;Wei Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1&quot;&gt;Roger Zimmermann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08988">
<title>EVIL: Evidential Inference Learning for Trustworthy Semi-supervised Medical Image Segmentation. (arXiv:2307.08988v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.08988</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, uncertainty-aware methods have attracted increasing attention in
semi-supervised medical image segmentation. However, current methods usually
suffer from the drawback that it is difficult to balance the computational
cost, estimation accuracy, and theoretical support in a unified framework. To
alleviate this problem, we introduce the Dempster-Shafer Theory of Evidence
(DST) into semi-supervised medical image segmentation, dubbed Evidential
Inference Learning (EVIL). EVIL provides a theoretically guaranteed solution to
infer accurate uncertainty quantification in a single forward pass. Trustworthy
pseudo labels on unlabeled data are generated after uncertainty estimation. The
recently proposed consistency regularization-based training paradigm is adopted
in our framework, which enforces the consistency on the perturbed predictions
to enhance the generalization with few labeled data. Experimental results show
that EVIL achieves competitive performance in comparison with several
state-of-the-art methods on the public dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yingyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Ziyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Chenyu Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhiwen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yang Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08991">
<title>EgoVM: Achieving Precise Ego-Localization using Lightweight Vectorized Maps. (arXiv:2307.08991v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.08991</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate and reliable ego-localization is critical for autonomous driving. In
this paper, we present EgoVM, an end-to-end localization network that achieves
comparable localization accuracy to prior state-of-the-art methods, but uses
lightweight vectorized maps instead of heavy point-based maps. To begin with,
we extract BEV features from online multi-view images and LiDAR point cloud.
Then, we employ a set of learnable semantic embeddings to encode the semantic
types of map elements and supervise them with semantic segmentation, to make
their feature representation consistent with BEV features. After that, we feed
map queries, composed of learnable semantic embeddings and coordinates of map
elements, into a transformer decoder to perform cross-modality matching with
BEV features. Finally, we adopt a robust histogram-based pose solver to
estimate the optimal pose by searching exhaustively over candidate poses. We
comprehensively validate the effectiveness of our method using both the
nuScenes dataset and a newly collected dataset. The experimental results show
that our method achieves centimeter-level localization accuracy, and
outperforms existing methods using vectorized maps by a large margin.
Furthermore, our model has been extensively tested in a large fleet of
autonomous vehicles under various challenging urban scenes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yuzhe He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1&quot;&gt;Shuang Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rui_X/0/1/0/all/0/1&quot;&gt;Xiaofei Rui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1&quot;&gt;Chengying Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_G/0/1/0/all/0/1&quot;&gt;Guowei Wan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08992">
<title>Arbitrary point cloud upsampling via Dual Back-Projection Network. (arXiv:2307.08992v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.08992</link>
<description rdf:parseType="Literal">&lt;p&gt;Point clouds acquired from 3D sensors are usually sparse and noisy. Point
cloud upsampling is an approach to increase the density of the point cloud so
that detailed geometric information can be restored. In this paper, we propose
a Dual Back-Projection network for point cloud upsampling (DBPnet). A Dual
Back-Projection is formulated in an up-down-up manner for point cloud
upsampling. It not only back projects feature residues but also coordinates
residues so that the network better captures the point correlations in the
feature and space domains, achieving lower reconstruction errors on both
uniform and non-uniform sparse point clouds. Our proposed method is also
generalizable for arbitrary upsampling tasks (e.g. 4x, 5.5x). Experimental
results show that the proposed method achieves the lowest point set matching
losses with respect to the benchmark. In addition, the success of our approach
demonstrates that generative networks are not necessarily needed for
non-uniform point clouds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhi-Song Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zijia Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1&quot;&gt;Zhen Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08994">
<title>Human Action Recognition in Still Images Using ConViT. (arXiv:2307.08994v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.08994</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding the relationship between different parts of the image plays a
crucial role in many visual recognition tasks. Despite the fact that
Convolutional Neural Networks (CNNs) have demonstrated impressive results in
detecting single objects, they lack the capability to extract the relationship
between various regions of an image, which is a crucial factor in human action
recognition. To address this problem, this paper proposes a new module that
functions like a convolutional layer using Vision Transformer (ViT). The
proposed action recognition model comprises two components: the first part is a
deep convolutional network that extracts high-level spatial features from the
image, and the second component of the model utilizes a Vision Transformer that
extracts the relationship between various regions of the image using the
feature map generated by the CNN output. The proposed model has been evaluated
on the Stanford40 and PASCAL VOC 2012 action datasets and has achieved 95.5%
mAP and 91.5% mAP results, respectively, which are promising compared to other
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hosseyni_S/0/1/0/all/0/1&quot;&gt;Seyed Rohollah Hosseyni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taheri_H/0/1/0/all/0/1&quot;&gt;Hasan Taheri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seyedin_S/0/1/0/all/0/1&quot;&gt;Sanaz Seyedin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahmani_A/0/1/0/all/0/1&quot;&gt;Ali Ahmad Rahmani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08995">
<title>Revisiting Latent Space of GAN Inversion for Real Image Editing. (arXiv:2307.08995v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.08995</link>
<description rdf:parseType="Literal">&lt;p&gt;The exploration of the latent space in StyleGANs and GAN inversion exemplify
impressive real-world image editing, yet the trade-off between reconstruction
quality and editing quality remains an open problem. In this study, we revisit
StyleGANs&apos; hyperspherical prior $\mathcal{Z}$ and combine it with highly
capable latent spaces to build combined spaces that faithfully invert real
images while maintaining the quality of edited images. More specifically, we
propose $\mathcal{F}/\mathcal{Z}^{+}$ space consisting of two subspaces:
$\mathcal{F}$ space of an intermediate feature map of StyleGANs enabling
faithful reconstruction and $\mathcal{Z}^{+}$ space of an extended StyleGAN
prior supporting high editing quality. We project the real images into the
proposed space to obtain the inverted codes, by which we then move along
$\mathcal{Z}^{+}$, enabling semantic editing without sacrificing image quality.
Comprehensive experiments show that $\mathcal{Z}^{+}$ can replace the most
commonly-used $\mathcal{W}$, $\mathcal{W}^{+}$, and $\mathcal{S}$ spaces while
preserving reconstruction quality, resulting in reduced distortion of edited
images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katsumata_K/0/1/0/all/0/1&quot;&gt;Kai Katsumata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vo_D/0/1/0/all/0/1&quot;&gt;Duc Minh Vo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakayama_H/0/1/0/all/0/1&quot;&gt;Hideki Nakayama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08996">
<title>Towards Authentic Face Restoration with Iterative Diffusion Models and Beyond. (arXiv:2307.08996v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.08996</link>
<description rdf:parseType="Literal">&lt;p&gt;An authentic face restoration system is becoming increasingly demanding in
many computer vision applications, e.g., image enhancement, video
communication, and taking portrait. Most of the advanced face restoration
models can recover high-quality faces from low-quality ones but usually fail to
faithfully generate realistic and high-frequency details that are favored by
users. To achieve authentic restoration, we propose $\textbf{IDM}$, an
$\textbf{I}$teratively learned face restoration system based on denoising
$\textbf{D}$iffusion $\textbf{M}$odels (DDMs). We define the criterion of an
authentic face restoration system, and argue that denoising diffusion models
are naturally endowed with this property from two aspects: intrinsic iterative
refinement and extrinsic iterative enhancement. Intrinsic learning can preserve
the content well and gradually refine the high-quality details, while extrinsic
enhancement helps clean the data and improve the restoration task one step
further. We demonstrate superior performance on blind face restoration tasks.
Beyond restoration, we find the authentically cleaned data by the proposed
restoration system is also helpful to image generation tasks in terms of
training stabilization and sample quality. Without modifying the models, we
achieve better quality than state-of-the-art on FFHQ and ImageNet generation
using either GANs or diffusion models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_T/0/1/0/all/0/1&quot;&gt;Tingbo Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yu-Chuan Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuhui Jia. Yandong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grundmann_M/0/1/0/all/0/1&quot;&gt;Matthias Grundmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09000">
<title>TractCloud: Registration-free tractography parcellation with a novel local-global streamline point cloud representation. (arXiv:2307.09000v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09000</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion MRI tractography parcellation classifies streamlines into
anatomical fiber tracts to enable quantification and visualization for clinical
and scientific applications. Current tractography parcellation methods rely
heavily on registration, but registration inaccuracies can affect parcellation
and the computational cost of registration is high for large-scale datasets.
Recently, deep-learning-based methods have been proposed for tractography
parcellation using various types of representations for streamlines. However,
these methods only focus on the information from a single streamline, ignoring
geometric relationships between the streamlines in the brain. We propose
TractCloud, a registration-free framework that performs whole-brain
tractography parcellation directly in individual subject space. We propose a
novel, learnable, local-global streamline representation that leverages
information from neighboring and whole-brain streamlines to describe the local
anatomy and global pose of the brain. We train our framework on a large-scale
labeled tractography dataset, which we augment by applying synthetic transforms
including rotation, scaling, and translations. We test our framework on five
independently acquired datasets across populations and health conditions.
TractCloud significantly outperforms several state-of-the-art methods on all
testing datasets. TractCloud achieves efficient and consistent whole-brain
white matter parcellation across the lifespan (from neonates to elderly
subjects, including brain tumor patients) without the need for registration.
The robustness and high inference speed of TractCloud make it suitable for
large-scale tractography data analysis. Our project page is available at
https://tractcloud.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_T/0/1/0/all/0/1&quot;&gt;Tengfei Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuqian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chaoyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golby_A/0/1/0/all/0/1&quot;&gt;Alexandra J. Golby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Makris_N/0/1/0/all/0/1&quot;&gt;Nikos Makris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rathi_Y/0/1/0/all/0/1&quot;&gt;Yogesh Rathi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1&quot;&gt;Weidong Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+ODonnell_L/0/1/0/all/0/1&quot;&gt;Lauren J. O&amp;#x27;Donnell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09004">
<title>Ord2Seq: Regard Ordinal Regression as Label Sequence Prediction. (arXiv:2307.09004v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.09004</link>
<description rdf:parseType="Literal">&lt;p&gt;Ordinal regression refers to classifying object instances into ordinal
categories. It has been widely studied in many scenarios, such as medical
disease grading, movie rating, etc. Known methods focused only on learning
inter-class ordinal relationships, but still incur limitations in
distinguishing adjacent categories thus far. In this paper, we propose a simple
sequence prediction framework for ordinal regression called Ord2Seq, which, for
the first time, transforms each ordinal category label into a special label
sequence and thus regards an ordinal regression task as a sequence prediction
process. In this way, we decompose an ordinal regression task into a series of
recursive binary classification steps, so as to subtly distinguish adjacent
categories. Comprehensive experiments show the effectiveness of distinguishing
adjacent categories for performance improvement and our new approach exceeds
state-of-the-art performances in four different scenarios. Codes will be
available upon acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinhong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jintai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tingting Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Danny Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jian Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09005">
<title>Frequency-mixed Single-source Domain Generalization for Medical Image Segmentation. (arXiv:2307.09005v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.09005</link>
<description rdf:parseType="Literal">&lt;p&gt;The annotation scarcity of medical image segmentation poses challenges in
collecting sufficient training data for deep learning models. Specifically,
models trained on limited data may not generalize well to other unseen data
domains, resulting in a domain shift issue. Consequently, domain generalization
(DG) is developed to boost the performance of segmentation models on unseen
domains. However, the DG setup requires multiple source domains, which impedes
the efficient deployment of segmentation algorithms in clinical scenarios. To
address this challenge and improve the segmentation model&apos;s generalizability,
we propose a novel approach called the Frequency-mixed Single-source Domain
Generalization method (FreeSDG). By analyzing the frequency&apos;s effect on domain
discrepancy, FreeSDG leverages a mixed frequency spectrum to augment the
single-source domain. Additionally, self-supervision is constructed in the
domain augmentation to learn robust context-aware representations for the
segmentation task. Experimental results on five datasets of three modalities
demonstrate the effectiveness of the proposed algorithm. FreeSDG outperforms
state-of-the-art methods and significantly improves the segmentation model&apos;s
generalizability. Therefore, FreeSDG provides a promising solution for
enhancing the generalization of medical image segmentation models, especially
when annotated data is scarce. The code is available at
https://github.com/liamheng/Non-IID_Medical_Image_Segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Heng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haojin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wei Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Huazhu Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Su_X/0/1/0/all/0/1&quot;&gt;Xiuyun Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09008">
<title>Soft-IntroVAE for Continuous Latent space Image Super-Resolution. (arXiv:2307.09008v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.09008</link>
<description rdf:parseType="Literal">&lt;p&gt;Continuous image super-resolution (SR) recently receives a lot of attention
from researchers, for its practical and flexible image scaling for various
displays. Local implicit image representation is one of the methods that can
map the coordinates and 2D features for latent space interpolation. Inspired by
Variational AutoEncoder, we propose a Soft-introVAE for continuous latent space
image super-resolution (SVAE-SR). A novel latent space adversarial training is
achieved for photo-realistic image restoration. To further improve the quality,
a positional encoding scheme is used to extend the original pixel coordinates
by aggregating frequency information over the pixel areas. We show the
effectiveness of the proposed SVAE-SR through quantitative and qualitative
comparisons, and further, illustrate its generalization in denoising and
real-image super-resolution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhi-Song Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zijia Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jia_Z/0/1/0/all/0/1&quot;&gt;Zhen Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09019">
<title>U-shaped Transformer: Retain High Frequency Context in Time Series Analysis. (arXiv:2307.09019v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09019</link>
<description rdf:parseType="Literal">&lt;p&gt;Time series prediction plays a crucial role in various industrial fields. In
recent years, neural networks with a transformer backbone have achieved
remarkable success in many domains, including computer vision and NLP. In time
series analysis domain, some studies have suggested that even the simplest MLP
networks outperform advanced transformer-based networks on time series forecast
tasks. However, we believe these findings indicate there to be low-rank
properties in time series sequences. In this paper, we consider the low-pass
characteristics of transformers and try to incorporate the advantages of MLP.
We adopt skip-layer connections inspired by Unet into traditional transformer
backbone, thus preserving high-frequency context from input to output, namely
U-shaped Transformer. We introduce patch merge and split operation to extract
features with different scales and use larger datasets to fully make use of the
transformer backbone. Our experiments demonstrate that the model performs at an
advanced level across multiple datasets with relatively low cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qingkui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yiqin Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09020">
<title>Face-PAST: Facial Pose Awareness and Style Transfer Networks. (arXiv:2307.09020v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09020</link>
<description rdf:parseType="Literal">&lt;p&gt;Facial style transfer has been quite popular among researchers due to the
rise of emerging technologies such as eXtended Reality (XR), Metaverse, and
Non-Fungible Tokens (NFTs). Furthermore, StyleGAN methods along with
transfer-learning strategies have reduced the problem of limited data to some
extent. However, most of the StyleGAN methods overfit the styles while adding
artifacts to facial images. In this paper, we propose a facial pose awareness
and style transfer (Face-PAST) network that preserves facial details and
structures while generating high-quality stylized images. Dual StyleGAN
inspires our work, but in contrast, our work uses a pre-trained style
generation network in an external style pass with a residual modulation block
instead of a transform coding block. Furthermore, we use the gated mapping unit
and facial structure, identity, and segmentation losses to preserve the facial
structure and details. This enables us to train the network with a very limited
amount of data while generating high-quality stylized images. Our training
process adapts curriculum learning strategy to perform efficient and flexible
style mixing in the generative space. We perform extensive experiments to show
the superiority of Face-PAST in comparison to existing state-of-the-art
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khowaja_S/0/1/0/all/0/1&quot;&gt;Sunder Ali Khowaja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mujtaba_G/0/1/0/all/0/1&quot;&gt;Ghulam Mujtaba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1&quot;&gt;Jiseok Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1&quot;&gt;Ik Hyun Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09023">
<title>LA-Net: Landmark-Aware Learning for Reliable Facial Expression Recognition under Label Noise. (arXiv:2307.09023v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09023</link>
<description rdf:parseType="Literal">&lt;p&gt;Facial expression recognition (FER) remains a challenging task due to the
ambiguity of expressions. The derived noisy labels significantly harm the
performance in real-world scenarios. To address this issue, we present a new
FER model named Landmark-Aware Net~(LA-Net), which leverages facial landmarks
to mitigate the impact of label noise from two perspectives. Firstly, LA-Net
uses landmark information to suppress the uncertainty in expression space and
constructs the label distribution of each sample by neighborhood aggregation,
which in turn improves the quality of training supervision. Secondly, the model
incorporates landmark information into expression representations using the
devised expression-landmark contrastive loss. The enhanced expression feature
extractor can be less susceptible to label noise. Our method can be integrated
with any deep neural network for better training supervision without
introducing extra inference costs. We conduct extensive experiments on both
in-the-wild datasets and synthetic noisy datasets and demonstrate that LA-Net
achieves state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhiyu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1&quot;&gt;Jinshi Cui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09026">
<title>ActionPrompt: Action-Guided 3D Human Pose Estimation With Text and Pose Prompting. (arXiv:2307.09026v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09026</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent 2D-to-3D human pose estimation (HPE) utilizes temporal consistency
across sequences to alleviate the depth ambiguity problem but ignore the action
related prior knowledge hidden in the pose sequence. In this paper, we propose
a plug-and-play module named Action Prompt Module (APM) that effectively mines
different kinds of action clues for 3D HPE. The highlight is that, the mining
scheme of APM can be widely adapted to different frameworks and bring
consistent benefits. Specifically, we first present a novel Action-related Text
Prompt module (ATP) that directly embeds action labels and transfers the rich
language information in the label to the pose sequence. Besides, we further
introduce Action-specific Pose Prompt module (APP) to mine the position-aware
pose pattern of each action, and exploit the correlation between the mined
patterns and input pose sequence for further pose refinement. Experiments show
that APM can improve the performance of most video-based 2D-to-3D HPE
frameworks by a large margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Hongwei Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Han Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Bowen Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1&quot;&gt;Wenrui Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_B/0/1/0/all/0/1&quot;&gt;Botao Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1&quot;&gt;Min Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Hongkai Xiong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09027">
<title>Online Self-Supervised Thermal Water Segmentation for Aerial Vehicles. (arXiv:2307.09027v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09027</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a new method to adapt an RGB-trained water segmentation network to
target-domain aerial thermal imagery using online self-supervision by
leveraging texture and motion cues as supervisory signals. This new thermal
capability enables current autonomous aerial robots operating in near-shore
environments to perform tasks such as visual navigation, bathymetry, and flow
tracking at night. Our method overcomes the problem of scarce and
difficult-to-obtain near-shore thermal data that prevents the application of
conventional supervised and unsupervised methods. In this work, we curate the
first aerial thermal near-shore dataset, show that our approach outperforms
fully-supervised segmentation models trained on limited target-domain thermal
data, and demonstrate real-time capabilities onboard an Nvidia Jetson embedded
computing platform. Code and datasets used in this work will be available at:
https://github.com/connorlee77/uav-thermal-water-segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1&quot;&gt;Connor Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frennert_J/0/1/0/all/0/1&quot;&gt;Jonathan Gustafsson Frennert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_L/0/1/0/all/0/1&quot;&gt;Lu Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anderson_M/0/1/0/all/0/1&quot;&gt;Matthew Anderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1&quot;&gt;Soon-Jo Chung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09039">
<title>PottsMGNet: A Mathematical Explanation of Encoder-Decoder Based Neural Networks. (arXiv:2307.09039v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09039</link>
<description rdf:parseType="Literal">&lt;p&gt;For problems in image processing and many other fields, a large class of
effective neural networks has encoder-decoder-based architectures. Although
these networks have made impressive performances, mathematical explanations of
their architectures are still underdeveloped. In this paper, we study the
encoder-decoder-based network architecture from the algorithmic perspective and
provide a mathematical explanation. We use the two-phase Potts model for image
segmentation as an example for our explanations. We associate the segmentation
problem with a control problem in the continuous setting. Then, multigrid
method and operator splitting scheme, the PottsMGNet, are used to discretize
the continuous control model. We show that the resulting discrete PottsMGNet is
equivalent to an encoder-decoder-based network. With minor modifications, it is
shown that a number of the popular encoder-decoder-based neural networks are
just instances of the proposed PottsMGNet. By incorporating the
Soft-Threshold-Dynamics into the PottsMGNet as a regularizer, the PottsMGNet
has shown to be robust with the network parameters such as network width and
depth and achieved remarkable performance on datasets with very large noise. In
nearly all our experiments, the new network always performs better or as good
on accuracy and dice score than existing networks for image segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tai_X/0/1/0/all/0/1&quot;&gt;Xue-Cheng Tai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_R/0/1/0/all/0/1&quot;&gt;Raymond Chan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09050">
<title>R-Cut: Enhancing Explainability in Vision Transformers with Relationship Weighted Out and Cut. (arXiv:2307.09050v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09050</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer-based models have gained popularity in the field of natural
language processing (NLP) and are extensively utilized in computer vision tasks
and multi-modal models such as GPT4. This paper presents a novel method to
enhance the explainability of Transformer-based image classification models.
Our method aims to improve trust in classification results and empower users to
gain a deeper understanding of the model for downstream tasks by providing
visualizations of class-specific maps. We introduce two modules: the
``Relationship Weighted Out&quot; and the ``Cut&quot; modules. The ``Relationship
Weighted Out&quot; module focuses on extracting class-specific information from
intermediate layers, enabling us to highlight relevant features. Additionally,
the ``Cut&quot; module performs fine-grained feature decomposition, taking into
account factors such as position, texture, and color. By integrating these
modules, we generate dense class-specific visual explainability maps. We
validate our method with extensive qualitative and quantitative experiments on
the ImageNet dataset. Furthermore, we conduct a large number of experiments on
the LRN dataset, specifically designed for automatic driving danger alerts, to
evaluate the explainability of our method in complex backgrounds. The results
demonstrate a significant improvement over previous methods. Moreover, we
conduct ablation experiments to validate the effectiveness of each module.
Through these experiments, we are able to confirm the respective contributions
of each module, thus solidifying the overall effectiveness of our proposed
approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1&quot;&gt;Yingjie Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1&quot;&gt;Ming Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_M/0/1/0/all/0/1&quot;&gt;Maoning Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karlsson_R/0/1/0/all/0/1&quot;&gt;Robin Karlsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuxiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takeda_K/0/1/0/all/0/1&quot;&gt;Kazuya Takeda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09052">
<title>Connections between Operator-splitting Methods and Deep Neural Networks with Applications in Image Segmentation. (arXiv:2307.09052v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09052</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural network is a powerful tool for many tasks. Understanding why it
is so successful and providing a mathematical explanation is an important
problem and has been one popular research direction in past years. In the
literature of mathematical analysis of deep deep neural networks, a lot of
works are dedicated to establishing representation theories. How to make
connections between deep neural networks and mathematical algorithms is still
under development. In this paper, we give an algorithmic explanation for deep
neural networks, especially in their connection with operator splitting and
multigrid methods. We show that with certain splitting strategies,
operator-splitting methods have the same structure as networks. Utilizing this
connection and the Potts model for image segmentation, two networks inspired by
operator-splitting methods are proposed. The two networks are essentially two
operator-splitting algorithms solving the Potts model. Numerical experiments
are presented to demonstrate the effectiveness of the proposed networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tai_X/0/1/0/all/0/1&quot;&gt;Xue-Cheng Tai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_R/0/1/0/all/0/1&quot;&gt;Raymond Chan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09055">
<title>Outlier-Robust Tensor Low-Rank Representation for Data Clustering. (arXiv:2307.09055v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2307.09055</link>
<description rdf:parseType="Literal">&lt;p&gt;Low-rank tensor analysis has received widespread attention with many
practical applications. However, the tensor data are often contaminated by
outliers or sample-specific corruptions. How to recover the tensor data that
are corrupted by outliers and perform data clustering remains a challenging
problem. This paper develops an outlier-robust tensor low-rank representation
(OR-TLRR) method for simultaneous outlier detection and tensor data clustering
based on the tensor singular value decomposition (t-SVD) algebraic framework.
It is motivated by the recently proposed tensor-tensor product induced by
invertible linear transforms that satisfy certain conditions. For tensor
observations with arbitrary outlier corruptions, OR-TLRR has provable
performance guarantee for exactly recovering the row space of clean data and
detecting outliers under mild conditions. Moreover, an extension of OR-TLRR is
also proposed to handle the case when parts of the data are missing. Finally,
extensive experimental results on both synthetic and real data demonstrate the
effectiveness of the proposed algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tong Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09059">
<title>Unleashing the Imagination of Text: A Novel Framework for Text-to-image Person Retrieval via Exploring the Power of Words. (arXiv:2307.09059v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.09059</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of Text-to-image person retrieval is to retrieve person images from
a large gallery that match the given textual descriptions. The main challenge
of this task lies in the significant differences in information representation
between the visual and textual modalities. The textual modality conveys
abstract and precise information through vocabulary and grammatical structures,
while the visual modality conveys concrete and intuitive information through
images. To fully leverage the expressive power of textual representations, it
is essential to accurately map abstract textual descriptions to specific
images.
&lt;/p&gt;
&lt;p&gt;To address this issue, we propose a novel framework to Unleash the
Imagination of Text (UIT) in text-to-image person retrieval, aiming to fully
explore the power of words in sentences. Specifically, the framework employs
the pre-trained full CLIP model as a dual encoder for the images and texts ,
taking advantage of prior cross-modal alignment knowledge. The Text-guided
Image Restoration auxiliary task is proposed with the aim of implicitly mapping
abstract textual entities to specific image regions, facilitating alignment
between textual and visual embeddings. Additionally, we introduce a cross-modal
triplet loss tailored for handling hard samples, enhancing the model&apos;s ability
to distinguish minor differences.
&lt;/p&gt;
&lt;p&gt;To focus the model on the key components within sentences, we propose a novel
text data augmentation technique. Our proposed methods achieve state-of-the-art
results on three popular benchmark datasets, and the source code will be made
publicly available shortly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Delong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haiwen Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09065">
<title>Learning Adaptive Neighborhoods for Graph Neural Networks. (arXiv:2307.09065v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09065</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph convolutional networks (GCNs) enable end-to-end learning on graph
structured data. However, many works assume a given graph structure. When the
input graph is noisy or unavailable, one approach is to construct or learn a
latent graph structure. These methods typically fix the choice of node degree
for the entire graph, which is suboptimal. Instead, we propose a novel
end-to-end differentiable graph generator which builds graph topologies where
each node selects both its neighborhood and its size. Our module can be readily
integrated into existing pipelines involving graph convolution operations,
replacing the predetermined or existing adjacency matrix with one that is
learned, and optimized, as part of the general objective. As such it is
applicable to any GCN. We integrate our module into trajectory prediction,
point cloud classification and node classification pipelines resulting in
improved accuracy over other structure-learning methods across a wide range of
datasets and GCN backbones.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1&quot;&gt;Avishkar Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mendez_O/0/1/0/all/0/1&quot;&gt;Oscar Mendez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russell_C/0/1/0/all/0/1&quot;&gt;Chris Russell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bowden_R/0/1/0/all/0/1&quot;&gt;Richard Bowden&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09066">
<title>PatchCT: Aligning Patch Set and Label Set with Conditional Transport for Multi-Label Image Classification. (arXiv:2307.09066v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09066</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-label image classification is a prediction task that aims to identify
more than one label from a given image. This paper considers the semantic
consistency of the latent space between the visual patch and linguistic label
domains and introduces the conditional transport (CT) theory to bridge the
acknowledged gap. While recent cross-modal attention-based studies have
attempted to align such two representations and achieved impressive
performance, they required carefully-designed alignment modules and extra
complex operations in the attention computation. We find that by formulating
the multi-label classification as a CT problem, we can exploit the interactions
between the image and label efficiently by minimizing the bidirectional CT
cost. Specifically, after feeding the images and textual labels into the
modality-specific encoders, we view each image as a mixture of patch embeddings
and a mixture of label embeddings, which capture the local region features and
the class prototypes, respectively. CT is then employed to learn and align
those two semantic sets by defining the forward and backward navigators.
Importantly, the defined navigators in CT distance model the similarities
between patches and labels, which provides an interpretable tool to visualize
the learned prototypes. Extensive experiments on three public image benchmarks
show that the proposed model consistently outperforms the previous methods. Our
code is available at https://github.com/keepgoingjkg/PatchCT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Miaoge Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dongsheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1&quot;&gt;Zequn Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1&quot;&gt;Ruiying Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mingyuan Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09067">
<title>Evaluate Fine-tuning Strategies for Fetal Head Ultrasound Image Segmentation with U-Net. (arXiv:2307.09067v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.09067</link>
<description rdf:parseType="Literal">&lt;p&gt;Fetal head segmentation is a crucial step in measuring the fetal head
circumference (HC) during gestation, an important biometric in obstetrics for
monitoring fetal growth. However, manual biometry generation is time-consuming
and results in inconsistent accuracy. To address this issue, convolutional
neural network (CNN) models have been utilized to improve the efficiency of
medical biometry. But training a CNN network from scratch is a challenging
task, we proposed a Transfer Learning (TL) method. Our approach involves
fine-tuning (FT) a U-Net network with a lightweight MobileNet as the encoder to
perform segmentation on a set of fetal head ultrasound (US) images with limited
effort. This method addresses the challenges associated with training a CNN
network from scratch. It suggests that our proposed FT strategy yields
segmentation performance that is comparable when trained with a reduced number
of parameters by 85.8%. And our proposed FT strategy outperforms other
strategies with smaller trainable parameter sizes below 4.4 million. Thus, we
contend that it can serve as a dependable FT approach for reducing the size of
models in medical image analysis. Our key findings highlight the importance of
the balance between model performance and size in developing Artificial
Intelligence (AI) applications by TL methods. Code is available at
https://github.&lt;a href=&quot;/abs/com/1320494&quot;&gt;com/1320494&lt;/a&gt;2/FT_Methods_for_Fetal_Head_Segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fangyijie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Silvestre_G/0/1/0/all/0/1&quot;&gt;Gu&amp;#xe9;nol&amp;#xe9; Silvestre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Curran_K/0/1/0/all/0/1&quot;&gt;Kathleen M. Curran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09070">
<title>PixelHuman: Animatable Neural Radiance Fields from Few Images. (arXiv:2307.09070v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09070</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose PixelHuman, a novel human rendering model that
generates animatable human scenes from a few images of a person with unseen
identity, views, and poses. Previous work have demonstrated reasonable
performance in novel view and pose synthesis, but they rely on a large number
of images to train and are trained per scene from videos, which requires
significant amount of time to produce animatable scenes from unseen human
images. Our method differs from existing methods in that it can generalize to
any input image for animatable human synthesis. Given a random pose sequence,
our method synthesizes each target scene using a neural radiance field that is
conditioned on a canonical representation and pose-aware pixel-aligned
features, both of which can be obtained through deformation fields learned in a
data-driven manner. Our experiments show that our method achieves
state-of-the-art performance in multiview and novel pose synthesis from
few-shot images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shim_G/0/1/0/all/0/1&quot;&gt;Gyumin Shim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jaeseong Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hyung_J/0/1/0/all/0/1&quot;&gt;Junha Hyung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1&quot;&gt;Jaegul Choo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09099">
<title>A Survey on Multi-Objective Neural Architecture Search. (arXiv:2307.09099v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09099</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the expert-crafted neural architectures is increasing overtaken by
the utilization of neural architecture search (NAS) and automatic generation
(and tuning) of network structures which has a close relation to the
Hyperparameter Optimization and Auto Machine Learning (AutoML). After the
earlier NAS attempts to optimize only the prediction accuracy, Multi-Objective
Neural architecture Search (MONAS) has been attracting attentions which
considers more goals such as computational complexity, power consumption, and
size of the network for optimization, reaching a trade-off between the accuracy
and other features like the computational cost. In this paper, we present an
overview of principal and state-of-the-art works in the field of MONAS.
Starting from a well-categorized taxonomy and formulation for the NAS, we
address and correct some miscategorizations in previous surveys of the NAS
field. We also provide a list of all known objectives used and add a number of
new ones and elaborate their specifications. We have provides analyses about
the most important objectives and shown that the stochastic properties of some
the them should be differed from deterministic ones in the multi-objective
optimization procedure of NAS. We finalize this paper with a number of future
directions and topics in the field of MONAS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shariatzadeh_S/0/1/0/all/0/1&quot;&gt;Seyed Mahdi Shariatzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fathy_M/0/1/0/all/0/1&quot;&gt;Mahmood Fathy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berangi_R/0/1/0/all/0/1&quot;&gt;Reza Berangi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahverdy_M/0/1/0/all/0/1&quot;&gt;Mohammad Shahverdy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09104">
<title>Division Gets Better: Learning Brightness-Aware and Detail-Sensitive Representations for Low-Light Image Enhancement. (arXiv:2307.09104v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09104</link>
<description rdf:parseType="Literal">&lt;p&gt;Low-light image enhancement strives to improve the contrast, adjust the
visibility, and restore the distortion in color and texture. Existing methods
usually pay more attention to improving the visibility and contrast via
increasing the lightness of low-light images, while disregarding the
significance of color and texture restoration for high-quality images. Against
above issue, we propose a novel luminance and chrominance dual branch network,
termed LCDBNet, for low-light image enhancement, which divides low-light image
enhancement into two sub-tasks, e.g., luminance adjustment and chrominance
restoration. Specifically, LCDBNet is composed of two branches, namely
luminance adjustment network (LAN) and chrominance restoration network (CRN).
LAN takes responsibility for learning brightness-aware features leveraging
long-range dependency and local attention correlation. While CRN concentrates
on learning detail-sensitive features via multi-level wavelet decomposition.
Finally, a fusion network is designed to blend their learned features to
produce visually impressive images. Extensive experiments conducted on seven
benchmark datasets validate the effectiveness of our proposed LCDBNet, and the
results manifest that LCDBNet achieves superior performance in terms of
multiple reference/non-reference quality evaluators compared to other
state-of-the-art competitors. Our code and pretrained model will be available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Huake Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1&quot;&gt;Xiaoyang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1&quot;&gt;Xingsong Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Junhui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dun_Y/0/1/0/all/0/1&quot;&gt;Yujie Dun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kaibing Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09109">
<title>Mining of Single-Class by Active Learning for Semantic Segmentation. (arXiv:2307.09109v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09109</link>
<description rdf:parseType="Literal">&lt;p&gt;Several Active Learning (AL) policies require retraining a target model
several times in order to identify the most informative samples and rarely
offer the option to focus on the acquisition of samples from underrepresented
classes. Here the Mining of Single-Class by Active Learning (MiSiCAL) paradigm
is introduced where an AL policy is constructed through deep reinforcement
learning and exploits quantity-accuracy correlations to build datasets on which
high-performance models can be trained with regards to specific classes.
MiSiCAL is especially helpful in the case of very large batch sizes since it
does not require repeated model training sessions as is common in other AL
methods. This is thanks to its ability to exploit fixed representations of the
candidate data points. We find that MiSiCAL is able to outperform a random
policy on 150 out of 171 COCO10k classes, while the strongest baseline only
outperforms random on 101 classes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lambert_H/0/1/0/all/0/1&quot;&gt;Hugues Lambert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Slade_E/0/1/0/all/0/1&quot;&gt;Emma Slade&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09112">
<title>NU-MCC: Multiview Compressive Coding with Neighborhood Decoder and Repulsive UDF. (arXiv:2307.09112v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09112</link>
<description rdf:parseType="Literal">&lt;p&gt;Remarkable progress has been made in 3D reconstruction from single-view RGB-D
inputs. MCC is the current state-of-the-art method in this field, which
achieves unprecedented success by combining vision Transformers with
large-scale training. However, we identified two key limitations of MCC: 1) The
Transformer decoder is inefficient in handling large number of query points; 2)
The 3D representation struggles to recover high-fidelity details. In this
paper, we propose a new approach called NU-MCC that addresses these
limitations. NU-MCC includes two key innovations: a Neighborhood decoder and a
Repulsive Unsigned Distance Function (Repulsive UDF). First, our Neighborhood
decoder introduces center points as an efficient proxy of input visual
features, allowing each query point to only attend to a small neighborhood.
This design not only results in much faster inference speed but also enables
the exploitation of finer-scale visual features for improved recovery of 3D
textures. Second, our Repulsive UDF is a novel alternative to the occupancy
field used in MCC, significantly improving the quality of 3D object
reconstruction. Compared to standard UDFs that suffer from holes in results,
our proposed Repulsive UDF can achieve more complete surface reconstruction.
Experimental results demonstrate that NU-MCC is able to learn a strong 3D
representation, significantly advancing the state of the art in single-view 3D
reconstruction. Particularly, it outperforms MCC by 9.7% in terms of the
F1-score on the CO3D-v2 dataset with more than 5x faster running speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lionar_S/0/1/0/all/0/1&quot;&gt;Stefan Lionar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiangyu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1&quot;&gt;Min Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1&quot;&gt;Gim Hee Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09120">
<title>Light-Weight Vision Transformer with Parallel Local and Global Self-Attention. (arXiv:2307.09120v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09120</link>
<description rdf:parseType="Literal">&lt;p&gt;While transformer architectures have dominated computer vision in recent
years, these models cannot easily be deployed on hardware with limited
resources for autonomous driving tasks that require real-time-performance.
Their computational complexity and memory requirements limits their use,
especially for applications with high-resolution inputs. In our work, we
redesign the powerful state-of-the-art Vision Transformer PLG-ViT to a much
more compact and efficient architecture that is suitable for such tasks. We
identify computationally expensive blocks in the original PLG-ViT architecture
and propose several redesigns aimed at reducing the number of parameters and
floating-point operations. As a result of our redesign, we are able to reduce
PLG-ViT in size by a factor of 5, with a moderate drop in performance. We
propose two variants, optimized for the best trade-off between parameter count
to runtime as well as parameter count to accuracy. With only 5 million
parameters, we achieve 79.5$\%$ top-1 accuracy on the ImageNet-1K
classification benchmark. Our networks demonstrate great performance on general
vision benchmarks like COCO instance segmentation. In addition, we conduct a
series of experiments, demonstrating the potential of our approach in solving
various tasks specifically tailored to the challenges of autonomous driving and
transportation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ebert_N/0/1/0/all/0/1&quot;&gt;Nikolas Ebert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reichardt_L/0/1/0/all/0/1&quot;&gt;Laurenz Reichardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1&quot;&gt;Didier Stricker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wasenmuller_O/0/1/0/all/0/1&quot;&gt;Oliver Wasenm&amp;#xfc;ller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09136">
<title>DropMix: Reducing Class Dependency in Mixed Sample Data Augmentation. (arXiv:2307.09136v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09136</link>
<description rdf:parseType="Literal">&lt;p&gt;Mixed sample data augmentation (MSDA) is a widely used technique that has
been found to improve performance in a variety of tasks. However, in this
paper, we show that the effects of MSDA are class-dependent, with some classes
seeing an improvement in performance while others experience a decline. To
reduce class dependency, we propose the DropMix method, which excludes a
specific percentage of data from the MSDA computation. By training on a
combination of MSDA and non-MSDA data, the proposed method not only improves
the performance of classes that were previously degraded by MSDA, but also
increases overall average accuracy, as shown in experiments on two datasets
(CIFAR-100 and ImageNet) using three MSDA methods (Mixup, CutMix and
PuzzleMix).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Haeil Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hansang Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Junmo Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09143">
<title>MVA2023 Small Object Detection Challenge for Spotting Birds: Dataset, Methods, and Results. (arXiv:2307.09143v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09143</link>
<description rdf:parseType="Literal">&lt;p&gt;Small Object Detection (SOD) is an important machine vision topic because (i)
a variety of real-world applications require object detection for distant
objects and (ii) SOD is a challenging task due to the noisy, blurred, and
less-informative image appearances of small objects. This paper proposes a new
SOD dataset consisting of 39,070 images including 137,121 bird instances, which
is called the Small Object Detection for Spotting Birds (SOD4SB) dataset. The
detail of the challenge with the SOD4SB dataset is introduced in this paper. In
total, 223 participants joined this challenge. This paper briefly introduces
the award-winning methods. The dataset, the baseline code, and the website for
evaluation on the public testset are publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kondo_Y/0/1/0/all/0/1&quot;&gt;Yuki Kondo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ukita_N/0/1/0/all/0/1&quot;&gt;Norimichi Ukita&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamaguchi_T/0/1/0/all/0/1&quot;&gt;Takayuki Yamaguchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_H/0/1/0/all/0/1&quot;&gt;Hao-Yu Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1&quot;&gt;Mu-Yi Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1&quot;&gt;Chia-Chi Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_E/0/1/0/all/0/1&quot;&gt;En-Ming Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yu-Chen Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yu-Cheng Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chien-Yao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1&quot;&gt;Chun-Yi Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huo_D/0/1/0/all/0/1&quot;&gt;Da Huo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kastner_M/0/1/0/all/0/1&quot;&gt;Marc A. Kastner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tingwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawanishi_Y/0/1/0/all/0/1&quot;&gt;Yasutomo Kawanishi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hirayama_T/0/1/0/all/0/1&quot;&gt;Takatsugu Hirayama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Komamizu_T/0/1/0/all/0/1&quot;&gt;Takahiro Komamizu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ide_I/0/1/0/all/0/1&quot;&gt;Ichiro Ide&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shinya_Y/0/1/0/all/0/1&quot;&gt;Yosuke Shinya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinyao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_G/0/1/0/all/0/1&quot;&gt;Guang Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yasui_S/0/1/0/all/0/1&quot;&gt;Syusuke Yasui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09146">
<title>PRO-Face S: Privacy-preserving Reversible Obfuscation of Face Images via Secure Flow. (arXiv:2307.09146v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09146</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a novel paradigm for facial privacy protection that
unifies multiple characteristics including anonymity, diversity, reversibility
and security within a single lightweight framework. We name it PRO-Face S,
short for Privacy-preserving Reversible Obfuscation of Face images via Secure
flow-based model. In the framework, an Invertible Neural Network (INN) is
utilized to process the input image along with its pre-obfuscated form, and
generate the privacy protected image that visually approximates to the
pre-obfuscated one, thus ensuring privacy. The pre-obfuscation applied can be
in diversified form with different strengths and styles specified by users.
Along protection, a secret key is injected into the network such that the
original image can only be recovered from the protection image via the same
model given the correct key provided. Two modes of image recovery are devised
to deal with malicious recovery attempts in different scenarios. Finally,
extensive experiments conducted on three public image datasets demonstrate the
superiority of the proposed framework over multiple state-of-the-art
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1&quot;&gt;Lin Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1&quot;&gt;Kai Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pu_X/0/1/0/all/0/1&quot;&gt;Xiao Pu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leng_J/0/1/0/all/0/1&quot;&gt;Jiaxu Leng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Nannan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xinbo Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09153">
<title>OPHAvatars: One-shot Photo-realistic Head Avatars. (arXiv:2307.09153v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09153</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a method for synthesizing photo-realistic digital avatars from
only one portrait as the reference. Given a portrait, our method synthesizes a
coarse talking head video using driving keypoints features. And with the coarse
video, our method synthesizes a coarse talking head avatar with a deforming
neural radiance field. With rendered images of the coarse avatar, our method
updates the low-quality images with a blind face restoration model. With
updated images, we retrain the avatar for higher quality. After several
iterations, our method can synthesize a photo-realistic animatable 3D neural
head avatar. The motivation of our method is deformable neural radiance field
can eliminate the unnatural distortion caused by the image2video method. Our
method outperforms state-of-the-art methods in quantitative and qualitative
studies on various subjects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shaoxu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09155">
<title>MLF-DET: Multi-Level Fusion for Cross-Modal 3D Object Detection. (arXiv:2307.09155v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09155</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel and effective Multi-Level Fusion network,
named as MLF-DET, for high-performance cross-modal 3D object DETection, which
integrates both the feature-level fusion and decision-level fusion to fully
utilize the information in the image. For the feature-level fusion, we present
the Multi-scale Voxel Image fusion (MVI) module, which densely aligns
multi-scale voxel features with image features. For the decision-level fusion,
we propose the lightweight Feature-cued Confidence Rectification (FCR) module
which further exploits image semantics to rectify the confidence of detection
candidates. Besides, we design an effective data augmentation strategy termed
Occlusion-aware GT Sampling (OGS) to reserve more sampled objects in the
training scenes, so as to reduce overfitting. Extensive experiments on the
KITTI dataset demonstrate the effectiveness of our method. Notably, on the
extremely competitive KITTI car 3D object detection benchmark, our method
reaches 82.89% moderate AP and achieves state-of-the-art performance without
bells and whistles.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zewei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yanqing Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Sanping Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shitao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1&quot;&gt;Nanning Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09158">
<title>Class-relation Knowledge Distillation for Novel Class Discovery. (arXiv:2307.09158v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09158</link>
<description rdf:parseType="Literal">&lt;p&gt;We tackle the problem of novel class discovery, which aims to learn novel
classes without supervision based on labeled data from known classes. A key
challenge lies in transferring the knowledge in the known-class data to the
learning of novel classes. Previous methods mainly focus on building a shared
representation space for knowledge transfer and often ignore modeling class
relations. To address this, we introduce a class relation representation for
the novel classes based on the predicted class distribution of a model trained
on known classes. Empirically, we find that such class relation becomes less
informative during typical discovery training. To prevent such information
loss, we propose a novel knowledge distillation framework, which utilizes our
class-relation representation to regularize the learning of novel classes. In
addition, to enable a flexible knowledge distillation scheme for each data
point in novel classes, we develop a learnable weighting function for the
regularization, which adaptively promotes knowledge transfer based on the
semantic similarity between the novel and known classes. To validate the
effectiveness and generalization of our method, we conduct extensive
experiments on multiple benchmarks, including CIFAR100, Stanford Cars, CUB, and
FGVC-Aircraft datasets. Our results demonstrate that the proposed method
outperforms the previous state-of-the-art methods by a significant margin on
almost all benchmarks. Code is available at
\href{https://github.com/kleinzcy/Cr-KD-NCD}{here}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_P/0/1/0/all/0/1&quot;&gt;Peiyan Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chuyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Ruijie Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xuming He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09160">
<title>Constraining Depth Map Geometry for Multi-View Stereo: A Dual-Depth Approach with Saddle-shaped Depth Cells. (arXiv:2307.09160v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09160</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning-based multi-view stereo (MVS) methods deal with predicting accurate
depth maps to achieve an accurate and complete 3D representation. Despite the
excellent performance, existing methods ignore the fact that a suitable depth
geometry is also critical in MVS. In this paper, we demonstrate that different
depth geometries have significant performance gaps, even using the same depth
prediction error. Therefore, we introduce an ideal depth geometry composed of
Saddle-Shaped Cells, whose predicted depth map oscillates upward and downward
around the ground-truth surface, rather than maintaining a continuous and
smooth depth plane. To achieve it, we develop a coarse-to-fine framework called
Dual-MVSNet (DMVSNet), which can produce an oscillating depth plane.
Technically, we predict two depth values for each pixel (Dual-Depth), and
propose a novel loss function and a checkerboard-shaped selecting strategy to
constrain the predicted depth geometry. Compared to existing methods,DMVSNet
achieves a high rank on the DTU benchmark and obtains the top performance on
challenging scenes of Tanks and Temples, demonstrating its strong performance
and generalization ability. Our method also points to a new research direction
for considering depth geometry in MVS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1&quot;&gt;Xinyi Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Weiyue Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tianqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zihao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1&quot;&gt;Zhiguo Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09161">
<title>CG-fusion CAM: Online segmentation of laser-induced damage on large-aperture optics. (arXiv:2307.09161v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09161</link>
<description rdf:parseType="Literal">&lt;p&gt;Online segmentation of laser-induced damage on large-aperture optics in
high-power laser facilities is challenged by complicated damage morphology,
uneven illumination and stray light interference. Fully supervised semantic
segmentation algorithms have achieved state-of-the-art performance, but rely on
plenty of pixel-level labels, which are time-consuming and labor-consuming to
produce. LayerCAM, an advanced weakly supervised semantic segmentation
algorithm, can generate pixel-accurate results using only image-level labels,
but its scattered and partially under-activated class activation regions
degrade segmentation performance. In this paper, we propose a weakly supervised
semantic segmentation method with Continuous Gradient CAM and its nonlinear
multi-scale fusion (CG-fusion CAM). The method redesigns the way of
back-propagating gradients and non-linearly activates the multi-scale fused
heatmaps to generate more fine-grained class activation maps with appropriate
activation degree for different sizes of damage sites. Experiments on our
dataset show that the proposed method can achieve segmentation performance
comparable to that of fully supervised algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yueyue Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yingyan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hangcheng Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1&quot;&gt;Fengdong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_F/0/1/0/all/0/1&quot;&gt;Fa Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1&quot;&gt;Zhitao Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1&quot;&gt;Qihua Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guodong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09165">
<title>Towards Trustworthy Dataset Distillation. (arXiv:2307.09165v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09165</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficiency and trustworthiness are two eternal pursuits when applying deep
learning in real-world applications. With regard to efficiency, dataset
distillation (DD) endeavors to reduce training costs by distilling the large
dataset into a tiny synthetic dataset. However, existing methods merely
concentrate on in-distribution (InD) classification in a closed-world setting,
disregarding out-of-distribution (OOD) samples. On the other hand, OOD
detection aims to enhance models&apos; trustworthiness, which is always
inefficiently achieved in full-data settings. For the first time, we
simultaneously consider both issues and propose a novel paradigm called
Trustworthy Dataset Distillation (TrustDD). By distilling both InD samples and
outliers, the condensed datasets are capable to train models competent in both
InD classification and OOD detection. To alleviate the requirement of real
outlier data and make OOD detection more practical, we further propose to
corrupt InD samples to generate pseudo-outliers and introduce Pseudo-Outlier
Exposure (POE). Comprehensive experiments on various settings demonstrate the
effectiveness of TrustDD, and the proposed POE surpasses state-of-the-art
method Outlier Exposure (OE). Compared with the preceding DD, TrustDD is more
trustworthy and applicable to real open-world scenarios. Our code will be
publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Shijie Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1&quot;&gt;Fei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1&quot;&gt;Zhen Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xu-Yao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09172">
<title>Jean-Luc Picard at Touch\&apos;e 2023: Comparing Image Generation, Stance Detection and Feature Matching for Image Retrieval for Arguments. (arXiv:2307.09172v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2307.09172</link>
<description rdf:parseType="Literal">&lt;p&gt;Participating in the shared task &quot;Image Retrieval for arguments&quot;, we used
different pipelines for image retrieval containing Image Generation, Stance
Detection, Preselection and Feature Matching. We submitted four different runs
with different pipeline layout and compare them to given baseline. Our
pipelines perform similarly to the baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moebius_M/0/1/0/all/0/1&quot;&gt;Max Moebius&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Enderling_M/0/1/0/all/0/1&quot;&gt;Maximilian Enderling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bachinger_S/0/1/0/all/0/1&quot;&gt;Sarah T. Bachinger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09183">
<title>Pixel-wise Graph Attention Networks for Person Re-identification. (arXiv:2307.09183v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09183</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph convolutional networks (GCN) is widely used to handle irregular data
since it updates node features by using the structure information of graph.
With the help of iterated GCN, high-order information can be obtained to
further enhance the representation of nodes. However, how to apply GCN to
structured data (such as pictures) has not been deeply studied. In this paper,
we explore the application of graph attention networks (GAT) in image feature
extraction. First of all, we propose a novel graph generation algorithm to
convert images into graphs through matrix transformation. It is one magnitude
faster than the algorithm based on K Nearest Neighbors (KNN). Then, GAT is used
on the generated graph to update the node features. Thus, a more robust
representation is obtained. These two steps are combined into a module called
pixel-wise graph attention module (PGA). Since the graph obtained by our graph
generation algorithm can still be transformed into a picture after processing,
PGA can be well combined with CNN. Based on these two modules, we consulted the
ResNet and design a pixel-wise graph attention network (PGANet). The PGANet is
applied to the task of person re-identification in the datasets Market1501,
DukeMTMC-reID and Occluded-DukeMTMC (outperforms state-of-the-art by 0.8\%,
1.1\% and 11\% respectively, in mAP scores). Experiment results show that it
achieves the state-of-the-art performance.
\href{https://github.com/wenyu1009/PGANet}{The code is available here}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Q/0/1/0/all/0/1&quot;&gt;Qing Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jian Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yi Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1&quot;&gt;Mingzhe Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09184">
<title>You&apos;ve Got Two Teachers: Co-evolutionary Image and Report Distillation for Semi-supervised Anatomical Abnormality Detection in Chest X-ray. (arXiv:2307.09184v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09184</link>
<description rdf:parseType="Literal">&lt;p&gt;Chest X-ray (CXR) anatomical abnormality detection aims at localizing and
characterising cardiopulmonary radiological findings in the radiographs, which
can expedite clinical workflow and reduce observational oversights. Most
existing methods attempted this task in either fully supervised settings which
demanded costly mass per-abnormality annotations, or weakly supervised settings
which still lagged badly behind fully supervised methods in performance. In
this work, we propose a co-evolutionary image and report distillation (CEIRD)
framework, which approaches semi-supervised abnormality detection in CXR by
grounding the visual detection results with text-classified abnormalities from
paired radiology reports, and vice versa. Concretely, based on the classical
teacher-student pseudo label distillation (TSD) paradigm, we additionally
introduce an auxiliary report classification model, whose prediction is used
for report-guided pseudo detection label refinement (RPDLR) in the primary
vision detection task. Inversely, we also use the prediction of the vision
detection model for abnormality-guided pseudo classification label refinement
(APCLR) in the auxiliary report classification task, and propose a co-evolution
strategy where the vision and report models mutually promote each other with
RPDLR and APCLR performed alternatively. To this end, we effectively
incorporate the weak supervision by reports into the semi-supervised TSD
pipeline. Besides the cross-modal pseudo label refinement, we further propose
an intra-image-modal self-adaptive non-maximum suppression, where the pseudo
detection labels generated by the teacher vision model are dynamically
rectified by high-confidence predictions by the student. Experimental results
on the public MIMIC-CXR benchmark demonstrate CEIRD&apos;s superior performance to
several up-to-date weakly and semi-supervised methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jinghan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1&quot;&gt;Dong Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhe Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1&quot;&gt;Donghuan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liansheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yefeng Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09218">
<title>A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning. (arXiv:2307.09218v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09218</link>
<description rdf:parseType="Literal">&lt;p&gt;Forgetting refers to the loss or deterioration of previously acquired
information or knowledge. While the existing surveys on forgetting have
primarily focused on continual learning, forgetting is a prevalent phenomenon
observed in various other research domains within deep learning. Forgetting
manifests in research fields such as generative models due to generator shifts,
and federated learning due to heterogeneous data distributions across clients.
Addressing forgetting encompasses several challenges, including balancing the
retention of old task knowledge with fast learning of new tasks, managing task
interference with conflicting goals, and preventing privacy leakage, etc.
Moreover, most existing surveys on continual learning implicitly assume that
forgetting is always harmful. In contrast, our survey argues that forgetting is
a double-edged sword and can be beneficial and desirable in certain cases, such
as privacy-preserving scenarios. By exploring forgetting in a broader context,
we aim to present a more nuanced understanding of this phenomenon and highlight
its potential advantages. Through this comprehensive survey, we aspire to
uncover potential solutions by drawing upon ideas and approaches from various
fields that have dealt with forgetting. By examining forgetting beyond its
conventional boundaries, in future work, we hope to encourage the development
of novel strategies for mitigating, harnessing, or even embracing forgetting in
real applications. A comprehensive list of papers about forgetting in various
research fields is available at
\url{https://github.com/EnnengYang/Awesome-Forgetting-in-Deep-Learning}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhenyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1&quot;&gt;Enneng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Li Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Heng Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09220">
<title>A Survey on Open-Vocabulary Detection and Segmentation: Past, Present, and Future. (arXiv:2307.09220v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09220</link>
<description rdf:parseType="Literal">&lt;p&gt;As the most fundamental tasks of computer vision, object detection and
segmentation have made tremendous progress in the deep learning era. Due to the
expensive manual labeling, the annotated categories in existing datasets are
often small-scale and pre-defined, i.e., state-of-the-art detectors and
segmentors fail to generalize beyond the closed-vocabulary. To resolve this
limitation, the last few years have witnessed increasing attention toward
Open-Vocabulary Detection (OVD) and Segmentation (OVS). In this survey, we
provide a comprehensive review on the past and recent development of OVD and
OVS. To this end, we develop a taxonomy according to the type of task and
methodology. We find that the permission and usage of weak supervision signals
can well discriminate different methodologies, including: visual-semantic space
mapping, novel visual feature synthesis, region-aware training,
pseudo-labeling, knowledge distillation-based, and transfer learning-based. The
proposed taxonomy is universal across different tasks, covering object
detection, semantic/instance/panoptic segmentation, 3D scene and video
understanding. In each category, its main principles, key challenges,
development routes, strengths, and weaknesses are thoroughly discussed. In
addition, we benchmark each task along with the vital components of each
method. Finally, several promising directions are provided to stimulate future
research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Chaoyang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Long Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09233">
<title>Augmenting CLIP with Improved Visio-Linguistic Reasoning. (arXiv:2307.09233v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09233</link>
<description rdf:parseType="Literal">&lt;p&gt;Image-text contrastive models such as CLIP are useful for a variety of
downstream applications including zero-shot classification, image-text
retrieval and transfer learning. However, these contrastively trained
vision-language models often fail on compositional visio-linguistic tasks such
as Winoground with performance equivalent to random chance. In our paper, we
address this issue and propose a sample-efficient light-weight method called
SDS-CLIP to improve the compositional visio-linguistic reasoning capabilities
of CLIP. The core idea of our method is to use differentiable image
parameterizations to fine-tune CLIP with a distillation objective from large
text-to-image generative models such as Stable-Diffusion which are relatively
good at visio-linguistic reasoning tasks. On the challenging Winoground
compositional reasoning benchmark, our method improves the absolute
visio-linguistic performance of different CLIP models by up to 7%, while on the
ARO dataset, our method improves the visio-linguistic performance by upto 3%.
As a byproduct of inducing visio-linguistic reasoning into CLIP, we also find
that the zero-shot performance improves marginally on a variety of downstream
datasets. Our method reinforces that carefully designed distillation objectives
from generative models can be leveraged to extend existing contrastive
image-text models with improved visio-linguistic reasoning capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1&quot;&gt;Samyadeep Basu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanjabi_M/0/1/0/all/0/1&quot;&gt;Maziar Sanjabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Massiceti_D/0/1/0/all/0/1&quot;&gt;Daniela Massiceti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shell Xu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1&quot;&gt;Soheil Feizi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09238">
<title>Fusing Hand and Body Skeletons for Human Action Recognition in Assembly. (arXiv:2307.09238v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09238</link>
<description rdf:parseType="Literal">&lt;p&gt;As collaborative robots (cobots) continue to gain popularity in industrial
manufacturing, effective human-robot collaboration becomes crucial. Cobots
should be able to recognize human actions to assist with assembly tasks and act
autonomously. To achieve this, skeleton-based approaches are often used due to
their ability to generalize across various people and environments. Although
body skeleton approaches are widely used for action recognition, they may not
be accurate enough for assembly actions where the worker&apos;s fingers and hands
play a significant role. To address this limitation, we propose a method in
which less detailed body skeletons are combined with highly detailed hand
skeletons. We investigate CNNs and transformers, the latter of which are
particularly adept at extracting and combining important information from both
skeleton types using attention. This paper demonstrates the effectiveness of
our proposed approach in enhancing action recognition in assembly scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aganian_D/0/1/0/all/0/1&quot;&gt;Dustin Aganian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kohler_M/0/1/0/all/0/1&quot;&gt;Mona K&amp;#xf6;hler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stephan_B/0/1/0/all/0/1&quot;&gt;Benedict Stephan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eisenbach_M/0/1/0/all/0/1&quot;&gt;Markus Eisenbach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gross_H/0/1/0/all/0/1&quot;&gt;Horst-Michael Gross&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09239">
<title>Generation of High Spatial Resolution Terrestrial Surface from Low Spatial Resolution Elevation Contour Maps via Hierarchical Computation of Median Elevation Regions. (arXiv:2307.09239v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09239</link>
<description rdf:parseType="Literal">&lt;p&gt;We proposed a simple yet effective morphological approach to convert a sparse
Digital Elevation Model (DEM) to a dense Digital Elevation Model. The
conversion is similar to that of the generation of high-resolution DEM from its
low-resolution DEM. The approach involves the generation of median contours to
achieve the purpose. It is a sequential step of the I) decomposition of the
existing sparse Contour map into the maximum possible Threshold Elevation
Region (TERs). II) Computing all possible non-negative and non-weighted Median
Elevation Region (MER) hierarchically between the successive TER decomposed
from a sparse contour map. III) Computing the gradient of all TER, and MER
computed from previous steps would yield the predicted intermediate elevation
contour at a higher spatial resolution. We presented this approach initially
with some self-made synthetic data to show how the contour prediction works and
then experimented with the available contour map of Washington, NH to justify
its usefulness. This approach considers the geometric information of existing
contours and interpolates the elevation contour at a new spatial region of a
topographic surface until no elevation contours are necessary to generate. This
novel approach is also very low-cost and robust as it uses elevation contours.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barman_G/0/1/0/all/0/1&quot;&gt;Geetika Barman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sagar_B/0/1/0/all/0/1&quot;&gt;B.S. Daya Sagar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09259">
<title>Adaptive Topological Feature via Persistent Homology: Filtration Learning for Point Clouds. (arXiv:2307.09259v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09259</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning for point clouds has been attracting much attention, with
many applications in various fields, such as shape recognition and material
science. To enhance the accuracy of such machine learning methods, it is known
to be effective to incorporate global topological features, which are typically
extracted by persistent homology. In the calculation of persistent homology for
a point cloud, we need to choose a filtration for the point clouds, an
increasing sequence of spaces. Because the performance of machine learning
methods combined with persistent homology is highly affected by the choice of a
filtration, we need to tune it depending on data and tasks. In this paper, we
propose a framework that learns a filtration adaptively with the use of neural
networks. In order to make the resulting persistent homology
isometry-invariant, we develop a neural network architecture with such
invariance. Additionally, we theoretically show a finite-dimensional
approximation result that justifies our architecture. Experimental results
demonstrated the efficacy of our framework in several classification tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nishikawa_N/0/1/0/all/0/1&quot;&gt;Naoki Nishikawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ike_Y/0/1/0/all/0/1&quot;&gt;Yuichi Ike&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamanishi_K/0/1/0/all/0/1&quot;&gt;Kenji Yamanishi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09262">
<title>Neuromorphic spintronics simulated using an unconventional data-driven Thiele equation approach. (arXiv:2307.09262v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09262</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we developed a quantitative description of the dynamics of
spin-torque vortex nano-oscillators (STVOs) through an unconventional model
based on the combination of the Thiele equation approach (TEA) and data from
micromagnetic simulations (MMS). Solving the STVO dynamics with our analytical
model allows to accelerate the simulations by 9 orders of magnitude compared to
MMS while reaching the same level of accuracy. Here, we showcase our model by
simulating a STVO-based neural network for solving a classification task. We
assess its performance with respect to the input signal current intensity and
the level of noise that might affect such a system. Our approach is promising
for accelerating the design of STVO-based neuromorphic computing devices while
decreasing drastically its computational cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moureaux_A/0/1/0/all/0/1&quot;&gt;Anatole Moureaux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wergifosse_S/0/1/0/all/0/1&quot;&gt;Simon de Wergifosse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chopin_C/0/1/0/all/0/1&quot;&gt;Chlo&amp;#xe9; Chopin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Araujo_F/0/1/0/all/0/1&quot;&gt;Flavio Abreu Araujo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09264">
<title>Knowledge Distillation for Object Detection: from generic to remote sensing datasets. (arXiv:2307.09264v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09264</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge distillation, a well-known model compression technique, is an
active research area in both computer vision and remote sensing communities. In
this paper, we evaluate in a remote sensing context various off-the-shelf
object detection knowledge distillation methods which have been originally
developed on generic computer vision datasets such as Pascal VOC. In
particular, methods covering both logit mimicking and feature imitation
approaches are applied for vehicle detection using the well-known benchmarks
such as xView and VEDAI datasets. Extensive experiments are performed to
compare the relative performance and interrelationships of the methods.
Experimental results show high variations and confirm the importance of result
aggregation and cross validation on remote sensing datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1&quot;&gt;Ho&amp;#xe0;ng-&amp;#xc2;n L&amp;#xea;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_M/0/1/0/all/0/1&quot;&gt;Minh-Tan Pham&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09267">
<title>Distilling Coarse-to-Fine Semantic Matching Knowledge for Weakly Supervised 3D Visual Grounding. (arXiv:2307.09267v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09267</link>
<description rdf:parseType="Literal">&lt;p&gt;3D visual grounding involves finding a target object in a 3D scene that
corresponds to a given sentence query. Although many approaches have been
proposed and achieved impressive performance, they all require dense
object-sentence pair annotations in 3D point clouds, which are both
time-consuming and expensive. To address the problem that fine-grained
annotated data is difficult to obtain, we propose to leverage weakly supervised
annotations to learn the 3D visual grounding model, i.e., only coarse
scene-sentence correspondences are used to learn object-sentence links. To
accomplish this, we design a novel semantic matching model that analyzes the
semantic similarity between object proposals and sentences in a coarse-to-fine
manner. Specifically, we first extract object proposals and coarsely select the
top-K candidates based on feature and class similarity matrices. Next, we
reconstruct the masked keywords of the sentence using each candidate one by
one, and the reconstructed accuracy finely reflects the semantic similarity of
each candidate to the query. Additionally, we distill the coarse-to-fine
semantic matching knowledge into a typical two-stage 3D visual grounding model,
which reduces inference costs and improves performance by taking full advantage
of the well-studied structure of the existing architectures. We conduct
extensive experiments on ScanRefer, Nr3D, and Sr3D, which demonstrate the
effectiveness of our proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zehan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Haifeng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Linjun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xize Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yichen Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_A/0/1/0/all/0/1&quot;&gt;Aoxiong Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhou Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09279">
<title>Regression-free Blind Image Quality Assessment. (arXiv:2307.09279v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09279</link>
<description rdf:parseType="Literal">&lt;p&gt;Regression-based blind image quality assessment (IQA) models are susceptible
to biased training samples, leading to a biased estimation of model parameters.
To mitigate this issue, we propose a regression-free framework for image
quality evaluation, which is founded upon retrieving similar instances by
incorporating semantic and distortion features. The motivation behind this
approach is rooted in the observation that the human visual system (HVS) has
analogous visual responses to semantically similar image contents degraded by
the same distortion. The proposed framework comprises two classification-based
modules: semantic-based classification (SC) module and distortion-based
classification (DC) module. Given a test image and an IQA database, the SC
module retrieves multiple pristine images based on semantic similarity. The DC
module then retrieves instances based on distortion similarity from the
distorted images that correspond to each retrieved pristine image. Finally, the
predicted quality score is derived by aggregating the subjective quality scores
of multiple retrieved instances. Experimental results on four benchmark
databases validate that the proposed model can remarkably outperform the
state-of-the-art regression-based models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaoqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1&quot;&gt;Jian Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1&quot;&gt;Hao Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Weisi Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09283">
<title>RepViT: Revisiting Mobile CNN From ViT Perspective. (arXiv:2307.09283v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09283</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, lightweight Vision Transformers (ViTs) demonstrate superior
performance and lower latency compared with lightweight Convolutional Neural
Networks (CNNs) on resource-constrained mobile devices. This improvement is
usually attributed to the multi-head self-attention module, which enables the
model to learn global representations. However, the architectural disparities
between lightweight ViTs and lightweight CNNs have not been adequately
examined. In this study, we revisit the efficient design of lightweight CNNs
and emphasize their potential for mobile devices. We incrementally enhance the
mobile-friendliness of a standard lightweight CNN, specifically MobileNetV3, by
integrating the efficient architectural choices of lightweight ViTs. This ends
up with a new family of pure lightweight CNNs, namely RepViT. Extensive
experiments show that RepViT outperforms existing state-of-the-art lightweight
ViTs and exhibits favorable latency in various vision tasks. On ImageNet,
RepViT achieves over 80\% top-1 accuracy with nearly 1ms latency on an iPhone
12, which is the first time for a lightweight model, to the best of our
knowledge. Our largest model, RepViT-M3, obtains 81.4\% accuracy with only
1.3ms latency. The code and trained models are available at
\url{https://github.com/jameslahm/RepViT}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1&quot;&gt;Ao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zijia Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pu_H/0/1/0/all/0/1&quot;&gt;Hengjun Pu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1&quot;&gt;Guiguang Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09302">
<title>Conformal prediction under ambiguous ground truth. (arXiv:2307.09302v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09302</link>
<description rdf:parseType="Literal">&lt;p&gt;In safety-critical classification tasks, conformal prediction allows to
perform rigorous uncertainty quantification by providing confidence sets
including the true class with a user-specified probability. This generally
assumes the availability of a held-out calibration set with access to ground
truth labels. Unfortunately, in many domains, such labels are difficult to
obtain and usually approximated by aggregating expert opinions. In fact, this
holds true for almost all datasets, including well-known ones such as CIFAR and
ImageNet. Applying conformal prediction using such labels underestimates
uncertainty. Indeed, when expert opinions are not resolvable, there is inherent
ambiguity present in the labels. That is, we do not have ``crisp&apos;&apos;, definitive
ground truth labels and this uncertainty should be taken into account during
calibration. In this paper, we develop a conformal prediction framework for
such ambiguous ground truth settings which relies on an approximation of the
underlying posterior distribution of labels given inputs. We demonstrate our
methodology on synthetic and real datasets, including a case study of skin
condition classification in dermatology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stutz_D/0/1/0/all/0/1&quot;&gt;David Stutz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1&quot;&gt;Abhijit Guha Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matejovicova_T/0/1/0/all/0/1&quot;&gt;Tatiana Matejovicova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strachan_P/0/1/0/all/0/1&quot;&gt;Patricia Strachan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cemgil_A/0/1/0/all/0/1&quot;&gt;Ali Taylan Cemgil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doucet_A/0/1/0/all/0/1&quot;&gt;Arnaud Doucet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09306">
<title>EigenTrajectory: Low-Rank Descriptors for Multi-Modal Trajectory Forecasting. (arXiv:2307.09306v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09306</link>
<description rdf:parseType="Literal">&lt;p&gt;Capturing high-dimensional social interactions and feasible futures is
essential for predicting trajectories. To address this complex nature, several
attempts have been devoted to reducing the dimensionality of the output
variables via parametric curve fitting such as the B\&apos;ezier curve and B-spline
function. However, these functions, which originate in computer graphics
fields, are not suitable to account for socially acceptable human dynamics. In
this paper, we present EigenTrajectory ($\mathbb{ET}$), a trajectory prediction
approach that uses a novel trajectory descriptor to form a compact space, known
here as $\mathbb{ET}$ space, in place of Euclidean space, for representing
pedestrian movements. We first reduce the complexity of the trajectory
descriptor via a low-rank approximation. We transform the pedestrians&apos; history
paths into our $\mathbb{ET}$ space represented by spatio-temporal principle
components, and feed them into off-the-shelf trajectory forecasting models. The
inputs and outputs of the models as well as social interactions are all
gathered and aggregated in the corresponding $\mathbb{ET}$ space. Lastly, we
propose a trajectory anchor-based refinement method to cover all possible
futures in the proposed $\mathbb{ET}$ space. Extensive experiments demonstrate
that our EigenTrajectory predictor can significantly improve both the
prediction accuracy and reliability of existing trajectory forecasting models
on public benchmarks, indicating that the proposed descriptor is suited to
represent pedestrian behaviors. Code is publicly available at
https://github.com/inhwanbae/EigenTrajectory .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bae_I/0/1/0/all/0/1&quot;&gt;Inhwan Bae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1&quot;&gt;Jean Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeon_H/0/1/0/all/0/1&quot;&gt;Hae-Gon Jeon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09316">
<title>MarS3D: A Plug-and-Play Motion-Aware Model for Semantic Segmentation on Multi-Scan 3D Point Clouds. (arXiv:2307.09316v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09316</link>
<description rdf:parseType="Literal">&lt;p&gt;3D semantic segmentation on multi-scan large-scale point clouds plays an
important role in autonomous systems. Unlike the single-scan-based semantic
segmentation task, this task requires distinguishing the motion states of
points in addition to their semantic categories. However, methods designed for
single-scan-based segmentation tasks perform poorly on the multi-scan task due
to the lacking of an effective way to integrate temporal information. We
propose MarS3D, a plug-and-play motion-aware module for semantic segmentation
on multi-scan 3D point clouds. This module can be flexibly combined with
single-scan models to allow them to have multi-scan perception abilities. The
model encompasses two key designs: the Cross-Frame Feature Embedding module for
enriching representation learning and the Motion-Aware Feature Learning module
for enhancing motion awareness. Extensive experiments show that MarS3D can
improve the performance of the baseline model by a large margin. The code is
available at https://github.com/CVMI-Lab/MarS3D.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiahui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1&quot;&gt;Chirui Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jianhui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaoyang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1&quot;&gt;Xiaojuan Qi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09323">
<title>Efficient Region-Aware Neural Radiance Fields for High-Fidelity Talking Portrait Synthesis. (arXiv:2307.09323v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09323</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents ER-NeRF, a novel conditional Neural Radiance Fields
(NeRF) based architecture for talking portrait synthesis that can concurrently
achieve fast convergence, real-time rendering, and state-of-the-art performance
with small model size. Our idea is to explicitly exploit the unequal
contribution of spatial regions to guide talking portrait modeling.
Specifically, to improve the accuracy of dynamic head reconstruction, a compact
and expressive NeRF-based Tri-Plane Hash Representation is introduced by
pruning empty spatial regions with three planar hash encoders. For speech
audio, we propose a Region Attention Module to generate region-aware condition
feature via an attention mechanism. Different from existing methods that
utilize an MLP-based encoder to learn the cross-modal relation implicitly, the
attention mechanism builds an explicit connection between audio features and
spatial regions to capture the priors of local motions. Moreover, a direct and
fast Adaptive Pose Encoding is introduced to optimize the head-torso separation
problem by mapping the complex transformation of the head pose into spatial
coordinates. Extensive experiments demonstrate that our method renders better
high-fidelity and audio-lips synchronized talking portrait videos, with
realistic details and high efficiency compared to previous methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiahe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiawei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1&quot;&gt;Xiao Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_L/0/1/0/all/0/1&quot;&gt;Lin Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09329">
<title>Towards a performance analysis on pre-trained Visual Question Answering models for autonomous driving. (arXiv:2307.09329v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09329</link>
<description rdf:parseType="Literal">&lt;p&gt;This short paper presents a preliminary analysis of three popular Visual
Question Answering (VQA) models, namely ViLBERT, ViLT, and LXMERT, in the
context of answering questions relating to driving scenarios. The performance
of these models is evaluated by comparing the similarity of responses to
reference answers provided by computer vision experts. Model selection is
predicated on the analysis of transformer utilization in multimodal
architectures. The results indicate that models incorporating cross-modal
attention and late fusion techniques exhibit promising potential for generating
improved answers within a driving perspective. This initial analysis serves as
a launchpad for a forthcoming comprehensive comparative study involving nine
VQA models and sets the scene for further investigations into the effectiveness
of VQA model queries in self-driving scenarios. Supplementary material is
available at
https://github.com/KaavyaRekanar/Towards-a-performance-analysis-on-pre-trained-VQA-models-for-autonomous-driving.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rekanar_K/0/1/0/all/0/1&quot;&gt;Kaavya Rekanar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eising_C/0/1/0/all/0/1&quot;&gt;Ciar&amp;#xe1;n Eising&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sistu_G/0/1/0/all/0/1&quot;&gt;Ganesh Sistu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayes_M/0/1/0/all/0/1&quot;&gt;Martin Hayes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09330">
<title>Visual Validation versus Visual Estimation: A Study on the Average Value in Scatterplots. (arXiv:2307.09330v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09330</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the ability of individuals to visually validate statistical
models in terms of their fit to the data. While visual model estimation has
been studied extensively, visual model validation remains under-investigated.
It is unknown how well people are able to visually validate models, and how
their performance compares to visual and computational estimation. As a
starting point, we conducted a study across two populations (crowdsourced and
volunteers). Participants had to both visually estimate (i.e, draw) and
visually validate (i.e., accept or reject) the frequently studied model of
averages. Across both populations, the level of accuracy of the models that
were considered valid was lower than the accuracy of the estimated models. We
find that participants&apos; validation and estimation were unbiased. Moreover,
their natural critical point between accepting and rejecting a given mean value
is close to the boundary of its 95% confidence interval, indicating that the
visually perceived confidence interval corresponds to a common statistical
standard. Our work contributes to the understanding of visual model validation
and opens new research opportunities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braun_D/0/1/0/all/0/1&quot;&gt;Daniel Braun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suh_A/0/1/0/all/0/1&quot;&gt;Ashley Suh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_R/0/1/0/all/0/1&quot;&gt;Remco Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gleicher_M/0/1/0/all/0/1&quot;&gt;Michael Gleicher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Landesberger_T/0/1/0/all/0/1&quot;&gt;Tatiana von Landesberger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09351">
<title>SphereNet: Learning a Noise-Robust and General Descriptor for Point Cloud Registration. (arXiv:2307.09351v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09351</link>
<description rdf:parseType="Literal">&lt;p&gt;Point cloud registration is to estimate a transformation to align point
clouds collected in different perspectives. In learning-based point cloud
registration, a robust descriptor is vital for high-accuracy registration.
However, most methods are susceptible to noise and have poor generalization
ability on unseen datasets. Motivated by this, we introduce SphereNet to learn
a noise-robust and unseen-general descriptor for point cloud registration. In
our method, first, the spheroid generator builds a geometric domain based on
spherical voxelization to encode initial features. Then, the spherical
interpolation of the sphere is introduced to realize robustness against noise.
Finally, a new spherical convolutional neural network with spherical integrity
padding completes the extraction of descriptors, which reduces the loss of
features and fully captures the geometric features. To evaluate our methods, a
new benchmark 3DMatch-noise with strong noise is introduced. Extensive
experiments are carried out on both indoor and outdoor datasets. Under
high-intensity noise, SphereNet increases the feature matching recall by more
than 25 percentage points on 3DMatch-noise. In addition, it sets a new
state-of-the-art performance for the 3DMatch and 3DLoMatch benchmarks with
93.5\% and 75.6\% registration recall and also has the best generalization
ability on unseen datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1&quot;&gt;Guiyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zhentao Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Hongbin Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09356">
<title>OnlineRefer: A Simple Online Baseline for Referring Video Object Segmentation. (arXiv:2307.09356v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09356</link>
<description rdf:parseType="Literal">&lt;p&gt;Referring video object segmentation (RVOS) aims at segmenting an object in a
video following human instruction. Current state-of-the-art methods fall into
an offline pattern, in which each clip independently interacts with text
embedding for cross-modal understanding. They usually present that the offline
pattern is necessary for RVOS, yet model limited temporal association within
each clip. In this work, we break up the previous offline belief and propose a
simple yet effective online model using explicit query propagation, named
OnlineRefer. Specifically, our approach leverages target cues that gather
semantic information and position prior to improve the accuracy and ease of
referring predictions for the current frame. Furthermore, we generalize our
online model into a semi-online framework to be compatible with video-based
backbones. To show the effectiveness of our method, we evaluate it on four
benchmarks, \ie, Refer-Youtube-VOS, Refer-DAVIS17, A2D-Sentences, and
JHMDB-Sentences. Without bells and whistles, our OnlineRefer with a Swin-L
backbone achieves 63.5 J&amp;amp;F and 64.8 J&amp;amp;F on Refer-Youtube-VOS and Refer-DAVIS17,
outperforming all other offline methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Dongming Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tiancai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiangyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1&quot;&gt;Jianbing Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09361">
<title>MOCA: Self-supervised Representation Learning by Predicting Masked Online Codebook Assignments. (arXiv:2307.09361v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09361</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised learning can be used for mitigating the greedy needs of
Vision Transformer networks for very large fully-annotated datasets. Different
classes of self-supervised learning offer representations with either good
contextual reasoning properties, e.g., using masked image modeling strategies,
or invariance to image perturbations, e.g., with contrastive methods. In this
work, we propose a single-stage and standalone method, MOCA, which unifies both
desired properties using novel mask-and-predict objectives defined with
high-level features (instead of pixel-level details). Moreover, we show how to
effectively employ both learning paradigms in a synergistic and
computation-efficient way. Doing so, we achieve new state-of-the-art results on
low-shot settings and strong experimental results in various evaluation
protocols with a training that is at least 3 times faster than prior methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gidaris_S/0/1/0/all/0/1&quot;&gt;Spyros Gidaris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bursuc_A/0/1/0/all/0/1&quot;&gt;Andrei Bursuc&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simeoni_O/0/1/0/all/0/1&quot;&gt;Oriane Simeoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vobecky_A/0/1/0/all/0/1&quot;&gt;Antonin Vobecky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Komodakis_N/0/1/0/all/0/1&quot;&gt;Nikos Komodakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1&quot;&gt;Matthieu Cord&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1&quot;&gt;Patrick P&amp;#xe9;rez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09362">
<title>Disentangle then Parse:Night-time Semantic Segmentation with Illumination Disentanglement. (arXiv:2307.09362v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09362</link>
<description rdf:parseType="Literal">&lt;p&gt;Most prior semantic segmentation methods have been developed for day-time
scenes, while typically underperforming in night-time scenes due to
insufficient and complicated lighting conditions. In this work, we tackle this
challenge by proposing a novel night-time semantic segmentation paradigm, i.e.,
disentangle then parse (DTP). DTP explicitly disentangles night-time images
into light-invariant reflectance and light-specific illumination components and
then recognizes semantics based on their adaptive fusion. Concretely, the
proposed DTP comprises two key components: 1) Instead of processing
lighting-entangled features as in prior works, our Semantic-Oriented
Disentanglement (SOD) framework enables the extraction of reflectance component
without being impeded by lighting, allowing the network to consistently
recognize the semantics under cover of varying and complicated lighting
conditions. 2) Based on the observation that the illumination component can
serve as a cue for some semantically confused regions, we further introduce an
Illumination-Aware Parser (IAParser) to explicitly learn the correlation
between semantics and lighting, and aggregate the illumination features to
yield more precise predictions. Extensive experiments on the night-time
segmentation task with various settings demonstrate that DTP significantly
outperforms state-of-the-art methods. Furthermore, with negligible additional
parameters, DTP can be directly used to benefit existing day-time methods for
night-time segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1&quot;&gt;Zhixiang Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_T/0/1/0/all/0/1&quot;&gt;Tao Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huaian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_P/0/1/0/all/0/1&quot;&gt;Pengyang Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yi Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09365">
<title>An Evaluation of Zero-Cost Proxies -- from Neural Architecture Performance to Model Robustness. (arXiv:2307.09365v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09365</link>
<description rdf:parseType="Literal">&lt;p&gt;Zero-cost proxies are nowadays frequently studied and used to search for
neural architectures. They show an impressive ability to predict the
performance of architectures by making use of their untrained weights. These
techniques allow for immense search speed-ups. So far the joint search for
well-performing and robust architectures has received much less attention in
the field of NAS. Therefore, the main focus of zero-cost proxies is the clean
accuracy of architectures, whereas the model robustness should play an evenly
important part. In this paper, we analyze the ability of common zero-cost
proxies to serve as performance predictors for robustness in the popular
NAS-Bench-201 search space. We are interested in the single prediction task for
robustness and the joint multi-objective of clean and robust accuracy. We
further analyze the feature importance of the proxies and show that predicting
the robustness makes the prediction task from existing zero-cost proxies more
challenging. As a result, the joint consideration of several proxies becomes
necessary to predict a model&apos;s robustness while the clean accuracy can be
regressed from a single such feature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lukasik_J/0/1/0/all/0/1&quot;&gt;Jovita Lukasik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moeller_M/0/1/0/all/0/1&quot;&gt;Michael Moeller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keuper_M/0/1/0/all/0/1&quot;&gt;Margret Keuper&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09367">
<title>LEST: Large-scale LiDAR Semantic Segmentation with Transformer. (arXiv:2307.09367v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09367</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale LiDAR-based point cloud semantic segmentation is a critical task
in autonomous driving perception. Almost all of the previous state-of-the-art
LiDAR semantic segmentation methods are variants of sparse 3D convolution.
Although the Transformer architecture is becoming popular in the field of
natural language processing and 2D computer vision, its application to
large-scale point cloud semantic segmentation is still limited. In this paper,
we propose a LiDAR sEmantic Segmentation architecture with pure Transformer,
LEST. LEST comprises two novel components: a Space Filling Curve (SFC) Grouping
strategy and a Distance-based Cosine Linear Transformer, DISCO. On the public
nuScenes semantic segmentation validation set and SemanticKITTI test set, our
model outperforms all the other state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1&quot;&gt;Chuanyu Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1&quot;&gt;Nuo Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Sikun Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Han Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaohan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1&quot;&gt;Shengguang Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Pu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09368">
<title>Plug the Leaks: Advancing Audio-driven Talking Face Generation by Preventing Unintended Information Flow. (arXiv:2307.09368v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09368</link>
<description rdf:parseType="Literal">&lt;p&gt;Audio-driven talking face generation is the task of creating a
lip-synchronized, realistic face video from given audio and reference frames.
This involves two major challenges: overall visual quality of generated images
on the one hand, and audio-visual synchronization of the mouth part on the
other hand. In this paper, we start by identifying several problematic aspects
of synchronization methods in recent audio-driven talking face generation
approaches. Specifically, this involves unintended flow of lip and pose
information from the reference to the generated image, as well as instabilities
during model training. Subsequently, we propose various techniques for
obviating these issues: First, a silent-lip reference image generator prevents
leaking of lips from the reference to the generated image. Second, an adaptive
triplet loss handles the pose leaking problem. Finally, we propose a stabilized
formulation of synchronization loss, circumventing aforementioned training
instabilities while additionally further alleviating the lip leaking issue.
Combining the individual improvements, we present state-of-the art performance
on LRS2 and LRW in both synchronization and visual quality. We further validate
our design in various ablation experiments, confirming the individual
contributions as well as their complementary effects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yaman_D/0/1/0/all/0/1&quot;&gt;Dogucan Yaman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eyiokur_F/0/1/0/all/0/1&quot;&gt;Fevziye Irem Eyiokur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barmann_L/0/1/0/all/0/1&quot;&gt;Leonard B&amp;#xe4;rmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ekenel_H/0/1/0/all/0/1&quot;&gt;Hazim Kemal Ekenel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1&quot;&gt;Alexander Waibel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09402">
<title>Study of Vision Transformers for Covid-19 Detection from Chest X-rays. (arXiv:2307.09402v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.09402</link>
<description rdf:parseType="Literal">&lt;p&gt;The COVID-19 pandemic has led to a global health crisis, highlighting the
need for rapid and accurate virus detection. This research paper examines
transfer learning with vision transformers for COVID-19 detection, known for
its excellent performance in image recognition tasks. We leverage the
capability of Vision Transformers to capture global context and learn complex
patterns from chest X-ray images. In this work, we explored the recent
state-of-art transformer models to detect Covid-19 using CXR images such as
vision transformer (ViT), Swin-transformer, Max vision transformer (MViT), and
Pyramid Vision transformer (PVT). Through the utilization of transfer learning
with IMAGENET weights, the models achieved an impressive accuracy range of
98.75% to 99.5%. Our experiments demonstrate that Vision Transformers achieve
state-of-the-art performance in COVID-19 detection, outperforming traditional
methods and even Convolutional Neural Networks (CNNs). The results highlight
the potential of Vision Transformers as a powerful tool for COVID-19 detection,
with implications for improving the efficiency and accuracy of screening and
diagnosis in clinical settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Angara_S/0/1/0/all/0/1&quot;&gt;Sandeep Angara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Thirunagaru_S/0/1/0/all/0/1&quot;&gt;Sharath Thirunagaru&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09416">
<title>Let&apos;s ViCE! Mimicking Human Cognitive Behavior in Image Generation Evaluation. (arXiv:2307.09416v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09416</link>
<description rdf:parseType="Literal">&lt;p&gt;Research in Image Generation has recently made significant progress,
particularly boosted by the introduction of Vision-Language models which are
able to produce high-quality visual content based on textual inputs. Despite
ongoing advancements in terms of generation quality and realism, no methodical
frameworks have been defined yet to quantitatively measure the quality of the
generated content and the adherence with the prompted requests: so far, only
human-based evaluations have been adopted for quality satisfaction and for
comparing different generative methods. We introduce a novel automated method
for Visual Concept Evaluation (ViCE), i.e. to assess consistency between a
generated/edited image and the corresponding prompt/instructions, with a
process inspired by the human cognitive behaviour. ViCE combines the strengths
of Large Language Models (LLMs) and Visual Question Answering (VQA) into a
unified pipeline, aiming to replicate the human cognitive process in quality
assessment. This method outlines visual concepts, formulates image-specific
verification questions, utilizes the Q&amp;amp;A system to investigate the image, and
scores the combined outcome. Although this brave new hypothesis of mimicking
humans in the image evaluation process is in its preliminary assessment stage,
results are promising and open the door to a new form of automatic evaluation
which could have significant impact as the image generation or the image target
editing tasks become more and more sophisticated.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Betti_F/0/1/0/all/0/1&quot;&gt;Federico Betti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Staiano_J/0/1/0/all/0/1&quot;&gt;Jacopo Staiano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1&quot;&gt;Lorenzo Baraldi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1&quot;&gt;Lorenzo Baraldi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1&quot;&gt;Rita Cucchiara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1&quot;&gt;Nicu Sebe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09420">
<title>Measuring Student Behavioral Engagement using Histogram of Actions. (arXiv:2307.09420v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09420</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel technique for measuring behavioral
engagement through students&apos; actions recognition. The proposed approach
recognizes student actions then predicts the student behavioral engagement
level. For student action recognition, we use human skeletons to model student
postures and upper body movements. To learn the dynamics of student upper body,
a 3D-CNN model is used. The trained 3D-CNN model is used to recognize actions
within every 2minute video segment then these actions are used to build a
histogram of actions which encodes the student actions and their frequencies.
This histogram is utilized as an input to SVM classifier to classify whether
the student is engaged or disengaged. To evaluate the proposed framework, we
build a dataset consisting of 1414 2-minute video segments annotated with 13
actions and 112 video segments annotated with two engagement levels.
Experimental results indicate that student actions can be recognized with top 1
accuracy 83.63% and the proposed framework can capture the average engagement
of the class.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdelkawy_A/0/1/0/all/0/1&quot;&gt;Ahmed Abdelkawy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alkabbany_I/0/1/0/all/0/1&quot;&gt;Islam Alkabbany&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1&quot;&gt;Asem Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farag_A/0/1/0/all/0/1&quot;&gt;Aly Farag&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09437">
<title>Unsupervised Conditional Slot Attention for Object Centric Learning. (arXiv:2307.09437v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09437</link>
<description rdf:parseType="Literal">&lt;p&gt;Extracting object-level representations for downstream reasoning tasks is an
emerging area in AI. Learning object-centric representations in an unsupervised
setting presents multiple challenges, a key one being binding an arbitrary
number of object instances to a specialized object slot. Recent object-centric
representation methods like Slot Attention utilize iterative attention to learn
composable representations with dynamic inference level binding but fail to
achieve specialized slot level binding. To address this, in this paper we
propose Unsupervised Conditional Slot Attention using a novel Probabilistic
Slot Dictionary (PSD). We define PSD with (i) abstract object-level property
vectors as key and (ii) parametric Gaussian distribution as its corresponding
value. We demonstrate the benefits of the learnt specific object-level
conditioning distributions in multiple downstream tasks, namely object
discovery, compositional scene generation, and compositional visual reasoning.
We show that our method provides scene composition capabilities and a
significant boost in a few shot adaptability tasks of compositional visual
reasoning, while performing similarly or better than slot attention in object
discovery tasks
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kori_A/0/1/0/all/0/1&quot;&gt;Avinash Kori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1&quot;&gt;Francesco Locatello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toni_F/0/1/0/all/0/1&quot;&gt;Francesca Toni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1&quot;&gt;Ben Glocker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09456">
<title>A comparative analysis of SR-GAN models. (arXiv:2307.09456v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09456</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we evaluate the performance of multiple state-of-the-art SR
GAN (Super Resolution Generative Adversarial Network) models, ESRGAN,
Real-ESRGAN and EDSR, on a benchmark dataset of real-world images which undergo
degradation using a pipeline. Our results show that some models seem to
significantly increase the resolution of the input images while preserving
their visual quality, this is assessed using Tesseract OCR engine. We observe
that EDSR-BASE model from huggingface outperforms the remaining candidate
models in terms of both quantitative metrics and subjective visual quality
assessments with least compute overhead. Specifically, EDSR generates images
with higher peak signal-to-noise ratio (PSNR) and structural similarity index
(SSIM) values and are seen to return high quality OCR results with Tesseract
OCR engine. These findings suggest that EDSR is a robust and effective approach
for single-image super-resolution and may be particularly well-suited for
applications where high-quality visual fidelity is critical and optimized
compute.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikroo_F/0/1/0/all/0/1&quot;&gt;Fatemeh Rezapoor Nikroo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deshmukh_A/0/1/0/all/0/1&quot;&gt;Ajinkya Deshmukh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1&quot;&gt;Anantha Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tam_A/0/1/0/all/0/1&quot;&gt;Adrian Tam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_K/0/1/0/all/0/1&quot;&gt;Kaarthik Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noris_C/0/1/0/all/0/1&quot;&gt;Cleo Noris&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09465">
<title>Occlusion Aware Student Emotion Recognition based on Facial Action Unit Detection. (arXiv:2307.09465v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09465</link>
<description rdf:parseType="Literal">&lt;p&gt;Given that approximately half of science, technology, engineering, and
mathematics (STEM) undergraduate students in U.S. colleges and universities
leave by the end of the first year [15], it is crucial to improve the quality
of classroom environments. This study focuses on monitoring students&apos; emotions
in the classroom as an indicator of their engagement and proposes an approach
to address this issue. The impact of different facial parts on the performance
of an emotional recognition model is evaluated through experimentation. To test
the proposed model under partial occlusion, an artificially occluded dataset is
introduced. The novelty of this work lies in the proposal of an occlusion-aware
architecture for facial action units (AUs) extraction, which employs attention
mechanism and adaptive feature learning. The AUs can be used later to classify
facial expressions in classroom settings.
&lt;/p&gt;
&lt;p&gt;This research paper&apos;s findings provide valuable insights into handling
occlusion in analyzing facial images for emotional engagement analysis. The
proposed experiments demonstrate the significance of considering occlusion and
enhancing the reliability of facial analysis models in classroom environments.
These findings can also be extended to other settings where occlusions are
prevalent.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wally_S/0/1/0/all/0/1&quot;&gt;Shrouk Wally&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elsayed_A/0/1/0/all/0/1&quot;&gt;Ahmed Elsayed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alkabbany_I/0/1/0/all/0/1&quot;&gt;Islam Alkabbany&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1&quot;&gt;Asem Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farag_A/0/1/0/all/0/1&quot;&gt;Aly Farag&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09472">
<title>GroupLane: End-to-End 3D Lane Detection with Channel-wise Grouping. (arXiv:2307.09472v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09472</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficiency is quite important for 3D lane detection due to practical
deployment demand. In this work, we propose a simple, fast, and end-to-end
detector that still maintains high detection precision. Specifically, we devise
a set of fully convolutional heads based on row-wise classification. In
contrast to previous counterparts, ours supports recognizing both vertical and
horizontal lanes. Besides, our method is the first one to perform row-wise
classification in bird-eye-view. In the heads, we split feature into multiple
groups and every group of feature corresponds to a lane instance. During
training, the predictions are associated with lane labels using the proposed
single-win one-to-one matching to compute loss, and no post-processing
operation is demanded for inference. In this way, our proposed fully
convolutional detector, GroupLane, realizes end-to-end detection like DETR.
Evaluated on 3 real world 3D lane benchmarks, OpenLane, Once-3DLanes, and
OpenLane-Huawei, GroupLane adopting ConvNext-Base as the backbone outperforms
the published state-of-the-art PersFormer by 13.6% F1 score in the OpenLane
validation set. Besides, GroupLane with ResNet18 still surpasses PersFormer by
4.9% F1 score, while the inference speed is nearly 7x faster and the FLOPs is
only 13.3% of it.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhuoling Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1&quot;&gt;Chunrui Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1&quot;&gt;Zheng Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jinrong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_E/0/1/0/all/0/1&quot;&gt;En Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haoqian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hengshuang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiangyu Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09474">
<title>ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning. (arXiv:2307.09474v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.09474</link>
<description rdf:parseType="Literal">&lt;p&gt;Human-AI interactivity is a critical aspect that reflects the usability of
multimodal large language models (MLLMs). However, existing end-to-end MLLMs
only allow users to interact with them through language instructions, leading
to the limitation of the interactive accuracy and efficiency. In this study, we
present precise referring instructions that utilize diverse reference
representations such as points and boxes as referring prompts to refer to the
special region. This enables MLLMs to focus on the region of interest and
achieve finer-grained interaction. Based on precise referring instruction, we
propose ChatSpot, a unified end-to-end multimodal large language model that
supports diverse forms of interactivity including mouse clicks, drag-and-drop,
and drawing boxes, which provides a more flexible and seamless interactive
experience. We also construct a multi-grained vision-language
instruction-following dataset based on existing datasets and GPT-4 generating.
Furthermore, we design a series of evaluation tasks to assess the effectiveness
of region recognition and interaction. Experimental results showcase ChatSpot&apos;s
promising performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Liang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_E/0/1/0/all/0/1&quot;&gt;En Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1&quot;&gt;Zheng Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jinrong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1&quot;&gt;Haoran Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hongyu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jianjian Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yuang Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1&quot;&gt;Runpei Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1&quot;&gt;Chunrui Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiangyu Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09480">
<title>FACTS: Facial Animation Creation using the Transfer of Styles. (arXiv:2307.09480v1 [cs.GR])</title>
<link>http://arxiv.org/abs/2307.09480</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to accurately capture and express emotions is a critical aspect
of creating believable characters in video games and other forms of
entertainment. Traditionally, this animation has been achieved with artistic
effort or performance capture, both requiring costs in time and labor. More
recently, audio-driven models have seen success, however, these often lack
expressiveness in areas not correlated to the audio signal. In this paper, we
present a novel approach to facial animation by taking existing animations and
allowing for the modification of style characteristics. Specifically, we
explore the use of a StarGAN to enable the conversion of 3D facial animations
into different emotions and person-specific styles. We are able to maintain the
lip-sync of the animations with this method thanks to the use of a novel
viseme-preserving loss.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saunders_J/0/1/0/all/0/1&quot;&gt;Jack Saunders&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caulkin_S/0/1/0/all/0/1&quot;&gt;Steven Caulkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1&quot;&gt;Vinay Namboodiri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09481">
<title>AnyDoor: Zero-shot Object-level Image Customization. (arXiv:2307.09481v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09481</link>
<description rdf:parseType="Literal">&lt;p&gt;This work presents AnyDoor, a diffusion-based image generator with the power
to teleport target objects to new scenes at user-specified locations in a
harmonious way. Instead of tuning parameters for each object, our model is
trained only once and effortlessly generalizes to diverse object-scene
combinations at the inference stage. Such a challenging zero-shot setting
requires an adequate characterization of a certain object. To this end, we
complement the commonly used identity feature with detail features, which are
carefully designed to maintain texture details yet allow versatile local
variations (e.g., lighting, orientation, posture, etc.), supporting the object
in favorably blending with different surroundings. We further propose to borrow
knowledge from video datasets, where we can observe various forms (i.e., along
the time axis) of a single object, leading to stronger model generalizability
and robustness. Extensive experiments demonstrate the superiority of our
approach over existing alternatives as well as its great potential in
real-world applications, such as virtual try-on and object moving. Project page
is https://damo-vilab.github.io/AnyDoor-Page/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Lianghua Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yujun Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;Deli Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hengshuang Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2011.04408">
<title>SeasonDepth: Cross-Season Monocular Depth Prediction Dataset and Benchmark under Multiple Environments. (arXiv:2011.04408v7 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2011.04408</link>
<description rdf:parseType="Literal">&lt;p&gt;Different environments pose a great challenge to the outdoor robust visual
perception for long-term autonomous driving, and the generalization of
learning-based algorithms on different environments is still an open problem.
Although monocular depth prediction has been well studied recently, few works
focus on the robustness of learning-based depth prediction across different
environments, e.g. changing illumination and seasons, owing to the lack of such
a multi-environment real-world dataset and benchmark. To this end, the first
cross-season monocular depth prediction dataset and benchmark, SeasonDepth, is
introduced to benchmark the depth estimation performance under different
environments. We investigate several state-of-the-art representative
open-source supervised and self-supervised depth prediction methods using
newly-formulated metrics. Through extensive experimental evaluation on the
proposed dataset and cross-dataset evaluation with current autonomous driving
datasets, the performance and robustness against the influence of multiple
environments are analyzed qualitatively and quantitatively. We show that
long-term monocular depth prediction is still challenging and believe our work
can boost further research on the long-term robustness and generalization for
outdoor visual perception. The dataset is available on
https://seasondepth.github.io, and the benchmark toolkit is available on
https://github.com/ SeasonDepth/SeasonDepth.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Hanjiang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Baoquan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1&quot;&gt;Zhijian Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shiqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jiacheng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zuxin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1&quot;&gt;Wenhao Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;Ding Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hesheng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.02159">
<title>Robustness Analysis of Video-Language Models Against Visual and Language Perturbations. (arXiv:2207.02159v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.02159</link>
<description rdf:parseType="Literal">&lt;p&gt;Joint visual and language modeling on large-scale datasets has recently shown
good progress in multi-modal tasks when compared to single modal learning.
However, robustness of these approaches against real-world perturbations has
not been studied. In this work, we perform the first extensive robustness study
of video-language models against various real-world perturbations. We focus on
text-to-video retrieval and propose two large-scale benchmark datasets,
MSRVTT-P and YouCook2-P, which utilize 90 different visual and 35 different
text perturbations. The study reveals some interesting initial findings from
the studied models: 1) models are generally more susceptible when only video is
perturbed as opposed to when only text is perturbed, 2) models that are
pre-trained are more robust than those trained from scratch, 3) models attend
more to scene and objects rather than motion and action. We hope this study
will serve as a benchmark and guide future research in robust video-language
learning. The benchmark introduced in this study along with the code and
datasets is available at https://bit.ly/3CNOly4.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schiappa_M/0/1/0/all/0/1&quot;&gt;Madeline C. Schiappa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vyas_S/0/1/0/all/0/1&quot;&gt;Shruti Vyas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palangi_H/0/1/0/all/0/1&quot;&gt;Hamid Palangi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rawat_Y/0/1/0/all/0/1&quot;&gt;Yogesh S. Rawat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vineet_V/0/1/0/all/0/1&quot;&gt;Vibhav Vineet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.03824">
<title>Boosting Zero-shot Learning via Contrastive Optimization of Attribute Representations. (arXiv:2207.03824v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.03824</link>
<description rdf:parseType="Literal">&lt;p&gt;Zero-shot learning (ZSL) aims to recognize classes that do not have samples
in the training set. One representative solution is to directly learn an
embedding function associating visual features with corresponding class
semantics for recognizing new classes. Many methods extend upon this solution,
and recent ones are especially keen on extracting rich features from images,
e.g. attribute features. These attribute features are normally extracted within
each individual image; however, the common traits for features across images
yet belonging to the same attribute are not emphasized. In this paper, we
propose a new framework to boost ZSL by explicitly learning attribute
prototypes beyond images and contrastively optimizing them with attribute-level
features within images. Besides the novel architecture, two elements are
highlighted for attribute representations: a new prototype generation module is
designed to generate attribute prototypes from attribute semantics; a hard
example-based contrastive optimization scheme is introduced to reinforce
attribute-level features in the embedding space. We explore two alternative
backbones, CNN-based and transformer-based, to build our framework and conduct
experiments on three standard benchmarks, CUB, SUN, AwA2. Results on these
benchmarks demonstrate that our method improves the state of the art by a
considerable margin. Our codes will be available at
https://github.com/dyabel/CoAR-ZSL.git
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yu Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1&quot;&gt;Miaojing Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1&quot;&gt;Fangyun Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guoqi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.15304">
<title>Hiding Visual Information via Obfuscating Adversarial Perturbations. (arXiv:2209.15304v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.15304</link>
<description rdf:parseType="Literal">&lt;p&gt;Growing leakage and misuse of visual information raise security and privacy
concerns, which promotes the development of information protection. Existing
adversarial perturbations-based methods mainly focus on the de-identification
against deep learning models. However, the inherent visual information of the
data has not been well protected. In this work, inspired by the Type-I
adversarial attack, we propose an adversarial visual information hiding method
to protect the visual privacy of data. Specifically, the method generates
obfuscating adversarial perturbations to obscure the visual information of the
data. Meanwhile, it maintains the hidden objectives to be correctly predicted
by models. In addition, our method does not modify the parameters of the
applied model, which makes it flexible for different scenarios. Experimental
results on the recognition and classification tasks demonstrate that the
proposed method can effectively hide visual information and hardly affect the
performances of models. The code is available in the supplementary material.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1&quot;&gt;Zhigang Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1&quot;&gt;Dawei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Decheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Nannan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xinbo Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.13108">
<title>Heat Demand Forecasting with Multi-Resolutional Representation of Heterogeneous Temporal Ensemble. (arXiv:2210.13108v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.13108</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the primal challenges faced by utility companies is ensuring efficient
supply with minimal greenhouse gas emissions. The advent of smart meters and
smart grids provide an unprecedented advantage in realizing an optimised supply
of thermal energies through proactive techniques such as load forecasting. In
this paper, we propose a forecasting framework for heat demand based on neural
networks where the time series are encoded as scalograms equipped with the
capacity of embedding exogenous variables such as weather, and
holiday/non-holiday. Subsequently, CNNs are utilized to predict the heat load
multi-step ahead. Finally, the proposed framework is compared with other
state-of-the-art methods, such as SARIMAX and LSTM. The quantitative results
from retrospective experiments show that the proposed framework consistently
outperforms the state-of-the-art baseline method with real-world data acquired
from Denmark. A minimal mean error of 7.54% for MAPE and 417kW for RMSE is
achieved with the proposed framework in comparison to all other methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramachandran_A/0/1/0/all/0/1&quot;&gt;Adithya Ramachandran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatterjee_S/0/1/0/all/0/1&quot;&gt;Satyaki Chatterjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bayer_S/0/1/0/all/0/1&quot;&gt;Siming Bayer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1&quot;&gt;Andreas Maier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flensmark_T/0/1/0/all/0/1&quot;&gt;Thorkil Flensmark&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.03507">
<title>Ensuring Visual Commonsense Morality for Text-to-Image Generation. (arXiv:2212.03507v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.03507</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-image generation methods produce high-resolution and high-quality
images, but these methods should not produce immoral images that may contain
inappropriate content from the perspective of commonsense morality. In this
paper, we aim to automatically judge the immorality of synthesized images and
manipulate these images into morally acceptable alternatives. To this end, we
build a model that has three main primitives: (1) recognition of the visual
commonsense immorality in a given image, (2) localization or highlighting of
immoral visual (and textual) attributes that contribute to the immorality of
the image, and (3) manipulation of an immoral image to create a
morally-qualifying alternative. We conduct experiments and human studies using
the state-of-the-art Stable Diffusion text-to-image generation model,
demonstrating the effectiveness of our ethical image manipulation approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Seongbeom Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1&quot;&gt;Suhong Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jinkyu Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.14581">
<title>HopFIR: Hop-wise GraphFormer with Intragroup Joint Refinement for 3D Human Pose Estimation. (arXiv:2302.14581v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.14581</link>
<description rdf:parseType="Literal">&lt;p&gt;2D-to-3D human pose lifting is fundamental for 3D human pose estimation
(HPE). Graph Convolutional Network (GCN) has been proven inherently suitable to
model the human skeletal topology. However, current GCN-based 3D HPE methods
update the node features by aggregating their neighbors&apos; information without
considering the interaction of joints in different motion patterns. Although
some studies import limb information to learn the movement patterns, the latent
synergies among joints, such as maintaining balance in the motion are seldom
investigated. We propose a hop-wise GraphFormer with intragroup joint
refinement (HopFIR) to tackle the 3D HPE problem. The HopFIR mainly consists of
a novel Hop-wise GraphFormer(HGF) module and an Intragroup Joint
Refinement(IJR) module which leverages the prior limb information for
peripheral joints refinement. The HGF module groups the joints by $k$-hop
neighbors and utilizes a hop-wise transformer-like attention mechanism among
these groups to discover latent joint synergy. Extensive experimental results
show that HopFIR outperforms the SOTA methods with a large margin (on the
Human3.6M dataset, the mean per joint position error (MPJPE) is 32.67mm).
Furthermore, it is also demonstrated that previous SOTA GCN-based methods can
benefit from the proposed hop-wise attention mechanism efficiently with
significant performance promotion, such as SemGCN and MGCN are improved by 8.9%
and 4.5%, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_K/0/1/0/all/0/1&quot;&gt;Kai Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_Q/0/1/0/all/0/1&quot;&gt;Qiang Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_B/0/1/0/all/0/1&quot;&gt;Bo Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;ShanLin Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.02401">
<title>Open-Vocabulary Affordance Detection in 3D Point Clouds. (arXiv:2303.02401v4 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2303.02401</link>
<description rdf:parseType="Literal">&lt;p&gt;Affordance detection is a challenging problem with a wide variety of robotic
applications. Traditional affordance detection methods are limited to a
predefined set of affordance labels, hence potentially restricting the
adaptability of intelligent robots in complex and dynamic environments. In this
paper, we present the Open-Vocabulary Affordance Detection (OpenAD) method,
which is capable of detecting an unbounded number of affordances in 3D point
clouds. By simultaneously learning the affordance text and the point feature,
OpenAD successfully exploits the semantic relationships between affordances.
Therefore, our proposed method enables zero-shot detection and can be able to
detect previously unseen affordances without a single annotation example.
Intensive experimental results show that OpenAD works effectively on a wide
range of affordance detection setups and outperforms other baselines by a large
margin. Additionally, we demonstrate the practicality of the proposed OpenAD in
real-world robotic applications with a fast inference speed (~100ms). Our
project is available at https://openad2023.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Toan Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vu_M/0/1/0/all/0/1&quot;&gt;Minh Nhat Vu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vuong_A/0/1/0/all/0/1&quot;&gt;An Vuong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Dzung Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vo_T/0/1/0/all/0/1&quot;&gt;Thieu Vo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1&quot;&gt;Ngan Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1&quot;&gt;Anh Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.02885">
<title>Improving Transformer-based Image Matching by Cascaded Capturing Spatially Informative Keypoints. (arXiv:2303.02885v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.02885</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning robust local image feature matching is a fundamental low-level
vision task, which has been widely explored in the past few years. Recently,
detector-free local feature matchers based on transformers have shown promising
results, which largely outperform pure Convolutional Neural Network (CNN) based
ones. But correlations produced by transformer-based methods are spatially
limited to the center of source views&apos; coarse patches, because of the costly
attention learning. In this work, we rethink this issue and find that such
matching formulation degrades pose estimation, especially for low-resolution
images. So we propose a transformer-based cascade matching model -- Cascade
feature Matching TRansformer (CasMTR), to efficiently learn dense feature
correlations, which allows us to choose more reliable matching pairs for the
relative pose estimation. Instead of re-training a new detector, we use a
simple yet effective Non-Maximum Suppression (NMS) post-process to filter
keypoints through the confidence map, and largely improve the matching
precision. CasMTR achieves state-of-the-art performance in indoor and outdoor
pose estimation as well as visual localization. Moreover, thorough ablations
show the efficacy of the proposed components and techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1&quot;&gt;Chenjie Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yanwei Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.05118">
<title>SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model. (arXiv:2303.05118v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.05118</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of continual learning is to improve the performance of recognition
models in learning sequentially arrived data. Although most existing works are
established on the premise of learning from scratch, growing efforts have been
devoted to incorporating the benefits of pre-training. However, how to
adaptively exploit the pre-trained knowledge for each incremental task while
maintaining its generalizability remains an open question. In this work, we
present an extensive analysis for continual learning on a pre-trained model
(CLPM), and attribute the key challenge to a progressive overfitting problem.
Observing that selectively reducing the learning rate can almost resolve this
issue in the representation layer, we propose a simple but extremely effective
approach named Slow Learner with Classifier Alignment (SLCA), which further
improves the classification layer by modeling the class-wise distributions and
aligning the classification layers in a post-hoc fashion. Across a variety of
scenarios, our proposal provides substantial improvements for CLPM (e.g., up to
49.76%, 50.05%, 44.69% and 40.16% on Split CIFAR-100, Split ImageNet-R, Split
CUB-200 and Split Cars-196, respectively), and thus outperforms
state-of-the-art approaches by a large margin. Based on such a strong baseline,
critical factors and promising directions are analyzed in-depth to facilitate
subsequent research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Gengwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_G/0/1/0/all/0/1&quot;&gt;Guoliang Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Ling Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yunchao Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.15932">
<title>Unify, Align and Refine: Multi-Level Semantic Alignment for Radiology Report Generation. (arXiv:2303.15932v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.15932</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic radiology report generation has attracted enormous research
interest due to its practical value in reducing the workload of radiologists.
However, simultaneously establishing global correspondences between the image
(e.g., Chest X-ray) and its related report and local alignments between image
patches and keywords remains challenging. To this end, we propose an Unify,
Align and then Refine (UAR) approach to learn multi-level cross-modal
alignments and introduce three novel modules: Latent Space Unifier (LSU),
Cross-modal Representation Aligner (CRA) and Text-to-Image Refiner (TIR).
Specifically, LSU unifies multimodal data into discrete tokens, making it
flexible to learn common knowledge among modalities with a shared network. The
modality-agnostic CRA learns discriminative features via a set of orthonormal
basis and a dual-gate mechanism first and then globally aligns visual and
textual representations under a triplet contrastive loss. TIR boosts
token-level local alignment via calibrating text-to-image attention with a
learnable mask. Additionally, we design a two-stage training procedure to make
UAR gradually grasp cross-modal alignments at different levels, which imitates
radiologists&apos; workflow: writing sentence by sentence first and then checking
word by word. Extensive experiments and analyses on IU-Xray and MIMIC-CXR
benchmark datasets demonstrate the superiority of our UAR against varied
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yaowei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xuxin Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhihong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongxiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1&quot;&gt;Yuexian Zou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.01580">
<title>Untargeted Near-collision Attacks in Biometric Recognition. (arXiv:2304.01580v3 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2304.01580</link>
<description rdf:parseType="Literal">&lt;p&gt;A biometric recognition system can operate in two distinct modes,
identification or verification. In the first mode, the system recognizes an
individual by searching the enrolled templates of all the users for a match. In
the second mode, the system validates a user&apos;s identity claim by comparing the
fresh provided template with the enrolled template. The biometric
transformation schemes usually produce binary templates that are better handled
by cryptographic schemes, and the comparison is based on a distance that leaks
information about the similarities between two biometric templates. Both the
experimentally determined false match rate and false non-match rate through
recognition threshold adjustment define the recognition accuracy, and hence the
security of the system. To the best of our knowledge, few works provide a
formal treatment of the security under minimum leakage of information, i.e.,
the binary outcome of a comparison with a threshold. In this paper, we rely on
probabilistic modelling to quantify the security strength of binary templates.
We investigate the influence of template size, database size and threshold on
the probability of having a near-collision. We highlight several untargeted
attacks on biometric systems considering naive and adaptive adversaries.
Interestingly, these attacks can be launched both online and offline and, both
in the identification mode and in the verification mode. We discuss the choice
of parameters through the generic presented attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durbet_A/0/1/0/all/0/1&quot;&gt;Axel Durbet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grollemund_P/0/1/0/all/0/1&quot;&gt;Paul-Marie Grollemund&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thiry_Atighehchi_K/0/1/0/all/0/1&quot;&gt;Kevin Thiry-Atighehchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.02689">
<title>ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast. (arXiv:2304.02689v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.02689</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical data often exhibits long-tail distributions with heavy class
imbalance, which naturally leads to difficulty in classifying the minority
classes (i.e., boundary regions or rare objects). Recent work has significantly
improved semi-supervised medical image segmentation in long-tailed scenarios by
equipping them with unsupervised contrastive criteria. However, it remains
unclear how well they will perform in the labeled portion of data where class
distribution is also highly imbalanced. In this work, we present ACTION++, an
improved contrastive learning framework with adaptive anatomical contrast for
semi-supervised medical segmentation. Specifically, we propose an adaptive
supervised contrastive loss, where we first compute the optimal locations of
class centers uniformly distributed on the embedding space (i.e., off-line),
and then perform online contrastive matching training by encouraging different
class features to adaptively match these distinct and uniformly distributed
class centers. Moreover, we argue that blindly adopting a constant temperature
$\tau$ in the contrastive loss on long-tailed medical data is not optimal, and
propose to use a dynamic $\tau$ via a simple cosine schedule to yield better
separation between majority and minority classes. Empirically, we evaluate
ACTION++ on ACDC and LA benchmarks and show that it achieves state-of-the-art
across two semi-supervised settings. Theoretically, we analyze the performance
of adaptive anatomical contrast and confirm its superiority in label
efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1&quot;&gt;Chenyu You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1&quot;&gt;Weicheng Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_Y/0/1/0/all/0/1&quot;&gt;Yifei Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Staib_L/0/1/0/all/0/1&quot;&gt;Lawrence Staib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sekhon_J/0/1/0/all/0/1&quot;&gt;Jasjeet S. Sekhon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duncan_J/0/1/0/all/0/1&quot;&gt;James S. Duncan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.03209">
<title>Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts. (arXiv:2304.03209v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.03209</link>
<description rdf:parseType="Literal">&lt;p&gt;Integrating high-level semantically correlated contents and low-level
anatomical features is of central importance in medical image segmentation.
Towards this end, recent deep learning-based medical segmentation methods have
shown great promise in better modeling such information. However, convolution
operators for medical segmentation typically operate on regular grids, which
inherently blur the high-frequency regions, i.e., boundary regions. In this
work, we propose MORSE, a generic implicit neural rendering framework designed
at an anatomical level to assist learning in medical image segmentation. Our
method is motivated by the fact that implicit neural representation has been
shown to be more effective in fitting complex signals and solving computer
graphics problems than discrete grid-based representation. The core of our
approach is to formulate medical image segmentation as a rendering problem in
an end-to-end manner. Specifically, we continuously align the coarse
segmentation prediction with the ambiguous coordinate-based point
representations and aggregate these features to adaptively refine the boundary
region. To parallelly optimize multi-scale pixel-level features, we leverage
the idea from Mixture-of-Expert (MoE) to design and train our MORSE with a
stochastic gating mechanism. Our experiments demonstrate that MORSE can work
well with different medical segmentation backbones, consistently achieving
competitive performance improvements in both 2D and 3D supervised medical
segmentation methods. We also theoretically analyze the superiority of MORSE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1&quot;&gt;Chenyu You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1&quot;&gt;Weicheng Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_Y/0/1/0/all/0/1&quot;&gt;Yifei Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Staib_L/0/1/0/all/0/1&quot;&gt;Lawrence Staib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duncan_J/0/1/0/all/0/1&quot;&gt;James S. Duncan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.08101">
<title>LLA-FLOW: A Lightweight Local Aggregation on Cost Volume for Optical Flow Estimation. (arXiv:2304.08101v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.08101</link>
<description rdf:parseType="Literal">&lt;p&gt;Lack of texture often causes ambiguity in matching, and handling this issue
is an important challenge in optical flow estimation. Some methods insert
stacked transformer modules that allow the network to use global information of
cost volume for estimation. But the global information aggregation often incurs
serious memory and time costs during training and inference, which hinders
model deployment. We draw inspiration from the traditional local region
constraint and design the local similarity aggregation (LSA) and the shifted
local similarity aggregation (SLSA). The aggregation for cost volume is
implemented with lightweight modules that act on the feature maps. Experiments
on the final pass of Sintel show the lower cost required for our approach while
maintaining competitive performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiawei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zongqing Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1&quot;&gt;Qingmin Liao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.10769">
<title>Deep Multiview Clustering by Contrasting Cluster Assignments. (arXiv:2304.10769v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.10769</link>
<description rdf:parseType="Literal">&lt;p&gt;Multiview clustering (MVC) aims to reveal the underlying structure of
multiview data by categorizing data samples into clusters. Deep learning-based
methods exhibit strong feature learning capabilities on large-scale datasets.
For most existing deep MVC methods, exploring the invariant representations of
multiple views is still an intractable problem. In this paper, we propose a
cross-view contrastive learning (CVCL) method that learns view-invariant
representations and produces clustering results by contrasting the cluster
assignments among multiple views. Specifically, we first employ deep
autoencoders to extract view-dependent features in the pretraining stage. Then,
a cluster-level CVCL strategy is presented to explore consistent semantic label
information among the multiple views in the fine-tuning stage. Thus, the
proposed CVCL method is able to produce more discriminative cluster assignments
by virtue of this learning strategy. Moreover, we provide a theoretical
analysis of soft cluster assignment alignment. Extensive experimental results
obtained on several datasets demonstrate that the proposed CVCL method
outperforms several state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1&quot;&gt;Hua Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woo_W/0/1/0/all/0/1&quot;&gt;Wai Lok Woo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1&quot;&gt;Xi Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.11862">
<title>Universal Domain Adaptation via Compressive Attention Matching. (arXiv:2304.11862v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.11862</link>
<description rdf:parseType="Literal">&lt;p&gt;Universal domain adaptation (UniDA) aims to transfer knowledge from the
source domain to the target domain without any prior knowledge about the label
set. The challenge lies in how to determine whether the target samples belong
to common categories. The mainstream methods make judgments based on the sample
features, which overemphasizes global information while ignoring the most
crucial local objects in the image, resulting in limited accuracy. To address
this issue, we propose a Universal Attention Matching (UniAM) framework by
exploiting the self-attention mechanism in vision transformer to capture the
crucial object information. The proposed framework introduces a novel
Compressive Attention Matching (CAM) approach to explore the core information
by compressively representing attentions. Furthermore, CAM incorporates a
residual-based measurement to determine the sample commonness. By utilizing the
measurement, UniAM achieves domain-wise and category-wise Common Feature
Alignment (CFA) and Target Class Separation (TCS). Notably, UniAM is the first
method utilizing the attention in vision transformer directly to perform
classification tasks. Extensive experiments show that UniAM outperforms the
current state-of-the-art methods on various benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1&quot;&gt;Didi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yincuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1&quot;&gt;Junkun Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zexi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1&quot;&gt;Kun Kuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chao Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.12306">
<title>Segment Anything in Medical Images. (arXiv:2304.12306v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.12306</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical image segmentation is a critical component in clinical practice,
facilitating accurate diagnosis, treatment planning, and disease monitoring.
However, current methods predominantly rely on customized models, which exhibit
limited generality across diverse tasks. In this study, we present MedSAM, the
inaugural foundation model designed for universal medical image segmentation.
Harnessing the power of a meticulously curated dataset comprising over one
million images, MedSAM not only outperforms existing state-of-the-art
segmentation foundation models, but also exhibits comparable or even superior
performance to specialist models. Moreover, MedSAM enables the precise
extraction of essential biomarkers for tumor burden quantification. By
delivering accurate and efficient segmentation across a wide spectrum of tasks,
MedSAM holds significant potential to expedite the evolution of diagnostic
tools and the personalization of treatment plans.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jun Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yuting He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Feifei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Han_L/0/1/0/all/0/1&quot;&gt;Lin Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+You_C/0/1/0/all/0/1&quot;&gt;Chenyu You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bo Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.04422">
<title>Performance Gaps of Artificial Intelligence Models Screening Mammography -- Towards Fair and Interpretable Models. (arXiv:2305.04422v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.04422</link>
<description rdf:parseType="Literal">&lt;p&gt;Even though deep learning models for abnormality classification can perform
well in screening mammography, the demographic and imaging characteristics
associated with increased risk of failure for abnormality classification in
screening mammograms remain unclear. This retrospective study used data from
the Emory BrEast Imaging Dataset (EMBED) including mammograms from 115,931
patients imaged at Emory University Healthcare between 2013 to 2020. Clinical
and imaging data includes Breast Imaging Reporting and Data System (BI-RADS)
assessment, region of interest coordinates for abnormalities, imaging features,
pathologic outcomes, and patient demographics. Deep learning models including
InceptionV3, VGG16, ResNet50V2, and ResNet152V2 were developed to distinguish
between patches of abnormal tissue and randomly selected patches of normal
tissue from the screening mammograms. The distributions of the training,
validation and test sets are 29,144 (55.6%) patches of 10,678 (54.2%) patients,
9,910 (18.9%) patches of 3,609 (18.3%) patients, and 13,390 (25.5%) patches of
5,404 (27.5%) patients. We assessed model performance overall and within
subgroups defined by age, race, pathologic outcome, and imaging characteristics
to evaluate reasons for misclassifications. On the test set, a ResNet152V2
model trained to classify normal versus abnormal tissue patches achieved an
accuracy of 92.6% (95%CI=92.0-93.2%), and area under the receiver operative
characteristics curve 0.975 (95%CI=0.972-0.978). Imaging characteristics
associated with higher misclassifications of images include higher tissue
densities (risk ratio [RR]=1.649; p=.010, BI-RADS density C and RR=2.026;
p=.003, BI-RADS density D), and presence of architectural distortion (RR=1.026;
p&amp;lt;.001). Small but statistically significant differences in performance were
observed by age, race, pathologic outcome, and other imaging features (p&amp;lt;.001).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Linglin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Brown_Mulry_B/0/1/0/all/0/1&quot;&gt;Beatrice Brown-Mulry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nalla_V/0/1/0/all/0/1&quot;&gt;Vineela Nalla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hwang_I/0/1/0/all/0/1&quot;&gt;InChan Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gichoya_J/0/1/0/all/0/1&quot;&gt;Judy Wawira Gichoya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gastounioti_A/0/1/0/all/0/1&quot;&gt;Aimilia Gastounioti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Banerjee_I/0/1/0/all/0/1&quot;&gt;Imon Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Seyyed_Kalantari_L/0/1/0/all/0/1&quot;&gt;Laleh Seyyed-Kalantari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Woo_M/0/1/0/all/0/1&quot;&gt;MinJae Woo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Trivedi_H/0/1/0/all/0/1&quot;&gt;Hari Trivedi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.07848">
<title>Meta-Polyp: a baseline for efficient Polyp segmentation. (arXiv:2305.07848v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.07848</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, polyp segmentation has gained significant importance, and
many methods have been developed using CNN, Vision Transformer, and Transformer
techniques to achieve competitive results. However, these methods often face
difficulties when dealing with out-of-distribution datasets, missing
boundaries, and small polyps. In 2022, Meta-Former was introduced as a new
baseline for vision, which not only improved the performance of multi-task
computer vision but also addressed the limitations of the Vision Transformer
and CNN family backbones. To further enhance segmentation, we propose a fusion
of Meta-Former with UNet, along with the introduction of a Multi-scale
Upsampling block with a level-up combination in the decoder stage to enhance
the texture, also we propose the Convformer block base on the idea of the
Meta-former to enhance the crucial information of the local feature. These
blocks enable the combination of global information, such as the overall shape
of the polyp, with local information and boundary information, which is crucial
for the decision of the medical segmentation. Our proposed approach achieved
competitive performance and obtained the top result in the State of the Art on
the CVC-300 dataset, Kvasir, and CVC-ColonDB dataset. Apart from Kvasir-SEG,
others are out-of-distribution datasets. The implementation can be found at:
https://github.com/huyquoctrinh/MetaPolyp-CBMS2023.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Trinh_Q/0/1/0/all/0/1&quot;&gt;Quoc-Huy Trinh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.08546">
<title>Towards Visual Saliency Explanations of Face Verification. (arXiv:2305.08546v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.08546</link>
<description rdf:parseType="Literal">&lt;p&gt;In the past years, deep convolutional neural networks have been pushing the
frontier of face recognition (FR) techniques in both verification and
identification scenarios. Despite the high accuracy, they are often criticized
for lacking explainability. There has been an increasing demand for
understanding the decision-making process of deep face recognition systems.
Recent studies have investigated the usage of visual saliency maps as an
explanation, but they often lack a discussion and analysis in the context of
face recognition. This paper concentrates on explainable face verification
tasks and conceives a new explanation framework. First, a definition of the
saliency-based explanation method is provided, which focuses on the decisions
made by the deep FR model. Then, a new model-agnostic explanation method named
CorrRISE is proposed to produce saliency maps, which reveal both the similar
and dissimilar regions of any given pair of face images. Besides, two
evaluation metrics are designed to measure the performance of general visual
saliency explanation methods in face verification. Consequently, substantial
visual and quantitative results have shown that the proposed CorrRISE method
demonstrates promising results in comparison with other state-of-the-art
explainable face verification approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yuhang Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zewei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ebrahimi_T/0/1/0/all/0/1&quot;&gt;Touradj Ebrahimi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.09211">
<title>CB-HVTNet: A channel-boosted hybrid vision transformer network for lymphocyte assessment in histopathological images. (arXiv:2305.09211v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.09211</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformers, due to their ability to learn long range dependencies, have
overcome the shortcomings of convolutional neural networks (CNNs) for global
perspective learning. Therefore, they have gained the focus of researchers for
several vision related tasks including medical diagnosis. However, their
multi-head attention module only captures global level feature representations,
which is insufficient for medical images. To address this issue, we propose a
Channel Boosted Hybrid Vision Transformer (CB HVT) that uses transfer learning
to generate boosted channels and employs both transformers and CNNs to analyse
lymphocytes in histopathological images. The proposed CB HVT comprises five
modules, including a channel generation module, channel exploitation module,
channel merging module, region-aware module, and a detection and segmentation
head, which work together to effectively identify lymphocytes. The channel
generation module uses the idea of channel boosting through transfer learning
to extract diverse channels from different auxiliary learners. In the CB HVT,
these boosted channels are first concatenated and ranked using an attention
mechanism in the channel exploitation module. A fusion block is then utilized
in the channel merging module for a gradual and systematic merging of the
diverse boosted channels to improve the network&apos;s learning representations. The
CB HVT also employs a proposal network in its region aware module and a head to
effectively identify objects, even in overlapping regions and with artifacts.
We evaluated the proposed CB HVT on two publicly available datasets for
lymphocyte assessment in histopathological images. The results show that CB HVT
outperformed other state of the art detection models, and has good
generalization ability, demonstrating its value as a tool for pathologists.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ali_M/0/1/0/all/0/1&quot;&gt;Momina Liaqat Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rauf_Z/0/1/0/all/0/1&quot;&gt;Zunaira Rauf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Asifullah Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sohail_A/0/1/0/all/0/1&quot;&gt;Anabia Sohail&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ullah_R/0/1/0/all/0/1&quot;&gt;Rafi Ullah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gwak_J/0/1/0/all/0/1&quot;&gt;Jeonghwan Gwak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13399">
<title>Efficient Large-Scale Visual Representation Learning And Evaluation. (arXiv:2305.13399v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13399</link>
<description rdf:parseType="Literal">&lt;p&gt;In this article, we present our approach to single-modality visual
representation learning. Understanding visual representations of items is vital
for fashion recommendations in e-commerce. We detail and contrast techniques
used to finetune large-scale visual representation learning models in an
efficient manner under low-resource settings, including several pretrained
backbone architectures, both in the convolutional neural network as well as the
vision transformer family. We describe the challenges for e-commerce
applications at-scale and highlight the efforts to more efficiently train,
evaluate, and serve visual representations. We present ablation studies
evaluating the representation offline performance for several downstream tasks,
including visually similar ad recommendations on mobile devices. To this end,
we present a novel multilingual text-to-image generative offline evaluation
method for visually similar recommendation systems. Finally, we include online
results from deployed machine learning systems in production at Etsy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dolev_E/0/1/0/all/0/1&quot;&gt;Eden Dolev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awad_A/0/1/0/all/0/1&quot;&gt;Alaa Awad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roberts_D/0/1/0/all/0/1&quot;&gt;Denisa Roberts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ebrahimzadeh_Z/0/1/0/all/0/1&quot;&gt;Zahra Ebrahimzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mejran_M/0/1/0/all/0/1&quot;&gt;Marcin Mejran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malpani_V/0/1/0/all/0/1&quot;&gt;Vaibhav Malpani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yavuz_M/0/1/0/all/0/1&quot;&gt;Mahir Yavuz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16656">
<title>Clustering Method for Time-Series Images Using Quantum-Inspired Computing Technology. (arXiv:2305.16656v3 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16656</link>
<description rdf:parseType="Literal">&lt;p&gt;Time-series clustering serves as a powerful data mining technique for
time-series data in the absence of prior knowledge about clusters. A large
amount of time-series data with large size has been acquired and used in
various research fields. Hence, clustering method with low computational cost
is required. Given that a quantum-inspired computing technology, such as a
simulated annealing machine, surpasses conventional computers in terms of fast
and accurately solving combinatorial optimization problems, it holds promise
for accomplishing clustering tasks that are challenging to achieve using
existing methods. This study proposes a novel time-series clustering method
that leverages an annealing machine. The proposed method facilitates an even
classification of time-series data into clusters close to each other while
maintaining robustness against outliers. Moreover, its applicability extends to
time-series images. We compared the proposed method with a standard existing
method for clustering an online distributed dataset. In the existing method,
the distances between each data are calculated based on the Euclidean distance
metric, and the clustering is performed using the k-means++ method. We found
that both methods yielded comparable results. Furthermore, the proposed method
was applied to a flow measurement image dataset containing noticeable noise
with a signal-to-noise ratio of approximately 1. Despite a small signal
variation of approximately 2%, the proposed method effectively classified the
data without any overlap among the clusters. In contrast, the clustering
results by the standard existing method and the conditional image sampling
(CIS) method, a specialized technique for flow measurement data, displayed
overlapping clusters. Consequently, the proposed method provides better results
than the other two methods, demonstrating its potential as a superior
clustering method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Inoue_T/0/1/0/all/0/1&quot;&gt;Tomoki Inoue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kubota_K/0/1/0/all/0/1&quot;&gt;Koyo Kubota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ikami_T/0/1/0/all/0/1&quot;&gt;Tsubasa Ikami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Egami_Y/0/1/0/all/0/1&quot;&gt;Yasuhiro Egami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nagai_H/0/1/0/all/0/1&quot;&gt;Hiroki Nagai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kashikawa_T/0/1/0/all/0/1&quot;&gt;Takahiro Kashikawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kimura_K/0/1/0/all/0/1&quot;&gt;Koichi Kimura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Matsuda_Y/0/1/0/all/0/1&quot;&gt;Yu Matsuda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05439">
<title>Contrastive Representation Disentanglement for Clustering. (arXiv:2306.05439v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05439</link>
<description rdf:parseType="Literal">&lt;p&gt;Clustering continues to be a significant and challenging task. Recent studies
have demonstrated impressive results by applying clustering to feature
representations acquired through self-supervised learning, particularly on
small datasets. However, when dealing with datasets containing a large number
of clusters, such as ImageNet, current methods struggle to achieve satisfactory
clustering performance. In this paper, we introduce a novel method called
Contrastive representation Disentanglement for Clustering (CDC) that leverages
contrastive learning to directly disentangle the feature representation for
clustering. In CDC, we decompose the representation into two distinct
components: one component encodes categorical information under an
equipartition constraint, and the other component captures instance-specific
factors. To train our model, we propose a contrastive loss that effectively
utilizes both components of the representation. We conduct a theoretical
analysis of the proposed loss and highlight how it assigns different weights to
negative samples during the process of disentangling the feature
representation. Further analysis of the gradients reveals that larger weights
emphasize a stronger focus on hard negative samples. As a result, the proposed
loss exhibits strong expressiveness, enabling efficient disentanglement of
categorical information. Through experimental evaluation on various benchmark
datasets, our method demonstrates either state-of-the-art or highly competitive
clustering performance. Notably, on the complete ImageNet dataset, we achieve
an accuracy of 53.4%, surpassing existing methods by a substantial margin of
+10.2%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1&quot;&gt;Fei Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krovi_V/0/1/0/all/0/1&quot;&gt;Venkat Krovi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1&quot;&gt;Feng Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06849">
<title>Mitigating Transformer Overconfidence via Lipschitz Regularization. (arXiv:2306.06849v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06849</link>
<description rdf:parseType="Literal">&lt;p&gt;Though Transformers have achieved promising results in many computer vision
tasks, they tend to be over-confident in predictions, as the standard Dot
Product Self-Attention (DPSA) can barely preserve distance for the unbounded
input domain. In this work, we fill this gap by proposing a novel Lipschitz
Regularized Transformer (LRFormer). Specifically, we present a new similarity
function with the distance within Banach Space to ensure the Lipschitzness and
also regularize the term by a contractive Lipschitz Bound. The proposed method
is analyzed with a theoretical guarantee, providing a rigorous basis for its
effectiveness and reliability. Extensive experiments conducted on standard
vision benchmarks demonstrate that our method outperforms the state-of-the-art
single forward pass approaches in prediction, calibration, and uncertainty
estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1&quot;&gt;Wenqian Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yunsheng Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xu Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1&quot;&gt;Kun Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07615">
<title>UOD: Universal One-shot Detection of Anatomical Landmarks. (arXiv:2306.07615v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07615</link>
<description rdf:parseType="Literal">&lt;p&gt;One-shot medical landmark detection gains much attention and achieves great
success for its label-efficient training process. However, existing one-shot
learning methods are highly specialized in a single domain and suffer domain
preference heavily in the situation of multi-domain unlabeled data. Moreover,
one-shot learning is not robust that it faces performance drop when annotating
a sub-optimal image. To tackle these issues, we resort to developing a
domain-adaptive one-shot landmark detection framework for handling multi-domain
medical images, named Universal One-shot Detection (UOD). UOD consists of two
stages and two corresponding universal models which are designed as
combinations of domain-specific modules and domain-shared modules. In the first
stage, a domain-adaptive convolution model is self-supervised learned to
generate pseudo landmark labels. In the second stage, we design a
domain-adaptive transformer to eliminate domain preference and build the global
context for multi-domain data. Even though only one annotated sample from each
domain is available for training, the domain-shared modules help UOD aggregate
all one-shot samples to detect more robust and accurate landmarks. We
investigated both qualitatively and quantitatively the proposed UOD on three
widely-used public X-ray datasets in different anatomical domains (i.e., head,
hand, chest) and obtained state-of-the-art performances in each domain. The
code is available at
https://github.com/heqin-zhu/UOD_universal_oneshot_detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Heqin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quan_Q/0/1/0/all/0/1&quot;&gt;Quan Quan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1&quot;&gt;Qingsong Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zaiyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;S. Kevin Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.13074">
<title>Iterative Scale-Up ExpansionIoU and Deep Features Association for Multi-Object Tracking in Sports. (arXiv:2306.13074v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.13074</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-object tracking algorithms have made significant advancements due to
the recent developments in object detection. However, most existing methods
primarily focus on tracking pedestrians or vehicles, which exhibit relatively
simple and regular motion patterns. Consequently, there is a scarcity of
algorithms that address the tracking of targets with irregular or non-linear
motion, such as multi-athlete tracking. Furthermore, popular tracking
algorithms often rely on the Kalman filter for object motion modeling, which
fails to track objects when their motion contradicts the linear motion
assumption of the Kalman filter. Due to this reason, we proposed a novel online
and robust multi-object tracking approach, named Iterative Scale-Up
ExpansionIoU and Deep Features for multi-object tracking. Unlike conventional
methods, we abandon the use of the Kalman filter and propose utilizing the
iterative scale-up expansion IoU. This approach achieves superior tracking
performance without requiring additional training data or adopting a more
robust detector, all while maintaining a lower computational cost compared to
other appearance-based methods. Our proposed method demonstrates remarkable
effectiveness in tracking irregular motion objects, achieving a score of 76.9%
in HOTA. It outperforms all state-of-the-art tracking algorithms on the
SportsMOT dataset, covering various kinds of sport scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Hsiang-Wei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Cheng-Yen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiacheng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1&quot;&gt;Jenq-Neng Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chung-I Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15548">
<title>Geometric Ultrasound Localization Microscopy. (arXiv:2306.15548v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15548</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrast-Enhanced Ultra-Sound (CEUS) has become a viable method for
non-invasive, dynamic visualization in medical diagnostics, yet Ultrasound
Localization Microscopy (ULM) has enabled a revolutionary breakthrough by
offering ten times higher resolution. To date, Delay-And-Sum (DAS) beamformers
are used to render ULM frames, ultimately determining the image resolution
capability. To take full advantage of ULM, this study questions whether
beamforming is the most effective processing step for ULM, suggesting an
alternative approach that relies solely on Time-Difference-of-Arrival (TDoA)
information. To this end, a novel geometric framework for micro bubble
localization via ellipse intersections is proposed to overcome existing
beamforming limitations. We present a benchmark comparison based on a public
dataset for which our geometric ULM outperforms existing baseline methods in
terms of accuracy and robustness while only utilizing a portion of the
available transducer data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hahne_C/0/1/0/all/0/1&quot;&gt;Christopher Hahne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sznitman_R/0/1/0/all/0/1&quot;&gt;Raphael Sznitman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01740">
<title>Synchronous Image-Label Diffusion Probability Model with Application to Stroke Lesion Segmentation on Non-contrast CT. (arXiv:2307.01740v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01740</link>
<description rdf:parseType="Literal">&lt;p&gt;Stroke lesion volume is a key radiologic measurement for assessing the
prognosis of Acute Ischemic Stroke (AIS) patients, which is challenging to be
automatically measured on Non-Contrast CT (NCCT) scans. Recent diffusion
probabilistic models have shown potentials of being used for image
segmentation. In this paper, a novel Synchronous image-label Diffusion
Probability Model (SDPM) is proposed for stroke lesion segmentation on NCCT
using Markov diffusion process. The proposed SDPM is fully based on a Latent
Variable Model (LVM), offering a complete probabilistic elaboration. An
additional net-stream, parallel with a noise prediction stream, is introduced
to obtain initial noisy label estimates for efficiently inferring the final
labels. By optimizing the specified variational boundaries, the trained model
can infer multiple label estimates for reference given the input images with
noises. The proposed model was assessed on three stroke lesion datasets
including one public and two private datasets. Compared to several U-net and
transformer-based segmentation methods, our proposed SDPM model is able to
achieve state-of-the-art performance. The code is publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianhai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_T/0/1/0/all/0/1&quot;&gt;Tonghua Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+MacDonald_E/0/1/0/all/0/1&quot;&gt;Ethan MacDonald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menon_B/0/1/0/all/0/1&quot;&gt;Bijoy Menon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganesh_A/0/1/0/all/0/1&quot;&gt;Aravind Ganesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qiu Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02347">
<title>Detecting Images Generated by Deep Diffusion Models using their Local Intrinsic Dimensionality. (arXiv:2307.02347v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02347</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models recently have been successfully applied for the visual
synthesis of strikingly realistic appearing images. This raises strong concerns
about their potential for malicious purposes. In this paper, we propose using
the lightweight multi Local Intrinsic Dimensionality (multiLID), which has been
originally developed in context of the detection of adversarial examples, for
the automatic detection of synthetic images and the identification of the
according generator networks. In contrast to many existing detection
approaches, which often only work for GAN-generated images, the proposed method
provides close to perfect detection results in many realistic use cases.
Extensive experiments on known and newly created datasets demonstrate that the
proposed multiLID approach exhibits superiority in diffusion detection and
model identification. Since the empirical evaluations of recent publications on
the detection of generated images are often mainly focused on the
&quot;LSUN-Bedroom&quot; dataset, we further establish a comprehensive benchmark for the
detection of diffusion-generated images, including samples from several
diffusion models with different image sizes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lorenz_P/0/1/0/all/0/1&quot;&gt;Peter Lorenz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durall_R/0/1/0/all/0/1&quot;&gt;Ricard Durall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1&quot;&gt;Janis Keuper&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07250">
<title>Mitigating Adversarial Vulnerability through Causal Parameter Estimation by Adversarial Double Machine Learning. (arXiv:2307.07250v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07250</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial examples derived from deliberately crafted perturbations on
visual inputs can easily harm decision process of deep neural networks. To
prevent potential threats, various adversarial training-based defense methods
have grown rapidly and become a de facto standard approach for robustness.
Despite recent competitive achievements, we observe that adversarial
vulnerability varies across targets and certain vulnerabilities remain
prevalent. Intriguingly, such peculiar phenomenon cannot be relieved even with
deeper architectures and advanced defense methods. To address this issue, in
this paper, we introduce a causal approach called Adversarial Double Machine
Learning (ADML), which allows us to quantify the degree of adversarial
vulnerability for network predictions and capture the effect of treatments on
outcome of interests. ADML can directly estimate causal parameter of
adversarial perturbations per se and mitigate negative effects that can
potentially damage robustness, bridging a causal perspective into the
adversarial vulnerability. Through extensive experiments on various CNN and
Transformer architectures, we corroborate that ADML improves adversarial
robustness with large margins and relieve the empirical observation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1&quot;&gt;Byung-Kwan Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Junho Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ro_Y/0/1/0/all/0/1&quot;&gt;Yong Man Ro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07483">
<title>Multimodal Distillation for Egocentric Action Recognition. (arXiv:2307.07483v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07483</link>
<description rdf:parseType="Literal">&lt;p&gt;The focal point of egocentric video understanding is modelling hand-object
interactions. Standard models, e.g. CNNs or Vision Transformers, which receive
RGB frames as input perform well. However, their performance improves further
by employing additional input modalities that provide complementary cues, such
as object detections, optical flow, audio, etc. The added complexity of the
modality-specific modules, on the other hand, makes these models impractical
for deployment. The goal of this work is to retain the performance of such a
multimodal approach, while using only the RGB frames as input at inference
time. We demonstrate that for egocentric action recognition on the
Epic-Kitchens and the Something-Something datasets, students which are taught
by multimodal teachers tend to be more accurate and better calibrated than
architecturally equivalent models trained on ground truth labels in a unimodal
or multimodal fashion. We further adopt a principled multimodal knowledge
distillation framework, allowing us to deal with issues which occur when
applying multimodal knowledge distillation in a naive manner. Lastly, we
demonstrate the achieved reduction in computational complexity, and show that
our approach maintains higher performance with the reduction of the number of
input views. We release our code at
https://github.com/gorjanradevski/multimodal-distillation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radevski_G/0/1/0/all/0/1&quot;&gt;Gorjan Radevski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grujicic_D/0/1/0/all/0/1&quot;&gt;Dusan Grujicic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1&quot;&gt;Marie-Francine Moens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blaschko_M/0/1/0/all/0/1&quot;&gt;Matthew Blaschko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1&quot;&gt;Tinne Tuytelaars&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07754">
<title>Bidirectionally Deformable Motion Modulation For Video-based Human Pose Transfer. (arXiv:2307.07754v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07754</link>
<description rdf:parseType="Literal">&lt;p&gt;Video-based human pose transfer is a video-to-video generation task that
animates a plain source human image based on a series of target human poses.
Considering the difficulties in transferring highly structural patterns on the
garments and discontinuous poses, existing methods often generate
unsatisfactory results such as distorted textures and flickering artifacts. To
address these issues, we propose a novel Deformable Motion Modulation (DMM)
that utilizes geometric kernel offset with adaptive weight modulation to
simultaneously perform feature alignment and style transfer. Different from
normal style modulation used in style transfer, the proposed modulation
mechanism adaptively reconstructs smoothed frames from style codes according to
the object shape through an irregular receptive field of view. To enhance the
spatio-temporal consistency, we leverage bidirectional propagation to extract
the hidden motion information from a warped image sequence generated by noisy
poses. The proposed feature propagation significantly enhances the motion
prediction ability by forward and backward propagation. Both quantitative and
qualitative experimental results demonstrate superiority over the
state-of-the-arts in terms of image fidelity and visual continuity. The source
code is publicly available at github.com/rocketappslab/bdmm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1&quot;&gt;Wing-Yin Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Po_L/0/1/0/all/0/1&quot;&gt;Lai-Man Po&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheung_R/0/1/0/all/0/1&quot;&gt;Ray C.C. Cheung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yuzhi Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1&quot;&gt;Yu Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kun Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07813">
<title>Ultra-Fast and Ultra-Low-Power In-Sensor Edge Vision for Gaze Estimation. (arXiv:2307.07813v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07813</link>
<description rdf:parseType="Literal">&lt;p&gt;Intelligent edge vision tasks encounter the critical challenge of ensuring
power and latency efficiency due to the typically heavy computational load they
impose on edge platforms.This work leverages one of the first &quot;AI in sensor&quot;
vision platforms, IMX500 by Sony, to achieve ultra-fast and ultra-low-power
end-to-end edge vision applications. We evaluate the IMX500 and compare it to
other edge platforms, such as the Google Coral Dev Micro and Sony Spresense, by
exploring gaze estimation as a case study. We propose TinyTracker, a highly
efficient, fully quantized model for 2D gaze estimation designed to maximize
the performance of the edge vision systems considered in this study.
TinyTracker achieves a 41x size reduction (600Kb) compared to iTracker [1]
without significant loss in gaze estimation accuracy (maximum of 0.16 cm when
fully quantized). TinyTracker&apos;s deployment on the Sony IMX500 vision sensor
results in end-to-end latency of around 19ms. The camera takes around 17.9ms to
read, process and transmit the pixels to the accelerator. The inference time of
the network is 0.86ms with an additional 0.24 ms for retrieving the results
from the sensor. The overall energy consumption of the end-to-end system is 4.9
mJ, including 0.06 mJ for inference. The end-to-end study shows that IMX500 is
1.7x faster than CoralMicro (19ms vs 34.4ms) and 7x more power efficient (4.9mJ
VS 34.2mJ)
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonazzi_P/0/1/0/all/0/1&quot;&gt;Pietro Bonazzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruegg_T/0/1/0/all/0/1&quot;&gt;Thomas Ruegg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_S/0/1/0/all/0/1&quot;&gt;Sizhen Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yawei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magno_M/0/1/0/all/0/1&quot;&gt;Michele Magno&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07916">
<title>On the Robustness of Split Learning against Adversarial Attacks. (arXiv:2307.07916v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07916</link>
<description rdf:parseType="Literal">&lt;p&gt;Split learning enables collaborative deep learning model training while
preserving data privacy and model security by avoiding direct sharing of raw
data and model details (i.e., sever and clients only hold partial sub-networks
and exchange intermediate computations). However, existing research has mainly
focused on examining its reliability for privacy protection, with little
investigation into model security. Specifically, by exploring full models,
attackers can launch adversarial attacks, and split learning can mitigate this
severe threat by only disclosing part of models to untrusted servers.This paper
aims to evaluate the robustness of split learning against adversarial attacks,
particularly in the most challenging setting where untrusted servers only have
access to the intermediate layers of the model.Existing adversarial attacks
mostly focus on the centralized setting instead of the collaborative setting,
thus, to better evaluate the robustness of split learning, we develop a
tailored attack called SPADV, which comprises two stages: 1) shadow model
training that addresses the issue of lacking part of the model and 2) local
adversarial attack that produces adversarial examples to evaluate.The first
stage only requires a few unlabeled non-IID data, and, in the second stage,
SPADV perturbs the intermediate output of natural samples to craft the
adversarial ones. The overall cost of the proposed attack process is relatively
low, yet the empirical attack effectiveness is significantly high,
demonstrating the surprising vulnerability of split learning to adversarial
attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1&quot;&gt;Mingyuan Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Cen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chengyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wenmeng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jun Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08397">
<title>CLIP-Guided StyleGAN Inversion for Text-Driven Real Image Editing. (arXiv:2307.08397v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08397</link>
<description rdf:parseType="Literal">&lt;p&gt;Researchers have recently begun exploring the use of StyleGAN-based models
for real image editing. One particularly interesting application is using
natural language descriptions to guide the editing process. Existing approaches
for editing images using language either resort to instance-level latent code
optimization or map predefined text prompts to some editing directions in the
latent space. However, these approaches have inherent limitations. The former
is not very efficient, while the latter often struggles to effectively handle
multi-attribute changes. To address these weaknesses, we present CLIPInverter,
a new text-driven image editing approach that is able to efficiently and
reliably perform multi-attribute changes. The core of our method is the use of
novel, lightweight text-conditioned adapter layers integrated into pretrained
GAN-inversion networks. We demonstrate that by conditioning the initial
inversion step on the CLIP embedding of the target description, we are able to
obtain more successful edit directions. Additionally, we use a CLIP-guided
refinement step to make corrections in the resulting residual latent codes,
which further improves the alignment with the text prompt. Our method
outperforms competing approaches in terms of manipulation accuracy and
photo-realism on various domains including human faces, cats, and birds, as
shown by our qualitative and quantitative results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baykal_A/0/1/0/all/0/1&quot;&gt;Ahmet Canberk Baykal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anees_A/0/1/0/all/0/1&quot;&gt;Abdul Basit Anees&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ceylan_D/0/1/0/all/0/1&quot;&gt;Duygu Ceylan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erdem_E/0/1/0/all/0/1&quot;&gt;Erkut Erdem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erdem_A/0/1/0/all/0/1&quot;&gt;Aykut Erdem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuret_D/0/1/0/all/0/1&quot;&gt;Deniz Yuret&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08466">
<title>Generalizable Classification of UHF Partial Discharge Signals in Gas-Insulated HVDC Systems Using Neural Networks. (arXiv:2307.08466v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08466</link>
<description rdf:parseType="Literal">&lt;p&gt;Undetected partial discharges (PDs) are a safety critical issue in high
voltage (HV) gas insulated systems (GIS). While the diagnosis of PDs under AC
voltage is well-established, the analysis of PDs under DC voltage remains an
active research field. A key focus of these investigations is the
classification of different PD sources to enable subsequent sophisticated
analysis.
&lt;/p&gt;
&lt;p&gt;In this paper, we propose and analyze a neural network-based approach for
classifying PD signals caused by metallic protrusions and conductive particles
on the insulator of HVDC GIS, without relying on pulse sequence analysis
features. In contrast to previous approaches, our proposed model can
discriminate the studied PD signals obtained at negative and positive
potentials, while also generalizing to unseen operating voltage multiples.
Additionally, we compare the performance of time- and frequency-domain input
signals and explore the impact of different normalization schemes to mitigate
the influence of free-space path loss between the sensor and defect location.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seitz_S/0/1/0/all/0/1&quot;&gt;Steffen Seitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gotz_T/0/1/0/all/0/1&quot;&gt;Thomas G&amp;#xf6;tz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lindenberg_C/0/1/0/all/0/1&quot;&gt;Christopher Lindenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tetzlaff_R/0/1/0/all/0/1&quot;&gt;Ronald Tetzlaff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlegel_S/0/1/0/all/0/1&quot;&gt;Stephan Schlegel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08535">
<title>Multi-class point cloud completion networks for 3D cardiac anatomy reconstruction from cine magnetic resonance images. (arXiv:2307.08535v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08535</link>
<description rdf:parseType="Literal">&lt;p&gt;Cine magnetic resonance imaging (MRI) is the current gold standard for the
assessment of cardiac anatomy and function. However, it typically only acquires
a set of two-dimensional (2D) slices of the underlying three-dimensional (3D)
anatomy of the heart, thus limiting the understanding and analysis of both
healthy and pathological cardiac morphology and physiology. In this paper, we
propose a novel fully automatic surface reconstruction pipeline capable of
reconstructing multi-class 3D cardiac anatomy meshes from raw cine MRI
acquisitions. Its key component is a multi-class point cloud completion network
(PCCN) capable of correcting both the sparsity and misalignment issues of the
3D reconstruction task in a unified model. We first evaluate the PCCN on a
large synthetic dataset of biventricular anatomies and observe Chamfer
distances between reconstructed and gold standard anatomies below or similar to
the underlying image resolution for multiple levels of slice misalignment.
Furthermore, we find a reduction in reconstruction error compared to a
benchmark 3D U-Net by 32% and 24% in terms of Hausdorff distance and mean
surface distance, respectively. We then apply the PCCN as part of our automated
reconstruction pipeline to 1000 subjects from the UK Biobank study in a
cross-domain transfer setting and demonstrate its ability to reconstruct
accurate and topologically plausible biventricular heart meshes with clinical
metrics comparable to the previous literature. Finally, we investigate the
robustness of our proposed approach and observe its capacity to successfully
handle multiple common outlier conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Beetz_M/0/1/0/all/0/1&quot;&gt;Marcel Beetz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Banerjee_A/0/1/0/all/0/1&quot;&gt;Abhirup Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ossenberg_Engels_J/0/1/0/all/0/1&quot;&gt;Julius Ossenberg-Engels&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Grau_V/0/1/0/all/0/1&quot;&gt;Vicente Grau&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>