<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-12-21T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13299" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13304" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13305" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13307" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13308" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13309" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13310" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13313" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13314" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13316" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13317" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13319" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13324" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13328" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13330" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13332" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13377" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13396" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13422" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13440" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13449" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13469" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13471" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13489" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13494" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13500" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13503" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13506" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13509" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13514" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13528" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13534" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13537" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13555" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13558" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13575" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13578" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13594" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13604" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13620" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13630" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13631" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13632" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13633" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13641" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13646" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13655" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13663" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13691" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13714" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13729" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13735" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13746" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13752" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13763" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13764" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13770" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13771" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13776" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13778" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13783" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13789" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13792" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13820" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13822" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13832" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13834" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13839" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13841" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13845" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13848" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13906" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13913" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13941" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13964" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13977" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13980" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13993" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14001" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14024" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14053" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14055" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14074" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14091" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14115" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14124" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14125" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14126" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14132" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14134" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14135" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14138" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14140" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14149" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14150" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14154" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14157" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2108.02893" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.02980" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.00400" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.08965" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.02998" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.14404" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.15136" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.07864" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.13495" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.14742" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.00114" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.01841" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.00586" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.06088" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.03693" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.08506" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.09854" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.03815" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.04743" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05807" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10300" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12743" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15194" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16150" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18273" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18295" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.01423" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07915" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09077" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00764" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15254" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15588" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.16586" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01196" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.06668" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.07528" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.08638" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.08746" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.10045" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07277" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.08154" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.08738" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12559" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12780" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04247" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.14859" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16898" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19583" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02358" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03830" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05152" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07967" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11059" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14981" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16512" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.18260" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03763" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03775" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06723" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07488" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07871" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08887" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09709" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10208" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10600" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11396" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11460" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11562" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12337" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12635" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12763" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13016" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13139" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13271" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2312.13299">
<title>Compact 3D Scene Representation via Self-Organizing Gaussian Grids. (arXiv:2312.13299v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13299</link>
<description rdf:parseType="Literal">&lt;p&gt;3D Gaussian Splatting has recently emerged as a highly promising technique
for modeling of static 3D scenes. In contrast to Neural Radiance Fields, it
utilizes efficient rasterization allowing for very fast rendering at
high-quality. However, the storage size is significantly higher, which hinders
practical deployment, e.g.~on resource constrained devices. In this paper, we
introduce a compact scene representation organizing the parameters of 3D
Gaussian Splatting (3DGS) into a 2D grid with local homogeneity, ensuring a
drastic reduction in storage requirements without compromising visual quality
during rendering. Central to our idea is the explicit exploitation of
perceptual redundancies present in natural scenes. In essence, the inherent
nature of a scene allows for numerous permutations of Gaussian parameters to
equivalently represent it. To this end, we propose a novel highly parallel
algorithm that regularly arranges the high-dimensional Gaussian parameters into
a 2D grid while preserving their neighborhood structure. During training, we
further enforce local smoothness between the sorted parameters in the grid. The
uncompressed Gaussians use the same structure as 3DGS, ensuring a seamless
integration with established renderers. Our method achieves a reduction factor
of 8x to 26x in size for complex scenes with no increase in training time,
marking a substantial leap forward in the domain of 3D scene distribution and
consumption. Additional information can be found on our project page:
https://fraunhoferhhi.github.io/Self-Organizing-Gaussians/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morgenstern_W/0/1/0/all/0/1&quot;&gt;Wieland Morgenstern&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barthel_F/0/1/0/all/0/1&quot;&gt;Florian Barthel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hilsmann_A/0/1/0/all/0/1&quot;&gt;Anna Hilsmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eisert_P/0/1/0/all/0/1&quot;&gt;Peter Eisert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13304">
<title>End-to-end Rain Streak Removal with RAW Images. (arXiv:2312.13304v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.13304</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we address the problem of rain streak removal with RAW images.
The general approach is firstly processing RAW data into RGB images and
removing rain streak with RGB images. Actually the original information of rain
in RAW images is affected by image signal processing (ISP) pipelines including
none-linear algorithms, unexpected noise, artifacts and so on. It gains more
benefit to directly remove rain in RAW data before being processed into RGB
format. To solve this problem, we propose a joint solution for rain removal and
RAW processing to obtain clean color images from rainy RAW image. To be
specific, we generate rainy RAW data by converting color rain streak into RAW
space and design simple but efficient RAW processing algorithms to synthesize
both rainy and clean color images. The rainy color images are used as reference
to help color corrections. Different backbones show that our method conduct a
better result compared with several other state-of-the-art deraining methods
focused on color image. In addition, the proposed network generalizes well to
other cameras beyond our selected RAW dataset. Finally, we give the result
tested on images processed by different ISP pipelines to show the
generalization performance of our model is better compared with methods on
color images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Du_G/0/1/0/all/0/1&quot;&gt;GuoDong Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Deng_H/0/1/0/all/0/1&quot;&gt;HaoJian Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Su_J/0/1/0/all/0/1&quot;&gt;JiaHao Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yuan Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13305">
<title>DVIS++: Improved Decoupled Framework for Universal Video Segmentation. (arXiv:2312.13305v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13305</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the \textbf{D}ecoupled \textbf{VI}deo \textbf{S}egmentation (DVIS)
framework, a novel approach for the challenging task of universal video
segmentation, including video instance segmentation (VIS), video semantic
segmentation (VSS), and video panoptic segmentation (VPS). Unlike previous
methods that model video segmentation in an end-to-end manner, our approach
decouples video segmentation into three cascaded sub-tasks: segmentation,
tracking, and refinement. This decoupling design allows for simpler and more
effective modeling of the spatio-temporal representations of objects,
especially in complex scenes and long videos. Accordingly, we introduce two
novel components: the referring tracker and the temporal refiner. These
components track objects frame by frame and model spatio-temporal
representations based on pre-aligned features. To improve the tracking
capability of DVIS, we propose a denoising training strategy and introduce
contrastive learning, resulting in a more robust framework named DVIS++.
Furthermore, we evaluate DVIS++ in various settings, including open vocabulary
and using a frozen pre-trained backbone. By integrating CLIP with DVIS++, we
present OV-DVIS++, the first open-vocabulary universal video segmentation
framework. We conduct extensive experiments on six mainstream benchmarks,
including the VIS, VSS, and VPS datasets. Using a unified architecture, DVIS++
significantly outperforms state-of-the-art specialized methods on these
benchmarks in both close- and open-vocabulary settings.
Code:~\url{https://github.com/zhang-tao-whu/DVIS_Plus}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1&quot;&gt;Xingye Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yikang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1&quot;&gt;Shunping Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xuebo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1&quot;&gt;Xin Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_P/0/1/0/all/0/1&quot;&gt;Pengfei Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhongyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yu Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13307">
<title>Not All Steps are Equal: Efficient Generation with Progressive Diffusion Models. (arXiv:2312.13307v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.13307</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have demonstrated remarkable efficacy in various generative
tasks with the predictive prowess of denoising model. Currently, these models
employ a uniform denoising approach across all timesteps. However, the inherent
variations in noisy latents at each timestep lead to conflicts during training,
constraining the potential of diffusion models. To address this challenge, we
propose a novel two-stage training strategy termed Step-Adaptive Training. In
the initial stage, a base denoising model is trained to encompass all
timesteps. Subsequently, we partition the timesteps into distinct groups,
fine-tuning the model within each group to achieve specialized denoising
capabilities. Recognizing that the difficulties of predicting noise at
different timesteps vary, we introduce a diverse model size requirement. We
dynamically adjust the model size for each timestep by estimating task
difficulty based on its signal-to-noise ratio before fine-tuning. This
adjustment is facilitated by a proxy-based structural importance assessment
mechanism, enabling precise and efficient pruning of the base denoising model.
Our experiments validate the effectiveness of the proposed training strategy,
demonstrating an improvement in the FID score on CIFAR10 by over 0.3 while
utilizing only 80\% of the computational resources. This innovative approach
not only enhances model performance but also significantly reduces
computational costs, opening new avenues for the development and application of
diffusion models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenhao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1&quot;&gt;Xiu Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1&quot;&gt;Shan You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1&quot;&gt;Chen Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chang Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13308">
<title>SWAGS: Sampling Windows Adaptively for Dynamic 3D Gaussian Splatting. (arXiv:2312.13308v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13308</link>
<description rdf:parseType="Literal">&lt;p&gt;Novel view synthesis has shown rapid progress recently, with methods capable
of producing evermore photo-realistic results. 3D Gaussian Splatting has
emerged as a particularly promising method, producing high-quality renderings
of static scenes and enabling interactive viewing at real-time frame rates.
However, it is currently limited to static scenes only. In this work, we extend
3D Gaussian Splatting to reconstruct dynamic scenes. We model the dynamics of a
scene using a tunable MLP, which learns the deformation field from a canonical
space to a set of 3D Gaussians per frame. To disentangle the static and dynamic
parts of the scene, we learn a tuneable parameter for each Gaussian, which
weighs the respective MLP parameters to focus attention on the dynamic parts.
This improves the model&apos;s ability to capture dynamics in scenes with an
imbalance of static to dynamic regions. To handle scenes of arbitrary length
whilst maintaining high rendering quality, we introduce an adaptive window
sampling strategy to partition the sequence into windows based on the amount of
movement in the sequence. We train a separate dynamic Gaussian Splatting model
for each window, allowing the canonical representation to change, thus enabling
the reconstruction of scenes with significant geometric or topological changes.
Temporal consistency is enforced using a fine-tuning step with self-supervising
consistency loss on randomly sampled novel views. As a result, our method
produces high-quality renderings of general dynamic scenes with competitive
quantitative performance, which can be viewed in real-time with our dynamic
interactive viewer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaw_R/0/1/0/all/0/1&quot;&gt;Richard Shaw&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jifei Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moreau_A/0/1/0/all/0/1&quot;&gt;Arthur Moreau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nazarczuk_M/0/1/0/all/0/1&quot;&gt;Michal Nazarczuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Catley_Chandar_S/0/1/0/all/0/1&quot;&gt;Sibi Catley-Chandar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhamo_H/0/1/0/all/0/1&quot;&gt;Helisa Dhamo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_Pellitero_E/0/1/0/all/0/1&quot;&gt;Eduardo Perez-Pellitero&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13309">
<title>Generate E-commerce Product Background by Integrating Category Commonality and Personalized Style. (arXiv:2312.13309v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13309</link>
<description rdf:parseType="Literal">&lt;p&gt;The state-of-the-art methods for e-commerce product background generation
suffer from the inefficiency of designing product-wise prompts when scaling up
the production, as well as the ineffectiveness of describing fine-grained
styles when customizing personalized backgrounds for some specific brands. To
address these obstacles, we integrate the category commonality and personalized
style into diffusion models. Concretely, we propose a Category-Wise Generator
to enable large-scale background generation for the first time. A unique
identifier in the prompt is assigned to each category, whose attention is
located on the background by a mask-guided cross attention layer to learn the
category-wise style. Furthermore, for products with specific and fine-grained
requirements in layout, elements, etc, a Personality-Wise Generator is devised
to learn such personalized style directly from a reference image to resolve
textual ambiguities, and is trained in a self-supervised manner for more
efficient training data usage. To advance research in this field, the first
large-scale e-commerce product background generation dataset BG60k is
constructed, which covers more than 60k product images from over 2k categories.
Experiments demonstrate that our method could generate high-quality backgrounds
for different categories, and maintain the personalized background style of
reference images. The link to BG60k and codes will be available soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haohan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1&quot;&gt;Wei Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yang Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yaoyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1&quot;&gt;Jingjing Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1&quot;&gt;Junjie Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhangang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1&quot;&gt;Lixing Bo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1&quot;&gt;Jingping Shao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13310">
<title>Computational Spectral Imaging with Unified Encoding Model: A Comparative Study and Beyond. (arXiv:2312.13310v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.13310</link>
<description rdf:parseType="Literal">&lt;p&gt;Computational spectral imaging is drawing increasing attention owing to the
snapshot advantage, and amplitude, phase, and wavelength encoding systems are
three types of representative implementations. Fairly comparing and
understanding the performance of these systems is essential, but challenging
due to the heterogeneity in encoding design. To overcome this limitation, we
propose the unified encoding model (UEM) that covers all physical systems using
the three encoding types. Specifically, the UEM comprises physical amplitude,
physical phase, and physical wavelength encoding models that can be combined
with a digital decoding model in a joint encoder-decoder optimization framework
to compare the three systems under a unified experimental setup fairly.
Furthermore, we extend the UEMs to ideal versions, namely, ideal amplitude,
ideal phase, and ideal wavelength encoding models, which are free from physical
constraints, to explore the full potential of the three types of computational
spectral imaging systems. Finally, we conduct a holistic comparison of the
three types of computational spectral imaging systems and provide valuable
insights for designing and exploiting these systems in the future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lizhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lingen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xue Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Song_F/0/1/0/all/0/1&quot;&gt;Fenglong Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Youliang Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13313">
<title>ParamISP: Learned Forward and Inverse ISPs using Camera Parameters. (arXiv:2312.13313v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.13313</link>
<description rdf:parseType="Literal">&lt;p&gt;RAW images are rarely shared mainly due to its excessive data size compared
to their sRGB counterparts obtained by camera ISPs. Learning the forward and
inverse processes of camera ISPs has been recently demonstrated, enabling
physically-meaningful RAW-level image processing on input sRGB images. However,
existing learning-based ISP methods fail to handle the large variations in the
ISP processes with respect to camera parameters such as ISO and exposure time,
and have limitations when used for various applications. In this paper, we
propose ParamISP, a learning-based method for forward and inverse conversion
between sRGB and RAW images, that adopts a novel neural-network module to
utilize camera parameters, which is dubbed as ParamNet. Given the camera
parameters provided in the EXIF data, ParamNet converts them into a feature
vector to control the ISP networks. Extensive experiments demonstrate that
ParamISP achieve superior RAW and sRGB reconstruction results compared to
previous methods and it can be effectively used for a variety of applications
such as deblurring dataset synthesis, raw deblurring, HDR reconstruction, and
camera-to-camera transfer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kim_W/0/1/0/all/0/1&quot;&gt;Woohyeok Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kim_G/0/1/0/all/0/1&quot;&gt;Geonu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Junyong Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seungyong Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Baek_S/0/1/0/all/0/1&quot;&gt;Seung-Hwan Baek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cho_S/0/1/0/all/0/1&quot;&gt;Sunghyun Cho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13314">
<title>Unlocking Pre-trained Image Backbones for Semantic Image Synthesis. (arXiv:2312.13314v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13314</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic image synthesis, i.e., generating images from user-provided semantic
label maps, is an important conditional image generation task as it allows to
control both the content as well as the spatial layout of generated images.
Although diffusion models have pushed the state of the art in generative image
modeling, the iterative nature of their inference process makes them
computationally demanding. Other approaches such as GANs are more efficient as
they only need a single feed-forward pass for generation, but the image quality
tends to suffer on large and diverse datasets. In this work, we propose a new
class of GAN discriminators for semantic image synthesis that generates highly
realistic images by exploiting feature backbone networks pre-trained for tasks
such as image classification. We also introduce a new generator architecture
with better context modeling and using cross-attention to inject noise into
latent variables, leading to more diverse generated images. Our model, which we
dub DP-SIMS, achieves state-of-the-art results in terms of image quality and
consistency with the input label maps on ADE-20K, COCO-Stuff, and Cityscapes,
surpassing recent diffusion models while requiring two orders of magnitude less
compute for inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berrada_T/0/1/0/all/0/1&quot;&gt;Tariq Berrada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verbeek_J/0/1/0/all/0/1&quot;&gt;Jakob Verbeek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Couprie_C/0/1/0/all/0/1&quot;&gt;Camille Couprie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alahari_K/0/1/0/all/0/1&quot;&gt;Karteek Alahari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13316">
<title>ECAMP: Entity-centered Context-aware Medical Vision Language Pre-training. (arXiv:2312.13316v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13316</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite significant advancements in medical vision-language pre-training,
existing methods have largely overlooked the inherent entity-specific context
within radiology reports and the complex cross-modality contextual
relationships between text and images. To close this gap, we propose a novel
Entity-centered Context-aware Medical Vision-language Pre-training (ECAMP)
framework, which is designed to enable a more entity-centered and
context-sensitive interpretation of medical data. Utilizing the recent powerful
large language model, we distill entity-centered context from medical reports,
which enables ECAMP to gain more effective supervision from the text modality.
By further pre-training our model with carefully designed entity-aware,
context-enhanced masked language modeling and context-guided super-resolution
tasks, ECAMP significantly refines the interplay between text and image
modalities, leading to an enhanced ability to extract entity-centered
contextual features. Besides, our proposed multi-scale context fusion design
also improves the semantic integration of both coarse and fine-level image
representations, prompting better performance for multi-scale downstream
applications. Combining these components leads to significant performance leaps
over current state-of-the-art methods and establishes a new standard for
cross-modality learning in medical imaging, whose effectiveness is demonstrated
by our extensive experiments on various tasks including classification,
segmentation, and detection across several public datasets. Code and models are
available at https://github.com/ToniChopp/ECAMP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rongsheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1&quot;&gt;Qingsong Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_H/0/1/0/all/0/1&quot;&gt;Haoran Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zhiyang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1&quot;&gt;Xiaodong Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zihang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;S.Kevin Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13317">
<title>Deep Hybrid Camera Deblurring. (arXiv:2312.13317v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13317</link>
<description rdf:parseType="Literal">&lt;p&gt;Mobile cameras, despite their significant advancements, still face low-light
challenges due to compact sensors and lenses, leading to longer exposures and
motion blur. Traditional solutions like blind deconvolution and learning-based
methods often fall short in handling ill-posedness of the deblurring problem.
To address this, we propose a novel deblurring framework for multi-camera
smartphones, utilizing a hybrid imaging technique. We simultaneously capture a
long exposure wide-angle image and ultra-wide burst images from a smartphone,
and use the sharp burst to estimate blur kernels for deblurring the wide-angle
image. For learning and evaluation of our network, we introduce the HCBlur
dataset, which includes pairs of blurry wide-angle and sharp ultra-wide burst
images, and their sharp wide-angle counterparts. We extensively evaluate our
method, and the result shows the state-of-the-art quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rim_J/0/1/0/all/0/1&quot;&gt;Jaesung Rim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Junyong Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Heemin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1&quot;&gt;Sunghyun Cho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13319">
<title>In2SET: Intra-Inter Similarity Exploiting Transformer for Dual-Camera Compressive Hyperspectral Imaging. (arXiv:2312.13319v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.13319</link>
<description rdf:parseType="Literal">&lt;p&gt;Dual-Camera Compressed Hyperspectral Imaging (DCCHI) offers the capability to
reconstruct 3D Hyperspectral Image (HSI) by fusing compressive and Panchromatic
(PAN) image, which has shown great potential for snapshot hyperspectral imaging
in practice. In this paper, we introduce a novel DCCHI reconstruction network,
the Intra-Inter Similarity Exploiting Transformer (In2SET). Our key insight is
to make full use of the PAN image to assist the reconstruction. To this end, we
propose using the intra-similarity within the PAN image as a proxy for
approximating the intra-similarity in the original HSI, thereby offering an
enhanced content prior for more accurate HSI reconstruction. Furthermore, we
aim to align the features from the underlying HSI with those of the PAN image,
maintaining semantic consistency and introducing new contextual information for
the reconstruction process. By integrating In2SET into a PAN-guided unrolling
framework, our method substantially enhances the spatial-spectral fidelity and
detail of the reconstructed images, providing a more comprehensive and accurate
depiction of the scene. Extensive experiments conducted on both real and
simulated datasets demonstrate that our approach consistently outperforms
existing state-of-the-art methods in terms of reconstruction quality and
computational complexity. Code will be released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lizhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiangtian Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Maoqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Lin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Hua Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13324">
<title>ShowRoom3D: Text to High-Quality 3D Room Generation Using 3D Priors. (arXiv:2312.13324v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13324</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce ShowRoom3D, a three-stage approach for generating high-quality
3D room-scale scenes from texts. Previous methods using 2D diffusion priors to
optimize neural radiance fields for generating room-scale scenes have shown
unsatisfactory quality. This is primarily attributed to the limitations of 2D
priors lacking 3D awareness and constraints in the training methodology. In
this paper, we utilize a 3D diffusion prior, MVDiffusion, to optimize the 3D
room-scale scene. Our contributions are in two aspects. Firstly, we propose a
progressive view selection process to optimize NeRF. This involves dividing the
training process into three stages, gradually expanding the camera sampling
scope. Secondly, we propose the pose transformation method in the second stage.
It will ensure MVDiffusion provide the accurate view guidance. As a result,
ShowRoom3D enables the generation of rooms with improved structural integrity,
enhanced clarity from any view, reduced content repetition, and higher
consistency across different perspectives. Extensive experiments demonstrate
that our method, significantly outperforms state-of-the-art approaches by a
large margin in terms of user study.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1&quot;&gt;Weijia Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yan-Pei Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jia-Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhongcong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1&quot;&gt;Mike Zheng Shou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13328">
<title>NeLF-Pro: Neural Light Field Probes. (arXiv:2312.13328v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13328</link>
<description rdf:parseType="Literal">&lt;p&gt;We present NeLF-Pro, a novel representation for modeling and reconstructing
light fields in diverse natural scenes that vary in extend and spatial
granularity. In contrast to previous fast reconstruction methods that represent
the 3D scene globally, we model the light field of a scene as a set of local
light field feature probes, parameterized with position and multi-channel 2D
feature maps. Our central idea is to bake the scene&apos;s light field into
spatially varying learnable representations and to query point features by
weighted blending of probes close to the camera - allowing for mipmap
representation and rendering. We introduce a novel vector-matrix-matrix (VMM)
factorization technique that effectively represents the light field feature
probes as products of core factors (i.e., VM) shared among local feature
probes, and a basis factor (i.e., M) - efficiently encoding internal
relationships and patterns within the scene.Experimentally, we demonstrate that
NeLF-Pro significantly boosts the performance of feature grid-based
representations, and achieves fast reconstruction with better rendering quality
while maintaining compact modeling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_Z/0/1/0/all/0/1&quot;&gt;Zinuo You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1&quot;&gt;Andreas Geiger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1&quot;&gt;Anpei Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13330">
<title>Subject-Oriented Video Captioning. (arXiv:2312.13330v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13330</link>
<description rdf:parseType="Literal">&lt;p&gt;Describing video content according to users&apos; needs is a long-held goal.
Although existing video captioning methods have made significant progress, the
generated captions may not focus on the entity that users are particularly
interested in. To address this problem, we propose a new video captioning task,
subject-oriented video captioning, which allows users to specify the describing
target via a bounding box. To support this task, we construct two
subject-oriented video captioning datasets based on two widely used video
captioning datasets: MSVD and MSRVTT, by annotating subjects in each video for
each caption. These datasets pave the way for future technique development. As
the first attempt, we evaluate four state-of-the-art general video captioning
models, and have observed a large performance drop. We then explore several
strategies to enable them to describe the desired target. Experimental results
show obvious improvement, but there is still a large room for further
exploration in this field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yunchuan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teng_C/0/1/0/all/0/1&quot;&gt;Chang Teng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1&quot;&gt;Yuankai Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guorong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qing_L/0/1/0/all/0/1&quot;&gt;Laiyu Qing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qingming Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13332">
<title>Ternary-type Opacity and Hybrid Odometry for RGB-only NeRF-SLAM. (arXiv:2312.13332v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13332</link>
<description rdf:parseType="Literal">&lt;p&gt;The opacity of rigid 3D scenes with opaque surfaces is considered to be of a
binary type. However, we observed that this property is not followed by the
existing RGB-only NeRF-SLAM. Therefore, we are motivated to introduce this
prior into the RGB-only NeRF-SLAM pipeline. Unfortunately, the optimization
through the volumetric rendering function does not facilitate easy integration
of the desired prior. Instead, we observed that the opacity of ternary-type
(TT) is well supported. In this work, we study why ternary-type opacity is
well-suited and desired for the task at hand. In particular, we provide
theoretical insights into the process of jointly optimizing radiance and
opacity through the volumetric rendering process. Through exhaustive
experiments on benchmark datasets, we validate our claim and provide insights
into the optimization process, which we believe will unleash the potential of
RGB-only NeRF-SLAM. To foster this line of research, we also propose a simple
yet novel visual odometry scheme that uses a hybrid combination of volumetric
and warping-based image renderings. More specifically, the proposed hybrid
odometry (HO) additionally uses image warping-based coarse odometry, leading up
to an order of magnitude final speed-up. Furthermore, we show that the proposed
TT and HO well complement each other, offering state-of-the-art results on
benchmark datasets in terms of both speed and accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Junru Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nachkov_A/0/1/0/all/0/1&quot;&gt;Asen Nachkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1&quot;&gt;Songyou Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1&quot;&gt;Luc Van Gool&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1&quot;&gt;Danda Pani Paudel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13377">
<title>SADA: Semantic adversarial unsupervised domain adaptation for Temporal Action Localization. (arXiv:2312.13377v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13377</link>
<description rdf:parseType="Literal">&lt;p&gt;Temporal Action Localization (TAL) is a complex task that poses relevant
challenges, particularly when attempting to generalize on new -- unseen --
domains in real-world applications. These scenarios, despite realistic, are
often neglected in the literature, exposing these solutions to important
performance degradation. In this work, we tackle this issue by introducing, for
the first time, an approach for Unsupervised Domain Adaptation (UDA) in sparse
TAL, which we refer to as Semantic Adversarial unsupervised Domain Adaptation
(SADA). Our contribution is threefold: (1) we pioneer the development of a
domain adaptation model that operates on realistic sparse action detection
benchmarks; (2) we tackle the limitations of global-distribution alignment
techniques by introducing a novel adversarial loss that is sensitive to local
class distributions, ensuring finer-grained adaptation; and (3) we present a
novel experimental setup, based on EpicKitchens100, that evaluates multiple
types of domain shifts in a comprehensive manner. Our experimental results
indicate that SADA improves the adaptation across domains when compared to
fully supervised state-of-the-art and alternative UDA methods, attaining a
relative performance boost of up to 14%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pujol_Perich_D/0/1/0/all/0/1&quot;&gt;David Pujol-Perich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clapes_A/0/1/0/all/0/1&quot;&gt;Albert Clap&amp;#xe9;s&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1&quot;&gt;Sergio Escalera&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13396">
<title>EPNet: An Efficient Pyramid Network for Enhanced Single-Image Super-Resolution with Reduced Computational Requirements. (arXiv:2312.13396v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13396</link>
<description rdf:parseType="Literal">&lt;p&gt;Single-image super-resolution (SISR) has seen significant advancements
through the integration of deep learning. However, the substantial
computational and memory requirements of existing methods often limit their
practical application. This paper introduces a new Efficient Pyramid Network
(EPNet) that harmoniously merges an Edge Split Pyramid Module (ESPM) with a
Panoramic Feature Extraction Module (PFEM) to overcome the limitations of
existing methods, particularly in terms of computational efficiency. The ESPM
applies a pyramid-based channel separation strategy, boosting feature
extraction while maintaining computational efficiency. The PFEM, a novel fusion
of CNN and Transformer structures, enables the concurrent extraction of local
and global features, thereby providing a panoramic view of the image landscape.
Our architecture integrates the PFEM in a manner that facilitates the
streamlined exchange of feature information and allows for the further
refinement of image texture details. Experimental results indicate that our
model outperforms existing state-of-the-art methods in image resolution
quality, while considerably decreasing computational and memory costs. This
research contributes to the ongoing evolution of efficient and practical SISR
methodologies, bearing broader implications for the field of computer vision.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Jinman Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fieguth_P/0/1/0/all/0/1&quot;&gt;Paul Fieguth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13422">
<title>Texture Matching GAN for CT Image Enhancement. (arXiv:2312.13422v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.13422</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNN) are commonly used to denoise and sharpen X-ray
computed tomography (CT) images with the goal of reducing patient X-ray dosage
while maintaining reconstruction quality. However, naive application of
DNN-based methods can result in image texture that is undesirable in clinical
applications. Alternatively, generative adversarial network (GAN) based methods
can produce appropriate texture, but naive application of GANs can introduce
inaccurate or even unreal image detail. In this paper, we propose a texture
matching generative adversarial network (TMGAN) that enhances CT images while
generating an image texture that can be matched to a target texture. We use
parallel generators to separate anatomical features from the generated texture,
which allows the GAN to be trained to match the desired texture without
directly affecting the underlying CT image. We demonstrate that TMGAN generates
enhanced image quality while also producing image texture that is desirable for
clinical application.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nagare_M/0/1/0/all/0/1&quot;&gt;Madhuri Nagare&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Buzzard_G/0/1/0/all/0/1&quot;&gt;Gregery T. Buzzard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bouman_C/0/1/0/all/0/1&quot;&gt;Charles A. Bouman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13440">
<title>MGAug: Multimodal Geometric Augmentation in Latent Spaces of Image Deformations. (arXiv:2312.13440v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13440</link>
<description rdf:parseType="Literal">&lt;p&gt;Geometric transformations have been widely used to augment the size of
training images. Existing methods often assume a unimodal distribution of the
underlying transformations between images, which limits their power when data
with multimodal distributions occur. In this paper, we propose a novel model,
Multimodal Geometric Augmentation (MGAug), that for the first time generates
augmenting transformations in a multimodal latent space of geometric
deformations. To achieve this, we first develop a deep network that embeds the
learning of latent geometric spaces of diffeomorphic transformations (a.k.a.
diffeomorphisms) in a variational autoencoder (VAE). A mixture of multivariate
Gaussians is formulated in the tangent space of diffeomorphisms and serves as a
prior to approximate the hidden distribution of image transformations. We then
augment the original training dataset by deforming images using randomly
sampled transformations from the learned multimodal latent space of VAE. To
validate the efficiency of our model, we jointly learn the augmentation
strategy with two distinct domain-specific tasks: multi-class classification on
2D synthetic datasets and segmentation on real 3D brain magnetic resonance
images (MRIs). We also compare MGAug with state-of-the-art transformation-based
image augmentation algorithms. Experimental results show that our proposed
approach outperforms all baselines by significantly improved prediction
accuracy. Our code is publicly available at
https://github.com/tonmoy-hossain/MGAug.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hossain_T/0/1/0/all/0/1&quot;&gt;Tonmoy Hossain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Miaomiao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13449">
<title>Building Lane-Level Maps from Aerial Images. (arXiv:2312.13449v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13449</link>
<description rdf:parseType="Literal">&lt;p&gt;Detecting lane lines from sensors is becoming an increasingly significant
part of autonomous driving systems. However, less development has been made on
high-definition lane-level mapping based on aerial images, which could
automatically build and update offline maps for auto-driving systems. To this
end, our work focuses on extracting fine-level detailed lane lines together
with their topological structures. This task is challenging since it requires
large amounts of data covering different lane types, terrain and regions. In
this paper, we introduce for the first time a large-scale aerial image dataset
built for lane detection, with high-quality polyline lane annotations on
high-resolution images of around 80 kilometers of road. Moreover, we developed
a baseline deep learning lane detection method from aerial images, called
AerialLaneNet, consisting of two stages. The first stage is to produce
coarse-grained results at point level, and the second stage exploits the
coarse-grained results and feature to perform the vertex-matching task,
producing fine-grained lanes with topology. The experiments show our approach
achieves significant improvement compared with the state-of-the-art methods on
our new dataset. Our code and new dataset are available at
https://github.com/Jiawei-Yao0812/AerialLaneNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Jiawei Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1&quot;&gt;Xiaochao Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13469">
<title>Neural feels with neural fields: Visuo-tactile perception for in-hand manipulation. (arXiv:2312.13469v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.13469</link>
<description rdf:parseType="Literal">&lt;p&gt;To achieve human-level dexterity, robots must infer spatial awareness from
multimodal sensing to reason over contact interactions. During in-hand
manipulation of novel objects, such spatial awareness involves estimating the
object&apos;s pose and shape. The status quo for in-hand perception primarily
employs vision, and restricts to tracking a priori known objects. Moreover,
visual occlusion of objects in-hand is imminent during manipulation, preventing
current systems to push beyond tasks without occlusion. We combine vision and
touch sensing on a multi-fingered hand to estimate an object&apos;s pose and shape
during in-hand manipulation. Our method, NeuralFeels, encodes object geometry
by learning a neural field online and jointly tracks it by optimizing a pose
graph problem. We study multimodal in-hand perception in simulation and the
real-world, interacting with different objects via a proprioception-driven
policy. Our experiments show final reconstruction F-scores of $81$% and average
pose drifts of $4.7\,\text{mm}$, further reduced to $2.3\,\text{mm}$ with known
CAD models. Additionally, we observe that under heavy visual occlusion we can
achieve up to $94$% improvements in tracking compared to vision-only methods.
Our results demonstrate that touch, at the very least, refines and, at the very
best, disambiguates visual estimates during in-hand manipulation. We release
our evaluation dataset of 70 experiments, FeelSight, as a step towards
benchmarking in this domain. Our neural representation driven by multimodal
sensing can serve as a perception backbone towards advancing robot dexterity.
Videos can be found on our project website
https://suddhu.github.io/neural-feels/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suresh_S/0/1/0/all/0/1&quot;&gt;Sudharshan Suresh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1&quot;&gt;Haozhi Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tingfan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_T/0/1/0/all/0/1&quot;&gt;Taosha Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pineda_L/0/1/0/all/0/1&quot;&gt;Luis Pineda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lambeta_M/0/1/0/all/0/1&quot;&gt;Mike Lambeta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1&quot;&gt;Jitendra Malik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalakrishnan_M/0/1/0/all/0/1&quot;&gt;Mrinal Kalakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calandra_R/0/1/0/all/0/1&quot;&gt;Roberto Calandra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaess_M/0/1/0/all/0/1&quot;&gt;Michael Kaess&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortiz_J/0/1/0/all/0/1&quot;&gt;Joseph Ortiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukadam_M/0/1/0/all/0/1&quot;&gt;Mustafa Mukadam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13471">
<title>NeRF-VO: Real-Time Sparse Visual Odometry with Neural Radiance Fields. (arXiv:2312.13471v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13471</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a novel monocular visual odometry (VO) system, NeRF-VO, that
integrates learning-based sparse visual odometry for low-latency camera
tracking and a neural radiance scene representation for sophisticated dense
reconstruction and novel view synthesis. Our system initializes camera poses
using sparse visual odometry and obtains view-dependent dense geometry priors
from a monocular depth prediction network. We harmonize the scale of poses and
dense geometry, treating them as supervisory cues to train a neural implicit
scene representation. NeRF-VO demonstrates exceptional performance in both
photometric and geometric fidelity of the scene representation by jointly
optimizing a sliding window of keyframed poses and the underlying dense
geometry, which is accomplished through training the radiance field with volume
rendering. We surpass state-of-the-art methods in pose estimation accuracy,
novel view synthesis fidelity, and dense reconstruction quality across a
variety of synthetic and real-world datasets, while achieving a higher camera
tracking frequency and consuming less GPU memory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naumann_J/0/1/0/all/0/1&quot;&gt;Jens Naumann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1&quot;&gt;Binbin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leutenegger_S/0/1/0/all/0/1&quot;&gt;Stefan Leutenegger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1&quot;&gt;Xingxing Zuo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13489">
<title>Embedded Shape Matching in Photogrammetry Data for Modeling Making Knowledge. (arXiv:2312.13489v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13489</link>
<description rdf:parseType="Literal">&lt;p&gt;In three-dimensional models obtained by photogrammetry of existing
structures, all of the shapes that the eye can select cannot always find their
equivalents in the geometric components of the model. However, the matching of
meaningful parts and assemblages with the records acquired with rapid and
detailed documentation methods will provide an advantage for the creation of
information models of existing structures. While aiming to produce answers to
this problem and in order to overcome the difficulties of pattern recognition
in three-dimensional models, we used two-dimensional samples obtained by
projection. Processing techniques such as ambient occlusion, curvature and
normal maps are commonly used in modern computer graphics applications that
enable the representation of three-dimensional surface properties in
two-dimensional data sets. The method we propose is based on the recognition of
patterns through these mappings instead of the usual light-based visualization.
The first stage of the application is photogrammetric capture of a few examples
of Zeugma mosaics and three-dimensional digital modeling of a set of Seljuk era
brick walls based on knowledge obtained through architectural history
literature. The second stage covers the creation of digital models byprocessing
the surface representation obtained from this data using Alice Vision,
OpenCV-Python, and Autodesk Maya to include information on aspects of the
making of the walls. What is envisioned for the next stages is that the mapping
data contributes and supports the knowledge for rule-based design and making
processesof cultural heritage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tas_D/0/1/0/all/0/1&quot;&gt;Demircan Tas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozkar_M/0/1/0/all/0/1&quot;&gt;Mine &amp;#xd6;zkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13494">
<title>Visual Tomography: Physically Faithful Volumetric Models of Partially Translucent Objects. (arXiv:2312.13494v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13494</link>
<description rdf:parseType="Literal">&lt;p&gt;When created faithfully from real-world data, Digital 3D representations of
objects can be useful for human or computer-assisted analysis. Such models can
also serve for generating training data for machine learning approaches in
settings where data is difficult to obtain or where too few training data
exists, e.g. by providing novel views or images in varying conditions. While
the vast amount of visual 3D reconstruction approaches focus on non-physical
models, textured object surfaces or shapes, in this contribution we propose a
volumetric reconstruction approach that obtains a physical model including the
interior of partially translucent objects such as plankton or insects. Our
technique photographs the object under different poses in front of a bright
white light source and computes absorption and scattering per voxel. It can be
interpreted as visual tomography that we solve by inverse raytracing. We
additionally suggest a method to convert non-physical NeRF media into a
physically-based volumetric grid for initialization and illustrate the
usefulness of the approach using two real-world plankton validation sets, the
lab-scanned models being finally also relighted and virtually submerged in a
scenario with augmented medium and illumination conditions. Please visit the
project homepage at www.marine.informatik.uni-kiel.de/go/vito
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakath_D/0/1/0/all/0/1&quot;&gt;David Nakath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weng_X/0/1/0/all/0/1&quot;&gt;Xiangyu Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+She_M/0/1/0/all/0/1&quot;&gt;Mengkun She&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koser_K/0/1/0/all/0/1&quot;&gt;Kevin K&amp;#xf6;ser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13500">
<title>Federated Continual Novel Class Learning. (arXiv:2312.13500v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13500</link>
<description rdf:parseType="Literal">&lt;p&gt;In a privacy-focused era, Federated Learning (FL) has emerged as a promising
machine learning technique. However, most existing FL studies assume that the
data distribution remains nearly fixed over time, while real-world scenarios
often involve dynamic and continual changes. To equip FL systems with continual
model evolution capabilities, we focus on an important problem called Federated
Continual Novel Class Learning (FedCN) in this work. The biggest challenge in
FedCN is to merge and align novel classes that are discovered and learned by
different clients without compromising privacy. To address this, we propose a
Global Alignment Learning (GAL) framework that can accurately estimate the
global novel class number and provide effective guidance for local training
from a global perspective, all while maintaining privacy protection.
Specifically, GAL first locates high-density regions in the representation
space through a bi-level clustering mechanism to estimate the novel class
number, with which the global prototypes corresponding to novel classes can be
constructed. Then, GAL uses a novel semantic weighted loss to capture all
possible correlations between these prototypes and the training data for
mitigating the impact of pseudo-label noise and data heterogeneity. Extensive
experiments on various datasets demonstrate GAL&apos;s superior performance over
state-of-the-art novel class discovery methods. In particular, GAL achieves
significant improvements in novel-class performance, increasing the accuracy by
5.1% to 10.6% in the case of one novel class learning stage and by 7.8% to
17.9% in the case of two novel class learning stages, without sacrificing
known-class performance. Moreover, GAL is shown to be effective in equipping a
variety of different mainstream FL algorithms with novel class discovery and
learning capability, highlighting its potential for many real-world
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lixu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chenxi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Junfeng Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1&quot;&gt;Jiahua Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Heng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1&quot;&gt;Qi Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13503">
<title>InfoVisDial: An Informative Visual Dialogue Dataset by Bridging Large Multimodal and Language Models. (arXiv:2312.13503v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13503</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we build a visual dialogue dataset, named InfoVisDial, which
provides rich informative answers in each round even with external knowledge
related to the visual content. Different from existing datasets where the
answer is compact and short, InfoVisDial contains long free-form answers with
rich information in each round of dialogue. For effective data collection, the
key idea is to bridge the large-scale multimodal model (e.g., GIT) and the
language models (e.g., GPT-3). GIT can describe the image content even with
scene text, while GPT-3 can generate informative dialogue based on the image
description and appropriate prompting techniques. With such automatic pipeline,
we can readily generate informative visual dialogue data at scale. Then, we ask
human annotators to rate the generated dialogues to filter the low-quality
conversations.Human analyses show that InfoVisDial covers informative and
diverse dialogue topics: $54.4\%$ of the dialogue rounds are related to image
scene texts, and $36.7\%$ require external knowledge. Each round&apos;s answer is
also long and open-ended: $87.3\%$ of answers are unique with an average length
of $8.9$, compared with $27.37\%$ and $2.9$ in VisDial. Last, we propose a
strong baseline by adapting the GIT model for the visual dialogue task and
fine-tune the model on InfoVisDial. Hopefully, our work can motivate more
effort on this direction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1&quot;&gt;Bingbing Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhengyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianfeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1&quot;&gt;Zhe Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Howe_B/0/1/0/all/0/1&quot;&gt;Bill Howe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lijuan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13506">
<title>SPDGAN: A Generative Adversarial Network based on SPD Manifold Learning for Automatic Image Colorization. (arXiv:2312.13506v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13506</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper addresses the automatic colorization problem, which converts a
gray-scale image to a colorized one. Recent deep-learning approaches can
colorize automatically grayscale images. However, when it comes to different
scenes which contain distinct color styles, it is difficult to accurately
capture the color characteristics. In this work, we propose a fully automatic
colorization approach based on Symmetric Positive Definite (SPD) Manifold
Learning with a generative adversarial network (SPDGAN) that improves the
quality of the colorization results. Our SPDGAN model establishes an
adversarial game between two discriminators and a generator. The latter is
based on ResNet architecture with few alterations. Its goal is to generate fake
colorized images without losing color information across layers through
residual connections. Then, we employ two discriminators from different
domains. The first one is devoted to the image pixel domain, while the second
one is to the Riemann manifold domain which helps to avoid color misalignment.
Extensive experiments are conducted on the Places365 and COCO-stuff databases
to test the effect of each component of our SPDGAN. In addition, quantitative
and qualitative comparisons with state-of-the-art methods demonstrate the
effectiveness of our model by achieving more realistic colorized images with
less artifacts visually, and good results of PSNR, SSIM, and FID values.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mourchid_Y/0/1/0/all/0/1&quot;&gt;Youssef Mourchid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Donias_M/0/1/0/all/0/1&quot;&gt;Marc Donias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berthoumieu_Y/0/1/0/all/0/1&quot;&gt;Yannick Berthoumieu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Najim_M/0/1/0/all/0/1&quot;&gt;Mohamed Najim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13509">
<title>MR-STGN: Multi-Residual Spatio Temporal Graph Network Using Attention Fusion for Patient Action Assessment. (arXiv:2312.13509v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13509</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate assessment of patient actions plays a crucial role in healthcare as
it contributes significantly to disease progression monitoring and treatment
effectiveness. However, traditional approaches to assess patient actions often
rely on manual observation and scoring, which are subjective and
time-consuming. In this paper, we propose an automated approach for patient
action assessment using a Multi-Residual Spatio Temporal Graph Network
(MR-STGN) that incorporates both angular and positional 3D skeletons. The
MR-STGN is specifically designed to capture the spatio-temporal dynamics of
patient actions. It achieves this by integrating information from multiple
residual layers, with each layer extracting features at distinct levels of
abstraction. Furthermore, we integrate an attention fusion mechanism into the
network, which facilitates the adaptive weighting of various features. This
empowers the model to concentrate on the most pertinent aspects of the
patient&apos;s movements, offering precise instructions regarding specific body
parts or movements that require attention. Ablation studies are conducted to
analyze the impact of individual components within the proposed model. We
evaluate our model on the UI-PRMD dataset demonstrating its performance in
accurately predicting real-time patient action scores, surpassing
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mourchid_Y/0/1/0/all/0/1&quot;&gt;Youssef Mourchid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Slama_R/0/1/0/all/0/1&quot;&gt;Rim Slama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13514">
<title>Rethinking of Feature Interaction for Multi-task Learning on Dense Prediction. (arXiv:2312.13514v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13514</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing works generally adopt the encoder-decoder structure for Multi-task
Dense Prediction, where the encoder extracts the task-generic features, and
multiple decoders generate task-specific features for predictions. We observe
that low-level representations with rich details and high-level representations
with abundant task information are not both involved in the multi-task
interaction process. Additionally, low-quality and low-efficiency issues also
exist in current multi-task learning architectures. In this work, we propose to
learn a comprehensive intermediate feature globally from both task-generic and
task-specific features, we reveal an important fact that this intermediate
feature, namely the bridge feature, is a good solution to the above issues.
Based on this, we propose a novel Bridge-Feature-Centirc Interaction (BRFI)
method. A Bridge Feature Extractor (BFE) is designed for the generation of
strong bridge features and Task Pattern Propagation (TPP) is applied to ensure
high-quality task interaction participants. Then a Task-Feature Refiner (TFR)
is developed to refine final task predictions with the well-learned knowledge
from the bridge features. Extensive experiments are conducted on NYUD-v2 and
PASCAL Context benchmarks, and the superior performance shows the proposed
architecture is effective and powerful in promoting different dense prediction
tasks simultaneously.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jingdong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1&quot;&gt;Jiayuan Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_P/0/1/0/all/0/1&quot;&gt;Peng Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1&quot;&gt;Hancheng Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Baopu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1&quot;&gt;Yancheng Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tao Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13528">
<title>DyBluRF: Dynamic Deblurring Neural Radiance Fields for Blurry Monocular Video. (arXiv:2312.13528v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13528</link>
<description rdf:parseType="Literal">&lt;p&gt;Video view synthesis, allowing for the creation of visually appealing frames
from arbitrary viewpoints and times, offers immersive viewing experiences.
Neural radiance fields, particularly NeRF, initially developed for static
scenes, have spurred the creation of various methods for video view synthesis.
However, the challenge for video view synthesis arises from motion blur, a
consequence of object or camera movement during exposure, which hinders the
precise synthesis of sharp spatio-temporal views. In response, we propose a
novel dynamic deblurring NeRF framework for blurry monocular video, called
DyBluRF, consisting of an Interleave Ray Refinement (IRR) stage and a Motion
Decomposition-based Deblurring (MDD) stage. Our DyBluRF is the first that
addresses and handles the novel view synthesis for blurry monocular video. The
IRR stage jointly reconstructs dynamic 3D scenes and refines the inaccurate
camera pose information to combat imprecise pose information extracted from the
given blurry frames. The MDD stage is a novel incremental latent sharp-rays
prediction (ILSP) approach for the blurry monocular video frames by decomposing
the latent sharp rays into global camera motion and local object motion
components. Extensive experimental results demonstrate that our DyBluRF
outperforms qualitatively and quantitatively the very recent state-of-the-art
methods. Our project page including source codes and pretrained model are
publicly available at https://kaist-viclab.github.io/dyblurf-site/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bui_M/0/1/0/all/0/1&quot;&gt;Minh-Quan Viet Bui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Jongmin Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1&quot;&gt;Jihyong Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Munchurl Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13534">
<title>SE(3)-Equivariant and Noise-Invariant 3D Motion Tracking in Medical Images. (arXiv:2312.13534v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.13534</link>
<description rdf:parseType="Literal">&lt;p&gt;Rigid motion tracking is paramount in many medical imaging applications where
movements need to be detected, corrected, or accounted for. Modern strategies
rely on convolutional neural networks (CNN) and pose this problem as rigid
registration. Yet, CNNs do not exploit natural symmetries in this task, as they
are equivariant to translations (their outputs shift with their inputs) but not
to rotations. Here we propose EquiTrack, the first method that uses recent
steerable SE(3)-equivariant CNNs (E-CNN) for motion tracking. While steerable
E-CNNs can extract corresponding features across different poses, testing them
on noisy medical images reveals that they do not have enough learning capacity
to learn noise invariance. Thus, we introduce a hybrid architecture that pairs
a denoiser with an E-CNN to decouple the processing of anatomically irrelevant
intensity features from the extraction of equivariant spatial features. Rigid
transforms are then estimated in closed-form. EquiTrack outperforms
state-of-the-art learning and optimisation methods for motion tracking in adult
brain MRI and fetal MRI time series. Our code is available at
github.com/BBillot/equitrack.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Billot_B/0/1/0/all/0/1&quot;&gt;Benjamin Billot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Moyer_D/0/1/0/all/0/1&quot;&gt;Daniel Moyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dey_N/0/1/0/all/0/1&quot;&gt;Neel Dey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hoffmann_M/0/1/0/all/0/1&quot;&gt;Malte Hoffmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Turk_E/0/1/0/all/0/1&quot;&gt;Esra Abaci Turk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gagoski_B/0/1/0/all/0/1&quot;&gt;Borjan Gagoski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Grant_E/0/1/0/all/0/1&quot;&gt;Ellen Grant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Golland_P/0/1/0/all/0/1&quot;&gt;Polina Golland&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13537">
<title>HyperEditor: Achieving Both Authenticity and Cross-Domain Capability in Image Editing via Hypernetworks. (arXiv:2312.13537v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13537</link>
<description rdf:parseType="Literal">&lt;p&gt;Editing real images authentically while also achieving cross-domain editing
remains a challenge. Recent studies have focused on converting real images into
latent codes and accomplishing image editing by manipulating these codes.
However, merely manipulating the latent codes would constrain the edited images
to the generator&apos;s image domain, hindering the attainment of diverse editing
goals. In response, we propose an innovative image editing method called
HyperEditor, which utilizes weight factors generated by hypernetworks to
reassign the weights of the pre-trained StyleGAN2&apos;s generator. Guided by CLIP&apos;s
cross-modal image-text semantic alignment, this innovative approach enables us
to simultaneously accomplish authentic attribute editing and cross-domain style
transfer, a capability not realized in previous methods. Additionally, we
ascertain that modifying only the weights of specific layers in the generator
can yield an equivalent editing result. Therefore, we introduce an adaptive
layer selector, enabling our hypernetworks to autonomously identify the layers
requiring output weight factors, which can further improve our hypernetworks&apos;
efficiency. Extensive experiments on abundant challenging datasets demonstrate
the effectiveness of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chunwei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_G/0/1/0/all/0/1&quot;&gt;Guitao Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hailing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_W/0/1/0/all/0/1&quot;&gt;Wenming Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13555">
<title>CR-SAM: Curvature Regularized Sharpness-Aware Minimization. (arXiv:2312.13555v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.13555</link>
<description rdf:parseType="Literal">&lt;p&gt;The capacity to generalize to future unseen data stands as one of the utmost
crucial attributes of deep neural networks. Sharpness-Aware Minimization (SAM)
aims to enhance the generalizability by minimizing worst-case loss using
one-step gradient ascent as an approximation. However, as training progresses,
the non-linearity of the loss landscape increases, rendering one-step gradient
ascent less effective. On the other hand, multi-step gradient ascent will incur
higher training cost. In this paper, we introduce a normalized Hessian trace to
accurately measure the curvature of loss landscape on {\em both} training and
test sets. In particular, to counter excessive non-linearity of loss landscape,
we propose Curvature Regularized SAM (CR-SAM), integrating the normalized
Hessian trace as a SAM regularizer. Additionally, we present an efficient way
to compute the trace via finite differences with parallelism. Our theoretical
analysis based on PAC-Bayes bounds establishes the regularizer&apos;s efficacy in
reducing generalization error. Empirical evaluation on CIFAR and ImageNet
datasets shows that CR-SAM consistently enhances classification performance for
ResNet and Vision Transformer (ViT) models across various datasets. Our code is
available at https://github.com/TrustAIoT/CR-SAM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1&quot;&gt;Tie Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wunsch_D/0/1/0/all/0/1&quot;&gt;Donald C. Wunsch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13558">
<title>The Truth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction. (arXiv:2312.13558v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.13558</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer-based Large Language Models (LLMs) have become a fixture in
modern machine learning. Correspondingly, significant resources are allocated
towards research that aims to further advance this technology, typically
resulting in models of increasing size that are trained on increasing amounts
of data. This work, however, demonstrates the surprising result that it is
often possible to significantly improve the performance of LLMs by selectively
removing higher-order components of their weight matrices. This simple
intervention, which we call LAyer-SElective Rank reduction (LASER), can be done
on a model after training has completed, and requires no additional parameters
or data. We show extensive experiments demonstrating the generality of this
finding across language models and datasets, and provide in-depth analyses
offering insights into both when LASER is effective and the mechanism by which
it operates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1&quot;&gt;Pratyusha Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ash_J/0/1/0/all/0/1&quot;&gt;Jordan T. Ash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1&quot;&gt;Dipendra Misra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13575">
<title>ARBiBench: Benchmarking Adversarial Robustness of Binarized Neural Networks. (arXiv:2312.13575v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13575</link>
<description rdf:parseType="Literal">&lt;p&gt;Network binarization exhibits great potential for deployment on
resource-constrained devices due to its low computational cost. Despite the
critical importance, the security of binarized neural networks (BNNs) is rarely
investigated. In this paper, we present ARBiBench, a comprehensive benchmark to
evaluate the robustness of BNNs against adversarial perturbations on CIFAR-10
and ImageNet. We first evaluate the robustness of seven influential BNNs on
various white-box and black-box attacks. The results reveal that 1) The
adversarial robustness of BNNs exhibits a completely opposite performance on
the two datasets under white-box attacks. 2) BNNs consistently exhibit better
adversarial robustness under black-box attacks. 3) Different BNNs exhibit
certain similarities in their robustness performance. Then, we conduct
experiments to analyze the adversarial robustness of BNNs based on these
insights. Our research contributes to inspiring future research on enhancing
the robustness of BNNs and advancing their application in real-world scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1&quot;&gt;Peng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiehua Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1&quot;&gt;Bowen Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Longguang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;YingMei Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Li Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13578">
<title>DREAM-Talk: Diffusion-based Realistic Emotional Audio-driven Method for Single Image Talking Face Generation. (arXiv:2312.13578v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13578</link>
<description rdf:parseType="Literal">&lt;p&gt;The generation of emotional talking faces from a single portrait image
remains a significant challenge. The simultaneous achievement of expressive
emotional talking and accurate lip-sync is particularly difficult, as
expressiveness is often compromised for the accuracy of lip-sync. As widely
adopted by many prior works, the LSTM network often fails to capture the
subtleties and variations of emotional expressions. To address these
challenges, we introduce DREAM-Talk, a two-stage diffusion-based audio-driven
framework, tailored for generating diverse expressions and accurate lip-sync
concurrently. In the first stage, we propose EmoDiff, a novel diffusion module
that generates diverse highly dynamic emotional expressions and head poses in
accordance with the audio and the referenced emotion style. Given the strong
correlation between lip motion and audio, we then refine the dynamics with
enhanced lip-sync accuracy using audio features and emotion style. To this end,
we deploy a video-to-video rendering module to transfer the expressions and lip
motions from our proxy 3D avatar to an arbitrary portrait. Both quantitatively
and qualitatively, DREAM-Talk outperforms state-of-the-art methods in terms of
expressiveness, lip-sync accuracy and perceptual quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chenxu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianfeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hongyi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1&quot;&gt;Guoxian Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;You Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1&quot;&gt;Linjie Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yapeng Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xiaohu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jiashi Feng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13594">
<title>Towards More Faithful Natural Language Explanation Using Multi-Level Contrastive Learning in VQA. (arXiv:2312.13594v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.13594</link>
<description rdf:parseType="Literal">&lt;p&gt;Natural language explanation in visual question answer (VQA-NLE) aims to
explain the decision-making process of models by generating natural language
sentences to increase users&apos; trust in the black-box systems. Existing post-hoc
methods have achieved significant progress in obtaining a plausible
explanation. However, such post-hoc explanations are not always aligned with
human logical inference, suffering from the issues on: 1) Deductive
unsatisfiability, the generated explanations do not logically lead to the
answer; 2) Factual inconsistency, the model falsifies its counterfactual
explanation for answers without considering the facts in images; and 3)
Semantic perturbation insensitivity, the model can not recognize the semantic
changes caused by small perturbations. These problems reduce the faithfulness
of explanations generated by models. To address the above issues, we propose a
novel self-supervised \textbf{M}ulti-level \textbf{C}ontrastive
\textbf{L}earning based natural language \textbf{E}xplanation model (MCLE) for
VQA with semantic-level, image-level, and instance-level factual and
counterfactual samples. MCLE extracts discriminative features and aligns the
feature spaces from explanations with visual question and answer to generate
more consistent explanations. We conduct extensive experiments, ablation
analysis, and case study to demonstrate the effectiveness of our method on two
VQA-NLE benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1&quot;&gt;Chengen Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Shengli Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_S/0/1/0/all/0/1&quot;&gt;Shiqi Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jingyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1&quot;&gt;Sitong Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1&quot;&gt;Guangneng Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13604">
<title>Ponymation: Learning 3D Animal Motions from Unlabeled Online Videos. (arXiv:2312.13604v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13604</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Ponymation, a new method for learning a generative model of
articulated 3D animal motions from raw, unlabeled online videos. Unlike
existing approaches for motion synthesis, our model does not require any pose
annotations or parametric shape models for training, and is learned purely from
a collection of raw video clips obtained from the Internet. We build upon a
recent work, MagicPony, which learns articulated 3D animal shapes purely from
single image collections, and extend it on two fronts. First, instead of
training on static images, we augment the framework with a video training
pipeline that incorporates temporal regularizations, achieving more accurate
and temporally consistent reconstructions. Second, we learn a generative model
of the underlying articulated 3D motion sequences via a spatio-temporal
transformer VAE, simply using 2D reconstruction losses without relying on any
explicit pose annotations. At inference time, given a single 2D image of a new
animal instance, our model reconstructs an articulated, textured 3D mesh, and
generates plausible 3D animations by sampling from the learned motion latent
space.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1&quot;&gt;Keqiang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Litvak_D/0/1/0/all/0/1&quot;&gt;Dor Litvak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunzhi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongsheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiajun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Shangzhe Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13620">
<title>A Comprehensive End-to-End Computer Vision Framework for Restoration and Recognition of Low-Quality Engineering Drawings. (arXiv:2312.13620v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13620</link>
<description rdf:parseType="Literal">&lt;p&gt;The digitization of engineering drawings is crucial for efficient reuse,
distribution, and archiving. Existing computer vision approaches for digitizing
engineering drawings typically assume the input drawings have high quality.
However, in reality, engineering drawings are often blurred and distorted due
to improper scanning, storage, and transmission, which may jeopardize the
effectiveness of existing approaches. This paper focuses on restoring and
recognizing low-quality engineering drawings, where an end-to-end framework is
proposed to improve the quality of the drawings and identify the graphical
symbols on them. The framework uses K-means clustering to classify different
engineering drawing patches into simple and complex texture patches based on
their gray level co-occurrence matrix statistics. Computer vision operations
and a modified Enhanced Super-Resolution Generative Adversarial Network
(ESRGAN) model are then used to improve the quality of the two types of
patches, respectively. A modified Faster Region-based Convolutional Neural
Network (Faster R-CNN) model is used to recognize the quality-enhanced
graphical symbols. Additionally, a multi-stage task-driven collaborative
learning strategy is proposed to train the modified ESRGAN and Faster R-CNN
models to improve the resolution of engineering drawings in the direction that
facilitates graphical symbol recognition, rather than human visual perception.
A synthetic data generation method is also proposed to construct
quality-degraded samples for training the framework. Experiments on real-world
electrical diagrams show that the proposed framework achieves an accuracy of
98.98% and a recall of 99.33%, demonstrating its superiority over previous
approaches. Moreover, the framework is integrated into a widely-used power
system software application to showcase its practicality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lvyang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiankang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huaiqiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1&quot;&gt;Longfei Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1&quot;&gt;Dongyuan Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13630">
<title>MFABA: A More Faithful and Accelerated Boundary-based Attribution Method for Deep Neural Networks. (arXiv:2312.13630v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13630</link>
<description rdf:parseType="Literal">&lt;p&gt;To better understand the output of deep neural networks (DNN), attribution
based methods have been an important approach for model interpretability, which
assign a score for each input dimension to indicate its importance towards the
model outcome. Notably, the attribution methods use the axioms of sensitivity
and implementation invariance to ensure the validity and reliability of
attribution results. Yet, the existing attribution methods present challenges
for effective interpretation and efficient computation. In this work, we
introduce MFABA, an attribution algorithm that adheres to axioms, as a novel
method for interpreting DNN. Additionally, we provide the theoretical proof and
in-depth analysis for MFABA algorithm, and conduct a large scale experiment.
The results demonstrate its superiority by achieving over 101.5142 times faster
speed than the state-of-the-art attribution algorithms. The effectiveness of
MFABA is thoroughly evaluated through the statistical analysis in comparison to
other methods, and the full implementation package is open-source at:
https://github.com/LMBTough/MFABA
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhiyu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huaming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiayu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1&quot;&gt;Zhibo Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1&quot;&gt;Minhui Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1&quot;&gt;Dongxiao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choo_K/0/1/0/all/0/1&quot;&gt;Kim-Kwang Raymond Choo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13631">
<title>Diff-Oracle: Diffusion Model for Oracle Character Generation with Controllable Styles and Contents. (arXiv:2312.13631v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13631</link>
<description rdf:parseType="Literal">&lt;p&gt;Deciphering the oracle bone script plays a significant role in Chinese
archaeology and philology. However, it is significantly challenging due to the
scarcity of oracle character images. To overcome this issue, we propose
Diff-Oracle, based on diffusion models (DMs), to generate sufficient
controllable oracle characters. In contrast to most DMs that rely on text
prompts, we incorporate a style encoder to control style information during the
generation process. This encoder extracts style prompts from existing oracle
character images, where style details are converted from a CLIP model into a
text embedding format. Inspired by ControlNet, we introduce a content encoder
to capture desired content information from content images, ensuring the
fidelity of character glyphs. To train Diff-Oracle effectively, we propose to
obtain pixel-level paired oracle character images (i.e., style and content
images) by a pre-trained image-to-image translation model. Extensive
qualitative and quantitative experiments conducted on two benchmark datasets,
Oracle-241 and OBC306, demonstrate that our Diff-Oracle outperforms existing
generative methods in terms of image generation, further enhancing recognition
accuracy. Source codes will be available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qiu-Feng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kaizhu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Siyuan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13632">
<title>ProvFL: Client-Driven Interpretability of Global Model Predictions in Federated Learning. (arXiv:2312.13632v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.13632</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) trains a collaborative machine learning model by
aggregating multiple privately trained clients&apos; models over several training
rounds. Such a long, continuous action of model aggregations poses significant
challenges in reasoning about the origin and composition of such a global
model. Regardless of the quality of the global model or if it has a fault,
understanding the model&apos;s origin is equally important for debugging,
interpretability, and explainability in federated learning. FL application
developers often question: (1) what clients contributed towards a global model
and (2) if a global model predicts a label, which clients are responsible for
it?
&lt;/p&gt;
&lt;p&gt;We introduce, neuron provenance, a fine-grained lineage capturing mechanism
that tracks the flow of information between the individual participating
clients in FL and the final global model. We operationalize this concept in
ProvFL that functions on two key principles. First, recognizing that monitoring
every neuron of every client&apos;s model statically is ineffective and noisy due to
the uninterpretable nature of individual neurons, ProvFL dynamically isolates
influential and sensitive neurons in the global model, significantly reducing
the search space. Second, as multiple clients&apos; models are fused in each round
to form a global model, tracking each client&apos;s contribution becomes
challenging. ProvFL leverages the invertible nature of fusion algorithms to
precisely isolate each client&apos;s contribution derived from selected neurons.
When asked to localize the clients responsible for the given behavior (i.e.,
prediction) of the global model, ProvFL successfully localizes them with an
average provenance accuracy of 97%. Additionally, ProvFL outperforms the
state-of-the-art FL fault localization approach by an average margin of 50%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gill_W/0/1/0/all/0/1&quot;&gt;Waris Gill&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anwar_A/0/1/0/all/0/1&quot;&gt;Ali Anwar&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gulzar_M/0/1/0/all/0/1&quot;&gt;Muhammad Ali Gulzar&lt;/a&gt; (1) ((1) Virginia Tech, (2) University of Minnesota Twin Cities)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13633">
<title>Multi-Modal Domain Adaptation Across Video Scenes for Temporal Video Grounding. (arXiv:2312.13633v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13633</link>
<description rdf:parseType="Literal">&lt;p&gt;Temporal Video Grounding (TVG) aims to localize the temporal boundary of a
specific segment in an untrimmed video based on a given language query. Since
datasets in this domain are often gathered from limited video scenes, models
tend to overfit to scene-specific factors, which leads to suboptimal
performance when encountering new scenes in real-world applications. In a new
scene, the fine-grained annotations are often insufficient due to the expensive
labor cost, while the coarse-grained video-query pairs are easier to obtain.
Thus, to address this issue and enhance model performance on new scenes, we
explore the TVG task in an unsupervised domain adaptation (UDA) setting across
scenes for the first time, where the video-query pairs in the source scene
(domain) are labeled with temporal boundaries, while those in the target scene
are not. Under the UDA setting, we introduce a novel Adversarial Multi-modal
Domain Adaptation (AMDA) method to adaptively adjust the model&apos;s scene-related
knowledge by incorporating insights from the target data. Specifically, we
tackle the domain gap by utilizing domain discriminators, which help identify
valuable scene-related features effective across both domains. Concurrently, we
mitigate the semantic gap between different modalities by aligning video-query
pairs with related semantics. Furthermore, we employ a mask-reconstruction
approach to enhance the understanding of temporal semantics within a scene.
Extensive experiments on Charades-STA, ActivityNet Captions, and YouCook2
demonstrate the effectiveness of our proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Haifeng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zehan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yan Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhou Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13641">
<title>SPGroup3D: Superpoint Grouping Network for Indoor 3D Object Detection. (arXiv:2312.13641v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13641</link>
<description rdf:parseType="Literal">&lt;p&gt;Current 3D object detection methods for indoor scenes mainly follow the
voting-and-grouping strategy to generate proposals. However, most methods
utilize instance-agnostic groupings, such as ball query, leading to
inconsistent semantic information and inaccurate regression of the proposals.
To this end, we propose a novel superpoint grouping network for indoor
anchor-free one-stage 3D object detection. Specifically, we first adopt an
unsupervised manner to partition raw point clouds into superpoints, areas with
semantic consistency and spatial similarity. Then, we design a geometry-aware
voting module that adapts to the centerness in anchor-free detection by
constraining the spatial relationship between superpoints and object centers.
Next, we present a superpoint-based grouping module to explore the consistent
representation within proposals. This module includes a superpoint attention
layer to learn feature interaction between neighboring superpoints, and a
superpoint-voxel fusion layer to propagate the superpoint-level information to
the voxel level. Finally, we employ effective multiple matching to capitalize
on the dynamic receptive fields of proposals based on superpoints during the
training. Experimental results demonstrate our method achieves state-of-the-art
performance on ScanNet V2, SUN RGB-D, and S3DIS datasets in the indoor
one-stage 3D object detection. Source code is available at
https://github.com/zyrant/SPGroup3D.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hui_L/0/1/0/all/0/1&quot;&gt;Le Hui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yaqi Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1&quot;&gt;Jin Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13646">
<title>Weakly Supervised Semantic Segmentation for Driving Scenes. (arXiv:2312.13646v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13646</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art techniques in weakly-supervised semantic segmentation (WSSS)
using image-level labels exhibit severe performance degradation on driving
scene datasets such as Cityscapes. To address this challenge, we develop a new
WSSS framework tailored to driving scene datasets. Based on extensive analysis
of dataset characteristics, we employ Contrastive Language-Image Pre-training
(CLIP) as our baseline to obtain pseudo-masks. However, CLIP introduces two key
challenges: (1) pseudo-masks from CLIP lack in representing small object
classes, and (2) these masks contain notable noise. We propose solutions for
each issue as follows. (1) We devise Global-Local View Training that seamlessly
incorporates small-scale patches during model training, thereby enhancing the
model&apos;s capability to handle small-sized yet critical objects in driving scenes
(e.g., traffic light). (2) We introduce Consistency-Aware Region Balancing
(CARB), a novel technique that discerns reliable and noisy regions through
evaluating the consistency between CLIP masks and segmentation predictions. It
prioritizes reliable pixels over noisy pixels via adaptive loss weighting.
Notably, the proposed method achieves 51.8\% mIoU on the Cityscapes test
dataset, showcasing its potential as a strong WSSS baseline on driving scene
datasets. Experimental results on CamVid and WildDash2 demonstrate the
effectiveness of our method across diverse datasets, even with small-scale
datasets or visually challenging conditions. The code is available at
https://github.com/k0u-id/CARB.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Dongseob Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seungho Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choe_J/0/1/0/all/0/1&quot;&gt;Junsuk Choe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shim_H/0/1/0/all/0/1&quot;&gt;Hyunjung Shim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13655">
<title>Compositional Zero-Shot Learning for Attribute-Based Object Reference in Human-Robot Interaction. (arXiv:2312.13655v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.13655</link>
<description rdf:parseType="Literal">&lt;p&gt;Language-enabled robots have been widely studied over the past years to
enable natural human-robot interaction and teaming in various real-world
applications. Language-enabled robots must be able to comprehend referring
expressions to identify a particular object from visual perception using a set
of referring attributes extracted from natural language. However, visual
observations of an object may not be available when it is referred to, and the
number of objects and attributes may also be unbounded in open worlds. To
address the challenges, we implement an attribute-based compositional zero-shot
learning method that uses a list of attributes to perform referring expression
comprehension in open worlds. We evaluate the approach on two datasets
including the MIT-States and the Clothing 16K. The preliminary experimental
results show that our implemented approach allows a robot to correctly identify
the objects referred to by human commands.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1&quot;&gt;Peng Gao&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaafar_A/0/1/0/all/0/1&quot;&gt;Ahmed Jaafar&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reily_B/0/1/0/all/0/1&quot;&gt;Brian Reily&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reardon_C/0/1/0/all/0/1&quot;&gt;Christopher Reardon&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt; (1) ((1) University of Massachusetts Amherst, (2) DEVCOM Army Research Laboratory, (3) University of Denver)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13663">
<title>Free-Editor: Zero-shot Text-driven 3D Scene Editing. (arXiv:2312.13663v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13663</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-Image (T2I) diffusion models have gained popularity recently due to
their multipurpose and easy-to-use nature, e.g. image and video generation as
well as editing. However, training a diffusion model specifically for 3D scene
editing is not straightforward due to the lack of large-scale datasets. To
date, editing 3D scenes requires either re-training the model to adapt to
various 3D edited scenes or design-specific methods for each special editing
type. Furthermore, state-of-the-art (SOTA) methods require multiple
synchronized edited images from the same scene to facilitate the scene editing.
Due to the current limitations of T2I models, it is very challenging to apply
consistent editing effects to multiple images, i.e. multi-view inconsistency in
editing. This in turn compromises the desired 3D scene editing performance if
these images are used. In our work, we propose a novel training-free 3D scene
editing technique, Free-Editor, which allows users to edit 3D scenes without
further re-training the model during test time. Our proposed method
successfully avoids the multi-view style inconsistency issue in SOTA methods
with the help of a &quot;single-view editing&quot; scheme. Specifically, we show that
editing a particular 3D scene can be performed by only modifying a single view.
To this end, we introduce an Edit Transformer that enforces intra-view
consistency and inter-view style transfer by utilizing self- and
cross-attention, respectively. Since it is no longer required to re-train the
model and edit every view in a scene, the editing time, as well as memory
resources, are reduced significantly, e.g., the runtime being $\sim \textbf{20}
\times$ faster than SOTA. We have conducted extensive experiments on a wide
range of benchmark datasets and achieve diverse editing capabilities with our
proposed technique.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karim_N/0/1/0/all/0/1&quot;&gt;Nazmul Karim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khalid_U/0/1/0/all/0/1&quot;&gt;Umar Khalid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iqbal_H/0/1/0/all/0/1&quot;&gt;Hasan Iqbal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_J/0/1/0/all/0/1&quot;&gt;Jing Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13691">
<title>DreamTuner: Single Image is Enough for Subject-Driven Generation. (arXiv:2312.13691v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13691</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion-based models have demonstrated impressive capabilities for
text-to-image generation and are expected for personalized applications of
subject-driven generation, which require the generation of customized concepts
with one or a few reference images. However, existing methods based on
fine-tuning fail to balance the trade-off between subject learning and the
maintenance of the generation capabilities of pretrained models. Moreover,
other methods that utilize additional image encoders tend to lose important
details of the subject due to encoding compression. To address these
challenges, we propose DreamTurner, a novel method that injects reference
information from coarse to fine to achieve subject-driven image generation more
effectively. DreamTurner introduces a subject-encoder for coarse subject
identity preservation, where the compressed general subject features are
introduced through an attention layer before visual-text cross-attention. We
then modify the self-attention layers within pretrained text-to-image models to
self-subject-attention layers to refine the details of the target subject. The
generated image queries detailed features from both the reference image and
itself in self-subject-attention. It is worth emphasizing that
self-subject-attention is an effective, elegant, and training-free method for
maintaining the detailed features of customized subjects and can serve as a
plug-and-play solution during inference. Finally, with additional
subject-driven fine-tuning, DreamTurner achieves remarkable performance in
subject-driven image generation, which can be controlled by a text or other
conditions such as pose. For further details, please visit the project page at
https://dreamtuner-diffusion.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_M/0/1/0/all/0/1&quot;&gt;Miao Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiawei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1&quot;&gt;Fei Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jie Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1&quot;&gt;Qian He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13714">
<title>Bootstrap Masked Visual Modeling via Hard Patches Mining. (arXiv:2312.13714v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13714</link>
<description rdf:parseType="Literal">&lt;p&gt;Masked visual modeling has attracted much attention due to its promising
potential in learning generalizable representations. Typical approaches urge
models to predict specific contents of masked tokens, which can be intuitively
considered as teaching a student (the model) to solve given problems
(predicting masked contents). Under such settings, the performance is highly
correlated with mask strategies (the difficulty of provided problems). We argue
that it is equally important for the model to stand in the shoes of a teacher
to produce challenging problems by itself. Intuitively, patches with high
values of reconstruction loss can be regarded as hard samples, and masking
those hard patches naturally becomes a demanding reconstruction task. To
empower the model as a teacher, we propose Hard Patches Mining (HPM),
predicting patch-wise losses and subsequently determining where to mask.
Technically, we introduce an auxiliary loss predictor, which is trained with a
relative objective to prevent overfitting to exact loss values. Also, to
gradually guide the training procedure, we propose an easy-to-hard mask
strategy. Empirically, HPM brings significant improvements under both image and
video benchmarks. Interestingly, solely incorporating the extra loss prediction
objective leads to better representations, verifying the efficacy of
determining where is hard to reconstruct. The code is available at
https://github.com/Haochen-Wang409/HPM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haochen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1&quot;&gt;Junsong Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuxi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1&quot;&gt;Kaiyou Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tiancai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiangyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhaoxiang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13729">
<title>Gaussian Splitting Algorithm with Color and Opacity Depended on Viewing Direction. (arXiv:2312.13729v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13729</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Fields (NeRFs) have demonstrated the remarkable potential of
neural networks to capture the intricacies of 3D objects. By encoding the shape
and color information within neural network weights, NeRFs excel at producing
strikingly sharp novel views of 3D objects. Recently, numerous generalizations
of NeRFs utilizing generative models have emerged, expanding its versatility.
In contrast, Gaussian Splatting (GS) offers a similar renders quality with
faster training and inference as it does not need neural networks to work. We
encode information about the 3D objects in the set of Gaussian distributions
that can be rendered in 3D similarly to classical meshes. Unfortunately, GS are
difficult to condition since they usually require circa hundred thousand
Gaussian components. To mitigate the caveats of both models, we propose a
hybrid model that uses GS representation of the 3D object&apos;s shape and
NeRF-based encoding of color and opacity. Our model uses Gaussian distributions
with trainable positions (i.e. means of Gaussian), shape (i.e. covariance of
Gaussian), color and opacity, and neural network, which takes parameters of
Gaussian and viewing direction to produce changes in color and opacity.
Consequently, our model better describes shadows, light reflections, and
transparency of 3D objects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malarz_D/0/1/0/all/0/1&quot;&gt;Dawid Malarz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smolak_W/0/1/0/all/0/1&quot;&gt;Weronika Smolak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tabor_J/0/1/0/all/0/1&quot;&gt;Jacek Tabor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tadeja_S/0/1/0/all/0/1&quot;&gt;S&amp;#x142;awomir Tadeja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spurek_P/0/1/0/all/0/1&quot;&gt;Przemys&amp;#x142;aw Spurek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13735">
<title>DECO: Query-Based End-to-End Object Detection with ConvNets. (arXiv:2312.13735v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13735</link>
<description rdf:parseType="Literal">&lt;p&gt;Detection Transformer (DETR) and its variants have shown great potential for
accurate object detection in recent years. The mechanism of object query
enables DETR family to directly obtain a fixed number of object predictions and
streamlines the detection pipeline. Meanwhile, recent studies also reveal that
with proper architecture design, convolution networks (ConvNets) also achieve
competitive performance with transformers, \eg, ConvNeXt. To this end, in this
paper we explore whether we could build a query-based end-to-end object
detection framework with ConvNets instead of sophisticated transformer
architecture. The proposed framework, \ie, Detection ConvNet (DECO), is
composed of a backbone and convolutional encoder-decoder architecture. We
carefully design the DECO encoder and propose a novel mechanism for our DECO
decoder to perform interaction between object queries and image features via
convolutional layers. We compare the proposed DECO against prior detectors on
the challenging COCO benchmark. Despite its simplicity, our DECO achieves
competitive performance in terms of detection accuracy and running speed.
Specifically, with the ResNet-50 and ConvNeXt-Tiny backbone, DECO obtains
$38.6\%$ and $40.8\%$ AP on COCO \textit{val} set with $35$ and $28$ FPS
respectively and outperforms the DETR model. Incorporated with advanced
multi-scale feature module, our DECO+ achieves $47.8\%$ AP with $34$ FPS. We
hope the proposed DECO brings another perspective for designing object
detection framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinghao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Siwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yijing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunhe Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13746">
<title>Video Recognition in Portrait Mode. (arXiv:2312.13746v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13746</link>
<description rdf:parseType="Literal">&lt;p&gt;The creation of new datasets often presents new challenges for video
recognition and can inspire novel ideas while addressing these challenges.
While existing datasets mainly comprise landscape mode videos, our paper seeks
to introduce portrait mode videos to the research community and highlight the
unique challenges associated with this video format. With the growing
popularity of smartphones and social media applications, recognizing portrait
mode videos is becoming increasingly important. To this end, we have developed
the first dataset dedicated to portrait mode video recognition, namely
PortraitMode-400. The taxonomy of PortraitMode-400 was constructed in a
data-driven manner, comprising 400 fine-grained categories, and rigorous
quality assurance was implemented to ensure the accuracy of human annotations.
In addition to the new dataset, we conducted a comprehensive analysis of the
impact of video format (portrait mode versus landscape mode) on recognition
accuracy and spatial bias due to the different formats. Furthermore, we
designed extensive experiments to explore key aspects of portrait mode video
recognition, including the choice of data augmentation, evaluation procedure,
the importance of temporal information, and the role of audio modality.
Building on the insights from our experimental results and the introduction of
PortraitMode-400, our paper aims to inspire further research efforts in this
emerging research area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1&quot;&gt;Mingfei Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Linjie Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1&quot;&gt;Xiaojie Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jiashi Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1&quot;&gt;Xiaojun Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Heng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13752">
<title>Hunting imaging biomarkers in pulmonary fibrosis: Benchmarks of the AIIB23 challenge. (arXiv:2312.13752v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.13752</link>
<description rdf:parseType="Literal">&lt;p&gt;Airway-related quantitative imaging biomarkers are crucial for examination,
diagnosis, and prognosis in pulmonary diseases. However, the manual delineation
of airway trees remains prohibitively time-consuming. While significant efforts
have been made towards enhancing airway modelling, current public-available
datasets concentrate on lung diseases with moderate morphological variations.
The intricate honeycombing patterns present in the lung tissues of fibrotic
lung disease patients exacerbate the challenges, often leading to various
prediction errors. To address this issue, the &apos;Airway-Informed Quantitative CT
Imaging Biomarker for Fibrotic Lung Disease 2023&apos; (AIIB23) competition was
organized in conjunction with the official 2023 International Conference on
Medical Image Computing and Computer Assisted Intervention (MICCAI). The airway
structures were meticulously annotated by three experienced radiologists.
Competitors were encouraged to develop automatic airway segmentation models
with high robustness and generalization abilities, followed by exploring the
most correlated QIB of mortality prediction. A training set of 120
high-resolution computerised tomography (HRCT) scans were publicly released
with expert annotations and mortality status. The online validation set
incorporated 52 HRCT scans from patients with fibrotic lung disease and the
offline test set included 140 cases from fibrosis and COVID-19 patients. The
results have shown that the capacity of extracting airway trees from patients
with fibrotic lung disease could be enhanced by introducing voxel-wise weighted
general union loss and continuity loss. In addition to the competitive image
biomarkers for prognosis, a strong airway-derived biomarker (Hazard ratio&amp;gt;1.5,
p&amp;lt;0.0001) was revealed for survival prognostication compared with existing
clinical measurements, clinician assessment and AI-based biomarkers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nan_Y/0/1/0/all/0/1&quot;&gt;Yang Nan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xing_X/0/1/0/all/0/1&quot;&gt;Xiaodan Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shiyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tang_Z/0/1/0/all/0/1&quot;&gt;Zeyu Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Felder_F/0/1/0/all/0/1&quot;&gt;Federico N Felder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Sheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ledda_R/0/1/0/all/0/1&quot;&gt;Roberta Eufrasia Ledda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ding_X/0/1/0/all/0/1&quot;&gt;Xiaoliu Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yu_R/0/1/0/all/0/1&quot;&gt;Ruiqi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Weiping Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shi_F/0/1/0/all/0/1&quot;&gt;Feng Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sun_T/0/1/0/all/0/1&quot;&gt;Tianyang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cao_Z/0/1/0/all/0/1&quot;&gt;Zehong Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Minghui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yun Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hanxiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jian Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tang_W/0/1/0/all/0/1&quot;&gt;Wen Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yu_P/0/1/0/all/0/1&quot;&gt;Pengxin Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kang_H/0/1/0/all/0/1&quot;&gt;Han Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junqiang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xing Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Boyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mamalakis_M/0/1/0/all/0/1&quot;&gt;Michail Mamalakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Prinzi_F/0/1/0/all/0/1&quot;&gt;Francesco Prinzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Carlini_G/0/1/0/all/0/1&quot;&gt;Gianluca Carlini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cuneo_L/0/1/0/all/0/1&quot;&gt;Lisa Cuneo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Banerjee_A/0/1/0/all/0/1&quot;&gt;Abhirup Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xing_Z/0/1/0/all/0/1&quot;&gt;Zhaohu Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Lei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mesbah_Z/0/1/0/all/0/1&quot;&gt;Zacharia Mesbah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jain_D/0/1/0/all/0/1&quot;&gt;Dhruv Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mayet_T/0/1/0/all/0/1&quot;&gt;Tsiry Mayet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yuan_H/0/1/0/all/0/1&quot;&gt;Hongyu Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lyu_Q/0/1/0/all/0/1&quot;&gt;Qing Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wells_A/0/1/0/all/0/1&quot;&gt;Athol Wells&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Walsh_S/0/1/0/all/0/1&quot;&gt;Simon LF Walsh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guang Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13763">
<title>Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed Diffusion Models. (arXiv:2312.13763v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13763</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-guided diffusion models have revolutionized image and video generation
and have also been successfully used for optimization-based 3D object
synthesis. Here, we instead focus on the underexplored text-to-4D setting and
synthesize dynamic, animated 3D objects using score distillation methods with
an additional temporal dimension. Compared to previous work, we pursue a novel
compositional generation-based approach, and combine text-to-image,
text-to-video, and 3D-aware multiview diffusion models to provide feedback
during 4D object optimization, thereby simultaneously enforcing temporal
consistency, high-quality visual appearance and realistic geometry. Our method,
called Align Your Gaussians (AYG), leverages dynamic 3D Gaussian Splatting with
deformation fields as 4D representation. Crucial to AYG is a novel method to
regularize the distribution of the moving 3D Gaussians and thereby stabilize
the optimization and induce motion. We also propose a motion amplification
mechanism as well as a new autoregressive synthesis scheme to generate and
combine multiple 4D sequences for longer generation. These techniques allow us
to synthesize vivid dynamic scenes, outperform previous work qualitatively and
quantitatively and achieve state-of-the-art text-to-4D performance. Due to the
Gaussian 4D representation, different 4D animations can be seamlessly combined,
as we demonstrate. AYG opens up promising avenues for animation, simulation and
digital content creation as well as synthetic data generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1&quot;&gt;Huan Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seung Wook Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1&quot;&gt;Antonio Torralba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1&quot;&gt;Sanja Fidler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kreis_K/0/1/0/all/0/1&quot;&gt;Karsten Kreis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13764">
<title>A Semantic Space is Worth 256 Language Descriptions: Make Stronger Segmentation Models with Descriptive Properties. (arXiv:2312.13764v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13764</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces ProLab, a novel approach using property-level label
space for creating strong interpretable segmentation models. Instead of relying
solely on category-specific annotations, ProLab uses descriptive properties
grounded in common sense knowledge for supervising segmentation models. It is
based on two core designs. First, we employ Large Language Models (LLMs) and
carefully crafted prompts to generate descriptions of all involved categories
that carry meaningful common sense knowledge and follow a structured format.
Second, we introduce a description embedding model preserving semantic
correlation across descriptions and then cluster them into a set of descriptive
properties (e.g., 256) using K-Means. These properties are based on
interpretable common sense knowledge consistent with theories of human
recognition. We empirically show that our approach makes segmentation models
perform stronger on five classic benchmarks (e.g., ADE20K, COCO-Stuff, Pascal
Context, Cityscapes, and BDD). Our method also shows better scalability with
extended training steps than category-level supervision. Our interpretable
segmentation framework also emerges with the generalization ability to segment
out-of-domain or unknown categories using only in-domain descriptive
properties. Code is available at https://github.com/lambert-x/ProLab.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1&quot;&gt;Junfei Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Ziqi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenxuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_S/0/1/0/all/0/1&quot;&gt;Shiyi Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1&quot;&gt;Jieru Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhiding Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1&quot;&gt;Alan Yuille&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuyin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1&quot;&gt;Cihang Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13770">
<title>3D Points Splatting for Real-Time Dynamic Hand Reconstruction. (arXiv:2312.13770v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13770</link>
<description rdf:parseType="Literal">&lt;p&gt;We present 3D Points Splatting Hand Reconstruction (3D-PSHR), a real-time and
photo-realistic hand reconstruction approach. We propose a self-adaptive
canonical points upsampling strategy to achieve high-resolution hand geometry
representation. This is followed by a self-adaptive deformation that deforms
the hand from the canonical space to the target pose, adapting to the dynamic
changing of canonical points which, in contrast to the common practice of
subdividing the MANO model, offers greater flexibility and results in improved
geometry fitting. To model texture, we disentangle the appearance color into
the intrinsic albedo and pose-aware shading, which are learned through a
Context-Attention module. Moreover, our approach allows the geometric and the
appearance models to be trained simultaneously in an end-to-end manner. We
demonstrate that our method is capable of producing animatable, photorealistic
and relightable hand reconstructions using multiple datasets, including
monocular videos captured with handheld smartphones and large-scale multi-view
videos featuring various hand poses. We also demonstrate that our approach
achieves real-time rendering speeds while simultaneously maintaining superior
performance compared to existing state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zheheng Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahmani_H/0/1/0/all/0/1&quot;&gt;Hossein Rahmani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Black_S/0/1/0/all/0/1&quot;&gt;Sue Black&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1&quot;&gt;Bryan M. Williams&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13771">
<title>AppAgent: Multimodal Agents as Smartphone Users. (arXiv:2312.13771v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13771</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in large language models (LLMs) have led to the creation
of intelligent agents capable of performing complex tasks. This paper
introduces a novel LLM-based multimodal agent framework designed to operate
smartphone applications. Our framework enables the agent to operate smartphone
applications through a simplified action space, mimicking human-like
interactions such as tapping and swiping. This novel approach bypasses the need
for system back-end access, thereby broadening its applicability across diverse
apps. Central to our agent&apos;s functionality is its innovative learning method.
The agent learns to navigate and use new apps either through autonomous
exploration or by observing human demonstrations. This process generates a
knowledge base that the agent refers to for executing complex tasks across
different applications. To demonstrate the practicality of our agent, we
conducted extensive testing over 50 tasks in 10 different applications,
including social media, email, maps, shopping, and sophisticated image editing
tools. The results affirm our agent&apos;s proficiency in handling a diverse array
of high-level tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiaxuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yucheng Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zebiao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_B/0/1/0/all/0/1&quot;&gt;Bin Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1&quot;&gt;Gang Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13776">
<title>Pose-based Tremor Type and Level Analysis for Parkinson&apos;s Disease from Video. (arXiv:2312.13776v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13776</link>
<description rdf:parseType="Literal">&lt;p&gt;Purpose:Current methods for diagnosis of PD rely on clinical examination. The
accuracy of diagnosis ranges between 73% and 84%, and is influenced by the
experience of the clinical assessor. Hence, an automatic, effective and
interpretable supporting system for PD symptom identification would support
clinicians in making more robust PD diagnostic decisions. Methods: We propose
to analyze Parkinson&apos;s tremor (PT) to support the analysis of PD, since PT is
one of the most typical symptoms of PD with broad generalizability. To realize
the idea, we present SPA-PTA, a deep learning-based PT classification and
severity estimation system that takes consumer-grade videos of front-facing
humans as input. The core of the system is a novel attention module with a
lightweight pyramidal channel-squeezing-fusion architecture that effectively
extracts relevant PT information and filters noise. It enhances modeling
performance while improving system interpretability. Results:We validate our
system via individual-based leave-one-out cross-validation on two tasks: the PT
classification task and the tremor severity rating estimation task. Our system
presents a 91.3% accuracy and 80.0% F1-score in classifying PT with non-PT
class, while providing a 76.4% accuracy and 76.7% F1-score in more complex
multiclass tremor rating classification task. Conclusion: Our system offers a
cost-effective PT classification and tremor severity estimation results as
warning signs of PD for undiagnosed patients with PT symptoms. In addition, it
provides a potential solution for supporting PD diagnosis in regions with
limited clinical resources.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haozheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_E/0/1/0/all/0/1&quot;&gt;Edmond S. L. Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiatian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Din_S/0/1/0/all/0/1&quot;&gt;Silvia Del Din&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1&quot;&gt;Hubert P. H. Shum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13778">
<title>Progressive Evolution from Single-Point to Polygon for Scene Text. (arXiv:2312.13778v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13778</link>
<description rdf:parseType="Literal">&lt;p&gt;The advancement of text shape representations towards compactness has
enhanced text detection and spotting performance, but at a high annotation
cost. Current models use single-point annotations to reduce costs, yet they
lack sufficient localization information for downstream applications. To
overcome this limitation, we introduce Point2Polygon, which can efficiently
transform single-points into compact polygons. Our method uses a coarse-to-fine
process, starting with creating and selecting anchor points based on
recognition confidence, then vertically and horizontally refining the polygon
using recognition information to optimize its shape. We demonstrate the
accuracy of the generated polygons through extensive experiments: 1) By
creating polygons from ground truth points, we achieved an accuracy of 82.0% on
ICDAR 2015; 2) In training detectors with polygons generated by our method, we
attained 86% of the accuracy relative to training with ground truth (GT); 3)
Additionally, the proposed Point2Polygon can be seamlessly integrated to
empower single-point spotters to generate polygons. This integration led to an
impressive 82.5% accuracy for the generated polygons. It is worth mentioning
that our method relies solely on synthetic recognition information, eliminating
the need for any manual annotation beyond single points.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1&quot;&gt;Linger Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1&quot;&gt;Mingxin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xudong Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1&quot;&gt;Lianwen Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1&quot;&gt;Xiang Bai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13783">
<title>Few Shot Part Segmentation Reveals Compositional Logic for Industrial Anomaly Detection. (arXiv:2312.13783v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13783</link>
<description rdf:parseType="Literal">&lt;p&gt;Logical anomalies (LA) refer to data violating underlying logical constraints
e.g., the quantity, arrangement, or composition of components within an image.
Detecting accurately such anomalies requires models to reason about various
component types through segmentation. However, curation of pixel-level
annotations for semantic segmentation is both time-consuming and expensive.
Although there are some prior few-shot or unsupervised co-part segmentation
algorithms, they often fail on images with industrial object. These images have
components with similar textures and shapes, and a precise differentiation
proves challenging. In this study, we introduce a novel component segmentation
model for LA detection that leverages a few labeled samples and unlabeled
images sharing logical constraints. To ensure consistent segmentation across
unlabeled images, we employ a histogram matching loss in conjunction with an
entropy loss. As segmentation predictions play a crucial role, we propose to
enhance both local and global sample validity detection by capturing key
aspects from visual semantics via three memory banks: class histograms,
component composition embeddings and patch-level representations. For effective
LA detection, we propose an adaptive scaling strategy to standardize anomaly
scores from different memory banks in inference. Extensive experiments on the
public benchmark MVTec LOCO AD reveal our method achieves 98.1% AUROC in LA
detection vs. 89.6% from competing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Soopil Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1&quot;&gt;Sion An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chikontwe_P/0/1/0/all/0/1&quot;&gt;Philip Chikontwe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1&quot;&gt;Myeongkyun Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1&quot;&gt;Ehsan Adeli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1&quot;&gt;Kilian M. Pohl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Sanghyun Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13789">
<title>TinySAM: Pushing the Envelope for Efficient Segment Anything Model. (arXiv:2312.13789v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13789</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently segment anything model (SAM) has shown powerful segmentation
capability and has drawn great attention in computer vision fields. Massive
following works have developed various applications based on the pretrained SAM
and achieved impressive performance on downstream vision tasks. However, SAM
consists of heavy architectures and requires massive computational capacity,
which hinders the further application of SAM on computation constrained edge
devices. To this end, in this paper we propose a framework to obtain a tiny
segment anything model (TinySAM) while maintaining the strong zero-shot
performance. We first propose a full-stage knowledge distillation method with
online hard prompt sampling strategy to distill a lightweight student model. We
also adapt the post-training quantization to the promptable segmentation task
and further reduce the computational cost. Moreover, a hierarchical segmenting
everything strategy is proposed to accelerate the everything inference by
$2\times$ with almost no performance degradation. With all these proposed
methods, our TinySAM leads to orders of magnitude computational reduction and
pushes the envelope for efficient segment anything task. Extensive experiments
on various zero-shot transfer tasks demonstrate the significantly advantageous
performance of our TinySAM against counterpart methods. Pre-trained models and
codes will be available at https://github.com/xinghaochen/TinySAM and
https://gitee.com/mindspore/models/tree/master/research/cv/TinySAM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_H/0/1/0/all/0/1&quot;&gt;Han Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenshuo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yehui Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yiman Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yihao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Houqiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunhe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinghao Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13792">
<title>An Approach to Colour Morphological Supremum Formation using the LogSumExp Approximation. (arXiv:2312.13792v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13792</link>
<description rdf:parseType="Literal">&lt;p&gt;Mathematical morphology is a part of image processing that has proven to be
fruitful for numerous applications. Two main operations in mathematical
morphology are dilation and erosion. These are based on the construction of a
supremum or infimum with respect to an order over the tonal range in a certain
section of the image. The tonal ordering can easily be realised in grey-scale
morphology, and some morphological methods have been proposed for colour
morphology. However, all of these have certain limitations. In this paper we
present a novel approach to colour morphology extending upon previous work in
the field based on the Loewner order. We propose to consider an approximation
of the supremum by means of a log-sum exponentiation introduced by Maslov. We
apply this to the embedding of an RGB image in a field of symmetric $2\times2$
matrices. In this way we obtain nearly isotropic matrices representing colours
and the structural advantage of transitivity. In numerical experiments we
highlight some remarkable properties of the proposed approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kahra_M/0/1/0/all/0/1&quot;&gt;Marvin Kahra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Breuss_M/0/1/0/all/0/1&quot;&gt;Michael Breu&amp;#xdf;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleefeld_A/0/1/0/all/0/1&quot;&gt;Andreas Kleefeld&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Welk_M/0/1/0/all/0/1&quot;&gt;Martin Welk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13820">
<title>Super-resolution of THz time-domain images based on low-rank representation. (arXiv:2312.13820v1 [physics.optics])</title>
<link>http://arxiv.org/abs/2312.13820</link>
<description rdf:parseType="Literal">&lt;p&gt;Terahertz time-domain spectroscopy (THz-TDS) employs sub-picosecond pulses to
probe dielectric properties of materials giving as a result a 3-dimensional
hyperspectral data cube. The spatial resolution of THz images is primarily
limited by two sources: a non-zero THz beam waist and the acquisition step
size. Acquisition with a small step size allows for the visualisation of
smaller details in images at the expense of acquisition time, but the
frequency-dependent point-spread function remains the biggest bottleneck for
THz imaging. This work presents a super-resolution approach to restore THz
time-domain images acquired with medium-to-big step sizes. The results show the
optimized and robust performance for different frequency bands (from 0.5 to 3.5
THz) obtaining higher resolution and additionally removing effects of blur at
lower frequencies and noise at higher frequencies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ljubenovic_M/0/1/0/all/0/1&quot;&gt;Marina Ljubenovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Artesani_A/0/1/0/all/0/1&quot;&gt;Alessia Artesani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bonetti_S/0/1/0/all/0/1&quot;&gt;Stefano Bonetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Traviglia_A/0/1/0/all/0/1&quot;&gt;Arianna Traviglia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13822">
<title>Universal Noise Annotation: Unveiling the Impact of Noisy annotation on Object Detection. (arXiv:2312.13822v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13822</link>
<description rdf:parseType="Literal">&lt;p&gt;For object detection task with noisy labels, it is important to consider not
only categorization noise, as in image classification, but also localization
noise, missing annotations, and bogus bounding boxes. However, previous studies
have only addressed certain types of noise (e.g., localization or
categorization). In this paper, we propose Universal-Noise Annotation (UNA), a
more practical setting that encompasses all types of noise that can occur in
object detection, and analyze how UNA affects the performance of the detector.
We analyzed the development direction of previous works of detection algorithms
and examined the factors that impact the robustness of detection model learning
method. We open-source the code for injecting UNA into the dataset and all the
training log and weight are also shared.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryoo_K/0/1/0/all/0/1&quot;&gt;Kwangrok Ryoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jo_Y/0/1/0/all/0/1&quot;&gt;Yeonsik Jo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seungjun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Mira Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jo_A/0/1/0/all/0/1&quot;&gt;Ahra Jo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seung Hwan Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seungryong Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Soonyoung Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13832">
<title>SyncDreamer for 3D Reconstruction of Endangered Animal Species with NeRF and NeuS. (arXiv:2312.13832v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13832</link>
<description rdf:parseType="Literal">&lt;p&gt;The main aim of this study is to demonstrate how innovative view synthesis
and 3D reconstruction techniques can be used to create models of endangered
species using monocular RGB images. To achieve this, we employed SyncDreamer to
produce unique perspectives and NeuS and NeRF to reconstruct 3D
representations. We chose four different animals, including the oriental stork,
frog, dragonfly, and tiger, as our subjects for this study. Our results show
that the combination of SyncDreamer, NeRF, and NeuS techniques can successfully
create 3D models of endangered animals. However, we also observed that NeuS
produced blurry images, while NeRF generated sharper but noisier images. This
study highlights the potential of modeling endangered animals and offers a new
direction for future research in this field. By showcasing the effectiveness of
these advanced techniques, we hope to encourage further exploration and
development of techniques for preserving and studying endangered species.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ornek_A/0/1/0/all/0/1&quot;&gt;Ahmet Haydar Ornek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sen_D/0/1/0/all/0/1&quot;&gt;Deniz Sen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Civil_E/0/1/0/all/0/1&quot;&gt;Esmanur Civil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13834">
<title>Fairy: Fast Parallelized Instruction-Guided Video-to-Video Synthesis. (arXiv:2312.13834v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13834</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce Fairy, a minimalist yet robust adaptation of
image-editing diffusion models, enhancing them for video editing applications.
Our approach centers on the concept of anchor-based cross-frame attention, a
mechanism that implicitly propagates diffusion features across frames, ensuring
superior temporal coherence and high-fidelity synthesis. Fairy not only
addresses limitations of previous models, including memory and processing
speed. It also improves temporal consistency through a unique data augmentation
strategy. This strategy renders the model equivariant to affine transformations
in both source and target images. Remarkably efficient, Fairy generates
120-frame 512x384 videos (4-second duration at 30 FPS) in just 14 seconds,
outpacing prior works by at least 44x. A comprehensive user study, involving
1000 generated samples, confirms that our approach delivers superior quality,
decisively outperforming established methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Bichen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chuang_C/0/1/0/all/0/1&quot;&gt;Ching-Yao Chuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaoyan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1&quot;&gt;Yichen Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnakumar_K/0/1/0/all/0/1&quot;&gt;Kapil Krishnakumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1&quot;&gt;Tong Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_F/0/1/0/all/0/1&quot;&gt;Feng Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Licheng Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vajda_P/0/1/0/all/0/1&quot;&gt;Peter Vajda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13839">
<title>Q-SENN: Quantized Self-Explaining Neural Networks. (arXiv:2312.13839v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13839</link>
<description rdf:parseType="Literal">&lt;p&gt;Explanations in Computer Vision are often desired, but most Deep Neural
Networks can only provide saliency maps with questionable faithfulness.
Self-Explaining Neural Networks (SENN) extract interpretable concepts with
fidelity, diversity, and grounding to combine them linearly for
decision-making. While they can explain what was recognized, initial
realizations lack accuracy and general applicability. We propose the
Quantized-Self-Explaining Neural Network Q-SENN. Q-SENN satisfies or exceeds
the desiderata of SENN while being applicable to more complex datasets and
maintaining most or all of the accuracy of an uninterpretable baseline model,
out-performing previous work in all considered metrics. Q-SENN describes the
relationship between every class and feature as either positive, negative or
neutral instead of an arbitrary number of possible relations, enforcing more
binary human-friendly features. Since every class is assigned just 5
interpretable features on average, Q-SENN shows convincing local and global
interpretability. Additionally, we propose a feature alignment method, capable
of aligning learned features with human language-based concepts without
additional supervision. Thus, what is learned can be more easily verbalized.
The code is published: https://github.com/ThomasNorr/Q-SENN
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Norrenbrock_T/0/1/0/all/0/1&quot;&gt;Thomas Norrenbrock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rudolph_M/0/1/0/all/0/1&quot;&gt;Marco Rudolph&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1&quot;&gt;Bodo Rosenhahn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13841">
<title>Towards Efficient Time Stepping for Numerical Shape Correspondence. (arXiv:2312.13841v1 [math.NA])</title>
<link>http://arxiv.org/abs/2312.13841</link>
<description rdf:parseType="Literal">&lt;p&gt;The computation of correspondences between shapes is a principal task in
shape analysis. To this end, methods based on partial differential equations
(PDEs) have been established, encompassing e.g. the classic heat kernel
signature as well as numerical solution schemes for geometric PDEs. In this
work we focus on the latter approach.
&lt;/p&gt;
&lt;p&gt;We consider here several time stepping schemes. The goal of this
investigation is to assess, if one may identify a useful property of methods
for time integration for the shape analysis context. Thereby we investigate the
dependence on time step size, since the class of implicit schemes that are
useful candidates in this context should ideally yield an invariant behaviour
with respect to this parameter.
&lt;/p&gt;
&lt;p&gt;To this end we study integration of heat and wave equation on a manifold. In
order to facilitate this study, we propose an efficient, unified model order
reduction framework for these models. We show that specific $l_0$ stable
schemes are favourable for numerical shape analysis. We give an experimental
evaluation of the methods at hand of classical TOSCA data sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kohler_A/0/1/0/all/0/1&quot;&gt;Alexander K&amp;#xf6;hler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Breuss_M/0/1/0/all/0/1&quot;&gt;Michael Breu&amp;#xdf;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13845">
<title>Image Clustering using Restricted Boltzman Machine. (arXiv:2312.13845v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13845</link>
<description rdf:parseType="Literal">&lt;p&gt;In various verification systems, Restricted Boltzmann Machines (RBMs) have
demonstrated their efficacy in both front-end and back-end processes. In this
work, we propose the use of RBMs to the image clustering tasks. RBMs are
trained to convert images into image embeddings. We employ the conventional
bottom-up Agglomerative Hierarchical Clustering (AHC) technique. To address the
challenge of limited test face image data, we introduce Agglomerative
Hierarchical Clustering based Method for Image Clustering using Restricted
Boltzmann Machine (AHC-RBM) with two major steps. Initially, a universal RBM
model is trained using all available training dataset. Subsequently, we train
an adapted RBM model using the data from each test image. Finally, RBM vectors
which is the embedding vector is generated by concatenating the
visible-to-hidden weight matrices of these adapted models, and the bias
vectors. These vectors effectively preserve class-specific information and are
utilized in image clustering tasks. Our experimental results, conducted on two
benchmark image datasets (MS-Celeb-1M and DeepFashion), demonstrate that our
proposed approach surpasses well-known clustering algorithms such as k-means,
spectral clustering, and approximate Rank-order.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woubie_A/0/1/0/all/0/1&quot;&gt;Abraham Woubie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solomon_E/0/1/0/all/0/1&quot;&gt;Enoch Solomon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emiru_E/0/1/0/all/0/1&quot;&gt;Eyael Solomon Emiru&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13848">
<title>Reducing Hallucinations: Enhancing VQA for Flood Disaster Damage Assessment with Visual Contexts. (arXiv:2312.13848v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13848</link>
<description rdf:parseType="Literal">&lt;p&gt;The zero-shot performance of visual question answering (VQA) models relies
heavily on prompts. For example, a zero-shot VQA for disaster scenarios could
leverage well-designed Chain of Thought (CoT) prompts to stimulate the model&apos;s
potential. However, using CoT prompts has some problems, such as causing an
incorrect answer in the end due to the hallucination in the thought process. In
this paper, we propose a zero-shot VQA named Flood Disaster VQA with Two-Stage
Prompt (VQA-TSP). The model generates the thought process in the first stage
and then uses the thought process to generate the final answer in the second
stage. In particular, visual context is added in the second stage to relieve
the hallucination problem that exists in the thought process. Experimental
results show that our method exceeds the performance of state-of-the-art
zero-shot VQA models for flood disaster scenarios in total. Our study provides
a research basis for improving the performance of CoT-based zero-shot VQA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yimin Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yan Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13906">
<title>EfficientPPS: Part-aware Panoptic Segmentation of Transparent Objects for Robotic Manipulation. (arXiv:2312.13906v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.13906</link>
<description rdf:parseType="Literal">&lt;p&gt;The use of autonomous robots for assistance tasks in hospitals has the
potential to free up qualified staff and im-prove patient care. However, the
ubiquity of deformable and transparent objects in hospital settings poses
signif-icant challenges to vision-based perception systems. We present
EfficientPPS, a neural architecture for part-aware panoptic segmentation that
provides robots with semantically rich visual information for grasping and
ma-nipulation tasks. We also present an unsupervised data collection and
labelling method to reduce the need for human involvement in the training
process. EfficientPPS is evaluated on a dataset containing real-world hospital
objects and demonstrated to be robust and efficient in grasping transparent
transfusion bags with a collaborative robot arm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alt_B/0/1/0/all/0/1&quot;&gt;Benjamin Alt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1&quot;&gt;Minh Dang Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hermann_A/0/1/0/all/0/1&quot;&gt;Andreas Hermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katic_D/0/1/0/all/0/1&quot;&gt;Darko Katic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jakel_R/0/1/0/all/0/1&quot;&gt;Rainer J&amp;#xe4;kel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dillmann_R/0/1/0/all/0/1&quot;&gt;R&amp;#xfc;diger Dillmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sax_E/0/1/0/all/0/1&quot;&gt;Eric Sax&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13913">
<title>Paint3D: Paint Anything 3D with Lighting-Less Texture Diffusion Models. (arXiv:2312.13913v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13913</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents Paint3D, a novel coarse-to-fine generative framework that
is capable of producing high-resolution, lighting-less, and diverse 2K UV
texture maps for untextured 3D meshes conditioned on text or image inputs. The
key challenge addressed is generating high-quality textures without embedded
illumination information, which allows the textures to be re-lighted or
re-edited within modern graphics pipelines. To achieve this, our method first
leverages a pre-trained depth-aware 2D diffusion model to generate
view-conditional images and perform multi-view texture fusion, producing an
initial coarse texture map. However, as 2D models cannot fully represent 3D
shapes and disable lighting effects, the coarse texture map exhibits incomplete
areas and illumination artifacts. To resolve this, we train separate UV
Inpainting and UVHD diffusion models specialized for the shape-aware refinement
of incomplete areas and the removal of illumination artifacts. Through this
coarse-to-fine process, Paint3D can produce high-quality 2K UV textures that
maintain semantic consistency while being lighting-less, significantly
advancing the state-of-the-art in texturing 3D objects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1&quot;&gt;Xianfang Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13941">
<title>Controllable 3D Face Generation with Conditional Style Code Diffusion. (arXiv:2312.13941v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13941</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating photorealistic 3D faces from given conditions is a challenging
task. Existing methods often rely on time-consuming one-by-one optimization
approaches, which are not efficient for modeling the same distribution content,
e.g., faces. Additionally, an ideal controllable 3D face generation model
should consider both facial attributes and expressions. Thus we propose a novel
approach called TEx-Face(TExt &amp;amp; Expression-to-Face) that addresses these
challenges by dividing the task into three components, i.e., 3D GAN Inversion,
Conditional Style Code Diffusion, and 3D Face Decoding. For 3D GAN inversion,
we introduce two methods which aim to enhance the representation of style codes
and alleviate 3D inconsistencies. Furthermore, we design a style code denoiser
to incorporate multiple conditions into the style code and propose a data
augmentation strategy to address the issue of insufficient paired
visual-language data. Extensive experiments conducted on FFHQ, CelebA-HQ, and
CelebA-Dialog demonstrate the promising performance of our TEx-Face in
achieving the efficient and controllable generation of photorealistic 3D faces.
The code will be available at https://github.com/sxl142/TEx-Face.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1&quot;&gt;Xiaolong Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jianxin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zongxin Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13964">
<title>PIA: Your Personalized Image Animator via Plug-and-Play Modules in Text-to-Image Models. (arXiv:2312.13964v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13964</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in personalized text-to-image (T2I) models have
revolutionized content creation, empowering non-experts to generate stunning
images with unique styles. While promising, adding realistic motions into these
personalized images by text poses significant challenges in preserving distinct
styles, high-fidelity details, and achieving motion controllability by text. In
this paper, we present PIA, a Personalized Image Animator that excels in
aligning with condition images, achieving motion controllability by text, and
the compatibility with various personalized T2I models without specific tuning.
To achieve these goals, PIA builds upon a base T2I model with well-trained
temporal alignment layers, allowing for the seamless transformation of any
personalized T2I model into an image animation model. A key component of PIA is
the introduction of the condition module, which utilizes the condition frame
and inter-frame affinity as input to transfer appearance information guided by
the affinity hint for individual frame synthesis in the latent space. This
design mitigates the challenges of appearance-related image alignment within
and allows for a stronger focus on aligning with motion-related guidance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yiming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1&quot;&gt;Zhening Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1&quot;&gt;Yanhong Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Youqing Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13977">
<title>NeuSurf: On-Surface Priors for Neural Surface Reconstruction from Sparse Input Views. (arXiv:2312.13977v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13977</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, neural implicit functions have demonstrated remarkable results in
the field of multi-view reconstruction. However, most existing methods are
tailored for dense views and exhibit unsatisfactory performance when dealing
with sparse views. Several latest methods have been proposed for generalizing
implicit reconstruction to address the sparse view reconstruction task, but
they still suffer from high training costs and are merely valid under carefully
selected perspectives. In this paper, we propose a novel sparse view
reconstruction framework that leverages on-surface priors to achieve highly
faithful surface reconstruction. Specifically, we design several constraints on
global geometry alignment and local geometry refinement for jointly optimizing
coarse shapes and fine details. To achieve this, we train a neural network to
learn a global implicit field from the on-surface points obtained from SfM and
then leverage it as a coarse geometric constraint. To exploit local geometric
consistency, we project on-surface points onto seen and unseen views, treating
the consistent loss of projected features as a fine geometric constraint. The
experimental results with DTU and BlendedMVS datasets in two prevalent sparse
settings demonstrate significant improvements over the state-of-the-art
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Han Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yulun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Junsheng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1&quot;&gt;Ge Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_M/0/1/0/all/0/1&quot;&gt;Ming Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yushen Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13980">
<title>Carve3D: Improving Multi-view Reconstruction Consistency for Diffusion Models with RL Finetuning. (arXiv:2312.13980v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13980</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in the text-to-3D task leverage finetuned text-to-image
diffusion models to generate multi-view images, followed by NeRF
reconstruction. Yet, existing supervised finetuned (SFT) diffusion models still
suffer from multi-view inconsistency and the resulting NeRF artifacts. Although
training longer with SFT improves consistency, it also causes distribution
shift, which reduces diversity and realistic details. We argue that the SFT of
multi-view diffusion models resembles the instruction finetuning stage of the
LLM alignment pipeline and can benefit from RL finetuning (RLFT) methods.
Essentially, RLFT methods optimize models beyond their SFT data distribution by
using their own outputs, effectively mitigating distribution shift. To this
end, we introduce Carve3D, a RLFT method coupled with the Multi-view
Reconstruction Consistency (MRC) metric, to improve the consistency of
multi-view diffusion models. To compute MRC on a set of multi-view images, we
compare them with their corresponding renderings of the reconstructed NeRF at
the same viewpoints. We validate the robustness of MRC with extensive
experiments conducted under controlled inconsistency levels. We enhance the
base RLFT algorithm to stabilize the training process, reduce distribution
shift, and identify scaling laws. Through qualitative and quantitative
experiments, along with a user study, we demonstrate Carve3D&apos;s improved
multi-view consistency, the resulting superior NeRF reconstruction quality, and
minimal distribution shift compared to longer SFT. Project webpage:
https://desaixie.github.io/carve-3d.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_D/0/1/0/all/0/1&quot;&gt;Desai Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiahao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1&quot;&gt;Hao Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xin Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_Z/0/1/0/all/0/1&quot;&gt;Zhixin Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_S/0/1/0/all/0/1&quot;&gt;Sai Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pirk_S/0/1/0/all/0/1&quot;&gt;S&amp;#xf6;ren Pirk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaufman_A/0/1/0/all/0/1&quot;&gt;Arie E. Kaufman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13993">
<title>Open-Set: ID Card Presentation Attack Detection using Neural Transfer Style. (arXiv:2312.13993v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13993</link>
<description rdf:parseType="Literal">&lt;p&gt;The accurate detection of ID card Presentation Attacks (PA) is becoming
increasingly important due to the rising number of online/remote services that
require the presentation of digital photographs of ID cards for digital
onboarding or authentication. Furthermore, cybercriminals are continuously
searching for innovative ways to fool authentication systems to gain
unauthorized access to these services. Although advances in neural network
design and training have pushed image classification to the state of the art,
one of the main challenges faced by the development of fraud detection systems
is the curation of representative datasets for training and evaluation. The
handcrafted creation of representative presentation attack samples often
requires expertise and is very time-consuming, thus an automatic process of
obtaining high-quality data is highly desirable. This work explores ID card
Presentation Attack Instruments (PAI) in order to improve the generation of
samples with four Generative Adversarial Networks (GANs) based image
translation models and analyses the effectiveness of the generated data for
training fraud detection systems. Using open-source data, we show that
synthetic attack presentations are an adequate complement for additional real
attack presentations, where we obtain an EER performance increase of 0.63%
points for print attacks and a loss of 0.29% for screen capture attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Markham_R/0/1/0/all/0/1&quot;&gt;Reuben Markham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Espin_J/0/1/0/all/0/1&quot;&gt;Juan M. Espin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nieto_Hidalgo_M/0/1/0/all/0/1&quot;&gt;Mario Nieto-Hidalgo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tapia_J/0/1/0/all/0/1&quot;&gt;Juan E. Tapia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14001">
<title>Deep Learning Based Face Recognition Method using Siamese Network. (arXiv:2312.14001v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14001</link>
<description rdf:parseType="Literal">&lt;p&gt;Achieving state-of-the-art results in face verification systems typically
hinges on the availability of labeled face training data, a resource that often
proves challenging to acquire in substantial quantities. In this research
endeavor, we proposed employing Siamese networks for face recognition,
eliminating the need for labeled face images. We achieve this by strategically
leveraging negative samples alongside nearest neighbor counterparts, thereby
establishing positive and negative pairs through an unsupervised methodology.
The architectural framework adopts a VGG encoder, trained as a double branch
siamese network. Our primary aim is to circumvent the necessity for labeled
face image data, thus proposing the generation of training pairs in an entirely
unsupervised manner. Positive training data are selected within a dataset based
on their highest cosine similarity scores with a designated anchor, while
negative training data are culled in a parallel fashion, though drawn from an
alternate dataset. During training, the proposed siamese network conducts
binary classification via cross-entropy loss. Subsequently, during the testing
phase, we directly extract face verification scores from the network&apos;s output
layer. Experimental results reveal that the proposed unsupervised system
delivers a performance on par with a similar but fully supervised baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solomon_E/0/1/0/all/0/1&quot;&gt;Enoch Solomon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woubie_A/0/1/0/all/0/1&quot;&gt;Abraham Woubie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emiru_E/0/1/0/all/0/1&quot;&gt;Eyael Solomon Emiru&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14024">
<title>Geometric Awareness in Neural Fields for 3D Human Registration. (arXiv:2312.14024v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14024</link>
<description rdf:parseType="Literal">&lt;p&gt;Aligning a template to 3D human point clouds is a long-standing problem
crucial for tasks like animation, reconstruction, and enabling supervised
learning pipelines. Recent data-driven methods leverage predicted surface
correspondences; however, they are not robust to varied poses or distributions.
In contrast, industrial solutions often rely on expensive manual annotations or
multi-view capturing systems. Recently, neural fields have shown promising
results, but their purely data-driven nature lacks geometric awareness, often
resulting in a trivial misalignment of the template registration. In this work,
we propose two solutions: LoVD, a novel neural field model that predicts the
direction towards the localized SMPL vertices on the target surface; and INT,
the first self-supervised task dedicated to neural fields that, at test time,
refines the backbone, exploiting the target geometry. We combine them into
INLoVD, a robust 3D Human body registration pipeline trained on a large MoCap
dataset. INLoVD is efficient (takes less than a minute), solidly achieves the
state of the art over public benchmarks, and provides unprecedented
generalization on out-of-distribution data. We will release code and
checkpoints in \url{url}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marin_R/0/1/0/all/0/1&quot;&gt;Riccardo Marin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corona_E/0/1/0/all/0/1&quot;&gt;Enric Corona&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pons_Moll_G/0/1/0/all/0/1&quot;&gt;Gerard Pons-Moll&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14053">
<title>Dual Attention U-Net with Feature Infusion: Pushing the Boundaries of Multiclass Defect Segmentation. (arXiv:2312.14053v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14053</link>
<description rdf:parseType="Literal">&lt;p&gt;The proposed architecture, Dual Attentive U-Net with Feature Infusion (DAU-FI
Net), addresses challenges in semantic segmentation, particularly on multiclass
imbalanced datasets with limited samples. DAU-FI Net integrates multiscale
spatial-channel attention mechanisms and feature injection to enhance precision
in object localization. The core employs a multiscale depth-separable
convolution block, capturing localized patterns across scales. This block is
complemented by a spatial-channel squeeze and excitation (scSE) attention unit,
modeling inter-dependencies between channels and spatial regions in feature
maps. Additionally, additive attention gates refine segmentation by connecting
encoder-decoder pathways.
&lt;/p&gt;
&lt;p&gt;To augment the model, engineered features using Gabor filters for textural
analysis, Sobel and Canny filters for edge detection are injected guided by
semantic masks to expand the feature space strategically. Comprehensive
experiments on a challenging sewer pipe and culvert defect dataset and a
benchmark dataset validate DAU-FI Net&apos;s capabilities. Ablation studies
highlight incremental benefits from attention blocks and feature injection.
DAU-FI Net achieves state-of-the-art mean Intersection over Union (IoU) of
95.6% and 98.8% on the defect test set and benchmark respectively, surpassing
prior methods by 8.9% and 12.6%, respectively. Ablation studies highlight
incremental benefits from attention blocks and feature injection. The proposed
architecture provides a robust solution, advancing semantic segmentation for
multiclass problems with limited training data. Our sewer-culvert defects
dataset, featuring pixel-level annotations, opens avenues for further research
in this crucial domain. Overall, this work delivers key innovations in
architecture, attention, and feature engineering to elevate semantic
segmentation efficacy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alshawi_R/0/1/0/all/0/1&quot;&gt;Rasha Alshawi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoque_M/0/1/0/all/0/1&quot;&gt;Md Tamjidul Hoque&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferdaus_M/0/1/0/all/0/1&quot;&gt;Md Meftahul Ferdaus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdelguerfi_M/0/1/0/all/0/1&quot;&gt;Mahdi Abdelguerfi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niles_K/0/1/0/all/0/1&quot;&gt;Kendall Niles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prathak_K/0/1/0/all/0/1&quot;&gt;Ken Prathak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tom_J/0/1/0/all/0/1&quot;&gt;Joe Tom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klein_J/0/1/0/all/0/1&quot;&gt;Jordan Klein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mousa_M/0/1/0/all/0/1&quot;&gt;Murtada Mousa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopez_J/0/1/0/all/0/1&quot;&gt;Johny Javier Lopez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14055">
<title>A Strong Baseline for Temporal Video-Text Alignment. (arXiv:2312.14055v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14055</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we consider the problem of temporally aligning the video and
texts from instructional videos, specifically, given a long-term video, and
associated text sentences, our goal is to determine their corresponding
timestamps in the video. To this end, we establish a simple, yet strong model
that adopts a Transformer-based architecture with all texts as queries,
iteratively attending to the visual features, to infer the optimal timestamp.
We conduct thorough experiments to investigate: (i) the effect of upgrading ASR
systems to reduce errors from speech recognition, (ii) the effect of various
visual-textual backbones, ranging from CLIP to S3D, to the more recent
InternVideo, (iii) the effect of transforming noisy ASR transcripts into
descriptive steps by prompting a large language model (LLM), to summarize the
core activities within the ASR transcript as a new training dataset. As a
result, our proposed simple model demonstrates superior performance on both
narration alignment and procedural step grounding tasks, surpassing existing
state-of-the-art methods by a significant margin on three public benchmarks,
namely, 9.3% on HT-Step, 3.4% on HTM-Align and 4.7% on CrossTask. We believe
the proposed model and dataset with descriptive steps can be treated as a
strong baseline for future research in temporal video-text alignment. All
codes, models, and the resulting dataset will be publicly released to the
research community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zeqian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qirui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1&quot;&gt;Tengda Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Ya Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanfeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1&quot;&gt;Weidi Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14074">
<title>LiDAR-LLM: Exploring the Potential of Large Language Models for 3D LiDAR Understanding. (arXiv:2312.14074v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14074</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, Large Language Models (LLMs) and Multimodal Large Language Models
(MLLMs) have shown promise in instruction following and 2D image understanding.
While these models are powerful, they have not yet been developed to comprehend
the more challenging 3D physical scenes, especially when it comes to the sparse
outdoor LiDAR data. In this paper, we introduce LiDAR-LLM, which takes raw
LiDAR data as input and harnesses the remarkable reasoning capabilities of LLMs
to gain a comprehensive understanding of outdoor 3D scenes. The central insight
of our LiDAR-LLM is the reformulation of 3D outdoor scene cognition as a
language modeling problem, encompassing tasks such as 3D captioning, 3D
grounding, 3D question answering, etc. Specifically, due to the scarcity of 3D
LiDAR-text pairing data, we introduce a three-stage training strategy and
generate relevant datasets, progressively aligning the 3D modality with the
language embedding space of LLM. Furthermore, we design a View-Aware
Transformer (VAT) to connect the 3D encoder with the LLM, which effectively
bridges the modality gap and enhances the LLM&apos;s spatial orientation
comprehension of visual features. Our experiments show that LiDAR-LLM possesses
favorable capabilities to comprehend various instructions regarding 3D scenes
and engage in complex spatial reasoning. LiDAR-LLM attains a 40.9 BLEU-1 on the
3D captioning task and achieves a 63.1\% classification accuracy and a 14.3\%
BEV mIoU on the 3D grounding task. Web page:
https://sites.google.com/view/lidar-llm
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Senqiao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiaming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ray Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_M/0/1/0/all/0/1&quot;&gt;Mingjie Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zoey Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zehui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1&quot;&gt;Peng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yandong Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shanghang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14091">
<title>HD-Painter: High-Resolution and Prompt-Faithful Text-Guided Image Inpainting with Diffusion Models. (arXiv:2312.14091v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14091</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent progress in text-guided image inpainting, based on the unprecedented
success of text-to-image diffusion models, has led to exceptionally realistic
and visually plausible results. However, there is still significant potential
for improvement in current text-to-image inpainting models, particularly in
better aligning the inpainted area with user prompts and performing
high-resolution inpainting. Therefore, in this paper we introduce HD-Painter, a
completely training-free approach that accurately follows to prompts and
coherently scales to high-resolution image inpainting. To this end, we design
the Prompt-Aware Introverted Attention (PAIntA) layer enhancing self-attention
scores by prompt information and resulting in better text alignment
generations. To further improve the prompt coherence we introduce the
Reweighting Attention Score Guidance (RASG) mechanism seamlessly integrating a
post-hoc sampling strategy into general form of DDIM to prevent
out-of-distribution latent shifts. Moreover, HD-Painter allows extension to
larger scales by introducing a specialized super-resolution technique
customized for inpainting, enabling the completion of missing regions in images
of up to 2K resolution. Our experiments demonstrate that HD-Painter surpasses
existing state-of-the-art approaches qualitatively and quantitatively,
achieving an impressive generation accuracy improvement of 61.4% vs 51.9%. We
will make the codes publicly available at:
https://github.com/Picsart-AI-Research/HD-Painter
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manukyan_H/0/1/0/all/0/1&quot;&gt;Hayk Manukyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sargsyan_A/0/1/0/all/0/1&quot;&gt;Andranik Sargsyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atanyan_B/0/1/0/all/0/1&quot;&gt;Barsegh Atanyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navasardyan_S/0/1/0/all/0/1&quot;&gt;Shant Navasardyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Humphrey Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14115">
<title>LingoQA: Video Question Answering for Autonomous Driving. (arXiv:2312.14115v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.14115</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous driving has long faced a challenge with public acceptance due to
the lack of explainability in the decision-making process. Video
question-answering (QA) in natural language provides the opportunity for
bridging this gap. Nonetheless, evaluating the performance of Video QA models
has proved particularly tough due to the absence of comprehensive benchmarks.
To fill this gap, we introduce LingoQA, a benchmark specifically for autonomous
driving Video QA. The LingoQA trainable metric demonstrates a 0.95 Spearman
correlation coefficient with human evaluations. We introduce a Video QA dataset
of central London consisting of 419k samples that we release with the paper. We
establish a baseline vision-language model and run extensive ablation studies
to understand its performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marcu_A/0/1/0/all/0/1&quot;&gt;Ana-Maria Marcu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Long Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hunermann_J/0/1/0/all/0/1&quot;&gt;Jan H&amp;#xfc;nermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karnsund_A/0/1/0/all/0/1&quot;&gt;Alice Karnsund&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanotte_B/0/1/0/all/0/1&quot;&gt;Benoit Hanotte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chidananda_P/0/1/0/all/0/1&quot;&gt;Prajwal Chidananda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1&quot;&gt;Saurabh Nair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Badrinarayanan_V/0/1/0/all/0/1&quot;&gt;Vijay Badrinarayanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kendall_A/0/1/0/all/0/1&quot;&gt;Alex Kendall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shotton_J/0/1/0/all/0/1&quot;&gt;Jamie Shotton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sinavski_O/0/1/0/all/0/1&quot;&gt;Oleg Sinavski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14124">
<title>Neural Point Cloud Diffusion for Disentangled 3D Shape and Appearance Generation. (arXiv:2312.14124v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14124</link>
<description rdf:parseType="Literal">&lt;p&gt;Controllable generation of 3D assets is important for many practical
applications like content creation in movies, games and engineering, as well as
in AR/VR. Recently, diffusion models have shown remarkable results in
generation quality of 3D objects. However, none of the existing models enable
disentangled generation to control the shape and appearance separately. For the
first time, we present a suitable representation for 3D diffusion models to
enable such disentanglement by introducing a hybrid point cloud and neural
radiance field approach. We model a diffusion process over point positions
jointly with a high-dimensional feature space for a local density and radiance
decoder. While the point positions represent the coarse shape of the object,
the point features allow modeling the geometry and appearance details. This
disentanglement enables us to sample both independently and therefore to
control both separately. Our approach sets a new state of the art in generation
compared to previous disentanglement-capable methods by reduced FID scores of
30-90% and is on-par with other non disentanglement-capable state-of-the art
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schroppel_P/0/1/0/all/0/1&quot;&gt;Philipp Schr&amp;#xf6;ppel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wewer_C/0/1/0/all/0/1&quot;&gt;Christopher Wewer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lenssen_J/0/1/0/all/0/1&quot;&gt;Jan Eric Lenssen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilg_E/0/1/0/all/0/1&quot;&gt;Eddy Ilg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1&quot;&gt;Thomas Brox&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14125">
<title>VideoPoet: A Large Language Model for Zero-Shot Video Generation. (arXiv:2312.14125v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14125</link>
<description rdf:parseType="Literal">&lt;p&gt;We present VideoPoet, a language model capable of synthesizing high-quality
video, with matching audio, from a large variety of conditioning signals.
VideoPoet employs a decoder-only transformer architecture that processes
multimodal inputs -- including images, videos, text, and audio. The training
protocol follows that of Large Language Models (LLMs), consisting of two
stages: pretraining and task-specific adaptation. During pretraining, VideoPoet
incorporates a mixture of multimodal generative objectives within an
autoregressive Transformer framework. The pretrained LLM serves as a foundation
that can be adapted for a range of video generation tasks. We present empirical
results demonstrating the model&apos;s state-of-the-art capabilities in zero-shot
video generation, specifically highlighting VideoPoet&apos;s ability to generate
high-fidelity motions. Project page: &lt;a href=&quot;http://sites.research.google/videopoet/&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kondratyuk_D/0/1/0/all/0/1&quot;&gt;Dan Kondratyuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Lijun Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1&quot;&gt;Xiuye Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lezama_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Lezama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jonathan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hornung_R/0/1/0/all/0/1&quot;&gt;Rachel Hornung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adam_H/0/1/0/all/0/1&quot;&gt;Hartwig Adam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akbari_H/0/1/0/all/0/1&quot;&gt;Hassan Akbari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alon_Y/0/1/0/all/0/1&quot;&gt;Yair Alon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Birodkar_V/0/1/0/all/0/1&quot;&gt;Vighnesh Birodkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yong Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiu_M/0/1/0/all/0/1&quot;&gt;Ming-Chang Chiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dillon_J/0/1/0/all/0/1&quot;&gt;Josh Dillon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Essa_I/0/1/0/all/0/1&quot;&gt;Irfan Essa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Agrim Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hahn_M/0/1/0/all/0/1&quot;&gt;Meera Hahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hauth_A/0/1/0/all/0/1&quot;&gt;Anja Hauth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hendon_D/0/1/0/all/0/1&quot;&gt;David Hendon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_A/0/1/0/all/0/1&quot;&gt;Alonso Martinez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Minnen_D/0/1/0/all/0/1&quot;&gt;David Minnen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ross_D/0/1/0/all/0/1&quot;&gt;David Ross&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schindler_G/0/1/0/all/0/1&quot;&gt;Grant Schindler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sirotenko_M/0/1/0/all/0/1&quot;&gt;Mikhail Sirotenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1&quot;&gt;Kihyuk Sohn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Somandepalli_K/0/1/0/all/0/1&quot;&gt;Krishna Somandepalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Huisheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Jimmy Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Ming-Hsuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seybold_B/0/1/0/all/0/1&quot;&gt;Bryan Seybold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1&quot;&gt;Lu Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14126">
<title>Entropic Open-set Active Learning. (arXiv:2312.14126v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14126</link>
<description rdf:parseType="Literal">&lt;p&gt;Active Learning (AL) aims to enhance the performance of deep models by
selecting the most informative samples for annotation from a pool of unlabeled
data. Despite impressive performance in closed-set settings, most AL methods
fail in real-world scenarios where the unlabeled data contains unknown
categories. Recently, a few studies have attempted to tackle the AL problem for
the open-set setting. However, these methods focus more on selecting known
samples and do not efficiently utilize unknown samples obtained during AL
rounds. In this work, we propose an Entropic Open-set AL (EOAL) framework which
leverages both known and unknown distributions effectively to select
informative samples during AL rounds. Specifically, our approach employs two
different entropy scores. One measures the uncertainty of a sample with respect
to the known-class distributions. The other measures the uncertainty of the
sample with respect to the unknown-class distributions. By utilizing these two
entropy scores we effectively separate the known and unknown samples from the
unlabeled data resulting in better sampling. Through extensive experiments, we
show that the proposed method outperforms existing state-of-the-art methods on
CIFAR-10, CIFAR-100, and TinyImageNet datasets. Code is available at
\url{https://github.com/bardisafa/EOAL}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Safaei_B/0/1/0/all/0/1&quot;&gt;Bardia Safaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+VS_V/0/1/0/all/0/1&quot;&gt;Vibashan VS&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melo_C/0/1/0/all/0/1&quot;&gt;Celso M. de Melo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1&quot;&gt;Vishal M. Patel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14132">
<title>DUSt3R: Geometric 3D Vision Made Easy. (arXiv:2312.14132v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14132</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-view stereo reconstruction (MVS) in the wild requires to first estimate
the camera parameters e.g. intrinsic and extrinsic parameters. These are
usually tedious and cumbersome to obtain, yet they are mandatory to triangulate
corresponding pixels in 3D space, which is the core of all best performing MVS
algorithms. In this work, we take an opposite stance and introduce DUSt3R, a
radically novel paradigm for Dense and Unconstrained Stereo 3D Reconstruction
of arbitrary image collections, i.e. operating without prior information about
camera calibration nor viewpoint poses. We cast the pairwise reconstruction
problem as a regression of pointmaps, relaxing the hard constraints of usual
projective camera models. We show that this formulation smoothly unifies the
monocular and binocular reconstruction cases. In the case where more than two
images are provided, we further propose a simple yet effective global alignment
strategy that expresses all pairwise pointmaps in a common reference frame. We
base our network architecture on standard Transformer encoders and decoders,
allowing us to leverage powerful pretrained models. Our formulation directly
provides a 3D model of the scene as well as depth information, but
interestingly, we can seamlessly recover from it, pixel matches, relative and
absolute camera. Exhaustive experiments on all these tasks showcase that the
proposed DUSt3R can unify various 3D vision tasks and set new SoTAs on
monocular/multi-view depth estimation as well as relative pose estimation. In
summary, DUSt3R makes many geometric 3D vision tasks easy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuzhe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leroy_V/0/1/0/all/0/1&quot;&gt;Vincent Leroy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cabon_Y/0/1/0/all/0/1&quot;&gt;Yohann Cabon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chidlovskii_B/0/1/0/all/0/1&quot;&gt;Boris Chidlovskii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Revaud_J/0/1/0/all/0/1&quot;&gt;Jerome Revaud&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14134">
<title>Diffusion Reward: Learning Rewards via Conditional Video Diffusion. (arXiv:2312.14134v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.14134</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning rewards from expert videos offers an affordable and effective
solution to specify the intended behaviors for reinforcement learning tasks. In
this work, we propose Diffusion Reward, a novel framework that learns rewards
from expert videos via conditional video diffusion models for solving complex
visual RL problems. Our key insight is that lower generative diversity is
observed when conditioned on expert trajectories. Diffusion Reward is
accordingly formalized by the negative of conditional entropy that encourages
productive exploration of expert-like behaviors. We show the efficacy of our
method over 10 robotic manipulation tasks from MetaWorld and Adroit with visual
input and sparse reward. Moreover, Diffusion Reward could even solve unseen
tasks successfully and effectively, largely surpassing baseline methods.
Project page and code: https://diffusion-reward.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_G/0/1/0/all/0/1&quot;&gt;Guangqi Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ze_Y/0/1/0/all/0/1&quot;&gt;Yanjie Ze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Huazhe Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14135">
<title>$\textit{V}^*$: Guided Visual Search as a Core Mechanism in Multimodal LLMs. (arXiv:2312.14135v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14135</link>
<description rdf:parseType="Literal">&lt;p&gt;When we look around and perform complex tasks, how we see and selectively
process what we see is crucial. However, the lack of this visual search
mechanism in current multimodal LLMs (MLLMs) hinders their ability to focus on
important visual details, especially when handling high-resolution and visually
crowded images. To address this, we introduce $\textit{V}^*$, an LLM-guided
visual search mechanism that employs the world knowledge in LLMs for efficient
visual querying. When combined with an MLLM, this mechanism enhances
collaborative reasoning, contextual understanding, and precise targeting of
specific visual elements. This integration results in a new MLLM
meta-architecture, named $\textbf{S}$how, s$\textbf{EA}$rch, and
Tel$\textbf{L}$ (SEAL). We further create $\textit{V}^*$Bench, a benchmark
specifically designed to evaluate MLLMs in their ability to process
high-resolution images and focus on visual details. Our study highlights the
necessity of incorporating visual search capabilities into multimodal systems.
The code is available https://github.com/penghao-wu/vstar.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1&quot;&gt;Penghao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1&quot;&gt;Saining Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14138">
<title>Revisiting Foreground and Background Separation in Weakly-supervised Temporal Action Localization: A Clustering-based Approach. (arXiv:2312.14138v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14138</link>
<description rdf:parseType="Literal">&lt;p&gt;Weakly-supervised temporal action localization aims to localize action
instances in videos with only video-level action labels. Existing methods
mainly embrace a localization-by-classification pipeline that optimizes the
snippet-level prediction with a video classification loss. However, this
formulation suffers from the discrepancy between classification and detection,
resulting in inaccurate separation of foreground and background (F\&amp;amp;B)
snippets. To alleviate this problem, we propose to explore the underlying
structure among the snippets by resorting to unsupervised snippet clustering,
rather than heavily relying on the video classification loss. Specifically, we
propose a novel clustering-based F\&amp;amp;B separation algorithm. It comprises two
core components: a snippet clustering component that groups the snippets into
multiple latent clusters and a cluster classification component that further
classifies the cluster as foreground or background. As there are no
ground-truth labels to train these two components, we introduce a unified
self-labeling mechanism based on optimal transport to produce high-quality
pseudo-labels that match several plausible prior distributions. This ensures
that the cluster assignments of the snippets can be accurately associated with
their F\&amp;amp;B labels, thereby boosting the F\&amp;amp;B separation. We evaluate our method
on three benchmarks: THUMOS14, ActivityNet v1.2 and v1.3. Our method achieves
promising performance on all three benchmarks while being significantly more
lightweight than previous methods. Code is available at
https://github.com/Qinying-Liu/CASE
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qinying Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zilei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rong_S/0/1/0/all/0/1&quot;&gt;Shenghai Rong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Junjie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yixin Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14140">
<title>HeadCraft: Modeling High-Detail Shape Variations for Animated 3DMMs. (arXiv:2312.14140v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14140</link>
<description rdf:parseType="Literal">&lt;p&gt;Current advances in human head modeling allow to generate plausible-looking
3D head models via neural representations. Nevertheless, constructing complete
high-fidelity head models with explicitly controlled animation remains an
issue. Furthermore, completing the head geometry based on a partial
observation, e.g. coming from a depth sensor, while preserving details is often
problematic for the existing methods. We introduce a generative model for
detailed 3D head meshes on top of an articulated 3DMM which allows explicit
animation and high-detail preservation at the same time. Our method is trained
in two stages. First, we register a parametric head model with vertex
displacements to each mesh of the recently introduced NPHM dataset of accurate
3D head scans. The estimated displacements are baked into a hand-crafted UV
layout. Second, we train a StyleGAN model in order to generalize over the UV
maps of displacements. The decomposition of the parametric model and
high-quality vertex displacements allows us to animate the model and modify it
semantically. We demonstrate the results of unconditional generation and
fitting to the full or partial observation. The project page is available at
https://seva100.github.io/headcraft.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sevastopolsky_A/0/1/0/all/0/1&quot;&gt;Artem Sevastopolsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grassal_P/0/1/0/all/0/1&quot;&gt;Philip-William Grassal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giebenhain_S/0/1/0/all/0/1&quot;&gt;Simon Giebenhain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Athar_S/0/1/0/all/0/1&quot;&gt;ShahRukh Athar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verdoliva_L/0/1/0/all/0/1&quot;&gt;Luisa Verdoliva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1&quot;&gt;Matthias Niessner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14149">
<title>TagAlign: Improving Vision-Language Alignment with Multi-Tag Classification. (arXiv:2312.14149v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14149</link>
<description rdf:parseType="Literal">&lt;p&gt;The crux of learning vision-language models is to extract semantically
aligned information from visual and linguistic data. Existing attempts usually
face the problem of coarse alignment, \textit{e.g.}, the vision encoder
struggles in localizing an attribute-specified object. In this work, we propose
an embarrassingly simple approach to better align image and text features with
no need of additional data formats other than image-text pairs. Concretely,
given an image and its paired text, we manage to parse objects (\textit{e.g.},
cat) and attributes (\textit{e.g.}, black) from the description, which are
highly likely to exist in the image. It is noteworthy that the parsing pipeline
is fully automatic and thus enjoys good scalability. With these parsed
semantics as supervision signals, we can complement the commonly used
image-text contrastive loss with the multi-tag classification loss. Extensive
experimental results on a broad suite of semantic segmentation datasets
substantiate the average 3.65\% improvement of our framework over existing
alternatives. Furthermore, the visualization results indicate that attribute
supervision makes vision-language models accurately localize
attribute-specified objects. Project page can be found at
https://qinying-liu.github.io/Tag-Align/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qinying Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1&quot;&gt;Kecheng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1&quot;&gt;Wu Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_Z/0/1/0/all/0/1&quot;&gt;Zhan Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zilei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yujun Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14150">
<title>DriveLM: Driving with Graph Visual Question Answering. (arXiv:2312.14150v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14150</link>
<description rdf:parseType="Literal">&lt;p&gt;We study how vision-language models (VLMs) trained on web-scale data can be
integrated into end-to-end driving systems to boost generalization and enable
interactivity with human users. While recent approaches adapt VLMs to driving
via single-round visual question answering (VQA), human drivers reason about
decisions in multiple steps. Starting from the localization of key objects,
humans estimate object interactions before taking actions. The key insight is
that with our proposed task, Graph VQA, where we model graph-structured
reasoning through perception, prediction and planning question-answer pairs, we
obtain a suitable proxy task to mimic the human reasoning process. We
instantiate datasets (DriveLM-Data) built upon nuScenes and CARLA, and propose
a VLM-based baseline approach (DriveLM-Agent) for jointly performing Graph VQA
and end-to-end driving. The experiments demonstrate that Graph VQA provides a
simple, principled framework for reasoning about a driving scene, and
DriveLM-Data provides a challenging benchmark for this task. Our DriveLM-Agent
baseline performs end-to-end autonomous driving competitively in comparison to
state-of-the-art driving-specific architectures. Notably, its benefits are
pronounced when it is evaluated zero-shot on unseen objects or sensor
configurations. We hope this work can be the starting point to shed new light
on how to apply VLMs for autonomous driving. To facilitate future research, all
code, data, and models are available to the public.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sima_C/0/1/0/all/0/1&quot;&gt;Chonghao Sima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Renz_K/0/1/0/all/0/1&quot;&gt;Katrin Renz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chitta_K/0/1/0/all/0/1&quot;&gt;Kashyap Chitta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Li Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hanxue Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1&quot;&gt;Chengen Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1&quot;&gt;Andreas Geiger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongyang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14154">
<title>Virtual Pets: Animatable Animal Generation in 3D Scenes. (arXiv:2312.14154v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14154</link>
<description rdf:parseType="Literal">&lt;p&gt;Toward unlocking the potential of generative models in immersive 4D
experiences, we introduce Virtual Pet, a novel pipeline to model realistic and
diverse motions for target animal species within a 3D environment. To
circumvent the limited availability of 3D motion data aligned with
environmental geometry, we leverage monocular internet videos and extract
deformable NeRF representations for the foreground and static NeRF
representations for the background. For this, we develop a reconstruction
strategy, encompassing species-level shared template learning and per-video
fine-tuning. Utilizing the reconstructed data, we then train a conditional 3D
motion model to learn the trajectory and articulation of foreground animals in
the context of 3D backgrounds. We showcase the efficacy of our pipeline with
comprehensive qualitative and quantitative evaluations using cat videos. We
also demonstrate versatility across unseen cats and indoor environments,
producing temporally coherent 4D outputs for enriched virtual experiences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yen-Chi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chieh Hubert Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chaoyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kant_Y/0/1/0/all/0/1&quot;&gt;Yash Kant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1&quot;&gt;Sergey Tulyakov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1&quot;&gt;Alexander Schwing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gui_L/0/1/0/all/0/1&quot;&gt;Liangyan Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hsin-Ying Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14157">
<title>3D Pose Estimation of Two Interacting Hands from a Monocular Event Camera. (arXiv:2312.14157v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14157</link>
<description rdf:parseType="Literal">&lt;p&gt;3D hand tracking from a monocular video is a very challenging problem due to
hand interactions, occlusions, left-right hand ambiguity, and fast motion. Most
existing methods rely on RGB inputs, which have severe limitations under
low-light conditions and suffer from motion blur. In contrast, event cameras
capture local brightness changes instead of full image frames and do not suffer
from the described effects. Unfortunately, existing image-based techniques
cannot be directly applied to events due to significant differences in the data
modalities. In response to these challenges, this paper introduces the first
framework for 3D tracking of two fast-moving and interacting hands from a
single monocular event camera. Our approach tackles the left-right hand
ambiguity with a novel semi-supervised feature-wise attention mechanism and
integrates an intersection loss to fix hand collisions. To facilitate advances
in this research domain, we release a new synthetic large-scale dataset of two
interacting hands, Ev2Hands-S, and a new real benchmark with real event streams
and ground-truth 3D annotations, Ev2Hands-R. Our approach outperforms existing
methods in terms of the 3D reconstruction accuracy and generalises to real data
under severe light conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Millerdurai_C/0/1/0/all/0/1&quot;&gt;Christen Millerdurai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luvizon_D/0/1/0/all/0/1&quot;&gt;Diogo Luvizon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rudnev_V/0/1/0/all/0/1&quot;&gt;Viktor Rudnev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jonas_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Jonas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiayi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1&quot;&gt;Christian Theobalt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1&quot;&gt;Vladislav Golyanik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2108.02893">
<title>Basis Scaling and Double Pruning for Efficient Inference in Network-Based Transfer Learning. (arXiv:2108.02893v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2108.02893</link>
<description rdf:parseType="Literal">&lt;p&gt;Network-based transfer learning allows the reuse of deep learning features
with limited data, but the resulting models can be unnecessarily large.
Although network pruning can improve inference efficiency, existing algorithms
usually require fine-tuning that may not be suitable for small datasets. In
this paper, using the singular value decomposition, we decompose a
convolutional layer into two layers: a convolutional layer with the orthonormal
basis vectors as the filters, and a &quot;BasisScalingConv&quot; layer which is
responsible for rescaling the features and transforming them back to the
original space. As the filters in each decomposed layer are linearly
independent, when using the proposed basis scaling factors with the Taylor
approximation of importance, pruning can be more effective and fine-tuning
individual weights is unnecessary. Furthermore, as the numbers of input and
output channels of the original convolutional layer remain unchanged after
basis pruning, it is applicable to virtually all architectures and can be
combined with existing pruning algorithms for double pruning to further
increase the pruning capability. When transferring knowledge from ImageNet
pre-trained models to different target domains, with less than 1% reduction in
classification accuracies, we can achieve pruning ratios up to 74.6% for
CIFAR-10 and 98.9% for MNIST in model parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1&quot;&gt;Ken C. L. Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kashyap_S/0/1/0/all/0/1&quot;&gt;Satyananda Kashyap&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1&quot;&gt;Mehdi Moradi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.02980">
<title>3D Object Detection from Images for Autonomous Driving: A Survey. (arXiv:2202.02980v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2202.02980</link>
<description rdf:parseType="Literal">&lt;p&gt;3D object detection from images, one of the fundamental and challenging
problems in autonomous driving, has received increasing attention from both
industry and academia in recent years. Benefiting from the rapid development of
deep learning technologies, image-based 3D detection has achieved remarkable
progress. Particularly, more than 200 works have studied this problem from 2015
to 2021, encompassing a broad spectrum of theories, algorithms, and
applications. However, to date no recent survey exists to collect and organize
this knowledge. In this paper, we fill this gap in the literature and provide
the first comprehensive survey of this novel and continuously growing research
field, summarizing the most commonly used pipelines for image-based 3D
detection and deeply analyzing each of their components. Additionally, we also
propose two new taxonomies to organize the state-of-the-art methods into
different categories, with the intent of providing a more systematic review of
existing methods and facilitating fair comparisons with future works. In
retrospect of what has been achieved so far, we also analyze the current
challenges in the field and discuss future directions for image-based 3D
detection research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xinzhu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1&quot;&gt;Wanli Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simonelli_A/0/1/0/all/0/1&quot;&gt;Andrea Simonelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1&quot;&gt;Elisa Ricci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.00400">
<title>Unleashing the Potential of Adjacent Snippets for Weakly-supervised Temporal Action Localization. (arXiv:2205.00400v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2205.00400</link>
<description rdf:parseType="Literal">&lt;p&gt;Weakly-supervised temporal action localization (WTAL) intends to detect
action instances with only weak supervision, \eg, video-level labels. The
current~\textit{de facto} pipeline locates action instances by thresholding and
grouping continuous high-score regions on temporal class activation sequences.
In this route, the capacity of the model to recognize the relationships between
adjacent snippets is of vital importance which determines the quality of the
action boundaries. However, it is error-prone since the variations between
adjacent snippets are typically subtle, and unfortunately this is overlooked in
the literature. To tackle the issue, we propose a novel WTAL approach named
Convex Combination Consistency between Neighbors (C$^3$BN). C$^3$BN consists of
two key ingredients: a micro data augmentation strategy that increases the
diversity in-between adjacent snippets by convex combination of adjacent
snippets, and a macro-micro consistency regularization that enforces the model
to be invariant to the transformations~\textit{w.r.t.} video semantics, snippet
predictions, and snippet representations. Consequently, fine-grained patterns
in-between adjacent snippets are enforced to be explored, thereby resulting in
a more robust action boundary localization. Experimental results demonstrate
the effectiveness of C$^3$BN on top of various baselines for WTAL with
video-level and point-level supervisions. Code is at
https://github.com/Qinying-Liu/C3BN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qinying Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zilei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Ruoxi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhilin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.08965">
<title>KitBit: A New AI Model for Solving Intelligence Tests and Numerical Series. (arXiv:2206.08965v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2206.08965</link>
<description rdf:parseType="Literal">&lt;p&gt;The resolution of intelligence tests, in particular numerical sequences, has
been of great interest in the evaluation of AI systems. We present a new
computational model called KitBit that uses a reduced set of algorithms and
their combinations to build a predictive model that finds the underlying
pattern in numerical sequences, such as those included in IQ tests and others
of much greater complexity. We present the fundamentals of the model and its
application in different cases. First, the system is tested on a set of number
series used in IQ tests collected from various sources. Next, our model is
successfully applied on the sequences used to evaluate the models reported in
the literature. In both cases, the system is capable of solving these types of
problems in less than a second using standard computing power. Finally,
KitBit&apos;s algorithms have been applied for the first time to the complete set of
entire sequences of the well-known OEIS database. We find a pattern in the form
of a list of algorithms and predict the following terms in the largest number
of series to date. These results demonstrate the potential of KitBit to solve
complex problems that could be represented numerically.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corsino_V/0/1/0/all/0/1&quot;&gt;V&amp;#xed;ctor Corsino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilperez_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Manuel Gilp&amp;#xe9;rez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herrera_L/0/1/0/all/0/1&quot;&gt;Luis Herrera&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.02998">
<title>ThoraX-PriorNet: A Novel Attention-Based Architecture Using Anatomical Prior Probability Maps for Thoracic Disease Classification. (arXiv:2210.02998v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.02998</link>
<description rdf:parseType="Literal">&lt;p&gt;Objective: Computer-aided disease diagnosis and prognosis based on medical
images is a rapidly emerging field. Many Convolutional Neural Network (CNN)
architectures have been developed by researchers for disease classification and
localization from chest X-ray images. It is known that different thoracic
disease lesions are more likely to occur in specific anatomical regions
compared to others. This article aims to incorporate this disease and
region-dependent prior probability distribution within a deep learning
framework. Methods: We present the ThoraX-PriorNet, a novel attention-based CNN
model for thoracic disease classification. We first estimate a
disease-dependent spatial probability, i.e., an anatomical prior, that
indicates the probability of occurrence of a disease in a specific region in a
chest X-ray image. Next, we develop a novel attention-based classification
model that combines information from the estimated anatomical prior and
automatically extracted chest region of interest (ROI) masks to provide
attention to the feature maps generated from a deep convolution network. Unlike
previous works that utilize various self-attention mechanisms, the proposed
method leverages the extracted chest ROI masks along with the probabilistic
anatomical prior information, which selects the region of interest for
different diseases to provide attention. Results: The proposed method shows
superior performance in disease classification on the NIH ChestX-ray14 dataset
compared to existing state-of-the-art methods while reaching an area under the
ROC curve (%AUC) of 84.67. Regarding disease localization, the anatomy prior
attention method shows competitive performance compared to state-of-the-art
methods, achieving an accuracy of 0.80, 0.63, 0.49, 0.33, 0.28, 0.21, and 0.04
with an Intersection over Union (IoU) threshold of 0.1, 0.2, 0.3, 0.4, 0.5,
0.6, and 0.7, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hossain_M/0/1/0/all/0/1&quot;&gt;Md. Iqbal Hossain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zunaed_M/0/1/0/all/0/1&quot;&gt;Mohammad Zunaed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ahmed_M/0/1/0/all/0/1&quot;&gt;Md. Kawsar Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hossain_S/0/1/0/all/0/1&quot;&gt;S. M. Jawwad Hossain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hasan_A/0/1/0/all/0/1&quot;&gt;Anwarul Hasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hasan_T/0/1/0/all/0/1&quot;&gt;Taufiq Hasan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.14404">
<title>Adversarial Purification with the Manifold Hypothesis. (arXiv:2210.14404v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.14404</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we formulate a novel framework for adversarial robustness using
the manifold hypothesis. This framework provides sufficient conditions for
defending against adversarial examples. We develop an adversarial purification
method with this framework. Our method combines manifold learning with
variational inference to provide adversarial robustness without the need for
expensive adversarial training. Experimentally, our approach can provide
adversarial robustness even if attackers are aware of the existence of the
defense. In addition, our method can also serve as a test-time defense
mechanism for variational autoencoders.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhaoyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hartley_R/0/1/0/all/0/1&quot;&gt;Richard Hartley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_P/0/1/0/all/0/1&quot;&gt;Peter Tu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.15136">
<title>3D Shape Knowledge Graph for Cross-domain 3D Shape Retrieval. (arXiv:2210.15136v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.15136</link>
<description rdf:parseType="Literal">&lt;p&gt;The surge in 3D modeling has led to a pronounced research emphasis on the
field of 3D shape retrieval. Numerous contemporary approaches have been put
forth to tackle this intricate challenge. Nevertheless, effectively addressing
the intricacies of cross-modal 3D shape retrieval remains a formidable
undertaking, owing to inherent modality-based disparities. This study presents
an innovative notion, termed &quot;geometric words&quot;, which functions as elemental
constituents for representing entities through combinations. To establish the
knowledge graph, we employ geometric words as nodes, connecting them via shape
categories and geometry attributes. Subsequently, we devise a unique graph
embedding method for knowledge acquisition. Finally, an effective similarity
measure is introduced for retrieval purposes. Importantly, each 3D or 2D entity
can anchor its geometric terms within the knowledge graph, thereby serving as a
link between cross-domain data. As a result, our approach facilitates multiple
cross-domain 3D shape retrieval tasks. We evaluate the proposed method&apos;s
performance on the ModelNet40 and ShapeNetCore55 datasets, encompassing
scenarios related to 3D shape retrieval and cross-domain retrieval.
Furthermore, we employ the established cross-modal dataset (MI3DOR) to assess
cross-modal 3D shape retrieval. The resulting experimental outcomes, in
conjunction with comparisons against state-of-the-art techniques, clearly
highlight the superiority of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_R/0/1/0/all/0/1&quot;&gt;Rihao Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yongtao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_T/0/1/0/all/0/1&quot;&gt;Tong Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_W/0/1/0/all/0/1&quot;&gt;Weizhi Nie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.07864">
<title>Federated Adaptive Prompt Tuning for Multi-domain Collaborative Learning. (arXiv:2211.07864v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.07864</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) enables multiple clients to collaboratively train a
global model without disclosing their data. Previous researches often require
training the complete model parameters. However, the emergence of powerful
pre-trained models makes it possible to achieve higher performance with fewer
learnable parameters in FL. In this paper, we propose a federated adaptive
prompt tuning algorithm, FedAPT, for multi-domain collaborative image
classification with powerful foundation models, like CLIP. Compared with direct
federated prompt tuning, our core idea is to adaptively unlock specific domain
knowledge for each test sample in order to provide them with personalized
prompts. To implement this idea, we design an adaptive prompt tuning module,
which consists of a meta prompt, an adaptive network, and some keys. The server
randomly generates a set of keys and assigns a unique key to each client. Then
all clients cooperatively train the global adaptive network and meta prompt
with the local datasets and the frozen keys. Ultimately, the global aggregation
model can assign a personalized prompt to CLIP based on the domain features of
each test sample. We perform extensive experiments on two multi-domain image
classification datasets across two different settings -- supervised and
unsupervised. The results show that FedAPT can achieve better performance with
less than 10\% of the number of parameters of the fully trained model, and the
global model can perform well in diverse client domains simultaneously.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1&quot;&gt;Shangchao Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Mingzhao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1&quot;&gt;Xiangyang Xue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.13495">
<title>Few-shot Object Detection with Refined Contrastive Learning. (arXiv:2211.13495v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.13495</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the scarcity of sampling data in reality, few-shot object detection
(FSOD) has drawn more and more attention because of its ability to quickly
train new detection concepts with less data. However, there are still failure
identifications due to the difficulty in distinguishing confusable classes. We
also notice that the high standard deviation of average precision reveals the
inconsistent detection performance. To this end, we propose a novel FSOD method
with Refined Contrastive Learning (FSRC). A pre-determination component is
introduced to find out the Resemblance Group from novel classes which contains
confusable classes. Afterwards, Refined Contrastive Learning (RCL) is pointedly
performed on this group of classes in order to increase the inter-class
distances among them. In the meantime, the detection results distribute more
uniformly which further improve the performance. Experimental results based on
PASCAL VOC and COCO datasets demonstrate our proposed method outperforms the
current state-of-the-art research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shangguan_Z/0/1/0/all/0/1&quot;&gt;Zeyu Shangguan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huai_L/0/1/0/all/0/1&quot;&gt;Lian Huai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xingqun Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.14742">
<title>Dynamic Feature Pruning and Consolidation for Occluded Person Re-Identification. (arXiv:2211.14742v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.14742</link>
<description rdf:parseType="Literal">&lt;p&gt;Occluded person re-identification (ReID) is a challenging problem due to
contamination from occluders. Existing approaches address the issue with prior
knowledge cues, such as human body key points and semantic segmentations, which
easily fail in the presence of heavy occlusion and other humans as occluders.
In this paper, we propose a feature pruning and consolidation (FPC) framework
to circumvent explicit human structure parsing. The framework mainly consists
of a sparse encoder, a multi-view feature mathcing module, and a feature
consolidation decoder. Specifically, the sparse encoder drops less important
image tokens, mostly related to background noise and occluders, solely based on
correlation within the class token attention. Subsequently, the matching stage
relies on the preserved tokens produced by the sparse encoder to identify
k-nearest neighbors in the gallery by measuring the image and patch-level
combined similarity. Finally, we use the feature consolidation module to
compensate pruned features using identified neighbors for recovering essential
information while disregarding disturbance from noise and occlusion.
Experimental results demonstrate the effectiveness of our proposed framework on
occluded, partial, and holistic Re-ID datasets. In particular, our method
outperforms state-of-the-art results by at least 8.6\% mAP and 6.0\% Rank-1
accuracy on the challenging Occluded-Duke dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1&quot;&gt;YuTeng Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1&quot;&gt;Jiale Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Chenxing Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Youjia Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junle Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1&quot;&gt;Qiang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Junqing Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wei Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.00114">
<title>Skeletal Video Anomaly Detection using Deep Learning: Survey, Challenges and Future Directions. (arXiv:2301.00114v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.00114</link>
<description rdf:parseType="Literal">&lt;p&gt;The existing methods for video anomaly detection mostly utilize videos
containing identifiable facial and appearance-based features. The use of videos
with identifiable faces raises privacy concerns, especially when used in a
hospital or community-based setting. Appearance-based features can also be
sensitive to pixel-based noise, straining the anomaly detection methods to
model the changes in the background and making it difficult to focus on the
actions of humans in the foreground. Structural information in the form of
skeletons describing the human motion in the videos is privacy-protecting and
can overcome some of the problems posed by appearance-based features. In this
paper, we present a survey of privacy-protecting deep learning anomaly
detection methods using skeletons extracted from videos. We present a novel
taxonomy of algorithms based on the various learning approaches. We conclude
that skeleton-based approaches for anomaly detection can be a plausible
privacy-protecting alternative for video anomaly detection. Lastly, we identify
major open research questions and provide guidelines to address them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_P/0/1/0/all/0/1&quot;&gt;Pratik K. Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mihailidis_A/0/1/0/all/0/1&quot;&gt;Alex Mihailidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Shehroz S. Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.01841">
<title>Classification of Single Tree Decay Stages from Combined Airborne LiDAR Data and CIR Imagery. (arXiv:2301.01841v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.01841</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding forest health is of great importance for the conservation of
the integrity of forest ecosystems. In this regard, evaluating the amount and
quality of dead wood is of utmost interest as they are favorable indicators of
biodiversity. Apparently, remote sensing-based machine learning techniques have
proven to be more efficient and sustainable with unprecedented accuracy in
forest inventory. This study, for the first time, automatically categorizing
individual coniferous trees (Norway spruce) into five decay stages (live,
declining, dead, loose bark, and clean) from combined airborne laser scanning
(ALS) point clouds and color infrared (CIR) images using three different
Machine Learning methods - 3D point cloud-based deep learning (KPConv),
Convolutional Neural Network (CNN), and Random Forest (RF). First, CIR
colorized point clouds are created by fusing the ALS point clouds and color
infrared images. Then, individual tree segmentation is conducted, after which
the results are further projected onto four orthogonal planes. Finally, the
classification is conducted on the two datasets (3D multispectral point clouds
and 2D projected images) based on the three Machine Learning algorithms. All
models achieved promising results, reaching overall accuracy (OA) of up to
88.8%, 88.4% and 85.9% for KPConv, CNN and RF, respectively. The experimental
results reveal that color information, 3D coordinates, and intensity of point
clouds have significant impact on the promising classification performance. The
performance of our models, therefore, shows the significance of machine/deep
learning for individual tree decay stages classification and landscape-wide
assessment of the dead wood amount and quality by using modern airborne remote
sensing techniques. The proposed method can contribute as an important and
reliable tool for monitoring biodiversity in forest ecosystems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_T/0/1/0/all/0/1&quot;&gt;Tsz Chung Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sani_Mohammed_A/0/1/0/all/0/1&quot;&gt;Abubakar Sani-Mohammed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinhong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Puzuo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1&quot;&gt;Wei Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heurich_M/0/1/0/all/0/1&quot;&gt;Marco Heurich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.00586">
<title>FAIR-Ensemble: When Fairness Naturally Emerges From Deep Ensembling. (arXiv:2303.00586v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2303.00586</link>
<description rdf:parseType="Literal">&lt;p&gt;Ensembling multiple Deep Neural Networks (DNNs) is a simple and effective way
to improve top-line metrics and to outperform a larger single model. In this
work, we go beyond top-line metrics and instead explore the impact of
ensembling on subgroup performances. Surprisingly, we observe that even with a
simple homogeneous ensemble -- all the individual DNNs share the same training
set, architecture, and design choices -- the minority group performance
disproportionately improves with the number of models compared to the majority
group, i.e. fairness naturally emerges from ensembling. Even more surprising,
we find that this gain keeps occurring even when a large number of models is
considered, e.g. $20$, despite the fact that the average performance of the
ensemble plateaus with fewer models. Our work establishes that simple DNN
ensembles can be a powerful tool for alleviating disparate impact from DNN
classifiers, thus curbing algorithmic harm. We also explore why this is the
case. We find that even in homogeneous ensembles, varying the sources of
stochasticity through parameter initialization, mini-batch sampling, and
data-augmentation realizations, results in different fairness outcomes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ko_W/0/1/0/all/0/1&quot;&gt;Wei-Yin Ko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dsouza_D/0/1/0/all/0/1&quot;&gt;Daniel D&amp;#x27;souza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nguyen_K/0/1/0/all/0/1&quot;&gt;Karina Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Balestriero_R/0/1/0/all/0/1&quot;&gt;Randall Balestriero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hooker_S/0/1/0/all/0/1&quot;&gt;Sara Hooker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.06088">
<title>Towards domain-invariant Self-Supervised Learning with Batch Styles Standardization. (arXiv:2303.06088v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.06088</link>
<description rdf:parseType="Literal">&lt;p&gt;In Self-Supervised Learning (SSL), models are typically pretrained,
fine-tuned, and evaluated on the same domains. However, they tend to perform
poorly when evaluated on unseen domains, a challenge that Unsupervised Domain
Generalization (UDG) seeks to address. Current UDG methods rely on domain
labels, which are often challenging to collect, and domain-specific
architectures that lack scalability when confronted with numerous domains,
making the current methodology impractical and rigid. Inspired by
contrastive-based UDG methods that mitigate spurious correlations by
restricting comparisons to examples from the same domain, we hypothesize that
eliminating style variability within a batch could provide a more convenient
and flexible way to reduce spurious correlations without requiring domain
labels. To verify this hypothesis, we introduce Batch Styles Standardization
(BSS), a relatively simple yet powerful Fourier-based method to standardize the
style of images in a batch specifically designed for integration with SSL
methods to tackle UDG. Combining BSS with existing SSL methods offers serious
advantages over prior UDG methods: (1) It eliminates the need for domain labels
or domain-specific network components to enhance domain-invariance in SSL
representations, and (2) offers flexibility as BSS can be seamlessly integrated
with diverse contrastive-based but also non-contrastive-based SSL methods.
Experiments on several UDG datasets demonstrate that it significantly improves
downstream task performances on unseen domains, often outperforming or rivaling
with UDG methods. Finally, this work clarifies the underlying mechanisms
contributing to BSS&apos;s effectiveness in improving domain-invariance in SSL
representations and performances on unseen domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scalbert_M/0/1/0/all/0/1&quot;&gt;Marin Scalbert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vakalopoulou_M/0/1/0/all/0/1&quot;&gt;Maria Vakalopoulou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Couzinie_Devy_F/0/1/0/all/0/1&quot;&gt;Florent Couzini&amp;#xe9;-Devy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.03693">
<title>Model-Agnostic Gender Debiased Image Captioning. (arXiv:2304.03693v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.03693</link>
<description rdf:parseType="Literal">&lt;p&gt;Image captioning models are known to perpetuate and amplify harmful societal
bias in the training set. In this work, we aim to mitigate such gender bias in
image captioning models. While prior work has addressed this problem by forcing
models to focus on people to reduce gender misclassification, it conversely
generates gender-stereotypical words at the expense of predicting the correct
gender. From this observation, we hypothesize that there are two types of
gender bias affecting image captioning models: 1) bias that exploits context to
predict gender, and 2) bias in the probability of generating certain (often
stereotypical) words because of gender. To mitigate both types of gender
biases, we propose a framework, called LIBRA, that learns from synthetically
biased samples to decrease both types of biases, correcting gender
misclassification and changing gender-stereotypical words to more neutral ones.
Code is available at https://github.com/rebnej/LIBRA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hirota_Y/0/1/0/all/0/1&quot;&gt;Yusuke Hirota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakashima_Y/0/1/0/all/0/1&quot;&gt;Yuta Nakashima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_N/0/1/0/all/0/1&quot;&gt;Noa Garcia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.08506">
<title>When SAM Meets Medical Images: An Investigation of Segment Anything Model (SAM) on Multi-phase Liver Tumor Segmentation. (arXiv:2304.08506v6 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.08506</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning to segmentation without large-scale samples is an inherent
capability of human. Recently, Segment Anything Model (SAM) performs the
significant zero-shot image segmentation, attracting considerable attention
from the computer vision community. Here, we investigate the capability of SAM
for medical image analysis, especially for multi-phase liver tumor segmentation
(MPLiTS), in terms of prompts, data resolution, phases. Experimental results
demonstrate that there might be a large gap between SAM and expected
performance. Fortunately, the qualitative results show that SAM is a powerful
annotation tool for the community of interactive medical image segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_C/0/1/0/all/0/1&quot;&gt;Chuanfei Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xia_T/0/1/0/all/0/1&quot;&gt;Tianyi Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ju_S/0/1/0/all/0/1&quot;&gt;Shenghong Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xinde Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.09854">
<title>Transformer-Based Visual Segmentation: A Survey. (arXiv:2304.09854v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.09854</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual segmentation seeks to partition images, video frames, or point clouds
into multiple segments or groups. This technique has numerous real-world
applications, such as autonomous driving, image editing, robot sensing, and
medical analysis. Over the past decade, deep learning-based methods have made
remarkable strides in this area. Recently, transformers, a type of neural
network based on self-attention originally designed for natural language
processing, have considerably surpassed previous convolutional or recurrent
approaches in various vision processing tasks. Specifically, vision
transformers offer robust, unified, and even simpler solutions for various
segmentation tasks. This survey provides a thorough overview of
transformer-based visual segmentation, summarizing recent advancements. We
first review the background, encompassing problem definitions, datasets, and
prior convolutional methods. Next, we summarize a meta-architecture that
unifies all recent transformer-based approaches. Based on this
meta-architecture, we examine various method designs, including modifications
to the meta-architecture and associated applications. We also present several
closely related settings, including 3D point cloud segmentation, foundation
model tuning, domain-aware segmentation, efficient segmentation, and medical
segmentation. Additionally, we compile and re-evaluate the reviewed methods on
several well-established datasets. Finally, we identify open challenges in this
field and propose directions for future research. The project page can be found
at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer. We will also
continually monitor developments in this rapidly evolving field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiangtai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Henghui Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1&quot;&gt;Haobo Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1&quot;&gt;Jiangmiao Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1&quot;&gt;Guangliang Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1&quot;&gt;Chen Change Loy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.03815">
<title>Persistent Homology Meets Object Unity: Object Recognition in Clutter. (arXiv:2305.03815v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.03815</link>
<description rdf:parseType="Literal">&lt;p&gt;Recognition of occluded objects in unseen and unstructured indoor
environments is a challenging problem for mobile robots. To address this
challenge, we propose a new descriptor, TOPS, for point clouds generated from
depth images and an accompanying recognition framework, THOR, inspired by human
reasoning. The descriptor employs a novel slicing-based approach to compute
topological features from filtrations of simplicial complexes using persistent
homology, and facilitates reasoning-based recognition using object unity. Apart
from a benchmark dataset, we report performance on a new dataset, the UW Indoor
Scenes (UW-IS) Occluded dataset, curated using commodity hardware to reflect
real-world scenarios with different environmental conditions and degrees of
object occlusion. THOR outperforms state-of-the-art methods on both the
datasets and achieves substantially higher recognition accuracy for all the
scenarios of the UW-IS Occluded dataset. Therefore, THOR, is a promising step
toward robust recognition in low-cost robots, meant for everyday use in indoor
settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samani_E/0/1/0/all/0/1&quot;&gt;Ekta U. Samani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1&quot;&gt;Ashis G. Banerjee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.04743">
<title>MARS: Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation. (arXiv:2305.04743v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.04743</link>
<description rdf:parseType="Literal">&lt;p&gt;Evaluating car damages from misfortune is critical to the car insurance
industry. However, the accuracy is still insufficient for real-world
applications since the deep learning network is not designed for car damage
images as inputs, and its segmented masks are still very coarse. This paper
presents MARS (Mask Attention Refinement with Sequential quadtree nodes) for
car damage instance segmentation. Our MARS represents self-attention mechanisms
to draw global dependencies between the sequential quadtree nodes layer and
quadtree transformer to recalibrate channel weights and predict highly accurate
instance masks. Our extensive experiments demonstrate that MARS outperforms
state-of-the-art (SOTA) instance segmentation methods on three popular
benchmarks such as Mask R-CNN [9], PointRend [13], and Mask Transfiner [12], by
a large margin of +1.3 maskAP-based R50-FPN backbone and +2.3 maskAP-based
R101-FPN backbone on Thai car-damage dataset. Our demos are available at
https://github.com/kaopanboonyuen/MARS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panboonyuen_T/0/1/0/all/0/1&quot;&gt;Teerapong Panboonyuen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nithisopa_N/0/1/0/all/0/1&quot;&gt;Naphat Nithisopa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pienroj_P/0/1/0/all/0/1&quot;&gt;Panin Pienroj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jirachuphun_L/0/1/0/all/0/1&quot;&gt;Laphonchai Jirachuphun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watthanasirikrit_C/0/1/0/all/0/1&quot;&gt;Chaiwasut Watthanasirikrit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pornwiriyakul_N/0/1/0/all/0/1&quot;&gt;Naruepon Pornwiriyakul&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05807">
<title>Even Small Correlation and Diversity Shifts Pose Dataset-Bias Issues. (arXiv:2305.05807v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05807</link>
<description rdf:parseType="Literal">&lt;p&gt;Distribution shifts are common in real-world datasets and can affect the
performance and reliability of deep learning models. In this paper, we study
two types of distribution shifts: diversity shifts, which occur when test
samples exhibit patterns unseen during training, and correlation shifts, which
occur when test data present a different correlation between seen invariant and
spurious features. We propose an integrated protocol to analyze both types of
shifts using datasets where they co-exist in a controllable manner. Finally, we
apply our approach to a real-world classification problem of skin cancer
analysis, using out-of-distribution datasets and specialized bias annotations.
Our protocol reveals three findings: 1) Models learn and propagate correlation
shifts even with low-bias training; this poses a risk of accumulating and
combining unaccountable weak biases; 2) Models learn robust features in high-
and low-bias scenarios but use spurious ones if test samples have them; this
suggests that spurious correlations do not impair the learning of robust
features; 3) Diversity shift can reduce the reliance on spurious correlations;
this is counter intuitive since we expect biased models to depend more on
biases when invariant features are missing. Our work has implications for
distribution shift research and practice, providing new insights into how
models learn and rely on spurious correlations under different types of shifts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bissoto_A/0/1/0/all/0/1&quot;&gt;Alceu Bissoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barata_C/0/1/0/all/0/1&quot;&gt;Catarina Barata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valle_E/0/1/0/all/0/1&quot;&gt;Eduardo Valle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Avila_S/0/1/0/all/0/1&quot;&gt;Sandra Avila&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10300">
<title>One-Prompt to Segment All Medical Images. (arXiv:2305.10300v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10300</link>
<description rdf:parseType="Literal">&lt;p&gt;Large foundation models, known for their strong zero-shot generalization,
have excelled in visual and language applications. However, applying them to
medical image segmentation, a domain with diverse imaging types and target
labels, remains an open challenge. Current approaches, such as adapting
interactive segmentation models like Segment Anything Model (SAM), require user
prompts for each sample during inference. Alternatively, transfer learning
methods like few/one-shot models demand labeled samples, leading to high costs.
This paper introduces a new paradigm toward the universal medical image
segmentation, termed &apos;One-Prompt Segmentation.&apos; One-Prompt Segmentation
combines the strengths of one-shot and interactive methods. In the inference
stage, with just \textbf{one prompted sample}, it can adeptly handle the unseen
task in a single forward pass. We train One-Prompt Model on 64 open-source
medical datasets, accompanied by the collection of over 3,000 clinician-labeled
prompts. Tested on 14 previously unseen tasks, the One-Prompt Model showcases
superior zero-shot segmentation capabilities, outperforming a wide range of
related methods. The code and annotated data will be publicly released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Junde Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jiayuan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuanpei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yueming Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Min Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12743">
<title>Semantic Invariant Multi-view Clustering with Fully Incomplete Information. (arXiv:2305.12743v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12743</link>
<description rdf:parseType="Literal">&lt;p&gt;Robust multi-view learning with incomplete information has received
significant attention due to issues such as incomplete correspondences and
incomplete instances that commonly affect real-world multi-view applications.
Existing approaches heavily rely on paired samples to realign or impute
defective ones, but such preconditions cannot always be satisfied in practice
due to the complexity of data collection and transmission. To address this
problem, we present a novel framework called SeMantic Invariance LEarning
(SMILE) for multi-view clustering with incomplete information that does not
require any paired samples. To be specific, we discover the existence of
invariant semantic distribution across different views, which enables SMILE to
alleviate the cross-view discrepancy to learn consensus semantics without
requiring any paired samples. The resulting consensus semantics remain
unaffected by cross-view distribution shifts, making them useful for
realigning/imputing defective instances and forming clusters. We demonstrate
the effectiveness of SMILE through extensive comparison experiments with 13
state-of-the-art baselines on five benchmarks. Our approach improves the
clustering accuracy of NoisyMNIST from 19.3\%/23.2\% to 82.7\%/69.0\% when the
correspondences/instances are fully incomplete. The code could be accessed from
https://pengxi.me.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_P/0/1/0/all/0/1&quot;&gt;Pengxin Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Mouxing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yiding Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Changqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1&quot;&gt;Peng Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1&quot;&gt;Xi Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15194">
<title>DiffBlender: Scalable and Composable Multimodal Text-to-Image Diffusion Models. (arXiv:2305.15194v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15194</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we aim to extend the capabilities of diffusion-based
text-to-image (T2I) generation models by incorporating diverse modalities
beyond textual description, such as sketch, box, color palette, and style
embedding, within a single model. We thus design a multimodal T2I diffusion
model, coined as DiffBlender, by separating the channels of conditions into
three types, i.e., image forms, spatial tokens, and non-spatial tokens. The
unique architecture of DiffBlender facilitates adding new input modalities,
pioneering a scalable framework for conditional image generation. Notably, we
achieve this without altering the parameters of the existing generative model,
Stable Diffusion, only with updating partial components. Our study establishes
new benchmarks in multimodal generation through quantitative and qualitative
comparisons with existing conditional generation methods. We demonstrate that
DiffBlender faithfully blends all the provided information and showcase its
various applications in the detailed image synthesis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sungnyun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Junsoo Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_K/0/1/0/all/0/1&quot;&gt;Kibeom Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Daesik Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahn_N/0/1/0/all/0/1&quot;&gt;Namhyuk Ahn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16150">
<title>Unifying GANs and Score-Based Diffusion as Generative Particle Models. (arXiv:2305.16150v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16150</link>
<description rdf:parseType="Literal">&lt;p&gt;Particle-based deep generative models, such as gradient flows and score-based
diffusion models, have recently gained traction thanks to their striking
performance. Their principle of displacing particle distributions using
differential equations is conventionally seen as opposed to the previously
widespread generative adversarial networks (GANs), which involve training a
pushforward generator network. In this paper we challenge this interpretation,
and propose a novel framework that unifies particle and adversarial generative
models by framing generator training as a generalization of particle models.
This suggests that a generator is an optional addition to any such generative
model. Consequently, integrating a generator into a score-based diffusion model
and training a GAN without a generator naturally emerge from our framework. We
empirically test the viability of these original models as proofs of concepts
of potential applications of our framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Franceschi_J/0/1/0/all/0/1&quot;&gt;Jean-Yves Franceschi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gartrell_M/0/1/0/all/0/1&quot;&gt;Mike Gartrell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_L/0/1/0/all/0/1&quot;&gt;Ludovic Dos Santos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Issenhuth_T/0/1/0/all/0/1&quot;&gt;Thibaut Issenhuth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bezenac_E/0/1/0/all/0/1&quot;&gt;Emmanuel de B&amp;#xe9;zenac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Micka&amp;#xeb;l Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rakotomamonjy_A/0/1/0/all/0/1&quot;&gt;Alain Rakotomamonjy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18273">
<title>Pix2Repair: Implicit Shape Restoration from Images. (arXiv:2305.18273v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18273</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Pix2Repair, an automated shape repair approach that generates
restoration shapes from images to repair fractured objects. Prior repair
approaches require a high-resolution watertight 3D mesh of the fractured object
as input. Input 3D meshes must be obtained using expensive 3D scanners, and
scanned meshes require manual cleanup, limiting accessibility and scalability.
Pix2Repair takes an image of the fractured object as input and automatically
generates a 3D printable restoration shape. We contribute a novel shape
function that deconstructs a latent code representing the fractured object into
a complete shape and a break surface. We also introduce Fantastic Breaks
Imaged, the first large-scale dataset of 11,653 real-world images of fractured
objects for training and evaluating image-based shape repair approaches. Our
dataset contains images of objects from Fantastic Breaks, complete with rich
annotations. We show restorations for real fractures from our dataset, and for
synthetic fractures from the Geometric Breaks and Breaking Bad datasets. Our
approach outperforms shape completion approaches adapted for shape repair in
terms of chamfer distance, normal consistency, and percent restorations
generated.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1&quot;&gt;Xinchao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lamb_N/0/1/0/all/0/1&quot;&gt;Nikolas Lamb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1&quot;&gt;Sean Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_N/0/1/0/all/0/1&quot;&gt;Natasha Kholgade Banerjee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18295">
<title>RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths. (arXiv:2305.18295v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18295</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-image generation has recently witnessed remarkable achievements. We
introduce a text-conditional image diffusion model, termed RAPHAEL, to generate
highly artistic images, which accurately portray the text prompts, encompassing
multiple nouns, adjectives, and verbs. This is achieved by stacking tens of
mixture-of-experts (MoEs) layers, i.e., space-MoE and time-MoE layers, enabling
billions of diffusion paths (routes) from the network input to the output. Each
path intuitively functions as a &quot;painter&quot; for depicting a particular textual
concept onto a specified image region at a diffusion timestep. Comprehensive
experiments reveal that RAPHAEL outperforms recent cutting-edge models, such as
Stable Diffusion, ERNIE-ViLG 2.0, DeepFloyd, and DALL-E 2, in terms of both
image quality and aesthetic appeal. Firstly, RAPHAEL exhibits superior
performance in switching images across diverse styles, such as Japanese comics,
realism, cyberpunk, and ink illustration. Secondly, a single model with three
billion parameters, trained on 1,000 A100 GPUs for two months, achieves a
state-of-the-art zero-shot FID score of 6.61 on the COCO dataset. Furthermore,
RAPHAEL significantly surpasses its counterparts in human evaluation on the
ViLG-300 benchmark. We believe that RAPHAEL holds the potential to propel the
frontiers of image generation research in both academia and industry, paving
the way for future breakthroughs in this rapidly evolving field. More details
can be found on a webpage: https://raphael-painter.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1&quot;&gt;Zeyue Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1&quot;&gt;Guanglu Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qiushan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Boxiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1&quot;&gt;Zhuofan Zong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.01423">
<title>Improving Gradient-Trend Identification: Fast-Adaptive Moment Estimation with Finance-Inspired Triple Exponential Moving Average. (arXiv:2306.01423v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.01423</link>
<description rdf:parseType="Literal">&lt;p&gt;The performance improvement of deep networks significantly depends on their
optimizers. With existing optimizers, precise and efficient recognition of the
gradients trend remains a challenge. Existing optimizers predominantly adopt
techniques based on the first-order exponential moving average (EMA), which
results in noticeable delays that impede the real-time tracking of gradients
trend and consequently yield sub-optimal performance. To overcome this
limitation, we introduce a novel optimizer called fast-adaptive moment
estimation (FAME). Inspired by the triple exponential moving average (TEMA)
used in the financial domain, FAME leverages the potency of higher-order TEMA
to improve the precision of identifying gradient trends. TEMA plays a central
role in the learning process as it actively influences optimization dynamics;
this role differs from its conventional passive role as a technical indicator
in financial contexts. Because of the introduction of TEMA into the
optimization process, FAME can identify gradient trends with higher accuracy
and fewer lag issues, thereby offering smoother and more consistent responses
to gradient fluctuations compared to conventional first-order EMA. To study the
effectiveness of our novel FAME optimizer, we conducted comprehensive
experiments encompassing six diverse computer-vision benchmarks and tasks,
spanning detection, classification, and semantic comprehension. We integrated
FAME into 15 learning architectures and compared its performance with those of
six popular optimizers. Results clearly showed that FAME is more robust and
accurate and provides superior performance stability by minimizing noise (i.e.,
trend fluctuations). Notably, FAME achieves higher accuracy levels in
remarkably fewer training epochs than its counterparts, clearly indicating its
significance for optimizing deep networks in computer-vision tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peleg_R/0/1/0/all/0/1&quot;&gt;Roi Peleg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lazebnik_T/0/1/0/all/0/1&quot;&gt;Teddy Lazebnik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoogi_A/0/1/0/all/0/1&quot;&gt;Assaf Hoogi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07915">
<title>Image Captioners Are Scalable Vision Learners Too. (arXiv:2306.07915v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07915</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive pretraining on image-text pairs from the web is one of the most
popular large-scale pretraining strategies for vision backbones, especially in
the context of large multimodal models. At the same time, image captioning on
this type of data is commonly considered an inferior pretraining strategy. In
this paper, we perform a fair comparison of these two pretraining strategies,
carefully matching training data, compute, and model capacity. Using a standard
encoder-decoder transformer, we find that captioning alone is surprisingly
effective: on classification tasks, captioning produces vision encoders
competitive with contrastively pretrained encoders, while surpassing them on
vision &amp;amp; language tasks. We further analyze the effect of the model
architecture and scale, as well as the pretraining data on the representation
quality, and find that captioning exhibits the same or better scaling behavior
along these axes. Overall our results show that plain image captioning is a
more powerful pretraining strategy than was previously believed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tschannen_M/0/1/0/all/0/1&quot;&gt;Michael Tschannen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1&quot;&gt;Manoj Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steiner_A/0/1/0/all/0/1&quot;&gt;Andreas Steiner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1&quot;&gt;Xiaohua Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1&quot;&gt;Neil Houlsby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1&quot;&gt;Lucas Beyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09077">
<title>Estimating Generic 3D Room Structures from 2D Annotations. (arXiv:2306.09077v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09077</link>
<description rdf:parseType="Literal">&lt;p&gt;Indoor rooms are among the most common use cases in 3D scene understanding.
Current state-of-the-art methods for this task are driven by large annotated
datasets. Room layouts are especially important, consisting of structural
elements in 3D, such as wall, floor, and ceiling. However, they are difficult
to annotate, especially on pure RGB video. We propose a novel method to produce
generic 3D room layouts just from 2D segmentation masks, which are easy to
annotate for humans. Based on these 2D annotations, we automatically
reconstruct 3D plane equations for the structural elements and their spatial
extent in the scene, and connect adjacent elements at the appropriate contact
edges. We annotate and publicly release 2246 3D room layouts on the
RealEstate10k dataset, containing YouTube videos. We demonstrate the high
quality of these 3D layouts annotations with extensive experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rozumnyi_D/0/1/0/all/0/1&quot;&gt;Denys Rozumnyi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Popov_S/0/1/0/all/0/1&quot;&gt;Stefan Popov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maninis_K/0/1/0/all/0/1&quot;&gt;Kevis-Kokitsi Maninis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1&quot;&gt;Matthias Nie&amp;#xdf;ner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1&quot;&gt;Vittorio Ferrari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00764">
<title>Hierarchical Open-vocabulary Universal Image Segmentation. (arXiv:2307.00764v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00764</link>
<description rdf:parseType="Literal">&lt;p&gt;Open-vocabulary image segmentation aims to partition an image into semantic
regions according to arbitrary text descriptions. However, complex visual
scenes can be naturally decomposed into simpler parts and abstracted at
multiple levels of granularity, introducing inherent segmentation ambiguity.
Unlike existing methods that typically sidestep this ambiguity and treat it as
an external factor, our approach actively incorporates a hierarchical
representation encompassing different semantic-levels into the learning
process. We propose a decoupled text-image fusion mechanism and representation
learning modules for both &quot;things&quot; and &quot;stuff&quot;. Additionally, we systematically
examine the differences that exist in the textual and visual features between
these types of categories. Our resulting model, named HIPIE, tackles
HIerarchical, oPen-vocabulary, and unIvErsal segmentation tasks within a
unified framework. Benchmarked on over 40 datasets, e.g., ADE20K, COCO,
Pascal-VOC Part, RefCOCO/RefCOCOg, ODinW and SeginW, HIPIE achieves the
state-of-the-art results at various levels of image comprehension, including
semantic-level (e.g., semantic segmentation), instance-level (e.g.,
panoptic/referring segmentation and object detection), as well as part-level
(e.g., part/subpart segmentation) tasks. Our code is released at
https://github.com/berkeley-hipie/HIPIE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xudong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shufan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kallidromitis_K/0/1/0/all/0/1&quot;&gt;Konstantinos Kallidromitis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kato_Y/0/1/0/all/0/1&quot;&gt;Yusuke Kato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kozuka_K/0/1/0/all/0/1&quot;&gt;Kazuki Kozuka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1&quot;&gt;Trevor Darrell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15254">
<title>Multiple Instance Learning Framework with Masked Hard Instance Mining for Whole Slide Image Classification. (arXiv:2307.15254v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.15254</link>
<description rdf:parseType="Literal">&lt;p&gt;The whole slide image (WSI) classification is often formulated as a multiple
instance learning (MIL) problem. Since the positive tissue is only a small
fraction of the gigapixel WSI, existing MIL methods intuitively focus on
identifying salient instances via attention mechanisms. However, this leads to
a bias towards easy-to-classify instances while neglecting hard-to-classify
instances. Some literature has revealed that hard examples are beneficial for
modeling a discriminative boundary accurately. By applying such an idea at the
instance level, we elaborate a novel MIL framework with masked hard instance
mining (MHIM-MIL), which uses a Siamese structure (Teacher-Student) with a
consistency constraint to explore the potential hard instances. With several
instance masking strategies based on attention scores, MHIM-MIL employs a
momentum teacher to implicitly mine hard instances for training the student
model, which can be any attention-based MIL model. This counter-intuitive
strategy essentially enables the student to learn a better discriminating
boundary. Moreover, the student is used to update the teacher with an
exponential moving average (EMA), which in turn identifies new hard instances
for subsequent training iterations and stabilizes the optimization.
Experimental results on the CAMELYON-16 and TCGA Lung Cancer datasets
demonstrate that MHIM-MIL outperforms other latest methods in terms of
performance and training cost. The code is available at:
https://github.com/DearCaat/MHIM-MIL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1&quot;&gt;Wenhao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Sheng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoxian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1&quot;&gt;Fengtao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bo Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15588">
<title>OAFuser: Towards Omni-Aperture Fusion for Light Field Semantic Segmentation. (arXiv:2307.15588v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.15588</link>
<description rdf:parseType="Literal">&lt;p&gt;Light field cameras, by harnessing the power of micro-lens array, are capable
of capturing intricate angular and spatial details. This allows for acquiring
complex light patterns and details from multiple angles, significantly
enhancing the precision of image semantic segmentation, a critical aspect of
scene interpretation in vision intelligence. However, the extensive angular
information of light field cameras contains a large amount of redundant data,
which is overwhelming for the limited hardware resources of intelligent
vehicles. Besides, inappropriate compression leads to information corruption
and data loss. To excavate representative information, we propose a new
paradigm, Omni-Aperture Fusion model (OAFuser), which leverages dense context
from the central view and discovers the angular information from sub-aperture
images to generate a semantically consistent result. To avoid feature loss
during network propagation and simultaneously streamline the redundant
information from the light field camera, we present a simple yet very effective
Sub-Aperture Fusion Module (SAFM) to embed sub-aperture images into angular
features without any additional memory cost. Furthermore, to address the
mismatched spatial information across viewpoints, we present a Center Angular
Rectification Module (CARM) to realize feature resorting and prevent feature
occlusion caused by asymmetric information. Our proposed OAFuser achieves
state-of-the-art performance on the UrbanLF-Real and -Syn datasets and sets a
new record of 84.93% in mIoU on the UrbanLF-Real Extended dataset, with a gain
of +4.53%. The source code of OAFuser will be available at
https://github.com/FeiBryantkit/OAFuser.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teng_F/0/1/0/all/0/1&quot;&gt;Fei Teng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiaming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1&quot;&gt;Kunyu Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaonan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1&quot;&gt;Rainer Stiefelhagen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kailun Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.16586">
<title>SAMFlow: Eliminating Any Fragmentation in Optical Flow with Segment Anything Model. (arXiv:2307.16586v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.16586</link>
<description rdf:parseType="Literal">&lt;p&gt;Optical Flow Estimation aims to find the 2D dense motion field between two
frames. Due to the limitation of model structures and training datasets,
existing methods often rely too much on local clues and ignore the integrity of
objects, resulting in fragmented motion estimation. Through theoretical
analysis, we find the pre-trained large vision models are helpful in optical
flow estimation, and we notice that the recently famous Segment Anything Model
(SAM) demonstrates a strong ability to segment complete objects, which is
suitable for solving the fragmentation problem. We thus propose a solution to
embed the frozen SAM image encoder into FlowFormer to enhance object
perception. To address the challenge of in-depth utilizing SAM in
non-segmentation tasks like optical flow estimation, we propose an Optical Flow
Task-Specific Adaption scheme, including a Context Fusion Module to fuse the
SAM encoder with the optical flow context encoder, and a Context Adaption
Module to adapt the SAM features for optical flow task with Learned
Task-Specific Embedding. Our proposed SAMFlow model reaches 0.86/2.10
clean/final EPE and 3.55/12.32 EPE/F1-all on Sintel and KITTI-15 training set,
surpassing Flowformer by 8.5%/9.9% and 13.2%/16.3%. Furthermore, our model
achieves state-of-the-art performance on the Sintel and KITTI-15 benchmarks,
ranking #1 among all two-frame methods on Sintel clean pass.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shili Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1&quot;&gt;Ruian He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1&quot;&gt;Weimin Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1&quot;&gt;Bo Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01196">
<title>Sustainable Transparency in Recommender Systems: Bayesian Ranking of Images for Explainability. (arXiv:2308.01196v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2308.01196</link>
<description rdf:parseType="Literal">&lt;p&gt;Recommender Systems have become crucial in the modern world, commonly guiding
users towards relevant content or products, and having a large influence over
the decisions of users and citizens. However, ensuring transparency and user
trust in these systems remains a challenge; personalized explanations have
emerged as a solution, offering justifications for recommendations. Among the
existing approaches for generating personalized explanations, using existing
visual content created by users is a promising option to maximize transparency
and user trust. State-of-the-art models that follow this approach, despite
leveraging highly optimized architectures, employ surrogate learning tasks that
do not efficiently model the objective of ranking images as explanations for a
given recommendation; this leads to a suboptimal training process with high
computational costs that may not be reduced without affecting model
performance. This work presents BRIE, a novel model where we leverage Bayesian
Pairwise Ranking to enhance the training process, allowing us to consistently
outperform state-of-the-art models in six real-world datasets while reducing
its model size by up to 64 times and its CO${_2}$ emissions by up to 75% in
training and inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paz_Ruza_J/0/1/0/all/0/1&quot;&gt;Jorge Paz-Ruza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alonso_Betanzos_A/0/1/0/all/0/1&quot;&gt;Amparo Alonso-Betanzos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guijarro_Berdinas_B/0/1/0/all/0/1&quot;&gt;Berta Guijarro-Berdi&amp;#xf1;as&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cancela_B/0/1/0/all/0/1&quot;&gt;Brais Cancela&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eiras_Franco_C/0/1/0/all/0/1&quot;&gt;Carlos Eiras-Franco&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.06668">
<title>Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges. (arXiv:2308.06668v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.06668</link>
<description rdf:parseType="Literal">&lt;p&gt;The past decade has witnessed the rapid development of ML and DL
methodologies in agricultural systems, showcased by great successes in variety
of agricultural applications. However, these conventional ML/DL models have
certain limitations: They heavily rely on large, costly-to-acquire labeled
datasets for training, require specialized expertise for development and
maintenance, and are mostly tailored for specific tasks, thus lacking
generalizability. Recently, foundation models have demonstrated remarkable
successes in language and vision tasks across various domains. These models are
trained on a vast amount of data from multiple domains and modalities. Once
trained, they can accomplish versatile tasks with just minor fine-tuning and
minimal task-specific labeled data. Despite their proven effectiveness and huge
potential, there has been little exploration of applying FMs to agriculture
fields. Therefore, this study aims to explore the potential of FMs in the field
of smart agriculture. In particular, we present conceptual tools and technical
background to facilitate the understanding of the problem space and uncover new
research directions in this field. To this end, we first review recent FMs in
the general computer science domain and categorize them into four categories:
language FMs, vision FMs, multimodal FMs, and reinforcement learning FMs.
Subsequently, we outline the process of developing agriculture FMs and discuss
their potential applications in smart agriculture. We also discuss the unique
challenges associated with developing AFMs, including model training,
validation, and deployment. Through this study, we contribute to the
advancement of AI in agriculture by introducing AFMs as a promising paradigm
that can significantly mitigate the reliance on extensive labeled datasets and
enhance the efficiency, effectiveness, and generalization of agricultural AI
systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiajia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Mingle Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1&quot;&gt;Lirong Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Dong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_W/0/1/0/all/0/1&quot;&gt;Weichao Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1&quot;&gt;Xunyuan Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhaojian Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.07528">
<title>Confidence Contours: Uncertainty-Aware Annotation for Medical Semantic Segmentation. (arXiv:2308.07528v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.07528</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical image segmentation modeling is a high-stakes task where understanding
of uncertainty is crucial for addressing visual ambiguity. Prior work has
developed segmentation models utilizing probabilistic or generative mechanisms
to infer uncertainty from labels where annotators draw a singular boundary.
However, as these annotations cannot represent an individual annotator&apos;s
uncertainty, models trained on them produce uncertainty maps that are difficult
to interpret. We propose a novel segmentation representation, Confidence
Contours, which uses high- and low-confidence ``contours&apos;&apos; to capture
uncertainty directly, and develop a novel annotation system for collecting
contours. We conduct an evaluation on the Lung Image Dataset Consortium (LIDC)
and a synthetic dataset. From an annotation study with 30 participants, results
show that Confidence Contours provide high representative capacity without
considerably higher annotator effort. We also find that general-purpose
segmentation models can learn Confidence Contours at the same performance level
as standard singular annotations. Finally, from interviews with 5 medical
experts, we find that Confidence Contour maps are more interpretable than
Bayesian maps due to representation of structural uncertainty.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_A/0/1/0/all/0/1&quot;&gt;Andre Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Quan Ze Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Amy Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.08638">
<title>Fair GANs through model rebalancing for extremely imbalanced class distributions. (arXiv:2308.08638v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.08638</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep generative models require large amounts of training data. This often
poses a problem as the collection of datasets can be expensive and difficult,
in particular datasets that are representative of the appropriate underlying
distribution (e.g. demographic). This introduces biases in datasets which are
further propagated in the models. We present an approach to construct an
unbiased generative adversarial network (GAN) from an existing biased GAN by
rebalancing the model distribution. We do so by generating balanced data from
an existing imbalanced deep generative model using an evolutionary algorithm
and then using this data to train a balanced generative model. Additionally, we
propose a bias mitigation loss function that minimizes the deviation of the
learned class distribution from being equiprobable. We show results for the
StyleGAN2 models while training on the Flickr Faces High Quality (FFHQ) dataset
for racial fairness and see that the proposed approach improves on the fairness
metric by almost 5 times, whilst maintaining image quality. We further validate
our approach by applying it to an imbalanced CIFAR10 dataset where we show that
we can obtain comparable fairness and image quality as when training on a
balanced CIFAR10 dataset which is also twice as large. Lastly, we argue that
the traditionally used image quality metrics such as Frechet inception distance
(FID) are unsuitable for scenarios where the class distributions are imbalanced
and a balanced reference set is not available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1&quot;&gt;Anubhav Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Memon_N/0/1/0/all/0/1&quot;&gt;Nasir Memon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Togelius_J/0/1/0/all/0/1&quot;&gt;Julian Togelius&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.08746">
<title>SurgicalSAM: Efficient Class Promptable Surgical Instrument Segmentation. (arXiv:2308.08746v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.08746</link>
<description rdf:parseType="Literal">&lt;p&gt;The Segment Anything Model (SAM) is a powerful foundation model that has
revolutionised image segmentation. To apply SAM to surgical instrument
segmentation, a common approach is to locate precise points or boxes of
instruments and then use them as prompts for SAM in a zero-shot manner.
However, we observe two problems with this naive pipeline: (1) the domain gap
between natural objects and surgical instruments leads to inferior
generalisation of SAM; and (2) SAM relies on precise point or box locations for
accurate segmentation, requiring either extensive manual guidance or a
well-performing specialist detector for prompt preparation, which leads to a
complex multi-stage pipeline. To address these problems, we introduce
SurgicalSAM, a novel end-to-end efficient-tuning approach for SAM to
effectively integrate surgical-specific information with SAM&apos;s pre-trained
knowledge for improved generalisation. Specifically, we propose a lightweight
prototype-based class prompt encoder for tuning, which directly generates
prompt embeddings from class prototypes and eliminates the use of explicit
prompts for improved robustness and a simpler pipeline. In addition, to address
the low inter-class variance among surgical instrument categories, we propose
contrastive prototype learning, further enhancing the discrimination of the
class prototypes for more accurate class prompting. The results of extensive
experiments on both EndoVis2018 and EndoVis2017 datasets demonstrate that
SurgicalSAM achieves state-of-the-art performance while only requiring a small
number of tunable parameters. The source code is available at
https://github.com/wenxi-yue/SurgicalSAM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_W/0/1/0/all/0/1&quot;&gt;Wenxi Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1&quot;&gt;Kun Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yong Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Jiebo Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.10045">
<title>An Empirical Study of CLIP for Text-based Person Search. (arXiv:2308.10045v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.10045</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-based Person Search (TBPS) aims to retrieve the person images using
natural language descriptions. Recently, Contrastive Language Image Pretraining
(CLIP), a universal large cross-modal vision-language pre-training model, has
remarkably performed over various cross-modal downstream tasks due to its
powerful cross-modal semantic learning capacity. TPBS, as a fine-grained
cross-modal retrieval task, is also facing the rise of research on the
CLIP-based TBPS. In order to explore the potential of the visual-language
pre-training model for downstream TBPS tasks, this paper makes the first
attempt to conduct a comprehensive empirical study of CLIP for TBPS and thus
contribute a straightforward, incremental, yet strong TBPS-CLIP baseline to the
TBPS community. We revisit critical design considerations under CLIP, including
data augmentation and loss function. The model, with the aforementioned designs
and practical training tricks, can attain satisfactory performance without any
sophisticated modules. Also, we conduct the probing experiments of TBPS-CLIP in
model generalization and model compression, demonstrating the effectiveness of
TBPS-CLIP from various aspects. This work is expected to provide empirical
insights and highlight future CLIP-based TBPS research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1&quot;&gt;Min Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yang Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1&quot;&gt;Ziyin Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1&quot;&gt;Mang Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Min Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07277">
<title>Limitations of Face Image Generation. (arXiv:2309.07277v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.07277</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-image diffusion models have achieved widespread popularity due to
their unprecedented image generation capability. In particular, their ability
to synthesize and modify human faces has spurred research into using generated
face images in both training data augmentation and model performance
assessments. In this paper, we study the efficacy and shortcomings of
generative models in the context of face generation. Utilizing a combination of
qualitative and quantitative measures, including embedding-based metrics and
user studies, we present a framework to audit the characteristics of generated
faces conditioned on a set of social attributes. We applied our framework on
faces generated through state-of-the-art text-to-image diffusion models. We
identify several limitations of face image generation that include faithfulness
to the text prompt, demographic disparities, and distributional shifts.
Furthermore, we present an analytical model that provides insights into how
training data selection contributes to the performance of generative models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosenberg_H/0/1/0/all/0/1&quot;&gt;Harrison Rosenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1&quot;&gt;Shimaa Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramesh_G/0/1/0/all/0/1&quot;&gt;Guruprasad V Ramesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vinayak_R/0/1/0/all/0/1&quot;&gt;Ramya Korlakai Vinayak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fawaz_K/0/1/0/all/0/1&quot;&gt;Kassem Fawaz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.08154">
<title>Dynamic Visual Semantic Sub-Embeddings and Fast Re-Ranking. (arXiv:2309.08154v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.08154</link>
<description rdf:parseType="Literal">&lt;p&gt;The core of cross-modal matching is to accurately measure the similarity
between different modalities in a unified representation space. However,
compared to textual descriptions of a certain perspective, the visual modality
has more semantic variations. So, images are usually associated with multiple
textual captions in databases. Although popular symmetric embedding methods
have explored numerous modal interaction approaches, they often learn toward
increasing the average expression probability of multiple semantic variations
within image embeddings. Consequently, information entropy in embeddings is
increased, resulting in redundancy and decreased accuracy. In this work, we
propose a Dynamic Visual Semantic Sub-Embeddings framework (DVSE) to reduce the
information entropy. Specifically, we obtain a set of heterogeneous visual
sub-embeddings through dynamic orthogonal constraint loss. To encourage the
generated candidate embeddings to capture various semantic variations, we
construct a mixed distribution and employ a variance-aware weighting loss to
assign different weights to the optimization process. In addition, we develop a
Fast Re-ranking strategy (FR) to efficiently evaluate the retrieval results and
enhance the performance. We compare the performance with existing set-based
method using four image feature encoders and two text feature encoders on three
benchmark datasets: MSCOCO, Flickr30K and CUB Captions. We also show the role
of different components by ablation studies and perform a sensitivity analysis
of the hyperparameters. The qualitative analysis of visualized bidirectional
retrieval and attention maps further demonstrates the ability of our method to
encode semantic variations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1&quot;&gt;Wenzhang Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gui_Z/0/1/0/all/0/1&quot;&gt;Zhipeng Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Changguang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_A/0/1/0/all/0/1&quot;&gt;Anqi Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1&quot;&gt;Dehua Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Huayi Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.08738">
<title>AV-MaskEnhancer: Enhancing Video Representations through Audio-Visual Masked Autoencoder. (arXiv:2309.08738v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.08738</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning high-quality video representation has shown significant applications
in computer vision and remains challenging. Previous work based on mask
autoencoders such as ImageMAE and VideoMAE has proven the effectiveness of
learning representations in images and videos through reconstruction strategy
in the visual modality. However, these models exhibit inherent limitations,
particularly in scenarios where extracting features solely from the visual
modality proves challenging, such as when dealing with low-resolution and
blurry original videos. Based on this, we propose AV-MaskEnhancer for learning
high-quality video representation by combining visual and audio information.
Our approach addresses the challenge by demonstrating the complementary nature
of audio and video features in cross-modality content. Moreover, our result of
the video classification task on the UCF101 dataset outperforms the existing
work and reaches the state-of-the-art, with a top-1 accuracy of 98.8% and a
top-5 accuracy of 99.9%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diao_X/0/1/0/all/0/1&quot;&gt;Xingjian Diao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1&quot;&gt;Ming Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;Shitong Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12559">
<title>Invariant Learning via Probability of Sufficient and Necessary Causes. (arXiv:2309.12559v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.12559</link>
<description rdf:parseType="Literal">&lt;p&gt;Out-of-distribution (OOD) generalization is indispensable for learning models
in the wild, where testing distribution typically unknown and different from
the training. Recent methods derived from causality have shown great potential
in achieving OOD generalization. However, existing methods mainly focus on the
invariance property of causes, while largely overlooking the property of
\textit{sufficiency} and \textit{necessity} conditions. Namely, a necessary but
insufficient cause (feature) is invariant to distribution shift, yet it may not
have required accuracy. By contrast, a sufficient yet unnecessary cause
(feature) tends to fit specific data well but may have a risk of adapting to a
new domain. To capture the information of sufficient and necessary causes, we
employ a classical concept, the probability of sufficiency and necessary causes
(PNS), which indicates the probability of whether one is the necessary and
sufficient cause. To associate PNS with OOD generalization, we propose PNS risk
and formulate an algorithm to learn representation with a high PNS value. We
theoretically analyze and prove the generalizability of the PNS risk.
Experiments on both synthetic and real-world benchmarks demonstrate the
effectiveness of the proposed method. The details of the implementation can be
found at the GitHub repository: https://github.com/ymy4323460/CaSN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Mengyue Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1&quot;&gt;Zhen Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yonggang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yali Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Furui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ton_J/0/1/0/all/0/1&quot;&gt;Jean-Francois Ton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianhong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12780">
<title>LMC: Large Model Collaboration with Cross-assessment for Training-Free Open-Set Object Recognition. (arXiv:2309.12780v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.12780</link>
<description rdf:parseType="Literal">&lt;p&gt;Open-set object recognition aims to identify if an object is from a class
that has been encountered during training or not. To perform open-set object
recognition accurately, a key challenge is how to reduce the reliance on
spurious-discriminative features. In this paper, motivated by that different
large models pre-trained through different paradigms can possess very rich
while distinct implicit knowledge, we propose a novel framework named Large
Model Collaboration (LMC) to tackle the above challenge via collaborating
different off-the-shelf large models in a training-free manner. Moreover, we
also incorporate the proposed framework with several novel designs to
effectively extract implicit knowledge from large models. Extensive experiments
demonstrate the efficacy of our proposed framework. Code is available
https://github.com/Harryqu123/LMC
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1&quot;&gt;Haoxuan Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hui_X/0/1/0/all/0/1&quot;&gt;Xiaofei Hui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1&quot;&gt;Yujun Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jun Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04247">
<title>Semantic segmentation of longitudinal thermal images for identification of hot and cool spots in urban areas. (arXiv:2310.04247v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04247</link>
<description rdf:parseType="Literal">&lt;p&gt;This work presents the analysis of semantically segmented, longitudinally,
and spatially rich thermal images collected at the neighborhood scale to
identify hot and cool spots in urban areas. An infrared observatory was
operated over a few months to collect thermal images of different types of
buildings on the educational campus of the National University of Singapore. A
subset of the thermal image dataset was used to train state-of-the-art deep
learning models to segment various urban features such as buildings,
vegetation, sky, and roads. It was observed that the U-Net segmentation model
with `resnet34&apos; CNN backbone has the highest mIoU score of 0.99 on the test
dataset, compared to other models such as DeepLabV3, DeeplabV3+, FPN, and
PSPnet. The masks generated using the segmentation models were then used to
extract the temperature from thermal images and correct for differences in the
emissivity of various urban features. Further, various statistical measure of
the temperature extracted using the predicted segmentation masks is shown to
closely match the temperature extracted using the ground truth masks. Finally,
the masks were used to identify hot and cool spots in the urban feature at
various instances of time. This forms one of the very few studies demonstrating
the automated analysis of thermal images, which can be of potential use to
urban planners for devising mitigation strategies for reducing the urban heat
island (UHI) effect, improving building energy efficiency, and maximizing
outdoor thermal comfort.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramani_V/0/1/0/all/0/1&quot;&gt;Vasantha Ramani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arjunan_P/0/1/0/all/0/1&quot;&gt;Pandarasamy Arjunan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poolla_K/0/1/0/all/0/1&quot;&gt;Kameshwar Poolla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miller_C/0/1/0/all/0/1&quot;&gt;Clayton Miller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.14859">
<title>3M-TRANSFORMER: A Multi-Stage Multi-Stream Multimodal Transformer for Embodied Turn-Taking Prediction. (arXiv:2310.14859v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.14859</link>
<description rdf:parseType="Literal">&lt;p&gt;Predicting turn-taking in multiparty conversations has many practical
applications in human-computer/robot interaction. However, the complexity of
human communication makes it a challenging task. Recent advances have shown
that synchronous multi-perspective egocentric data can significantly improve
turn-taking prediction compared to asynchronous, single-perspective
transcriptions. Building on this research, we propose a new multimodal
transformer-based architecture for predicting turn-taking in embodied,
synchronized multi-perspective data. Our experimental results on the recently
introduced EgoCom dataset show a substantial performance improvement of up to
14.01% on average compared to existing baselines and alternative
transformer-based approaches. The source code, and the pre-trained models of
our 3M-Transformer will be available upon acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fatan_M/0/1/0/all/0/1&quot;&gt;Mehdi Fatan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mincato_E/0/1/0/all/0/1&quot;&gt;Emanuele Mincato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pintzou_D/0/1/0/all/0/1&quot;&gt;Dimitra Pintzou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dimiccoli_M/0/1/0/all/0/1&quot;&gt;Mariella Dimiccoli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16898">
<title>MCUFormer: Deploying Vision Transformers on Microcontrollers with Limited Memory. (arXiv:2310.16898v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.16898</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the high price and heavy energy consumption of GPUs, deploying deep
models on IoT devices such as microcontrollers makes significant contributions
for ecological AI. Conventional methods successfully enable convolutional
neural network inference of high resolution images on microcontrollers, while
the framework for vision transformers that achieve the state-of-the-art
performance in many vision applications still remains unexplored. In this
paper, we propose a hardware-algorithm co-optimizations method called MCUFormer
to deploy vision transformers on microcontrollers with extremely limited
memory, where we jointly design transformer architecture and construct the
inference operator library to fit the memory resource constraint. More
specifically, we generalize the one-shot network architecture search (NAS) to
discover the optimal architecture with highest task performance given the
memory budget from the microcontrollers, where we enlarge the existing search
space of vision transformers by considering the low-rank decomposition
dimensions and patch resolution for memory reduction. For the construction of
the inference operator library of vision transformers, we schedule the memory
buffer during inference through operator integration, patch embedding
decomposition, and token overwriting, allowing the memory buffer to be fully
utilized to adapt to the forward pass of the vision transformer. Experimental
results demonstrate that our MCUFormer achieves 73.62\% top-1 accuracy on
ImageNet for image classification with 320KB memory on STM32F746
microcontroller. Code is available at https://github.com/liangyn22/MCUFormer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yinan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiuwei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yansong Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jie Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiwen Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19583">
<title>GC-MVSNet: Multi-View, Multi-Scale, Geometrically-Consistent Multi-View Stereo. (arXiv:2310.19583v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19583</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional multi-view stereo (MVS) methods rely heavily on photometric and
geometric consistency constraints, but newer machine learning-based MVS methods
check geometric consistency across multiple source views only as a
post-processing step. In this paper, we present a novel approach that
explicitly encourages geometric consistency of reference view depth maps across
multiple source views at different scales during learning (see Fig. 1). We find
that adding this geometric consistency loss significantly accelerates learning
by explicitly penalizing geometrically inconsistent pixels, reducing the
training iteration requirements to nearly half that of other MVS methods. Our
extensive experiments show that our approach achieves a new state-of-the-art on
the DTU and BlendedMVS datasets, and competitive results on the Tanks and
Temples benchmark. To the best of our knowledge, GC-MVSNet is the first attempt
to enforce multi-view, multi-scale geometric consistency during learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vats_V/0/1/0/all/0/1&quot;&gt;Vibhas K. Vats&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1&quot;&gt;Sripad Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crandall_D/0/1/0/all/0/1&quot;&gt;David J. Crandall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reza_M/0/1/0/all/0/1&quot;&gt;Md. Alimoor Reza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1&quot;&gt;Soon-heung Jung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02358">
<title>Domain Transfer in Latent Space (DTLS) Wins on Image Super-Resolution -- a Non-Denoising Model. (arXiv:2311.02358v4 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02358</link>
<description rdf:parseType="Literal">&lt;p&gt;Large scale image super-resolution is a challenging computer vision task,
since vast information is missing in a highly degraded image, say for example
forscale x16 super-resolution. Diffusion models are used successfully in recent
years in extreme super-resolution applications, in which Gaussian noise is used
as a means to form a latent photo-realistic space, and acts as a link between
the space of latent vectors and the latent photo-realistic space. There are
quite a few sophisticated mathematical derivations on mapping the statistics of
Gaussian noises making Diffusion Models successful. In this paper we propose a
simple approach which gets away from using Gaussian noise but adopts some basic
structures of diffusion models for efficient image super-resolution.
Essentially, we propose a DNN to perform domain transfer between neighbor
domains, which can learn the differences in statistical properties to
facilitate gradual interpolation with results of reasonable quality. Further
quality improvement is achieved by conditioning the domain transfer with
reference to the input LR image. Experimental results show that our method
outperforms not only state-of-the-art large scale super resolution models, but
also the current diffusion models for image super-resolution. The approach can
readily be extended to other image-to-image tasks, such as image enlightening,
inpainting, denoising, etc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hui_C/0/1/0/all/0/1&quot;&gt;Chun-Chuen Hui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Siu_W/0/1/0/all/0/1&quot;&gt;Wan-Chi Siu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Law_N/0/1/0/all/0/1&quot;&gt;Ngai-Fong Law&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03830">
<title>Reducing Spatial Fitting Error in Distillation of Denoising Diffusion Models. (arXiv:2311.03830v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03830</link>
<description rdf:parseType="Literal">&lt;p&gt;Denoising Diffusion models have exhibited remarkable capabilities in image
generation. However, generating high-quality samples requires a large number of
iterations. Knowledge distillation for diffusion models is an effective method
to address this limitation with a shortened sampling process but causes
degraded generative quality. Based on our analysis with bias-variance
decomposition and experimental observations, we attribute the degradation to
the spatial fitting error occurring in the training of both the teacher and
student model. Accordingly, we propose $\textbf{S}$patial
$\textbf{F}$itting-$\textbf{E}$rror $\textbf{R}$eduction
$\textbf{D}$istillation model ($\textbf{SFERD}$). SFERD utilizes attention
guidance from the teacher model and a designed semantic gradient predictor to
reduce the student&apos;s fitting error. Empirically, our proposed model facilitates
high-quality sample generation in a few function evaluations. We achieve an FID
of 5.31 on CIFAR-10 and 9.39 on ImageNet 64$\times$64 with only one step,
outperforming existing diffusion methods. Our study provides a new perspective
on diffusion distillation by highlighting the intrinsic denoising ability of
models. Project link: \url{https://github.com/Sainzerjj/SFERD}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shengzhe Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Z/0/1/0/all/0/1&quot;&gt;Zejian Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shengyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1&quot;&gt;Lefan Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Changyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Lingyun Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05152">
<title>Cross-modal Prompts: Adapting Large Pre-trained Models for Audio-Visual Downstream Tasks. (arXiv:2311.05152v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.05152</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the deployment of large-scale pre-trained models in
audio-visual downstream tasks has yielded remarkable outcomes. However, these
models, primarily trained on single-modality unconstrained datasets, still
encounter challenges in feature extraction for multi-modal tasks, leading to
suboptimal performance. This limitation arises due to the introduction of
irrelevant modality-specific information during encoding, which adversely
affects the performance of downstream tasks. To address this challenge, this
paper proposes a novel Dual-Guided Spatial-Channel-Temporal (DG-SCT) attention
mechanism. This mechanism leverages audio and visual modalities as soft prompts
to dynamically adjust the parameters of pre-trained models based on the current
multi-modal input features. Specifically, the DG-SCT module incorporates
trainable cross-modal interaction layers into pre-trained audio-visual
encoders, allowing adaptive extraction of crucial information from the current
modality across spatial, channel, and temporal dimensions, while preserving the
frozen parameters of large-scale pre-trained models. Experimental evaluations
demonstrate that our proposed model achieves state-of-the-art results across
multiple downstream tasks, including AVE, AVVP, AVS, and AVQA. Furthermore, our
model exhibits promising performance in challenging few-shot and zero-shot
scenarios. The source code and pre-trained models are available at
https://github.com/haoyi-duan/DG-SCT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_H/0/1/0/all/0/1&quot;&gt;Haoyi Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yan Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mingze Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1&quot;&gt;Li Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jieming Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhou Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07967">
<title>Comparison of two data fusion approaches for land use classification. (arXiv:2311.07967v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.07967</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate land use maps, describing the territory from an anthropic
utilisation point of view, are useful tools for land management and planning.
To produce them, the use of optical images alone remains limited. It is
therefore necessary to make use of several heterogeneous sources, each carrying
complementary or contradictory information due to their imperfections or their
different specifications. This study compares two different approaches i.e. a
pre-classification and a post-classification fusion approach for combining
several sources of spatial data in the context of land use classification. The
approaches are applied on authoritative land use data located in the Gers
department in the southwest of France. Pre-classification fusion, while not
explicitly modeling imperfections, has the best final results, reaching an
overall accuracy of 97% and a macro-mean F1 score of 88%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cubaud_M/0/1/0/all/0/1&quot;&gt;Martin Cubaud&lt;/a&gt; (LaSTIG), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bris_A/0/1/0/all/0/1&quot;&gt;Arnaud Le Bris&lt;/a&gt; (LaSTIG), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jolivet_L/0/1/0/all/0/1&quot;&gt;Laurence Jolivet&lt;/a&gt; (LaSTIG), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olteanu_Raimond_A/0/1/0/all/0/1&quot;&gt;Ana-Maria Olteanu-Raimond&lt;/a&gt; (LaSTIG)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11059">
<title>HIDRO-VQA: High Dynamic Range Oracle for Video Quality Assessment. (arXiv:2311.11059v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11059</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce HIDRO-VQA, a no-reference (NR) video quality assessment model
designed to provide precise quality evaluations of High Dynamic Range (HDR)
videos. HDR videos exhibit a broader spectrum of luminance, detail, and color
than Standard Dynamic Range (SDR) videos. As HDR content becomes increasingly
popular, there is a growing demand for video quality assessment (VQA)
algorithms that effectively address distortions unique to HDR content. To
address this challenge, we propose a self-supervised contrastive fine-tuning
approach to transfer quality-aware features from the SDR to the HDR domain,
utilizing unlabeled HDR videos. Our findings demonstrate that self-supervised
pre-trained neural networks on SDR content can be further fine-tuned in a
self-supervised setting using limited unlabeled HDR videos to achieve
state-of-the-art performance on the only publicly available VQA database for
HDR content, the LIVE-HDR VQA database. Moreover, our algorithm can be extended
to the Full Reference VQA setting, also achieving state-of-the-art performance.
Our code is available publicly at https://github.com/avinabsaha/HIDRO-VQA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saini_S/0/1/0/all/0/1&quot;&gt;Shreshth Saini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1&quot;&gt;Avinab Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bovik_A/0/1/0/all/0/1&quot;&gt;Alan C. Bovik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14981">
<title>Multi-task Planar Reconstruction with Feature Warping Guidance. (arXiv:2311.14981v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.14981</link>
<description rdf:parseType="Literal">&lt;p&gt;Piece-wise planar 3D reconstruction simultaneously segments plane instances
and recovers their 3D plane parameters from an image, which is particularly
useful for indoor or man-made environments. Efficient reconstruction of 3D
planes coupled with semantic predictions offers advantages for a wide range of
applications requiring scene understanding and concurrent spatial mapping.
However, most existing planar reconstruction models either neglect semantic
predictions or do not run efficiently enough for real-time applications. We
introduce SOLOPlanes, a real-time planar reconstruction model based on a
modified instance segmentation architecture which simultaneously predicts
semantics for each plane instance, along with plane parameters and piece-wise
plane instance masks. We achieve an improvement in instance mask segmentation
by including multi-view guidance for plane predictions in the training process.
This cross-task improvement, training for plane prediction but improving the
mask segmentation, is due to the nature of feature sharing in multi-task
learning. Our model simultaneously predicts semantics using single images at
inference time, while achieving real-time predictions at 43 FPS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1&quot;&gt;Luan Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hilsmann_A/0/1/0/all/0/1&quot;&gt;Anna Hilsmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eisert_P/0/1/0/all/0/1&quot;&gt;Peter Eisert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16512">
<title>CoSeR: Bridging Image and Language for Cognitive Super-Resolution. (arXiv:2311.16512v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16512</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing super-resolution (SR) models primarily focus on restoring local
texture details, often neglecting the global semantic information within the
scene. This oversight can lead to the omission of crucial semantic details or
the introduction of inaccurate textures during the recovery process. In our
work, we introduce the Cognitive Super-Resolution (CoSeR) framework, empowering
SR models with the capacity to comprehend low-resolution images. We achieve
this by marrying image appearance and language understanding to generate a
cognitive embedding, which not only activates prior information from large
text-to-image diffusion models but also facilitates the generation of
high-quality reference images to optimize the SR process. To further improve
image fidelity, we propose a novel condition injection scheme called
&quot;All-in-Attention&quot;, consolidating all conditional information into a single
module. Consequently, our method successfully restores semantically correct and
photorealistic details, demonstrating state-of-the-art performance across
multiple benchmarks. Code: https://github.com/VINHYU/CoSeR
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Haoze Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenbo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jianzhuang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haoyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_R/0/1/0/all/0/1&quot;&gt;Renjing Pei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1&quot;&gt;Xueyi Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Youliang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yujiu Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.18260">
<title>Consensus, dissensus and synergy between clinicians and specialist foundation models in radiology report generation. (arXiv:2311.18260v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.18260</link>
<description rdf:parseType="Literal">&lt;p&gt;Radiology reports are an instrumental part of modern medicine, informing key
clinical decisions such as diagnosis and treatment. The worldwide shortage of
radiologists, however, restricts access to expert care and imposes heavy
workloads, contributing to avoidable errors and delays in report delivery.
While recent progress in automated report generation with vision-language
models offer clear potential in ameliorating the situation, the path to
real-world adoption has been stymied by the challenge of evaluating the
clinical quality of AI-generated reports. In this study, we build a
state-of-the-art report generation system for chest radiographs,
$\textit{Flamingo-CXR}$, by fine-tuning a well-known vision-language foundation
model on radiology data. To evaluate the quality of the AI-generated reports, a
group of 16 certified radiologists provide detailed evaluations of AI-generated
and human written reports for chest X-rays from an intensive care setting in
the United States and an inpatient setting in India. At least one radiologist
(out of two per case) preferred the AI report to the ground truth report in
over 60$\%$ of cases for both datasets. Amongst the subset of AI-generated
reports that contain errors, the most frequently cited reasons were related to
the location and finding, whereas for human written reports, most mistakes were
related to severity and finding. This disparity suggested potential
complementarity between our AI system and human experts, prompting us to
develop an assistive scenario in which Flamingo-CXR generates a first-draft
report, which is subsequently revised by a clinician. This is the first
demonstration of clinician-AI collaboration for report writing, and the
resultant reports are assessed to be equivalent or preferred by at least one
radiologist to reports written by experts alone in 80$\%$ of in-patient cases
and 60$\%$ of intensive care cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tanno_R/0/1/0/all/0/1&quot;&gt;Ryutaro Tanno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Barrett_D/0/1/0/all/0/1&quot;&gt;David G.T. Barrett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sellergren_A/0/1/0/all/0/1&quot;&gt;Andrew Sellergren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ghaisas_S/0/1/0/all/0/1&quot;&gt;Sumedh Ghaisas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dathathri_S/0/1/0/all/0/1&quot;&gt;Sumanth Dathathri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+See_A/0/1/0/all/0/1&quot;&gt;Abigail See&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Welbl_J/0/1/0/all/0/1&quot;&gt;Johannes Welbl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Singhal_K/0/1/0/all/0/1&quot;&gt;Karan Singhal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Azizi_S/0/1/0/all/0/1&quot;&gt;Shekoofeh Azizi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tu_T/0/1/0/all/0/1&quot;&gt;Tao Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schaekermann_M/0/1/0/all/0/1&quot;&gt;Mike Schaekermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+May_R/0/1/0/all/0/1&quot;&gt;Rhys May&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_R/0/1/0/all/0/1&quot;&gt;Roy Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Man_S/0/1/0/all/0/1&quot;&gt;SiWai Man&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ahmed_Z/0/1/0/all/0/1&quot;&gt;Zahra Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mahdavi_S/0/1/0/all/0/1&quot;&gt;Sara Mahdavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Matias_Y/0/1/0/all/0/1&quot;&gt;Yossi Matias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Barral_J/0/1/0/all/0/1&quot;&gt;Joelle Barral&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Eslami_A/0/1/0/all/0/1&quot;&gt;Ali Eslami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Belgrave_D/0/1/0/all/0/1&quot;&gt;Danielle Belgrave&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Natarajan_V/0/1/0/all/0/1&quot;&gt;Vivek Natarajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shetty_S/0/1/0/all/0/1&quot;&gt;Shravya Shetty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kohli_P/0/1/0/all/0/1&quot;&gt;Pushmeet Kohli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_P/0/1/0/all/0/1&quot;&gt;Po-Sen Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Karthikesalingam_A/0/1/0/all/0/1&quot;&gt;Alan Karthikesalingam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ktena_I/0/1/0/all/0/1&quot;&gt;Ira Ktena&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03763">
<title>Gaussian3Diff: 3D Gaussian Diffusion for 3D Full Head Synthesis and Editing. (arXiv:2312.03763v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.03763</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel framework for generating photorealistic 3D human head and
subsequently manipulating and reposing them with remarkable flexibility. The
proposed approach leverages an implicit function representation of 3D human
heads, employing 3D Gaussians anchored on a parametric face model. To enhance
representational capabilities and encode spatial information, we embed a
lightweight tri-plane payload within each Gaussian rather than directly storing
color and opacity. Additionally, we parameterize the Gaussians in a 2D UV space
via a 3DMM, enabling effective utilization of the diffusion model for 3D head
avatar generation. Our method facilitates the creation of diverse and realistic
3D human heads with fine-grained editing over facial features and expressions.
Extensive experiments demonstrate the effectiveness of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1&quot;&gt;Yushi Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_F/0/1/0/all/0/1&quot;&gt;Feitong Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_D/0/1/0/all/0/1&quot;&gt;Di Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qiangeng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Genova_K/0/1/0/all/0/1&quot;&gt;Kyle Genova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zeng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fanello_S/0/1/0/all/0/1&quot;&gt;Sean Fanello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandey_R/0/1/0/all/0/1&quot;&gt;Rohit Pandey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Funkhouser_T/0/1/0/all/0/1&quot;&gt;Thomas Funkhouser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1&quot;&gt;Chen Change Loy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yinda Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03775">
<title>FAAC: Facial Animation Generation with Anchor Frame and Conditional Control for Superior Fidelity and Editability. (arXiv:2312.03775v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.03775</link>
<description rdf:parseType="Literal">&lt;p&gt;Over recent years, diffusion models have facilitated significant advancements
in video generation. Yet, the creation of face-related videos still confronts
issues such as low facial fidelity, lack of frame consistency, limited
editability and uncontrollable human poses. To address these challenges, we
introduce a facial animation generation method that enhances both face identity
fidelity and editing capabilities while ensuring frame consistency. This
approach incorporates the concept of an anchor frame to counteract the
degradation of generative ability in original text-to-image models when
incorporating a motion module. We propose two strategies towards this
objective: training-free and training-based anchor frame methods. Our method&apos;s
efficacy has been validated on multiple representative DreamBooth and LoRA
models, delivering substantial improvements over the original outcomes in terms
of facial fidelity, text-to-image editability, and video motion. Moreover, we
introduce conditional control using a 3D parametric face model to capture
accurate facial movements and expressions. This solution augments the creative
possibilities for facial animation generation through the integration of
multiple control signals. For additional samples, please visit
https://paper-faac.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Linze Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1&quot;&gt;Sunqi Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pu_H/0/1/0/all/0/1&quot;&gt;Hengjun Pu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bing_Z/0/1/0/all/0/1&quot;&gt;Zhaodong Bing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1&quot;&gt;Tianzhu Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Tong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Liangyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Jiajun Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06723">
<title>Learning to See Low-Light Images via Feature Domain Adaptation. (arXiv:2312.06723v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06723</link>
<description rdf:parseType="Literal">&lt;p&gt;Raw low light image enhancement (LLIE) has achieved much better performance
than the sRGB domain enhancement methods due to the merits of raw data.
However, the ambiguity between noisy to clean and raw to sRGB mappings may
mislead the single-stage enhancement networks. The two-stage networks avoid
ambiguity by decoupling the two mappings but usually have large computing
complexity. To solve this problem, we propose a single-stage network empowered
by Feature Domain Adaptation (FDA) to decouple the denoising and color mapping
tasks in raw LLIE. The denoising encoder is supervised by the clean raw image,
and then the denoised features are adapted for the color mapping task by an FDA
module. We propose a Lineformer to serve as the FDA, which can well explore the
global and local correlations with fewer line buffers (friendly to the
line-based imaging process). During inference, the raw supervision branch is
removed. In this way, our network combines the advantage of a two-stage
enhancement process with the efficiency of single-stage inference. Experiments
on four benchmark datasets demonstrate that our method achieves
state-of-the-art performance with fewer computing costs (60% FLOPs of the
two-stage method DNF). Our codes will be released after the acceptance of this
work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qirui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1&quot;&gt;Qihua Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_H/0/1/0/all/0/1&quot;&gt;Huanjing Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Le Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yihao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jingyu Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07488">
<title>LMDrive: Closed-Loop End-to-End Driving with Large Language Models. (arXiv:2312.07488v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07488</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite significant recent progress in the field of autonomous driving,
modern methods still struggle and can incur serious accidents when encountering
long-tail unforeseen events and challenging urban scenarios. On the one hand,
large language models (LLM) have shown impressive reasoning capabilities that
approach &quot;Artificial General Intelligence&quot;. On the other hand, previous
autonomous driving methods tend to rely on limited-format inputs (e.g. sensor
data and navigation waypoints), restricting the vehicle&apos;s ability to understand
language information and interact with humans. To this end, this paper
introduces LMDrive, a novel language-guided, end-to-end, closed-loop autonomous
driving framework. LMDrive uniquely processes and integrates multi-modal sensor
data with natural language instructions, enabling interaction with humans and
navigation software in realistic instructional settings. To facilitate further
research in language-based closed-loop autonomous driving, we also publicly
release the corresponding dataset which includes approximately 64K
instruction-following data clips, and the LangAuto benchmark that tests the
system&apos;s ability to handle complex instructions and challenging driving
scenarios. Extensive closed-loop experiments are conducted to demonstrate
LMDrive&apos;s effectiveness. To the best of our knowledge, we&apos;re the very first
work to leverage LLMs for closed-loop end-to-end autonomous driving. Codes,
models, and datasets can be found at https://github.com/opendilab/LMDrive
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_H/0/1/0/all/0/1&quot;&gt;Hao Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Letian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waslander_S/0/1/0/all/0/1&quot;&gt;Steven L. Waslander&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongsheng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07871">
<title>MLNet: Mutual Learning Network with Neighborhood Invariance for Universal Domain Adaptation. (arXiv:2312.07871v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07871</link>
<description rdf:parseType="Literal">&lt;p&gt;Universal domain adaptation (UniDA) is a practical but challenging problem,
in which information about the relation between the source and the target
domains is not given for knowledge transfer. Existing UniDA methods may suffer
from the problems of overlooking intra-domain variations in the target domain
and difficulty in separating between the similar known and unknown class. To
address these issues, we propose a novel Mutual Learning Network (MLNet) with
neighborhood invariance for UniDA. In our method, confidence-guided invariant
feature learning with self-adaptive neighbor selection is designed to reduce
the intra-domain variations for more generalizable feature representation. By
using the cross-domain mixup scheme for better unknown-class identification,
the proposed method compensates for the misidentified known-class errors by
mutual learning between the closed-set and open-set classifiers. Extensive
experiments on three publicly available benchmarks demonstrate that our method
achieves the best results compared to the state-of-the-arts in most cases and
significantly outperforms the baseline across all the four settings in UniDA.
Code is available at https://github.com/YanzuoLu/MLNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yanzuo Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1&quot;&gt;Meng Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_A/0/1/0/all/0/1&quot;&gt;Andy J Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiaohua Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1&quot;&gt;Jian-Huang Lai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08887">
<title>SpeedUpNet: A Plug-and-Play Hyper-Network for Accelerating Text-to-Image Diffusion Models. (arXiv:2312.08887v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08887</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-image diffusion models (SD) exhibit significant advancements while
requiring extensive computational resources. Though many acceleration methods
have been proposed, they suffer from generation quality degradation or extra
training cost generalizing to new fine-tuned models. To address these
limitations, we propose a novel and universal Stable-Diffusion (SD)
acceleration module called SpeedUpNet(SUN). SUN can be directly plugged into
various fine-tuned SD models without extra training. This technique utilizes
cross-attention layers to learn the relative offsets in the generated image
results between negative and positive prompts achieving classifier-free
guidance distillation with negative prompts controllable, and introduces a
Multi-Step Consistency (MSC) loss to ensure a harmonious balance between
reducing inference steps and maintaining consistency in the generated output.
Consequently, SUN significantly reduces the number of inference steps to just 4
steps and eliminates the need for classifier-free guidance. It leads to an
overall speedup of more than 10 times for SD models compared to the
state-of-the-art 25-step DPM-solver++, and offers two extra advantages: (1)
classifier-free guidance distillation with controllable negative prompts and
(2) seamless integration into various fine-tuned Stable-Diffusion models
without training. The effectiveness of the SUN has been verified through
extensive experimentation. Project Page:
https://williechai.github.io/speedup-plugin-for-stable-diffusions.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chai_W/0/1/0/all/0/1&quot;&gt;Weilong Chai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1&quot;&gt;DanDan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jiajiong Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhiquan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Changbao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chenguang Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09709">
<title>ParsNets: A Parsimonious Orthogonal and Low-Rank Linear Networks for Zero-Shot Learning. (arXiv:2312.09709v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.09709</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper provides a novel parsimonious yet efficient design for zero-shot
learning (ZSL), dubbed ParsNets, where we are interested in learning a
composition of on-device friendly linear networks, each with orthogonality and
low-rankness properties, to achieve equivalent or even better performance
against existing deep models. Concretely, we first refactor the core module of
ZSL, i.e., visual-semantics mapping function, into several base linear networks
that correspond to diverse components of the semantic space, where the complex
nonlinearity can be collapsed into simple local linearities. Then, to
facilitate the generalization of local linearities, we construct a maximal
margin geometry on the learned features by enforcing low-rank constraints on
intra-class samples and high-rank constraints on inter-class samples, resulting
in orthogonal subspaces for different classes and each subspace lies on a
compact manifold. To enhance the model&apos;s adaptability and counterbalance
over/under-fittings in ZSL, a set of sample-wise indicators is employed to
select a sparse subset from these base linear networks to form a composite
semantic predictor for each sample. Notably, maximal margin geometry can
guarantee the diversity of features, and meanwhile, local linearities guarantee
efficiency. Thus, our ParsNets can generalize better to unseen classes and can
be deployed flexibly on resource-constrained devices. Theoretical explanations
and extensive experiments are conducted to verify the effectiveness of the
proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jingcai Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1&quot;&gt;Qihua Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ruibing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xiaocheng Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junyang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xin Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10208">
<title>Video-based Surgical Skill Assessment using Tree-based Gaussian Process Classifier. (arXiv:2312.10208v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10208</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper aims to present a novel pipeline for automated surgical skill
assessment using video data and to showcase the effectiveness of the proposed
approach in evaluating surgeon proficiency, its potential for targeted training
interventions, and quality assurance in surgical departments. The pipeline
incorporates a representation flow convolutional neural network and a novel
tree-based Gaussian process classifier, which is robust to noise, while being
computationally efficient. Additionally, new kernels are introduced to enhance
accuracy. The performance of the pipeline is evaluated using the JIGSAWS
dataset. Comparative analysis with existing literature reveals significant
improvement in accuracy and betterment in computation cost. The proposed
pipeline contributes to computational efficiency and accuracy improvement in
surgical skill assessment using video data. Results of our study based on
comments of our colleague surgeons show that the proposed method has the
potential to facilitate skill improvement among surgery fellows and enhance
patient safety through targeted training interventions and quality assurance in
surgical departments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rezaei_A/0/1/0/all/0/1&quot;&gt;Arefeh Rezaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmadi_M/0/1/0/all/0/1&quot;&gt;Mohammad Javad Ahmadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molaei_A/0/1/0/all/0/1&quot;&gt;Amir Molaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taghirad_H/0/1/0/all/0/1&quot;&gt;Hamid. D. Taghirad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10600">
<title>How to Efficiently Annotate Images for Best-Performing Deep Learning Based Segmentation Models: An Empirical Study with Weak and Noisy Annotations and Segment Anything Model. (arXiv:2312.10600v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10600</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) have been deployed for many image segmentation
tasks and achieved outstanding performance. However, preparing a dataset for
training segmentation DNNs is laborious and costly since typically pixel-level
annotations are provided for each object of interest. To alleviate this issue,
one can provide only weak labels such as bounding boxes or scribbles, or less
accurate (noisy) annotations of the objects. These are significantly faster to
generate and thus result in more annotated images given the same time budget.
However, the reduction in quality might negatively affect the segmentation
performance of the resulting model. In this study, we perform a thorough
cost-effectiveness evaluation of several weak and noisy labels. We considered
11 variants of annotation strategies and 4 datasets. We conclude that the
common practice of accurately outlining the objects of interest is virtually
never the optimal approach when the annotation time is limited, even if notable
annotation time is available (10s of hours). Annotation approaches that stood
out in such scenarios were (1) contour-based annotation with rough continuous
traces, (2) polygon-based annotation with few vertices, and (3) box annotations
combined with the Segment Anything Model (SAM). In situations where unlimited
annotation time was available, precise annotations still lead to the highest
segmentation model performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yixin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_H/0/1/0/all/0/1&quot;&gt;Hanxue Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazurowski_M/0/1/0/all/0/1&quot;&gt;Maciej A. Mazurowski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11396">
<title>MAG-Edit: Localized Image Editing in Complex Scenarios via Mask-Based Attention-Adjusted Guidance. (arXiv:2312.11396v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11396</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent diffusion-based image editing approaches have exhibited impressive
editing capabilities in images with simple compositions. However, localized
editing in complex scenarios has not been well-studied in the literature,
despite its growing real-world demands. Existing mask-based inpainting methods
fall short of retaining the underlying structure within the edit region.
Meanwhile, mask-free attention-based methods often exhibit editing leakage and
misalignment in more complex compositions. In this work, we develop MAG-Edit, a
training-free, inference-stage optimization method, which enables localized
image editing in complex scenarios. In particular, MAG-Edit optimizes the noise
latent feature in diffusion models by maximizing two mask-based cross-attention
constraints of the edit token, which in turn gradually enhances the local
alignment with the desired prompt. Extensive quantitative and qualitative
experiments demonstrate the effectiveness of our method in achieving both text
alignment and structure preservation for localized editing within complex
scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Q/0/1/0/all/0/1&quot;&gt;Qi Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yuchao Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1&quot;&gt;Zhen Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1&quot;&gt;Mike Zheng Shou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11460">
<title>Hybrid Internal Model: A Simple and Efficient Learner for Agile Legged Locomotion. (arXiv:2312.11460v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11460</link>
<description rdf:parseType="Literal">&lt;p&gt;Robust locomotion control depends on accurate state estimations. However, the
sensors of most legged robots can only provide partial and noisy observations,
making the estimation particularly challenging, especially for external states
like terrain frictions and elevation maps. Inspired by the classical Internal
Model Control principle, we consider these external states as disturbances and
introduce Hybrid Internal Model (HIM) to estimate them according to the
response of the robot. The response, which we refer to as the hybrid internal
embedding, contains the robot&apos;s explicit velocity and implicit stability
representation, corresponding to two primary goals for locomotion tasks:
explicitly tracking velocity and implicitly maintaining stability. We use
contrastive learning to optimize the embedding to be close to the robot&apos;s
successor state, in which the response is naturally embedded. HIM has several
appealing benefits: It only needs the robot&apos;s proprioceptions, i.e., those from
joint encoders and IMU as observations. It innovatively maintains consistent
observations between simulation reference and reality that avoids information
loss in mimicking learning. It exploits batch-level information that is more
robust to noises and keeps better sample efficiency. It only requires 1 hour of
training on an RTX 4090 to enable a quadruped robot to traverse any terrain
under any disturbances. A wealth of real-world experiments demonstrates its
agility, even in high-difficulty tasks and cases never occurred during the
training process, revealing remarkable open-world generalizability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_J/0/1/0/all/0/1&quot;&gt;Junfeng Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zirui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Quanyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jiawei Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1&quot;&gt;Liu Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1&quot;&gt;Jiangmiao Pang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11562">
<title>A Survey of Reasoning with Foundation Models: Concepts, Methodologies, and Outlook. (arXiv:2312.11562v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11562</link>
<description rdf:parseType="Literal">&lt;p&gt;Reasoning, a crucial ability for complex problem-solving, plays a pivotal
role in various real-world settings such as negotiation, medical diagnosis, and
criminal investigation. It serves as a fundamental methodology in the field of
Artificial General Intelligence (AGI). With the ongoing development of
foundation models, there is a growing interest in exploring their abilities in
reasoning tasks. In this paper, we introduce seminal foundation models proposed
or adaptable for reasoning, highlighting the latest advancements in various
reasoning tasks, methods, and benchmarks. We then delve into the potential
future directions behind the emergence of reasoning abilities within foundation
models. We also discuss the relevance of multimodal learning, autonomous
agents, and super alignment in the context of reasoning. By discussing these
future research directions, we hope to inspire researchers in their exploration
of this field, stimulate further advancements in reasoning with foundation
models, and contribute to the development of AGI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiankai Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Chuanyang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1&quot;&gt;Enze Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhengying Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_R/0/1/0/all/0/1&quot;&gt;Ruihang Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1&quot;&gt;Jianing Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiaqi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1&quot;&gt;Mingyu Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_M/0/1/0/all/0/1&quot;&gt;Mengzhe Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yue Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenhai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junsong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Zhangyue Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xiaozhe Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Junxian He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1&quot;&gt;Wu Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xihui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yu Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Ming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1&quot;&gt;Pheng Ann Heng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1&quot;&gt;Jifeng Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingdong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Ji-Rong Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1&quot;&gt;Xipeng Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yike Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Hui Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenguo Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12337">
<title>pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction. (arXiv:2312.12337v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.12337</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce pixelSplat, a feed-forward model that learns to reconstruct 3D
radiance fields parameterized by 3D Gaussian primitives from pairs of images.
Our model features real-time and memory-efficient rendering for scalable
training as well as fast 3D reconstruction at inference time. To overcome local
minima inherent to sparse and locally supported representations, we predict a
dense probability distribution over 3D and sample Gaussian means from that
probability distribution. We make this sampling operation differentiable via a
reparameterization trick, allowing us to back-propagate gradients through the
Gaussian splatting representation. We benchmark our method on wide-baseline
novel view synthesis on the real-world RealEstate10k and ACID datasets, where
we outperform state-of-the-art light field transformers and accelerate
rendering by 2.5 orders of magnitude while reconstructing an interpretable and
editable 3D radiance field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Charatan_D/0/1/0/all/0/1&quot;&gt;David Charatan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sizhe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1&quot;&gt;Andrea Tagliasacchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sitzmann_V/0/1/0/all/0/1&quot;&gt;Vincent Sitzmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12635">
<title>RealCraft: Attention Control as A Solution for Zero-shot Long Video Editing. (arXiv:2312.12635v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.12635</link>
<description rdf:parseType="Literal">&lt;p&gt;Although large-scale text-to-image generative models have shown promising
performance in synthesizing high-quality images, directly applying these models
to image editing remains a significant challenge. This challenge is further
amplified in video editing due to the additional dimension of time. Especially
for editing real videos as it necessitates maintaining a stable semantic layout
across the frames while executing localized edits precisely without disrupting
the existing backgrounds. In this paper, we propose RealCraft, an
attention-control-based method for zero-shot editing in real videos. By
employing the object-centric manipulation of cross-attention between prompts
and frames and spatial-temporal attention within the frames, we achieve precise
shape-wise editing along with enhanced consistency. Our model can be used
directly with Stable Diffusion and operates without the need for additional
localized information. We showcase our zero-shot attention-control-based method
across a range of videos, demonstrating localized, high-fidelity, shape-precise
and time-consistent editing in videos of various lengths, up to 64 frames.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1&quot;&gt;Shutong Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruiyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pokorny_F/0/1/0/all/0/1&quot;&gt;Florian T. Pokorny&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12763">
<title>AMD:Anatomical Motion Diffusion with Interpretable Motion Decomposition and Fusion. (arXiv:2312.12763v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.12763</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating realistic human motion sequences from text descriptions is a
challenging task that requires capturing the rich expressiveness of both
natural language and human motion.Recent advances in diffusion models have
enabled significant progress in human motion synthesis.However, existing
methods struggle to handle text inputs that describe complex or long motions.In
this paper, we propose the Adaptable Motion Diffusion (AMD) model, which
leverages a Large Language Model (LLM) to parse the input text into a sequence
of concise and interpretable anatomical scripts that correspond to the target
motion.This process exploits the LLM&apos;s ability to provide anatomical guidance
for complex motion synthesis.We then devise a two-branch fusion scheme that
balances the influence of the input text and the anatomical scripts on the
inverse diffusion process, which adaptively ensures the semantic fidelity and
diversity of the synthesized motion.Our method can effectively handle texts
with complex or long motion descriptions, where existing methods often fail.
Experiments on datasets with relatively more complex motions, such as CLCD1 and
CLCD2, demonstrate that our AMD significantly outperforms existing
state-of-the-art models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_B/0/1/0/all/0/1&quot;&gt;Beibei Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Youjia Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zikai Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Junqing Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wei Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13016">
<title>DiffPortrait3D: Controllable Diffusion for Zero-Shot Portrait View Synthesis. (arXiv:2312.13016v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.13016</link>
<description rdf:parseType="Literal">&lt;p&gt;We present DiffPortrait3D, a conditional diffusion model that is capable of
synthesizing 3D-consistent photo-realistic novel views from as few as a single
in-the-wild portrait. Specifically, given a single RGB input, we aim to
synthesize plausible but consistent facial details rendered from novel camera
views with retained both identity and facial expression. In lieu of
time-consuming optimization and fine-tuning, our zero-shot method generalizes
well to arbitrary face portraits with unposed camera views, extreme facial
expressions, and diverse artistic depictions. At its core, we leverage the
generative prior of 2D diffusion models pre-trained on large-scale image
datasets as our rendering backbone, while the denoising is guided with
disentangled attentive control of appearance and camera pose. To achieve this,
we first inject the appearance context from the reference image into the
self-attention layers of the frozen UNets. The rendering view is then
manipulated with a novel conditional control module that interprets the camera
pose by watching a condition image of a crossed subject from the same view.
Furthermore, we insert a trainable cross-view attention module to enhance view
consistency, which is further strengthened with a novel 3D-aware noise
generation process during inference. We demonstrate state-of-the-art results
both qualitatively and quantitatively on our challenging in-the-wild and
multi-view benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yuming Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hongyi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;You Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1&quot;&gt;Guoxian Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yichun Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1&quot;&gt;Di Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1&quot;&gt;Linjie Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13139">
<title>Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation. (arXiv:2312.13139v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2312.13139</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative pre-trained models have demonstrated remarkable effectiveness in
language and vision domains by learning useful representations. In this paper,
we extend the scope of this effectiveness by showing that visual robot
manipulation can significantly benefit from large-scale video generative
pre-training. We introduce GR-1, a straightforward GPT-style model designed for
multi-task language-conditioned visual robot manipulation. GR-1 takes as inputs
a language instruction, a sequence of observation images, and a sequence of
robot states. It predicts robot actions as well as future images in an
end-to-end manner. Thanks to a flexible design, GR-1 can be seamlessly
finetuned on robot data after pre-trained on a large-scale video dataset. We
perform extensive experiments on the challenging CALVIN benchmark and a real
robot. On CALVIN benchmark, our method outperforms state-of-the-art baseline
methods and improves the success rate from 88.9% to 94.9%. In the setting of
zero-shot unseen scene generalization, GR-1 improves the success rate from
53.3% to 85.4%. In real robot experiments, GR-1 also outperforms baseline
methods and shows strong potentials in generalization to unseen scenes and
objects. We provide inaugural evidence that a unified GPT-style transformer,
augmented with large-scale video generative pre-training, exhibits remarkable
generalization to multi-task visual robot manipulation. Project page:
https://GR1-Manipulation.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hongtao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_Y/0/1/0/all/0/1&quot;&gt;Ya Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheang_C/0/1/0/all/0/1&quot;&gt;Chilam Cheang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guangzeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiafeng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xinghang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Minghuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_T/0/1/0/all/0/1&quot;&gt;Tao Kong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13271">
<title>Repaint123: Fast and High-quality One Image to 3D Generation with Progressive Controllable 2D Repainting. (arXiv:2312.13271v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.13271</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent one image to 3D generation methods commonly adopt Score Distillation
Sampling (SDS). Despite the impressive results, there are multiple deficiencies
including multi-view inconsistency, over-saturated and over-smoothed textures,
as well as the slow generation speed. To address these deficiencies, we present
Repaint123 to alleviate multi-view bias as well as texture degradation and
speed up the generation process. The core idea is to combine the powerful image
generation capability of the 2D diffusion model and the texture alignment
ability of the repainting strategy for generating high-quality multi-view
images with consistency. We further propose visibility-aware adaptive
repainting strength for overlap regions to enhance the generated image quality
in the repainting process. The generated high-quality and multi-view consistent
images enable the use of simple Mean Square Error (MSE) loss for fast 3D
content generation. We conduct extensive experiments and show that our method
has a superior ability to generate high-quality 3D content with multi-view
consistency and fine textures in 2 minutes from scratch. Our webpage is
available at https://junwuzhang19.github.io/repaint123/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Junwu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1&quot;&gt;Yatian Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xinhua Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1&quot;&gt;Peng Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yida Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_M/0/1/0/all/0/1&quot;&gt;Munan Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1&quot;&gt;Li Yuan&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>