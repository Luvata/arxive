<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.LG updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Machine Learning (cs.LG) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-12-19T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11466" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11467" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11473" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11475" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11476" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11480" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11486" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11487" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11489" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11502" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11504" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11507" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11509" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11510" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11511" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11513" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11514" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11517" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11522" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11529" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11530" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11531" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11532" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11534" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11536" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11539" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11540" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11545" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11547" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11549" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11550" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11551" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11553" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11559" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11560" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11561" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11562" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11563" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11566" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11571" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11572" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11573" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11581" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11582" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11583" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11584" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11598" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11629" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11663" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11669" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11671" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11703" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11707" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11708" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11712" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11714" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11718" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11730" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11735" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11742" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11747" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11752" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11754" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11768" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11769" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11779" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11788" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11797" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11801" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11818" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11819" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11822" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11831" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11832" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11834" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11835" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11846" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11858" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11861" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11862" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11863" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11875" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11880" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11882" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11891" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11894" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11898" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11903" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11905" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11918" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11926" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11927" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11929" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11933" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11934" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11939" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11952" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11969" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11973" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11976" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12003" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12009" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12022" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12028" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12044" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12049" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12050" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2003.08433" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2004.08697" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2010.03744" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2010.10258" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2106.03354" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.10985" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.15677" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.06009" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.08012" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.08563" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.12756" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.01905" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.07780" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.15657" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.16058" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.16222" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.08494" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.09273" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.01071" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.08645" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.09010" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.08830" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.01190" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.01242" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.04977" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.05428" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.09532" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.06999" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.10343" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.16454" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.16532" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.16737" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.05805" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.14082" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.09820" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13030" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14160" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14901" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14978" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16901" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17205" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.01843" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06154" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14048" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05152" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.16676" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02068" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04690" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12681" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.13304" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.13976" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.06453" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.08765" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11518" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11651" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12211" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15188" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15289" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00149" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00757" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02651" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05161" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.06009" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15516" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17658" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18313" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02775" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03498" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05144" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05587" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06281" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09506" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10090" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14743" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15570" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.18826" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00038" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00812" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05571" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08528" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08670" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09131" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09259" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09323" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09775" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09783" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09844" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10087" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10088" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10130" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10194" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10237" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10276" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10418" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11026" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11315" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00760" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2312.11466">
<title>Extracting Interpretable Local and Global Representations from Attention on Time Series. (arXiv:2312.11466v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11466</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper targets two transformer attention based interpretability methods
working with local abstraction and global representation, in the context of
time series data. We distinguish local and global contexts, and provide a
comprehensive framework for both general interpretation options. We discuss
their specific instantiation via different methods in detail, also outlining
their respective computational implementation and abstraction variants.
Furthermore, we provide extensive experimentation demonstrating the efficacy of
the presented approaches. In particular, we perform our experiments using a
selection of univariate datasets from the UCR UEA time series repository where
we both assess the performance of the proposed approaches, as well as their
impact on explainability and interpretability/complexity. Here, with an
extensive analysis of hyperparameters, the presented approaches demonstrate an
significant improvement in interpretability/complexity, while capturing many
core decisions of and maintaining a similar performance to the baseline model.
Finally, we draw general conclusions outlining and guiding the application of
the presented methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwenke_L/0/1/0/all/0/1&quot;&gt;Leonid Schwenke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atzmueller_M/0/1/0/all/0/1&quot;&gt;Martin Atzmueller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11467">
<title>Glioblastoma Tumor Segmentation using an Ensemble of Vision Transformers. (arXiv:2312.11467v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.11467</link>
<description rdf:parseType="Literal">&lt;p&gt;Glioblastoma is one of the most aggressive and deadliest types of brain
cancer, with low survival rates compared to other types of cancer. Analysis of
Magnetic Resonance Imaging (MRI) scans is one of the most effective methods for
the diagnosis and treatment of brain cancers such as glioblastoma. Accurate
tumor segmentation in MRI images is often required for treatment planning and
risk assessment of treatment methods. Here, we propose a novel pipeline, Brain
Radiology Aided by Intelligent Neural NETworks (BRAINNET), which leverages
MaskFormer, a vision transformer model, and generates robust tumor segmentation
maks. We use an ensemble of nine predictions from three models separately
trained on each of the three orthogonal 2D slice directions (axial, sagittal,
and coronal) of a 3D brain MRI volume. We train and test our models on the
publicly available UPenn-GBM dataset, consisting of 3D multi-parametric MRI
(mpMRI) scans from 611 subjects. Using Dice coefficient (DC) and 95% Hausdorff
distance (HD) for evaluation, our models achieved state-of-the-art results in
segmenting all three different tumor regions -- tumor core (DC = 0.894, HD =
2.308), whole tumor (DC = 0.891, HD = 3.552), and enhancing tumor (DC = 0.812,
HD = 1.608).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huafeng Liu&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dowdell_B/0/1/0/all/0/1&quot;&gt;Benjamin Dowdell&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Engelder_T/0/1/0/all/0/1&quot;&gt;Todd Engelder&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pulmano_Z/0/1/0/all/0/1&quot;&gt;Zarah Pulmano&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Osa_N/0/1/0/all/0/1&quot;&gt;Nicolas Osa&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Barman_A/0/1/0/all/0/1&quot;&gt;Arko Barman&lt;/a&gt; (1) ((1) Rice University)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11473">
<title>Synthetic Shifts to Initial Seed Vector Exposes the Brittle Nature of Latent-Based Diffusion Models. (arXiv:2312.11473v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.11473</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in Conditional Diffusion Models have led to substantial
capabilities in various domains. However, understanding the impact of
variations in the initial seed vector remains an underexplored area of concern.
Particularly, latent-based diffusion models display inconsistencies in image
generation under standard conditions when initialized with suboptimal initial
seed vectors. To understand the impact of the initial seed vector on generated
samples, we propose a reliability evaluation framework that evaluates the
generated samples of a diffusion model when the initial seed vector is
subjected to various synthetic shifts. Our results indicate that slight
manipulations to the initial seed vector of the state-of-the-art Stable
Diffusion (Rombach et al., 2022) can lead to significant disturbances in the
generated samples, consequently creating images without the effect of
conditioning variables. In contrast, GLIDE (Nichol et al., 2022) stands out in
generating reliable samples even when the initial seed vector is transformed.
Thus, our study sheds light on the importance of the selection and the impact
of the initial seed vector in the latent-based diffusion model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Po_Yuan_M/0/1/0/all/0/1&quot;&gt;Mao Po-Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kotyan_S/0/1/0/all/0/1&quot;&gt;Shashank Kotyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foong_T/0/1/0/all/0/1&quot;&gt;Tham Yik Foong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vargas_D/0/1/0/all/0/1&quot;&gt;Danilo Vasconcellos Vargas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11475">
<title>A Hybrid SOM and K-means Model for Time Series Energy Consumption Clustering. (arXiv:2312.11475v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11475</link>
<description rdf:parseType="Literal">&lt;p&gt;Energy consumption analysis plays a pivotal role in addressing the challenges
of sustainability and resource management. This paper introduces a novel
approach to effectively cluster monthly energy consumption patterns by
integrating two powerful techniques: Self-organizing maps and K-means
clustering. The proposed method aims to exploit the benefits of both of these
algorithms to enhance the accuracy and interpretability of clustering results
for a dataset in which finding patterns is difficult. The main focus of this
study is on a selection of time series energy consumption data from the Smart
meters in London dataset. The data was preprocessed and reduced in
dimensionality to capture essential temporal patterns while retaining their
underlying structures. The SOM algorithm was utilized to extract the central
representatives of the consumption patterns for each one of the houses over the
course of each month, effectively reducing the dimensionality of the dataset
and making it easier for analysis. Subsequently, the obtained SOM centroids
were clustered using K-means, a popular centroid-based clustering technique.
The experimental results demonstrated a significant silhouette score of 66%,
indicating strong intra-cluster cohesion and inter-cluster separation which
confirms the effectiveness of the proposed approach in the clustering task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Majidi_F/0/1/0/all/0/1&quot;&gt;Farideh Majidi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11476">
<title>The geometry of flow: Advancing predictions of river geometry with multi-model machine learning. (arXiv:2312.11476v1 [physics.geo-ph])</title>
<link>http://arxiv.org/abs/2312.11476</link>
<description rdf:parseType="Literal">&lt;p&gt;Hydraulic geometry parameters describing river hydrogeomorphic is important
for flood forecasting. Although well-established, power-law hydraulic geometry
curves have been widely used to understand riverine systems and mapping
flooding inundation worldwide for the past 70 years, we have become
increasingly aware of the limitations of these approaches. In the present
study, we have moved beyond these traditional power-law relationships for river
geometry, testing the ability of machine-learning models to provide improved
predictions of river width and depth. For this work, we have used an
unprecedentedly large river measurement dataset (HYDRoSWOT) as well as a suite
of watershed predictor data to develop novel data-driven approaches to better
estimate river geometries over the contiguous United States (CONUS). Our Random
Forest, XGBoost, and neural network models out-performed the traditional,
regionalized power law-based hydraulic geometry equations for both width and
depth, providing R-squared values of as high as 0.75 for width and as high as
0.67 for depth, compared with R-squared values of 0.57 for width and 0.18 for
depth from the regional hydraulic geometry equations. Our results also show
diverse performance outcomes across stream orders and geographical regions for
the different machine-learning models, demonstrating the value of using
multi-model approaches to maximize the predictability of river geometry. The
developed models have been used to create the newly publicly available
STREAM-geo dataset, which provides river width, depth, width/depth ratio, and
river and stream surface area (%RSSA) for nearly 2.7 million NHDPlus stream
reaches across the rivers and streams across the contiguous US.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chang_S/0/1/0/all/0/1&quot;&gt;Shuyu Y Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ghahremani_Z/0/1/0/all/0/1&quot;&gt;Zahra Ghahremani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Manuel_L/0/1/0/all/0/1&quot;&gt;Laura Manuel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Erfani_M/0/1/0/all/0/1&quot;&gt;Mohammad Erfani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Chaopeng Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Cohen_S/0/1/0/all/0/1&quot;&gt;Sagy Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Meter_K/0/1/0/all/0/1&quot;&gt;Kimberly Van Meter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Pierce_J/0/1/0/all/0/1&quot;&gt;Jennifer L Pierce&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Meselhe_E/0/1/0/all/0/1&quot;&gt;Ehab A Meselhe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Goharian_E/0/1/0/all/0/1&quot;&gt;Erfan Goharian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11480">
<title>Adaptive Smooth Activation for Improved Disease Diagnosis and Organ Segmentation from Radiology Scans. (arXiv:2312.11480v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2312.11480</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we propose a new activation function, called Adaptive Smooth
Activation Unit (ASAU), tailored for optimized gradient propagation, thereby
enhancing the proficiency of convolutional networks in medical image analysis.
We apply this new activation function to two important and commonly used
general tasks in medical image analysis: automatic disease diagnosis and organ
segmentation in CT and MRI. Our rigorous evaluation on the RadImageNet
abdominal/pelvis (CT and MRI) dataset and Liver Tumor Segmentation Benchmark
(LiTS) 2017 demonstrates that our ASAU-integrated frameworks not only achieve a
substantial (4.80\%) improvement over ReLU in classification accuracy (disease
detection) on abdominal CT and MRI but also achieves 1\%-3\% improvement in
dice coefficient compared to widely used activations for `healthy liver tissue&apos;
segmentation. These improvements offer new baselines for developing a
diagnostic tool, particularly for complex, challenging pathologies. The
superior performance and adaptability of ASAU highlight its potential for
integration into a wide range of image classification and segmentation tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biswas_K/0/1/0/all/0/1&quot;&gt;Koushik Biswas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jha_D/0/1/0/all/0/1&quot;&gt;Debesh Jha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomar_N/0/1/0/all/0/1&quot;&gt;Nikhil Kumar Tomar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durak_G/0/1/0/all/0/1&quot;&gt;Gorkem Durak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Medetalibeyoglu_A/0/1/0/all/0/1&quot;&gt;Alpay Medetalibeyoglu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antalek_M/0/1/0/all/0/1&quot;&gt;Matthew Antalek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Velichko_Y/0/1/0/all/0/1&quot;&gt;Yury Velichko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ladner_D/0/1/0/all/0/1&quot;&gt;Daniela Ladner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bohrani_A/0/1/0/all/0/1&quot;&gt;Amir Bohrani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagci_U/0/1/0/all/0/1&quot;&gt;Ulas Bagci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11486">
<title>Preference and Concurrence Aware Bayesian Graph Neural Networks for Recommender Systems. (arXiv:2312.11486v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2312.11486</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph-based collaborative filtering methods have prevailing performance for
recommender systems since they can capture high-order information between users
and items, in which the graphs are constructed from the observed user-item
interactions that might miss links or contain spurious positive interactions in
industrial scenarios. The Bayesian Graph Neural Network framework approaches
this issue with generative models for the interaction graphs. The critical
problem is to devise a proper family of graph generative models tailored to
recommender systems. We propose an efficient generative model that jointly
considers the preferences of users, the concurrence of items and some important
graph structure information. Experiments on four popular benchmark datasets
demonstrate the effectiveness of our proposed graph generative methods for
recommender systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_H/0/1/0/all/0/1&quot;&gt;Hongjian Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yaochen Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yingxue Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11487">
<title>Symbolic Learning for Material Discovery. (arXiv:2312.11487v1 [cond-mat.mtrl-sci])</title>
<link>http://arxiv.org/abs/2312.11487</link>
<description rdf:parseType="Literal">&lt;p&gt;Discovering new materials is essential to solve challenges in climate change,
sustainability and healthcare. A typical task in materials discovery is to
search for a material in a database which maximises the value of a function.
That function is often expensive to evaluate, and can rely upon a simulation or
an experiment. Here, we introduce SyMDis, a sample efficient optimisation
method based on symbolic learning, that discovers near-optimal materials in a
large database. SyMDis performs comparably to a state-of-the-art optimiser,
whilst learning interpretable rules to aid physical and chemical verification.
Furthermore, the rules learned by SyMDis generalise to unseen datasets and
return high performing candidates in a zero-shot evaluation, which is difficult
to achieve with other approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Cunnington_D/0/1/0/all/0/1&quot;&gt;Daniel Cunnington&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Cipcigan_F/0/1/0/all/0/1&quot;&gt;Flaviu Cipcigan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Ferreira_R/0/1/0/all/0/1&quot;&gt;Rodrigo Neumann Barros Ferreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Booth_J/0/1/0/all/0/1&quot;&gt;Jonathan Booth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11489">
<title>Agglomerative Federated Learning: Empowering Larger Model Training via End-Edge-Cloud Collaboration. (arXiv:2312.11489v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2312.11489</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) enables training Artificial Intelligence (AI) models
over end devices without compromising their privacy. As computing tasks are
increasingly performed by a combination of cloud, edge, and end devices, FL can
benefit from this End-Edge-Cloud Collaboration (EECC) paradigm to achieve
collaborative device-scale expansion with real-time access. Although
Hierarchical Federated Learning (HFL) supports multi-tier model aggregation
suitable for EECC, prior works assume the same model structure on all computing
nodes, constraining the model scale by the weakest end devices. To address this
issue, we propose Agglomerative Federated Learning (FedAgg), which is a novel
EECC-empowered FL framework that allows the trained models from end, edge, to
cloud to grow larger in size and stronger in generalization ability. FedAgg
recursively organizes computing nodes among all tiers based on Bridge Sample
Based Online Distillation Protocol (BSBODP), which enables every pair of
parent-child computing nodes to mutually transfer and distill knowledge
extracted from generated bridge samples. This design enhances the performance
by exploiting the potential of larger models, with privacy constraints of FL
and flexibility requirements of EECC both satisfied. Experiments under various
settings demonstrate that FedAgg outperforms state-of-the-art methods by an
average of 4.53\% accuracy gains and remarkable improvements in convergence
rate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Sheng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Min Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_B/0/1/0/all/0/1&quot;&gt;Bo Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Q/0/1/0/all/0/1&quot;&gt;Quyang Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1&quot;&gt;Tianliu He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xuefeng Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11502">
<title>Labrador: Exploring the Limits of Masked Language Modeling for Laboratory Data. (arXiv:2312.11502v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.11502</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we introduce Labrador, a pre-trained Transformer model for
laboratory data. Labrador and BERT were pre-trained on a corpus of 100 million
lab test results from electronic health records (EHRs) and evaluated on various
downstream outcome prediction tasks. Both models demonstrate mastery of the
pre-training task but neither consistently outperform XGBoost on downstream
supervised tasks. Our ablation studies reveal that transfer learning shows
limited effectiveness for BERT and achieves marginal success with Labrador. We
explore the reasons for the failure of transfer learning and suggest that the
data generating process underlying each patient cannot be characterized
sufficiently using labs alone, among other factors. We encourage future work to
focus on joint modeling of multiple EHR data categories and to include
tree-based baselines in their evaluations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellamy_D/0/1/0/all/0/1&quot;&gt;David R. Bellamy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_B/0/1/0/all/0/1&quot;&gt;Bhawesh Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Cindy Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beam_A/0/1/0/all/0/1&quot;&gt;Andrew Beam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11504">
<title>The performance of multiple language models in identifying offensive language on social media. (arXiv:2312.11504v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.11504</link>
<description rdf:parseType="Literal">&lt;p&gt;Text classification is an important topic in the field of natural language
processing. It has been preliminarily applied in information retrieval, digital
library, automatic abstracting, text filtering, word semantic discrimination
and many other fields. The aim of this research is to use a variety of
algorithms to test the ability to identify offensive posts and evaluate their
performance against a variety of assessment methods. The motivation for this
project is to reduce the harm of these languages to human censors by automating
the screening of offending posts. The field is a new one, and despite much
interest in the past two years, there has been no focus on the object of the
offence. Through the experiment of this project, it should inspire future
research on identification methods as well as identification content.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bennett_B/0/1/0/all/0/1&quot;&gt;Brandon Bennett&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11507">
<title>Explain To Decide: A Human-Centric Review on the Role of Explainable Artificial Intelligence in AI-assisted Decision Making. (arXiv:2312.11507v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2312.11507</link>
<description rdf:parseType="Literal">&lt;p&gt;The unprecedented performance of machine learning models in recent years,
particularly Deep Learning and transformer models, has resulted in their
application in various domains such as finance, healthcare, and education.
However, the models are error-prone and cannot be used autonomously, especially
in decision-making scenarios where, technically or ethically, the cost of error
is high. Moreover, because of the black-box nature of these models, it is
frequently difficult for the end user to comprehend the models&apos; outcomes and
underlying processes to trust and use the model outcome to make a decision.
Explainable Artificial Intelligence (XAI) aids end-user understanding of the
model by utilizing approaches, including visualization techniques, to explain
and interpret the inner workings of the model and how it arrives at a result.
Although numerous research studies have been conducted recently focusing on the
performance of models and the XAI approaches, less work has been done on the
impact of explanations on human-AI team performance. This paper surveyed the
recent empirical studies on XAI&apos;s impact on human-AI decision-making,
identified the challenges, and proposed future research directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rogha_M/0/1/0/all/0/1&quot;&gt;Milad Rogha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11509">
<title>Toward A Reinforcement-Learning-Based System for Adjusting Medication to Minimize Speech Disfluency. (arXiv:2312.11509v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.11509</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a Reinforcement-Learning-based system that would automatically
prescribe a hypothetical patient medications that may help the patient with
their mental-health-related speech disfluency, and adjust the medication and
the dosages in response to data from the patient. We demonstrate the components
of the system: a module that detects and evaluates speech disfluency on a large
dataset we built, and a Reinforcement Learning algorithm that automatically
finds good combinations of medications. To support the two modules, we collect
data on the effect of psychiatric medications for speech disfluency from the
literature, and build a plausible patient simulation system. We demonstrate
that the Reinforcement Learning system is, under some circumstances, able to
converge to a good medication regime. We collect and label a dataset of people
with possible speech disfluency and demonstrate our methods using that dataset.
Our work is a proof of concept: we show that there is promise in the idea of
using automatic data collection to address disfluency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Constas_P/0/1/0/all/0/1&quot;&gt;Pavlos Constas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rawal_V/0/1/0/all/0/1&quot;&gt;Vikram Rawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliveira_M/0/1/0/all/0/1&quot;&gt;Matthew Honorio Oliveira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Constas_A/0/1/0/all/0/1&quot;&gt;Andreas Constas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Aditya Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheung_K/0/1/0/all/0/1&quot;&gt;Kaison Cheung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sultani_N/0/1/0/all/0/1&quot;&gt;Najma Sultani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Carrie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Altomare_M/0/1/0/all/0/1&quot;&gt;Micol Altomare&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akzam_M/0/1/0/all/0/1&quot;&gt;Michael Akzam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiacheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_V/0/1/0/all/0/1&quot;&gt;Vhea He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Altomare_L/0/1/0/all/0/1&quot;&gt;Lauren Altomare&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murqi_H/0/1/0/all/0/1&quot;&gt;Heraa Murqi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Asad Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhanshali_N/0/1/0/all/0/1&quot;&gt;Nimit Amikumar Bhanshali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rachad_Y/0/1/0/all/0/1&quot;&gt;Youssef Rachad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guerzhoy_M/0/1/0/all/0/1&quot;&gt;Michael Guerzhoy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11510">
<title>QuadAttack: A Quadratic Programming Approach to Ordered Top-K Attacks. (arXiv:2312.11510v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.11510</link>
<description rdf:parseType="Literal">&lt;p&gt;The adversarial vulnerability of Deep Neural Networks (DNNs) has been
well-known and widely concerned, often under the context of learning top-$1$
attacks (e.g., fooling a DNN to classify a cat image as dog). This paper shows
that the concern is much more serious by learning significantly more aggressive
ordered top-$K$ clear-box~\footnote{ This is often referred to as
white/black-box attacks in the literature. We choose to adopt neutral
terminology, clear/opaque-box attacks in this paper, and omit the prefix
clear-box for simplicity.} targeted attacks proposed in Adversarial
Distillation. We propose a novel and rigorous quadratic programming (QP) method
of learning ordered top-$K$ attacks with low computing cost, dubbed as
\textbf{QuadAttac$K$}. Our QuadAttac$K$ directly solves the QP to satisfy the
attack constraint in the feature embedding space (i.e., the input space to the
final linear classifier), which thus exploits the semantics of the feature
embedding space (i.e., the principle of class coherence). With the optimized
feature embedding vector perturbation, it then computes the adversarial
perturbation in the data space via the vanilla one-step back-propagation. In
experiments, the proposed QuadAttac$K$ is tested in the ImageNet-1k
classification using ResNet-50, DenseNet-121, and Vision Transformers (ViT-B
and DEiT-S). It successfully pushes the boundary of successful ordered top-$K$
attacks from $K=10$ up to $K=20$ at a cheap budget ($1\times 60$) and further
improves attack success rates for $K=5$ for all tested models, while retaining
the performance for $K=1$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paniagua_T/0/1/0/all/0/1&quot;&gt;Thomas Paniagua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grainger_R/0/1/0/all/0/1&quot;&gt;Ryan Grainger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tianfu Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11511">
<title>ComplexityNet: Increasing LLM Inference Efficiency by Learning Task Complexity. (arXiv:2312.11511v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.11511</link>
<description rdf:parseType="Literal">&lt;p&gt;We present ComplexityNet, a streamlined language model designed for assessing
task complexity. This model predicts the likelihood of accurate output by
various language models, each with different capabilities. Our initial
application of ComplexityNet involves the Mostly Basic Python Problems (MBPP)
dataset. We pioneered the creation of the first set of labels to define task
complexity. ComplexityNet achieved a notable 79% accuracy in determining task
complexity, a significant improvement over the 34% accuracy of the original,
non fine-tuned model. Furthermore, ComplexityNet effectively reduces
computational resource usage by 90% compared to using the highest complexity
model, while maintaining a high code generation accuracy of 86.7%. This study
demonstrates that fine-tuning smaller models to categorize tasks based on their
complexity can lead to a more balanced trade-off between accuracy and
efficiency in the use of Large Language Models. Our findings suggest a
promising direction for optimizing LLM applications, especially in
resource-constrained environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bae_H/0/1/0/all/0/1&quot;&gt;Henry Bae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deeb_A/0/1/0/all/0/1&quot;&gt;Aghyad Deeb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fleury_A/0/1/0/all/0/1&quot;&gt;Alex Fleury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1&quot;&gt;Kehang Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11513">
<title>Maatphor: Automated Variant Analysis for Prompt Injection Attacks. (arXiv:2312.11513v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.11513</link>
<description rdf:parseType="Literal">&lt;p&gt;Prompt injection has emerged as a serious security threat to large language
models (LLMs). At present, the current best-practice for defending against
newly-discovered prompt injection techniques is to add additional guardrails to
the system (e.g., by updating the system prompt or using classifiers on the
input and/or output of the model.) However, in the same way that variants of a
piece of malware are created to evade anti-virus software, variants of a prompt
injection can be created to evade the LLM&apos;s guardrails. Ideally, when a new
prompt injection technique is discovered, candidate defenses should be tested
not only against the successful prompt injection, but also against possible
variants.
&lt;/p&gt;
&lt;p&gt;In this work, we present, a tool to assist defenders in performing automated
variant analysis of known prompt injection attacks. This involves solving two
main challenges: (1) automatically generating variants of a given prompt
according, and (2) automatically determining whether a variant was effective
based only on the output of the model. This tool can also assist in generating
datasets for jailbreak and prompt injection attacks, thus overcoming the
scarcity of data in this domain.
&lt;/p&gt;
&lt;p&gt;We evaluate Maatphor on three different types of prompt injection tasks.
Starting from an ineffective (0%) seed prompt, Maatphor consistently generates
variants that are at least 60% effective within the first 40 iterations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salem_A/0/1/0/all/0/1&quot;&gt;Ahmed Salem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paverd_A/0/1/0/all/0/1&quot;&gt;Andrew Paverd&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kopf_B/0/1/0/all/0/1&quot;&gt;Boris K&amp;#xf6;pf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11514">
<title>LLM in a flash: Efficient Large Language Model Inference with Limited Memory. (arXiv:2312.11514v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.11514</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) are central to modern natural language
processing, delivering exceptional performance in various tasks. However, their
intensive computational and memory requirements present challenges, especially
for devices with limited DRAM capacity. This paper tackles the challenge of
efficiently running LLMs that exceed the available DRAM capacity by storing the
model parameters on flash memory but bringing them on demand to DRAM. Our
method involves constructing an inference cost model that harmonizes with the
flash memory behavior, guiding us to optimize in two critical areas: reducing
the volume of data transferred from flash and reading data in larger, more
contiguous chunks. Within this flash memory-informed framework, we introduce
two principal techniques. First, &quot;windowing&apos;&quot; strategically reduces data
transfer by reusing previously activated neurons, and second, &quot;row-column
bundling&quot;, tailored to the sequential data access strengths of flash memory,
increases the size of data chunks read from flash memory. These methods
collectively enable running models up to twice the size of the available DRAM,
with a 4-5x and 20-25x increase in inference speed compared to naive loading
approaches in CPU and GPU, respectively. Our integration of sparsity awareness,
context-adaptive loading, and a hardware-oriented design paves the way for
effective inference of LLMs on devices with limited memory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alizadeh_K/0/1/0/all/0/1&quot;&gt;Keivan Alizadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirzadeh_I/0/1/0/all/0/1&quot;&gt;Iman Mirzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belenko_D/0/1/0/all/0/1&quot;&gt;Dmitry Belenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khatamifard_K/0/1/0/all/0/1&quot;&gt;Karen Khatamifard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1&quot;&gt;Minsik Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mundo_C/0/1/0/all/0/1&quot;&gt;Carlo C Del Mundo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rastegari_M/0/1/0/all/0/1&quot;&gt;Mohammad Rastegari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farajtabar_M/0/1/0/all/0/1&quot;&gt;Mehrdad Farajtabar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11517">
<title>Unlocking Musculoskeletal Disorder Risk Factors: NLP-Based Classification and Mode-Based Ranking. (arXiv:2312.11517v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.11517</link>
<description rdf:parseType="Literal">&lt;p&gt;This research delves into the intricate landscape of Musculoskeletal Disorder
(MSD) risk factors, employing a novel fusion of Natural Language Processing
(NLP) techniques and mode-based ranking methodologies. The primary objective is
to advance the comprehension of MSD risk factors, their classification, and
their relative severity, facilitating more targeted preventive and management
interventions. The study utilizes eight diverse models, integrating pre-trained
transformers, cosine similarity, and various distance metrics to classify risk
factors into personal, biomechanical, workplace, psychological, and
organizational classes. Key findings reveal that the BERT model with cosine
similarity attains an overall accuracy of 28\%, while the sentence transformer,
coupled with Euclidean, Bray-Curtis, and Minkowski distances, achieves a
flawless accuracy score of 100\%. In tandem with the classification efforts,
the research employs a mode-based ranking approach on survey data to discern
the severity hierarchy of MSD risk factors. Intriguingly, the rankings align
precisely with the previous literature, reaffirming the consistency and
reliability of the approach. ``Working posture&quot; emerges as the most severe risk
factor, emphasizing the critical role of proper posture in preventing MSDs. The
collective perceptions of survey participants underscore the significance of
factors like ``Job insecurity,&quot; ``Effort reward imbalance,&quot; and ``Poor employee
facility&quot; in contributing to MSD risks. The convergence of rankings provides
actionable insights for organizations aiming to reduce the prevalence of MSDs.
The study concludes with implications for targeted interventions,
recommendations for improving workplace conditions, and avenues for future
research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jahin_M/0/1/0/all/0/1&quot;&gt;Md Abrar Jahin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talapatra_S/0/1/0/all/0/1&quot;&gt;Subrata Talapatra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11522">
<title>Assessing SATNet&apos;s Ability to Solve the Symbol Grounding Problem. (arXiv:2312.11522v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.11522</link>
<description rdf:parseType="Literal">&lt;p&gt;SATNet is an award-winning MAXSAT solver that can be used to infer logical
rules and integrated as a differentiable layer in a deep neural network. It had
been shown to solve Sudoku puzzles visually from examples of puzzle digit
images, and was heralded as an impressive achievement towards the longstanding
AI goal of combining pattern recognition with logical reasoning. In this paper,
we clarify SATNet&apos;s capabilities by showing that in the absence of intermediate
labels that identify individual Sudoku digit images with their logical
representations, SATNet completely fails at visual Sudoku (0% test accuracy).
More generally, the failure can be pinpointed to its inability to learn to
assign symbols to perceptual phenomena, also known as the symbol grounding
problem, which has long been thought to be a prerequisite for intelligent
agents to perform real-world logical reasoning. We propose an MNIST based test
as an easy instance of the symbol grounding problem that can serve as a sanity
check for differentiable symbolic solvers in general. Naive applications of
SATNet on this test lead to performance worse than that of models without
logical reasoning capabilities. We report on the causes of SATNet&apos;s failure and
how to prevent them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_O/0/1/0/all/0/1&quot;&gt;Oscar Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flokas_L/0/1/0/all/0/1&quot;&gt;Lampros Flokas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipson_H/0/1/0/all/0/1&quot;&gt;Hod Lipson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spranger_M/0/1/0/all/0/1&quot;&gt;Michael Spranger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11529">
<title>Efficient and Scalable Graph Generation through Iterative Local Expansion. (arXiv:2312.11529v1 [cs.SI])</title>
<link>http://arxiv.org/abs/2312.11529</link>
<description rdf:parseType="Literal">&lt;p&gt;In the realm of generative models for graphs, extensive research has been
conducted. However, most existing methods struggle with large graphs due to the
complexity of representing the entire joint distribution across all node pairs
and capturing both global and local graph structures simultaneously. To
overcome these issues, we introduce a method that generates a graph by
progressively expanding a single node to a target graph. In each step, nodes
and edges are added in a localized manner through denoising diffusion, building
first the global structure, and then refining the local details. The local
generation avoids modeling the entire joint distribution over all node pairs,
achieving substantial computational savings with subquadratic runtime relative
to node count while maintaining high expressivity through multiscale
generation. Our experiments show that our model achieves state-of-the-art
performance on well-established benchmark datasets while successfully scaling
to graphs with at least 5000 nodes. Our method is also the first to
successfully extrapolate to graphs outside of the training distribution,
showcasing a much better generalization capability over existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bergmeister_A/0/1/0/all/0/1&quot;&gt;Andreas Bergmeister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinkus_K/0/1/0/all/0/1&quot;&gt;Karolis Martinkus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perraudin_N/0/1/0/all/0/1&quot;&gt;Nathana&amp;#xeb;l Perraudin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1&quot;&gt;Roger Wattenhofer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11530">
<title>Twitter Permeability to financial events: an experiment towards a model for sensing irregularities. (arXiv:2312.11530v1 [q-fin.ST])</title>
<link>http://arxiv.org/abs/2312.11530</link>
<description rdf:parseType="Literal">&lt;p&gt;There is a general consensus of the good sensing and novelty characteristics
of Twitter as an information media for the complex financial market. This paper
investigates the permeability of Twittersphere, the total universe of Twitter
users and their habits, towards relevant events in the financial market.
Analysis shows that a general purpose social media is permeable to
financial-specific events and establishes Twitter as a relevant feeder for
taking decisions regarding the financial market and event fraudulent activities
in that market. However, the provenance of contributions, their different
levels of credibility and quality and even the purpose or intention behind them
should to be considered and carefully contemplated if Twitter is used as a
single source for decision taking. With the overall aim of this research, to
deploy an architecture for real-time monitoring of irregularities in the
financial market, this paper conducts a series of experiments on the level of
permeability and the permeable features of Twitter in the event of one of these
irregularities. To be precise, Twitter data is collected concerning an event
comprising of a specific financial action on the 27th January 2017:{~ }the
announcement about the merge of two companies Tesco PLC and Booker Group PLC,
listed in the main market of the London Stock Exchange (LSE), to create the
UK&apos;s Leading Food Business. The experiment attempts to answer five key research
questions which aim to characterize the features of Twitter permeability to the
financial market. The experimental results confirm that a far-impacting
financial event, such as the merger considered, caused apparent disturbances in
all the features considered, that is, information volume, content and sentiment
as well as geographical provenance. Analysis shows that despite, Twitter not
being a specific financial forum, it is permeable to financial events.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Vilas_A/0/1/0/all/0/1&quot;&gt;Ana Fern&amp;#xe1;ndez Vilas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Redondo_R/0/1/0/all/0/1&quot;&gt;Rebeca P. D&amp;#xed;az Redondo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Crockett_K/0/1/0/all/0/1&quot;&gt;Keeley Crockett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Owda_M/0/1/0/all/0/1&quot;&gt;Majdi Owda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Evans_L/0/1/0/all/0/1&quot;&gt;Lewis Evans&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11531">
<title>The irruption of cryptocurrencies into Twitter cashtags: a classifying solution. (arXiv:2312.11531v1 [q-fin.ST])</title>
<link>http://arxiv.org/abs/2312.11531</link>
<description rdf:parseType="Literal">&lt;p&gt;There is a consensus about the good sensing characteristics of Twitter to
mine and uncover knowledge in financial markets, being considered a relevant
feeder for taking decisions about buying or holding stock shares and even for
detecting stock manipulation. Although Twitter hashtags allow to aggregate
topic-related content, a specific mechanism for financial information also
exists: Cashtag. However, the irruption of cryptocurrencies has resulted in a
significant degradation on the cashtag-based aggregation of posts.
Unfortunately, Twitter&apos; users may use homonym tickers to refer to
cryptocurrencies and to companies in stock markets, which means that filtering
by cashtag may result on both posts referring to stock companies and
cryptocurrencies. This research proposes automated classifiers to distinguish
conflicting cashtags and, so, their container tweets by analyzing the
distinctive features of tweets referring to stock companies and
cryptocurrencies. As experiment, this paper analyses the interference between
cryptocurrencies and company tickers in the London Stock Exchange (LSE),
specifically, companies in the main and alternative market indices FTSE-100 and
AIM-100. Heuristic-based as well as supervised classifiers are proposed and
their advantages and drawbacks, including their ability to self-adapt to
Twitter usage changes, are discussed. The experiment confirms a significant
distortion in collected data when colliding or homonym cashtags exist, i.e.,
the same \$ acronym to refer to company tickers and cryptocurrencies. According
to our results, the distinctive features of posts including cryptocurrencies or
company tickers support accurate classification of colliding tweets (homonym
cashtags) and Independent Models, as the most detached classifiers from
training data, have the potential to be trans-applicability (in different stock
markets) while retaining performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Vilas_A/0/1/0/all/0/1&quot;&gt;Ana Fern&amp;#xe1;ndez Vilas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Redondo_R/0/1/0/all/0/1&quot;&gt;Rebeca D&amp;#xed;az Redondo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Garcia_A/0/1/0/all/0/1&quot;&gt;Ant&amp;#xf3;n Lorenzo Garc&amp;#xed;a&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11532">
<title>Topic-VQ-VAE: Leveraging Latent Codebooks for Flexible Topic-Guided Document Generation. (arXiv:2312.11532v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.11532</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a novel approach for topic modeling utilizing latent
codebooks from Vector-Quantized Variational Auto-Encoder~(VQ-VAE), discretely
encapsulating the rich information of the pre-trained embeddings such as the
pre-trained language model. From the novel interpretation of the latent
codebooks and embeddings as conceptual bag-of-words, we propose a new
generative topic model called Topic-VQ-VAE~(TVQ-VAE) which inversely generates
the original documents related to the respective latent codebook. The TVQ-VAE
can visualize the topics with various generative distributions including the
traditional BoW distribution and the autoregressive image generation. Our
experimental results on document analysis and image generation demonstrate that
TVQ-VAE effectively captures the topic context which reveals the underlying
structures of the dataset and supports flexible forms of document generation.
Official implementation of the proposed TVQ-VAE is available at
https://github.com/clovaai/TVQ-VAE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1&quot;&gt;YoungJoon Yoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jongwon Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11534">
<title>Improved Differentially Private and Lazy Online Convex Optimization. (arXiv:2312.11534v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.11534</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the task of $(\epsilon, \delta)$-differentially private online
convex optimization (OCO). In the online setting, the release of each distinct
decision or iterate carries with it the potential for privacy loss. This
problem has a long history of research starting with Jain et al. [2012] and the
best known results for the regime of {\epsilon} being very small are presented
in Agarwal et al. [2023]. In this paper we improve upon the results of Agarwal
et al. [2023] in terms of the dimension factors as well as removing the
requirement of smoothness. Our results are now the best known rates for DP-OCO
in this regime.
&lt;/p&gt;
&lt;p&gt;Our algorithms builds upon the work of [Asi et al., 2023] which introduced
the idea of explicitly limiting the number of switches via rejection sampling.
The main innovation in our algorithm is the use of sampling from a strongly
log-concave density which allows us to trade-off the dimension factors better
leading to improved results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_N/0/1/0/all/0/1&quot;&gt;Naman Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kale_S/0/1/0/all/0/1&quot;&gt;Satyen Kale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1&quot;&gt;Karan Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thakurta_A/0/1/0/all/0/1&quot;&gt;Abhradeep Guha Thakurta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11536">
<title>Fast Decision Boundary based Out-of-Distribution Detector. (arXiv:2312.11536v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11536</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficient and effective Out-of-Distribution (OOD) detection is essential for
the safe deployment of AI in latency-critical applications. Recently, studies
have revealed that detecting OOD based on feature space information can be
highly effective. Despite their effectiveness, however, exiting feature space
OOD methods may incur non-negligible computational overhead, given their
reliance on auxiliary models built from training features. In this paper, we
aim to obviate auxiliary models to optimize computational efficiency while
leveraging the rich information embedded in the feature space. We investigate
from the novel perspective of decision boundaries and propose to detect OOD
using the feature distance to decision boundaries. To minimize the cost of
measuring the distance, we introduce an efficient closed-form estimation,
analytically proven to tightly lower bound the distance. We observe that ID
features tend to reside further from the decision boundaries than OOD features.
Our observation aligns with the intuition that models tend to be more decisive
on ID samples, considering that distance to decision boundaries quantifies
model uncertainty. From our understanding, we propose a hyperparameter-free,
auxiliary model-free OOD detector. Our OOD detector matches or surpasses the
effectiveness of state-of-the-art methods across extensive experiments.
Meanwhile, our OOD detector incurs practically negligible overhead in inference
latency. Overall, we significantly enhance the efficiency-effectiveness
trade-off in OOD detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Litian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yao Qin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11539">
<title>KGLens: A Parameterized Knowledge Graph Solution to Assess What an LLM Does and Doesn&apos;t Know. (arXiv:2312.11539v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.11539</link>
<description rdf:parseType="Literal">&lt;p&gt;Current approaches to evaluating large language models (LLMs) with
pre-existing Knowledge Graphs (KG) mostly ignore the structure of the KG and
make arbitrary choices of which part of the graph to evaluate. In this paper,
we introduce KGLens, a method to evaluate LLMs by generating natural language
questions from a KG in a structure aware manner so that we can characterize its
performance on a more aggregated level. KGLens uses a parameterized KG, where
each edge is augmented with a beta distribution that guides how to sample edges
from the KG for QA testing. As the evaluation proceeds, different edges of the
parameterized KG are sampled and assessed appropriately, converging to a more
global picture of the performance of the LLMs on the KG as a whole. In our
experiments, we construct three domain-specific KGs for knowledge assessment,
comprising over 19,000 edges, 700 relations, and 21,000 entities. The results
demonstrate that KGLens can not only assess overall performance but also
provide topic, temporal, and relation analyses of LLMs. This showcases the
adaptability and customizability of KGLens, emphasizing its ability to focus
the evaluation based on specific criteria.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Shangshang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1&quot;&gt;He Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yizhe Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yi Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_X/0/1/0/all/0/1&quot;&gt;Xiaochuan Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaitly_N/0/1/0/all/0/1&quot;&gt;Navdeep Jaitly&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11540">
<title>On the Trade-off between the Number of Nodes and the Number of Trees in a Random Forest. (arXiv:2312.11540v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11540</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we focus on the prediction phase of a random forest and study
the problem of representing a bag of decision trees using a smaller bag of
decision trees, where we only consider binary decision problems on the binary
domain and simple decision trees in which an internal node is limited to
querying the Boolean value of a single variable. As a main result, we show that
the majority function of $n$ variables can be represented by a bag of $T$ ($&amp;lt;
n$) decision trees each with polynomial size if $n-T$ is a constant, where $n$
and $T$ must be odd (in order to avoid the tie break). We also show that a bag
of $n$ decision trees can be represented by a bag of $T$ decision trees each
with polynomial size if $n-T$ is a constant and a small classification error is
allowed. A related result on the $k$-out-of-$n$ functions is presented too.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akutsu_T/0/1/0/all/0/1&quot;&gt;Tatsuya Akutsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melkman_A/0/1/0/all/0/1&quot;&gt;Avraham A. Melkman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takasu_A/0/1/0/all/0/1&quot;&gt;Atsuhiro Takasu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11545">
<title>Robust Communicative Multi-Agent Reinforcement Learning with Active Defense. (arXiv:2312.11545v1 [cs.MA])</title>
<link>http://arxiv.org/abs/2312.11545</link>
<description rdf:parseType="Literal">&lt;p&gt;Communication in multi-agent reinforcement learning (MARL) has been proven to
effectively promote cooperation among agents recently. Since communication in
real-world scenarios is vulnerable to noises and adversarial attacks, it is
crucial to develop robust communicative MARL technique. However, existing
research in this domain has predominantly focused on passive defense
strategies, where agents receive all messages equally, making it hard to
balance performance and robustness. We propose an active defense strategy,
where agents automatically reduce the impact of potentially harmful messages on
the final decision. There are two challenges to implement this strategy, that
are defining unreliable messages and adjusting the unreliable messages&apos; impact
on the final decision properly. To address them, we design an Active Defense
Multi-Agent Communication framework (ADMAC), which estimates the reliability of
received messages and adjusts their impact on the final decision accordingly
with the help of a decomposable decision structure. The superiority of ADMAC
over existing methods is validated by experiments in three
communication-critical tasks under four types of attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Lebin Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1&quot;&gt;Yunbo Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1&quot;&gt;Quanming Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yuan Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xudong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jian Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11547">
<title>A Unified Pre-training and Adaptation Framework for Combinatorial Optimization on Graphs. (arXiv:2312.11547v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.11547</link>
<description rdf:parseType="Literal">&lt;p&gt;Combinatorial optimization (CO) on graphs is a classic topic that has been
extensively studied across many scientific and industrial fields. Recently,
solving CO problems on graphs through learning methods has attracted great
attention. Advanced deep learning methods, e.g., graph neural networks (GNNs),
have been used to effectively assist the process of solving COs. However,
current frameworks based on GNNs are mainly designed for certain CO problems,
thereby failing to consider their transferable and generalizable abilities
among different COs on graphs. Moreover, simply using original graphs to model
COs only captures the direct correlations among objects, which does not
consider the mathematical logicality and properties of COs. In this paper, we
propose a unified pre-training and adaptation framework for COs on graphs with
the help of the maximum satisfiability (Max-SAT) problem. We first use Max-SAT
to bridge different COs on graphs since they can be converted to Max-SAT
problems represented by standard formulas and clauses with logical information.
Then, we further design a pre-training and domain adaptation framework to
extract the transferable and generalizable features so that different COs can
benefit from them. In the pre-training stage, Max-SAT instances are generated
to initialize the parameters of the model. In the fine-tuning stage, instances
from CO and Max-SAT problems are used for adaptation so that the transferable
ability can be further improved. Numerical experiments on several datasets show
that features extracted by our framework exhibit superior transferability and
Max-SAT can boost the ability to solve COs on graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_R/0/1/0/all/0/1&quot;&gt;Ruibin Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_M/0/1/0/all/0/1&quot;&gt;Minglong Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1&quot;&gt;Lingfeng Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1&quot;&gt;Lan Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11549">
<title>Label-Free Multivariate Time Series Anomaly Detection. (arXiv:2312.11549v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11549</link>
<description rdf:parseType="Literal">&lt;p&gt;Anomaly detection in multivariate time series (MTS) has been widely studied
in one-class classification (OCC) setting. The training samples in OCC are
assumed to be normal, which is difficult to guarantee in practical situations.
Such a case may degrade the performance of OCC-based anomaly detection methods
which fit the training distribution as the normal distribution. In this paper,
we propose MTGFlow, an unsupervised anomaly detection approach for MTS anomaly
detection via dynamic Graph and entity-aware normalizing Flow. MTGFlow first
estimates the density of the entire training samples and then identifies
anomalous instances based on the density of the test samples within the fitted
distribution. This relies on a widely accepted assumption that anomalous
instances exhibit more sparse densities than normal ones, with no reliance on
the clean training dataset. However, it is intractable to directly estimate the
density due to complex dependencies among entities and their diverse inherent
characteristics. To mitigate this, we utilize the graph structure learning
model to learn interdependent and evolving relations among entities, which
effectively captures complex and accurate distribution patterns of MTS. In
addition, our approach incorporates the unique characteristics of individual
entities by employing an entity-aware normalizing flow. This enables us to
represent each entity as a parameterized normal distribution. Furthermore,
considering that some entities present similar characteristics, we propose a
cluster strategy that capitalizes on the commonalities of entities with similar
characteristics, resulting in more precise and detailed density estimation. We
refer to this cluster-aware extension as MTGFlow_cluster. Extensive experiments
are conducted on six widely used benchmark datasets, in which MTGFlow and
MTGFlow cluster demonstrate their superior detection performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1&quot;&gt;Qihang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1&quot;&gt;Shibo He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haoyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_W/0/1/0/all/0/1&quot;&gt;Wenchao Meng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11550">
<title>A Study on Transferability of Deep Learning Models for Network Intrusion Detection. (arXiv:2312.11550v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.11550</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we explore transferability in learning between different
attack classes in a network intrusion detection setup. We evaluate
transferability of attack classes by training a deep learning model with a
specific attack class and testing it on a separate attack class. We observe the
effects of real and synthetically generated data augmentation techniques on
transferability. We investigate the nature of observed transferability
relationships, which can be either symmetric or asymmetric. We also examine
explainability of the transferability relationships using the recursive feature
elimination algorithm. We study data preprocessing techniques to boost model
performance. The code for this work can be found at
https://github.com/ghosh64/transferability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Shreya Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jameel_A/0/1/0/all/0/1&quot;&gt;Abu Shafin Mohammad Mahdee Jameel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gamal_A/0/1/0/all/0/1&quot;&gt;Aly El Gamal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11551">
<title>Probabilistic Offline Policy Ranking with Approximate Bayesian Computation. (arXiv:2312.11551v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11551</link>
<description rdf:parseType="Literal">&lt;p&gt;In practice, it is essential to compare and rank candidate policies offline
before real-world deployment for safety and reliability. Prior work seeks to
solve this offline policy ranking (OPR) problem through value-based methods,
such as Off-policy evaluation (OPE). However, they fail to analyze special
cases performance (e.g., worst or best cases), due to the lack of holistic
characterization of policies performance. It is even more difficult to estimate
precise policy values when the reward is not fully accessible under sparse
settings. In this paper, we present Probabilistic Offline Policy Ranking
(POPR), a framework to address OPR problems by leveraging expert data to
characterize the probability of a candidate policy behaving like experts, and
approximating its entire performance posterior distribution to help with
ranking. POPR does not rely on value estimation, and the derived performance
posterior can be used to distinguish candidates in worst, best, and
average-cases. To estimate the posterior, we propose POPR-EABC, an Energy-based
Approximate Bayesian Computation (ABC) method conducting likelihood-free
inference. POPR-EABC reduces the heuristic nature of ABC by a smooth energy
function, and improves the sampling efficiency by a pseudo-likelihood. We
empirically demonstrate that POPR-EABC is adequate for evaluating policies in
both discrete and continuous action spaces across various experiment
environments, and facilitates probabilistic comparisons of candidate policies
before deployment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Da_L/0/1/0/all/0/1&quot;&gt;Longchao Da&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jenkins_P/0/1/0/all/0/1&quot;&gt;Porter Jenkins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwantes_T/0/1/0/all/0/1&quot;&gt;Trevor Schwantes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dotson_J/0/1/0/all/0/1&quot;&gt;Jeffrey Dotson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1&quot;&gt;Hua Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11553">
<title>SeGA: Preference-Aware Self-Contrastive Learning with Prompts for Anomalous User Detection on Twitter. (arXiv:2312.11553v1 [cs.SI])</title>
<link>http://arxiv.org/abs/2312.11553</link>
<description rdf:parseType="Literal">&lt;p&gt;In the dynamic and rapidly evolving world of social media, detecting
anomalous users has become a crucial task to address malicious activities such
as misinformation and cyberbullying. As the increasing number of anomalous
users improves the ability to mimic normal users and evade detection, existing
methods only focusing on bot detection are ineffective in terms of capturing
subtle distinctions between users. To address these challenges, we proposed
SeGA, preference-aware self-contrastive learning for anomalous user detection,
which leverages heterogeneous entities and their relations in the Twittersphere
to detect anomalous users with different malicious strategies. SeGA utilizes
the knowledge of large language models to summarize user preferences via posts.
In addition, integrating user preferences with prompts as pseudo-labels for
preference-aware self-contrastive learning enables the model to learn
multifaceted aspects for describing the behaviors of users. Extensive
experiments on the proposed TwBNT benchmark demonstrate that SeGA significantly
outperforms the state-of-the-art methods (+3.5\% ~ 27.6\%) and empirically
validate the effectiveness of the model design and pre-training strategies. Our
code and data are publicly available at https://github.com/ying0409/SeGA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1&quot;&gt;Ying-Ying Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wei-Yao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1&quot;&gt;Wen-Chih Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11559">
<title>Android Malware Detection with Unbiased Confidence Guarantees. (arXiv:2312.11559v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.11559</link>
<description rdf:parseType="Literal">&lt;p&gt;The impressive growth of smartphone devices in combination with the rising
ubiquity of using mobile platforms for sensitive applications such as Internet
banking, have triggered a rapid increase in mobile malware. In recent
literature, many studies examine Machine Learning techniques, as the most
promising approach for mobile malware detection, without however quantifying
the uncertainty involved in their detections. In this paper, we address this
problem by proposing a machine learning dynamic analysis approach that provides
provably valid confidence guarantees in each malware detection. Moreover the
particular guarantees hold for both the malicious and benign classes
independently and are unaffected by any bias in the data. The proposed approach
is based on a novel machine learning framework, called Conformal Prediction,
combined with a random forests classifier. We examine its performance on a
large-scale dataset collected by installing 1866 malicious and 4816 benign
applications on a real android device. We make this collection of dynamic
analysis data available to the research community. The obtained experimental
results demonstrate the empirical validity, usefulness and unbiased nature of
the outputs produced by the proposed approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papadopoulos_H/0/1/0/all/0/1&quot;&gt;Harris Papadopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Georgiou_N/0/1/0/all/0/1&quot;&gt;Nestoras Georgiou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eliades_C/0/1/0/all/0/1&quot;&gt;Charalambos Eliades&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konstantinidis_A/0/1/0/all/0/1&quot;&gt;Andreas Konstantinidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11560">
<title>Emergence Learning: A Rising Direction from Emergent Abilities and a Monosemanticity-Based Study. (arXiv:2312.11560v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11560</link>
<description rdf:parseType="Literal">&lt;p&gt;In the past 20 years, artificial neural networks have become dominant in
various areas, continually growing in scale. However, the current analysis of
large models has mainly focused on functionality, overlooking the influence of
scale differences on their properties. To address this, we propose the concept
of Emergence Learning, which emphasizes the significance of scale. By studying
models of different scales, we have identified a key factor in achieving higher
performance in large models: the decrease of monosemantic neurons. Building on
this insight, we propose a proactive approach to inhibit monosemanticity for
improved performance. Our solution involves a two-phase process that includes
monosemantic neuron detection and inhibition, supported by theoretical
analysis. Experimental results on various tasks and neural networks demonstrate
the effectiveness of our proposed method.
&lt;/p&gt;
&lt;p&gt;Following the idea of Emergence Learning, though drawing inspiration from
scaling phenomena, the applicability of our method is not restricted to large
scale alone. Therefore, the experiment is self-contained. However, extending
this research to very large-scale datasets is appealing yet impossible for
research departments due to limited resources. We are delighted to share the
first co-authorship and eagerly await collaboration from any AI company before
submission.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiachuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Di_S/0/1/0/all/0/1&quot;&gt;Shimin Di&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_C/0/1/0/all/0/1&quot;&gt;Charles Wang Wai Ng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11561">
<title>COPD-FlowNet: Elevating Non-invasive COPD Diagnosis with CFD Simulations. (arXiv:2312.11561v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11561</link>
<description rdf:parseType="Literal">&lt;p&gt;Chronic Obstructive Pulmonary Disorder (COPD) is a prevalent respiratory
disease that significantly impacts the quality of life of affected individuals.
This paper presents COPDFlowNet, a novel deep-learning framework that leverages
a custom Generative Adversarial Network (GAN) to generate synthetic
Computational Fluid Dynamics (CFD) velocity flow field images specific to the
trachea of COPD patients. These synthetic images serve as a valuable resource
for data augmentation and model training. Additionally, COPDFlowNet
incorporates a custom Convolutional Neural Network (CNN) architecture to
predict the location of the obstruction site.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tyagi_A/0/1/0/all/0/1&quot;&gt;Aryan Tyagi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1&quot;&gt;Aryaman Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_S/0/1/0/all/0/1&quot;&gt;Shubhanshu Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1&quot;&gt;Raj Kumar Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11562">
<title>A Survey of Reasoning with Foundation Models. (arXiv:2312.11562v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.11562</link>
<description rdf:parseType="Literal">&lt;p&gt;Reasoning, a crucial ability for complex problem-solving, plays a pivotal
role in various real-world settings such as negotiation, medical diagnosis, and
criminal investigation. It serves as a fundamental methodology in the field of
Artificial General Intelligence (AGI). With the ongoing development of
foundation models, there is a growing interest in exploring their abilities in
reasoning tasks. In this paper, we introduce seminal foundation models proposed
or adaptable for reasoning, highlighting the latest advancements in various
reasoning tasks, methods, and benchmarks. We then delve into the potential
future directions behind the emergence of reasoning abilities within foundation
models. We also discuss the relevance of multimodal learning, autonomous
agents, and super alignment in the context of reasoning. By discussing these
future research directions, we hope to inspire researchers in their exploration
of this field, stimulate further advancements in reasoning with foundation
models, and contribute to the development of AGI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiankai Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Chuanyang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1&quot;&gt;Enze Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhengying Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_R/0/1/0/all/0/1&quot;&gt;Ruihang Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1&quot;&gt;Jianing Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiaqi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1&quot;&gt;Mingyu Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_M/0/1/0/all/0/1&quot;&gt;Mengzhe Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yue Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenhai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junsong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Zhangyue Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xiaozhe Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Junxian He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1&quot;&gt;Wu Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xihui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yu Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Ming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1&quot;&gt;Pheng Ann Heng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1&quot;&gt;Jifeng Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingdong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Ji-Rong Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1&quot;&gt;Xipeng Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yike Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Hui Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenguo Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11563">
<title>A review-based study on different Text-to-Speech technologies. (arXiv:2312.11563v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2312.11563</link>
<description rdf:parseType="Literal">&lt;p&gt;This research paper presents a comprehensive review-based study on various
Text-to-Speech (TTS) technologies. TTS technology is an important aspect of
human-computer interaction, enabling machines to convert written text into
audible speech. The paper examines the different TTS technologies available,
including concatenative TTS, formant synthesis TTS, and statistical parametric
TTS. The study focuses on comparing the advantages and limitations of these
technologies in terms of their naturalness of voice, the level of complexity of
the system, and their suitability for different applications. In addition, the
paper explores the latest advancements in TTS technology, including neural TTS
and hybrid TTS. The findings of this research will provide valuable insights
for researchers, developers, and users who want to understand the different TTS
technologies and their suitability for specific applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1&quot;&gt;Md. Jalal Uddin Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hussan_A/0/1/0/all/0/1&quot;&gt;Ashab Hussan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11566">
<title>Towards AI-driven Integrative Emissions Monitoring &amp; Management for Nature-Based Climate Solutions. (arXiv:2312.11566v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11566</link>
<description rdf:parseType="Literal">&lt;p&gt;AI has been proposed as an important tool to support several efforts related
to nature-based climate solutions such as the detection of wildfires that
affect forests and vegetation-based offsets. While this and other use-cases
provide important demonstrative value of the power of AI in climate change
mitigation, such efforts have typically been undertaken in silos, without
awareness of the integrative nature of real-world climate policy-making. In
this paper, we propose a novel overarching framework for AI-aided integrated
and comprehensive decision support for various aspects of nature-based climate
decision-making. Focusing on vegetation-based solutions such as forests, we
demonstrate how different AI-aided decision support models such as AI-aided
wildfire detection, AI-aided vegetation carbon stock assessment, reversal risk
mitigation, and disaster response planning can be integrated into a
comprehensive framework. Rather than being disparate elements, we posit that
the exchange of data and analytical results across elements of the framework,
and careful mitigation of uncertainty propagation will provide tremendous value
relative to the status-quo for real-world climate policy-making.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oladeji_O/0/1/0/all/0/1&quot;&gt;Olamide Oladeji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mousavi_S/0/1/0/all/0/1&quot;&gt;Seyed Shahabeddin Mousavi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11571">
<title>Model Stealing Attack against Recommender System. (arXiv:2312.11571v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.11571</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies have demonstrated the vulnerability of recommender systems to
data privacy attacks. However, research on the threat to model privacy in
recommender systems, such as model stealing attacks, is still in its infancy.
Some adversarial attacks have achieved model stealing attacks against
recommender systems, to some extent, by collecting abundant training data of
the target model (target data) or making a mass of queries. In this paper, we
constrain the volume of available target data and queries and utilize auxiliary
data, which shares the item set with the target data, to promote model stealing
attacks. Although the target model treats target and auxiliary data
differently, their similar behavior patterns allow them to be fused using an
attention mechanism to assist attacks. Besides, we design stealing functions to
effectively extract the recommendation list obtained by querying the target
model. Experimental results show that the proposed methods are applicable to
most recommender systems and various scenarios and exhibit excellent attack
performance on multiple datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhihao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1&quot;&gt;Rui Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chenwang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1&quot;&gt;Defu Lian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1&quot;&gt;Enhong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11572">
<title>Regularized Conditional Alignment for Multi-Domain Text Classification. (arXiv:2312.11572v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.11572</link>
<description rdf:parseType="Literal">&lt;p&gt;The most successful multi-domain text classification (MDTC) approaches employ
the shared-private paradigm to facilitate the enhancement of domain-invariant
features through domain-specific attributes. Additionally, they employ
adversarial training to align marginal feature distributions. Nevertheless,
these methodologies encounter two primary challenges: (1) Neglecting
class-aware information during adversarial alignment poses a risk of
misalignment; (2) The limited availability of labeled data across multiple
domains fails to ensure adequate discriminative capacity for the model. To
tackle these issues, we propose a method called Regularized Conditional
Alignment (RCA) to align the joint distributions of domains and classes, thus
matching features within the same category and amplifying the discriminative
qualities of acquired features. Moreover, we employ entropy minimization and
virtual adversarial training to constrain the uncertainty of predictions
pertaining to unlabeled data and enhance the model&apos;s robustness. Empirical
results on two benchmark datasets demonstrate that our RCA approach outperforms
state-of-the-art MDTC techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Juntao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yuan Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11573">
<title>Estimation of individual causal effects in network setup for multiple treatments. (arXiv:2312.11573v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11573</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of estimation of Individual Treatment Effects (ITE) in
the context of multiple treatments and networked observational data. Leveraging
the network information, we aim to utilize hidden confounders that may not be
directly accessible in the observed data, thereby enhancing the practical
applicability of the strong ignorability assumption. To achieve this, we first
employ Graph Convolutional Networks (GCN) to learn a shared representation of
the confounders. Then, our approach utilizes separate neural networks to infer
potential outcomes for each treatment. We design a loss function as a weighted
combination of two components: representation loss and Mean Squared Error (MSE)
loss on the factual outcomes. To measure the representation loss, we extend
existing metrics such as Wasserstein and Maximum Mean Discrepancy (MMD) from
the binary treatment setting to the multiple treatments scenario. To validate
the effectiveness of our proposed methodology, we conduct a series of
experiments on the benchmark datasets such as BlogCatalog and Flickr. The
experimental results consistently demonstrate the superior performance of our
models when compared to baseline methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thorat_A/0/1/0/all/0/1&quot;&gt;Abhinav Thorat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolla_R/0/1/0/all/0/1&quot;&gt;Ravi Kolla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedanekar_N/0/1/0/all/0/1&quot;&gt;Niranjan Pedanekar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Onoe_N/0/1/0/all/0/1&quot;&gt;Naoyuki Onoe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11581">
<title>Protect Your Score: Contact Tracing With Differential Privacy Guarantees. (arXiv:2312.11581v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.11581</link>
<description rdf:parseType="Literal">&lt;p&gt;The pandemic in 2020 and 2021 had enormous economic and societal
consequences, and studies show that contact tracing algorithms can be key in
the early containment of the virus. While large strides have been made towards
more effective contact tracing algorithms, we argue that privacy concerns
currently hold deployment back. The essence of a contact tracing algorithm
constitutes the communication of a risk score. Yet, it is precisely the
communication and release of this score to a user that an adversary can
leverage to gauge the private health status of an individual. We pinpoint a
realistic attack scenario and propose a contact tracing algorithm with
differential privacy guarantees against this attack. The algorithm is tested on
the two most widely used agent-based COVID19 simulators and demonstrates
superior performance in a wide range of settings. Especially for realistic test
scenarios and while releasing each risk score with epsilon=1 differential
privacy, we achieve a two to ten-fold reduction in the infection rate of the
virus. To the best of our knowledge, this presents the first contact tracing
algorithm with differential privacy guarantees when revealing risk scores for
COVID19.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romijnders_R/0/1/0/all/0/1&quot;&gt;Rob Romijnders&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Louizos_C/0/1/0/all/0/1&quot;&gt;Christos Louizos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1&quot;&gt;Yuki M. Asano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1&quot;&gt;Max Welling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11582">
<title>Shapley-PC: Constraint-based Causal Structure Learning with Shapley Values. (arXiv:2312.11582v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11582</link>
<description rdf:parseType="Literal">&lt;p&gt;Causal Structure Learning (CSL), amounting to extracting causal relations
among the variables in a dataset, is widely perceived as an important step
towards robust and transparent models. Constraint-based CSL leverages
conditional independence tests to perform causal discovery. We propose
Shapley-PC, a novel method to improve constraint-based CSL algorithms by using
Shapley values over the possible conditioning sets to decide which variables
are responsible for the observed conditional (in)dependences. We prove
soundness and asymptotic consistency and demonstrate that it can outperform
state-of-the-art constraint-based, search-based and functional causal
model-based methods, according to standard metrics in CSL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russo_F/0/1/0/all/0/1&quot;&gt;Fabrizio Russo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toni_F/0/1/0/all/0/1&quot;&gt;Francesca Toni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11583">
<title>AI-Based Energy Transportation Safety: Pipeline Radial Threat Estimation Using Intelligent Sensing System. (arXiv:2312.11583v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11583</link>
<description rdf:parseType="Literal">&lt;p&gt;The application of artificial intelligence technology has greatly enhanced
and fortified the safety of energy pipelines, particularly in safeguarding
against external threats. The predominant methods involve the integration of
intelligent sensors to detect external vibration, enabling the identification
of event types and locations, thereby replacing manual detection methods.
However, practical implementation has exposed a limitation in current methods -
their constrained ability to accurately discern the spatial dimensions of
external signals, which complicates the authentication of threat events. Our
research endeavors to overcome the above issues by harnessing deep learning
techniques to achieve a more fine-grained recognition and localization process.
This refinement is crucial in effectively identifying genuine threats to
pipelines, thus enhancing the safety of energy transportation. This paper
proposes a radial threat estimation method for energy pipelines based on
distributed optical fiber sensing technology. Specifically, we introduce a
continuous multi-view and multi-domain feature fusion methodology to extract
comprehensive signal features and construct a threat estimation and recognition
network. The utilization of collected acoustic signal data is optimized, and
the underlying principle is elucidated. Moreover, we incorporate the concept of
transfer learning through a pre-trained model, enhancing both recognition
accuracy and training efficiency. Empirical evidence gathered from real-world
scenarios underscores the efficacy of our method, notably in its substantial
reduction of false alarms and remarkable gains in recognition accuracy. More
generally, our method exhibits versatility and can be extrapolated to a broader
spectrum of recognition tasks and scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Chengyuan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yiyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kaixiang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haifeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qinmin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;C. L. Philip Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11584">
<title>ContraNovo: A Contrastive Learning Approach to Enhance De Novo Peptide Sequencing. (arXiv:2312.11584v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/2312.11584</link>
<description rdf:parseType="Literal">&lt;p&gt;De novo peptide sequencing from mass spectrometry (MS) data is a critical
task in proteomics research. Traditional de novo algorithms have encountered a
bottleneck in accuracy due to the inherent complexity of proteomics data. While
deep learning-based methods have shown progress, they reduce the problem to a
translation task, potentially overlooking critical nuances between spectra and
peptides. In our research, we present ContraNovo, a pioneering algorithm that
leverages contrastive learning to extract the relationship between spectra and
peptides and incorporates the mass information into peptide decoding, aiming to
address these intricacies more efficiently. Through rigorous evaluations on two
benchmark datasets, ContraNovo consistently outshines contemporary
state-of-the-art solutions, underscoring its promising potential in enhancing
de novo peptide sequencing. The source code is available at
https://github.com/BEAM-Labs/ContraNovo.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Jin_Z/0/1/0/all/0/1&quot;&gt;Zhi Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Sheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ling_T/0/1/0/all/0/1&quot;&gt;Tianze Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Dong_N/0/1/0/all/0/1&quot;&gt;Nanqing Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ouyang_W/0/1/0/all/0/1&quot;&gt;Wanli Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gao_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Chang_C/0/1/0/all/0/1&quot;&gt;Cheng Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Siqi Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11598">
<title>SkillDiffuser: Interpretable Hierarchical Planning via Skill Abstractions in Diffusion-Based Task Execution. (arXiv:2312.11598v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.11598</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have demonstrated strong potential for robotic trajectory
planning. However, generating coherent and long-horizon trajectories from
high-level instructions remains challenging, especially for complex tasks
requiring multiple sequential skills. We propose SkillDiffuser, an end-to-end
hierarchical planning framework integrating interpretable skill learning with
conditional diffusion planning to address this problem. At the higher level,
the skill abstraction module learns discrete, human-understandable skill
representations from visual observations and language instructions. These
learned skill embeddings are then used to condition the diffusion model to
generate customized latent trajectories aligned with the skills. It allows for
generating diverse state trajectories that adhere to the learnable skills. By
integrating skill learning with conditional trajectory generation,
SkillDiffuser produces coherent behavior following abstract instructions across
diverse tasks. Experiments on multi-task robotic manipulation benchmarks like
Meta-World and LOReL demonstrate state-of-the-art performance and
human-interpretable skill representations from SkillDiffuser.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1&quot;&gt;Zhixuan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1&quot;&gt;Yao Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Hengbo Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1&quot;&gt;Masayoshi Tomizuka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1&quot;&gt;Mingyu Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11629">
<title>Residual ANODE. (arXiv:2312.11629v1 [hep-ph])</title>
<link>http://arxiv.org/abs/2312.11629</link>
<description rdf:parseType="Literal">&lt;p&gt;We present R-ANODE, a new method for data-driven, model-agnostic resonant
anomaly detection that raises the bar for both performance and
interpretability. The key to R-ANODE is to enhance the inductive bias of the
anomaly detection task by fitting a normalizing flow directly to the small and
unknown signal component, while holding fixed a background model (also a
normalizing flow) learned from sidebands. In doing so, R-ANODE is able to
outperform all classifier-based, weakly-supervised approaches, as well as the
previous ANODE method which fit a density estimator to all of the data in the
signal region instead of just the signal. We show that the method works equally
well whether the unknown signal fraction is learned or fixed, and is even
robust to signal fraction misspecification. Finally, with the learned signal
model we can sample and gain qualitative insights into the underlying anomaly,
which greatly enhances the interpretability of resonant anomaly detection and
offers the possibility of simultaneously discovering and characterizing the new
physics that could be hiding in the data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Das_R/0/1/0/all/0/1&quot;&gt;Ranit Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Kasieczka_G/0/1/0/all/0/1&quot;&gt;Gregor Kasieczka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Shih_D/0/1/0/all/0/1&quot;&gt;David Shih&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11663">
<title>Eliciting Kemeny Rankings. (arXiv:2312.11663v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11663</link>
<description rdf:parseType="Literal">&lt;p&gt;We formulate the problem of eliciting agents&apos; preferences with the goal of
finding a Kemeny ranking as a Dueling Bandits problem. Here the bandits&apos; arms
correspond to alternatives that need to be ranked and the feedback corresponds
to a pairwise comparison between alternatives by a randomly sampled agent. We
consider both sampling with and without replacement, i.e., the possibility to
ask the same agent about some comparison multiple times or not.
&lt;/p&gt;
&lt;p&gt;We find approximation bounds for Kemeny rankings dependant on confidence
intervals over estimated winning probabilities of arms. Based on these we state
algorithms to find Probably Approximately Correct (PAC) solutions and elaborate
on their sample complexity for sampling with or without replacement.
Furthermore, if all agents&apos; preferences are strict rankings over the
alternatives, we provide means to prune confidence intervals and thereby guide
a more efficient elicitation. We formulate several adaptive sampling methods
that use look-aheads to estimate how much confidence intervals (and thus
approximation guarantees) might be tightened. All described methods are
compared on synthetic data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+George_A/0/1/0/all/0/1&quot;&gt;Anne-Marie George&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dimitrakakis_C/0/1/0/all/0/1&quot;&gt;Christos Dimitrakakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11669">
<title>Prediction and Control in Continual Reinforcement Learning. (arXiv:2312.11669v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11669</link>
<description rdf:parseType="Literal">&lt;p&gt;Temporal difference (TD) learning is often used to update the estimate of the
value function which is used by RL agents to extract useful policies. In this
paper, we focus on value function estimation in continual reinforcement
learning. We propose to decompose the value function into two components which
update at different timescales: a permanent value function, which holds general
knowledge that persists over time, and a transient value function, which allows
quick adaptation to new situations. We establish theoretical results showing
that our approach is well suited for continual learning and draw connections to
the complementary learning systems (CLS) theory from neuroscience. Empirically,
this approach improves performance significantly on both prediction and control
problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anand_N/0/1/0/all/0/1&quot;&gt;Nishanth Anand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1&quot;&gt;Doina Precup&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11671">
<title>Evaluating Language-Model Agents on Realistic Autonomous Tasks. (arXiv:2312.11671v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.11671</link>
<description rdf:parseType="Literal">&lt;p&gt;In this report, we explore the ability of language model agents to acquire
resources, create copies of themselves, and adapt to novel challenges they
encounter in the wild. We refer to this cluster of capabilities as &quot;autonomous
replication and adaptation&quot; or ARA. We believe that systems capable of ARA
could have wide-reaching and hard-to-anticipate consequences, and that
measuring and forecasting ARA may be useful for informing measures around
security, monitoring, and alignment. Additionally, once a system is capable of
ARA, placing bounds on a system&apos;s capabilities may become significantly more
difficult.
&lt;/p&gt;
&lt;p&gt;We construct four simple example agents that combine language models with
tools that allow them to take actions in the world. We then evaluate these
agents on 12 tasks relevant to ARA. We find that these language model agents
can only complete the easiest tasks from this list, although they make some
progress on the more challenging tasks. Unfortunately, these evaluations are
not adequate to rule out the possibility that near-future agents will be
capable of ARA. In particular, we do not think that these evaluations provide
good assurance that the ``next generation&apos;&apos; of language models (e.g. 100x
effective compute scaleup on existing models) will not yield agents capable of
ARA, unless intermediate evaluations are performed during pretraining.
Relatedly, we expect that fine-tuning of the existing models could produce
substantially more competent agents, even if the fine-tuning is not directly
targeted at ARA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kinniment_M/0/1/0/all/0/1&quot;&gt;Megan Kinniment&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sato_L/0/1/0/all/0/1&quot;&gt;Lucas Jun Koba Sato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1&quot;&gt;Haoxing Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodrich_B/0/1/0/all/0/1&quot;&gt;Brian Goodrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasin_M/0/1/0/all/0/1&quot;&gt;Max Hasin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_L/0/1/0/all/0/1&quot;&gt;Lawrence Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miles_L/0/1/0/all/0/1&quot;&gt;Luke Harold Miles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1&quot;&gt;Tao R. Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wijk_H/0/1/0/all/0/1&quot;&gt;Hjalmar Wijk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burget_J/0/1/0/all/0/1&quot;&gt;Joel Burget&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_A/0/1/0/all/0/1&quot;&gt;Aaron Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barnes_E/0/1/0/all/0/1&quot;&gt;Elizabeth Barnes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christiano_P/0/1/0/all/0/1&quot;&gt;Paul Christiano&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11703">
<title>Shaping Political Discourse using multi-source News Summarization. (arXiv:2312.11703v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.11703</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-document summarization is the process of automatically generating a
concise summary of multiple documents related to the same topic. This summary
can help users quickly understand the key information from a large collection
of documents. Multi-document summarization systems are more complex than
single-document summarization systems due to the need to identify and combine
information from multiple sources. In this paper, we have developed a machine
learning model that generates a concise summary of a topic from multiple news
documents. The model is designed to be unbiased by sampling its input equally
from all the different aspects of the topic, even if the majority of the news
sources lean one way.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajan_C/0/1/0/all/0/1&quot;&gt;Charles Rajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asnani_N/0/1/0/all/0/1&quot;&gt;Nishit Asnani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Shreya Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11707">
<title>Unified framework for diffusion generative models in SO(3): applications in computer vision and astrophysics. (arXiv:2312.11707v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11707</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion-based generative models represent the current state-of-the-art for
image generation. However, standard diffusion models are based on Euclidean
geometry and do not translate directly to manifold-valued data. In this work,
we develop extensions of both score-based generative models (SGMs) and
Denoising Diffusion Probabilistic Models (DDPMs) to the Lie group of 3D
rotations, SO(3). SO(3) is of particular interest in many disciplines such as
robotics, biochemistry and astronomy/cosmology science. Contrary to more
general Riemannian manifolds, SO(3) admits a tractable solution to heat
diffusion, and allows us to implement efficient training of diffusion models.
We apply both SO(3) DDPMs and SGMs to synthetic densities on SO(3) and
demonstrate state-of-the-art results. Additionally, we demonstrate the
practicality of our model on pose estimation tasks and in predicting correlated
galaxy orientations for astrophysics/cosmology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jagvaral_Y/0/1/0/all/0/1&quot;&gt;Yesukhei Jagvaral&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lanusse_F/0/1/0/all/0/1&quot;&gt;Francois Lanusse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mandelbaum_R/0/1/0/all/0/1&quot;&gt;Rachel Mandelbaum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11708">
<title>Accelerating the prediction of inorganic surfaces with machine learning interatomic potentials. (arXiv:2312.11708v1 [cond-mat.mtrl-sci])</title>
<link>http://arxiv.org/abs/2312.11708</link>
<description rdf:parseType="Literal">&lt;p&gt;The surface properties of solid-state materials often dictate their
functionality, especially for applications where nanoscale effects become
important. The relevant surface(s) and their properties are determined, in
large part, by the materials synthesis or operating conditions. These
conditions dictate thermodynamic driving forces and kinetic rates responsible
for yielding the observed surface structure and morphology. Computational
surface science methods have long been applied to connect thermochemical
conditions to surface phase stability, particularly in the heterogeneous
catalysis and thin film growth communities. This review provides a brief
introduction to first-principles approaches to compute surface phase diagrams
before introducing emerging data-driven approaches. The remainder of the review
focuses on the application of machine learning, predominantly in the form of
learned interatomic potentials, to study complex surfaces. As machine learning
algorithms and large datasets on which to train them become more commonplace in
materials science, computational methods are poised to become even more
predictive and powerful for modeling the complexities of inorganic surfaces at
the nanoscale.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Noordhoek_K/0/1/0/all/0/1&quot;&gt;Kyle Noordhoek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Bartel_C/0/1/0/all/0/1&quot;&gt;Christopher J. Bartel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11712">
<title>A Simple and Practical Method for Reducing the Disparate Impact of Differential Privacy. (arXiv:2312.11712v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.11712</link>
<description rdf:parseType="Literal">&lt;p&gt;Differentially private (DP) mechanisms have been deployed in a variety of
high-impact social settings (perhaps most notably by the U.S. Census). Since
all DP mechanisms involve adding noise to results of statistical queries, they
are expected to impact our ability to accurately analyze and learn from data,
in effect trading off privacy with utility. Alarmingly, the impact of DP on
utility can vary significantly among different sub-populations. A simple way to
reduce this disparity is with stratification. First compute an independent
private estimate for each group in the data set (which may be the intersection
of several protected classes), then, to compute estimates of global statistics,
appropriately recombine these group estimates. Our main observation is that
naive stratification often yields high-accuracy estimates of population-level
statistics, without the need for additional privacy budget. We support this
observation theoretically and empirically. Our theoretical results center on
the private mean estimation problem, while our empirical results center on
extensive experiments on private data synthesis to demonstrate the
effectiveness of stratification on a variety of private mechanisms. Overall, we
argue that this straightforward approach provides a strong baseline against
which future work on reducing utility disparities of DP mechanisms should be
compared.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosenblatt_L/0/1/0/all/0/1&quot;&gt;Lucas Rosenblatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoyanovich_J/0/1/0/all/0/1&quot;&gt;Julia Stoyanovich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1&quot;&gt;Christopher Musco&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11714">
<title>Time-Transformer: Integrating Local and Global Features for Better Time Series Generation. (arXiv:2312.11714v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11714</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating time series data is a promising approach to address data
deficiency problems. However, it is also challenging due to the complex
temporal properties of time series data, including local correlations as well
as global dependencies. Most existing generative models have failed to
effectively learn both the local and global properties of time series data. To
address this open problem, we propose a novel time series generative model
named &apos;Time-Transformer AAE&apos;, which consists of an adversarial autoencoder
(AAE) and a newly designed architecture named &apos;Time-Transformer&apos; within the
decoder. The Time-Transformer first simultaneously learns local and global
features in a layer-wise parallel design, combining the abilities of Temporal
Convolutional Networks and Transformer in extracting local features and global
dependencies respectively. Second, a bidirectional cross attention is proposed
to provide complementary guidance across the two branches and achieve proper
fusion between local and global features. Experimental results demonstrate that
our model can outperform existing state-of-the-art models in 5 out of 6
datasets, specifically on those with data containing both global and local
properties. Furthermore, we highlight our model&apos;s advantage on handling this
kind of data via an artificial dataset. Finally, we show our model&apos;s ability to
address a real-world problem: data augmentation to support learning with small
datasets and imbalanced datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuansan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wijewickrema_S/0/1/0/all/0/1&quot;&gt;Sudanthi Wijewickrema&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1&quot;&gt;Ang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bester_C/0/1/0/all/0/1&quot;&gt;Christofer Bester&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OLeary_S/0/1/0/all/0/1&quot;&gt;Stephen O&amp;#x27;Leary&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bailey_J/0/1/0/all/0/1&quot;&gt;James Bailey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11718">
<title>Human-Machine Teaming for UAVs: An Experimentation Platform. (arXiv:2312.11718v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.11718</link>
<description rdf:parseType="Literal">&lt;p&gt;Full automation is often not achievable or desirable in critical systems with
high-stakes decisions. Instead, human-AI teams can achieve better results. To
research, develop, evaluate, and validate algorithms suited for such teaming,
lightweight experimentation platforms that enable interactions between humans
and multiple AI agents are necessary. However, there are limited examples of
such platforms for defense environments. To address this gap, we present the
Cogment human-machine teaming experimentation platform, which implements
human-machine teaming (HMT) use cases that features heterogeneous multi-agent
systems and can involve learning AI agents, static AI agents, and humans. It is
built on the Cogment platform and has been used for academic research,
including work presented at the ALA workshop at AAMAS this year [1]. With this
platform, we hope to facilitate further research on human-machine teaming in
critical systems and defense environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moujtahid_L/0/1/0/all/0/1&quot;&gt;Laila El Moujtahid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gottipati_S/0/1/0/all/0/1&quot;&gt;Sai Krishna Gottipati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mars_C/0/1/0/all/0/1&quot;&gt;Clod&amp;#xe9;ric Mars&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_M/0/1/0/all/0/1&quot;&gt;Matthew E. Taylor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11730">
<title>Stronger Graph Transformer with Regularized Attention Scores. (arXiv:2312.11730v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11730</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks are notorious for its memory consumption. A recent
Transformer based GNN called Graph Transformer are shown to obtain superior
performances when long range dependencies exist. However, combining graph data
and Transformer architecture led to a combinationally worse memory issue. We
propose a novel version of &quot;edge regularization technique&quot; that alleviates the
need for Positional Encoding and ultimately alleviate GT&apos;s out of memory issue.
We observe that it is not clear whether having an edge regularization on top of
positional encoding is helpful. However, it seems evident when no positional
encoding is applied, edge regularization technique indeed stably improves GT&apos;s
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ku_E/0/1/0/all/0/1&quot;&gt;Eugene Ku&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arunraj_S/0/1/0/all/0/1&quot;&gt;Swetha Arunraj&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11735">
<title>Multiple Hypothesis Dropout: Estimating the Parameters of Multi-Modal Output Distributions. (arXiv:2312.11735v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11735</link>
<description rdf:parseType="Literal">&lt;p&gt;In many real-world applications, from robotics to pedestrian trajectory
prediction, there is a need to predict multiple real-valued outputs to
represent several potential scenarios. Current deep learning techniques to
address multiple-output problems are based on two main methodologies: (1)
mixture density networks, which suffer from poor stability at high dimensions,
or (2) multiple choice learning (MCL), an approach that uses $M$ single-output
functions, each only producing a point estimate hypothesis. This paper presents
a Mixture of Multiple-Output functions (MoM) approach using a novel variant of
dropout, Multiple Hypothesis Dropout. Unlike traditional MCL-based approaches,
each multiple-output function not only estimates the mean but also the variance
for its hypothesis. This is achieved through a novel stochastic winner-take-all
loss which allows each multiple-output function to estimate variance through
the spread of its subnetwork predictions. Experiments on supervised learning
problems illustrate that our approach outperforms existing solutions for
reconstructing multimodal output distributions. Additional studies on
unsupervised learning problems show that estimating the parameters of latent
posterior distributions within a discrete autoencoder significantly improves
codebook efficiency, sample quality, precision and recall.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;David D. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liebowitz_D/0/1/0/all/0/1&quot;&gt;David Liebowitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nepal_S/0/1/0/all/0/1&quot;&gt;Surya Nepal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanhere_S/0/1/0/all/0/1&quot;&gt;Salil S. Kanhere&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11742">
<title>ACCL+: an FPGA-Based Collective Engine for Distributed Applications. (arXiv:2312.11742v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2312.11742</link>
<description rdf:parseType="Literal">&lt;p&gt;FPGAs are increasingly prevalent in cloud deployments, serving as Smart NICs
or network-attached accelerators. Despite their potential, developing
distributed FPGA-accelerated applications remains cumbersome due to the lack of
appropriate infrastructure and communication abstractions. To facilitate the
development of distributed applications with FPGAs, in this paper we propose
ACCL+, an open-source versatile FPGA-based collective communication library.
Portable across different platforms and supporting UDP, TCP, as well as RDMA,
ACCL+ empowers FPGA applications to initiate direct FPGA-to-FPGA collective
communication. Additionally, it can serve as a collective offload engine for
CPU applications, freeing the CPU from networking tasks. It is user-extensible,
allowing new collectives to be implemented and deployed without having to
re-synthesize the FPGA circuit. We evaluated ACCL+ on an FPGA cluster with 100
Gb/s networking, comparing its performance against software MPI over RDMA. The
results demonstrate ACCL+&apos;s significant advantages for FPGA-based distributed
applications and highly competitive performance for CPU applications. We
showcase ACCL+&apos;s dual role with two use cases: seamlessly integrating as a
collective offload engine to distribute CPU-based vector-matrix multiplication,
and serving as a crucial and efficient component in designing fully FPGA-based
distributed deep-learning recommendation inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zhenhao He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korolija_D/0/1/0/all/0/1&quot;&gt;Dario Korolija&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramhorst_B/0/1/0/all/0/1&quot;&gt;Benjamin Ramhorst&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laan_T/0/1/0/all/0/1&quot;&gt;Tristan Laan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petrica_L/0/1/0/all/0/1&quot;&gt;Lucian Petrica&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blott_M/0/1/0/all/0/1&quot;&gt;Michaela Blott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alonso_G/0/1/0/all/0/1&quot;&gt;Gustavo Alonso&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11747">
<title>Robust Stochastic Graph Generator for Counterfactual Explanations. (arXiv:2312.11747v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11747</link>
<description rdf:parseType="Literal">&lt;p&gt;Counterfactual Explanation (CE) techniques have garnered attention as a means
to provide insights to the users engaging with AI systems. While extensively
researched in domains such as medical imaging and autonomous vehicles, Graph
Counterfactual Explanation (GCE) methods have been comparatively
under-explored. GCEs generate a new graph similar to the original one, with a
different outcome grounded on the underlying predictive model. Among these GCE
techniques, those rooted in generative mechanisms have received relatively
limited investigation despite demonstrating impressive accomplishments in other
domains, such as artistic styles and natural language modelling. The preference
for generative explainers stems from their capacity to generate counterfactual
instances during inference, leveraging autonomously acquired perturbations of
the input graph. Motivated by the rationales above, our study introduces
RSGG-CE, a novel Robust Stochastic Graph Generator for Counterfactual
Explanations able to produce counterfactual examples from the learned latent
space considering a partially ordered generation sequence. Furthermore, we
undertake quantitative and qualitative analyses to compare RSGG-CE&apos;s
performance against SoA generative explainers, highlighting its increased
ability to engendering plausible counterfactual candidates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prado_Romero_M/0/1/0/all/0/1&quot;&gt;Mario Alfonso Prado-Romero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prenkaj_B/0/1/0/all/0/1&quot;&gt;Bardh Prenkaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stilo_G/0/1/0/all/0/1&quot;&gt;Giovanni Stilo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11752">
<title>Learning a Diffusion Model Policy from Rewards via Q-Score Matching. (arXiv:2312.11752v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11752</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have become a popular choice for representing actor policies
in behavior cloning and offline reinforcement learning. This is due to their
natural ability to optimize an expressive class of distributions over a
continuous space. However, previous works fail to exploit the score-based
structure of diffusion models, and instead utilize a simple behavior cloning
term to train the actor, limiting their ability in the actor-critic setting. In
this paper, we focus on off-policy reinforcement learning and propose a new
method for learning a diffusion model policy that exploits the linked structure
between the score of the policy and the action gradient of the Q-function. We
denote this method Q-score matching and provide theoretical justification for
this approach. We conduct experiments in simulated environments to demonstrate
the effectiveness of our proposed method and compare to popular baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Psenka_M/0/1/0/all/0/1&quot;&gt;Michael Psenka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Escontrela_A/0/1/0/all/0/1&quot;&gt;Alejandro Escontrela&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1&quot;&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yi Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11754">
<title>A Bayesian Spatial Model to Correct Under-Reporting in Urban Crowdsourcing. (arXiv:2312.11754v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2312.11754</link>
<description rdf:parseType="Literal">&lt;p&gt;Decision-makers often observe the occurrence of events through a reporting
process. City governments, for example, rely on resident reports to find and
then resolve urban infrastructural problems such as fallen street trees,
flooded basements, or rat infestations. Without additional assumptions, there
is no way to distinguish events that occur but are not reported from events
that truly did not occur--a fundamental problem in settings with
positive-unlabeled data. Because disparities in reporting rates correlate with
resident demographics, addressing incidents only on the basis of reports leads
to systematic neglect in neighborhoods that are less likely to report events.
We show how to overcome this challenge by leveraging the fact that events are
spatially correlated. Our framework uses a Bayesian spatial latent variable
model to infer event occurrence probabilities and applies it to storm-induced
flooding reports in New York City, further pooling results across multiple
storms. We show that a model accounting for under-reporting and spatial
correlation predicts future reports more accurately than other models, and
further induces a more equitable set of inspections: its allocations better
reflect the population and provide equitable service to non-white, less
traditionally educated, and lower-income residents. This finding reflects
heterogeneous reporting behavior learned by the model: reporting rates are
higher in Census tracts with higher populations, proportions of white
residents, and proportions of owner-occupied households. Our work lays the
groundwork for more equitable proactive government services, even with
disparate reporting behavior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agostini_G/0/1/0/all/0/1&quot;&gt;Gabriel Agostini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pierson_E/0/1/0/all/0/1&quot;&gt;Emma Pierson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_N/0/1/0/all/0/1&quot;&gt;Nikhil Garg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11768">
<title>Curriculum Learning for Cooperation in Multi-Agent Reinforcement Learning. (arXiv:2312.11768v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.11768</link>
<description rdf:parseType="Literal">&lt;p&gt;While there has been significant progress in curriculum learning and
continuous learning for training agents to generalize across a wide variety of
environments in the context of single-agent reinforcement learning, it is
unclear if these algorithms would still be valid in a multi-agent setting. In a
competitive setting, a learning agent can be trained by making it compete with
a curriculum of increasingly skilled opponents. However, a general intelligent
agent should also be able to learn to act around other agents and cooperate
with them to achieve common goals. When cooperating with other agents, the
learning agent must (a) learn how to perform the task (or subtask), and (b)
increase the overall team reward. In this paper, we aim to answer the question
of what kind of cooperative teammate, and a curriculum of teammates should a
learning agent be trained with to achieve these two objectives. Our results on
the game Overcooked show that a pre-trained teammate who is less skilled is the
best teammate for overall team reward but the worst for the learning of the
agent. Moreover, somewhat surprisingly, a curriculum of teammates with
decreasing skill levels performs better than other types of curricula.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhati_R/0/1/0/all/0/1&quot;&gt;Rupali Bhati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gottipati_S/0/1/0/all/0/1&quot;&gt;Sai Krishna Gottipati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mars_C/0/1/0/all/0/1&quot;&gt;Clod&amp;#xe9;ric Mars&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_M/0/1/0/all/0/1&quot;&gt;Matthew E. Taylor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11769">
<title>Clustering Mixtures of Bounded Covariance Distributions Under Optimal Separation. (arXiv:2312.11769v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11769</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the clustering problem for mixtures of bounded covariance
distributions, under a fine-grained separation assumption. Specifically, given
samples from a $k$-component mixture distribution $D = \sum_{i =1}^k w_i P_i$,
where each $w_i \ge \alpha$ for some known parameter $\alpha$, and each $P_i$
has unknown covariance $\Sigma_i \preceq \sigma^2_i \cdot I_d$ for some unknown
$\sigma_i$, the goal is to cluster the samples assuming a pairwise mean
separation in the order of $(\sigma_i+\sigma_j)/\sqrt{\alpha}$ between every
pair of components $P_i$ and $P_j$. Our contributions are as follows:
&lt;/p&gt;
&lt;p&gt;For the special case of nearly uniform mixtures, we give the first poly-time
algorithm for this clustering task. Prior work either required separation
scaling with the maximum cluster standard deviation (i.e. $\max_i \sigma_i$)
[DKK+22b] or required both additional structural assumptions and mean
separation scaling as a large degree polynomial in $1/\alpha$ [BKK22].
&lt;/p&gt;
&lt;p&gt;For general-weight mixtures, we point out that accurate clustering is
information-theoretically impossible under our fine-grained mean separation
assumptions. We introduce the notion of a clustering refinement -- a list of
not-too-small subsets satisfying a similar separation, and which can be merged
into a clustering approximating the ground truth -- and show that it is
possible to efficiently compute an accurate clustering refinement of the
samples. Furthermore, under a variant of the &quot;no large sub-cluster&apos;&apos; condition
from in prior work [BKK22], we show that our algorithm outputs an accurate
clustering, not just a refinement, even for general-weight mixtures. As a
corollary, we obtain efficient clustering algorithms for mixtures of
well-conditioned high-dimensional log-concave distributions.
&lt;/p&gt;
&lt;p&gt;Moreover, our algorithm is robust to $\Omega(\alpha)$-fraction of adversarial
outliers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diakonikolas_I/0/1/0/all/0/1&quot;&gt;Ilias Diakonikolas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kane_D/0/1/0/all/0/1&quot;&gt;Daniel M. Kane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jasper C. H. Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pittas_T/0/1/0/all/0/1&quot;&gt;Thanasis Pittas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11779">
<title>Are you talking to [&apos;xem&apos;] or [&apos;x&apos;, &apos;em&apos;]? On Tokenization and Addressing Misgendering in LLMs with Pronoun Tokenization Parity. (arXiv:2312.11779v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.11779</link>
<description rdf:parseType="Literal">&lt;p&gt;A large body of NLP research has documented the ways gender biases manifest
and amplify within large language models (LLMs), though this research has
predominantly operated within a gender binary-centric context. A growing body
of work has identified the harmful limitations of this gender-exclusive
framing; many LLMs cannot correctly and consistently refer to persons outside
the gender binary, especially if they use neopronouns. While data scarcity has
been identified as a possible culprit, the precise mechanisms through which it
influences LLM misgendering remain underexplored. Our work addresses this gap
by studying data scarcity&apos;s role in subword tokenization and, consequently, the
formation of LLM word representations. We uncover how the Byte-Pair Encoding
(BPE) tokenizer, a backbone for many popular LLMs, contributes to neopronoun
misgendering through out-of-vocabulary behavior. We introduce pronoun
tokenization parity (PTP), a novel approach to reduce LLM neopronoun
misgendering by preserving a token&apos;s functional structure. We evaluate PTP&apos;s
efficacy using pronoun consistency-based metrics and a novel syntax-based
metric. Through several controlled experiments, finetuning LLMs with PTP
improves neopronoun consistency from 14.5% to 58.4%, highlighting the
significant role tokenization plays in LLM pronoun consistency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ovalle_A/0/1/0/all/0/1&quot;&gt;Anaelia Ovalle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehrabi_N/0/1/0/all/0/1&quot;&gt;Ninareh Mehrabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1&quot;&gt;Palash Goyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhamala_J/0/1/0/all/0/1&quot;&gt;Jwala Dhamala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1&quot;&gt;Kai-Wei Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1&quot;&gt;Richard Zemel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1&quot;&gt;Aram Galstyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pinter_Y/0/1/0/all/0/1&quot;&gt;Yuval Pinter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1&quot;&gt;Rahul Gupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11788">
<title>Faster Convergence with Multiway Preferences. (arXiv:2312.11788v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11788</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the problem of convex optimization with preference feedback, where
the goal is to minimize a convex function given a weaker form of comparison
queries. Each query consists of two points and the dueling feedback returns a
(noisy) single-bit binary comparison of the function values of the two queried
points. Here we consider the sign-function-based comparison feedback model and
analyze the convergence rates with batched and multiway (argmin of a set
queried points) comparisons. Our main goal is to understand the improved
convergence rates owing to parallelization in sign-feedback-based optimization
problems. Our work is the first to study the problem of convex optimization
with multiway preferences and analyze the optimal convergence rates. Our first
contribution lies in designing efficient algorithms with a convergence rate of
$\smash{\widetilde O}(\frac{d}{\min\{m,d\} \epsilon})$ for $m$-batched
preference feedback where the learner can query $m$-pairs in parallel. We next
study a $m$-multiway comparison (`battling&apos;) feedback, where the learner can
get to see the argmin feedback of $m$-subset of queried points and show a
convergence rate of $\smash{\widetilde O}(\frac{d}{ \min\{\log m,d\}\epsilon
})$. We show further improved convergence rates with an additional assumption
of strong convexity. Finally, we also study the convergence lower bounds for
batched preferences and multiway feedback optimization showing the optimality
of our convergence rates w.r.t. $m$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1&quot;&gt;Aadirupa Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feldman_V/0/1/0/all/0/1&quot;&gt;Vitaly Feldman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koren_T/0/1/0/all/0/1&quot;&gt;Tomer Koren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mansour_Y/0/1/0/all/0/1&quot;&gt;Yishay Mansour&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11797">
<title>Learning Merton&apos;s Strategies in an Incomplete Market: Recursive Entropy Regularization and Biased Gaussian Exploration. (arXiv:2312.11797v1 [q-fin.PM])</title>
<link>http://arxiv.org/abs/2312.11797</link>
<description rdf:parseType="Literal">&lt;p&gt;We study Merton&apos;s expected utility maximization problem in an incomplete
market, characterized by a factor process in addition to the stock price
process, where all the model primitives are unknown. We take the reinforcement
learning (RL) approach to learn optimal portfolio policies directly by
exploring the unknown market, without attempting to estimate the model
parameters. Based on the entropy-regularization framework for general
continuous-time RL formulated in Wang et al. (2020), we propose a recursive
weighting scheme on exploration that endogenously discounts the current
exploration reward by the past accumulative amount of exploration. Such a
recursive regularization restores the optimality of Gaussian exploration.
However, contrary to the existing results, the optimal Gaussian policy turns
out to be biased in general, due to the interwinding needs for hedging and for
exploration. We present an asymptotic analysis of the resulting errors to show
how the level of exploration affects the learned policies. Furthermore, we
establish a policy improvement theorem and design several RL algorithms to
learn Merton&apos;s optimal strategies. At last, we carry out both simulation and
empirical studies with a stochastic volatility environment to demonstrate the
efficiency and robustness of the RL algorithms in comparison to the
conventional plug-in method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Dai_M/0/1/0/all/0/1&quot;&gt;Min Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yuchao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Jia_Y/0/1/0/all/0/1&quot;&gt;Yanwei Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xun Yu Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11801">
<title>Fast, Scalable, Warm-Start Semidefinite Programming with Spectral Bundling and Sketching. (arXiv:2312.11801v1 [math.OC])</title>
<link>http://arxiv.org/abs/2312.11801</link>
<description rdf:parseType="Literal">&lt;p&gt;While semidefinite programming (SDP) has traditionally been limited to
moderate-sized problems, recent algorithms augmented with matrix sketching
techniques have enabled solving larger SDPs. However, these methods achieve
scalability at the cost of an increase in the number of necessary iterations,
resulting in slower convergence as the problem size grows. Furthermore, they
require iteration-dependent parameter schedules that prohibit effective
utilization of warm-start initializations important in practical applications
with incrementally-arriving data or mixed-integer programming. We present
SpecBM, a provably correct, fast and scalable algorithm for solving massive
SDPs that can leverage a warm-start initialization to further accelerate
convergence. Our proposed algorithm is a spectral bundle method for solving
general SDPs containing both equality and inequality constraints. Moveover,
when augmented with an optional matrix sketching technique, our algorithm
achieves the dramatically improved scalability of previous work while
sustaining convergence speed. We empirically demonstrate the effectiveness of
our method, both with and without warm-starting, across multiple applications
with large instances. For example, on a problem with 600 million decision
variables, SpecBM achieved a solution of standard accuracy in less than 7
minutes, where the previous state-of-the-art scalable SDP solver requires more
than 16 hours. Our method solves an SDP with more than 10^13 decision variables
on a single machine with 16 cores and no more than 128GB RAM; the previous
state-of-the-art method had not achieved an accurate solution after 72 hours on
the same instance. We make our implementation in pure JAX publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Angell_R/0/1/0/all/0/1&quot;&gt;Rico Angell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+McCallum_A/0/1/0/all/0/1&quot;&gt;Andrew McCallum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11818">
<title>Root Cause Explanation of Outliers under Noisy Mechanisms. (arXiv:2312.11818v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.11818</link>
<description rdf:parseType="Literal">&lt;p&gt;Identifying root causes of anomalies in causal processes is vital across
disciplines. Once identified, one can isolate the root causes and implement
necessary measures to restore the normal operation. Causal processes are often
modelled as graphs with entities being nodes and their paths/interconnections
as edge. Existing work only consider the contribution of nodes in the
generative process, thus can not attribute the outlier score to the edges of
the mechanism if the anomaly occurs in the connections. In this paper, we
consider both individual edge and node of each mechanism when identifying the
root causes. We introduce a noisy functional causal model to account for this
purpose. Then, we employ Bayesian learning and inference methods to infer the
noises of the nodes and edges. We then represent the functional form of a
target outlier leaf as a function of the node and edge noises. Finally, we
propose an efficient gradient-based attribution method to compute the anomaly
attribution scores which scales linearly with the number of nodes and edges.
Experiments on simulated datasets and two real-world scenario datasets show
better anomaly attribution performance of the proposed method compared to the
baselines. Our method scales to larger graphs with more nodes and edges.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1&quot;&gt;Phuoc Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1&quot;&gt;Truyen Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1&quot;&gt;Sunil Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Thin Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1&quot;&gt;Svetha Venkatesh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11819">
<title>An Adaptive Placement and Parallelism Framework for Accelerating RLHF Training. (arXiv:2312.11819v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11819</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, ChatGPT or InstructGPT like large language models (LLM) has made a
significant impact in the AI world. These models are incredibly versatile,
capable of performing language tasks on par or even exceeding the capabilities
of human experts. Many works have attempted to reproduce the complex
InstructGPT&apos;s RLHF (Reinforcement Learning with Human Feedback) training
pipeline. However, the mainstream distributed RLHF training methods typically
adopt a fixed model placement strategy, referred to as the Flattening strategy.
This strategy treats all four models involved in RLHF as a single entity and
places them on all devices, regardless of their differences. Unfortunately,
this strategy exacerbates the generation bottlenecks in the RLHF training and
degrades the overall training efficiency. To address these issues, we propose
an adaptive model placement framework that offers two flexible model placement
strategies. These strategies allow for the agile allocation of models across
devices in a fine-grained manner. The Interleaving strategy helps reduce memory
redundancy and communication costs during RLHF training. On the other hand, the
Separation strategy improves the throughput of model training by separating the
training and generation stages of the RLHF pipeline. Notably, this framework
seamlessly integrates with other mainstream techniques for acceleration and
enables automatic hyperparameter search. Extensive experiments have
demonstrated that our Interleaving and Separation strategies can achieve
notable improvements up to 11x, compared to the current state-of-the-art (SOTA)
approaches. These experiments encompassed a wide range of training scenarios,
involving models of varying sizes and devices of different scales. The results
highlight the effectiveness and superiority of our approaches in accelerating
the training of distributed RLHF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Youshao Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Weichang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhenglei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_F/0/1/0/all/0/1&quot;&gt;Fagui Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shangchun Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ju_L/0/1/0/all/0/1&quot;&gt;Lin Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1&quot;&gt;Lei Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaolu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jun Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11822">
<title>Classification of complex local environments in systems of particle shapes through shape-symmetry encoded data augmentation. (arXiv:2312.11822v1 [cond-mat.soft])</title>
<link>http://arxiv.org/abs/2312.11822</link>
<description rdf:parseType="Literal">&lt;p&gt;Detecting and analyzing the local environment is crucial for investigating
the dynamical processes of crystal nucleation and shape colloidal particle
self-assembly. Recent developments in machine learning provide a promising
avenue for better order parameters in complex systems that are challenging to
study using traditional approaches. However, the application of machine
learning to self-assembly on systems of particle shapes is still underexplored.
To address this gap, we propose a simple, physics-agnostic, yet powerful
approach that involves training a multilayer perceptron (MLP) as a local
environment classifier for systems of particle shapes, using input features
such as particle distances and orientations. Our MLP classifier is trained in a
supervised manner with a shape symmetry-encoded data augmentation technique
without the need for any conventional roto-translations invariant symmetry
functions. We evaluate the performance of our classifiers on four different
scenarios involving self-assembly of cubic structures, 2-dimensional and
3-dimensional patchy particle shape systems, hexagonal bipyramids with varying
aspect ratios, and truncated shapes with different degrees of truncation. The
proposed training process and data augmentation technique are both
straightforward and flexible, enabling easy application of the classifier to
other processes involving particle orientations. Our work thus presents a
valuable tool for investigating self-assembly processes on systems of particle
shapes, with potential applications in structure identification of any
particle-based or molecular system where orientations can be defined.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Shih-Kuang/0/1/0/all/0/1&quot;&gt;Shih-Kuang&lt;/a&gt; (Alex)Lee, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Tsai_S/0/1/0/all/0/1&quot;&gt;Sun-Ting Tsai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Glotzer_S/0/1/0/all/0/1&quot;&gt;Sharon Glotzer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11831">
<title>Locally-Minimal Probabilistic Explanations. (arXiv:2312.11831v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11831</link>
<description rdf:parseType="Literal">&lt;p&gt;Formal abductive explanations offer crucial guarantees of rigor and so are of
interest in high-stakes uses of machine learning (ML). One drawback of
abductive explanations is explanation size, justified by the cognitive limits
of human decision-makers. Probabilistic abductive explanations (PAXps) address
this limitation, but their theoretical and practical complexity makes their
exact computation most often unrealistic. This paper proposes novel efficient
algorithms for the computation of locally-minimal PXAps, which offer
high-quality approximations of PXAps in practice. The experimental results
demonstrate the practical efficiency of the proposed algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Izza_Y/0/1/0/all/0/1&quot;&gt;Yacine Izza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meel_K/0/1/0/all/0/1&quot;&gt;Kuldeep S. Meel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marques_Silva_J/0/1/0/all/0/1&quot;&gt;Joao Marques-Silva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11832">
<title>The Validity of a Machine Learning-Based Video Game in the Objective Screening of Attention Deficit Hyperactivity Disorder in Children Aged 5 to 12 Years. (arXiv:2312.11832v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11832</link>
<description rdf:parseType="Literal">&lt;p&gt;Objective: Early identification of ADHD is necessary to provide the
opportunity for timely treatment. However, screening the symptoms of ADHD on a
large scale is not easy. This study aimed to validate a video game (FishFinder)
for the screening of ADHD using objective measurement of the core symptoms of
this disorder. Method: The FishFinder measures attention and impulsivity
through in-game performance and evaluates the child&apos;s hyperactivity using
smartphone motion sensors. This game was tested on 26 children with ADHD and 26
healthy children aged 5 to 12 years. A Support Vector Machine was employed to
detect children with ADHD. results: This system showed 92.3% accuracy, 90%
sensitivity, and 93.7% specificity using a combination of in-game and movement
features. Conclusions: The FishFinder demonstrated a strong ability to identify
ADHD in children. So, this game can be used as an affordable, accessible, and
enjoyable method for the objective screening of ADHD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zakani_Z/0/1/0/all/0/1&quot;&gt;Zeinab Zakani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moradi_H/0/1/0/all/0/1&quot;&gt;Hadi Moradi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghasemzadeh_S/0/1/0/all/0/1&quot;&gt;Sogand Ghasemzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riazi_M/0/1/0/all/0/1&quot;&gt;Maryam Riazi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mortazavi_F/0/1/0/all/0/1&quot;&gt;Fatemeh Mortazavi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11834">
<title>Multi-agent reinforcement learning using echo-state network and its application to pedestrian dynamics. (arXiv:2312.11834v1 [cs.MA])</title>
<link>http://arxiv.org/abs/2312.11834</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, simulations of pedestrians using the multi-agent
reinforcement learning (MARL) have been studied. This study considered the
roads on a grid-world environment, and implemented pedestrians as MARL agents
using an echo-state network and the least squares policy iteration method.
Under this environment, the ability of these agents to learn to move forward by
avoiding other agents was investigated. Specifically, we considered two types
of tasks: the choice between a narrow direct route and a broad detour, and the
bidirectional pedestrian flow in a corridor. The simulations results indicated
that the learning was successful when the density of the agents was not that
high.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Komatsu_H/0/1/0/all/0/1&quot;&gt;Hisato Komatsu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11835">
<title>Provably Convergent Federated Trilevel Learning. (arXiv:2312.11835v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11835</link>
<description rdf:parseType="Literal">&lt;p&gt;Trilevel learning, also called trilevel optimization (TLO), has been
recognized as a powerful modelling tool for hierarchical decision process and
widely applied in many machine learning applications, such as robust neural
architecture search, hyperparameter optimization, and domain adaptation.
Tackling TLO problems has presented a great challenge due to their nested
decision-making structure. In addition, existing works on TLO face the
following key challenges: 1) they all focus on the non-distributed setting,
which may lead to privacy breach; 2) they do not offer any non-asymptotic
convergence analysis which characterizes how fast an algorithm converges. To
address the aforementioned challenges, this paper proposes an asynchronous
federated trilevel optimization method to solve TLO problems. The proposed
method utilizes $\mu$-cuts to construct a hyper-polyhedral approximation for
the TLO problem and solve it in an asynchronous manner. We demonstrate that the
proposed $\mu$-cuts are applicable to not only convex functions but also a wide
range of non-convex functions that meet the $\mu$-weakly convex assumption.
Furthermore, we theoretically analyze the non-asymptotic convergence rate for
the proposed method by showing its iteration complexity to obtain
$\epsilon$-stationary point is upper bounded by
$\mathcal{O}(\frac{1}{\epsilon^2})$. Extensive experiments on real-world
datasets have been conducted to elucidate the superiority of the proposed
method, e.g., it has a faster convergence rate with a maximum acceleration of
approximately 80$\%$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1&quot;&gt;Yang Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kai Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tiancheng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jian_C/0/1/0/all/0/1&quot;&gt;Chengtao Jian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jianwei Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11846">
<title>Initializing Services in Interactive ML Systems for Diverse Users. (arXiv:2312.11846v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11846</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies ML systems that interactively learn from users across
multiple subpopulations with heterogeneous data distributions. The primary
objective is to provide specialized services for different user groups while
also predicting user preferences. Once the users select a service based on how
well the service anticipated their preference, the services subsequently adapt
and refine themselves based on the user data they accumulate, resulting in an
iterative, alternating minimization process between users and services
(learning dynamics). Employing such tailored approaches has two main
challenges: (i) Unknown user preferences: Typically, data on user preferences
are unavailable without interaction, and uniform data collection across a large
and diverse user base can be prohibitively expensive. (ii) Suboptimal Local
Solutions: The total loss (sum of loss functions across all users and all
services) landscape is not convex even if the individual losses on a single
service are convex, making it likely for the learning dynamics to get stuck in
local minima. The final outcome of the aforementioned learning dynamics is thus
strongly influenced by the initial set of services offered to users, and is not
guaranteed to be close to the globally optimal outcome. In this work, we
propose a randomized algorithm to adaptively select very few users to collect
preference data from, while simultaneously initializing a set of services. We
prove that under mild assumptions on the loss functions, the expected total
loss achieved by the algorithm right after initialization is within a factor of
the globally optimal total loss with complete user preference data, and this
factor scales only logarithmically in the number of services. Our theory is
complemented by experiments on real as well as semi-synthetic datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bose_A/0/1/0/all/0/1&quot;&gt;Avinandan Bose&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Curmei_M/0/1/0/all/0/1&quot;&gt;Mihaela Curmei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1&quot;&gt;Daniel L. Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morgenstern_J/0/1/0/all/0/1&quot;&gt;Jamie Morgenstern&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dean_S/0/1/0/all/0/1&quot;&gt;Sarah Dean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ratliff_L/0/1/0/all/0/1&quot;&gt;Lillian J.Ratliff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fazel_M/0/1/0/all/0/1&quot;&gt;Maryam Fazel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11858">
<title>SimCalib: Graph Neural Network Calibration based on Similarity between Nodes. (arXiv:2312.11858v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11858</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph neural networks (GNNs) have exhibited impressive performance in
modeling graph data as exemplified in various applications. Recently, the GNN
calibration problem has attracted increasing attention, especially in
cost-sensitive scenarios. Previous work has gained empirical insights on the
issue, and devised effective approaches for it, but theoretical supports still
fall short. In this work, we shed light on the relationship between GNN
calibration and nodewise similarity via theoretical analysis. A novel
calibration framework, named SimCalib, is accordingly proposed to consider
similarity between nodes at global and local levels. At the global level, the
Mahalanobis distance between the current node and class prototypes is
integrated to implicitly consider similarity between the current node and all
nodes in the same class. At the local level, the similarity of node
representation movement dynamics, quantified by nodewise homophily and relative
degree, is considered. Informed about the application of nodewise movement
patterns in analyzing nodewise behavior on the over-smoothing problem, we
empirically present a possible relationship between over-smoothing and GNN
calibration problem. Experimentally, we discover a correlation between nodewise
similarity and model calibration improvement, in alignment with our theoretical
results. Additionally, we conduct extensive experiments investigating different
design factors and demonstrate the effectiveness of our proposed SimCalib
framework for GNN calibration by achieving state-of-the-art performance on 14
out of 16 benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1&quot;&gt;Boshi Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xixin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qiaochu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1&quot;&gt;Shun Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1&quot;&gt;Helen Meng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11861">
<title>MG-Skip: Random Multi-Gossip Skipping Method for Nonsmooth Distributed Optimization. (arXiv:2312.11861v1 [math.OC])</title>
<link>http://arxiv.org/abs/2312.11861</link>
<description rdf:parseType="Literal">&lt;p&gt;Distributed optimization methods with probabilistic local updates have
recently gained attention for their provable ability to communication
acceleration. Nevertheless, this capability is effective only when the loss
function is smooth and the network is sufficiently well-connected. In this
paper, we propose the first linear convergent method MG-Skip with probabilistic
local updates for nonsmooth distributed optimization. Without any extra
condition for the network connectivity, MG-Skip allows for the multiple-round
gossip communication to be skipped in most iterations, while its iteration
complexity is $\mathcal{O}\left(\kappa \log \frac{1}{\epsilon}\right)$ and
communication complexity is only
$\mathcal{O}\left(\sqrt{\frac{\kappa}{(1-\rho)}} \log
\frac{1}{\epsilon}\right)$, where $\kappa$ is the condition number of the loss
function and $\rho$ reflects the connectivity of the network topology. To the
best of our knowledge, MG-Skip achieves the best communication complexity when
the loss function has the smooth (strongly convex)+nonsmooth (convex) composite
form.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Guo_L/0/1/0/all/0/1&quot;&gt;Luyao Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Luqing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Shi_X/0/1/0/all/0/1&quot;&gt;Xinli Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jinde Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11862">
<title>Topo-MLP : A Simplicial Network Without Message Passing. (arXiv:2312.11862v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11862</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to their ability to model meaningful higher order relations among a set
of entities, higher order network models have emerged recently as a powerful
alternative for graph-based network models which are only capable of modeling
binary relationships. Message passing paradigm is still dominantly used to
learn representations even for higher order network models. While powerful,
message passing can have disadvantages during inference, particularly when the
higher order connectivity information is missing or corrupted. To overcome such
limitations, we propose Topo-MLP, a purely MLP-based simplicial neural network
algorithm to learn the representation of elements in a simplicial complex
without explicitly relying on message passing. Our framework utilizes a novel
Higher Order Neighborhood Contrastive (HONC) loss which implicitly incorporates
the simplicial structure into representation learning. Our proposed model&apos;s
simplicity makes it faster during inference. Moreover, we show that our model
is robust when faced with missing or corrupted connectivity structure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramamurthy_K/0/1/0/all/0/1&quot;&gt;Karthikeyan Natesan Ramamurthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guzman_Saenz_A/0/1/0/all/0/1&quot;&gt;Aldo Guzm&amp;#xe1;n-S&amp;#xe1;enz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hajij_M/0/1/0/all/0/1&quot;&gt;Mustafa Hajij&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11863">
<title>Neural Network Approximation for Pessimistic Offline Reinforcement Learning. (arXiv:2312.11863v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11863</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep reinforcement learning (RL) has shown remarkable success in specific
offline decision-making scenarios, yet its theoretical guarantees are still
under development. Existing works on offline RL theory primarily emphasize a
few trivial settings, such as linear MDP or general function approximation with
strong assumptions and independent data, which lack guidance for practical use.
The coupling of deep learning and Bellman residuals makes this problem
challenging, in addition to the difficulty of data dependence. In this paper,
we establish a non-asymptotic estimation error of pessimistic offline RL using
general neural network approximation with $\mathcal{C}$-mixing data regarding
the structure of networks, the dimension of datasets, and the concentrability
of data coverage, under mild assumptions. Our result shows that the estimation
error consists of two parts: the first converges to zero at a desired rate on
the sample size with partially controllable concentrability, and the second
becomes negligible if the residual constraint is tight. This result
demonstrates the explicit efficiency of deep adversarial offline RL frameworks.
We utilize the empirical process tool for $\mathcal{C}$-mixing sequences and
the neural network approximation theory for the H\&quot;{o}lder class to achieve
this. We also develop methods to bound the Bellman estimation error caused by
function approximation with empirical Bellman constraint perturbations.
Additionally, we present a result that lessens the curse of dimensionality
using data with low intrinsic dimensionality and function classes with low
complexity. Our estimation provides valuable insights into the development of
deep offline RL and guidance for algorithm model design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Di Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1&quot;&gt;Yuling Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Li Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Haizhao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xiliang Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11875">
<title>Sparse is Enough in Fine-tuning Pre-trained Large Language Model. (arXiv:2312.11875v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11875</link>
<description rdf:parseType="Literal">&lt;p&gt;With the prevalence of pre-training-fine-tuning paradigm, how to efficiently
adapt the pre-trained model to the downstream tasks has been an intriguing
issue. Parameter-Efficient Fine-Tuning (PEFT) methods have been proposed for
low-cost adaptation, including Adapters, Bia-only, and the recently widely used
Low-Rank Adaptation. Although these methods have demonstrated their
effectiveness to some extent and have been widely applied, the underlying
principles are still unclear. In this paper, we reveal the transition of loss
landscape in the downstream domain from random initialization to pre-trained
initialization, that is, from low-amplitude oscillation to high-amplitude
oscillation. The parameter gradients exhibit a property akin to sparsity, where
a small fraction of components dominate the total gradient norm, for instance,
1% of the components account for 99% of the gradient. This property ensures
that the pre-trained model can easily find a flat minimizer which guarantees
the model&apos;s ability to generalize even with a low number of trainable
parameters. Based on this, we propose a gradient-based sparse fine-tuning
algorithm, named Sparse Increment Fine-Tuning (SIFT), and validate its
effectiveness on a range of tasks including the GLUE Benchmark and
Instruction-tuning. The code is accessible at https://github.com/song-wx/SIFT/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1&quot;&gt;Weixi Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zuchao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lefei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hai Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1&quot;&gt;Bo Du&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11880">
<title>Point Cloud Segmentation Using Transfer Learning with RandLA-Net: A Case Study on Urban Areas. (arXiv:2312.11880v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.11880</link>
<description rdf:parseType="Literal">&lt;p&gt;Urban environments are characterized by complex structures and diverse
features, making accurate segmentation of point cloud data a challenging task.
This paper presents a comprehensive study on the application of RandLA-Net, a
state-of-the-art neural network architecture, for the 3D segmentation of
large-scale point cloud data in urban areas. The study focuses on three major
Chinese cities, namely Chengdu, Jiaoda, and Shenzhen, leveraging their unique
characteristics to enhance segmentation performance.
&lt;/p&gt;
&lt;p&gt;To address the limited availability of labeled data for these specific urban
areas, we employed transfer learning techniques. We transferred the learned
weights from the Sensat Urban and Toronto 3D datasets to initialize our
RandLA-Net model. Additionally, we performed class remapping to adapt the model
to the target urban areas, ensuring accurate segmentation results.
&lt;/p&gt;
&lt;p&gt;The experimental results demonstrate the effectiveness of the proposed
approach achieving over 80\% F1 score for each areas in 3D point cloud
segmentation. The transfer learning strategy proves to be crucial in overcoming
data scarcity issues, providing a robust solution for urban point cloud
analysis. The findings contribute to the advancement of point cloud
segmentation methods, especially in the context of rapidly evolving Chinese
urban areas.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bayar_A/0/1/0/all/0/1&quot;&gt;Alperen Enes Bayar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uyan_U/0/1/0/all/0/1&quot;&gt;Ufuk Uyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toprak_E/0/1/0/all/0/1&quot;&gt;Elif Toprak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuheng_C/0/1/0/all/0/1&quot;&gt;Cao Yuheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Juncheng_T/0/1/0/all/0/1&quot;&gt;Tang Juncheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kindiroglu_A/0/1/0/all/0/1&quot;&gt;Ahmet Alp Kindiroglu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11882">
<title>ConsistentEE: A Consistent and Hardness-Guided Early Exiting Method for Accelerating Language Models Inference. (arXiv:2312.11882v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.11882</link>
<description rdf:parseType="Literal">&lt;p&gt;Early Exiting is one of the most popular methods to achieve efficient
inference. Current early exiting methods adopt the (weighted) sum of the cross
entropy loss of all internal classifiers during training, imposing all these
classifiers to predict all instances correctly. However, during inference, as
long as one internal classifier predicts an instance correctly, it can
accelerate without losing accuracy. Thus, there is a notable gap between
training and inference. We propose ConsistentEE, an early exiting method that
is consistent in training and inference. ConsistentEE formulates the early
exiting process as a reinforcement learning problem. A policy network is added
to decide whether an instance should exit or continue. The training objective
of ConsistentEE only require each instance to be predicted correctly by one
internal classifier. Additionally, we introduce the concept Memorize Layer to
measure the hardness of an instance. We incorporate memorized layer into reward
function design, which allows ``easy&apos;&apos; instances to focus more on acceleration
while ``hard&apos;&apos; instances to focus more on accuracy. Experimental results show
that our method outperforms other baselines on various natural language
understanding and generation tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1&quot;&gt;Ziqian Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1&quot;&gt;Yihuai Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1&quot;&gt;Hongliang Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_H/0/1/0/all/0/1&quot;&gt;Huiping Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Cen Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11891">
<title>Hierarchical and Incremental Structural Entropy Minimization for Unsupervised Social Event Detection. (arXiv:2312.11891v1 [cs.SI])</title>
<link>http://arxiv.org/abs/2312.11891</link>
<description rdf:parseType="Literal">&lt;p&gt;As a trending approach for social event detection, graph neural network
(GNN)-based methods enable a fusion of natural language semantics and the
complex social network structural information, thus showing SOTA performance.
However, GNN-based methods can miss useful message correlations. Moreover, they
require manual labeling for training and predetermining the number of events
for prediction. In this work, we address social event detection via graph
structural entropy (SE) minimization. While keeping the merits of the GNN-based
methods, the proposed framework, HISEvent, constructs more informative message
graphs, is unsupervised, and does not require the number of events given a
priori. Specifically, we incrementally explore the graph neighborhoods using
1-dimensional (1D) SE minimization to supplement the existing message graph
with edges between semantically related messages. We then detect events from
the message graph by hierarchically minimizing 2-dimensional (2D) SE. Our
proposed 1D and 2D SE minimization algorithms are customized for social event
detection and effectively tackle the efficiency problem of the existing SE
minimization algorithms. Extensive experiments show that HISEvent consistently
outperforms GNN-based methods and achieves the new SOTA for social event
detection under both closed- and open-set settings while being efficient and
robust.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yuwei Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1&quot;&gt;Hao Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhengtao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1&quot;&gt;Philip S. Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11894">
<title>3D-LFM: Lifting Foundation Model. (arXiv:2312.11894v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.11894</link>
<description rdf:parseType="Literal">&lt;p&gt;The lifting of 3D structure and camera from 2D landmarks is at the
cornerstone of the entire discipline of computer vision. Traditional methods
have been confined to specific rigid objects, such as those in
Perspective-n-Point (PnP) problems, but deep learning has expanded our
capability to reconstruct a wide range of object classes (e.g. C3PDO and PAUL)
with resilience to noise, occlusions, and perspective distortions. All these
techniques, however, have been limited by the fundamental need to establish
correspondences across the 3D training data -- significantly limiting their
utility to applications where one has an abundance of &quot;in-correspondence&quot; 3D
data. Our approach harnesses the inherent permutation equivariance of
transformers to manage varying number of points per 3D data instance,
withstands occlusions, and generalizes to unseen categories. We demonstrate
state of the art performance across 2D-3D lifting task benchmarks. Since our
approach can be trained across such a broad class of structures we refer to it
simply as a 3D Lifting Foundation Model (3D-LFM) -- the first of its kind.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dabhi_M/0/1/0/all/0/1&quot;&gt;Mosam Dabhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeni_L/0/1/0/all/0/1&quot;&gt;Laszlo A. Jeni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucey_S/0/1/0/all/0/1&quot;&gt;Simon Lucey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11898">
<title>Short-Term Multi-Horizon Line Loss Rate Forecasting of a Distribution Network Using Attention-GCN-LSTM. (arXiv:2312.11898v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11898</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurately predicting line loss rates is vital for effective line loss
management in distribution networks, especially over short-term multi-horizons
ranging from one hour to one week. In this study, we propose
Attention-GCN-LSTM, a novel method that combines Graph Convolutional Networks
(GCN), Long Short-Term Memory (LSTM), and a three-level attention mechanism to
address this challenge. By capturing spatial and temporal dependencies, our
model enables accurate forecasting of line loss rates across multiple horizons.
Through comprehensive evaluation using real-world data from 10KV feeders, our
Attention-GCN-LSTM model consistently outperforms existing algorithms,
exhibiting superior performance in terms of prediction accuracy and
multi-horizon forecasting. This model holds significant promise for enhancing
line loss management in distribution networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yijia Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yixiu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1&quot;&gt;Wei Deng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11903">
<title>Sign Language Conversation Interpretation Using Wearable Sensors and Machine Learning. (arXiv:2312.11903v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2312.11903</link>
<description rdf:parseType="Literal">&lt;p&gt;The count of people suffering from various levels of hearing loss reached
1.57 billion in 2019. This huge number tends to suffer on many personal and
professional levels and strictly needs to be included with the rest of society
healthily. This paper presents a proof of concept of an automatic sign language
recognition system based on data obtained using a wearable device of 3 flex
sensors. The system is designed to interpret a selected set of American Sign
Language (ASL) dynamic words by collecting data in sequences of the performed
signs and using machine learning methods. The built models achieved
high-quality performances, such as Random Forest with 99% accuracy, Support
Vector Machine (SVM) with 99%, and two K-Nearest Neighbor (KNN) models with
98%. This indicates many possible paths toward the development of a full-scale
system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kalandar_B/0/1/0/all/0/1&quot;&gt;Basma Kalandar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dworakowski_Z/0/1/0/all/0/1&quot;&gt;Ziemowit Dworakowski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11905">
<title>Convergence Visualizer of Decentralized Federated Distillation with Reduced Communication Costs. (arXiv:2312.11905v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2312.11905</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) achieves collaborative learning without the need for
data sharing, thus preventing privacy leakage. To extend FL into a fully
decentralized algorithm, researchers have applied distributed optimization
algorithms to FL by considering machine learning (ML) tasks as parameter
optimization problems. Conversely, the consensus-based multi-hop federated
distillation (CMFD) proposed in the authors&apos; previous work makes neural network
(NN) models get close with others in a function space rather than in a
parameter space. Hence, this study solves two unresolved challenges of CMFD:
(1) communication cost reduction and (2) visualization of model convergence.
Based on a proposed dynamic communication cost reduction method (DCCR), the
amount of data transferred in a network is reduced; however, with a slight
degradation in the prediction accuracy. In addition, a technique for
visualizing the distance between the NN models in a function space is also
proposed. The technique applies a dimensionality reduction technique by
approximating infinite-dimensional functions as numerical vectors to visualize
the trajectory of how the models change by the distributed learning algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taya_A/0/1/0/all/0/1&quot;&gt;Akihito Taya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nishiyama_Y/0/1/0/all/0/1&quot;&gt;Yuuki Nishiyama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sezaki_K/0/1/0/all/0/1&quot;&gt;Kaoru Sezaki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11918">
<title>A Case Study in CUDA Kernel Fusion: Implementing FlashAttention-2 on NVIDIA Hopper Architecture using the CUTLASS Library. (arXiv:2312.11918v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11918</link>
<description rdf:parseType="Literal">&lt;p&gt;We provide an optimized implementation of the forward pass of
FlashAttention-2, a popular memory-aware scaled dot-product attention
algorithm, as a custom fused CUDA kernel targeting NVIDIA Hopper architecture
and written using the open-source CUTLASS library. In doing so, we explain the
challenges and techniques involved in fusing online-softmax with back-to-back
GEMM kernels, utilizing the Hopper-specific Tensor Memory Accelerator (TMA) and
Warpgroup Matrix-Multiply-Accumulate (WGMMA) instructions, defining and
transforming CUTLASS Layouts and Tensors, overlapping copy and GEMM operations,
and choosing optimal tile sizes for the Q, K and V attention matrices while
balancing the register pressure and shared memory utilization. In head-to-head
benchmarks on a single H100 PCIe GPU for some common choices of
hyperparameters, we observe 20-50% higher FLOPs/s over a version of
FlashAttention-2 optimized for last-generation NVIDIA Ampere architecture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bikshandi_G/0/1/0/all/0/1&quot;&gt;Ganesh Bikshandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_J/0/1/0/all/0/1&quot;&gt;Jay Shah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11926">
<title>Big Learning Expectation Maximization. (arXiv:2312.11926v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11926</link>
<description rdf:parseType="Literal">&lt;p&gt;Mixture models serve as one fundamental tool with versatile applications.
However, their training techniques, like the popular Expectation Maximization
(EM) algorithm, are notoriously sensitive to parameter initialization and often
suffer from bad local optima that could be arbitrarily worse than the optimal.
To address the long-lasting bad-local-optima challenge, we draw inspiration
from the recent ground-breaking foundation models and propose to leverage their
underlying big learning principle to upgrade the EM. Specifically, we present
the Big Learning EM (BigLearn-EM), an EM upgrade that simultaneously performs
joint, marginal, and orthogonally transformed marginal matchings between data
and model distributions. Through simulated experiments, we empirically show
that the BigLearn-EM is capable of delivering the optimal with high
probability; comparisons on benchmark clustering datasets further demonstrate
its effectiveness and advantages over existing techniques. The code is
available at
https://github.com/YulaiCong/Big-Learning-Expectation-Maximization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cong_Y/0/1/0/all/0/1&quot;&gt;Yulai Cong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sijia Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11927">
<title>Empowering Dual-Level Graph Self-Supervised Pretraining with Motif Discovery. (arXiv:2312.11927v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11927</link>
<description rdf:parseType="Literal">&lt;p&gt;While self-supervised graph pretraining techniques have shown promising
results in various domains, their application still experiences challenges of
limited topology learning, human knowledge dependency, and incompetent
multi-level interactions. To address these issues, we propose a novel solution,
Dual-level Graph self-supervised Pretraining with Motif discovery (DGPM), which
introduces a unique dual-level pretraining structure that orchestrates
node-level and subgraph-level pretext tasks. Unlike prior approaches, DGPM
autonomously uncovers significant graph motifs through an edge pooling module,
aligning learned motif similarities with graph kernel-based similarities. A
cross-matching task enables sophisticated node-motif interactions and novel
representation learning. Extensive experiments on 15 datasets validate DGPM&apos;s
effectiveness and generalizability, outperforming state-of-the-art methods in
unsupervised representation learning and transfer learning settings. The
autonomously discovered motifs demonstrate the potential of DGPM to enhance
robustness and interpretability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_P/0/1/0/all/0/1&quot;&gt;Pengwei Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1&quot;&gt;Kaisong Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhuoren Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1&quot;&gt;Yangyang Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1&quot;&gt;Tianqianjin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1&quot;&gt;Changlong Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaozhong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11929">
<title>Transformer Network for Multi-Person Tracking and Re-Identification in Unconstrained Environment. (arXiv:2312.11929v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.11929</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-object tracking (MOT) has profound applications in a variety of fields,
including surveillance, sports analytics, self-driving, and cooperative
robotics. Despite considerable advancements, existing MOT methodologies tend to
falter when faced with non-uniform movements, occlusions, and
appearance-reappearance scenarios of the objects. Recognizing this inadequacy,
we put forward an integrated MOT method that not only marries object detection
and identity linkage within a singular, end-to-end trainable framework but also
equips the model with the ability to maintain object identity links over long
periods of time. Our proposed model, named STMMOT, is built around four key
modules: 1) candidate proposal generation, which generates object proposals via
a vision-transformer encoder-decoder architecture that detects the object from
each frame in the video; 2) scale variant pyramid, a progressive pyramid
structure to learn the self-scale and cross-scale similarities in multi-scale
feature maps; 3) spatio-temporal memory encoder, extracting the essential
information from the memory associated with each object under tracking; and 4)
spatio-temporal memory decoder, simultaneously resolving the tasks of object
detection and identity association for MOT. Our system leverages a robust
spatio-temporal memory module that retains extensive historical observations
and effectively encodes them using an attention-based aggregator. The
uniqueness of STMMOT lies in representing objects as dynamic query embeddings
that are updated continuously, which enables the prediction of object states
with attention mechanisms and eradicates the need for post-processing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukhtar_H/0/1/0/all/0/1&quot;&gt;Hamza Mukhtar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Muhammad Usman Ghani Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11933">
<title>Dynamic Frequency Domain Graph Convolutional Network for Traffic Forecasting. (arXiv:2312.11933v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11933</link>
<description rdf:parseType="Literal">&lt;p&gt;Complex spatial dependencies in transportation networks make traffic
prediction extremely challenging. Much existing work is devoted to learning
dynamic graph structures among sensors, and the strategy of mining spatial
dependencies from traffic data, known as data-driven, tends to be an intuitive
and effective approach. However, Time-Shift of traffic patterns and noise
induced by random factors hinder data-driven spatial dependence modeling. In
this paper, we propose a novel dynamic frequency domain graph convolution
network (DFDGCN) to capture spatial dependencies. Specifically, we mitigate the
effects of time-shift by Fourier transform, and introduce the identity
embedding of sensors and time embedding when capturing data for graph learning
since traffic data with noise is not entirely reliable. The graph is combined
with static predefined and self-adaptive graphs during graph convolution to
predict future traffic data through classical causal convolutions. Extensive
experiments on four real-world datasets demonstrate that our model is effective
and outperforms the baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yujie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1&quot;&gt;Zezhi Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yongjun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_Q/0/1/0/all/0/1&quot;&gt;Qiang Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1&quot;&gt;Zhaogang Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fei Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11934">
<title>Identification of Causal Structure with Latent Variables Based on Higher Order Cumulants. (arXiv:2312.11934v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11934</link>
<description rdf:parseType="Literal">&lt;p&gt;Causal discovery with latent variables is a crucial but challenging task.
Despite the emergence of numerous methods aimed at addressing this challenge,
they are not fully identified to the structure that two observed variables are
influenced by one latent variable and there might be a directed edge in
between. Interestingly, we notice that this structure can be identified through
the utilization of higher-order cumulants. By leveraging the higher-order
cumulants of non-Gaussian data, we provide an analytical solution for
estimating the causal coefficients or their ratios. With the estimated (ratios
of) causal coefficients, we propose a novel approach to identify the existence
of a causal edge between two observed variables subject to latent variable
influence. In case when such a causal edge exits, we introduce an asymmetry
criterion to determine the causal direction. The experimental results
demonstrate the effectiveness of our proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhiyi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1&quot;&gt;Ruichu Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_Z/0/1/0/all/0/1&quot;&gt;Zhifeng Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11939">
<title>Time-Series Contrastive Learning against False Negatives and Class Imbalance. (arXiv:2312.11939v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11939</link>
<description rdf:parseType="Literal">&lt;p&gt;As an exemplary self-supervised approach for representation learning,
time-series contrastive learning has exhibited remarkable advancements in
contemporary research. While recent contrastive learning strategies have
focused on how to construct appropriate positives and negatives, in this study,
we conduct theoretical analysis and find they have overlooked the fundamental
issues: false negatives and class imbalance inherent in the InfoNCE loss-based
framework. Therefore, we introduce a straightforward modification grounded in
the SimCLR framework, universally adaptable to models engaged in the instance
discrimination task. By constructing instance graphs to facilitate interactive
learning among instances, we emulate supervised contrastive learning via the
multiple-instances discrimination task, mitigating the harmful impact of false
negatives. Moreover, leveraging the graph structure and few-labeled data, we
perform semi-supervised consistency classification and enhance the
representative ability of minority classes. We compared our method with the
most popular time-series contrastive learning methods on four real-world
time-series datasets and demonstrated our significant advantages in overall
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1&quot;&gt;Xiyuan Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Youfang Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11952">
<title>Automatic Parameter Selection for Non-Redundant Clustering. (arXiv:2312.11952v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11952</link>
<description rdf:parseType="Literal">&lt;p&gt;High-dimensional datasets often contain multiple meaningful clusterings in
different subspaces. For example, objects can be clustered either by color,
weight, or size, revealing different interpretations of the given dataset. A
variety of approaches are able to identify such non-redundant clusterings.
However, most of these methods require the user to specify the expected number
of subspaces and clusters for each subspace. Stating these values is a
non-trivial problem and usually requires detailed knowledge of the input
dataset. In this paper, we propose a framework that utilizes the Minimum
Description Length Principle (MDL) to detect the number of subspaces and
clusters per subspace automatically. We describe an efficient procedure that
greedily searches the parameter space by splitting and merging subspaces and
clusters within subspaces. Additionally, an encoding strategy is introduced
that allows us to detect outliers in each subspace. Extensive experiments show
that our approach is highly competitive to state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leiber_C/0/1/0/all/0/1&quot;&gt;Collin Leiber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mautz_D/0/1/0/all/0/1&quot;&gt;Dominik Mautz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plant_C/0/1/0/all/0/1&quot;&gt;Claudia Plant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bohm_C/0/1/0/all/0/1&quot;&gt;Christian B&amp;#xf6;hm&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11969">
<title>GroupMixNorm Layer for Learning Fair Models. (arXiv:2312.11969v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11969</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent research has identified discriminatory behavior of automated
prediction algorithms towards groups identified on specific protected
attributes (e.g., gender, ethnicity, age group, etc.). When deployed in
real-world scenarios, such techniques may demonstrate biased predictions
resulting in unfair outcomes. Recent literature has witnessed algorithms for
mitigating such biased behavior mostly by adding convex surrogates of fairness
metrics such as demographic parity or equalized odds in the loss function,
which are often not easy to estimate. This research proposes a novel
in-processing based GroupMixNorm layer for mitigating bias from deep learning
models. The GroupMixNorm layer probabilistically mixes group-level feature
statistics of samples across different groups based on the protected attribute.
The proposed method improves upon several fairness metrics with minimal impact
on overall accuracy. Analysis on benchmark tabular and image datasets
demonstrates the efficacy of the proposed method in achieving state-of-the-art
performance. Further, the experimental analysis also suggests the robustness of
the GroupMixNorm layer against new protected attributes during inference and
its utility in eliminating bias from a pre-trained network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandey_A/0/1/0/all/0/1&quot;&gt;Anubha Pandey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rai_A/0/1/0/all/0/1&quot;&gt;Aditi Rai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1&quot;&gt;Maneet Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatt_D/0/1/0/all/0/1&quot;&gt;Deepak Bhatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhowmik_T/0/1/0/all/0/1&quot;&gt;Tanmoy Bhowmik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11973">
<title>Continual Learning: Forget-free Winning Subnetworks for Video Representations. (arXiv:2312.11973v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.11973</link>
<description rdf:parseType="Literal">&lt;p&gt;Inspired by the Regularized Lottery Ticket Hypothesis (RLTH), which
highlights the presence of competitive subnetworks within dense networks for
continual learning tasks, we introduce Winning Subnetworks (WSN). This approach
utilizes reused weights in dense networks to enhance learning in Task
Incremental Learning (TIL) scenarios. To mitigate overfitting in Few-Shot Class
Incremental Learning (FSCIL), we have developed WSN variants referred to as the
Soft subnetwork (SoftNet). Furthermore, addressing WSN&apos;s limitation of sparse
reused weights in Video Incremental Learning (VIL), we propose the Fourier
Subneural Operator (FSO). The FSO, operating in Fourier space, adaptively and
compactly encodes videos, discovering reusable subnetworks with diverse
bandwidths. We have applied FSO&apos;s Fourier representations to various continual
learning contexts, including VIL, TIL, and FSCIL. Our extensive experiments
across these scenarios demonstrate FSO&apos;s remarkable efficacy in continual
learning, significantly enhancing task performance at various convolutional
representational levels: it boosts performance in the higher layers for TIL and
FSCIL and the lower layers for VIL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1&quot;&gt;Haeyong Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1&quot;&gt;Jaehong Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1&quot;&gt;Sung Ju Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1&quot;&gt;Chang D. Yoo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11976">
<title>When Model Meets New Normals: Test-time Adaptation for Unsupervised Time-series Anomaly Detection. (arXiv:2312.11976v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.11976</link>
<description rdf:parseType="Literal">&lt;p&gt;Time-series anomaly detection deals with the problem of detecting anomalous
timesteps by learning normality from the sequence of observations. However, the
concept of normality evolves over time, leading to a &quot;new normal problem&quot;,
where the distribution of normality can be changed due to the distribution
shifts between training and test data. This paper highlights the prevalence of
the new normal problem in unsupervised time-series anomaly detection studies.
To tackle this issue, we propose a simple yet effective test-time adaptation
strategy based on trend estimation and a self-supervised approach to learning
new normalities during inference. Extensive experiments on real-world
benchmarks demonstrate that incorporating the proposed strategy into the
anomaly detector consistently improves the model&apos;s performance compared to the
baselines, leading to robustness to the distribution shifts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Dongmin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Sunghyun Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1&quot;&gt;Jaegul Choo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12003">
<title>Modelling and characterization of fine Particulate Matter dynamics in Bujumbura using low cost sensors. (arXiv:2312.12003v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2312.12003</link>
<description rdf:parseType="Literal">&lt;p&gt;Air pollution is a result of multiple sources including both natural and
anthropogenic activities. The rapid urbanization of the cities such as
Bujumbura economic capital of Burundi, is one of these factors. The very first
characterization of the spatio-temporal variability of PM2.5 in Bujumbura and
the forecasting of PM2.5 concentration have been conducted in this paper using
data collected during a year, from august 2022 to august 2023, by low cost
sensors installed in Bujumbura city. For each commune, an hourly, daily and
seasonal analysis were carried out and the results showed that the mass
concentrations of PM2.5 in the three municipalities differ from one commune to
another. The average hourly and annual PM2.5 concentrations exceed the World
Health Organization standards. The range is between 28.3 and 35.0 microgram/m3
. In order to make prediction of PM2.5 concentration, an investigation of RNN
with Long Short Term Memory (LSTM) has been undertaken.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ndamuzi_E/0/1/0/all/0/1&quot;&gt;Egide Ndamuzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Akimana_R/0/1/0/all/0/1&quot;&gt;Rachel Akimana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gahungu_P/0/1/0/all/0/1&quot;&gt;Paterne Gahungu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bimenyimana_E/0/1/0/all/0/1&quot;&gt;Elie Bimenyimana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12009">
<title>Active Preference Inference using Language Models and Probabilistic Reasoning. (arXiv:2312.12009v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.12009</link>
<description rdf:parseType="Literal">&lt;p&gt;Actively inferring user preferences, for example by asking good questions, is
important for any human-facing decision-making system. Active inference allows
such systems to adapt and personalize themselves to nuanced individual
preferences. To enable this ability for instruction-tuned large language models
(LLMs), one may prompt them to ask users questions to infer their preferences,
transforming the language models into more robust, interactive systems.
However, out of the box, these models are not efficient at extracting
preferences: the questions they generate are not informative, requiring a high
number of user interactions and impeding the usability of the downstream
system. In this work, we introduce an inference-time algorithm that helps LLMs
quickly infer preferences by using more informative questions. Our algorithm
uses a probabilistic model whose conditional distributions are defined by
prompting an LLM, and returns questions that optimize expected entropy and
expected model change. Results in a simplified interactive web shopping setting
with real product items show that an LLM equipped with our entropy reduction
algorithm outperforms baselines with the same underlying LLM on task
performance while using fewer user interactions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piriyakulkij_T/0/1/0/all/0/1&quot;&gt;Top Piriyakulkij&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuleshov_V/0/1/0/all/0/1&quot;&gt;Volodymyr Kuleshov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ellis_K/0/1/0/all/0/1&quot;&gt;Kevin Ellis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12022">
<title>LightGCNet: A Lightweight Geometric Constructive Neural Network for Data-Driven Soft sensors. (arXiv:2312.12022v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2312.12022</link>
<description rdf:parseType="Literal">&lt;p&gt;Data-driven soft sensors provide a potentially cost-effective and more
accurate modeling approach to measure difficult-to-measure indices in
industrial processes compared to mechanistic approaches. Artificial
intelligence (AI) techniques, such as deep learning, have become a popular soft
sensors modeling approach in the area of machine learning and big data.
However, soft sensors models based deep learning potentially lead to complex
model structures and excessive training time. In addition, industrial processes
often rely on distributed control systems (DCS) characterized by resource
constraints. Herein, guided by spatial geometric, a lightweight geometric
constructive neural network, namely LightGCNet, is proposed, which utilizes
compact angle constraint to assign the hidden parameters from dynamic
intervals. At the same time, a node pool strategy and spatial geometric
relationships are used to visualize and optimize the process of assigning
hidden parameters, enhancing interpretability. In addition, the universal
approximation property of LightGCNet is proved by spatial geometric analysis.
Two versions algorithmic implementations of LightGCNet are presented in this
article. Simulation results concerning both benchmark datasets and the ore
grinding process indicate remarkable merits of LightGCNet in terms of small
network size, fast learning speed, and sound generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nan_J/0/1/0/all/0/1&quot;&gt;Jing Nan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yan Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dai_W/0/1/0/all/0/1&quot;&gt;Wei Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yuen_C/0/1/0/all/0/1&quot;&gt;Chau Yuen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12028">
<title>EyePreserve: Identity-Preserving Iris Synthesis. (arXiv:2312.12028v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12028</link>
<description rdf:parseType="Literal">&lt;p&gt;Synthesis of same-identity biometric iris images, both for existing and
non-existing identities while preserving the identity across a wide range of
pupil sizes, is complex due to intricate iris muscle constriction mechanism,
requiring a precise model of iris non-linear texture deformations to be
embedded into the synthesis pipeline. This paper presents the first method of
fully data-driven, identity-preserving, pupil size-varying s ynthesis of iris
images. This approach is capable of synthesizing images of irises with
different pupil sizes representing non-existing identities as well as
non-linearly deforming the texture of iris images of existing subjects given
the segmentation mask of the target iris image. Iris recognition experiments
suggest that the proposed deformation model not only preserves the identity
when changing the pupil size but offers better similarity between same-identity
iris samples with significant differences in pupil size, compared to
state-of-the-art linear and non-linear (bio-mechanical-based) iris deformation
models. Two immediate applications of the proposed approach are: (a) synthesis
of, or enhancement of the existing biometric datasets for iris recognition,
mimicking those acquired with iris sensors, and (b) helping forensic human
experts in examining iris image pairs with significant differences in pupil
dilation. Source codes and weights of the models are made available with the
paper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Siamul Karim Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tinsley_P/0/1/0/all/0/1&quot;&gt;Patrick Tinsley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitcheff_M/0/1/0/all/0/1&quot;&gt;Mahsa Mitcheff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flynn_P/0/1/0/all/0/1&quot;&gt;Patrick Flynn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bowyer_K/0/1/0/all/0/1&quot;&gt;Kevin W. Bowyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Czajka_A/0/1/0/all/0/1&quot;&gt;Adam Czajka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12044">
<title>XLand-MiniGrid: Scalable Meta-Reinforcement Learning Environments in JAX. (arXiv:2312.12044v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.12044</link>
<description rdf:parseType="Literal">&lt;p&gt;We present XLand-MiniGrid, a suite of tools and grid-world environments for
meta-reinforcement learning research inspired by the diversity and depth of
XLand and the simplicity and minimalism of MiniGrid. XLand-Minigrid is written
in JAX, designed to be highly scalable, and can potentially run on GPU or TPU
accelerators, democratizing large-scale experimentation with limited resources.
To demonstrate the generality of our library, we have implemented some
well-known single-task environments as well as new meta-learning environments
capable of generating $10^8$ distinct tasks. We have empirically shown that the
proposed environments can scale up to $2^{13}$ parallel instances on the GPU,
reaching tens of millions of steps per second.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikulin_A/0/1/0/all/0/1&quot;&gt;Alexander Nikulin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kurenkov_V/0/1/0/all/0/1&quot;&gt;Vladislav Kurenkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zisman_I/0/1/0/all/0/1&quot;&gt;Ilya Zisman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarkov_A/0/1/0/all/0/1&quot;&gt;Artem Agarkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sinii_V/0/1/0/all/0/1&quot;&gt;Viacheslav Sinii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolesnikov_S/0/1/0/all/0/1&quot;&gt;Sergey Kolesnikov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12049">
<title>EncryIP: A Practical Encryption-Based Framework for Model Intellectual Property Protection. (arXiv:2312.12049v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.12049</link>
<description rdf:parseType="Literal">&lt;p&gt;In the rapidly growing digital economy, protecting intellectual property (IP)
associated with digital products has become increasingly important. Within this
context, machine learning (ML) models, being highly valuable digital assets,
have gained significant attention for IP protection. This paper introduces a
practical encryption-based framework called \textit{EncryIP}, which seamlessly
integrates a public-key encryption scheme into the model learning process. This
approach enables the protected model to generate randomized and confused
labels, ensuring that only individuals with accurate secret keys, signifying
authorized users, can decrypt and reveal authentic labels. Importantly, the
proposed framework not only facilitates the protected model to multiple
authorized users without requiring repetitive training of the original ML model
with IP protection methods but also maintains the model&apos;s performance without
compromising its accuracy. Compared to existing methods like watermark-based,
trigger-based, and passport-based approaches, \textit{EncryIP} demonstrates
superior effectiveness in both training protected models and efficiently
detecting the unauthorized spread of ML models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_X/0/1/0/all/0/1&quot;&gt;Xin Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhengan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1&quot;&gt;Junzuo Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yehong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yue Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12050">
<title>Extension of the Dip-test Repertoire -- Efficient and Differentiable p-value Calculation for Clustering. (arXiv:2312.12050v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.12050</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the last decade, the Dip-test of unimodality has gained increasing
interest in the data mining community as it is a parameter-free statistical
test that reliably rates the modality in one-dimensional samples. It returns a
so called Dip-value and a corresponding probability for the sample&apos;s
unimodality (Dip-p-value). These two values share a sigmoidal relationship.
However, the specific transformation is dependent on the sample size. Many
Dip-based clustering algorithms use bootstrapped look-up tables translating
Dip- to Dip-p-values for a certain limited amount of sample sizes. We propose a
specifically designed sigmoid function as a substitute for these
state-of-the-art look-up tables. This accelerates computation and provides an
approximation of the Dip- to Dip-p-value transformation for every single sample
size. Further, it is differentiable and can therefore easily be integrated in
learning schemes using gradient descent. We showcase this by exploiting our
function in a novel subspace clustering algorithm called Dip&apos;n&apos;Sub. We
highlight in extensive experiments the various benefits of our proposal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bauer_L/0/1/0/all/0/1&quot;&gt;Lena G. M. Bauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leiber_C/0/1/0/all/0/1&quot;&gt;Collin Leiber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bohm_C/0/1/0/all/0/1&quot;&gt;Christian B&amp;#xf6;hm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plant_C/0/1/0/all/0/1&quot;&gt;Claudia Plant&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2003.08433">
<title>Neural Fuzzy Extractors: A Secure Way to Use Artificial Neural Networks for Biometric User Authentication. (arXiv:2003.08433v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2003.08433</link>
<description rdf:parseType="Literal">&lt;p&gt;Powered by new advances in sensor development and artificial intelligence,
the decreasing cost of computation, and the pervasiveness of handheld
computation devices, biometric user authentication (and identification) is
rapidly becoming ubiquitous. Modern approaches to biometric authentication,
based on sophisticated machine learning techniques, cannot avoid storing either
trained-classifier details or explicit user biometric data, thus exposing
users&apos; credentials to falsification. In this paper, we introduce a secure way
to handle user-specific information involved with the use of vector-space
classifiers or artificial neural networks for biometric authentication. Our
proposed architecture, called a Neural Fuzzy Extractor (NFE), allows the
coupling of pre-existing classifiers with fuzzy extractors, through a
artificial-neural-network-based buffer called an expander, with minimal or no
performance degradation. The NFE thus offers all the performance advantages of
modern deep-learning-based classifiers, and all the security of standard fuzzy
extractors. We demonstrate the NFE retrofit to a classic artificial neural
network for a simple scenario of fingerprint-based user authentication.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jana_A/0/1/0/all/0/1&quot;&gt;Abhishek Jana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarker_M/0/1/0/all/0/1&quot;&gt;Md Kamruzzaman Sarker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ebrahimi_M/0/1/0/all/0/1&quot;&gt;Monireh Ebrahimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hitzler_P/0/1/0/all/0/1&quot;&gt;Pascal Hitzler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amariucai_G/0/1/0/all/0/1&quot;&gt;George T Amariucai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2004.08697">
<title>CausalVAE: Structured Causal Disentanglement in Variational Autoencoder. (arXiv:2004.08697v7 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2004.08697</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning disentanglement aims at finding a low dimensional representation
which consists of multiple explanatory and generative factors of the
observational data. The framework of variational autoencoder (VAE) is commonly
used to disentangle independent factors from observations. However, in real
scenarios, factors with semantics are not necessarily independent. Instead,
there might be an underlying causal structure which renders these factors
dependent. We thus propose a new VAE based framework named CausalVAE, which
includes a Causal Layer to transform independent exogenous factors into causal
endogenous ones that correspond to causally related concepts in data. We
further analyze the model identifiabitily, showing that the proposed model
learned from observations recovers the true one up to a certain degree by
providing supervision signals (e.g. feature labels). Experiments are conducted
on various datasets, including synthetic and real word benchmark CelebA.
Results show that the causal representations learned by CausalVAE are
semantically interpretable, and their causal relationship as a Directed Acyclic
Graph (DAG) is identified with good accuracy. Furthermore, we demonstrate that
the proposed CausalVAE model is able to generate counterfactual data through
&quot;do-operation&quot; to the causal factors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Mengyue Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Furui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhitang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1&quot;&gt;Xinwei Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1&quot;&gt;Jianye Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2010.03744">
<title>Maximum Reward Formulation In Reinforcement Learning. (arXiv:2010.03744v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2010.03744</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning (RL) algorithms typically deal with maximizing the
expected cumulative return (discounted or undiscounted, finite or infinite
horizon). However, several crucial applications in the real world, such as drug
discovery, do not fit within this framework because an RL agent only needs to
identify states (molecules) that achieve the highest reward within a trajectory
and does not need to optimize for the expected cumulative return. In this work,
we formulate an objective function to maximize the expected maximum reward
along a trajectory, derive a novel functional form of the Bellman equation,
introduce the corresponding Bellman operators, and provide a proof of
convergence. Using this formulation, we achieve state-of-the-art results on the
task of molecule generation that mimics a real-world drug discovery pipeline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gottipati_S/0/1/0/all/0/1&quot;&gt;Sai Krishna Gottipati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pathak_Y/0/1/0/all/0/1&quot;&gt;Yashaswi Pathak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nuttall_R/0/1/0/all/0/1&quot;&gt;Rohan Nuttall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sahir/0/1/0/all/0/1&quot;&gt;Sahir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chunduru_R/0/1/0/all/0/1&quot;&gt;Raviteja Chunduru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Touati_A/0/1/0/all/0/1&quot;&gt;Ahmed Touati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramanian_S/0/1/0/all/0/1&quot;&gt;Sriram Ganapathi Subramanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_M/0/1/0/all/0/1&quot;&gt;Matthew E. Taylor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1&quot;&gt;Sarath Chandar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2010.10258">
<title>Hierarchical Autoregressive Modeling for Neural Video Compression. (arXiv:2010.10258v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2010.10258</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work by Marino et al. (2020) showed improved performance in sequential
density estimation by combining masked autoregressive flows with hierarchical
latent variable models. We draw a connection between such autoregressive
generative models and the task of lossy video compression. Specifically, we
view recent neural video compression methods (Lu et al., 2019; Yang et al.,
2020b; Agustssonet al., 2020) as instances of a generalized stochastic temporal
autoregressive transform, and propose avenues for enhancement based on this
insight. Comprehensive evaluations on large-scale video data show improved
rate-distortion performance over both state-of-the-art neural and conventional
video compression methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_R/0/1/0/all/0/1&quot;&gt;Ruihan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yibo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Marino_J/0/1/0/all/0/1&quot;&gt;Joseph Marino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mandt_S/0/1/0/all/0/1&quot;&gt;Stephan Mandt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2106.03354">
<title>AI without networks. (arXiv:2106.03354v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2106.03354</link>
<description rdf:parseType="Literal">&lt;p&gt;Contemporary Artificial Intelligence (AI) stands on two legs: large training
data corpora and many-parameter artificial neural networks (ANNs). The data
corpora are needed to represent the complexity and heterogeneity of the world.
The role of the networks is less transparent due to the obscure dependence of
the network parameters and outputs on the training data and inputs. This raises
problems, ranging from technical-scientific to legal-ethical. We hypothesize
that a transparent approach to machine learning is possible without using
networks at all. By generalizing a parameter-free, statistically consistent
data interpolation method, which we analyze theoretically in detail, we develop
a framework for generative modeling. Given the growing usage of machine
learning techniques in science, we demonstrate this framework with an example
from the field of animal behavior. We applied this generative Hilbert framework
to the trajectories of small groups of swimming fish. The framework outperforms
previously developed state-of-the-art traditional mathematical behavioral
models and contemporary ANN-based models in reproducing naturalistic behaviors.
We do not suggest that the proposed framework will outperform networks in all
applications, as over-parameterized networks can interpolate. However, our
framework is theoretically sound, transparent, deterministic and parameter
free: it does not require any compute-expensive training, does not involve
optimization, has no model selection, and is easily reproduced and ported. We
also propose an easily computed method of credit assignment based on this
framework that could help address ethical-legal challenges raised by generative
AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitra_P/0/1/0/all/0/1&quot;&gt;Partha P Mitra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sire_C/0/1/0/all/0/1&quot;&gt;Cl&amp;#xe9;ment Sire&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.10985">
<title>Learned ISTA with Error-based Thresholding for Adaptive Sparse Coding. (arXiv:2112.10985v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2112.10985</link>
<description rdf:parseType="Literal">&lt;p&gt;Drawing on theoretical insights, we advocate an error-based thresholding
(EBT) mechanism for learned ISTA (LISTA), which utilizes a function of the
layer-wise reconstruction error to suggest a specific threshold for each
observation in the shrinkage function of each layer. We show that the proposed
EBT mechanism well disentangles the learnable parameters in the shrinkage
functions from the reconstruction errors, endowing the obtained models with
improved adaptivity to possible data variations. With rigorous analyses, we
further show that the proposed EBT also leads to a faster convergence on the
basis of LISTA or its variants, in addition to its higher adaptivity. Extensive
experimental results confirm our theoretical analyses and verify the
effectiveness of our methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Ziang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1&quot;&gt;Kailun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yiwen Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Changshui Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.15677">
<title>Augmentation-Aware Self-Supervision for Data-Efficient GAN Training. (arXiv:2205.15677v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.15677</link>
<description rdf:parseType="Literal">&lt;p&gt;Training generative adversarial networks (GANs) with limited data is
challenging because the discriminator is prone to overfitting. Previously
proposed differentiable augmentation demonstrates improved data efficiency of
training GANs. However, the augmentation implicitly introduces undesired
invariance to augmentation for the discriminator since it ignores the change of
semantics in the label space caused by data transformation, which may limit the
representation learning ability of the discriminator and ultimately affect the
generative modeling performance of the generator. To mitigate the negative
impact of invariance while inheriting the benefits of data augmentation, we
propose a novel augmentation-aware self-supervised discriminator that predicts
the augmentation parameter of the augmented data. Particularly, the prediction
targets of real data and generated data are required to be distinguished since
they are different during training. We further encourage the generator to
adversarially learn from the self-supervised discriminator by generating
augmentation-predictable real and not fake data. This formulation connects the
learning objective of the generator and the arithmetic $-$ harmonic mean
divergence under certain assumptions. We compare our method with
state-of-the-art (SOTA) methods using the class-conditional BigGAN and
unconditional StyleGAN2 architectures on data-limited CIFAR-10, CIFAR-100,
FFHQ, LSUN-Cat, and five low-shot datasets. Experimental results demonstrate
significant improvements of our method over SOTA methods in training
data-efficient GANs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1&quot;&gt;Liang Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1&quot;&gt;Qi Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yige Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Songtao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chongyang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1&quot;&gt;Siyuan Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_P/0/1/0/all/0/1&quot;&gt;Pengfei Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhongyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Huawei Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xueqi Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.06009">
<title>Relative Policy-Transition Optimization for Fast Policy Transfer. (arXiv:2206.06009v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.06009</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of policy transfer between two Markov Decision
Processes (MDPs). We introduce a lemma based on existing theoretical results in
reinforcement learning to measure the relativity gap between two arbitrary
MDPs, that is the difference between any two cumulative expected returns
defined on different policies and environment dynamics. Based on this lemma, we
propose two new algorithms referred to as Relative Policy Optimization (RPO)
and Relative Transition Optimization (RTO), which offer fast policy transfer
and dynamics modelling, respectively. RPO transfers the policy evaluated in one
environment to maximize the return in another, while RTO updates the
parameterized dynamics model to reduce the gap between the dynamics of the two
environments. Integrating the two algorithms results in the complete Relative
Policy-Transition Optimization (RPTO) algorithm, in which the policy interacts
with the two environments simultaneously, such that data collections from two
environments, policy and transition updates are completed in one closed loop to
form a principled learning framework for policy transfer. We demonstrate the
effectiveness of RPTO on a set of MuJoCo continuous control tasks by creating
policy transfer problems via variant dynamics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiawei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Cheng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yizheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Baoxiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1&quot;&gt;Lei Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.08012">
<title>Meta-Referential Games to Learn Compositional Learning Behaviours. (arXiv:2207.08012v5 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2207.08012</link>
<description rdf:parseType="Literal">&lt;p&gt;Human beings use compositionality to generalise from past experiences to
novel experiences. We assume a separation of our experiences into fundamental
atomic components that can be recombined in novel ways to support our ability
to engage with novel experiences. We frame this as the ability to learn to
generalise compositionally, and we will refer to behaviours making use of this
ability as compositional learning behaviours (CLBs). A central problem to
learning CLBs is the resolution of a binding problem (BP). While it is another
feat of intelligence that human beings perform with ease, it is not the case
for state-of-the-art artificial agents. Thus, in order to build artificial
agents able to collaborate with human beings, we propose to develop a novel
benchmark to investigate agents&apos; abilities to exhibit CLBs by solving a
domain-agnostic version of the BP. We take inspiration from the language
emergence and grounding framework of referential games and propose a
meta-learning extension of referential games, entitled Meta-Referential Games,
and use this framework to build our benchmark, the Symbolic Behaviour Benchmark
(S2B). We provide baseline results and error analysis showing that our
benchmark is a compelling challenge that we hope will spur the research
community towards developing more capable artificial agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denamganai_K/0/1/0/all/0/1&quot;&gt;Kevin Denamgana&amp;#xef;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Missaoui_S/0/1/0/all/0/1&quot;&gt;Sondess Missaoui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walker_J/0/1/0/all/0/1&quot;&gt;James Alfred Walker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.08563">
<title>Submodularity, pairwise independence and correlation gap. (arXiv:2209.08563v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2209.08563</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we provide a characterization of the expected value of
monotone submodular set functions with $n$ pairwise independent random inputs.
Inspired by the notion of ``correlation gap&apos;&apos;, we study the ratio of the
maximum expected value of a function with arbitrary dependence among the random
inputs with given marginal probabilities to the maximum expected value of the
function with pairwise independent random inputs and the same marginal
probabilities. Our results show that the ratio is upper bounded by: (a) $4/3$
for $n = 3$ with general marginal probabilities and any monotone submodular set
function (b) $4/3$ for general $n$ with small and large marginal probabilities
and any monotone submodular set function and (c) $4k/(4k-1)$ for general $n$,
general identical probabilities and rank functions of $k$-uniform matroids. The
bound is tight in all three cases. This contrasts with the $e/(e-1)$ bound on
the correlation gap ratio for monotone submodular set functions with mutually
independent random inputs (which is known to be tight in case (b)), and
illustrates a fundamental difference in the behavior of submodular functions
with weaker notions of independence. These results can be immediately extended
beyond pairwise independence to correlated random inputs. We discuss
applications in distributionally robust optimization and mechanism design and
end the paper with a conjecture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ramachandra_A/0/1/0/all/0/1&quot;&gt;Arjun Ramachandra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Natarajan_K/0/1/0/all/0/1&quot;&gt;Karthik Natarajan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.12756">
<title>FAL-CUR: Fair Active Learning using Uncertainty and Representativeness on Fair Clustering. (arXiv:2209.12756v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.12756</link>
<description rdf:parseType="Literal">&lt;p&gt;Active Learning (AL) techniques have proven to be highly effective in
reducing data labeling costs across a range of machine learning tasks.
Nevertheless, one known challenge of these methods is their potential to
introduce unfairness towards sensitive attributes. Although recent approaches
have focused on enhancing fairness in AL, they tend to reduce the model&apos;s
accuracy. To address this issue, we propose a novel strategy, named Fair Active
Learning using fair Clustering, Uncertainty, and Representativeness (FAL-CUR),
to improve fairness in AL. FAL-CUR tackles the fairness problem in AL by
combining fair clustering with an acquisition function that determines which
samples to query based on their uncertainty and representativeness scores. We
evaluate the performance of FAL-CUR on four real-world datasets, and the
results demonstrate that FAL-CUR achieves a 15% - 20% improvement in fairness
compared to the best state-of-the-art method in terms of equalized odds while
maintaining stable accuracy scores. Furthermore, an ablation study highlights
the crucial roles of fair clustering in preserving fairness and the acquisition
function in stabilizing the accuracy performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fajri_R/0/1/0/all/0/1&quot;&gt;Ricky Fajri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saxena_A/0/1/0/all/0/1&quot;&gt;Akrati Saxena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_Y/0/1/0/all/0/1&quot;&gt;Yulong Pei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1&quot;&gt;Mykola Pechenizkiy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.01905">
<title>Polar Encoding: A Simple Baseline Approach for Classification with Missing Values. (arXiv:2210.01905v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.01905</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose polar encoding, a representation of categorical and numerical
$[0,1]$-valued attributes with missing values to be used in a classification
context. We argue that this is a good baseline approach, because it can be used
with any classification algorithm, preserves missingness information, is very
simple to apply and offers good performance. In particular, unlike the existing
missing-indicator approach, it does not require imputation, ensures that
missing values are equidistant from non-missing values, and lets decision tree
algorithms choose how to split missing values, thereby providing a practical
realisation of the &quot;missingness incorporated in attributes&quot; (MIA) proposal.
Furthermore, we show that categorical and $[0,1]$-valued attributes can be
viewed as special cases of a single attribute type, corresponding to the
classical concept of barycentric coordinates, and that this offers a natural
interpretation of polar encoding as a fuzzified form of one-hot encoding. With
an experiment based on twenty real-life datasets with missing values, we show
that, in terms of the resulting classification performance, polar encoding
performs better than the state-of-the-art strategies \e{multiple imputation by
chained equations} (MICE) and \e{multiple imputation with denoising
autoencoders} (MIDAS) and -- depending on the classifier -- about as well or
better than mean/mode imputation with missing-indicators.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lenz_O/0/1/0/all/0/1&quot;&gt;Oliver Urs Lenz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peralta_D/0/1/0/all/0/1&quot;&gt;Daniel Peralta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cornelis_C/0/1/0/all/0/1&quot;&gt;Chris Cornelis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.07780">
<title>Federated Best Arm Identification with Heterogeneous Clients. (arXiv:2210.07780v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.07780</link>
<description rdf:parseType="Literal">&lt;p&gt;We study best arm identification in a federated multi-armed bandit setting
with a central server and multiple clients, when each client has access to a
{\em subset} of arms and each arm yields independent Gaussian observations. The
goal is to identify the best arm of each client subject to an upper bound on
the error probability; here, the best arm is one that has the largest {\em
average} value of the means averaged across all clients having access to the
arm. Our interest is in the asymptotics as the error probability vanishes. We
provide an asymptotic lower bound on the growth rate of the expected stopping
time of any algorithm. Furthermore, we show that for any algorithm whose upper
bound on the expected stopping time matches with the lower bound up to a
multiplicative constant ({\em almost-optimal} algorithm), the ratio of any two
consecutive communication time instants must be {\em bounded}, a result that is
of independent interest. We thereby infer that an algorithm can communicate no
more sparsely than at exponential time instants in order to be almost-optimal.
For the class of almost-optimal algorithms, we present the first-of-its-kind
asymptotic lower bound on the expected number of {\em communication rounds}
until stoppage. We propose a novel algorithm that communicates at exponential
time instants, and demonstrate that it is asymptotically almost-optimal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhirui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karthik_P/0/1/0/all/0/1&quot;&gt;P. N. Karthik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1&quot;&gt;Vincent Y. F. Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chee_Y/0/1/0/all/0/1&quot;&gt;Yeow Meng Chee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.15657">
<title>Detecting fake accounts through Generative Adversarial Network in online social media. (arXiv:2210.15657v3 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/2210.15657</link>
<description rdf:parseType="Literal">&lt;p&gt;Online social media is integral to human life, facilitating messaging,
information sharing, and confidential communication while preserving privacy.
Platforms like Twitter, Instagram, and Facebook exemplify this phenomenon.
However, users face challenges due to network anomalies, often stemming from
malicious activities such as identity theft for financial gain or harm. This
paper proposes a novel method using user similarity measures and the Generative
Adversarial Network (GAN) algorithm to identify fake user accounts in the
Twitter dataset. Despite the problem&apos;s complexity, the method achieves an AUC
rate of 80\% in classifying and detecting fake accounts. Notably, the study
builds on previous research, highlighting advancements and insights into the
evolving landscape of anomaly detection in online social networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bordbar_J/0/1/0/all/0/1&quot;&gt;Jinus Bordbar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohammadrezaie_M/0/1/0/all/0/1&quot;&gt;Mohammadreza Mohammadrezaie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ardalan_S/0/1/0/all/0/1&quot;&gt;Saman Ardalan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shiri_M/0/1/0/all/0/1&quot;&gt;Mohammad Ebrahim Shiri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.16058">
<title>Goal Exploration Augmentation via Pre-trained Skills for Sparse-Reward Long-Horizon Goal-Conditioned Reinforcement Learning. (arXiv:2210.16058v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.16058</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning (RL) often struggles to accomplish a sparse-reward
long-horizon task in a complex environment. Goal-conditioned reinforcement
learning (GCRL) has been employed to tackle this difficult problem via a
curriculum of easy-to-reach sub-goals. In GCRL, exploring novel sub-goals is
essential for the agent to ultimately find the pathway to the desired goal. How
to explore novel sub-goals efficiently is one of the most challenging issues in
GCRL. Several goal exploration methods have been proposed to address this issue
but still struggle to find the desired goals efficiently. In this paper, we
propose a novel learning objective by optimizing the entropy of both achieved
and new goals to be explored for more efficient goal exploration in sub-goal
selection based GCRL. To optimize this objective, we first explore and exploit
the frequently occurring goal-transition patterns mined in the environments
similar to the current task to compose skills via skill learning. Then, the
pretrained skills are applied in goal exploration. Evaluation on a variety of
spare-reward long-horizon benchmark tasks suggests that incorporating our
method into several state-of-the-art GCRL baselines significantly boosts their
exploration efficiency while improving or maintaining their performance. The
source code is available at: https://github.com/GEAPS/GEAPS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Lisheng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Ke Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.16222">
<title>Improving Lipschitz-Constrained Neural Networks by Learning Activation Functions. (arXiv:2210.16222v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.16222</link>
<description rdf:parseType="Literal">&lt;p&gt;Lipschitz-constrained neural networks have several advantages over
unconstrained ones and can be applied to a variety of problems, making them a
topic of attention in the deep learning community. Unfortunately, it has been
shown both theoretically and empirically that they perform poorly when equipped
with ReLU activation functions. By contrast, neural networks with learnable
1-Lipschitz linear splines are known to be more expressive. In this paper, we
show that such networks correspond to global optima of a constrained functional
optimization problem that consists of the training of a neural network composed
of 1-Lipschitz linear layers and 1-Lipschitz freeform activation functions with
second-order total-variation regularization. Further, we propose an efficient
method to train these neural networks. Our numerical experiments show that our
trained networks compare favorably with existing 1-Lipschitz neural
architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ducotterd_S/0/1/0/all/0/1&quot;&gt;Stanislas Ducotterd&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goujon_A/0/1/0/all/0/1&quot;&gt;Alexis Goujon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bohra_P/0/1/0/all/0/1&quot;&gt;Pakshal Bohra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perdios_D/0/1/0/all/0/1&quot;&gt;Dimitris Perdios&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumayer_S/0/1/0/all/0/1&quot;&gt;Sebastian Neumayer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Unser_M/0/1/0/all/0/1&quot;&gt;Michael Unser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.08494">
<title>Who Reviews The Reviewers? A Multi-Level Jury Problem. (arXiv:2211.08494v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.08494</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of determining a binary ground truth using advice
from a group of independent reviewers (experts) who express their guess about a
ground truth correctly with some independent probability (competence). In this
setting, when all reviewers are competent (competence greater than one-half),
the Condorcet Jury Theorem tells us that adding more reviewers increases the
overall accuracy, and if all competences are known, then there exists an
optimal weighting of the reviewers. However, in practical settings, reviewers
may be noisy or incompetent, i.e., competence below half, and the number of
experts may be small, so the asymptotic Condorcet Jury Theorem is not
practically relevant. In such cases we explore appointing one or more chairs
(judges) who determine the weight of each reviewer for aggregation, creating
multiple levels. However, these chairs may be unable to correctly identify the
competence of the reviewers they oversee, and therefore unable to compute the
optimal weighting. We give conditions when a set of chairs is able to weight
the reviewers optimally, and depending on the competence distribution of the
agents, give results about when it is better to have more chairs or more
reviewers. Through numerical simulations we show that in some cases it is
better to have more chairs, but in many cases it is better to have more
reviewers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abramowitz_B/0/1/0/all/0/1&quot;&gt;Ben Abramowitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lev_O/0/1/0/all/0/1&quot;&gt;Omer Lev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mattei_N/0/1/0/all/0/1&quot;&gt;Nicholas Mattei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.09273">
<title>Privacy against Real-Time Speech Emotion Detection via Acoustic Adversarial Evasion of Machine Learning. (arXiv:2211.09273v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.09273</link>
<description rdf:parseType="Literal">&lt;p&gt;Smart speaker voice assistants (VAs) such as Amazon Echo and Google Home have
been widely adopted due to their seamless integration with smart home devices
and the Internet of Things (IoT) technologies. These VA services raise privacy
concerns, especially due to their access to our speech. This work considers one
such use case: the unaccountable and unauthorized surveillance of a user&apos;s
emotion via speech emotion recognition (SER). This paper presents DARE-GP, a
solution that creates additive noise to mask users&apos; emotional information while
preserving the transcription-relevant portions of their speech. DARE-GP does
this by using a constrained genetic programming approach to learn the spectral
frequency traits that depict target users&apos; emotional content, and then
generating a universal adversarial audio perturbation that provides this
privacy protection. Unlike existing works, DARE-GP provides: a) real-time
protection of previously unheard utterances, b) against previously unseen
black-box SER classifiers, c) while protecting speech transcription, and d)
does so in a realistic, acoustic environment. Further, this evasion is robust
against defenses employed by a knowledgeable adversary. The evaluations in this
work culminate with acoustic evaluations against two off-the-shelf commercial
smart speakers using a small-form-factor (raspberry pi) integrated with a
wake-word system to evaluate the efficacy of its real-world, real-time
deployment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Testa_B/0/1/0/all/0/1&quot;&gt;Brian Testa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Yi Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_H/0/1/0/all/0/1&quot;&gt;Harshit Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gump_A/0/1/0/all/0/1&quot;&gt;Avery Gump&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salekin_A/0/1/0/all/0/1&quot;&gt;Asif Salekin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.01071">
<title>Fake detection in imbalance dataset by Semi-supervised learning with GAN. (arXiv:2212.01071v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.01071</link>
<description rdf:parseType="Literal">&lt;p&gt;As social media continues to grow rapidly, the prevalence of harassment on
these platforms has also increased. This has piqued the interest of researchers
in the field of fake detection. Social media data, often forms complex graphs
with numerous nodes, posing several challenges. These challenges and
limitations include dealing with a significant amount of irrelevant features in
matrices and addressing issues such as high data dispersion and an imbalanced
class distribution within the dataset. To overcome these challenges and
limitations, researchers have employed auto-encoders and a combination of
semi-supervised learning with a GAN algorithm, referred to as SGAN. Our
proposed method utilizes auto-encoders for feature extraction and incorporates
SGAN. By leveraging an unlabeled dataset, the unsupervised layer of SGAN
compensates for the limited availability of labeled data, making efficient use
of the limited number of labeled instances. Multiple evaluation metrics were
employed, including the Confusion Matrix and the ROC curve. The dataset was
divided into training and testing sets, with 100 labeled samples for training
and 1,000 samples for testing. The novelty of our research lies in applying
SGAN to address the issue of imbalanced datasets in fake account detection. By
optimizing the use of a smaller number of labeled instances and reducing the
need for extensive computational power, our method offers a more efficient
solution. Additionally, our study contributes to the field by achieving an 81%
accuracy in detecting fake accounts using only 100 labeled samples. This
demonstrates the potential of SGAN as a powerful tool for handling minority
classes and addressing big data challenges in fake account detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bordbar_J/0/1/0/all/0/1&quot;&gt;Jinus Bordbar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ardalan_S/0/1/0/all/0/1&quot;&gt;Saman Ardalan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohammadrezaie_M/0/1/0/all/0/1&quot;&gt;Mohammadreza Mohammadrezaie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghasemi_Z/0/1/0/all/0/1&quot;&gt;Zahra Ghasemi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.08645">
<title>Efficient Conditionally Invariant Representation Learning. (arXiv:2212.08645v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.08645</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce the Conditional Independence Regression CovariancE (CIRCE), a
measure of conditional independence for multivariate continuous-valued
variables. CIRCE applies as a regularizer in settings where we wish to learn
neural features $\varphi(X)$ of data $X$ to estimate a target $Y$, while being
conditionally independent of a distractor $Z$ given $Y$. Both $Z$ and $Y$ are
assumed to be continuous-valued but relatively low dimensional, whereas $X$ and
its features may be complex and high dimensional. Relevant settings include
domain-invariant learning, fairness, and causal learning. The procedure
requires just a single ridge regression from $Y$ to kernelized features of $Z$,
which can be done in advance. It is then only necessary to enforce independence
of $\varphi(X)$ from residuals of this regression, which is possible with
attractive estimation properties and consistency guarantees. By contrast,
earlier measures of conditional feature dependence require multiple regressions
for each step of feature learning, resulting in more severe bias and variance,
and greater computational cost. When sufficiently rich features are used, we
establish that CIRCE is zero if and only if $\varphi(X) \perp \!\!\! \perp Z
\mid Y$. In experiments, we show superior performance to previous methods on
challenging benchmarks, including learning conditionally invariant image
features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pogodin_R/0/1/0/all/0/1&quot;&gt;Roman Pogodin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deka_N/0/1/0/all/0/1&quot;&gt;Namrata Deka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yazhe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sutherland_D/0/1/0/all/0/1&quot;&gt;Danica J. Sutherland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veitch_V/0/1/0/all/0/1&quot;&gt;Victor Veitch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gretton_A/0/1/0/all/0/1&quot;&gt;Arthur Gretton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.09010">
<title>Risk-Sensitive Reinforcement Learning with Exponential Criteria. (arXiv:2212.09010v4 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2212.09010</link>
<description rdf:parseType="Literal">&lt;p&gt;While reinforcement learning has shown experimental success in a number of
applications, it is known to be sensitive to noise and perturbations in the
parameters of the system, leading to high variance in the total reward amongst
different episodes in slightly different environments. To introduce robustness,
as well as sample efficiency, risk-sensitive reinforcement learning methods are
being thoroughly studied. In this work, we provide a definition of robust
reinforcement learning policies and formulate a risk-sensitive reinforcement
learning problem to approximate them, by solving an optimization problem with
respect to a modified objective based on exponential criteria. In particular,
we study a model-free risk-sensitive variation of the widely-used Monte Carlo
Policy Gradient algorithm and introduce a novel risk-sensitive online
Actor-Critic algorithm based on solving a multiplicative Bellman equation using
stochastic approximation updates. Analytical results suggest that the use of
exponential criteria generalizes commonly used ad-hoc regularization
approaches, improves sample efficiency, and introduces robustness with respect
to perturbations in the model parameters and the environment. The
implementation, performance, and robustness properties of the proposed methods
are evaluated in simulated experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Noorani_E/0/1/0/all/0/1&quot;&gt;Erfaun Noorani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mavridis_C/0/1/0/all/0/1&quot;&gt;Christos Mavridis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Baras_J/0/1/0/all/0/1&quot;&gt;John Baras&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.08830">
<title>Finding Nash equilibria by minimizing approximate exploitability with learned best responses. (arXiv:2301.08830v2 [cs.GT] UPDATED)</title>
<link>http://arxiv.org/abs/2301.08830</link>
<description rdf:parseType="Literal">&lt;p&gt;There has been substantial progress on finding game-theoretic equilibria.
Most of that work has focused on games with finite, discrete action spaces.
However, many games involving space, time, money, and other fine-grained
quantities have continuous action spaces (or are best modeled as such). We
study the problem of finding an approximate Nash equilibrium of games with
continuous action sets. The standard measure of closeness to Nash equilibrium
is exploitability, which measures how much players can benefit from
unilaterally changing their strategy. We propose two new methods that minimize
an approximation of the exploitability with respect to the strategy profile.
The first method uses a learned best-response function, which takes the current
strategy profile as input and returns candidate best responses for each player.
The strategy profile and best-response functions are trained simultaneously,
with the former trying to minimize exploitability while the latter tries to
maximize it. The second method maintains an ensemble of candidate best
responses for each player. In each iteration, the best-performing elements of
each ensemble are used to update the current strategy profile. The strategy
profile and best-response ensembles are simultaneously trained to minimize and
maximize the approximate exploitability, respectively. We evaluate our methods
on various continuous games, showing that they outperform prior methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_C/0/1/0/all/0/1&quot;&gt;Carlos Martin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sandholm_T/0/1/0/all/0/1&quot;&gt;Tuomas Sandholm&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.01190">
<title>On the Efficacy of Differentially Private Few-shot Image Classification. (arXiv:2302.01190v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2302.01190</link>
<description rdf:parseType="Literal">&lt;p&gt;There has been significant recent progress in training differentially private
(DP) models which achieve accuracy that approaches the best non-private models.
These DP models are typically pretrained on large public datasets and then
fine-tuned on private downstream datasets that are relatively large and similar
in distribution to the pretraining data. However, in many applications
including personalization and federated learning, it is crucial to perform well
(i) in the few-shot setting, as obtaining large amounts of labeled data may be
problematic; and (ii) on datasets from a wide variety of domains for use in
various specialist settings. To understand under which conditions few-shot DP
can be effective, we perform an exhaustive set of experiments that reveals how
the accuracy and vulnerability to attack of few-shot DP image classification
models are affected as the number of shots per class, privacy level, model
architecture, downstream dataset, and subset of learnable parameters in the
model vary. We show that to achieve DP accuracy on par with non-private models,
the shots per class must be increased as the privacy level increases. We also
show that learning parameter-efficient FiLM adapters under DP is competitive
with learning just the final classifier layer or learning all of the network
parameters. Finally, we evaluate DP federated learning systems and establish
state-of-the-art performance on the challenging FLAIR benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tobaben_M/0/1/0/all/0/1&quot;&gt;Marlon Tobaben&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shysheya_A/0/1/0/all/0/1&quot;&gt;Aliaksandra Shysheya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bronskill_J/0/1/0/all/0/1&quot;&gt;John Bronskill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Paverd_A/0/1/0/all/0/1&quot;&gt;Andrew Paverd&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tople_S/0/1/0/all/0/1&quot;&gt;Shruti Tople&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zanella_Beguelin_S/0/1/0/all/0/1&quot;&gt;Santiago Zanella-Beguelin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Turner_R/0/1/0/all/0/1&quot;&gt;Richard E Turner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Honkela_A/0/1/0/all/0/1&quot;&gt;Antti Honkela&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.01242">
<title>Neuro-Symbolic Continual Learning: Knowledge, Reasoning Shortcuts and Concept Rehearsal. (arXiv:2302.01242v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.01242</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Neuro-Symbolic Continual Learning, where a model has to solve a
sequence of neuro-symbolic tasks, that is, it has to map sub-symbolic inputs to
high-level concepts and compute predictions by reasoning consistently with
prior knowledge. Our key observation is that neuro-symbolic tasks, although
different, often share concepts whose semantics remains stable over time.
Traditional approaches fall short: existing continual strategies ignore
knowledge altogether, while stock neuro-symbolic architectures suffer from
catastrophic forgetting. We show that leveraging prior knowledge by combining
neuro-symbolic architectures with continual strategies does help avoid
catastrophic forgetting, but also that doing so can yield models affected by
reasoning shortcuts. These undermine the semantics of the acquired concepts,
even when detailed prior knowledge is provided upfront and inference is exact,
and in turn continual performance. To overcome these issues, we introduce COOL,
a COncept-level cOntinual Learning strategy tailored for neuro-symbolic
continual problems that acquires high-quality concepts and remembers them over
time. Our experiments on three novel benchmarks highlights how COOL attains
sustained high performance on neuro-symbolic continual learning tasks in which
other strategies fail.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marconato_E/0/1/0/all/0/1&quot;&gt;Emanuele Marconato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bontempo_G/0/1/0/all/0/1&quot;&gt;Gianpaolo Bontempo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ficarra_E/0/1/0/all/0/1&quot;&gt;Elisa Ficarra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calderara_S/0/1/0/all/0/1&quot;&gt;Simone Calderara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Passerini_A/0/1/0/all/0/1&quot;&gt;Andrea Passerini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teso_S/0/1/0/all/0/1&quot;&gt;Stefano Teso&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.04977">
<title>Mithridates: Auditing and Boosting Backdoor Resistance of Machine Learning Pipelines. (arXiv:2302.04977v3 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2302.04977</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning (ML) models trained on data from potentially untrusted
sources are vulnerable to poisoning. A small, maliciously crafted subset of the
training inputs can cause the model to learn a &quot;backdoor&quot; task (e.g.,
misclassify inputs with a certain feature) in addition to its main task. Recent
research proposed many hypothetical backdoor attacks whose efficacy heavily
depends on the configuration and training hyperparameters of the target model.
&lt;/p&gt;
&lt;p&gt;Given the variety of potential backdoor attacks, ML engineers who are not
security experts have no way to measure how vulnerable their current training
pipelines are, nor do they have a practical way to compare training
configurations so as to pick the more resistant ones. Deploying a defense
requires evaluating and choosing from among dozens of research papers and
re-engineering the training pipeline.
&lt;/p&gt;
&lt;p&gt;In this paper, we aim to provide ML engineers with pragmatic tools to audit
the backdoor resistance of their training pipelines and to compare different
training configurations, to help choose one that best balances accuracy and
security.
&lt;/p&gt;
&lt;p&gt;First, we propose a universal, attack-agnostic resistance metric based on the
minimum number of training inputs that must be compromised before the model
learns any backdoor.
&lt;/p&gt;
&lt;p&gt;Second, we design, implement, and evaluate Mithridates a multi-stage approach
that integrates backdoor resistance into the training-configuration search. ML
developers already rely on hyperparameter search to find configurations that
maximize the model&apos;s accuracy. Mithridates extends this standard tool to
balance accuracy and resistance without disruptive changes to the training
pipeline. We show that hyperparameters found by Mithridates increase resistance
to multiple types of backdoor attacks by 3-5x with only a slight impact on
accuracy. We also discuss extensions to AutoML and federated learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagdasaryan_E/0/1/0/all/0/1&quot;&gt;Eugene Bagdasaryan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shmatikov_V/0/1/0/all/0/1&quot;&gt;Vitaly Shmatikov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.05428">
<title>STERLING: Synergistic Representation Learning on Bipartite Graphs. (arXiv:2302.05428v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.05428</link>
<description rdf:parseType="Literal">&lt;p&gt;A fundamental challenge of bipartite graph representation learning is how to
extract informative node embeddings. Self-Supervised Learning (SSL) is a
promising paradigm to address this challenge. Most recent bipartite graph SSL
methods are based on contrastive learning which learns embeddings by
discriminating positive and negative node pairs. Contrastive learning usually
requires a large number of negative node pairs, which could lead to
computational burden and semantic errors. In this paper, we introduce a novel
synergistic representation learning model (STERLING) to learn node embeddings
without negative node pairs. STERLING preserves the unique local and global
synergies in bipartite graphs. The local synergies are captured by maximizing
the similarity of the inter-type and intra-type positive node pairs, and the
global synergies are captured by maximizing the mutual information of
co-clusters. Theoretical analysis demonstrates that STERLING could improve the
connectivity between different node types in the embedding space. Extensive
empirical evaluation on various benchmark datasets and tasks demonstrates the
effectiveness of STERLING for extracting node embeddings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_B/0/1/0/all/0/1&quot;&gt;Baoyu Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yuchen Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1&quot;&gt;Kaize Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1&quot;&gt;Chanyoung Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yada Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1&quot;&gt;Hanghang Tong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.09532">
<title>Pseudo Contrastive Learning for Graph-based Semi-supervised Learning. (arXiv:2302.09532v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.09532</link>
<description rdf:parseType="Literal">&lt;p&gt;Pseudo Labeling is a technique used to improve the performance of
semi-supervised Graph Neural Networks (GNNs) by generating additional
pseudo-labels based on confident predictions. However, the quality of generated
pseudo-labels has been a longstanding concern due to the sensitivity of the
classification objective with respect to the given labels. To avoid the
untrustworthy classification supervision indicating ``a node belongs to a
specific class,&apos;&apos; we favor the fault-tolerant contrasting supervision
demonstrating ``two nodes do not belong to the same class.&apos;&apos; Thus, the problem
of generating high-quality pseudo-labels is then transformed into a relaxed
version, i.e., identifying reliable negative pairs. To achieve this, we propose
a general framework for GNNs, termed Pseudo Contrastive Learning (PCL). It
separates two nodes whose positive and negative pseudo-labels target the same
class. To incorporate topological knowledge into learning, we devise a
topologically weighted contrastive loss that spends more effort separating
negative pairs with smaller topological distances. Experimentally, we apply PCL
to various GNNs, which consistently outperform their counterparts using other
popular general techniques on five real-world graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1&quot;&gt;Weigang Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_Z/0/1/0/all/0/1&quot;&gt;Ziyu Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wei Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yaming Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1&quot;&gt;Yuanhai Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1&quot;&gt;Lining Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Baosheng Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.06999">
<title>Identifying Label Errors in Object Detection Datasets by Loss Inspection. (arXiv:2303.06999v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.06999</link>
<description rdf:parseType="Literal">&lt;p&gt;Labeling datasets for supervised object detection is a dull and
time-consuming task. Errors can be easily introduced during annotation and
overlooked during review, yielding inaccurate benchmarks and performance
degradation of deep neural networks trained on noisy labels. In this work, we
for the first time introduce a benchmark for label error detection methods on
object detection datasets as well as a label error detection method and a
number of baselines. We simulate four different types of randomly introduced
label errors on train and test sets of well-labeled object detection datasets.
For our label error detection method we assume a two-stage object detector to
be given and consider the sum of both stages&apos; classification and regression
losses. The losses are computed with respect to the predictions and the noisy
labels including simulated label errors, aiming at detecting the latter. We
compare our method to three baselines: a naive one without deep learning, the
object detector&apos;s score and the entropy of the classification softmax
distribution. We outperform all baselines and demonstrate that among the
considered methods, ours is the only one that detects label errors of all four
types efficiently. Furthermore, we detect real label errors a) on commonly used
test datasets in object detection and b) on a proprietary dataset. In both
cases we achieve low false positives rates, i.e., we detect label errors with a
precision for a) of up to 71.5% and for b) with 97%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schubert_M/0/1/0/all/0/1&quot;&gt;Marius Schubert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riedlinger_T/0/1/0/all/0/1&quot;&gt;Tobias Riedlinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kahl_K/0/1/0/all/0/1&quot;&gt;Karsten Kahl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kroll_D/0/1/0/all/0/1&quot;&gt;Daniel Kr&amp;#xf6;ll&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schoenen_S/0/1/0/all/0/1&quot;&gt;Sebastian Schoenen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Segvic_S/0/1/0/all/0/1&quot;&gt;Sini&amp;#x161;a &amp;#x160;egvi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rottmann_M/0/1/0/all/0/1&quot;&gt;Matthias Rottmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.10343">
<title>Supervision Interpolation via LossMix: Generalizing Mixup for Object Detection and Beyond. (arXiv:2303.10343v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.10343</link>
<description rdf:parseType="Literal">&lt;p&gt;The success of data mixing augmentations in image classification tasks has
been well-received. However, these techniques cannot be readily applied to
object detection due to challenges such as spatial misalignment,
foreground/background distinction, and plurality of instances. To tackle these
issues, we first introduce a novel conceptual framework called Supervision
Interpolation (SI), which offers a fresh perspective on interpolation-based
augmentations by relaxing and generalizing Mixup. Based on SI, we propose
LossMix, a simple yet versatile and effective regularization that enhances the
performance and robustness of object detectors and more. Our key insight is
that we can effectively regularize the training on mixed data by interpolating
their loss errors instead of ground truth labels. Empirical results on the
PASCAL VOC and MS COCO datasets demonstrate that LossMix can consistently
outperform state-of-the-art methods widely adopted for detection. Furthermore,
by jointly leveraging LossMix with unsupervised domain adaptation, we
successfully improve existing approaches and set a new state of the art for
cross-domain object detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1&quot;&gt;Thanh Vu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1&quot;&gt;Baochen Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1&quot;&gt;Bodi Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ngai_A/0/1/0/all/0/1&quot;&gt;Alex Ngai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yueqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frahm_J/0/1/0/all/0/1&quot;&gt;Jan-Michael Frahm&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.16454">
<title>Conductivity Imaging from Internal Measurements with Mixed Least-Squares Deep Neural Networks. (arXiv:2303.16454v3 [math.NA] UPDATED)</title>
<link>http://arxiv.org/abs/2303.16454</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we develop a novel approach using deep neural networks to
reconstruct the conductivity distribution in elliptic problems from one
measurement of the solution over the whole domain. The approach is based on a
mixed reformulation of the governing equation and utilizes the standard
least-squares objective, with deep neural networks as ansatz functions to
approximate the conductivity and flux simultaneously. We provide a thorough
analysis of the deep neural network approximations of the conductivity for both
continuous and empirical losses, including rigorous error estimates that are
explicit in terms of the noise level, various penalty parameters and neural
network architectural parameters (depth, width and parameter bound). We also
provide multiple numerical experiments in two- and multi-dimensions to
illustrate distinct features of the approach, e.g., excellent stability with
respect to data noise and capability of solving high-dimensional problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Jin_B/0/1/0/all/0/1&quot;&gt;Bangti Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiyao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Quan_Q/0/1/0/all/0/1&quot;&gt;Qimeng Quan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhi Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.16532">
<title>Futures Quantitative Investment with Heterogeneous Continual Graph Neural Network. (arXiv:2303.16532v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.16532</link>
<description rdf:parseType="Literal">&lt;p&gt;This study aims to address the challenges of futures price prediction in
high-frequency trading (HFT) by proposing a continuous learning factor
predictor based on graph neural networks. The model integrates multi-factor
pricing theories with real-time market dynamics, effectively bypassing the
limitations of existing methods that lack financial theory guidance and ignore
various trend signals and their interactions. We propose three heterogeneous
tasks, including price moving average regression, price gap regression and
change-point detection to trace the short-, intermediate-, and long-term trend
factors present in the data. In addition, this study also considers the
cross-sectional correlation characteristics of future contracts, where prices
of different futures often show strong dynamic correlations. Each variable
(future contract) depends not only on its historical values (temporal) but also
on the observation of other variables (cross-sectional). To capture these
dynamic relationships more accurately, we resort to the spatio-temporal graph
neural network (STGNN) to enhance the predictive power of the model. The model
employs a continuous learning strategy to simultaneously consider these tasks
(factors). Additionally, due to the heterogeneity of the tasks, we propose to
calculate parameter importance with mutual information between original
observations and the extracted features to mitigate the catastrophic forgetting
(CF) problem. Empirical tests on 49 commodity futures in China&apos;s futures market
demonstrate that the proposed model outperforms other state-of-the-art models
in terms of prediction accuracy. Not only does this research promote the
integration of financial theory and deep learning, but it also provides a
scientific basis for actual trading decisions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1&quot;&gt;Min Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1&quot;&gt;Zhizhong Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_G/0/1/0/all/0/1&quot;&gt;Guosheng Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.16737">
<title>Multi-Agent Reinforcement Learning with Action Masking for UAV-enabled Mobile Communications. (arXiv:2303.16737v2 [cs.MA] UPDATED)</title>
<link>http://arxiv.org/abs/2303.16737</link>
<description rdf:parseType="Literal">&lt;p&gt;Unmanned Aerial Vehicles (UAVs) are increasingly used as aerial base stations
to provide ad hoc communications infrastructure. Building upon prior research
efforts which consider either static nodes, 2D trajectories or single UAV
systems, this paper focuses on the use of multiple UAVs for providing wireless
communication to mobile users in the absence of terrestrial communications
infrastructure. In particular, we jointly optimize UAV 3D trajectory and NOMA
power allocation to maximize system throughput. Firstly, a weighted
K-means-based clustering algorithm establishes UAV-user associations at regular
intervals. The efficacy of training a novel Shared Deep Q-Network (SDQN) with
action masking is then explored. Unlike training each UAV separately using DQN,
the SDQN reduces training time by using the experiences of multiple UAVs
instead of a single agent. We also show that SDQN can be used to train a
multi-agent system with differing action spaces. Simulation results confirm
that: 1) training a shared DQN outperforms a conventional DQN in terms of
maximum system throughput (+20%) and training time (-10%); 2) it can converge
for agents with different action spaces, yielding a 9% increase in throughput
compared to mutual learning algorithms; and 3) combining NOMA with an SDQN
architecture enables the network to achieve a better sum rate compared with
existing baseline schemes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rizvi_D/0/1/0/all/0/1&quot;&gt;Danish Rizvi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boyle_D/0/1/0/all/0/1&quot;&gt;David Boyle&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.05805">
<title>GDP nowcasting with artificial neural networks: How much does long-term memory matter?. (arXiv:2304.05805v2 [econ.EM] UPDATED)</title>
<link>http://arxiv.org/abs/2304.05805</link>
<description rdf:parseType="Literal">&lt;p&gt;In our study, we apply artificial neural networks (ANNs) to nowcast quarterly
GDP growth for the U.S. economy. Using the monthly FRED-MD database, we compare
the nowcasting performance of five different ANN architectures: the multilayer
perceptron (MLP), the one-dimensional convolutional neural network (1D CNN),
the Elman recurrent neural network (RNN), the long short-term memory network
(LSTM), and the gated recurrent unit (GRU). The empirical analysis presents the
results from two distinctively different evaluation periods. The first (2012:Q1
-- 2019:Q4) is characterized by balanced economic growth, while the second
(2012:Q1 -- 2022:Q4) also includes periods of the COVID-19 recession. According
to our results, longer input sequences result in more accurate nowcasts in
periods of balanced economic growth. However, this effect ceases above a
relatively low threshold value of around six quarters (eighteen months). During
periods of economic turbulence (e.g., during the COVID-19 recession), longer
input sequences do not help the models&apos; predictive performance; instead, they
seem to weaken their generalization capability. Combined results from the two
evaluation periods indicate that architectural features enabling for long-term
memory do not result in more accurate nowcasts. On the other hand, the 1D CNN
has proved to be a highly suitable model for GDP nowcasting. The network has
shown good nowcasting performance among the competitors during the first
evaluation period and achieved the overall best accuracy during the second
evaluation period. Consequently, first in the literature, we propose the
application of the 1D CNN for economic nowcasting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Nemeth_K/0/1/0/all/0/1&quot;&gt;Krist&amp;#xf3;f N&amp;#xe9;meth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Hadhazi_D/0/1/0/all/0/1&quot;&gt;D&amp;#xe1;niel Hadh&amp;#xe1;zi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.14082">
<title>JaxPruner: A concise library for sparsity research. (arXiv:2304.14082v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.14082</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces JaxPruner, an open-source JAX-based pruning and sparse
training library for machine learning research. JaxPruner aims to accelerate
research on sparse neural networks by providing concise implementations of
popular pruning and sparse training algorithms with minimal memory and latency
overhead. Algorithms implemented in JaxPruner use a common API and work
seamlessly with the popular optimization library Optax, which, in turn, enables
easy integration with existing JAX based libraries. We demonstrate this ease of
integration by providing examples in four different codebases: Scenic, t5x,
Dopamine and FedJAX and provide baseline experiments on popular benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Joo Hyung Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_W/0/1/0/all/0/1&quot;&gt;Wonpyo Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitchell_N/0/1/0/all/0/1&quot;&gt;Nicole Mitchell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pilault_J/0/1/0/all/0/1&quot;&gt;Jonathan Pilault&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Obando_Ceron_J/0/1/0/all/0/1&quot;&gt;Johan Obando-Ceron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Han-Byul Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1&quot;&gt;Namhoon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frantar_E/0/1/0/all/0/1&quot;&gt;Elias Frantar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1&quot;&gt;Yun Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yazdanbakhsh_A/0/1/0/all/0/1&quot;&gt;Amir Yazdanbakhsh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1&quot;&gt;Shivani Agrawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramanian_S/0/1/0/all/0/1&quot;&gt;Suvinay Subramanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kao_S/0/1/0/all/0/1&quot;&gt;Sheng-Chun Kao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xingyao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gale_T/0/1/0/all/0/1&quot;&gt;Trevor Gale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bik_A/0/1/0/all/0/1&quot;&gt;Aart Bik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1&quot;&gt;Woohyun Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferev_M/0/1/0/all/0/1&quot;&gt;Milen Ferev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1&quot;&gt;Zhonglin Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hong-Seok Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dauphin_Y/0/1/0/all/0/1&quot;&gt;Yann Dauphin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dziugaite_G/0/1/0/all/0/1&quot;&gt;Gintare Karolina Dziugaite&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castro_P/0/1/0/all/0/1&quot;&gt;Pablo Samuel Castro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Evci_U/0/1/0/all/0/1&quot;&gt;Utku Evci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.09820">
<title>Machine-Made Media: Monitoring the Mobilization of Machine-Generated Articles on Misinformation and Mainstream News Websites. (arXiv:2305.09820v3 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2305.09820</link>
<description rdf:parseType="Literal">&lt;p&gt;As large language models (LLMs) like ChatGPT have gained traction, an
increasing number of news websites have begun utilizing them to generate
articles. However, not only can these language models produce factually
inaccurate articles on reputable websites but disreputable news sites can
utilize LLMs to mass produce misinformation. To begin to understand this
phenomenon, we present one of the first large-scale studies of the prevalence
of synthetic articles within online news media. To do this, we train a
DeBERTa-based synthetic news detector and classify over 15.90 million articles
from 3,074 misinformation and mainstream news websites. We find that between
January 1, 2022, and May 1, 2023, the relative number of synthetic news
articles increased by 55.4% on mainstream websites while increasing by 457% on
misinformation sites. We find that this increase is largely driven by smaller
less popular websites. Analyzing the impact of the release of ChatGPT using an
interrupted-time-series, we show that while its release resulted in a marked
increase in synthetic articles on small sites as well as misinformation news
websites, there was not a corresponding increase on large mainstream news
websites.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanley_H/0/1/0/all/0/1&quot;&gt;Hans W. A. Hanley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durumeric_Z/0/1/0/all/0/1&quot;&gt;Zakir Durumeric&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13030">
<title>Adaptive action supervision in reinforcement learning from real-world multi-agent demonstrations. (arXiv:2305.13030v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13030</link>
<description rdf:parseType="Literal">&lt;p&gt;Modeling of real-world biological multi-agents is a fundamental problem in
various scientific and engineering fields. Reinforcement learning (RL) is a
powerful framework to generate flexible and diverse behaviors in cyberspace;
however, when modeling real-world biological multi-agents, there is a domain
gap between behaviors in the source (i.e., real-world data) and the target
(i.e., cyberspace for RL), and the source environment parameters are usually
unknown. In this paper, we propose a method for adaptive action supervision in
RL from real-world demonstrations in multi-agent scenarios. We adopt an
approach that combines RL and supervised learning by selecting actions of
demonstrations in RL based on the minimum distance of dynamic time warping for
utilizing the information of the unknown source dynamics. This approach can be
easily applied to many existing neural network architectures and provide us
with an RL model balanced between reproducibility as imitation and
generalization ability to obtain rewards in cyberspace. In the experiments,
using chase-and-escape and football tasks with the different dynamics between
the unknown source and target environments, we show that our approach achieved
a balance between the reproducibility and the generalization ability compared
with the baselines. In particular, we used the tracking data of professional
football players as expert demonstrations in football and show successful
performances despite the larger gap between behaviors in the source and target
environments than the chase-and-escape task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fujii_K/0/1/0/all/0/1&quot;&gt;Keisuke Fujii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsutsui_K/0/1/0/all/0/1&quot;&gt;Kazushi Tsutsui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scott_A/0/1/0/all/0/1&quot;&gt;Atom Scott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakahara_H/0/1/0/all/0/1&quot;&gt;Hiroshi Nakahara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takeishi_N/0/1/0/all/0/1&quot;&gt;Naoya Takeishi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawahara_Y/0/1/0/all/0/1&quot;&gt;Yoshinobu Kawahara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14160">
<title>Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning. (arXiv:2305.14160v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14160</link>
<description rdf:parseType="Literal">&lt;p&gt;In-context learning (ICL) emerges as a promising capability of large language
models (LLMs) by providing them with demonstration examples to perform diverse
tasks. However, the underlying mechanism of how LLMs learn from the provided
context remains under-explored. In this paper, we investigate the working
mechanism of ICL through an information flow lens. Our findings reveal that
label words in the demonstration examples function as anchors: (1) semantic
information aggregates into label word representations during the shallow
computation layers&apos; processing; (2) the consolidated information in label words
serves as a reference for LLMs&apos; final predictions. Based on these insights, we
introduce an anchor re-weighting method to improve ICL performance, a
demonstration compression technique to expedite inference, and an analysis
framework for diagnosing ICL errors in GPT2-XL. The promising applications of
our findings again validate the uncovered ICL working mechanism and pave the
way for future studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lean Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1&quot;&gt;Damai Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Deli Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1&quot;&gt;Fandong Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jie Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xu Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14901">
<title>Chain-of-Questions Training with Latent Answers for Robust Multistep Question Answering. (arXiv:2305.14901v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14901</link>
<description rdf:parseType="Literal">&lt;p&gt;We train a language model (LM) to robustly answer multistep questions by
generating and answering sub-questions. We propose Chain-of-Questions, a
framework that trains a model to generate sub-questions and sub-answers one at
a time by leveraging human annotated question decomposition meaning
representation (QDMR). The key technical challenge is that QDMR only contains
sub-questions but not answers to those sub-questions, so we treat sub-answers
as latent variables and optimize them using a novel dynamic mixture of Hard-EM
and MAPO. Chain-of-Questions greatly outperforms strong neuro-symbolic methods
by 9.0 F1 on DROP contrast set, and outperforms GPT-3.5 by 24.3 F1 on HOTPOTQA
adversarial set, thus demonstrating the effectiveness and robustness of our
framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1&quot;&gt;Jesse Thomason&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1&quot;&gt;Robin Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14978">
<title>Probabilistic Exponential Integrators. (arXiv:2305.14978v2 [math.NA] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14978</link>
<description rdf:parseType="Literal">&lt;p&gt;Probabilistic solvers provide a flexible and efficient framework for
simulation, uncertainty quantification, and inference in dynamical systems.
However, like standard solvers, they suffer performance penalties for certain
stiff systems, where small steps are required not for reasons of numerical
accuracy but for the sake of stability. This issue is greatly alleviated in
semi-linear problems by the probabilistic exponential integrators developed in
this paper. By including the fast, linear dynamics in the prior, we arrive at a
class of probabilistic integrators with favorable properties. Namely, they are
proven to be L-stable, and in a certain case reduce to a classic exponential
integrator -- with the added benefit of providing a probabilistic account of
the numerical error. The method is also generalized to arbitrary non-linear
systems by imposing piece-wise semi-linearity on the prior via Jacobians of the
vector field at the previous estimates, resulting in probabilistic exponential
Rosenbrock methods. We evaluate the proposed methods on multiple stiff
differential equations and demonstrate their improved stability and efficiency
over established probabilistic solvers. The present contribution thus expands
the range of problems that can be effectively tackled within probabilistic
numerics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Bosch_N/0/1/0/all/0/1&quot;&gt;Nathanael Bosch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hennig_P/0/1/0/all/0/1&quot;&gt;Philipp Hennig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Tronarp_F/0/1/0/all/0/1&quot;&gt;Filip Tronarp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16901">
<title>Generalizing Adam to Manifolds for Efficiently Training Transformers. (arXiv:2305.16901v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16901</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the primary reasons behind the success of neural networks has been the
emergence of an array of new, highly-successful optimizers, perhaps most
importantly the Adam optimizer. It is wiedely used for training neural
networks, yet notoriously hard to interpret. Lacking a clear physical
intuition, Adam is difficult to generalize to manifolds. Some attempts have
been made to directly apply parts of the Adam algorithm to manifolds or to find
an underlying structure, but a full generalization has remained elusive. In
this work a new approach is presented that leverages the special structure of
the manifolds which are relevant for optimization of neural networks, such as
the Stiefel manifold, the symplectic Stiefel manifold, the Grassmann manifold
and the symplectic Grassmann manifold: all of these are homogeneous spaces and
as such admit a global tangent space representation. This global tangent space
representation is used to perform all of the steps in the Adam optimizer. The
resulting algorithm is then applied to train a transformer for which
orthogonality constraints are enforced up to machine precision and we observe
significant speed-ups in the training process. Optimization of neural networks
where they weights do not lie on a manifold is identified as a special case of
the presented framkework. This allows for a flexible implementation in which
the learning rate is adapted simultaneously for all parameters, irrespective of
whether they are an element of a general manifold or a vector space.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brantner_B/0/1/0/all/0/1&quot;&gt;Benedikt Brantner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17205">
<title>Ghost Noise for Regularizing Deep Neural Networks. (arXiv:2305.17205v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17205</link>
<description rdf:parseType="Literal">&lt;p&gt;Batch Normalization (BN) is widely used to stabilize the optimization process
and improve the test performance of deep neural networks. The regularization
effect of BN depends on the batch size and explicitly using smaller batch sizes
with Batch Normalization, a method known as Ghost Batch Normalization (GBN),
has been found to improve generalization in many settings. We investigate the
effectiveness of GBN by disentangling the induced ``Ghost Noise&apos;&apos; from
normalization and quantitatively analyzing the distribution of noise as well as
its impact on model performance. Inspired by our analysis, we propose a new
regularization technique called Ghost Noise Injection (GNI) that imitates the
noise in GBN without incurring the detrimental train-test discrepancy effects
of small batch training. We experimentally show that GNI can provide a greater
generalization benefit than GBN. Ghost Noise Injection can also be beneficial
in otherwise non-noisy settings such as layer-normalized networks, providing
additional evidence of the usefulness of Ghost Noise in Batch Normalization as
a regularizer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kosson_A/0/1/0/all/0/1&quot;&gt;Atli Kosson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1&quot;&gt;Dongyang Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1&quot;&gt;Martin Jaggi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.01843">
<title>Lifting Architectural Constraints of Injective Flows. (arXiv:2306.01843v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.01843</link>
<description rdf:parseType="Literal">&lt;p&gt;Normalizing Flows explicitly maximize a full-dimensional likelihood on the
training data. However, real data is typically only supported on a
lower-dimensional manifold leading the model to expend significant compute on
modeling noise. Injective Flows fix this by jointly learning a manifold and the
distribution on it. So far, they have been limited by restrictive architectures
and/or high computational cost. We lift both constraints by a new efficient
estimator for the maximum likelihood loss, compatible with free-form bottleneck
architectures. We further show that naively learning both the data manifold and
the distribution on it can lead to divergent solutions, and use this insight to
motivate a stable maximum likelihood training objective. We perform extensive
experiments on toy, tabular and image data, demonstrating the competitive
performance of the resulting model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sorrenson_P/0/1/0/all/0/1&quot;&gt;Peter Sorrenson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Draxler_F/0/1/0/all/0/1&quot;&gt;Felix Draxler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rousselot_A/0/1/0/all/0/1&quot;&gt;Armand Rousselot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hummerich_S/0/1/0/all/0/1&quot;&gt;Sander Hummerich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zimmermann_L/0/1/0/all/0/1&quot;&gt;Lea Zimmermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kothe_U/0/1/0/all/0/1&quot;&gt;Ullrich K&amp;#xf6;the&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06154">
<title>HypLL: The Hyperbolic Learning Library. (arXiv:2306.06154v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06154</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning in hyperbolic space is quickly gaining traction in the fields
of machine learning, multimedia, and computer vision. Deep networks commonly
operate in Euclidean space, implicitly assuming that data lies on regular
grids. Recent advances have shown that hyperbolic geometry provides a viable
alternative foundation for deep learning, especially when data is hierarchical
in nature and when working with few embedding dimensions. Currently however, no
accessible open-source library exists to build hyperbolic network modules akin
to well-known deep learning libraries. We present HypLL, the Hyperbolic
Learning Library to bring the progress on hyperbolic deep learning together.
HypLL is built on top of PyTorch, with an emphasis in its design for
ease-of-use, in order to attract a broad audience towards this new and
open-ended research direction. The code is available at:
https://github.com/maxvanspengler/hyperbolic_learning_library.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spengler_M/0/1/0/all/0/1&quot;&gt;Max van Spengler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wirth_P/0/1/0/all/0/1&quot;&gt;Philipp Wirth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mettes_P/0/1/0/all/0/1&quot;&gt;Pascal Mettes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14048">
<title>H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models. (arXiv:2306.14048v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14048</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs), despite their recent impressive
accomplishments, are notably cost-prohibitive to deploy, particularly for
applications involving long-content generation, such as dialogue systems and
story writing. Often, a large amount of transient state information, referred
to as the KV cache, is stored in GPU memory in addition to model parameters,
scaling linearly with the sequence length and batch size. In this paper, we
introduce a novel approach for implementing the KV cache which significantly
reduces its memory footprint. Our approach is based on the noteworthy
observation that a small portion of tokens contributes most of the value when
computing attention scores. We call these tokens Heavy Hitters (H$_2$). Through
a comprehensive investigation, we find that (i) the emergence of H$_2$ is
natural and strongly correlates with the frequent co-occurrence of tokens in
the text, and (ii) removing them results in significant performance
degradation. Based on these insights, we propose Heavy Hitter Oracle (H$_2$O),
a KV cache eviction policy that dynamically retains a balance of recent and
H$_2$ tokens. We formulate the KV cache eviction as a dynamic submodular
problem and prove (under mild assumptions) a theoretical guarantee for our
novel eviction algorithm which could help guide future work. We validate the
accuracy of our algorithm with OPT, LLaMA, and GPT-NeoX across a wide range of
tasks. Our implementation of H$_2$O with 20% heavy hitters improves the
throughput over three leading inference systems DeepSpeed Zero-Inference,
Hugging Face Accelerate, and FlexGen by up to 29$\times$, 29$\times$, and
3$\times$ on OPT-6.7B and OPT-30B. With the same batch size, H2O can reduce the
latency by up to 1.9$\times$. The code is available at
https://github.com/FMInference/H2O.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_Y/0/1/0/all/0/1&quot;&gt;Ying Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1&quot;&gt;Tianyi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianlong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1&quot;&gt;Lianmin Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1&quot;&gt;Ruisi Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zhao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yuandong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1&quot;&gt;Christopher R&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barrett_C/0/1/0/all/0/1&quot;&gt;Clark Barrett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Beidi Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05152">
<title>Fast Neural Network Inference on FPGAs for Triggering on Long-Lived Particles at Colliders. (arXiv:2307.05152v2 [hep-ex] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05152</link>
<description rdf:parseType="Literal">&lt;p&gt;Experimental particle physics demands a sophisticated trigger and acquisition
system capable to efficiently retain the collisions of interest for further
investigation. Heterogeneous computing with the employment of FPGA cards may
emerge as a trending technology for the triggering strategy of the upcoming
high-luminosity program of the Large Hadron Collider at CERN. In this context,
we present two machine-learning algorithms for selecting events where neutral
long-lived particles decay within the detector volume studying their accuracy
and inference time when accelerated on commercially available Xilinx FPGA
accelerator cards. The inference time is also confronted with a CPU- and
GPU-based hardware setup. The proposed new algorithms are proven efficient for
the considered benchmark physics scenario and their accuracy is found to not
degrade when accelerated on the FPGA cards. The results indicate that all
tested architectures fit within the latency requirements of a second-level
trigger farm and that exploiting accelerator technologies for real-time
processing of particle-physics collisions is a promising research field that
deserves additional investigations, in particular with machine-learning models
with a large number of trainable parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Coccaro_A/0/1/0/all/0/1&quot;&gt;Andrea Coccaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Bello_F/0/1/0/all/0/1&quot;&gt;Francesco Armando Di Bello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Giagu_S/0/1/0/all/0/1&quot;&gt;Stefano Giagu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Rambelli_L/0/1/0/all/0/1&quot;&gt;Lucrezia Rambelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Stocchetti_N/0/1/0/all/0/1&quot;&gt;Nicola Stocchetti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.16676">
<title>End-to-End Reinforcement Learning for Torque Based Variable Height Hopping. (arXiv:2307.16676v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2307.16676</link>
<description rdf:parseType="Literal">&lt;p&gt;Legged locomotion is arguably the most suited and versatile mode to deal with
natural or unstructured terrains. Intensive research into dynamic walking and
running controllers has recently yielded great advances, both in the optimal
control and reinforcement learning (RL) literature. Hopping is a challenging
dynamic task involving a flight phase and has the potential to increase the
traversability of legged robots. Model based control for hopping typically
relies on accurate detection of different jump phases, such as lift-off or
touch down, and using different controllers for each phase. In this paper, we
present a end-to-end RL based torque controller that learns to implicitly
detect the relevant jump phases, removing the need to provide manual heuristics
for state detection. We also extend a method for simulation to reality transfer
of the learned controller to contact rich dynamic tasks, resulting in
successful deployment on the robot after training without parameter tuning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soni_R/0/1/0/all/0/1&quot;&gt;Raghav Soni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harnack_D/0/1/0/all/0/1&quot;&gt;Daniel Harnack&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isermann_H/0/1/0/all/0/1&quot;&gt;Hauke Isermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fushimi_S/0/1/0/all/0/1&quot;&gt;Sotaro Fushimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Shivesh Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirchner_F/0/1/0/all/0/1&quot;&gt;Frank Kirchner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02068">
<title>Specious Sites: Tracking the Spread and Sway of Spurious News Stories at Scale. (arXiv:2308.02068v2 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.02068</link>
<description rdf:parseType="Literal">&lt;p&gt;Misinformation, propaganda, and outright lies proliferate on the web, with
some narratives having dangerous real-world consequences on public health,
elections, and individual safety. However, despite the impact of
misinformation, the research community largely lacks automated and programmatic
approaches for tracking news narratives across online platforms. In this work,
utilizing daily scrapes of 1,334 unreliable news websites, the large-language
model MPNet, and DP-Means clustering, we introduce a system to automatically
identify and track the narratives spread within online ecosystems. Identifying
52,036 narratives on these 1,334 websites, we describe the most prevalent
narratives spread in 2022 and identify the most influential websites that
originate and amplify narratives. Finally, we show how our system can be
utilized to detect new narratives originating from unreliable news websites and
to aid fact-checkers in more quickly addressing misinformation. We release code
and data at https://github.com/hanshanley/specious-sites.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanley_H/0/1/0/all/0/1&quot;&gt;Hans W. A. Hanley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1&quot;&gt;Deepak Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durumeric_Z/0/1/0/all/0/1&quot;&gt;Zakir Durumeric&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04690">
<title>Finite Element Operator Network for Solving Parametric PDEs. (arXiv:2308.04690v2 [math.NA] UPDATED)</title>
<link>http://arxiv.org/abs/2308.04690</link>
<description rdf:parseType="Literal">&lt;p&gt;Partial differential equations (PDEs) underlie our understanding and
prediction of natural phenomena across numerous fields, including physics,
engineering, and finance. However, solving parametric PDEs is a complex task
that necessitates efficient numerical methods. In this paper, we propose a
novel approach for solving parametric PDEs using a Finite Element Operator
Network (FEONet). Our proposed method leverages the power of deep learning in
conjunction with traditional numerical methods, specifically the finite element
method, to solve parametric PDEs in the absence of any paired input-output
training data. We performed various experiments on several benchmark problems
and confirmed that our approach has demonstrated excellent performance across
various settings and environments, proving its versatility in terms of
accuracy, generalization, and computational flexibility. Our FEONet framework
shows potential for application in various fields where PDEs play a crucial
role in modeling complex domains with diverse boundary conditions and singular
behavior. Furthermore, we provide theoretical convergence analysis to support
our approach, utilizing finite element approximation in numerical analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jae Yong Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ko_S/0/1/0/all/0/1&quot;&gt;Seungchan Ko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hong_Y/0/1/0/all/0/1&quot;&gt;Youngjoon Hong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12681">
<title>LR-XFL: Logical Reasoning-based Explainable Federated Learning. (arXiv:2308.12681v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12681</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) is an emerging approach for training machine learning
models collaboratively while preserving data privacy. The need for privacy
protection makes it difficult for FL models to achieve global transparency and
explainability. To address this limitation, we incorporate logic-based
explanations into FL by proposing the Logical Reasoning-based eXplainable
Federated Learning (LR-XFL) approach. Under LR-XFL, FL clients create local
logic rules based on their local data and send them, along with model updates,
to the FL server. The FL server connects the local logic rules through a proper
logical connector that is derived based on properties of client data, without
requiring access to the raw data. In addition, the server also aggregates the
local model updates with weight values determined by the quality of the
clients&apos; local data as reflected by their uploaded logic rules. The results
show that LR-XFL outperforms the most relevant baseline by 1.19%, 5.81% and
5.41% in terms of classification accuracy, rule accuracy and rule fidelity,
respectively. The explicit rule evaluation and expression under LR-XFL enable
human experts to validate and correct the rules on the server side, hence
improving the global FL model&apos;s robustness to errors. It has the potential to
enhance the transparency of FL models for areas like healthcare and finance
where both data privacy and explainability are important.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yanci Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Han Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.13304">
<title>Rapid Artefact Removal and H&amp;E-Stained Tissue Segmentation. (arXiv:2308.13304v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.13304</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an innovative method for rapidly segmenting hematoxylin and eosin
(H&amp;amp;E)-stained tissue in whole-slide images (WSIs) that eliminates a wide range
of undesirable artefacts such as pen marks and scanning artefacts. Our method
involves taking a single-channel representation of a lowmagnification RGB
overview of the WSI in which the pixel values are bimodally distributed such
that H&amp;amp;E-stained tissue is easily distinguished from both background and a wide
variety of artefacts. We demonstrate our method on 30 WSIs prepared from a wide
range of institutions and WSI digital scanners, each containing substantial
artefacts, and compare it to segmentations provided by Otsu thresholding and
Histolab tissue segmentation and pen filtering tools. We found that our method
segmented the tissue and fully removed all artefacts in 29 out of 30 WSIs,
whereas Otsu thresholding failed to remove any artefacts, and the Histolab pen
filtering tools only partially removed the pen marks. The beauty of our
approach lies in its simplicity: manipulating RGB colour space and using Otsu
thresholding allows for the segmentation of H&amp;amp;E-stained tissue and the rapid
removal of artefacts without the need for machine learning or parameter tuning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schreiber_B/0/1/0/all/0/1&quot;&gt;B. A. Schreiber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Denholm_J/0/1/0/all/0/1&quot;&gt;J. Denholm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jaeckle_F/0/1/0/all/0/1&quot;&gt;F. Jaeckle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Arends_M/0/1/0/all/0/1&quot;&gt;M. J. Arends&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Branson_K/0/1/0/all/0/1&quot;&gt;K. M. Branson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schonlieb_C/0/1/0/all/0/1&quot;&gt;C.-B. Sch&amp;#xf6;nlieb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Soilleux_E/0/1/0/all/0/1&quot;&gt;E. J. Soilleux&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.13976">
<title>Label Denoising through Cross-Model Agreement. (arXiv:2308.13976v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.13976</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning from corrupted labels is very common in real-world machine-learning
applications. Memorizing such noisy labels could affect the learning of the
model, leading to sub-optimal performances. In this work, we propose a novel
framework to learn robust machine-learning models from noisy labels. Through an
empirical study, we find that different models make relatively similar
predictions on clean examples, while the predictions on noisy examples vary
much more across different models. Motivated by this observation, we propose
\em denoising with cross-model agreement \em (DeCA) which aims to minimize the
KL-divergence between the true label distributions parameterized by two machine
learning models while maximizing the likelihood of data observation. We employ
the proposed DeCA on both the binary label scenario and the multiple label
scenario. For the binary label scenario, we select implicit feedback
recommendation as the downstream task and conduct experiments with four
state-of-the-art recommendation models on four datasets. For the multiple-label
scenario, the downstream application is image classification on two benchmark
datasets. Experimental results demonstrate that the proposed methods
significantly improve the model performance compared with normal training and
other denoising methods on both binary and multiple-label scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xin_X/0/1/0/all/0/1&quot;&gt;Xin Xin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1&quot;&gt;Zaiqiao Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jose_J/0/1/0/all/0/1&quot;&gt;Joemon Jose&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1&quot;&gt;Fuli Feng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.06453">
<title>Narrowing the Gap between Supervised and Unsupervised Sentence Representation Learning with Large Language Model. (arXiv:2309.06453v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.06453</link>
<description rdf:parseType="Literal">&lt;p&gt;Sentence Representation Learning (SRL) is a fundamental task in Natural
Language Processing (NLP), with the Contrastive Learning of Sentence Embeddings
(CSE) being the mainstream technique due to its superior performance. An
intriguing phenomenon in CSE is the significant performance gap between
supervised and unsupervised methods, with their only difference lying in the
training data. Previous works attribute this performance gap to differences in
two representation properties (alignment and uniformity). However, since
alignment and uniformity only measure the results, they fail to answer &quot;What
aspects of the training data contribute to the performance gap?&quot; and &quot;How can
the performance gap be narrowed?&quot;, In this paper, we conduct empirical
experiments to answer these &quot;What&quot; and &quot;How&quot; questions. We first answer the
&quot;What&quot; question by thoroughly comparing the behavior of supervised and
unsupervised CSE during their respective training processes. From the
comparison, we identify the similarity pattern as a key factor to the
performance gap, and introduce a metric, called Relative Fitting Difficulty
(RFD), to measure the complexity of the similarity pattern. Then, based on the
insights gained from the &quot;What&quot; question, we tackle the &quot;How&quot; question by
increasing the pattern complexity of the training data. We achieve this by
leveraging the In-Context Learning (ICL) capability of the Large Language Model
(LLM) to generate data that simulates complex patterns. By utilizing the
hierarchical patterns in the LLM-generated data, we effectively narrow the gap
between supervised and unsupervised CSE. We release our codes and appendix at
https://github.com/BDBC-KG-NLP/NGCSE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mingxin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Richong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_Z/0/1/0/all/0/1&quot;&gt;Zhijie Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1&quot;&gt;Yongyi Mao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.08765">
<title>Mining Patents with Large Language Models Elucidates the Chemical Function Landscape. (arXiv:2309.08765v2 [q-bio.QM] UPDATED)</title>
<link>http://arxiv.org/abs/2309.08765</link>
<description rdf:parseType="Literal">&lt;p&gt;The fundamental goal of small molecule discovery is to generate chemicals
with target functionality. While this often proceeds through structure-based
methods, we set out to investigate the practicality of orthogonal methods that
leverage the extensive corpus of chemical literature. We hypothesize that a
sufficiently large text-derived chemical function dataset would mirror the
actual landscape of chemical functionality. Such a landscape would implicitly
capture complex physical and biological interactions given that chemical
function arises from both a molecule&apos;s structure and its interacting partners.
To evaluate this hypothesis, we built a Chemical Function (CheF) dataset of
patent-derived functional labels. This dataset, comprising 631K
molecule-function pairs, was created using an LLM- and embedding-based method
to obtain functional labels for approximately 100K molecules from their
corresponding 188K unique patents. We carry out a series of analyses
demonstrating that the CheF dataset contains a semantically coherent textual
representation of the functional landscape congruent with chemical structural
relationships, thus approximating the actual chemical function landscape. We
then demonstrate that this text-based functional landscape can be leveraged to
identify drugs with target functionality using a model able to predict
functional profiles from structure alone. We believe that functional
label-guided molecular discovery may serve as an orthogonal approach to
traditional structure-based methods in the pursuit of designing novel
functional molecules.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kosonocky_C/0/1/0/all/0/1&quot;&gt;Clayton W. Kosonocky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wilke_C/0/1/0/all/0/1&quot;&gt;Claus O. Wilke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Marcotte_E/0/1/0/all/0/1&quot;&gt;Edward M. Marcotte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ellington_A/0/1/0/all/0/1&quot;&gt;Andrew D. Ellington&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11518">
<title>Ad-load Balancing via Off-policy Learning in a Content Marketplace. (arXiv:2309.11518v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2309.11518</link>
<description rdf:parseType="Literal">&lt;p&gt;Ad-load balancing is a critical challenge in online advertising systems,
particularly in the context of social media platforms, where the goal is to
maximize user engagement and revenue while maintaining a satisfactory user
experience. This requires the optimization of conflicting objectives, such as
user satisfaction and ads revenue. Traditional approaches to ad-load balancing
rely on static allocation policies, which fail to adapt to changing user
preferences and contextual factors. In this paper, we present an approach that
leverages off-policy learning and evaluation from logged bandit feedback. We
start by presenting a motivating analysis of the ad-load balancing problem,
highlighting the conflicting objectives between user satisfaction and ads
revenue. We emphasize the nuances that arise due to user heterogeneity and the
dependence on the user&apos;s position within a session. Based on this analysis, we
define the problem as determining the optimal ad-load for a particular feed
fetch. To tackle this problem, we propose an off-policy learning framework that
leverages unbiased estimators such as Inverse Propensity Scoring (IPS) and
Doubly Robust (DR) to learn and estimate the policy values using offline
collected stochastic data. We present insights from online A/B experiments
deployed at scale across over 80 million users generating over 200 million
sessions, where we find statistically significant improvements in both user
satisfaction metrics and ads revenue for the platform.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sagtani_H/0/1/0/all/0/1&quot;&gt;Hitesh Sagtani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jhawar_M/0/1/0/all/0/1&quot;&gt;Madan Jhawar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehrotra_R/0/1/0/all/0/1&quot;&gt;Rishabh Mehrotra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeunen_O/0/1/0/all/0/1&quot;&gt;Olivier Jeunen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11651">
<title>Drift Control of High-Dimensional RBM: A Computational Method Based on Neural Networks. (arXiv:2309.11651v2 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2309.11651</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivated by applications in queueing theory, we consider a stochastic
control problem whose state space is the $d$-dimensional positive orthant. The
controlled process $Z$ evolves as a reflected Brownian motion whose covariance
matrix is exogenously specified, as are its directions of reflection from the
orthant&apos;s boundary surfaces. A system manager chooses a drift vector
$\theta(t)$ at each time $t$ based on the history of $Z$, and the cost rate at
time $t$ depends on both $Z(t)$ and $\theta(t)$. In our initial problem
formulation, the objective is to minimize expected discounted cost over an
infinite planning horizon, after which we treat the corresponding ergodic
control problem. Extending earlier work by Han et al. (Proceedings of the
National Academy of Sciences, 2018, 8505-8510), we develop and illustrate a
simulation-based computational method that relies heavily on deep neural
network technology. For test problems studied thus far, our method is accurate
to within a fraction of one percent, and is computationally feasible in
dimensions up to at least $d=30$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ata_B/0/1/0/all/0/1&quot;&gt;Baris Ata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Harrison_J/0/1/0/all/0/1&quot;&gt;J. Michael Harrison&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Si_N/0/1/0/all/0/1&quot;&gt;Nian Si&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12211">
<title>Physics-informed State-space Neural Networks for Transport Phenomena. (arXiv:2309.12211v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.12211</link>
<description rdf:parseType="Literal">&lt;p&gt;This work introduces Physics-informed State-space neural network Models
(PSMs), a novel solution to achieving real-time optimization, flexibility, and
fault tolerance in autonomous systems, particularly in transport-dominated
systems such as chemical, biomedical, and power plants. Traditional data-driven
methods fall short due to a lack of physical constraints like mass
conservation; PSMs address this issue by training deep neural networks with
sensor data and physics-informing using components&apos; Partial Differential
Equations (PDEs), resulting in a physics-constrained, end-to-end differentiable
forward dynamics model. Through two in silico experiments -- a heated channel
and a cooling system loop -- we demonstrate that PSMs offer a more accurate
approach than a purely data-driven model. In the former experiment, PSMs
demonstrated significantly lower average root-mean-square errors across test
datasets compared to a purely data-driven neural network, with reductions of 44
%, 48 %, and 94 % in predicting pressure, velocity, and temperature,
respectively.
&lt;/p&gt;
&lt;p&gt;Beyond accuracy, PSMs demonstrate a compelling multitask capability, making
them highly versatile. In this work, we showcase two: supervisory control of a
nonlinear system through a sequentially updated state-space representation and
the proposal of a diagnostic algorithm using residuals from each of the PDEs.
The former demonstrates PSMs&apos; ability to handle constant and time-dependent
constraints, while the latter illustrates their value in system diagnostics and
fault detection. We further posit that PSMs could serve as a foundation for
Digital Twins, constantly updated digital representations of physical systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dave_A/0/1/0/all/0/1&quot;&gt;Akshay J. Dave&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vilim_R/0/1/0/all/0/1&quot;&gt;Richard B. Vilim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15188">
<title>ICML 2023 Topological Deep Learning Challenge : Design and Results. (arXiv:2309.15188v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.15188</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents the computational challenge on topological deep learning
that was hosted within the ICML 2023 Workshop on Topology and Geometry in
Machine Learning. The competition asked participants to provide open-source
implementations of topological neural networks from the literature by
contributing to the python packages TopoNetX (data processing) and TopoModelX
(deep learning). The challenge attracted twenty-eight qualifying submissions in
its two-month duration. This paper describes the design of the challenge and
summarizes its main findings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papillon_M/0/1/0/all/0/1&quot;&gt;Mathilde Papillon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hajij_M/0/1/0/all/0/1&quot;&gt;Mustafa Hajij&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jenne_H/0/1/0/all/0/1&quot;&gt;Helen Jenne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mathe_J/0/1/0/all/0/1&quot;&gt;Johan Mathe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Myers_A/0/1/0/all/0/1&quot;&gt;Audun Myers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papamarkou_T/0/1/0/all/0/1&quot;&gt;Theodore Papamarkou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zamzmi_G/0/1/0/all/0/1&quot;&gt;Ghada Zamzmi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Birdal_T/0/1/0/all/0/1&quot;&gt;Tolga Birdal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dey_T/0/1/0/all/0/1&quot;&gt;Tamal Dey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doster_T/0/1/0/all/0/1&quot;&gt;Tim Doster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emerson_T/0/1/0/all/0/1&quot;&gt;Tegan Emerson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gopalakrishnan_G/0/1/0/all/0/1&quot;&gt;Gurusankar Gopalakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Govil_D/0/1/0/all/0/1&quot;&gt;Devendra Govil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guzman_Saenz_A/0/1/0/all/0/1&quot;&gt;Aldo Guzm&amp;#xe1;n-S&amp;#xe1;enz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kvinge_H/0/1/0/all/0/1&quot;&gt;Henry Kvinge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Livesay_N/0/1/0/all/0/1&quot;&gt;Neal Livesay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1&quot;&gt;Soham Mukherjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samaga_S/0/1/0/all/0/1&quot;&gt;Shreyas N. Samaga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramamurthy_K/0/1/0/all/0/1&quot;&gt;Karthikeyan Natesan Ramamurthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karri_M/0/1/0/all/0/1&quot;&gt;Maneel Reddy Karri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosen_P/0/1/0/all/0/1&quot;&gt;Paul Rosen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanborn_S/0/1/0/all/0/1&quot;&gt;Sophia Sanborn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walters_R/0/1/0/all/0/1&quot;&gt;Robin Walters&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agerberg_J/0/1/0/all/0/1&quot;&gt;Jens Agerberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barikbin_S/0/1/0/all/0/1&quot;&gt;Sadrodin Barikbin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Battiloro_C/0/1/0/all/0/1&quot;&gt;Claudio Battiloro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bazhenov_G/0/1/0/all/0/1&quot;&gt;Gleb Bazhenov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernardez_G/0/1/0/all/0/1&quot;&gt;Guillermo Bernardez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brent_A/0/1/0/all/0/1&quot;&gt;Aiden Brent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1&quot;&gt;Sergio Escalera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fiorellino_S/0/1/0/all/0/1&quot;&gt;Simone Fiorellino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gavrilev_D/0/1/0/all/0/1&quot;&gt;Dmitrii Gavrilev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassanin_M/0/1/0/all/0/1&quot;&gt;Mohammed Hassanin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hausner_P/0/1/0/all/0/1&quot;&gt;Paul H&amp;#xe4;usner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gardaa_O/0/1/0/all/0/1&quot;&gt;Odin Hoff Gardaa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khamis_A/0/1/0/all/0/1&quot;&gt;Abdelwahed Khamis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lecha_M/0/1/0/all/0/1&quot;&gt;Manuel Lecha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magai_G/0/1/0/all/0/1&quot;&gt;German Magai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malygina_T/0/1/0/all/0/1&quot;&gt;Tatiana Malygina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ballester_R/0/1/0/all/0/1&quot;&gt;Rub&amp;#xe9;n Ballester&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nadimpalli_K/0/1/0/all/0/1&quot;&gt;Kalyan Nadimpalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikitin_A/0/1/0/all/0/1&quot;&gt;Alexander Nikitin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rabinowitz_A/0/1/0/all/0/1&quot;&gt;Abraham Rabinowitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salatiello_A/0/1/0/all/0/1&quot;&gt;Alessandro Salatiello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scardapane_S/0/1/0/all/0/1&quot;&gt;Simone Scardapane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scofano_L/0/1/0/all/0/1&quot;&gt;Luca Scofano&lt;/a&gt;, et al. (11 additional authors not shown)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15289">
<title>SEPT: Towards Efficient Scene Representation Learning for Motion Prediction. (arXiv:2309.15289v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.15289</link>
<description rdf:parseType="Literal">&lt;p&gt;Motion prediction is crucial for autonomous vehicles to operate safely in
complex traffic environments. Extracting effective spatiotemporal relationships
among traffic elements is key to accurate forecasting. Inspired by the
successful practice of pretrained large language models, this paper presents
SEPT, a modeling framework that leverages self-supervised learning to develop
powerful spatiotemporal understanding for complex traffic scenes. Specifically,
our approach involves three masking-reconstruction modeling tasks on scene
inputs including agents&apos; trajectories and road network, pretraining the scene
encoder to capture kinematics within trajectory, spatial structure of road
network, and interactions among roads and agents. The pretrained encoder is
then finetuned on the downstream forecasting task. Extensive experiments
demonstrate that SEPT, without elaborate architectural design or manual feature
engineering, achieves state-of-the-art performance on the Argoverse 1 and
Argoverse 2 motion forecasting benchmarks, outperforming previous methods on
all main metrics by a large margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1&quot;&gt;Zhiqian Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1&quot;&gt;Yao Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shengbo Eben Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00149">
<title>One for All: Towards Training One Graph Model for All Classification Tasks. (arXiv:2310.00149v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00149</link>
<description rdf:parseType="Literal">&lt;p&gt;Designing a single model to address multiple tasks has been a long-standing
objective in artificial intelligence. Recently, large language models have
demonstrated exceptional capability in solving different tasks within the
language domain. However, a unified model for various graph tasks remains
underexplored, primarily due to the challenges unique to the graph learning
domain. First, graph data from different areas carry distinct attributes and
follow different distributions. Such discrepancy makes it hard to represent
graphs in a single representation space. Second, tasks on graphs diversify into
node, link, and graph tasks, requiring distinct embedding strategies. Finally,
an appropriate graph prompting paradigm for in-context learning is unclear. We
propose \textbf{One for All (OFA)}, the first general framework that can use a
single graph model to address the above challenges. Specifically, OFA proposes
text-attributed graphs to unify different graph data by describing nodes and
edges with natural language and uses language models to encode the diverse and
possibly cross-domain text attributes to feature vectors in the same embedding
space. Furthermore, OFA introduces the concept of nodes-of-interest to
standardize different tasks with a single task representation. For in-context
learning on graphs, OFA introduces a novel graph prompting paradigm that
appends prompting substructures to the input graph, which enables it to address
varied tasks without fine-tuning. We train the OFA model using graph data from
multiple domains (including citation networks, molecular graphs, knowledge
graphs, etc.) simultaneously and evaluate its ability in supervised, few-shot,
and zero-shot learning scenarios. OFA performs well across different tasks,
making it the first general-purpose across-domains classification model on
graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jiarui Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1&quot;&gt;Lecheng Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_N/0/1/0/all/0/1&quot;&gt;Ningyue Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yixin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Muhan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00757">
<title>Mind the Gap: Federated Learning Broadens Domain Generalization in Diagnostic AI Models. (arXiv:2310.00757v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00757</link>
<description rdf:parseType="Literal">&lt;p&gt;Developing robust artificial intelligence (AI) models that generalize well to
unseen datasets is challenging and usually requires large and variable
datasets, preferably from multiple institutions. In federated learning (FL), a
model is trained collaboratively at numerous sites that hold local datasets
without exchanging them. So far, the impact of training strategy, i.e., local
versus collaborative, on the diagnostic on-domain and off-domain performance of
AI models interpreting chest radiographs has not been assessed. Consequently,
using 610,000 chest radiographs from five institutions across the globe, we
assessed diagnostic performance as a function of training strategy (i.e., local
vs. collaborative), network architecture (i.e., convolutional vs.
transformer-based), generalization performance (i.e., on-domain vs.
off-domain), imaging finding (i.e., cardiomegaly, pleural effusion, pneumonia,
atelectasis, consolidation, pneumothorax, and no abnormality), dataset size
(i.e., from n=18,000 to 213,921 radiographs), and dataset diversity. Large
datasets not only showed minimal performance gains with FL but, in some
instances, even exhibited decreases. In contrast, smaller datasets revealed
marked improvements. Thus, on-domain performance was mainly driven by training
data size. However, off-domain performance leaned more on training diversity.
When trained collaboratively across diverse external institutions, AI models
consistently surpassed models trained locally for off-domain tasks, emphasizing
FL&apos;s potential in leveraging data diversity. In conclusion, FL can bolster
diagnostic privacy, reproducibility, and off-domain reliability of AI models
and, potentially, optimize healthcare outcomes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arasteh_S/0/1/0/all/0/1&quot;&gt;Soroosh Tayebi Arasteh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuhl_C/0/1/0/all/0/1&quot;&gt;Christiane Kuhl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saehn_M/0/1/0/all/0/1&quot;&gt;Marwin-Jonathan Saehn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isfort_P/0/1/0/all/0/1&quot;&gt;Peter Isfort&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Truhn_D/0/1/0/all/0/1&quot;&gt;Daniel Truhn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nebelung_S/0/1/0/all/0/1&quot;&gt;Sven Nebelung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02651">
<title>Hire When You Need to: Gradual Participant Recruitment for Auction-based Federated Learning. (arXiv:2310.02651v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02651</link>
<description rdf:parseType="Literal">&lt;p&gt;The success of Federated Learning (FL) depends on the quantity and quality of
the data owners (DOs) as well as their motivation to join FL model training.
Reputation-based FL participant selection methods have been proposed. However,
they still face the challenges of the cold start problem and potential
selection bias towards highly reputable DOs. Such a bias can result in lower
reputation DOs being prematurely excluded from future FL training rounds,
thereby reducing the diversity of training data and the generalizability of the
resulting models. To address these challenges, we propose the Gradual
Participant Selection scheme for Auction-based Federated Learning (GPS-AFL).
Unlike existing AFL incentive mechanisms which generally assume that all DOs
required for an FL task must be selected in one go, GPS-AFL gradually selects
the required DOs over multiple rounds of training as more information is
revealed through repeated interactions. It is designed to strike a balance
between cost saving and performance enhancement, while mitigating the drawbacks
of selection bias in reputation-based FL. Extensive experiments based on
real-world datasets demonstrate the significant advantages of GPS-AFL, which
reduces costs by 33.65% and improved total utility by 2.91%, on average
compared to the best-performing state-of-the-art approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1&quot;&gt;Xavier Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Han Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05161">
<title>Recurrent Neural Language Models as Probabilistic Finite-state Automata. (arXiv:2310.05161v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05161</link>
<description rdf:parseType="Literal">&lt;p&gt;Studying language models (LMs) in terms of well-understood formalisms allows
us to precisely characterize their abilities and limitations. Previous work has
investigated the representational capacity of recurrent neural network (RNN)
LMs in terms of their capacity to recognize unweighted formal languages.
However, LMs do not describe unweighted formal languages -- rather, they define
\emph{probability distributions} over strings. In this work, we study what
classes of such probability distributions RNN LMs can represent, which allows
us to make more direct statements about their capabilities. We show that simple
RNNs are equivalent to a subclass of probabilistic finite-state automata, and
can thus model a strict subset of probability distributions expressible by
finite-state models. Furthermore, we study the space complexity of representing
finite-state LMs with RNNs. We show that, to represent an arbitrary
deterministic finite-state LM with $N$ states over an alphabet $\alphabet$, an
RNN requires $\Omega\left(N |\Sigma|\right)$ neurons. These results present a
first step towards characterizing the classes of distributions RNN LMs can
represent and thus help us understand their capabilities and limitations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Svete_A/0/1/0/all/0/1&quot;&gt;Anej Svete&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1&quot;&gt;Ryan Cotterell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.06009">
<title>Divide-and-Conquer Dynamics in AI-Driven Disempowerment. (arXiv:2310.06009v2 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2310.06009</link>
<description rdf:parseType="Literal">&lt;p&gt;AI companies are attempting to create AI systems that outperform humans at
most economically valuable work. Current AI models are already automating away
the livelihoods of some artists, actors, and writers. But there is infighting
between those who prioritize current harms and future harms. We construct a
game-theoretic model of conflict to study the causes and consequences of this
disunity. Our model also helps explain why throughout history, stakeholders
sharing a common threat have found it advantageous to unite against it, and why
the common threat has in turn found it advantageous to divide and conquer.
&lt;/p&gt;
&lt;p&gt;Under realistic parameter assumptions, our model makes several predictions
that find preliminary corroboration in the historical-empirical record. First,
current victims of AI-driven disempowerment need the future victims to realize
that their interests are also under serious and imminent threat, so that future
victims are incentivized to support current victims in solidarity. Second, the
movement against AI-driven disempowerment can become more united, and thereby
more likely to prevail, if members believe that their efforts will be
successful as opposed to futile. Finally, the movement can better unite and
prevail if its members are less myopic. Myopic members prioritize their future
well-being less than their present well-being, and are thus disinclined to
solidarily support current victims today at personal cost, even if this is
necessary to counter the shared threat of AI-driven disempowerment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_P/0/1/0/all/0/1&quot;&gt;Peter S. Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tegmark_M/0/1/0/all/0/1&quot;&gt;Max Tegmark&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15516">
<title>Graph Attention-based Deep Reinforcement Learning for solving the Chinese Postman Problem with Load-dependent costs. (arXiv:2310.15516v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15516</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, Deep reinforcement learning (DRL) models have shown promising
results in solving routing problems. However, most DRL solvers are commonly
proposed to solve node routing problems, such as the Traveling Salesman Problem
(TSP). Meanwhile, there has been limited research on applying neural methods to
arc routing problems, such as the Chinese Postman Problem (CPP), since they
often feature irregular and complex solution spaces compared to TSP. To fill
these gaps, this paper proposes a novel DRL framework to address the CPP with
load-dependent costs (CPP-LC) (Corberan et al., 2018), which is a complex arc
routing problem with load constraints. The novelty of our method is two-fold.
First, we formulate the CPP-LC as a Markov Decision Process (MDP) sequential
model. Subsequently, we introduce an autoregressive model based on DRL, namely
Arc-DRL, consisting of an encoder and decoder to address the CPP-LC challenge
effectively. Such a framework allows the DRL model to work efficiently and
scalably to arc routing problems. Furthermore, we propose a new bio-inspired
meta-heuristic solution based on Evolutionary Algorithm (EA) for CPP-LC.
Extensive experiments show that Arc-DRL outperforms existing meta-heuristic
methods such as Iterative Local Search (ILS) and Variable Neighborhood Search
(VNS) proposed by (Corberan et al., 2018) on large benchmark datasets for
CPP-LC regarding both solution quality and running time; while the EA gives the
best solution quality with much more running time. We release our C++
implementations for metaheuristics such as EA, ILS and VNS along with the code
for data generation and our generated data at
https://github.com/HySonLab/Chinese_Postman_Problem
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_C/0/1/0/all/0/1&quot;&gt;Cong Dao Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hy_T/0/1/0/all/0/1&quot;&gt;Truong Son Hy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17658">
<title>Is Channel Independent strategy optimal for Time Series Forecasting?. (arXiv:2310.17658v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.17658</link>
<description rdf:parseType="Literal">&lt;p&gt;There has been an emergence of various models for long-term time series
forecasting. Recent studies have demonstrated that a single linear layer, using
Channel Dependent (CD) or Channel Independent (CI) modeling, can even
outperform a large number of sophisticated models. However, current research
primarily considers CD and CI as two complementary yet mutually exclusive
approaches, unable to harness these two extremes simultaneously. And it is also
a challenging issue that both CD and CI are static strategies that cannot be
determined to be optimal for a specific dataset without extensive experiments.
In this paper, we reconsider whether the current CI strategy is the best
solution for time series forecasting. First, we propose a simple yet effective
strategy called CSC, which stands for $\mathbf{C}$hannel
$\mathbf{S}$elf-$\mathbf{C}$lustering strategy, for linear models. Our Channel
Self-Clustering (CSC) enhances CI strategy&apos;s performance improvements while
reducing parameter size, for exmpale by over 10 times on electricity dataset,
and significantly cutting training time. Second, we further propose Channel
Rearrangement (CR), a method for deep models inspired by the self-clustering.
CR attains competitive performance against baselines. Finally, we also discuss
whether it is best to forecast the future values using the historical values of
the same channel as inputs. We hope our findings and methods could inspire new
solutions beyond CD/CI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peiwen_Y/0/1/0/all/0/1&quot;&gt;Yuan Peiwen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Changsheng_Z/0/1/0/all/0/1&quot;&gt;Zhu Changsheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18313">
<title>FP8-LM: Training FP8 Large Language Models. (arXiv:2310.18313v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18313</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we explore FP8 low-bit data formats for efficient training of
large language models (LLMs). Our key insight is that most variables, such as
gradients and optimizer states, in LLM training can employ low-precision data
formats without compromising model accuracy and requiring no changes to
hyper-parameters. Specifically, we propose a new FP8 automatic mixed-precision
framework for training LLMs. This framework offers three levels of FP8
utilization to streamline mixed-precision and distributed parallel training for
LLMs. It gradually incorporates 8-bit gradients, optimizer states, and
distributed learning in an incremental manner. Experiment results show that,
during the training of GPT-175B model on H100 GPU platform, our FP8
mixed-precision training framework not only achieved a remarkable 39% reduction
in real memory usage but also ran 75% faster than the widely adopted BF16
framework (i.e., Megatron-LM), surpassing the speed of Nvidia Transformer
Engine by 37%. This largely reduces the training costs for large foundation
models. Furthermore, our FP8 mixed-precision training methodology is generic.
It can be seamlessly applied to other tasks such as LLM instruction tuning and
reinforcement learning with human feedback, offering savings in fine-tuning
expenses. Our FP8 low-precision training framework is open-sourced at
{https://github.com/Azure/MS-AMP}{aka.ms/MS.AMP}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1&quot;&gt;Houwen Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1&quot;&gt;Kan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yixuan Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1&quot;&gt;Guoshuai Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuxiang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ze Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1&quot;&gt;Yifan Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Ziyue Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1&quot;&gt;Bolin Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jingcheng Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ruihang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Miaosen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_J/0/1/0/all/0/1&quot;&gt;Jia Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruizhe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shuguang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chau_J/0/1/0/all/0/1&quot;&gt;Joe Chau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Han Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1&quot;&gt;Peng Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02775">
<title>AI-TA: Towards an Intelligent Question-Answer Teaching Assistant using Open-Source LLMs. (arXiv:2311.02775v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02775</link>
<description rdf:parseType="Literal">&lt;p&gt;Responding to the thousands of student questions on online QA platforms each
semester has a considerable human cost, particularly in computing courses with
rapidly growing enrollments. To address the challenges of scalable and
intelligent question-answering (QA), we introduce an innovative solution that
leverages open-source Large Language Models (LLMs) from the LLaMA-2 family to
ensure data privacy. Our approach combines augmentation techniques such as
retrieval augmented generation (RAG), supervised fine-tuning (SFT), and
learning from human preferences data using Direct Preference Optimization
(DPO). Through extensive experimentation on a Piazza dataset from an
introductory CS course, comprising 10,000 QA pairs and 1,500 pairs of
preference data, we demonstrate a significant 30% improvement in the quality of
answers, with RAG being a particularly impactful addition. Our contributions
include the development of a novel architecture for educational QA, extensive
evaluations of LLM performance utilizing both human assessments and LLM-based
metrics, and insights into the challenges and future directions of educational
data processing. This work paves the way for the development of AI-TA, an
intelligent QA assistant customizable for courses with an online QA platform
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hicke_Y/0/1/0/all/0/1&quot;&gt;Yann Hicke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1&quot;&gt;Anmol Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1&quot;&gt;Qianou Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denny_P/0/1/0/all/0/1&quot;&gt;Paul Denny&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03498">
<title>In-Context Exemplars as Clues to Retrieving from Large Associative Memory. (arXiv:2311.03498v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03498</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, large language models (LLMs) have made remarkable progress in
natural language processing. The most representative ability of LLMs is
in-context learning (ICL), which enables LLMs to learn patterns from in-context
exemplars without training. The performance of ICL greatly depends on the
exemplars used. However, how to choose exemplars remains unclear due to the
lack of understanding of how in-context learning works. In this paper, we
present a novel perspective on ICL by conceptualizing it as contextual
retrieval from a model of associative memory. We establish a theoretical
framework of ICL based on Hopfield Networks. Based on our framework, we look
into how in-context exemplars influence the performance of ICL and propose more
efficient active exemplar selection. Our study sheds new light on the mechanism
of ICL by connecting it to memory retrieval, with potential implications for
advancing the understanding of LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jiachen Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05144">
<title>Counter-Empirical Attacking based on Adversarial Reinforcement Learning for Time-Relevant Scoring System. (arXiv:2311.05144v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.05144</link>
<description rdf:parseType="Literal">&lt;p&gt;Scoring systems are commonly seen for platforms in the era of big data. From
credit scoring systems in financial services to membership scores in E-commerce
shopping platforms, platform managers use such systems to guide users towards
the encouraged activity pattern, and manage resources more effectively and more
efficiently thereby. To establish such scoring systems, several &quot;empirical
criteria&quot; are firstly determined, followed by dedicated top-down design for
each factor of the score, which usually requires enormous effort to adjust and
tune the scoring function in the new application scenario. What&apos;s worse, many
fresh projects usually have no ground-truth or any experience to evaluate a
reasonable scoring system, making the designing even harder. To reduce the
effort of manual adjustment of the scoring function in every new scoring
system, we innovatively study the scoring system from the preset empirical
criteria without any ground truth, and propose a novel framework to improve the
system from scratch. In this paper, we propose a &quot;counter-empirical attacking&quot;
mechanism that can generate &quot;attacking&quot; behavior traces and try to break the
empirical rules of the scoring system. Then an adversarial &quot;enhancer&quot; is
applied to evaluate the scoring system and find the improvement strategy. By
training the adversarial learning problem, a proper scoring function can be
learned to be robust to the attacking activity traces that are trying to
violate the empirical criteria. Extensive experiments have been conducted on
two scoring systems including a shared computing resource platform and a
financial credit system. The experimental results have validated the
effectiveness of our proposed framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xiangguo Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1&quot;&gt;Hong Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hang Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_B/0/1/0/all/0/1&quot;&gt;Bo Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_S/0/1/0/all/0/1&quot;&gt;Si Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1&quot;&gt;Qingwei Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05587">
<title>Bayesian Methods for Media Mix Modelling with shape and funnel effects. (arXiv:2311.05587v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.05587</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, significant progress in generative AI has highlighted the
important role of physics-inspired models that utilize advanced mathematical
concepts based on fundamental physics principles to enhance artificial
intelligence capabilities. Among these models, those based on diffusion
equations have greatly improved image quality. This study aims to explore the
potential uses of Maxwell-Boltzmann equation, which forms the basis of the
kinetic theory of gases, and the Michaelis-Menten model in Marketing Mix
Modelling (MMM) applications. We propose incorporating these equations into
Hierarchical Bayesian models to analyse consumer behaviour in the context of
advertising. These equation sets excel in accurately describing the random
dynamics in complex systems like social interactions and consumer-advertising
interactions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marin_J/0/1/0/all/0/1&quot;&gt;Javier Marin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06281">
<title>Efficient Parallelization of a Ubiquitous Sequential Computation. (arXiv:2311.06281v3 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/2311.06281</link>
<description rdf:parseType="Literal">&lt;p&gt;We find a succinct expression for computing the sequence $x_t = a_t x_{t-1} +
b_t$ in parallel with two prefix sums, given $t = (1, 2, \dots, n)$, $a_t \in
\mathbb{R}^n$, $b_t \in \mathbb{R}^n$, and initial value $x_0 \in \mathbb{R}$.
On $n$ parallel processors, the computation of $n$ elements incurs
$\mathcal{O}(\log n)$ time and $\mathcal{O}(n)$ space. Sequences of this form
are ubiquitous in science and engineering, making efficient parallelization
useful for a vast number of applications. We implement our expression in
software, test it on parallel hardware, and verify that it executes faster than
sequential computation by a factor of $\frac{n}{\log n}$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heinsen_F/0/1/0/all/0/1&quot;&gt;Franz A. Heinsen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09506">
<title>Investigating the Impact of Weight Sharing Decisions on Knowledge Transfer in Continual Learning. (arXiv:2311.09506v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09506</link>
<description rdf:parseType="Literal">&lt;p&gt;Continual Learning (CL) has generated attention as a method of avoiding
Catastrophic Forgetting (CF) in the sequential training of neural networks,
improving network efficiency and adaptability to different tasks. Additionally,
CL serves as an ideal setting for studying network behavior and Forward
Knowledge Transfer (FKT) between tasks. Pruning methods for CL train
subnetworks to handle the sequential tasks which allows us to take a structured
approach to investigating FKT. Sharing prior subnetworks&apos; weights leverages
past knowledge for the current task through FKT. Understanding which weights to
share is important as sharing all weights can yield sub-optimal accuracy. This
paper investigates how different sharing decisions affect the FKT between
tasks. Through this lens we demonstrate how task complexity and similarity
influence the optimal weight sharing decisions, giving insights into the
relationships between tasks and helping inform decision making in similar CL
methods. We implement three sequential datasets designed to emphasize variation
in task complexity and similarity, reporting results for both ResNet-18 and
VGG-16. By sharing in accordance with the decisions supported by our findings,
we show that we can improve task accuracy compared to other sharing decisions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andle_J/0/1/0/all/0/1&quot;&gt;Josh Andle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Payani_A/0/1/0/all/0/1&quot;&gt;Ali Payani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yasaei_Sekeh_S/0/1/0/all/0/1&quot;&gt;Salimeh Yasaei-Sekeh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10090">
<title>JaxMARL: Multi-Agent RL Environments in JAX. (arXiv:2311.10090v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10090</link>
<description rdf:parseType="Literal">&lt;p&gt;Benchmarks play an important role in the development of machine learning
algorithms. For example, research in reinforcement learning (RL) has been
heavily influenced by available environments and benchmarks. However, RL
environments are traditionally run on the CPU, limiting their scalability with
typical academic compute. Recent advancements in JAX have enabled the wider use
of hardware acceleration to overcome these computational hurdles, enabling
massively parallel RL training pipelines and environments. This is particularly
useful for multi-agent reinforcement learning (MARL) research. First of all,
multiple agents must be considered at each environment step, adding
computational burden, and secondly, the sample complexity is increased due to
non-stationarity, decentralised partial observability, or other MARL
challenges. In this paper, we present JaxMARL, the first open-source code base
that combines ease-of-use with GPU enabled efficiency, and supports a large
number of commonly used MARL environments as well as popular baseline
algorithms. When considering wall clock time, our experiments show that per-run
our JAX-based training pipeline is up to 12500x faster than existing
approaches. This enables efficient and thorough evaluations, with the potential
to alleviate the evaluation crisis of the field. We also introduce and
benchmark SMAX, a vectorised, simplified version of the popular StarCraft
Multi-Agent Challenge, which removes the need to run the StarCraft II game
engine. This not only enables GPU acceleration, but also provides a more
flexible MARL environment, unlocking the potential for self-play,
meta-learning, and other future applications in MARL. We provide code at
https://github.com/flairox/jaxmarl.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rutherford_A/0/1/0/all/0/1&quot;&gt;Alexander Rutherford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ellis_B/0/1/0/all/0/1&quot;&gt;Benjamin Ellis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gallici_M/0/1/0/all/0/1&quot;&gt;Matteo Gallici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cook_J/0/1/0/all/0/1&quot;&gt;Jonathan Cook&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lupu_A/0/1/0/all/0/1&quot;&gt;Andrei Lupu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ingvarsson_G/0/1/0/all/0/1&quot;&gt;Gardar Ingvarsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Willi_T/0/1/0/all/0/1&quot;&gt;Timon Willi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Akbir Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Witt_C/0/1/0/all/0/1&quot;&gt;Christian Schroeder de Witt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Souly_A/0/1/0/all/0/1&quot;&gt;Alexandra Souly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bandyopadhyay_S/0/1/0/all/0/1&quot;&gt;Saptarashmi Bandyopadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samvelyan_M/0/1/0/all/0/1&quot;&gt;Mikayel Samvelyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1&quot;&gt;Minqi Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lange_R/0/1/0/all/0/1&quot;&gt;Robert Tjarko Lange&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1&quot;&gt;Shimon Whiteson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lacerda_B/0/1/0/all/0/1&quot;&gt;Bruno Lacerda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hawes_N/0/1/0/all/0/1&quot;&gt;Nick Hawes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1&quot;&gt;Tim Rocktaschel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Chris Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1&quot;&gt;Jakob Nicolaus Foerster&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14743">
<title>A Baseline Analysis of Reward Models&apos; Ability To Accurately Analyze Foundation Models Under Distribution Shift. (arXiv:2311.14743v5 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.14743</link>
<description rdf:parseType="Literal">&lt;p&gt;Foundation models, specifically Large Language Models (LLM&apos;s), have lately
gained wide-spread attention and adoption. Reinforcement Learning with Human
Feedback (RLHF) involves training a reward model to capture desired behaviors,
which is then used to align LLM&apos;s. These reward models are additionally used at
inference-time to estimate LLM responses&apos; adherence to those desired behaviors.
However, there is little work measuring how robust these reward models are to
distribution shifts. In this work, we evaluate how reward model performance -
measured via accuracy and calibration (i.e. alignment between accuracy and
confidence) - is affected by distribution shift. We show novel calibration
patterns and accuracy drops due to OOD prompts and responses, and that the
reward model is more sensitive to shifts in responses than prompts.
Additionally, we adapt an OOD detection technique commonly used in
classification to the reward model setting to detect these distribution shifts
in prompts and responses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+LeVine_W/0/1/0/all/0/1&quot;&gt;Will LeVine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pikus_B/0/1/0/all/0/1&quot;&gt;Ben Pikus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tony Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hendryx_S/0/1/0/all/0/1&quot;&gt;Sean Hendryx&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15570">
<title>UFDA: Universal Federated Domain Adaptation with Practical Assumptions. (arXiv:2311.15570v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.15570</link>
<description rdf:parseType="Literal">&lt;p&gt;Conventional Federated Domain Adaptation (FDA) approaches usually demand an
abundance of assumptions, which makes them significantly less feasible for
real-world situations and introduces security hazards. This paper relaxes the
assumptions from previous FDAs and studies a more practical scenario named
Universal Federated Domain Adaptation (UFDA). It only requires the black-box
model and the label set information of each source domain, while the label sets
of different source domains could be inconsistent, and the target-domain label
set is totally blind. Towards a more effective solution for our newly proposed
UFDA scenario, we propose a corresponding methodology called Hot-Learning with
Contrastive Label Disambiguation (HCLD). It particularly tackles UFDA&apos;s domain
shifts and category gaps problems by using one-hot outputs from the black-box
models of various source domains. Moreover, to better distinguish the shared
and unknown classes, we further present a cluster-level strategy named
Mutual-Voting Decision (MVD) to extract robust consensus knowledge across peer
classes from both source and target domains. Extensive experiments on three
benchmark datasets demonstrate that our method achieves comparable performance
for our UFDA scenario with much fewer assumptions, compared to previous
methodologies with comprehensive additional assumptions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinhui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhenghao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Luping Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Dong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xi_W/0/1/0/all/0/1&quot;&gt;Wei Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_G/0/1/0/all/0/1&quot;&gt;Gairui Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yihan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jizhong Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.18826">
<title>Geometry-Aware Normalizing Wasserstein Flows for Optimal Causal Inference. (arXiv:2311.18826v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.18826</link>
<description rdf:parseType="Literal">&lt;p&gt;This manuscript enriches the framework of continuous normalizing flows (CNFs)
within causal inference, primarily to augment the geometric properties of
parametric submodels used in targeted maximum likelihood estimation (TMLE). By
introducing an innovative application of CNFs, we construct a refined series of
parametric submodels that enable a directed interpolation between the prior
distribution $p_0$ and the empirical distribution $p_1$. This proposed
methodology serves to optimize the semiparametric efficiency bound in causal
inference by orchestrating CNFs to align with Wasserstein gradient flows. Our
approach not only endeavors to minimize the mean squared error in the
estimation but also imbues the estimators with geometric sophistication,
thereby enhancing robustness against misspecification. This robustness is
crucial, as it alleviates the dependence on the standard $n^{\frac{1}{4}}$ rate
for a doubly-robust perturbation direction in TMLE. By incorporating robust
optimization principles and differential geometry into the estimators, the
developed geometry-aware CNFs represent a significant advancement in the
pursuit of doubly robust causal inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_K/0/1/0/all/0/1&quot;&gt;Kaiwen Hou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00038">
<title>A Posteriori Evaluation of a Physics-Constrained Neural Ordinary Differential Equations Approach Coupled with CFD Solver for Modeling Stiff Chemical Kinetics. (arXiv:2312.00038v2 [physics.comp-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2312.00038</link>
<description rdf:parseType="Literal">&lt;p&gt;The high computational cost associated with solving for detailed chemistry
poses a significant challenge for predictive computational fluid dynamics (CFD)
simulations of turbulent reacting flows. These models often require solving a
system of coupled stiff ordinary differential equations (ODEs). While deep
learning techniques have been experimented with to develop faster surrogate
models, they often fail to integrate reliably with CFD solvers. This
instability arises because deep learning methods optimize for training error
without ensuring compatibility with ODE solvers, leading to accumulation of
errors over time. Recently, NeuralODE-based techniques have offered a promising
solution by effectively modeling chemical kinetics. In this study, we extend
the NeuralODE framework for stiff chemical kinetics by incorporating mass
conservation constraints directly into the loss function during training. This
ensures that the total mass and the elemental mass are conserved, a critical
requirement for reliable downstream integration with CFD solvers.
Proof-of-concept studies are performed with physics-constrained neuralODE
(PC-NODE) approach for homogeneous autoignition of hydrogen-air mixture over a
range of composition and thermodynamic conditions. Our results demonstrate that
this enhancement not only improves the physical consistency with respect to
mass conservation criteria but also ensures better robustness. Lastly, a
posteriori studies are performed wherein the trained PC-NODE model is coupled
with a 3D CFD solver for computing the chemical source terms. PC-NODE is shown
to be more accurate relative to the purely data-driven neuralODE approach.
Moreover, PC-NODE also exhibits robustness and generalizability to unseen
initial conditions from within (interpolative capability) as well as outside
(extrapolative capability) the training regime.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kumar_T/0/1/0/all/0/1&quot;&gt;Tadbhagya Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Anuj Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Pal_P/0/1/0/all/0/1&quot;&gt;Pinaki Pal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00812">
<title>Empowering Autonomous Driving with Large Language Models: A Safety Perspective. (arXiv:2312.00812v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.00812</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous Driving (AD) faces crucial hurdles for commercial launch, notably
in the form of diminished public trust and safety concerns from long-tail
unforeseen driving scenarios. This predicament is due to the limitation of deep
neural networks in AD software, which struggle with interpretability and
exhibit poor generalization capabilities in out-of-distribution and uncertain
scenarios. To this end, this paper advocates for the integration of Large
Language Models (LLMs) into the AD system, leveraging their robust common-sense
knowledge, reasoning abilities, and human-interaction capabilities. The
proposed approach deploys the LLM as an intelligent decision-maker in planning,
incorporating safety verifiers for contextual safety learning to enhance
overall AD performance and safety. We present results from two case studies
that affirm the efficacy of our approach. We further discuss the potential
integration of LLM for other AD software components including perception,
prediction, and simulation. Despite the observed challenges in the case
studies, the integration of LLMs is promising and beneficial for reinforcing
both safety and performance in AD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yixuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_R/0/1/0/all/0/1&quot;&gt;Ruochen Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lang_C/0/1/0/all/0/1&quot;&gt;Chengtian Lang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_S/0/1/0/all/0/1&quot;&gt;Sinong Simon Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhaoran Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhuoran Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1&quot;&gt;Qi Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05571">
<title>Frugal LMs Trained to Invoke Symbolic Solvers Achieve Parameter-Efficient Arithmetic Reasoning. (arXiv:2312.05571v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.05571</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLM) exhibit zero-shot mathematical reasoning capacity
as a behavior emergent with scale, commonly manifesting as chain-of-thoughts
(CoT) reasoning. However, multiple empirical findings suggest that this prowess
is exclusive to LLMs with exorbitant sizes (beyond 50 billion parameters).
Meanwhile, educational neuroscientists suggest that symbolic algebraic
manipulation be introduced around the same time as arithmetic word problems to
modularize language-to-formulation, symbolic manipulation of the formulation,
and endgame arithmetic. In this paper, we start with the hypothesis that much
smaller LMs, which are weak at multi-step reasoning, can achieve reasonable
arithmetic reasoning if arithmetic word problems are posed as a
formalize-then-solve task. In our architecture, which we call SYRELM, the LM
serves the role of a translator to map natural language arithmetic questions
into a formal language (FL) description. A symbolic solver then evaluates the
FL expression to obtain the answer. A small frozen LM, equipped with an
efficient low-rank adapter, is capable of generating FL expressions that
incorporate natural language descriptions of the arithmetic problem (e.g.,
variable names and their purposes, formal expressions combining variables,
etc.). We adopt policy-gradient reinforcement learning to train the adapted LM,
informed by the non-differentiable symbolic solver. This marks a sharp
departure from the recent development in tool-augmented LLMs, in which the
external tools (e.g., calculator, Web search, etc.) are essentially detached
from the learning phase of the LM. SYRELM shows massive improvements (e.g.,
+30.65 absolute point improvement in accuracy on the SVAMP dataset using GPT-J
6B model) over base LMs, while keeping our testbed easy to diagnose, interpret
and within reach of most researchers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1&quot;&gt;Subhabrata Dutta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1&quot;&gt;Joykirat Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandey_I/0/1/0/all/0/1&quot;&gt;Ishan Pandey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manchanda_S/0/1/0/all/0/1&quot;&gt;Sunny Manchanda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakrabarti_S/0/1/0/all/0/1&quot;&gt;Soumen Chakrabarti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1&quot;&gt;Tanmoy Chakraborty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08528">
<title>auto-sktime: Automated Time Series Forecasting. (arXiv:2312.08528v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08528</link>
<description rdf:parseType="Literal">&lt;p&gt;In today&apos;s data-driven landscape, time series forecasting is pivotal in
decision-making across various sectors. Yet, the proliferation of more diverse
time series data, coupled with the expanding landscape of available forecasting
methods, poses significant challenges for forecasters. To meet the growing
demand for efficient forecasting, we introduce auto-sktime, a novel framework
for automated time series forecasting. The proposed framework uses the power of
automated machine learning (AutoML) techniques to automate the creation of the
entire forecasting pipeline. The framework employs Bayesian optimization, to
automatically construct pipelines from statistical, machine learning (ML) and
deep neural network (DNN) models. Furthermore, we propose three essential
improvements to adapt AutoML to time series data: First, pipeline templates to
account for the different supported forecasting models. Second, a novel
warm-starting technique to start the optimization from prior optimization runs.
Third, we adapt multi-fidelity optimizations to make them applicable to a
search space containing statistical, ML and DNN models. Experimental results on
64 diverse real-world time series datasets demonstrate the effectiveness and
efficiency of the framework, outperforming traditional methods while requiring
minimal human involvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zoller_M/0/1/0/all/0/1&quot;&gt;Marc-Andr&amp;#xe9; Z&amp;#xf6;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lindauer_M/0/1/0/all/0/1&quot;&gt;Marius Lindauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huber_M/0/1/0/all/0/1&quot;&gt;Marco F. Huber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08670">
<title>Temporal-Spatial Entropy Balancing for Causal Continuous Treatment-Effect Estimation. (arXiv:2312.08670v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08670</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of intracity freight transportation, changes in order volume are
significantly influenced by temporal and spatial factors. When building subsidy
and pricing strategies, predicting the causal effects of these strategies on
order volume is crucial. In the process of calculating causal effects,
confounding variables can have an impact. Traditional methods to control
confounding variables handle data from a holistic perspective, which cannot
ensure the precision of causal effects in specific temporal and spatial
dimensions. However, temporal and spatial dimensions are extremely critical in
the logistics field, and this limitation may directly affect the precision of
subsidy and pricing strategies. To address these issues, this study proposes a
technique based on flexible temporal-spatial grid partitioning. Furthermore,
based on the flexible grid partitioning technique, we further propose a
continuous entropy balancing method in the temporal-spatial domain, which named
TS-EBCT (Temporal-Spatial Entropy Balancing for Causal Continue Treatments).
The method proposed in this paper has been tested on two simulation datasets
and two real datasets, all of which have achieved excellent performance. In
fact, after applying the TS-EBCT method to the intracity freight transportation
field, the prediction accuracy of the causal effect has been significantly
improved. It brings good business benefits to the company&apos;s subsidy and pricing
strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hu_T/0/1/0/all/0/1&quot;&gt;Tao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Honglong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zeng_F/0/1/0/all/0/1&quot;&gt;Fan Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Du_M/0/1/0/all/0/1&quot;&gt;Min Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Du_X/0/1/0/all/0/1&quot;&gt;XiangKun Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yue Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Quanqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mengran Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Dan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jihao Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09131">
<title>Physics-Informed Neural Network Lyapunov Functions: PDE Characterization, Learning, and Verification. (arXiv:2312.09131v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2312.09131</link>
<description rdf:parseType="Literal">&lt;p&gt;We provide a systematic investigation of using physics-informed neural
networks to compute Lyapunov functions. We encode Lyapunov conditions as a
partial differential equation (PDE) and use this for training neural network
Lyapunov functions. We analyze the analytical properties of the solutions to
the Lyapunov and Zubov PDEs. In particular, we show that employing the Zubov
equation in training neural Lyapunov functions can lead to approximate regions
of attraction close to the true domain of attraction. We also examine
approximation errors and the convergence of neural approximations to the unique
solution of Zubov&apos;s equation. We then provide sufficient conditions for the
learned neural Lyapunov functions that can be readily verified by
satisfiability modulo theories (SMT) solvers, enabling formal verification of
both local stability analysis and region-of-attraction estimates in the large.
Through a number of nonlinear examples, ranging from low to high dimensions, we
demonstrate that the proposed framework can outperform traditional
sums-of-squares (SOS) Lyapunov functions obtained using semidefinite
programming (SDP).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Meng_Y/0/1/0/all/0/1&quot;&gt;Yiming Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Fitzsimmons_M/0/1/0/all/0/1&quot;&gt;Maxwell Fitzsimmons&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhou_R/0/1/0/all/0/1&quot;&gt;Ruikun Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09259">
<title>Livestock feeding behavior: A tutorial review on automated techniques for ruminant monitoring. (arXiv:2312.09259v2 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/2312.09259</link>
<description rdf:parseType="Literal">&lt;p&gt;Livestock feeding behavior is an influential research area for those involved
in animal husbandry and agriculture. In recent years, there has been a growing
interest in automated systems for monitoring the behavior of ruminants. Despite
the developments accomplished in the last decade, there is still much to do and
learn about the methods for measuring and analyzing livestock feeding behavior.
Automated monitoring systems mainly use motion, acoustic, and image sensors to
collect animal behavioral data. The performance evaluation of existing methods
is a complex task and direct comparisons between studies are difficult. Several
factors prevent a direct comparison, starting from the diversity of data and
performance metrics used in the experiments. To the best of our knowledge, this
work represents the first tutorial-style review on the analysis of the feeding
behavior of ruminants, emphasizing the relationship between sensing
methodologies, signal processing and computational intelligence methods. It
assesses the main sensing methodologies (i.e. based on movement, sound,
images/videos and pressure) and the main techniques to measure and analyze the
signals associated with feeding behavior, evaluating their use in different
settings and situations. It also highlights the potentiality of automated
monitoring systems to provide valuable information that improves our
understanding of livestock feeding behavior. The relevance of these systems is
increasingly important due to their impact on production systems and research.
Finally, the paper closes by discussing future challenges and opportunities in
livestock feeding behavior monitoring.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chelotti_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Chelotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Martinez_Rau_L/0/1/0/all/0/1&quot;&gt;Luciano Martinez-Rau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ferrero_M/0/1/0/all/0/1&quot;&gt;Mariano Ferrero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vignolo_L/0/1/0/all/0/1&quot;&gt;Leandro Vignolo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Galli_J/0/1/0/all/0/1&quot;&gt;Julio Galli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Planisich_A/0/1/0/all/0/1&quot;&gt;Alejandra Planisich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rufiner_H/0/1/0/all/0/1&quot;&gt;H. Leonardo Rufiner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Giovanini_L/0/1/0/all/0/1&quot;&gt;Leonardo Giovanini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09323">
<title>Perspectives on the State and Future of Deep Learning - 2023. (arXiv:2312.09323v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.09323</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of this series is to chronicle opinions and issues in the field of
machine learning as they stand today and as they change over time. The plan is
to host this survey periodically until the AI singularity
paperclip-frenzy-driven doomsday, keeping an updated list of topical questions
and interviewing new community members for each edition. In this issue, we
probed people&apos;s opinions on interpretable AI, the value of benchmarking in
modern NLP, the state of progress towards understanding deep learning, and the
future of academia.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1&quot;&gt;Micah Goldblum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1&quot;&gt;Anima Anandkumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1&quot;&gt;Richard Baraniuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1&quot;&gt;Tom Goldstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1&quot;&gt;Kyunghyun Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1&quot;&gt;Zachary C Lipton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitchell_M/0/1/0/all/0/1&quot;&gt;Melanie Mitchell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakkiran_P/0/1/0/all/0/1&quot;&gt;Preetum Nakkiran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1&quot;&gt;Max Welling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1&quot;&gt;Andrew Gordon Wilson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09775">
<title>A Comparative Evaluation of Additive Separability Tests for Physics-Informed Machine Learning. (arXiv:2312.09775v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.09775</link>
<description rdf:parseType="Literal">&lt;p&gt;Many functions characterising physical systems are additively separable. This
is the case, for instance, of mechanical Hamiltonian functions in physics,
population growth equations in biology, and consumer preference and utility
functions in economics. We consider the scenario in which a surrogate of a
function is to be tested for additive separability. The detection that the
surrogate is additively separable can be leveraged to improve further learning.
Hence, it is beneficial to have the ability to test for such separability in
surrogates. The mathematical approach is to test if the mixed partial
derivative of the surrogate is zero; or empirically, lower than a threshold. We
present and comparatively and empirically evaluate the eight methods to compute
the mixed partial derivative of a surrogate function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khoo_Z/0/1/0/all/0/1&quot;&gt;Zi-Yu Khoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Low_J/0/1/0/all/0/1&quot;&gt;Jonathan Sze Choong Low&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bressan_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Bressan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09783">
<title>Keep the Faith: Faithful Explanations in Convolutional Neural Networks for Case-Based Reasoning. (arXiv:2312.09783v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.09783</link>
<description rdf:parseType="Literal">&lt;p&gt;Explaining predictions of black-box neural networks is crucial when applied
to decision-critical tasks. Thus, attribution maps are commonly used to
identify important image regions, despite prior work showing that humans prefer
explanations based on similar examples. To this end, ProtoPNet learns a set of
class-representative feature vectors (prototypes) for case-based reasoning.
During inference, similarities of latent features to prototypes are linearly
classified to form predictions and attribution maps are provided to explain the
similarity. In this work, we evaluate whether architectures for case-based
reasoning fulfill established axioms required for faithful explanations using
the example of ProtoPNet. We show that such architectures allow the extraction
of faithful explanations. However, we prove that the attribution maps used to
explain the similarities violate the axioms. We propose a new procedure to
extract explanations for trained ProtoPNets, named ProtoPFaith. Conceptually,
these explanations are Shapley values, calculated on the similarity scores of
each prototype. They allow to faithfully answer which prototypes are present in
an unseen image and quantify each pixel&apos;s contribution to that presence,
thereby complying with all axioms. The theoretical violations of ProtoPNet
manifest in our experiments on three datasets (CUB-200-2011, Stanford Dogs,
RSNA) and five architectures (ConvNet, ResNet, ResNet50, WideResNet50,
ResNeXt50). Our experiments show a qualitative difference between the
explanations given by ProtoPNet and ProtoPFaith. Additionally, we quantify the
explanations with the Area Over the Perturbation Curve, on which ProtoPFaith
outperforms ProtoPNet on all experiments by a factor $&amp;gt;10^3$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolf_T/0/1/0/all/0/1&quot;&gt;Tom Nuno Wolf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bongratz_F/0/1/0/all/0/1&quot;&gt;Fabian Bongratz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rickmann_A/0/1/0/all/0/1&quot;&gt;Anne-Marie Rickmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Polsterl_S/0/1/0/all/0/1&quot;&gt;Sebastian P&amp;#xf6;lsterl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wachinger_C/0/1/0/all/0/1&quot;&gt;Christian Wachinger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09844">
<title>Small Dataset, Big Gains: Enhancing Reinforcement Learning by Offline Pre-Training with Model Based Augmentation. (arXiv:2312.09844v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.09844</link>
<description rdf:parseType="Literal">&lt;p&gt;Offline reinforcement learning leverages pre-collected datasets of
transitions to train policies. It can serve as effective initialization for
online algorithms, enhancing sample efficiency and speeding up convergence.
However, when such datasets are limited in size and quality, offline
pre-training can produce sub-optimal policies and lead to degraded online
reinforcement learning performance. In this paper we propose a model-based data
augmentation strategy to maximize the benefits of offline reinforcement
learning pre-training and reduce the scale of data needed to be effective. Our
approach leverages a world model of the environment trained on the offline
dataset to augment states during offline pre-training. We evaluate our approach
on a variety of MuJoCo robotic tasks and our results show it can jump-start
online fine-tuning and substantially reduce - in some cases by an order of
magnitude - the required number of environment interactions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Macaluso_G/0/1/0/all/0/1&quot;&gt;Girolamo Macaluso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sestini_A/0/1/0/all/0/1&quot;&gt;Alessandro Sestini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagdanov_A/0/1/0/all/0/1&quot;&gt;Andrew D. Bagdanov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10087">
<title>Revisiting the Entropy Semiring for Neural Speech Recognition. (arXiv:2312.10087v2 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10087</link>
<description rdf:parseType="Literal">&lt;p&gt;In streaming settings, speech recognition models have to map sub-sequences of
speech to text before the full audio stream becomes available. However, since
alignment information between speech and text is rarely available during
training, models need to learn it in a completely self-supervised way. In
practice, the exponential number of possible alignments makes this extremely
challenging, with models often learning peaky or sub-optimal alignments. Prima
facie, the exponential nature of the alignment space makes it difficult to even
quantify the uncertainty of a model&apos;s alignment distribution. Fortunately, it
has been known for decades that the entropy of a probabilistic finite state
transducer can be computed in time linear to the size of the transducer via a
dynamic programming reduction based on semirings. In this work, we revisit the
entropy semiring for neural speech recognition models, and show how alignment
entropy can be used to supervise models through regularization or distillation.
We also contribute an open-source implementation of CTC and RNN-T in the
semiring framework that includes numerically stable and highly parallel
variants of the entropy semiring. Empirically, we observe that the addition of
alignment distillation improves the accuracy and latency of an already
well-optimized teacher-student distillation model, achieving state-of-the-art
performance on the Librispeech dataset in the streaming scenario.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chang_O/0/1/0/all/0/1&quot;&gt;Oscar Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hwang_D/0/1/0/all/0/1&quot;&gt;Dongseong Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Siohan_O/0/1/0/all/0/1&quot;&gt;Olivier Siohan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10088">
<title>On Robustness to Missing Video for Audiovisual Speech Recognition. (arXiv:2312.10088v2 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10088</link>
<description rdf:parseType="Literal">&lt;p&gt;It has been shown that learning audiovisual features can lead to improved
speech recognition performance over audio-only features, especially for noisy
speech. However, in many common applications, the visual features are partially
or entirely missing, e.g.~the speaker might move off screen. Multi-modal models
need to be robust: missing video frames should not degrade the performance of
an audiovisual model to be worse than that of a single-modality audio-only
model. While there have been many attempts at building robust models, there is
little consensus on how robustness should be evaluated. To address this, we
introduce a framework that allows claims about robustness to be evaluated in a
precise and testable way. We also conduct a systematic empirical study of the
robustness of common audiovisual speech recognition architectures on a range of
acoustic noise conditions and test suites. Finally, we show that an
architecture-agnostic solution based on cascades can consistently achieve
robustness to missing video, even in settings where existing techniques for
robustness like dropout fall short.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chang_O/0/1/0/all/0/1&quot;&gt;Oscar Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Braga_O/0/1/0/all/0/1&quot;&gt;Otavio Braga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liao_H/0/1/0/all/0/1&quot;&gt;Hank Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Serdyuk_D/0/1/0/all/0/1&quot;&gt;Dmitriy Serdyuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Siohan_O/0/1/0/all/0/1&quot;&gt;Olivier Siohan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10130">
<title>Improving new physics searches with diffusion models for event observables and jet constituents. (arXiv:2312.10130v2 [physics.data-an] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10130</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a new technique called Drapes to enhance the sensitivity in
searches for new physics at the LHC. By training diffusion models on side-band
data, we show how background templates for the signal region can be generated
either directly from noise, or by partially applying the diffusion process to
existing data. In the partial diffusion case, data can be drawn from side-band
regions, with the inverse diffusion performed for new target conditional
values, or from the signal region, preserving the distribution over the
conditional property that defines the signal region. We apply this technique to
the hunt for resonances using the LHCO di-jet dataset, and achieve
state-of-the-art performance for background template generation using high
level input features. We also show how Drapes can be applied to low level
inputs with jet constituents, reducing the model dependence on the choice of
input observables. Using jet constituents we can further improve sensitivity to
the signal process, but observe a loss in performance where the signal
significance before applying any selection is below 4$\sigma$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Sengupta_D/0/1/0/all/0/1&quot;&gt;Debajyoti Sengupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Leigh_M/0/1/0/all/0/1&quot;&gt;Matthew Leigh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Raine_J/0/1/0/all/0/1&quot;&gt;John Andrew Raine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Klein_S/0/1/0/all/0/1&quot;&gt;Samuel Klein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Golling_T/0/1/0/all/0/1&quot;&gt;Tobias Golling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10194">
<title>Pareto Envelope Augmented with Reinforcement Learning: Multi-objective reinforcement learning-based approach for Large-Scale Constrained Pressurized Water Reactor optimization. (arXiv:2312.10194v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10194</link>
<description rdf:parseType="Literal">&lt;p&gt;A novel method, the Pareto Envelope Augmented with Reinforcement Learning
(PEARL), has been developed to address the challenges posed by multi-objective
problems, particularly in the field of engineering where the evaluation of
candidate solutions can be time-consuming. PEARL distinguishes itself from
traditional policy-based multi-objective Reinforcement Learning methods by
learning a single policy, eliminating the need for multiple neural networks to
independently solve simpler sub-problems. Several versions inspired from deep
learning and evolutionary techniques have been crafted, catering to both
unconstrained and constrained problem domains. Curriculum Learning is harnessed
to effectively manage constraints in these versions. PEARL&apos;s performance is
first evaluated on classical multi-objective benchmarks. Additionally, it is
tested on two practical PWR core Loading Pattern optimization problems to
showcase its real-world applicability. The first problem involves optimizing
the Cycle length and the rod-integrated peaking factor as the primary
objectives, while the second problem incorporates the mean average enrichment
as an additional objective. Furthermore, PEARL addresses three types of
constraints related to boron concentration, peak pin burnup, and peak pin
power. The results are systematically compared against a conventional approach,
the Non-dominated Sorting Genetic Algorithm. Notably, PEARL, specifically the
PEARL-NdS variant, efficiently uncovers a Pareto front without necessitating
additional efforts from the algorithm designer, as opposed to a single
optimization with scaled objectives. It also outperforms the classical approach
across multiple performance metrics, including the Hyper-volume.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seurin_P/0/1/0/all/0/1&quot;&gt;Paul Seurin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shirvan_K/0/1/0/all/0/1&quot;&gt;Koroush Shirvan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10237">
<title>Vertical Federated Alzheimer&apos;s Detection on Multimodal Data. (arXiv:2312.10237v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10237</link>
<description rdf:parseType="Literal">&lt;p&gt;In the era of rapidly advancing medical technologies, the segmentation of
medical data has become inevitable, necessitating the development of privacy
preserving machine learning algorithms that can train on distributed data.
Consolidating sensitive medical data is not always an option particularly due
to the stringent privacy regulations imposed by the Health Insurance
Portability and Accountability Act (HIPAA). In this paper, we introduce a HIPAA
compliant framework that can train from distributed data. We then propose a
multimodal vertical federated model for Alzheimer&apos;s Disease (AD) detection, a
serious neurodegenerative condition that can cause dementia, severely impairing
brain function and hindering simple tasks, especially without preventative
care. This vertical federated model offers a distributed architecture that
enables collaborative learning across diverse sources of medical data while
respecting privacy constraints imposed by HIPAA. It is also able to leverage
multiple modalities of data, enhancing the robustness and accuracy of AD
detection. Our proposed model not only contributes to the advancement of
federated learning techniques but also holds promise for overcoming the hurdles
posed by data segmentation in medical research. By using vertical federated
learning, this research strives to provide a framework that enables healthcare
institutions to harness the collective intelligence embedded in their
distributed datasets without compromising patient privacy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mandal_P/0/1/0/all/0/1&quot;&gt;Paul K. Mandal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10276">
<title>Asymmetric Norms to Approximate the Minimum Action Distance. (arXiv:2312.10276v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10276</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a state representation for reward-free Markov decision
processes. The idea is to learn, in a self-supervised manner, an embedding
space where distances between pairs of embedded states correspond to the
minimum number of actions needed to transition between them. Unlike previous
methods, our approach incorporates an asymmetric norm parametrization, enabling
accurate approximations of minimum action distances in environments with
inherent asymmetry. We show how this representation can be leveraged to learn
goal-conditioned policies, providing a notion of similarity between states and
goals and a useful heuristic distance to guide planning. To validate our
approach, we conduct empirical experiments on both symmetric and asymmetric
environments. Our results show that our asymmetric norm parametrization
performs comparably to symmetric norms in symmetric environments and surpasses
symmetric norms in asymmetric environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steccanella_L/0/1/0/all/0/1&quot;&gt;Lorenzo Steccanella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jonsson_A/0/1/0/all/0/1&quot;&gt;Anders Jonsson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10418">
<title>Fractional Deep Reinforcement Learning for Age-Minimal Mobile Edge Computing. (arXiv:2312.10418v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10418</link>
<description rdf:parseType="Literal">&lt;p&gt;Mobile edge computing (MEC) is a promising paradigm for real-time
applications with intensive computational needs (e.g., autonomous driving), as
it can reduce the processing delay. In this work, we focus on the timeliness of
computational-intensive updates, measured by Age-ofInformation (AoI), and study
how to jointly optimize the task updating and offloading policies for AoI with
fractional form. Specifically, we consider edge load dynamics and formulate a
task scheduling problem to minimize the expected time-average AoI. The
uncertain edge load dynamics, the nature of the fractional objective, and
hybrid continuous-discrete action space (due to the joint optimization) make
this problem challenging and existing approaches not directly applicable. To
this end, we propose a fractional reinforcement learning(RL) framework and
prove its convergence. We further design a model-free fractional deep RL (DRL)
algorithm, where each device makes scheduling decisions with the hybrid action
space without knowing the system dynamics and decisions of other devices.
Experimental results show that our proposed algorithms reduce the average AoI
by up to 57.6% compared with several non-fractional benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1&quot;&gt;Lyudong Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1&quot;&gt;Ming Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Meng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11026">
<title>MISA: Unveiling the Vulnerabilities in Split Federated Learning. (arXiv:2312.11026v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11026</link>
<description rdf:parseType="Literal">&lt;p&gt;\textit{Federated learning} (FL) and \textit{split learning} (SL) are
prevailing distributed paradigms in recent years. They both enable shared
global model training while keeping data localized on users&apos; devices. The
former excels in parallel execution capabilities, while the latter enjoys low
dependence on edge computing resources and strong privacy protection.
\textit{Split federated learning} (SFL) combines the strengths of both FL and
SL, making it one of the most popular distributed architectures. Furthermore, a
recent study has claimed that SFL exhibits robustness against poisoning
attacks, with a fivefold improvement compared to FL in terms of robustness.
&lt;/p&gt;
&lt;p&gt;In this paper, we present a novel poisoning attack known as MISA. It poisons
both the top and bottom models, causing a \textbf{\underline{misa}}lignment in
the global model, ultimately leading to a drastic accuracy collapse. This
attack unveils the vulnerabilities in SFL, challenging the conventional belief
that SFL is robust against poisoning attacks. Extensive experiments demonstrate
that our proposed MISA poses a significant threat to the availability of SFL,
underscoring the imperative for academia and industry to accord this matter due
attention.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_W/0/1/0/all/0/1&quot;&gt;Wei Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shengshan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1&quot;&gt;Lulu Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Minghui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Leo Yu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1&quot;&gt;Hai Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11315">
<title>CaRe-CNN: Cascading Refinement CNN for Myocardial Infarct Segmentation with Microvascular Obstructions. (arXiv:2312.11315v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11315</link>
<description rdf:parseType="Literal">&lt;p&gt;Late gadolinium enhanced (LGE) magnetic resonance (MR) imaging is widely
established to assess the viability of myocardial tissue of patients after
acute myocardial infarction (MI). We propose the Cascading Refinement CNN
(CaRe-CNN), which is a fully 3D, end-to-end trained, 3-stage CNN cascade that
exploits the hierarchical structure of such labeled cardiac data. Throughout
the three stages of the cascade, the label definition changes and CaRe-CNN
learns to gradually refine its intermediate predictions accordingly.
Furthermore, to obtain more consistent qualitative predictions, we propose a
series of post-processing steps that take anatomical constraints into account.
Our CaRe-CNN was submitted to the FIMH 2023 MYOSAIQ challenge, where it ranked
second out of 18 participating teams. CaRe-CNN showed great improvements most
notably when segmenting the difficult but clinically most relevant myocardial
infarct tissue (MIT) as well as microvascular obstructions (MVO). When
computing the average scores over all labels, our method obtained the best
score in eight out of ten metrics. Thus, accurate cardiac segmentation after
acute MI via our CaRe-CNN allows generating patient-specific models of the
heart serving as an important step towards personalized medicine.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thaler_F/0/1/0/all/0/1&quot;&gt;Franz Thaler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gsell_M/0/1/0/all/0/1&quot;&gt;Matthias A.F. Gsell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plank_G/0/1/0/all/0/1&quot;&gt;Gernot Plank&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urschler_M/0/1/0/all/0/1&quot;&gt;Martin Urschler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00760">
<title>Efficient Failure Pattern Identification of Predictive Algorithms. (arXiv:2306.00760v1 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2306.00760</link>
<description rdf:parseType="Literal">&lt;p&gt;Given a (machine learning) classifier and a collection of unlabeled data, how
can we efficiently identify misclassification patterns presented in this
dataset? To address this problem, we propose a human-machine collaborative
framework that consists of a team of human annotators and a sequential
recommendation algorithm. The recommendation algorithm is conceptualized as a
stochastic sampler that, in each round, queries the annotators a subset of
samples for their true labels and obtains the feedback information on whether
the samples are misclassified. The sampling mechanism needs to balance between
discovering new patterns of misclassification (exploration) and confirming the
potential patterns of classification (exploitation). We construct a
determinantal point process, whose intensity balances the
exploration-exploitation trade-off through the weighted update of the posterior
at each round to form the generator of the stochastic sampler. The numerical
results empirically demonstrate the competitive performance of our framework on
multiple datasets at various signal-to-noise ratios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1&quot;&gt;Bao Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1&quot;&gt;Viet Anh Nguyen&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>