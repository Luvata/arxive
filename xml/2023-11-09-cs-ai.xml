<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-11-07T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03362" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03365" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03366" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03369" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03373" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03374" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03375" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03380" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03381" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03382" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03383" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03385" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03388" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03393" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03395" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03396" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03405" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03408" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03409" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03410" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03411" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03413" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03414" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03415" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03417" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03424" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03425" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03426" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03429" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03468" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03478" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03485" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03488" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03508" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03509" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03534" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03547" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03550" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03551" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03564" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03567" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03583" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03595" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03606" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03622" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03636" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03651" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03652" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03658" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03669" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03680" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03695" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03698" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03701" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03705" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03707" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03711" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03714" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03716" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03721" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03736" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03738" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03739" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03753" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03756" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03761" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03768" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03780" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03783" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03810" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03830" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03839" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03865" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03873" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03886" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03893" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03897" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03942" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03955" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03963" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03976" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03989" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2004.14254" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2012.08026" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.11048" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2111.11104" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.10581" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.08171" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.10540" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.12850" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.06318" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.14741" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.15042" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.01984" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.13709" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.01346" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.10884" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.04178" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.12000" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.09354" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.10093" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.12040" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.15714" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.16258" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.10749" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.11235" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.13105" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.03353" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14517" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19466" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.02866" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04061" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05284" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11412" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.13258" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14351" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14770" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16819" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04841" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05857" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07870" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08304" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13214" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13813" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03427" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11849" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.14132" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.00317" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.00543" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02641" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05309" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10149" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.11807" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12425" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12567" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18725" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19109" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.20246" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.20327" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00729" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00855" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01305" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02117" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02171" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02849" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03076" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03285" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2311.03362">
<title>Simulation-based Safety Assurance for an AVP System incorporating Learning-Enabled Components. (arXiv:2311.03362v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2311.03362</link>
<description rdf:parseType="Literal">&lt;p&gt;There have been major developments in Automated Driving (AD) and Driving
Assist Systems (ADAS) in recent years. However, their safety assurance, thus
methodologies for testing, verification and validation AD/ADAS safety-critical
applications remain as one the main challenges. Inevitably AI also penetrates
into AD/ADAS applications, such as object detection. Despite important
benefits, adoption of such learned-enabled components and systems in
safety-critical scenarios causes that conventional testing approaches (e.g.,
distance-based testing in automotive) quickly become infeasible. Similarly,
safety engineering approaches usually assume model-based components and do not
handle learning-enabled ones well. The authors have participated in the
public-funded project FOCETA , and developed an Automated Valet Parking (AVP)
use case. As the nature of the baseline implementation is imperfect, it offers
a space for continuous improvement based on modelling, verification,
validation, and monitoring techniques. In this publication, we explain the
simulation-based development platform that is designed to verify and validate
safety-critical learning-enabled systems in continuous engineering loops.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Esen_H/0/1/0/all/0/1&quot;&gt;Hasan Esen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_B/0/1/0/all/0/1&quot;&gt;Brian Hsuan-Cheng Liao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03365">
<title>Leveraging Generative AI: Improving Software Metadata Classification with Generated Code-Comment Pairs. (arXiv:2311.03365v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2311.03365</link>
<description rdf:parseType="Literal">&lt;p&gt;In software development, code comments play a crucial role in enhancing code
comprehension and collaboration. This research paper addresses the challenge of
objectively classifying code comments as &quot;Useful&quot; or &quot;Not Useful.&quot; We propose a
novel solution that harnesses contextualized embeddings, particularly BERT, to
automate this classification process. We address this task by incorporating
generated code and comment pairs. The initial dataset comprised 9048 pairs of
code and comments written in C, labeled as either Useful or Not Useful. To
augment this dataset, we sourced an additional 739 lines of code-comment pairs
and generated labels using a Large Language Model Architecture, specifically
BERT. The primary objective was to build classification models that can
effectively differentiate between useful and not useful code comments. Various
machine learning algorithms were employed, including Logistic Regression,
Decision Tree, K-Nearest Neighbors (KNN), Support Vector Machine (SVM),
Gradient Boosting, Random Forest, and a Neural Network. Each algorithm was
evaluated using precision, recall, and F1-score metrics, both with the original
seed dataset and the augmented dataset. This study showcases the potential of
generative AI for enhancing binary code comment quality classification models,
providing valuable insights for software developers and researchers in the
field of natural language processing and software engineering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Syed_S/0/1/0/all/0/1&quot;&gt;Samah Syed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+S_A/0/1/0/all/0/1&quot;&gt;Angel Deborah S&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03366">
<title>Neural Rankers for Code Generation via Inter-Cluster Modeling. (arXiv:2311.03366v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2311.03366</link>
<description rdf:parseType="Literal">&lt;p&gt;Code Large Language Models (CodeLLMs) have ushered in a new era of code
generation advancements. However, selecting the best solutions from among all
possible CodeLLM solutions remains a challenge. Previous methods frequently
overlooked the intricate functional similarities and interactions between
clusters, resulting in suboptimal results. In this work, we introduce
\textit{SRank}, a novel reranking strategy for selecting the best solution from
code generation that focuses on modeling inter-cluster relationship. By
quantifying the functional overlap between clusters, our approach provides a
better ranking strategy of code solutions. Empirical results show that our
method achieves a remarkable results on pass@1 score. For instance, on the
Human-Eval benchmark, we achieve 69.66\% in pass@1 with Codex002, 75.31\% for
WizardCoder, 53.99\% for StarCoder and 60.55\% for CodeGen, which surpass the
state-of-the-arts solution ranking methods, such as CodeT and Coder-Reviewer on
the same CodeLLM with significant margin ($\approx 6.1\%$ improvement on
average). Comparing to the random sampling method, we can achieve an average
improvement of $\approx 23.07\%$ on Human-Eval and 17.64\% on MBPP. Even in
scenarios with limited test inputs, our approach demonstrates robustness and
superiority, marking a new state-of-the-arts in code generation reranking.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+To_H/0/1/0/all/0/1&quot;&gt;Hung Quoc To&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1&quot;&gt;Minh Huynh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bui_N/0/1/0/all/0/1&quot;&gt;Nghi D. Q. Bui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03369">
<title>Can We Trust the Similarity Measurement in Federated Learning?. (arXiv:2311.03369v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.03369</link>
<description rdf:parseType="Literal">&lt;p&gt;Is it secure to measure the reliability of local models by similarity in
federated learning (FL)? This paper delves into an unexplored security threat
concerning applying similarity metrics, such as the L_2 norm, Euclidean
distance, and cosine similarity, in protecting FL. We first uncover the
deficiencies of similarity metrics that high-dimensional local models,
including benign and poisoned models, may be evaluated to have the same
similarity while being significantly different in the parameter values. We then
leverage this finding to devise a novel untargeted model poisoning attack,
Faker, which launches the attack by simultaneously maximizing the evaluated
similarity of the poisoned local model and the difference in the parameter
values. Experimental results based on seven datasets and eight defenses show
that Faker outperforms the state-of-the-art benchmark attacks by 1.1-9.0X in
reducing accuracy and 1.2-8.0X in saving time cost, which even holds for the
case of a single malicious client with limited knowledge about the FL system.
Moreover, Faker can degrade the performance of the global model by attacking
only once. We also preliminarily explore extending Faker to other attacks, such
as backdoor attacks and Sybil attacks. Lastly, we provide a model evaluation
strategy, called the similarity of partial parameters (SPP), to defend against
Faker. Given that numerous mechanisms in FL utilize similarity metrics to
assess local models, this work suggests that we should be vigilant regarding
the potential risks of using these metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhilin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1&quot;&gt;Qin Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1&quot;&gt;Xukai Zou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03373">
<title>Unscrambling the Rectification of Adversarial Attacks Transferability across Computer Networks. (arXiv:2311.03373v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2311.03373</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional neural networks (CNNs) models play a vital role in achieving
state-of-the-art performances in various technological fields. CNNs are not
limited to Natural Language Processing (NLP) or Computer Vision (CV) but also
have substantial applications in other technological domains, particularly in
cybersecurity. The reliability of CNN&apos;s models can be compromised because of
their susceptibility to adversarial attacks, which can be generated
effortlessly, easily applied, and transferred in real-world scenarios.
&lt;/p&gt;
&lt;p&gt;In this paper, we present a novel and comprehensive method to improve the
strength of attacks and assess the transferability of adversarial examples in
CNNs when such strength changes, as well as whether the transferability
property issue exists in computer network applications. In the context of our
study, we initially examined six distinct modes of attack: the Carlini and
Wagner (C&amp;amp;W), Fast Gradient Sign Method (FGSM), Iterative Fast Gradient Sign
Method (I-FGSM), Jacobian-based Saliency Map (JSMA), Limited-memory Broyden
fletcher Goldfarb Shanno (L-BFGS), and Projected Gradient Descent (PGD) attack.
We applied these attack techniques on two popular datasets: the CIC and UNSW
datasets. The outcomes of our experiment demonstrate that an improvement in
transferability occurs in the targeted scenarios for FGSM, JSMA, LBFGS, and
other attacks. Our findings further indicate that the threats to security posed
by adversarial examples, even in computer network applications, necessitate the
development of novel defense mechanisms to enhance the security of DL-based
techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nowroozi_E/0/1/0/all/0/1&quot;&gt;Ehsan Nowroozi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghelichkhani_S/0/1/0/all/0/1&quot;&gt;Samaneh Ghelichkhani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haider_I/0/1/0/all/0/1&quot;&gt;Imran Haider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dehghantanha_A/0/1/0/all/0/1&quot;&gt;Ali Dehghantanha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03374">
<title>Generative AI for Software Metadata: Overview of the Information Retrieval in Software Engineering Track at FIRE 2023. (arXiv:2311.03374v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2311.03374</link>
<description rdf:parseType="Literal">&lt;p&gt;The Information Retrieval in Software Engineering (IRSE) track aims to
develop solutions for automated evaluation of code comments in a machine
learning framework based on human and large language model generated labels. In
this track, there is a binary classification task to classify comments as
useful and not useful. The dataset consists of 9048 code comments and
surrounding code snippet pairs extracted from open source github C based
projects and an additional dataset generated individually by teams using large
language models. Overall 56 experiments have been submitted by 17 teams from
various universities and software companies. The submissions have been
evaluated quantitatively using the F1-Score and qualitatively based on the type
of features developed, the supervised learning model used and their
corresponding hyper-parameters. The labels generated from large language models
increase the bias in the prediction model but lead to less over-fitted results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1&quot;&gt;Srijoni Majumdar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1&quot;&gt;Soumen Paul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paul_D/0/1/0/all/0/1&quot;&gt;Debjyoti Paul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bandyopadhyay_A/0/1/0/all/0/1&quot;&gt;Ayan Bandyopadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chattopadhyay_S/0/1/0/all/0/1&quot;&gt;Samiran Chattopadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1&quot;&gt;Partha Pratim Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clough_P/0/1/0/all/0/1&quot;&gt;Paul D Clough&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Majumder_P/0/1/0/all/0/1&quot;&gt;Prasenjit Majumder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03375">
<title>Edge AI Inference in Heterogeneous Constrained Computing: Feasibility and Opportunities. (arXiv:2311.03375v1 [cs.AR])</title>
<link>http://arxiv.org/abs/2311.03375</link>
<description rdf:parseType="Literal">&lt;p&gt;The network edge&apos;s role in Artificial Intelligence (AI) inference processing
is rapidly expanding, driven by a plethora of applications seeking
computational advantages. These applications strive for data-driven efficiency,
leveraging robust AI capabilities and prioritizing real-time responsiveness.
However, as demand grows, so does system complexity. The proliferation of AI
inference accelerators showcases innovation but also underscores challenges,
particularly the varied software and hardware configurations of these devices.
This diversity, while advantageous for certain tasks, introduces hurdles in
device integration and coordination. In this paper, our objectives are
three-fold. Firstly, we outline the requirements and components of a framework
that accommodates hardware diversity. Next, we assess the impact of device
heterogeneity on AI inference performance, identifying strategies to optimize
outcomes without compromising service quality. Lastly, we shed light on the
prevailing challenges and opportunities in this domain, offering insights for
both the research community and industry stakeholders.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morabito_R/0/1/0/all/0/1&quot;&gt;Roberto Morabito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tatipamula_M/0/1/0/all/0/1&quot;&gt;Mallik Tatipamula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tarkoma_S/0/1/0/all/0/1&quot;&gt;Sasu Tarkoma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiang_M/0/1/0/all/0/1&quot;&gt;Mung Chiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03380">
<title>An attempt to generate new bridge types from latent space of variational autoencoder. (arXiv:2311.03380v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.03380</link>
<description rdf:parseType="Literal">&lt;p&gt;Try to generate new bridge types using generative artificial intelligence
technology. The grayscale images of the bridge facade with the change of
component width was rendered by 3dsMax animation software, and then the OpenCV
module performed an appropriate amount of geometric transformation (rotation,
horizontal scale, vertical scale) to obtain the image dataset of three-span
beam bridge, arch bridge, cable-stayed bridge and suspension bridge. Based on
Python programming language, TensorFlow and Keras deep learning platform
framework, variational autoencoder was constructed and trained, and
low-dimensional bridge-type latent space that is convenient for vector
operations was obtained. Variational autoencoder can combine two bridge types
on the basis of the original of human into one that is a new bridge type.
Generative artificial intelligence technology can assist bridge designers in
bridge-type innovation, and can be used as copilot.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongjun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03381">
<title>Separating and Learning Latent Confounders to Enhancing User Preferences Modeling. (arXiv:2311.03381v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2311.03381</link>
<description rdf:parseType="Literal">&lt;p&gt;Recommender models aim to capture user preferences from historical feedback
and then predict user-specific feedback on candidate items. However, the
presence of various unmeasured confounders causes deviations between the user
preferences in the historical feedback and the true preferences, resulting in
models not meeting their expected performance. Existing debias models either
(1) specific to solving one particular bias or (2) directly obtain auxiliary
information from user historical feedback, which cannot identify whether the
learned preferences are true user preferences or mixed with unmeasured
confounders. Moreover, we find that the former recommender system is not only a
successor to unmeasured confounders but also acts as an unmeasured confounder
affecting user preference modeling, which has always been neglected in previous
studies. To this end, we incorporate the effect of the former recommender
system and treat it as a proxy for all unmeasured confounders. We propose a
novel framework, \textbf{S}eparating and \textbf{L}earning Latent Confounders
\textbf{F}or \textbf{R}ecommendation (\textbf{SLFR}), which obtains the
representation of unmeasured confounders to identify the counterfactual
feedback by disentangling user preferences and unmeasured confounders, then
guides the target model to capture the true preferences of users. Extensive
experiments in five real-world datasets validate the advantages of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hangtong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yuanbo Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yongjian Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03382">
<title>Causal Structure Representation Learning of Confounders in Latent Space for Recommendation. (arXiv:2311.03382v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2311.03382</link>
<description rdf:parseType="Literal">&lt;p&gt;Inferring user preferences from the historical feedback of users is a
valuable problem in recommender systems. Conventional approaches often rely on
the assumption that user preferences in the feedback data are equivalent to the
real user preferences without additional noise, which simplifies the problem
modeling. However, there are various confounders during user-item interactions,
such as weather and even the recommendation system itself. Therefore,
neglecting the influence of confounders will result in inaccurate user
preferences and suboptimal performance of the model. Furthermore, the
unobservability of confounders poses a challenge in further addressing the
problem. To address these issues, we refine the problem and propose a more
rational solution. Specifically, we consider the influence of confounders,
disentangle them from user preferences in the latent space, and employ causal
graphs to model their interdependencies without specific labels. By cleverly
combining local and global causal graphs, we capture the user-specificity of
confounders on user preferences. We theoretically demonstrate the
identifiability of the obtained causal graph. Finally, we propose our model
based on Variational Autoencoders, named Causal Structure representation
learning of Confounders in latent space (CSC). We conducted extensive
experiments on one synthetic dataset and five real-world datasets,
demonstrating the superiority of our model. Furthermore, we demonstrate that
the learned causal representations of confounders are controllable, potentially
offering users fine-grained control over the objectives of their recommendation
lists with the learned causal graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hangtong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yuanbo Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yongjian Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03383">
<title>Toward Reinforcement Learning-based Rectilinear Macro Placement Under Human Constraints. (arXiv:2311.03383v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.03383</link>
<description rdf:parseType="Literal">&lt;p&gt;Macro placement is a critical phase in chip design, which becomes more
intricate when involving general rectilinear macros and layout areas.
Furthermore, macro placement that incorporates human-like constraints, such as
design hierarchy and peripheral bias, has the potential to significantly reduce
the amount of additional manual labor required from designers. This study
proposes a methodology that leverages an approach suggested by Google&apos;s Circuit
Training (G-CT) to provide a learning-based macro placer that not only supports
placing rectilinear cases, but also adheres to crucial human-like design
principles. Our experimental results demonstrate the effectiveness of our
framework in achieving power-performance-area (PPA) metrics and in obtaining
placements of high quality, comparable to those produced with human
intervention. Additionally, our methodology shows potential as a generalized
model to address diverse macro shapes and layout areas.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1&quot;&gt;Tuyen P. Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1&quot;&gt;Hieu T. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baek_S/0/1/0/all/0/1&quot;&gt;Seungyeol Baek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Taeyoun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jungwoo Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seongjung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyunjin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_M/0/1/0/all/0/1&quot;&gt;Misu Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Daehoon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seokyong Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1&quot;&gt;Daewoo Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03385">
<title>Intelligent Stress Assessment for e-Coaching. (arXiv:2311.03385v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2311.03385</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper considers the adaptation of the e-coaching concept at times of
emergencies and disasters, through aiding the e-coaching with intelligent tools
for monitoring humans&apos; affective state. The states such as anxiety, panic,
avoidance, and stress, if properly detected, can be mitigated using the
e-coaching tactic and strategy. In this work, we focus on a stress monitoring
assistant tool developed on machine learning techniques. We provide the results
of an experimental study using the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_K/0/1/0/all/0/1&quot;&gt;Kenneth Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yanushkevich_S/0/1/0/all/0/1&quot;&gt;Svetlana Yanushkevich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shmerko_V/0/1/0/all/0/1&quot;&gt;Vlad Shmerko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03388">
<title>Attention-based Models for Snow-Water Equivalent Prediction. (arXiv:2311.03388v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.03388</link>
<description rdf:parseType="Literal">&lt;p&gt;Snow Water-Equivalent (SWE) -- the amount of water available if snowpack is
melted -- is a key decision variable used by water management agencies to make
irrigation, flood control, power generation and drought management decisions.
SWE values vary spatiotemporally -- affected by weather, topography and other
environmental factors. While daily SWE can be measured by Snow Telemetry
(SNOTEL) stations with requisite instrumentation, such stations are spatially
sparse requiring interpolation techniques to create spatiotemporally complete
data. While recent efforts have explored machine learning (ML) for SWE
prediction, a number of recent ML advances have yet to be considered. The main
contribution of this paper is to explore one such ML advance, attention
mechanisms, for SWE prediction. Our hypothesis is that attention has a unique
ability to capture and exploit correlations that may exist across locations or
the temporal spectrum (or both). We present a generic attention-based modeling
framework for SWE prediction and adapt it to capture spatial attention and
temporal attention. Our experimental results on 323 SNOTEL stations in the
Western U.S. demonstrate that our attention-based models outperform other
machine learning approaches. We also provide key results highlighting the
differences between spatial and temporal attention in this context and a
roadmap toward deployment for generating spatially-complete SWE maps.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thapa_K/0/1/0/all/0/1&quot;&gt;Krishu K. Thapa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_B/0/1/0/all/0/1&quot;&gt;Bhupinderjeet Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savalkar_S/0/1/0/all/0/1&quot;&gt;Supriya Savalkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fern_A/0/1/0/all/0/1&quot;&gt;Alan Fern&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajagopalan_K/0/1/0/all/0/1&quot;&gt;Kirti Rajagopalan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalyanaraman_A/0/1/0/all/0/1&quot;&gt;Ananth Kalyanaraman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03393">
<title>Sketching Multidimensional Time Series for Fast Discord Mining. (arXiv:2311.03393v1 [cs.DB])</title>
<link>http://arxiv.org/abs/2311.03393</link>
<description rdf:parseType="Literal">&lt;p&gt;Time series discords are a useful primitive for time series anomaly
detection, and the matrix profile is capable of capturing discord effectively.
There exist many research efforts to improve the scalability of discord
discovery with respect to the length of time series. However, there is
surprisingly little work focused on reducing the time complexity of matrix
profile computation associated with dimensionality of a multidimensional time
series. In this work, we propose a sketch for discord mining among
multi-dimensional time series. After an initial pre-processing of the sketch as
fast as reading the data, the discord mining has runtime independent of the
dimensionality of the original data. On several real world examples from water
treatment and transportation, the proposed algorithm improves the throughput by
at least an order of magnitude (50X) and only has minimal impact on the quality
of the approximated solution. Additionally, the proposed method can handle the
dynamic addition or deletion of dimensions inconsequential overhead. This
allows a data analyst to consider &quot;what-if&quot; scenarios in real time while
exploring the data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1&quot;&gt;Chin-Chia Michael Yeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_M/0/1/0/all/0/1&quot;&gt;Menghai Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huiyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1&quot;&gt;Zhongfang Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junpeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phillips_J/0/1/0/all/0/1&quot;&gt;Jeff M. Phillips&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keogh_E/0/1/0/all/0/1&quot;&gt;Eamonn Keogh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03395">
<title>Newvision: application for helping blind people using deep learning. (arXiv:2311.03395v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2311.03395</link>
<description rdf:parseType="Literal">&lt;p&gt;As able-bodied people, we often take our vision for granted. For people who
are visually impaired, however, their disability can have a significant impact
on their daily lives. We are developing proprietary headgear that will help
visually impaired people navigate their surroundings, identify objects and
people, read text, and avoid obstacles. The headgear will use a combination of
computer vision, distance estimation with ultrasonic sensors, voice
recognition, and voice assistants to provide users with real-time information
about their environment. Users will be able to interact with the headgear
through voice commands, such as &apos;&apos;What is that?&apos;&apos; to identify an object or
&apos;&apos;Navigate to the front door&apos;&apos; to find their way around. The headgear will then
provide the user with a verbal description of the object or spoken navigation
instructions. We believe that this headgear has the potential to make a
significant difference in the lives of visually impaired people, allowing them
to live more independently and participate more fully in society.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bobba_K/0/1/0/all/0/1&quot;&gt;Kumar Srinivas Bobba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+K_K/0/1/0/all/0/1&quot;&gt;Kartheeban K&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boddu_V/0/1/0/all/0/1&quot;&gt;Vamsi Krishna Sai Boddu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bolla_V/0/1/0/all/0/1&quot;&gt;Vijaya Mani Surendra Bolla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bugga_D/0/1/0/all/0/1&quot;&gt;Dinesh Bugga&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03396">
<title>Differentially Private Pre-Trained Model Fusion using Decentralized Federated Graph Matching. (arXiv:2311.03396v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.03396</link>
<description rdf:parseType="Literal">&lt;p&gt;Model fusion is becoming a crucial component in the context of
model-as-a-service scenarios, enabling the delivery of high-quality model
services to local users. However, this approach introduces privacy risks and
imposes certain limitations on its applications. Ensuring secure model exchange
and knowledge fusion among users becomes a significant challenge in this
setting. To tackle this issue, we propose PrivFusion, a novel architecture that
preserves privacy while facilitating model fusion under the constraints of
local differential privacy. PrivFusion leverages a graph-based structure,
enabling the fusion of models from multiple parties without necessitating
retraining. By employing randomized mechanisms, PrivFusion ensures privacy
guarantees throughout the fusion process. To enhance model privacy, our
approach incorporates a hybrid local differentially private mechanism and
decentralized federated graph matching, effectively protecting both activation
values and weights. Additionally, we introduce a perturbation filter adapter to
alleviate the impact of randomized noise, thereby preserving the utility of the
fused model. Through extensive experiments conducted on diverse image datasets
and real-world healthcare applications, we provide empirical evidence
showcasing the effectiveness of PrivFusion in maintaining model performance
while preserving privacy. Our contributions offer valuable insights and
practical solutions for secure and collaborative data analysis within the
domain of privacy-preserving model fusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yiqiang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xinlong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Teng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1&quot;&gt;Weiwei Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wuliang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zhen Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_B/0/1/0/all/0/1&quot;&gt;Bo Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03405">
<title>Communication Efficient and Privacy-Preserving Federated Learning Based on Evolution Strategies. (arXiv:2311.03405v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.03405</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) is an emerging paradigm for training deep neural
networks (DNNs) in distributed manners. Current FL approaches all suffer from
high communication overhead and information leakage. In this work, we present a
federated learning algorithm based on evolution strategies (FedES), a
zeroth-order training method. Instead of transmitting model parameters, FedES
only communicates loss values, and thus has very low communication overhead.
Moreover, a third party is unable to estimate gradients without knowing the
pre-shared seed, which protects data privacy. Experimental results demonstrate
FedES can achieve the above benefits while keeping convergence performance the
same as that with back propagation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_G/0/1/0/all/0/1&quot;&gt;Guangchen Lan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03408">
<title>Training Multi-layer Neural Networks on Ising Machine. (arXiv:2311.03408v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.03408</link>
<description rdf:parseType="Literal">&lt;p&gt;As a dedicated quantum device, Ising machines could solve large-scale binary
optimization problems in milliseconds. There is emerging interest in utilizing
Ising machines to train feedforward neural networks due to the prosperity of
generative artificial intelligence. However, existing methods can only train
single-layer feedforward networks because of the complex nonlinear network
topology. This paper proposes an Ising learning algorithm to train quantized
neural network (QNN), by incorporating two essential techinques, namely binary
representation of topological network and order reduction of loss function. As
far as we know, this is the first algorithm to train multi-layer feedforward
networks on Ising machines, providing an alternative to gradient-based
backpropagation. Firstly, training QNN is formulated as a quadratic constrained
binary optimization (QCBO) problem by representing neuron connection and
activation function as equality constraints. All quantized variables are
encoded by binary bits based on binary encoding protocol. Secondly, QCBO is
converted to a quadratic unconstrained binary optimization (QUBO) problem, that
can be efficiently solved on Ising machines. The conversion leverages both
penalty function and Rosenberg order reduction, who together eliminate equality
constraints and reduce high-order loss function into a quadratic one. With some
assumptions, theoretical analysis shows the space complexity of our algorithm
is $\mathcal{O}(H^2L + HLN\log H)$, quantifying the required number of Ising
spins. Finally, the algorithm effectiveness is validated with a simulated Ising
machine on MNIST dataset. After annealing 700 ms, the classification accuracy
achieves 98.3%. Among 100 runs, the success probability of finding the optimal
solution is 72%. Along with the increasing number of spins on Ising machine,
our algorithm has the potential to train deeper neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1&quot;&gt;Xujie Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shengbo Eben Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1&quot;&gt;Jingliang Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenxuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Keqiang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03409">
<title>Visualizing DNA reaction trajectories with deep graph embedding approaches. (arXiv:2311.03409v1 [q-bio.BM])</title>
<link>http://arxiv.org/abs/2311.03409</link>
<description rdf:parseType="Literal">&lt;p&gt;Synthetic biologists and molecular programmers design novel nucleic acid
reactions, with many potential applications. Good visualization tools are
needed to help domain experts make sense of the complex outputs of folding
pathway simulations of such reactions. Here we present ViDa, a new approach for
visualizing DNA reaction folding trajectories over the energy landscape of
secondary structures. We integrate a deep graph embedding model with common
dimensionality reduction approaches, to map high-dimensional data onto 2D
Euclidean space. We assess ViDa on two well-studied and contrasting DNA
hybridization reactions. Our preliminary results suggest that ViDa&apos;s
visualization successfully separates trajectories with different folding
mechanisms, thereby providing useful insight to users, and is a big improvement
over the current state-of-the-art in DNA kinetics visualization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chenwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Duc_K/0/1/0/all/0/1&quot;&gt;Khanh Dao Duc&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Condon_A/0/1/0/all/0/1&quot;&gt;Anne Condon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03410">
<title>DP-DCAN: Differentially Private Deep Contrastive Autoencoder Network for Single-cell Clustering. (arXiv:2311.03410v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.03410</link>
<description rdf:parseType="Literal">&lt;p&gt;Single-cell RNA sequencing (scRNA-seq) is important to transcriptomic
analysis of gene expression. Recently, deep learning has facilitated the
analysis of high-dimensional single-cell data. Unfortunately, deep learning
models may leak sensitive information about users. As a result, Differential
Privacy (DP) is increasingly used to protect privacy. However, existing DP
methods usually perturb whole neural networks to achieve differential privacy,
and hence result in great performance overheads. To address this challenge, in
this paper, we take advantage of the uniqueness of the autoencoder that it
outputs only the dimension-reduced vector in the middle of the network, and
design a Differentially Private Deep Contrastive Autoencoder Network (DP-DCAN)
by partial network perturbation for single-cell clustering. Since only partial
network is added with noise, the performance improvement is obvious and
twofold: one part of network is trained with less noise due to a bigger privacy
budget, and the other part is trained without any noise. Experimental results
of six datasets have verified that DP-DCAN is superior to the traditional DP
scheme with whole network perturbation. Moreover, DP-DCAN demonstrates strong
robustness to adversarial attacks. The code is available at
https://github.com/LFD-byte/DP-DCAN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huifa Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhili Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaomin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haitao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_X/0/1/0/all/0/1&quot;&gt;Xinpeng Ling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03411">
<title>ViDa: Visualizing DNA hybridization trajectories with biophysics-informed deep graph embeddings. (arXiv:2311.03411v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/2311.03411</link>
<description rdf:parseType="Literal">&lt;p&gt;Visualization tools can help synthetic biologists and molecular programmers
understand the complex reactive pathways of nucleic acid reactions, which can
be designed for many potential applications and can be modelled using a
continuous-time Markov chain (CTMC). Here we present ViDa, a new visualization
approach for DNA reaction trajectories that uses a 2D embedding of the
secondary structure state space underlying the CTMC model. To this end, we
integrate a scattering transform of the secondary structure adjacency, a
variational autoencoder, and a nonlinear dimensionality reduction method. We
augment the training loss with domain-specific supervised terms that capture
both thermodynamic and kinetic features. We assess ViDa on two well-studied DNA
hybridization reactions. Our results demonstrate that the domain-specific
features lead to significant quality improvements over the state-of-the-art in
DNA state space visualization, successfully separating different folding
pathways and thus providing useful insights into dominant reaction mechanisms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chenwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lovrod_J/0/1/0/all/0/1&quot;&gt;Jordan Lovrod&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Beronov_B/0/1/0/all/0/1&quot;&gt;Boyan Beronov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Duc_K/0/1/0/all/0/1&quot;&gt;Khanh Dao Duc&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Condon_A/0/1/0/all/0/1&quot;&gt;Anne Condon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03413">
<title>Discret2Di -- Deep Learning based Discretization for Model-based Diagnosis. (arXiv:2311.03413v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.03413</link>
<description rdf:parseType="Literal">&lt;p&gt;Consistency-based diagnosis is an established approach to diagnose technical
applications, but suffers from significant modeling efforts, especially for
dynamic multi-modal time series. Machine learning seems to be an obvious
solution, which becomes less obvious when looking at details: Which notion of
consistency can be used? If logical calculi are still to be used, how can
dynamic time series be transferred into the discrete world?
&lt;/p&gt;
&lt;p&gt;This paper presents the methodology Discret2Di for automated learning of
logical expressions for consistency-based diagnosis. While these logical
calculi have advantages by providing a clear notion of consistency, they have
the key problem of relying on a discretization of the dynamic system. The
solution presented combines machine learning from both the time series and the
symbolic domain to automate the learning of logical rules for consistency-based
diagnosis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moddemann_L/0/1/0/all/0/1&quot;&gt;Lukas Moddemann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steude_H/0/1/0/all/0/1&quot;&gt;Henrik Sebastian Steude&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diedrich_A/0/1/0/all/0/1&quot;&gt;Alexander Diedrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niggemann_O/0/1/0/all/0/1&quot;&gt;Oliver Niggemann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03414">
<title>A Generative Neural Network Approach for 3D Multi-Criteria Design Generation and Optimization of an Engine Mount for an Unmanned Air Vehicle. (arXiv:2311.03414v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.03414</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the most promising developments in computer vision in recent years is
the use of generative neural networks for functionality condition-based 3D
design reconstruction and generation. Here, neural networks learn dependencies
between functionalities and a geometry in a very effective way. For a neural
network the functionalities are translated in conditions to a certain geometry.
But the more conditions the design generation needs to reflect, the more
difficult it is to learn clear dependencies. This leads to a multi criteria
design problem due various conditions, which are not considered in the neural
network structure so far.
&lt;/p&gt;
&lt;p&gt;In this paper, we address this multi-criteria challenge for a 3D design use
case related to an unmanned aerial vehicle (UAV) motor mount. We generate
10,000 abstract 3D designs and subject them all to simulations for three
physical disciplines: mechanics, thermodynamics, and aerodynamics. Then, we
train a Conditional Variational Autoencoder (CVAE) using the geometry and
corresponding multicriteria functional constraints as input. We use our trained
CVAE as well as the Marching cubes algorithm to generate meshes for simulation
based evaluation. The results are then evaluated with the generated UAV
designs. Subsequently, we demonstrate the ability to generate optimized designs
under self-defined functionality conditions using the trained neural network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petroll_C/0/1/0/all/0/1&quot;&gt;Christoph Petroll&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eilermann_S/0/1/0/all/0/1&quot;&gt;Sebastian Eilermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoefer_P/0/1/0/all/0/1&quot;&gt;Philipp Hoefer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niggemann_O/0/1/0/all/0/1&quot;&gt;Oliver Niggemann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03415">
<title>PowerFlowNet: Leveraging Message Passing GNNs for Improved Power Flow Approximation. (arXiv:2311.03415v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.03415</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate and efficient power flow (PF) analysis is crucial in modern
electrical networks&apos; efficient operation and planning. Therefore, there is a
need for scalable algorithms capable of handling large-scale power networks
that can provide accurate and fast solutions. Graph Neural Networks (GNNs) have
emerged as a promising approach for enhancing the speed of PF approximations by
leveraging their ability to capture distinctive features from the underlying
power network graph. In this study, we introduce PowerFlowNet, a novel GNN
architecture for PF approximation that showcases similar performance with the
traditional Newton-Raphson method but achieves it 4 times faster in the simple
IEEE 14-bus system and 145 times faster in the realistic case of the French
high voltage network (6470rte). Meanwhile, it significantly outperforms other
traditional approximation methods, such as the DC relaxation method, in terms
of performance and execution time; therefore, making PowerFlowNet a highly
promising solution for real-world PF analysis. Furthermore, we verify the
efficacy of our approach by conducting an in-depth experimental evaluation,
thoroughly examining the performance, scalability, interpretability, and
architectural dependability of PowerFlowNet. The evaluation provides insights
into the behavior and potential applications of GNNs in power system analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1&quot;&gt;Nan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orfanoudakis_S/0/1/0/all/0/1&quot;&gt;Stavros Orfanoudakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cardenas_N/0/1/0/all/0/1&quot;&gt;Nathan Ordonez Cardenas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giraldo_J/0/1/0/all/0/1&quot;&gt;Juan S. Giraldo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vergara_P/0/1/0/all/0/1&quot;&gt;Pedro P. Vergara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03417">
<title>Federated Learning for Clinical Structured Data: A Benchmark Comparison of Engineering and Statistical Approaches. (arXiv:2311.03417v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.03417</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) has shown promising potential in safeguarding data
privacy in healthcare collaborations. While the term &quot;FL&quot; was originally coined
by the engineering community, the statistical field has also explored similar
privacy-preserving algorithms. Statistical FL algorithms, however, remain
considerably less recognized than their engineering counterparts. Our goal was
to bridge the gap by presenting the first comprehensive comparison of FL
frameworks from both engineering and statistical domains. We evaluated five FL
frameworks using both simulated and real-world data. The results indicate that
statistical FL algorithms yield less biased point estimates for model
coefficients and offer convenient confidence interval estimations. In contrast,
engineering-based methods tend to generate more accurate predictions, sometimes
surpassing central pooled and statistical FL models. This study underscores the
relative strengths and weaknesses of both types of methods, emphasizing the
need for increased awareness and their integration in future FL applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Siqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_D/0/1/0/all/0/1&quot;&gt;Di Miao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qiming Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1&quot;&gt;Chuan Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DAgostino_D/0/1/0/all/0/1&quot;&gt;Danny D&amp;#x27;Agostino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_Y/0/1/0/all/0/1&quot;&gt;Yilin Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_Y/0/1/0/all/0/1&quot;&gt;Yuqing Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Huazhu Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ong_M/0/1/0/all/0/1&quot;&gt;Marcus Eng Hock Ong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haddadi_H/0/1/0/all/0/1&quot;&gt;Hamed Haddadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Nan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03424">
<title>Using Symmetries to Lift Satisfiability Checking. (arXiv:2311.03424v1 [cs.LO])</title>
<link>http://arxiv.org/abs/2311.03424</link>
<description rdf:parseType="Literal">&lt;p&gt;We analyze how symmetries can be used to compress structures (also known as
interpretations) onto a smaller domain without loss of information. This
analysis suggests the possibility to solve satisfiability problems in the
compressed domain for better performance. Thus, we propose a 2-step novel
method: (i) the sentence to be satisfied is automatically translated into an
equisatisfiable sentence over a ``lifted&apos;&apos; vocabulary that allows domain
compression; (ii) satisfiability of the lifted sentence is checked by growing
the (initially unknown) compressed domain until a satisfying structure is
found. The key issue is to ensure that this satisfying structure can always be
expanded into an uncompressed structure that satisfies the original sentence to
be satisfied. We present an adequate translation for sentences in typed
first-order logic extended with aggregates. Our experimental evaluation shows
large speedups for generative configuration problems. The method also has
applications in the verification of software operating on complex data
structures. Further refinements of the translation are left for future work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carbonnelle_P/0/1/0/all/0/1&quot;&gt;Pierre Carbonnelle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schenner_G/0/1/0/all/0/1&quot;&gt;Gottfried Schenner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bruynooghe_M/0/1/0/all/0/1&quot;&gt;Maurice Bruynooghe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bogaerts_B/0/1/0/all/0/1&quot;&gt;Bart Bogaerts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denecker_M/0/1/0/all/0/1&quot;&gt;Marc Denecker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03425">
<title>An AI-Guided Data Centric Strategy to Detect and Mitigate Biases in Healthcare Datasets. (arXiv:2311.03425v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.03425</link>
<description rdf:parseType="Literal">&lt;p&gt;The adoption of diagnosis and prognostic algorithms in healthcare has led to
concerns about the perpetuation of bias against disadvantaged groups of
individuals. Deep learning methods to detect and mitigate bias have revolved
around modifying models, optimization strategies, and threshold calibration
with varying levels of success. Here, we generate a data-centric,
model-agnostic, task-agnostic approach to evaluate dataset bias by
investigating the relationship between how easily different groups are learned
at small sample sizes (AEquity). We then apply a systematic analysis of AEq
values across subpopulations to identify and mitigate manifestations of racial
bias in two known cases in healthcare - Chest X-rays diagnosis with deep
convolutional neural networks and healthcare utilization prediction with
multivariate logistic regression. AEq is a novel and broadly applicable metric
that can be applied to advance equity by diagnosing and remediating bias in
healthcare datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gulamali_F/0/1/0/all/0/1&quot;&gt;Faris F. Gulamali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sawant_A/0/1/0/all/0/1&quot;&gt;Ashwin S. Sawant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liharska_L/0/1/0/all/0/1&quot;&gt;Lora Liharska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horowitz_C/0/1/0/all/0/1&quot;&gt;Carol R. Horowitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_L/0/1/0/all/0/1&quot;&gt;Lili Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kovatch_P/0/1/0/all/0/1&quot;&gt;Patricia H. Kovatch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hofer_I/0/1/0/all/0/1&quot;&gt;Ira Hofer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1&quot;&gt;Karandeep Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richardson_L/0/1/0/all/0/1&quot;&gt;Lynne D. Richardson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mensah_E/0/1/0/all/0/1&quot;&gt;Emmanuel Mensah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Charney_A/0/1/0/all/0/1&quot;&gt;Alexander W Charney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reich_D/0/1/0/all/0/1&quot;&gt;David L. Reich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jianying Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nadkarni_G/0/1/0/all/0/1&quot;&gt;Girish N. Nadkarni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03426">
<title>GQKVA: Efficient Pre-training of Transformers by Grouping Queries, Keys, and Values. (arXiv:2311.03426v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.03426</link>
<description rdf:parseType="Literal">&lt;p&gt;Massive transformer-based models face several challenges, including slow and
computationally intensive pre-training and over-parametrization. This paper
addresses these challenges by proposing a versatile method called GQKVA, which
generalizes query, key, and value grouping techniques. GQKVA is designed to
speed up transformer pre-training while reducing the model size. Our
experiments with various GQKVA variants highlight a clear trade-off between
performance and model size, allowing for customized choices based on resource
and time limitations. Our findings also indicate that the conventional
multi-head attention approach is not always the best choice, as there are
lighter and faster alternatives available. We tested our method on ViT, which
achieved an approximate 0.3% increase in accuracy while reducing the model size
by about 4% in the task of image classification. Additionally, our most
aggressive model reduction experiment resulted in a reduction of approximately
15% in model size, with only around a 1% drop in accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Javadi_F/0/1/0/all/0/1&quot;&gt;Farnoosh Javadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_W/0/1/0/all/0/1&quot;&gt;Walid Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hajimolahoseini_H/0/1/0/all/0/1&quot;&gt;Habib Hajimolahoseini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ataiefard_F/0/1/0/all/0/1&quot;&gt;Foozhan Ataiefard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassanpour_M/0/1/0/all/0/1&quot;&gt;Mohammad Hassanpour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asani_S/0/1/0/all/0/1&quot;&gt;Saina Asani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_A/0/1/0/all/0/1&quot;&gt;Austin Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awad_O/0/1/0/all/0/1&quot;&gt;Omar Mohamed Awad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kangling Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03429">
<title>ProPath: Disease-Specific Protein Language Model for Variant Pathogenicity. (arXiv:2311.03429v1 [q-bio.GN])</title>
<link>http://arxiv.org/abs/2311.03429</link>
<description rdf:parseType="Literal">&lt;p&gt;Clinical variant classification of pathogenic versus benign genetic variants
remains a pivotal challenge in clinical genetics. Recently, the proposition of
protein language models has improved the generic variant effect prediction
(VEP) accuracy via weakly-supervised or unsupervised training. However, these
VEPs are not disease-specific, limiting their adaptation at point-of-care. To
address this problem, we propose a disease-specific \textsc{pro}tein language
model for variant \textsc{path}ogenicity, termed ProPath, to capture the
pseudo-log-likelihood ratio in rare missense variants through a siamese
network. We evaluate the performance of ProPath against pre-trained language
models, using clinical variant sets in inherited cardiomyopathies and
arrhythmias that were not seen during training. Our results demonstrate that
ProPath surpasses the pre-trained ESM1b with an over $5\%$ improvement in AUC
across both datasets. Furthermore, our model achieved the highest performances
across all baselines for both datasets. Thus, our ProPath offers a potent
disease-specific variant effect prediction, particularly valuable for disease
associations and clinical applicability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhan_H/0/1/0/all/0/1&quot;&gt;Huixin Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zijun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03468">
<title>FinA: Fairness of Adverse Effects in Decision-Making of Human-Cyber-Physical-System. (arXiv:2311.03468v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.03468</link>
<description rdf:parseType="Literal">&lt;p&gt;Ensuring fairness in decision-making systems within
Human-Cyber-Physical-Systems (HCPS) is a pressing concern, particularly when
diverse individuals, each with varying behaviors and expectations, coexist
within the same application space, influenced by a shared set of control
actions in the system. The long-term adverse effects of these actions further
pose the challenge, as historical experiences and interactions shape individual
perceptions of fairness. This paper addresses the challenge of fairness from an
equity perspective of adverse effects, taking into account the dynamic nature
of human behavior and evolving preferences while recognizing the lasting impact
of adverse effects. We formally introduce the concept of
Fairness-in-Adverse-Effects (FinA) within the HCPS context. We put forth a
comprehensive set of five formulations for FinA, encompassing both the
instantaneous and long-term aspects of adverse effects. To empirically validate
the effectiveness of our FinA approach, we conducted an evaluation within the
domain of smart homes, a pertinent HCPS application. The outcomes of our
evaluation demonstrate that the adoption of FinA significantly enhances the
overall perception of fairness among individuals, yielding an average
improvement of 66.7% when compared to the state-of-the-art method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tianyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elmalaki_S/0/1/0/all/0/1&quot;&gt;Salma Elmalaki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03478">
<title>Multi Loss-based Feature Fusion and Top Two Voting Ensemble Decision Strategy for Facial Expression Recognition in the Wild. (arXiv:2311.03478v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03478</link>
<description rdf:parseType="Literal">&lt;p&gt;Facial expression recognition (FER) in the wild is a challenging task
affected by the image quality and has attracted broad interest in computer
vision. There is no research using feature fusion and ensemble strategy for FER
simultaneously. Different from previous studies, this paper applies both
internal feature fusion for a single model and feature fusion among multiple
networks, as well as the ensemble strategy. This paper proposes one novel
single model named R18+FAML, as well as one ensemble model named
R18+FAML-FGA-T2V to improve the performance of the FER in the wild. Based on
the structure of ResNet18 (R18), R18+FAML combines internal Feature fusion and
three Attention blocks using Multiple Loss functions (FAML) to improve the
diversity of the feature extraction. To improve the performance of R18+FAML, we
propose a Feature fusion among networks based on the Genetic Algorithm (FGA),
which can fuse the convolution kernels for feature extraction of multiple
networks. On the basis of R18+FAML and FGA, we propose one ensemble strategy,
i.e., the Top Two Voting (T2V) to support the classification of FER, which can
consider more classification information comprehensively. Combining the above
strategies, R18+FAML-FGA-T2V can focus on the main expression-aware areas.
Extensive experiments demonstrate that our single model R18+FAML and the
ensemble model R18+FAML-FGA-T2V achieve the accuracies of $\left( 90.32, 62.17,
65.83 \right)\%$ and $\left( 91.59, 63.27, 66.63 \right)\%$ on three
challenging unbalanced FER datasets RAF-DB, AffectNet-8 and AffectNet-7
respectively, both outperforming the state-of-the-art results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1&quot;&gt;Guangyao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yuanlun Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_W/0/1/0/all/0/1&quot;&gt;Wenhong Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03485">
<title>CLIP-Motion: Learning Reward Functions for Robotic Actions Using Consecutive Observations. (arXiv:2311.03485v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.03485</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel method for learning reward functions for robotic
motions by harnessing the power of a CLIP-based model. Traditional reward
function design often hinges on manual feature engineering, which can struggle
to generalize across an array of tasks. Our approach circumvents this challenge
by capitalizing on CLIP&apos;s capability to process both state features and image
inputs effectively. Given a pair of consecutive observations, our model excels
in identifying the motion executed between them. We showcase results spanning
various robotic activities, such as directing a gripper to a designated target
and adjusting the position of a cube. Through experimental evaluations, we
underline the proficiency of our method in precisely deducing motion and its
promise to enhance reinforcement learning training in the realm of robotics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dang_X/0/1/0/all/0/1&quot;&gt;Xuzhe Dang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Edelkamp_S/0/1/0/all/0/1&quot;&gt;Stefan Edelkamp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribault_N/0/1/0/all/0/1&quot;&gt;Nicolas Ribault&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03488">
<title>Multi-Resolution Diffusion for Privacy-Sensitive Recommender Systems. (arXiv:2311.03488v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2311.03488</link>
<description rdf:parseType="Literal">&lt;p&gt;While recommender systems have become an integral component of the Web
experience, their heavy reliance on user data raises privacy and security
concerns. Substituting user data with synthetic data can address these
concerns, but accurately replicating these real-world datasets has been a
notoriously challenging problem. Recent advancements in generative AI have
demonstrated the impressive capabilities of diffusion models in generating
realistic data across various domains. In this work we introduce a Score-based
Diffusion Recommendation Model (SDRM), which captures the intricate patterns of
real-world datasets required for training highly accurate recommender systems.
SDRM allows for the generation of synthetic data that can replace existing
datasets to preserve user privacy, or augment existing datasets to address
excessive data sparsity. Our method outperforms competing baselines such as
generative adversarial networks, variational autoencoders, and recently
proposed diffusion models in synthesizing various datasets to replace or
augment the original data by an average improvement of 4.30% in Recall@$n$ and
4.65% in NDCG@$n$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lilienthal_D/0/1/0/all/0/1&quot;&gt;Derek Lilienthal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mello_P/0/1/0/all/0/1&quot;&gt;Paul Mello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eirinaki_M/0/1/0/all/0/1&quot;&gt;Magdalini Eirinaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tiomkin_S/0/1/0/all/0/1&quot;&gt;Stas Tiomkin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03508">
<title>Astrocytes as a mechanism for meta-plasticity and contextually-guided network function. (arXiv:2311.03508v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/2311.03508</link>
<description rdf:parseType="Literal">&lt;p&gt;Astrocytes are a highly expressed and highly enigmatic cell-type in the
mammalian brain. Traditionally viewed as a mediator of basic physiological
sustenance, it is increasingly recognized that astrocytes may play a more
direct role in neural computation. A conceptual challenge to this idea is the
fact that astrocytic activity takes a very different form than that of neurons,
and in particular, occurs at orders-of-magnitude slower time-scales. In the
current paper, we engage how such time-scale separation may endow astrocytes
with the capability to enable learning in context-dependent settings, where
fluctuations in task parameters may occur much more slowly than within-task
requirements. This idea is based on the recent supposition that astrocytes,
owing to their sensitivity to a host of physiological covariates, may be
particularly well poised to modulate the dynamics of neural circuits in
functionally salient ways. We pose a general model of neural-synaptic-astrocyte
interaction and use formal analysis to characterize how astrocytic modulation
may constitute a form of meta-plasticity, altering the ways in which synapses
and neurons adapt as a function of time. We then embed this model in a
bandit-based reinforcement learning task environment, and show how the presence
of time-scale separated astrocytic modulation enables learning over multiple
fluctuating contexts. Indeed, these networks learn far more reliably versus
dynamically homogenous networks and conventional non-network-based bandit
algorithms. Our results indicate how the presence of neural-astrocyte
interaction in the brain may benefit learning over different time-scale and the
conveyance of task relevant contextual information onto circuit dynamics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gong_L/0/1/0/all/0/1&quot;&gt;Lulu Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Pasqualetti_F/0/1/0/all/0/1&quot;&gt;Fabio Pasqualetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Papouin_T/0/1/0/all/0/1&quot;&gt;Thomas Papouin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ching_S/0/1/0/all/0/1&quot;&gt;ShiNung Ching&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03509">
<title>MFAAN: Unveiling Audio Deepfakes with a Multi-Feature Authenticity Network. (arXiv:2311.03509v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2311.03509</link>
<description rdf:parseType="Literal">&lt;p&gt;In the contemporary digital age, the proliferation of deepfakes presents a
formidable challenge to the sanctity of information dissemination. Audio
deepfakes, in particular, can be deceptively realistic, posing significant
risks in misinformation campaigns. To address this threat, we introduce the
Multi-Feature Audio Authenticity Network (MFAAN), an advanced architecture
tailored for the detection of fabricated audio content. MFAAN incorporates
multiple parallel paths designed to harness the strengths of different audio
representations, including Mel-frequency cepstral coefficients (MFCC),
linear-frequency cepstral coefficients (LFCC), and Chroma Short Time Fourier
Transform (Chroma-STFT). By synergistically fusing these features, MFAAN
achieves a nuanced understanding of audio content, facilitating robust
differentiation between genuine and manipulated recordings. Preliminary
evaluations of MFAAN on two benchmark datasets, &apos;In-the-Wild&apos; Audio Deepfake
Data and The Fake-or-Real Dataset, demonstrate its superior performance,
achieving accuracies of 98.93% and 94.47% respectively. Such results not only
underscore the efficacy of MFAAN but also highlight its potential as a pivotal
tool in the ongoing battle against deepfake audio content.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnan_K/0/1/0/all/0/1&quot;&gt;Karthik Sivarama Krishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnan_K/0/1/0/all/0/1&quot;&gt;Koushik Sivarama Krishnan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03520">
<title>Brain Networks and Intelligence: A Graph Neural Network Based Approach to Resting State fMRI Data. (arXiv:2311.03520v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.03520</link>
<description rdf:parseType="Literal">&lt;p&gt;Resting-state functional magnetic resonance imaging (rsfMRI) is a powerful
tool for investigating the relationship between brain function and cognitive
processes as it allows for the functional organization of the brain to be
captured without relying on a specific task or stimuli. In this paper, we
present a novel modeling architecture called BrainRGIN for predicting
intelligence (fluid, crystallized, and total intelligence) using graph neural
networks on rsfMRI derived static functional network connectivity matrices.
Extending from the existing graph convolution networks, our approach
incorporates a clustering-based embedding and graph isomorphism network in the
graph convolutional layer to reflect the nature of the brain sub-network
organization and efficient network expression, in combination with TopK pooling
and attention-based readout functions. We evaluated our proposed architecture
on a large dataset, specifically the Adolescent Brain Cognitive Development
Dataset, and demonstrated its effectiveness in predicting individual
differences in intelligence. Our model achieved lower mean squared errors and
higher correlation scores than existing relevant graph architectures and other
traditional machine learning models for all of the intelligence prediction
tasks. The middle frontal gyrus exhibited a significant contribution to both
fluid and crystallized intelligence, suggesting their pivotal role in these
cognitive processes. Total composite scores identified a diverse set of brain
regions to be relevant which underscores the complex nature of total
intelligence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thapaliya_B/0/1/0/all/0/1&quot;&gt;Bishal Thapaliya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akbas_E/0/1/0/all/0/1&quot;&gt;Esra Akbas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiayu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sapkota_R/0/1/0/all/0/1&quot;&gt;Raam Sapkota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_B/0/1/0/all/0/1&quot;&gt;Bhaskar Ray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suresh_P/0/1/0/all/0/1&quot;&gt;Pranav Suresh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calhoun_V/0/1/0/all/0/1&quot;&gt;Vince Calhoun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jingyu Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03534">
<title>PcLast: Discovering Plannable Continuous Latent States. (arXiv:2311.03534v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.03534</link>
<description rdf:parseType="Literal">&lt;p&gt;Goal-conditioned planning benefits from learned low-dimensional
representations of rich, high-dimensional observations. While compact latent
representations, typically learned from variational autoencoders or inverse
dynamics, enable goal-conditioned planning they ignore state affordances, thus
hampering their sample-efficient planning capabilities. In this paper, we learn
a representation that associates reachable states together for effective onward
planning. We first learn a latent representation with multi-step inverse
dynamics (to remove distracting information); and then transform this
representation to associate reachable states together in $\ell_2$ space. Our
proposals are rigorously tested in various simulation testbeds. Numerical
results in reward-based and reward-free settings show significant improvements
in sampling efficiency, and yields layered state abstractions that enable
computationally efficient hierarchical planning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koul_A/0/1/0/all/0/1&quot;&gt;Anurag Koul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sujit_S/0/1/0/all/0/1&quot;&gt;Shivakanth Sujit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shaoru Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Evans_B/0/1/0/all/0/1&quot;&gt;Ben Evans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Lili Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1&quot;&gt;Byron Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chari_R/0/1/0/all/0/1&quot;&gt;Rajan Chari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_R/0/1/0/all/0/1&quot;&gt;Riashat Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seraj_R/0/1/0/all/0/1&quot;&gt;Raihan Seraj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Efroni_Y/0/1/0/all/0/1&quot;&gt;Yonathan Efroni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molu_L/0/1/0/all/0/1&quot;&gt;Lekan Molu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dudik_M/0/1/0/all/0/1&quot;&gt;Miro Dudik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Langford_J/0/1/0/all/0/1&quot;&gt;John Langford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lamb_A/0/1/0/all/0/1&quot;&gt;Alex Lamb&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03547">
<title>InterVLS: Interactive Model Understanding and Improvement with Vision-Language Surrogates. (arXiv:2311.03547v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.03547</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models are widely used in critical applications, highlighting
the need for pre-deployment model understanding and improvement. Visual
concept-based methods, while increasingly used for this purpose, face
challenges: (1) most concepts lack interpretability, (2) existing methods
require model knowledge, often unavailable at run time. Additionally, (3) there
lacks a no-code method for post-understanding model improvement. Addressing
these, we present InterVLS. The system facilitates model understanding by
discovering text-aligned concepts, measuring their influence with
model-agnostic linear surrogates. Employing visual analytics, InterVLS offers
concept-based explanations and performance insights. It enables users to adjust
concept influences to update a model, facilitating no-code model improvement.
We evaluate InterVLS in a user study, illustrating its functionality with two
scenarios. Results indicates that InterVLS is effective to help users identify
influential concepts to a model, gain insights and adjust concept influence to
improve the model. We conclude with a discussion based on our study results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jinbin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1&quot;&gt;Wenbin He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gou_L/0/1/0/all/0/1&quot;&gt;Liang Gou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1&quot;&gt;Liu Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bryan_C/0/1/0/all/0/1&quot;&gt;Chris Bryan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03550">
<title>United We Stand, Divided We Fall: UnityGraph for Unsupervised Procedure Learning from Videos. (arXiv:2311.03550v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03550</link>
<description rdf:parseType="Literal">&lt;p&gt;Given multiple videos of the same task, procedure learning addresses
identifying the key-steps and determining their order to perform the task. For
this purpose, existing approaches use the signal generated from a pair of
videos. This makes key-steps discovery challenging as the algorithms lack
inter-videos perspective. Instead, we propose an unsupervised Graph-based
Procedure Learning (GPL) framework. GPL consists of the novel UnityGraph that
represents all the videos of a task as a graph to obtain both intra-video and
inter-videos context. Further, to obtain similar embeddings for the same
key-steps, the embeddings of UnityGraph are updated in an unsupervised manner
using the Node2Vec algorithm. Finally, to identify the key-steps, we cluster
the embeddings using KMeans. We test GPL on benchmark ProceL, CrossTask, and
EgoProceL datasets and achieve an average improvement of 2% on third-person
datasets and 3.6% on EgoProceL over the state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_S/0/1/0/all/0/1&quot;&gt;Siddhant Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_C/0/1/0/all/0/1&quot;&gt;Chetan Arora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1&quot;&gt;C.V. Jawahar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03551">
<title>Context Unlocks Emotions: Text-based Emotion Classification Dataset Auditing with Large Language Models. (arXiv:2311.03551v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.03551</link>
<description rdf:parseType="Literal">&lt;p&gt;The lack of contextual information in text data can make the annotation
process of text-based emotion classification datasets challenging. As a result,
such datasets often contain labels that fail to consider all the relevant
emotions in the vocabulary. This misalignment between text inputs and labels
can degrade the performance of machine learning models trained on top of them.
As re-annotating entire datasets is a costly and time-consuming task that
cannot be done at scale, we propose to use the expressive capabilities of large
language models to synthesize additional context for input text to increase its
alignment with the annotated emotional labels. In this work, we propose a
formal definition of textual context to motivate a prompting strategy to
enhance such contextual information. We provide both human and empirical
evaluation to demonstrate the efficacy of the enhanced context. Our method
improves alignment between inputs and their human-annotated labels from both an
empirical and human-evaluated standpoint.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Daniel Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kommineni_A/0/1/0/all/0/1&quot;&gt;Aditya Kommineni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alshehri_M/0/1/0/all/0/1&quot;&gt;Mohammad Alshehri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohanty_N/0/1/0/all/0/1&quot;&gt;Nilamadhab Mohanty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Modi_V/0/1/0/all/0/1&quot;&gt;Vedant Modi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gratch_J/0/1/0/all/0/1&quot;&gt;Jonathan Gratch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1&quot;&gt;Shrikanth Narayanan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03564">
<title>Low-Rank MDPs with Continuous Action Spaces. (arXiv:2311.03564v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.03564</link>
<description rdf:parseType="Literal">&lt;p&gt;Low-Rank Markov Decision Processes (MDPs) have recently emerged as a
promising framework within the domain of reinforcement learning (RL), as they
allow for provably approximately correct (PAC) learning guarantees while also
incorporating ML algorithms for representation learning. However, current
methods for low-rank MDPs are limited in that they only consider finite action
spaces, and give vacuous bounds as $|\mathcal{A}| \to \infty$, which greatly
limits their applicability. In this work, we study the problem of extending
such methods to settings with continuous actions, and explore multiple concrete
approaches for performing this extension. As a case study, we consider the
seminal FLAMBE algorithm (Agarwal et al., 2020), which is a reward-agnostic
method for PAC RL with low-rank MDPs. We show that, without any modifications
to the algorithm, we obtain similar PAC bound when actions are allowed to be
continuous. Specifically, when the model for transition functions satisfies a
Holder smoothness condition w.r.t. actions, and either the policy class has a
uniformly bounded minimum density or the reward function is also Holder smooth,
we obtain a polynomial PAC bound that depends on the order of smoothness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bennett_A/0/1/0/all/0/1&quot;&gt;Andrew Bennett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kallus_N/0/1/0/all/0/1&quot;&gt;Nathan Kallus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oprescu_M/0/1/0/all/0/1&quot;&gt;Miruna Oprescu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03567">
<title>Inclusive Portraits: Race-Aware Human-in-the-Loop Technology. (arXiv:2311.03567v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2311.03567</link>
<description rdf:parseType="Literal">&lt;p&gt;AI has revolutionized the processing of various services, including the
automatic facial verification of people. Automated approaches have demonstrated
their speed and efficiency in verifying a large volume of faces, but they can
face challenges when processing content from certain communities, including
communities of people of color. This challenge has prompted the adoption of
&quot;human-in-the-loop&quot; (HITL) approaches, where human workers collaborate with the
AI to minimize errors. However, most HITL approaches do not consider workers&apos;
individual characteristics and backgrounds. This paper proposes a new approach,
called Inclusive Portraits (IP), that connects with social theories around race
to design a racially-aware human-in-the-loop system. Our experiments have
provided evidence that incorporating race into human-in-the-loop (HITL) systems
for facial verification can significantly enhance performance, especially for
services delivered to people of color. Our findings also highlight the
importance of considering individual worker characteristics in the design of
HITL systems, rather than treating workers as a homogenous group. Our research
has significant design implications for developing AI-enhanced services that
are more inclusive and equitable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flores_Saviaga_C/0/1/0/all/0/1&quot;&gt;Claudia Flores-Saviaga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Curtis_C/0/1/0/all/0/1&quot;&gt;Christopher Curtis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savage_S/0/1/0/all/0/1&quot;&gt;Saiph Savage&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03583">
<title>Finding Increasingly Large Extremal Graphs with AlphaZero and Tabu Search. (arXiv:2311.03583v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.03583</link>
<description rdf:parseType="Literal">&lt;p&gt;This work studies a central extremal graph theory problem inspired by a 1975
conjecture of Erd\H{o}s, which aims to find graphs with a given size (number of
nodes) that maximize the number of edges without having 3- or 4-cycles. We
formulate this problem as a sequential decision-making problem and compare
AlphaZero, a neural network-guided tree search, with tabu search, a heuristic
local search method. Using either method, by introducing a curriculum --
jump-starting the search for larger graphs using good graphs found at smaller
sizes -- we improve the state-of-the-art lower bounds for several sizes. We
also propose a flexible graph-generation environment and a
permutation-invariant network architecture for learning to search in the space
of graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehrabian_A/0/1/0/all/0/1&quot;&gt;Abbas Mehrabian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1&quot;&gt;Ankit Anand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyunjik Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sonnerat_N/0/1/0/all/0/1&quot;&gt;Nicolas Sonnerat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balog_M/0/1/0/all/0/1&quot;&gt;Matej Balog&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Comanici_G/0/1/0/all/0/1&quot;&gt;Gheorghe Comanici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berariu_T/0/1/0/all/0/1&quot;&gt;Tudor Berariu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1&quot;&gt;Andrew Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruoss_A/0/1/0/all/0/1&quot;&gt;Anian Ruoss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bulanova_A/0/1/0/all/0/1&quot;&gt;Anna Bulanova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toyama_D/0/1/0/all/0/1&quot;&gt;Daniel Toyama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blackwell_S/0/1/0/all/0/1&quot;&gt;Sam Blackwell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paredes_B/0/1/0/all/0/1&quot;&gt;Bernardino Romera Paredes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Velickovic_P/0/1/0/all/0/1&quot;&gt;Petar Veli&amp;#x10d;kovi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orseau_L/0/1/0/all/0/1&quot;&gt;Laurent Orseau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Joonkyung Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naredla_A/0/1/0/all/0/1&quot;&gt;Anurag Murty Naredla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1&quot;&gt;Doina Precup&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagner_A/0/1/0/all/0/1&quot;&gt;Adam Zsolt Wagner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03595">
<title>Brief for the Canada House of Commons Study on the Implications of Artificial Intelligence Technologies for the Canadian Labor Force: Generative Artificial Intelligence Shatters Models of AI and Labor. (arXiv:2311.03595v1 [econ.GN])</title>
<link>http://arxiv.org/abs/2311.03595</link>
<description rdf:parseType="Literal">&lt;p&gt;Exciting advances in generative artificial intelligence (AI) have sparked
concern for jobs, education, productivity, and the future of work. As with past
technologies, generative AI may not lead to mass unemployment. But, unlike past
technologies, generative AI is creative, cognitive, and potentially ubiquitous
which makes the usual assumptions of automation predictions ill-suited for
today. Existing projections suggest that generative AI will impact workers in
occupations that were previously considered immune to automation. As AI&apos;s full
set of capabilities and applications emerge, policy makers should promote
workers&apos; career adaptability. This goal requires improved data on job
separations and unemployment by locality and job titles in order to identify
early-indicators for the workers facing labor disruption. Further, prudent
policy should incentivize education programs to accommodate learning with AI as
a tool while preparing students for the demands of the future of work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Frank_M/0/1/0/all/0/1&quot;&gt;Morgan R. Frank&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03606">
<title>Multimodal Stress Detection Using Facial Landmarks and Biometric Signals. (arXiv:2311.03606v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03606</link>
<description rdf:parseType="Literal">&lt;p&gt;The development of various sensing technologies is improving measurements of
stress and the well-being of individuals. Although progress has been made with
single signal modalities like wearables and facial emotion recognition,
integrating multiple modalities provides a more comprehensive understanding of
stress, given that stress manifests differently across different people.
Multi-modal learning aims to capitalize on the strength of each modality rather
than relying on a single signal. Given the complexity of processing and
integrating high-dimensional data from limited subjects, more research is
needed. Numerous research efforts have been focused on fusing stress and
emotion signals at an early stage, e.g., feature-level fusion using basic
machine learning methods and 1D-CNN Methods. This paper proposes a multi-modal
learning approach for stress detection that integrates facial landmarks and
biometric signals. We test this multi-modal integration with various
early-fusion and late-fusion techniques to integrate the 1D-CNN model from
biometric signals and 2-D CNN using facial landmarks. We evaluate these
architectures using a rigorous test of models&apos; generalizability using the
leave-one-subject-out mechanism, i.e., all samples related to a single subject
are left out to train the model. Our findings show that late-fusion achieved
94.39\% accuracy, and early-fusion surpassed it with a 98.38\% accuracy rate.
This research contributes valuable insights into enhancing stress detection
through a multi-modal approach. The proposed research offers important
knowledge in improving stress detection using a multi-modal approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hosseini_M/0/1/0/all/0/1&quot;&gt;Majid Hosseini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bodaghi_M/0/1/0/all/0/1&quot;&gt;Morteza Bodaghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhupatiraju_R/0/1/0/all/0/1&quot;&gt;Ravi Teja Bhupatiraju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maida_A/0/1/0/all/0/1&quot;&gt;Anthony Maida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gottumukkala_R/0/1/0/all/0/1&quot;&gt;Raju Gottumukkala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03622">
<title>TWIST: Teacher-Student World Model Distillation for Efficient Sim-to-Real Transfer. (arXiv:2311.03622v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.03622</link>
<description rdf:parseType="Literal">&lt;p&gt;Model-based RL is a promising approach for real-world robotics due to its
improved sample efficiency and generalization capabilities compared to
model-free RL. However, effective model-based RL solutions for vision-based
real-world applications require bridging the sim-to-real gap for any world
model learnt. Due to its significant computational cost, standard domain
randomisation does not provide an effective solution to this problem. This
paper proposes TWIST (Teacher-Student World Model Distillation for Sim-to-Real
Transfer) to achieve efficient sim-to-real transfer of vision-based model-based
RL using distillation. Specifically, TWIST leverages state observations as
readily accessible, privileged information commonly garnered from a simulator
to significantly accelerate sim-to-real transfer. Specifically, a teacher world
model is trained efficiently on state information. At the same time, a matching
dataset is collected of domain-randomised image observations. The teacher world
model then supervises a student world model that takes the domain-randomised
image observations as input. By distilling the learned latent dynamics model
from the teacher to the student model, TWIST achieves efficient and effective
sim-to-real transfer for vision-based model-based RL tasks. Experiments in
simulated and real robotics tasks demonstrate that our approach outperforms
naive domain randomisation and model-free methods in terms of sample efficiency
and task performance of sim-to-real transfer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamada_J/0/1/0/all/0/1&quot;&gt;Jun Yamada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rigter_M/0/1/0/all/0/1&quot;&gt;Marc Rigter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Collins_J/0/1/0/all/0/1&quot;&gt;Jack Collins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Posner_I/0/1/0/all/0/1&quot;&gt;Ingmar Posner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03636">
<title>Analysis of the User Perception of Chatbots in Education Using A Partial Least Squares Structural Equation Modeling Approach. (arXiv:2311.03636v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2311.03636</link>
<description rdf:parseType="Literal">&lt;p&gt;The integration of Artificial Intelligence (AI) into education is a recent
development, with chatbots emerging as a noteworthy addition to this
transformative landscape. As online learning platforms rapidly advance,
students need to adapt swiftly to excel in this dynamic environment.
Consequently, understanding the acceptance of chatbots, particularly those
employing Large Language Model (LLM) such as Chat Generative Pretrained
Transformer (ChatGPT), Google Bard, and other interactive AI technologies, is
of paramount importance. However, existing research on chatbots in education
has overlooked key behavior-related aspects, such as Optimism, Innovativeness,
Discomfort, Insecurity, Transparency, Ethics, Interaction, Engagement, and
Accuracy, creating a significant literature gap. To address this gap, this
study employs Partial Least Squares Structural Equation Modeling (PLS-SEM) to
investigate the determinant of chatbots adoption in education among students,
considering the Technology Readiness Index (TRI) and Technology Acceptance
Model (TAM). Utilizing a five-point Likert scale for data collection, we
gathered a total of 185 responses, which were analyzed using R-Studio software.
We established 12 hypotheses to achieve its objectives. The results showed that
Optimism and Innovativeness are positively associated with Perceived Ease of
Use (PEOU) and Perceived Usefulness (PU). Conversely, Discomfort and Insecurity
negatively impact PEOU, with only Insecurity negatively affecting PU. These
findings provide insights for future technology designers, elucidating critical
user behavior factors influencing chatbots adoption and utilization in
educational contexts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1&quot;&gt;Md Rabiul Hasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_N/0/1/0/all/0/1&quot;&gt;Nahian Ismail Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1&quot;&gt;Md Hadisur Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Syed_M/0/1/0/all/0/1&quot;&gt;Md Asif Bin Syed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryu_J/0/1/0/all/0/1&quot;&gt;JuHyeong Ryu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03651">
<title>SeRO: Self-Supervised Reinforcement Learning for Recovery from Out-of-Distribution Situations. (arXiv:2311.03651v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.03651</link>
<description rdf:parseType="Literal">&lt;p&gt;Robotic agents trained using reinforcement learning have the problem of
taking unreliable actions in an out-of-distribution (OOD) state. Agents can
easily become OOD in real-world environments because it is almost impossible
for them to visit and learn the entire state space during training.
Unfortunately, unreliable actions do not ensure that agents perform their
original tasks successfully. Therefore, agents should be able to recognize
whether they are in OOD states and learn how to return to the learned state
distribution rather than continue to take unreliable actions. In this study, we
propose a novel method for retraining agents to recover from OOD situations in
a self-supervised manner when they fall into OOD states. Our in-depth
experimental results demonstrate that our method substantially improves the
agent&apos;s ability to recover from OOD situations in terms of sample efficiency
and restoration of the performance for the original tasks. Moreover, we show
that our method can retrain the agent to recover from OOD situations even when
in-distribution states are difficult to visit through exploration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1&quot;&gt;Chan Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1&quot;&gt;Jaekyung Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bobda_C/0/1/0/all/0/1&quot;&gt;Christophe Bobda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1&quot;&gt;Seung-Woo Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seong-Woo Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03652">
<title>Machine Learning Parameterization of the Multi-scale Kain-Fritsch (MSKF) Convection Scheme. (arXiv:2311.03652v1 [physics.ao-ph])</title>
<link>http://arxiv.org/abs/2311.03652</link>
<description rdf:parseType="Literal">&lt;p&gt;Warm-sector heavy rainfall often occurs along the coast of South China, and
it is usually localized and long-lasting, making it challenging to predict.
High-resolution numerical weather prediction (NWP) models are increasingly used
to better resolve topographic features and forecast such high-impact weather
events. However, when the grid spacing becomes comparable to the length scales
of convection, known as the gray zone, the turbulent eddies in the atmospheric
boundary layer are only partially resolved and parameterized to some extent.
Whether using a convection parameterization (CP) scheme in the gray zone
remains controversial. Scale-aware CP schemes are developed to enhance the
representation of convective transport within the gray zone. The multi-scale
Kain-Fritsch (MSKF) scheme includes modifications that allow for its effective
implementation at a grid resolution as high as 2 km. In recent years, there has
been an increasing application of machine learning (ML) models to various
domains of atmospheric sciences, including the replacement of physical
parameterizations with ML models. This work proposes a multi-output
bidirectional long short-term memory (Bi-LSTM) model as a replace the
scale-aware MSKF CP scheme. The Weather Research and Forecast (WRF) model is
used to generate training and testing data over South China at a horizontal
resolution of 5 km. Furthermore, the WRF model is coupled with the ML based CP
scheme and compared with WRF simulations with original MSKF scheme. The results
demonstrate that the Bi-LSTM model can achieve high accuracy, indicating the
potential use of ML models to substitute the MSKF scheme in the gray zone.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zhong_X/0/1/0/all/0/1&quot;&gt;Xiaohui Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xing Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hao Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03658">
<title>The Linear Representation Hypothesis and the Geometry of Large Language Models. (arXiv:2311.03658v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.03658</link>
<description rdf:parseType="Literal">&lt;p&gt;Informally, the &apos;linear representation hypothesis&apos; is the idea that
high-level concepts are represented linearly as directions in some
representation space. In this paper, we address two closely related questions:
What does &quot;linear representation&quot; actually mean? And, how do we make sense of
geometric notions (e.g., cosine similarity or projection) in the representation
space? To answer these, we use the language of counterfactuals to give two
formalizations of &quot;linear representation&quot;, one in the output (word)
representation space, and one in the input (sentence) space. We then prove
these connect to linear probing and model steering, respectively. To make sense
of geometric notions, we use the formalization to identify a particular
(non-Euclidean) inner product that respects language structure in a sense we
make precise. Using this causal inner product, we show how to unify all notions
of linear representation. In particular, this allows the construction of probes
and steering vectors using counterfactual pairs. Experiments with LLaMA-2
demonstrate the existence of linear representations of concepts, the connection
to interpretation and control, and the fundamental role of the choice of inner
product.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1&quot;&gt;Kiho Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choe_Y/0/1/0/all/0/1&quot;&gt;Yo Joong Choe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veitch_V/0/1/0/all/0/1&quot;&gt;Victor Veitch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03669">
<title>Stable Modular Control via Contraction Theory for Reinforcement Learning. (arXiv:2311.03669v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.03669</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel way to integrate control techniques with reinforcement
learning (RL) for stability, robustness, and generalization: leveraging
contraction theory to realize modularity in neural control, which ensures that
combining stable subsystems can automatically preserve the stability. We
realize such modularity via signal composition and dynamic decomposition.
Signal composition creates the latent space, within which RL applies to
maximizing rewards. Dynamic decomposition is realized by coordinate
transformation that creates an auxiliary space, within which the latent signals
are coupled in the way that their combination can preserve stability provided
each signal, that is, each subsystem, has stable self-feedbacks. Leveraging
modularity, the nonlinear stability problem is deconstructed into algebraically
solvable ones, the stability of the subsystems in the auxiliary space, yielding
linear constraints on the input gradients of control networks that can be as
simple as switching the signs of network weights. This minimally invasive
method for stability allows arguably easy integration into the modular neural
architectures in machine learning, like hierarchical RL, and improves their
performance. We demonstrate in simulation the necessity and the effectiveness
of our method: the necessity for robustness and generalization, and the
effectiveness in improving hierarchical RL for manipulation learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1&quot;&gt;Bing Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Slotine_J/0/1/0/all/0/1&quot;&gt;Jean-Jacques Slotine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_Q/0/1/0/all/0/1&quot;&gt;Quang-Cuong Pham&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03680">
<title>Deep Bayesian Reinforcement Learning for Spacecraft Proximity Maneuvers and Docking. (arXiv:2311.03680v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.03680</link>
<description rdf:parseType="Literal">&lt;p&gt;In the pursuit of autonomous spacecraft proximity maneuvers and docking(PMD),
we introduce a novel Bayesian actor-critic reinforcement learning algorithm to
learn a control policy with the stability guarantee. The PMD task is formulated
as a Markov decision process that reflects the relative dynamic model, the
docking cone and the cost function. Drawing from the principles of Lyapunov
theory, we frame the temporal difference learning as a constrained Gaussian
process regression problem. This innovative approach allows the state-value
function to be expressed as a Lyapunov function, leveraging the Gaussian
process and deep kernel learning. We develop a novel Bayesian quadrature policy
optimization procedure to analytically compute the policy gradient while
integrating Lyapunov-based stability constraints. This integration is pivotal
in satisfying the rigorous safety demands of spaceflight missions. The proposed
algorithm has been experimentally evaluated on a spacecraft air-bearing testbed
and shows impressive and promising performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_D/0/1/0/all/0/1&quot;&gt;Desong Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_N/0/1/0/all/0/1&quot;&gt;Naiming Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yanfang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1&quot;&gt;Wei Pan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03695">
<title>Context Shift Reduction for Offline Meta-Reinforcement Learning. (arXiv:2311.03695v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.03695</link>
<description rdf:parseType="Literal">&lt;p&gt;Offline meta-reinforcement learning (OMRL) utilizes pre-collected offline
datasets to enhance the agent&apos;s generalization ability on unseen tasks.
However, the context shift problem arises due to the distribution discrepancy
between the contexts used for training (from the behavior policy) and testing
(from the exploration policy). The context shift problem leads to incorrect
task inference and further deteriorates the generalization ability of the
meta-policy. Existing OMRL methods either overlook this problem or attempt to
mitigate it with additional information. In this paper, we propose a novel
approach called Context Shift Reduction for OMRL (CSRO) to address the context
shift problem with only offline datasets. The key insight of CSRO is to
minimize the influence of policy in context during both the meta-training and
meta-test phases. During meta-training, we design a max-min mutual information
representation learning mechanism to diminish the impact of the behavior policy
on task representation. In the meta-test phase, we introduce the non-prior
context collection strategy to reduce the effect of the exploration policy.
Experimental results demonstrate that CSRO significantly reduces the context
shift and improves the generalization ability, surpassing previous methods
across various challenging domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yunkai Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jiaming Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Fan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_Q/0/1/0/all/0/1&quot;&gt;Qi Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1&quot;&gt;Shaohui Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_S/0/1/0/all/0/1&quot;&gt;Siming Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Ruizhi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1&quot;&gt;Zidong Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xing Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qi Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Ling Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yunji Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03698">
<title>A Novel Variational Lower Bound for Inverse Reinforcement Learning. (arXiv:2311.03698v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.03698</link>
<description rdf:parseType="Literal">&lt;p&gt;Inverse reinforcement learning (IRL) seeks to learn the reward function from
expert trajectories, to understand the task for imitation or collaboration
thereby removing the need for manual reward engineering. However, IRL in the
context of large, high-dimensional problems with unknown dynamics has been
particularly challenging. In this paper, we present a new Variational Lower
Bound for IRL (VLB-IRL), which is derived under the framework of a
probabilistic graphical model with an optimality node. Our method
simultaneously learns the reward function and policy under the learned reward
function by maximizing the lower bound, which is equivalent to minimizing the
reverse Kullback-Leibler divergence between an approximated distribution of
optimality given the reward function and the true distribution of optimality
given trajectories. This leads to a new IRL method that learns a valid reward
function such that the policy under the learned reward achieves expert-level
performance on several known domains. Importantly, the method outperforms the
existing state-of-the-art IRL algorithms on these domains by demonstrating
better reward from the learned policy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gui_Y/0/1/0/all/0/1&quot;&gt;Yikang Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doshi_P/0/1/0/all/0/1&quot;&gt;Prashant Doshi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03701">
<title>Hypothesis Network Planned Exploration for Rapid Meta-Reinforcement Learning Adaptation. (arXiv:2311.03701v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.03701</link>
<description rdf:parseType="Literal">&lt;p&gt;Meta Reinforcement Learning (Meta RL) trains agents that adapt to
fast-changing environments and tasks. Current strategies often lose adaption
efficiency due to the passive nature of model exploration, causing delayed
understanding of new transition dynamics. This results in particularly
fast-evolving tasks being impossible to solve. We propose a novel approach,
Hypothesis Network Planned Exploration (HyPE), that integrates an active and
planned exploration process via the hypothesis network to optimize adaptation
speed. HyPE uses a generative hypothesis network to form potential models of
state transition dynamics, then eliminates incorrect models through
strategically devised experiments. Evaluated on a symbolic version of the
Alchemy game, HyPE outpaces baseline methods in adaptation speed and model
accuracy, validating its potential in enhancing reinforcement learning
adaptation in rapidly evolving settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacobson_M/0/1/0/all/0/1&quot;&gt;Maxwell Joseph Jacobson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1&quot;&gt;Yexiang Xue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03705">
<title>Efficient Bottom-Up Synthesis for Programs with Local Variables. (arXiv:2311.03705v1 [cs.PL])</title>
<link>http://arxiv.org/abs/2311.03705</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new synthesis algorithm that can efficiently search programs
with local variables (e.g., those introduced by lambdas). Prior bottom-up
synthesis algorithms are not able to evaluate programs with free local
variables, and therefore cannot effectively reduce the search space of such
programs (e.g., using standard observational equivalence reduction techniques),
making synthesis slow. Our algorithm can reduce the space of programs with
local variables. The key idea, dubbed lifted interpretation, is to lift up the
program interpretation process, from evaluating one program at a time to
simultaneously evaluating all programs from a grammar. Lifted interpretation
provides a mechanism to systematically enumerate all binding contexts for local
variables, thereby enabling us to evaluate and reduce the space of programs
with local variables. Our ideas are instantiated in the domain of web
automation. The resulting tool, Arborist, can automate a significantly broader
range of challenging tasks more efficiently than state-of-the-art techniques
including WebRobot and Helena.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xiangyu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1&quot;&gt;Rui Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yihong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinyu Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03707">
<title>The NeurIPS 2022 Neural MMO Challenge: A Massively Multiagent Competition with Specialization and Trade. (arXiv:2311.03707v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.03707</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present the results of the NeurIPS-2022 Neural MMO
Challenge, which attracted 500 participants and received over 1,600
submissions. Like the previous IJCAI-2022 Neural MMO Challenge, it involved
agents from 16 populations surviving in procedurally generated worlds by
collecting resources and defeating opponents. This year&apos;s competition runs on
the latest v1.6 Neural MMO, which introduces new equipment, combat, trading,
and a better scoring system. These elements combine to pose additional
robustness and generalization challenges not present in previous competitions.
This paper summarizes the design and results of the challenge, explores the
potential of this environment as a benchmark for learning methods, and presents
some practical reinforcement learning training approaches for complex tasks
with sparse rewards. Additionally, we have open-sourced our baselines,
including environment wrappers, benchmarks, and visualization tools for future
research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_E/0/1/0/all/0/1&quot;&gt;Enhong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suarez_J/0/1/0/all/0/1&quot;&gt;Joseph Suarez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1&quot;&gt;Chenhui You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Bo Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bingcheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jun Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiaxin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiaolong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Clare Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Togelius_J/0/1/0/all/0/1&quot;&gt;Julian Togelius&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohanty_S/0/1/0/all/0/1&quot;&gt;Sharada Mohanty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_W/0/1/0/all/0/1&quot;&gt;Weijun Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_R/0/1/0/all/0/1&quot;&gt;Rui Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yibing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qinwen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xinhang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1&quot;&gt;Zheng Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yuejia Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hanhui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Shiqi Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1&quot;&gt;Phillip Isola&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03711">
<title>Mitigating Estimation Errors by Twin TD-Regularized Actor and Critic for Deep Reinforcement Learning. (arXiv:2311.03711v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.03711</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the issue of estimation bias in deep reinforcement learning (DRL)
by introducing solution mechanisms that include a new, twin TD-regularized
actor-critic (TDR) method. It aims at reducing both over and under-estimation
errors. With TDR and by combining good DRL improvements, such as distributional
learning and long N-step surrogate stage reward (LNSS) method, we show that our
new TDR-based actor-critic learning has enabled DRL methods to outperform their
respective baselines in challenging environments in DeepMind Control Suite.
Furthermore, they elevate TD3 and SAC respectively to a level of performance
comparable to that of D4PG (the current SOTA), and they also improve the
performance of D4PG to a new SOTA level measured by mean reward, convergence
speed, learning success rate, and learning variance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_J/0/1/0/all/0/1&quot;&gt;Junmin Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1&quot;&gt;Ruofan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Si_J/0/1/0/all/0/1&quot;&gt;Jennie Si&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03714">
<title>Loss Balancing for Fair Supervised Learning. (arXiv:2311.03714v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.03714</link>
<description rdf:parseType="Literal">&lt;p&gt;Supervised learning models have been used in various domains such as lending,
college admission, face recognition, natural language processing, etc. However,
they may inherit pre-existing biases from training data and exhibit
discrimination against protected social groups. Various fairness notions have
been proposed to address unfairness issues. In this work, we focus on Equalized
Loss (EL), a fairness notion that requires the expected loss to be
(approximately) equalized across different groups. Imposing EL on the learning
process leads to a non-convex optimization problem even if the loss function is
convex, and the existing fair learning algorithms cannot properly be adopted to
find the fair predictor under the EL constraint. This paper introduces an
algorithm that can leverage off-the-shelf convex programming tools (e.g.,
CVXPY) to efficiently find the global optimum of this non-convex optimization.
In particular, we propose the ELminimizer algorithm, which finds the optimal
fair predictor under EL by reducing the non-convex optimization to a sequence
of convex optimization problems. We theoretically prove that our algorithm
finds the global optimal solution under certain conditions. Then, we support
our theoretical results through several empirical studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khalili_M/0/1/0/all/0/1&quot;&gt;Mohammad Mahdi Khalili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xueru Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abroshan_M/0/1/0/all/0/1&quot;&gt;Mahed Abroshan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03716">
<title>LLM as an Art Director (LaDi): Using LLMs to improve Text-to-Media Generators. (arXiv:2311.03716v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.03716</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in text-to-image generation have revolutionized numerous
fields, including art and cinema, by automating the generation of high-quality,
context-aware images and video. However, the utility of these technologies is
often limited by the inadequacy of text prompts in guiding the generator to
produce artistically coherent and subject-relevant images. In this paper, We
describe the techniques that can be used to make Large Language Models (LLMs)
act as Art Directors that enhance image and video generation. We describe our
unified system for this called &quot;LaDi&quot;. We explore how LaDi integrates multiple
techniques for augmenting the capabilities of text-to-image generators (T2Is)
and text-to-video generators (T2Vs), with a focus on constrained decoding,
intelligent prompting, fine-tuning, and retrieval. LaDi and these techniques
are being used today in apps and platforms developed by Plai Labs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roush_A/0/1/0/all/0/1&quot;&gt;Allen Roush&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zakirov_E/0/1/0/all/0/1&quot;&gt;Emil Zakirov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shirokov_A/0/1/0/all/0/1&quot;&gt;Artemiy Shirokov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lunina_P/0/1/0/all/0/1&quot;&gt;Polina Lunina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gane_J/0/1/0/all/0/1&quot;&gt;Jack Gane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duffy_A/0/1/0/all/0/1&quot;&gt;Alexander Duffy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basil_C/0/1/0/all/0/1&quot;&gt;Charlie Basil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whitcomb_A/0/1/0/all/0/1&quot;&gt;Aber Whitcomb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benedetto_J/0/1/0/all/0/1&quot;&gt;Jim Benedetto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DeWolfe_C/0/1/0/all/0/1&quot;&gt;Chris DeWolfe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03721">
<title>ClimateSet: A Large-Scale Climate Model Dataset for Machine Learning. (arXiv:2311.03721v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.03721</link>
<description rdf:parseType="Literal">&lt;p&gt;Climate models have been key for assessing the impact of climate change and
simulating future climate scenarios. The machine learning (ML) community has
taken an increased interest in supporting climate scientists&apos; efforts on
various tasks such as climate model emulation, downscaling, and prediction
tasks. Many of those tasks have been addressed on datasets created with single
climate models. However, both the climate science and ML communities have
suggested that to address those tasks at scale, we need large, consistent, and
ML-ready climate model datasets. Here, we introduce ClimateSet, a dataset
containing the inputs and outputs of 36 climate models from the Input4MIPs and
CMIP6 archives. In addition, we provide a modular dataset pipeline for
retrieving and preprocessing additional climate models and scenarios. We
showcase the potential of our dataset by using it as a benchmark for ML-based
climate model emulation. We gain new insights about the performance and
generalization capabilities of the different ML models by analyzing their
performance across different climate models. Furthermore, the dataset can be
used to train an ML emulator on several climate models instead of just one.
Such a &quot;super emulator&quot; can quickly project new climate change scenarios,
complementing existing scenarios already provided to policymakers. We believe
ClimateSet will create the basis needed for the ML community to tackle
climate-related tasks at scale.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaltenborn_J/0/1/0/all/0/1&quot;&gt;Julia Kaltenborn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lange_C/0/1/0/all/0/1&quot;&gt;Charlotte E. E. Lange&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramesh_V/0/1/0/all/0/1&quot;&gt;Venkatesh Ramesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brouillard_P/0/1/0/all/0/1&quot;&gt;Philippe Brouillard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurwicz_Y/0/1/0/all/0/1&quot;&gt;Yaniv Gurwicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagda_C/0/1/0/all/0/1&quot;&gt;Chandni Nagda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Runge_J/0/1/0/all/0/1&quot;&gt;Jakob Runge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nowack_P/0/1/0/all/0/1&quot;&gt;Peer Nowack&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rolnick_D/0/1/0/all/0/1&quot;&gt;David Rolnick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03736">
<title>Neural MMO 2.0: A Massively Multi-task Addition to Massively Multi-agent Learning. (arXiv:2311.03736v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.03736</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural MMO 2.0 is a massively multi-agent environment for reinforcement
learning research. The key feature of this new version is a flexible task
system that allows users to define a broad range of objectives and reward
signals. We challenge researchers to train agents capable of generalizing to
tasks, maps, and opponents never seen during training. Neural MMO features
procedurally generated maps with 128 agents in the standard setting and support
for up to. Version 2.0 is a complete rewrite of its predecessor with three-fold
improved performance and compatibility with CleanRL. We release the platform as
free and open-source software with comprehensive documentation available at
neuralmmo.github.io and an active community Discord. To spark initial research
on this new platform, we are concurrently running a competition at NeurIPS
2023.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suarez_J/0/1/0/all/0/1&quot;&gt;Joseph Su&amp;#xe1;rez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1&quot;&gt;Phillip Isola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choe_K/0/1/0/all/0/1&quot;&gt;Kyoung Whan Choe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bloomin_D/0/1/0/all/0/1&quot;&gt;David Bloomin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hao Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pinnaparaju_N/0/1/0/all/0/1&quot;&gt;Nikhil Pinnaparaju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanna_N/0/1/0/all/0/1&quot;&gt;Nishaanth Kanna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scott_D/0/1/0/all/0/1&quot;&gt;Daniel Scott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sullivan_R/0/1/0/all/0/1&quot;&gt;Ryan Sullivan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shuman_R/0/1/0/all/0/1&quot;&gt;Rose S. Shuman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alcantara_L/0/1/0/all/0/1&quot;&gt;Lucas de Alc&amp;#xe2;ntara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bradley_H/0/1/0/all/0/1&quot;&gt;Herbie Bradley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castricato_L/0/1/0/all/0/1&quot;&gt;Louis Castricato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_K/0/1/0/all/0/1&quot;&gt;Kirsty You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yuhao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qimai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiaxin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiaolong Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03738">
<title>deep-REMAP: Parameterization of Stellar Spectra Using Regularized Multi-Task Learning. (arXiv:2311.03738v1 [astro-ph.SR])</title>
<link>http://arxiv.org/abs/2311.03738</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional spectral analysis methods are increasingly challenged by the
exploding volumes of data produced by contemporary astronomical surveys. In
response, we develop deep-Regularized Ensemble-based Multi-task Learning with
Asymmetric Loss for Probabilistic Inference ($\rm{deep-REMAP}$), a novel
framework that utilizes the rich synthetic spectra from the PHOENIX library and
observational data from the MARVELS survey to accurately predict stellar
atmospheric parameters. By harnessing advanced machine learning techniques,
including multi-task learning and an innovative asymmetric loss function,
$\rm{deep-REMAP}$ demonstrates superior predictive capabilities in determining
effective temperature, surface gravity, and metallicity from observed spectra.
Our results reveal the framework&apos;s effectiveness in extending to other stellar
libraries and properties, paving the way for more sophisticated and automated
techniques in stellar characterization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Gilda_S/0/1/0/all/0/1&quot;&gt;Sankalp Gilda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03739">
<title>Leveraging Large Language Models for Automated Proof Synthesis in Rust. (arXiv:2311.03739v1 [cs.FL])</title>
<link>http://arxiv.org/abs/2311.03739</link>
<description rdf:parseType="Literal">&lt;p&gt;Formal verification can provably guarantee the correctness of critical system
software, but the high proof burden has long hindered its wide adoption.
Recently, Large Language Models (LLMs) have shown success in code analysis and
synthesis. In this paper, we present a combination of LLMs and static analysis
to synthesize invariants, assertions, and other proof structures for a
Rust-based formal verification framework called Verus. In a few-shot setting,
LLMs demonstrate impressive logical ability in generating postconditions and
loop invariants, especially when analyzing short code snippets. However, LLMs
lack the ability to retain and propagate context information, a strength of
traditional static analysis. Based on these observations, we developed a
prototype based on OpenAI&apos;s GPT-4 model. Our prototype decomposes the
verification task into multiple smaller ones, iteratively queries GPT-4, and
combines its output with lightweight static analysis. We evaluated the
prototype with a developer in the automation loop on 20 vector-manipulating
programs. The results demonstrate that it significantly reduces human effort in
writing entry-level proof code.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Jianan Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Ziqiao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weiteng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1&quot;&gt;Weidong Cui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03753">
<title>COOL: A Constraint Object-Oriented Logic Programming Language and its Neural-Symbolic Compilation System. (arXiv:2311.03753v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.03753</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the integration of neural networks with logic
programming, addressing the longstanding challenges of combining the
generalization and learning capabilities of neural networks with the precision
of symbolic logic. Traditional attempts at this integration have been hampered
by difficulties in initial data acquisition, the reliability of undertrained
networks, and the complexity of reusing and augmenting trained models. To
overcome these issues, we introduce the COOL (Constraint Object-Oriented Logic)
programming language, an innovative approach that seamlessly combines logical
reasoning with neural network technologies. COOL is engineered to autonomously
handle data collection, mitigating the need for user-supplied initial data. It
incorporates user prompts into the coding process to reduce the risks of
undertraining and enhances the interaction among models throughout their
lifecycle to promote the reuse and augmentation of networks. Furthermore, the
foundational principles and algorithms in COOL&apos;s design and its compilation
system could provide valuable insights for future developments in programming
languages and neural network architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jipeng Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03756">
<title>Learning Decentralized Traffic Signal Controllers with Multi-Agent Graph Reinforcement Learning. (arXiv:2311.03756v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.03756</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper considers optimal traffic signal control in smart cities, which
has been taken as a complex networked system control problem. Given the
interacting dynamics among traffic lights and road networks, attaining
controller adaptivity and scalability stands out as a primary challenge.
Capturing the spatial-temporal correlation among traffic lights under the
framework of Multi-Agent Reinforcement Learning (MARL) is a promising solution.
Nevertheless, existing MARL algorithms ignore effective information aggregation
which is fundamental for improving the learning capacity of decentralized
agents. In this paper, we design a new decentralized control architecture with
improved environmental observability to capture the spatial-temporal
correlation. Specifically, we first develop a topology-aware information
aggregation strategy to extract correlation-related information from
unstructured data gathered in the road network. Particularly, we transfer the
road network topology into a graph shift operator by forming a diffusion
process on the topology, which subsequently facilitates the construction of
graph signals. A diffusion convolution module is developed, forming a new MARL
algorithm, which endows agents with the capabilities of graph learning.
Extensive experiments based on both synthetic and real-world datasets verify
that our proposal outperforms existing decentralized algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhiwen Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luan_T/0/1/0/all/0/1&quot;&gt;Tom H. Luan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1&quot;&gt;Bin Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuen_C/0/1/0/all/0/1&quot;&gt;Chau Yuen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03761">
<title>Augmenting Radio Signals with Wavelet Transform for Deep Learning-Based Modulation Recognition. (arXiv:2311.03761v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.03761</link>
<description rdf:parseType="Literal">&lt;p&gt;The use of deep learning for radio modulation recognition has become
prevalent in recent years. This approach automatically extracts
high-dimensional features from large datasets, facilitating the accurate
classification of modulation schemes. However, in real-world scenarios, it may
not be feasible to gather sufficient training data in advance. Data
augmentation is a method used to increase the diversity and quantity of
training dataset and to reduce data sparsity and imbalance. In this paper, we
propose data augmentation methods that involve replacing detail coefficients
decomposed by discrete wavelet transform for reconstructing to generate new
samples and expand the training set. Different generation methods are used to
generate replacement sequences. Simulation results indicate that our proposed
methods significantly outperform the other augmentation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Shilian Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_K/0/1/0/all/0/1&quot;&gt;Kunfeng Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Luxin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xuan_Q/0/1/0/all/0/1&quot;&gt;Qi Xuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaoniu Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03768">
<title>PT-Tuning: Bridging the Gap between Time Series Masked Reconstruction and Forecasting via Prompt Token Tuning. (arXiv:2311.03768v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.03768</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised learning has been actively studied in time series domain
recently, especially for masked reconstruction. Most of these methods follow
the &quot;Pre-training + Fine-tuning&quot; paradigm in which a new decoder replaces the
pre-trained decoder to fit for a specific downstream task, leading to
inconsistency of upstream and downstream tasks. In this paper, we first point
out that the unification of task objectives and adaptation for task difficulty
are critical for bridging the gap between time series masked reconstruction and
forecasting. By reserving the pre-trained mask token during fine-tuning stage,
the forecasting task can be taken as a special case of masked reconstruction,
where the future values are masked and reconstructed based on history values.
It guarantees the consistency of task objectives but there is still a gap in
task difficulty. Because masked reconstruction can utilize contextual
information while forecasting can only use historical information to
reconstruct. To further mitigate the existed gap, we propose a simple yet
effective prompt token tuning (PT-Tuning) paradigm, in which all pre-trained
parameters are frozen and only a few trainable prompt tokens are added to
extended mask tokens in element-wise manner. Extensive experiments on
real-world datasets demonstrate the superiority of our proposed paradigm with
state-of-the-art performance compared to representation learning and end-to-end
supervised forecasting methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_J/0/1/0/all/0/1&quot;&gt;Jinrui Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1&quot;&gt;Xiaoxuan Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1&quot;&gt;Chuanxian Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_G/0/1/0/all/0/1&quot;&gt;Guangxin Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1&quot;&gt;Yucheng Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1&quot;&gt;Changwei Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Huan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03780">
<title>Ensembling Textual and Structure-Based Models for Knowledge Graph Completion. (arXiv:2311.03780v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.03780</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider two popular approaches to Knowledge Graph Completion (KGC):
textual models that rely on textual entity descriptions, and structure-based
models that exploit the connectivity structure of the Knowledge Graph (KG).
Preliminary experiments show that these approaches have complementary
strengths: structure-based models perform well when the gold answer is easily
reachable from the query head in the KG, while textual models exploit
descriptions to give good performance even when the gold answer is not
reachable. In response, we explore ensembling as a way of combining the best of
both approaches. We propose a novel method for learning query-dependent
ensemble weights by using the distributions of scores assigned by individual
models to all candidate entities. Our ensemble baseline achieves
state-of-the-art results on three standard KGC datasets, with up to 6.8 pt MRR
and 8.3 pt Hits@1 gains over best individual models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nandi_A/0/1/0/all/0/1&quot;&gt;Ananjan Nandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaur_N/0/1/0/all/0/1&quot;&gt;Navdeep Kaur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singla_P/0/1/0/all/0/1&quot;&gt;Parag Singla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1&quot;&gt;Mausam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03783">
<title>Scene-Driven Multimodal Knowledge Graph Construction for Embodied AI. (arXiv:2311.03783v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.03783</link>
<description rdf:parseType="Literal">&lt;p&gt;Embodied AI is one of the most popular studies in artificial intelligence and
robotics, which can effectively improve the intelligence of real-world agents
(i.e. robots) serving human beings. Scene knowledge is important for an agent
to understand the surroundings and make correct decisions in the varied open
world. Currently, knowledge base for embodied tasks is missing and most
existing work use general knowledge base or pre-trained models to enhance the
intelligence of an agent. For conventional knowledge base, it is sparse,
insufficient in capacity and cost in data collection. For pre-trained models,
they face the uncertainty of knowledge and hard maintenance. To overcome the
challenges of scene knowledge, we propose a scene-driven multimodal knowledge
graph (Scene-MMKG) construction method combining conventional knowledge
engineering and large language models. A unified scene knowledge injection
framework is introduced for knowledge representation. To evaluate the
advantages of our proposed method, we instantiate Scene-MMKG considering
typical indoor robotic functionalities (Manipulation and Mobility), named
ManipMob-MMKG. Comparisons in characteristics indicate our instantiated
ManipMob-MMKG has broad superiority in data-collection efficiency and knowledge
quality. Experimental results on typical embodied tasks show that
knowledge-enhanced methods using our instantiated ManipMob-MMKG can improve the
performance obviously without re-designing model structures complexly. Our
project can be found at https://sites.google.com/view/manipmob-mmkg
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yaoxian_S/0/1/0/all/0/1&quot;&gt;Song Yaoxian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Penglei_S/0/1/0/all/0/1&quot;&gt;Sun Penglei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haoyu_L/0/1/0/all/0/1&quot;&gt;Liu Haoyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhixu_L/0/1/0/all/0/1&quot;&gt;Li Zhixu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1&quot;&gt;Song Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yanghua_X/0/1/0/all/0/1&quot;&gt;Xiao Yanghua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiaofang_Z/0/1/0/all/0/1&quot;&gt;Zhou Xiaofang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03810">
<title>Rethinking and Improving Multi-task Learning for End-to-end Speech Translation. (arXiv:2311.03810v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.03810</link>
<description rdf:parseType="Literal">&lt;p&gt;Significant improvements in end-to-end speech translation (ST) have been
achieved through the application of multi-task learning. However, the extent to
which auxiliary tasks are highly consistent with the ST task, and how much this
approach truly helps, have not been thoroughly studied. In this paper, we
investigate the consistency between different tasks, considering different
times and modules. We find that the textual encoder primarily facilitates
cross-modal conversion, but the presence of noise in speech impedes the
consistency between text and speech representations. Furthermore, we propose an
improved multi-task learning (IMTL) approach for the ST task, which bridges the
modal gap by mitigating the difference in length and representation. We conduct
experiments on the MuST-C dataset. The results demonstrate that our method
attains state-of-the-art results. Moreover, when additional data is used, we
achieve the new SOTA result on MuST-C English to Spanish task with 20.8% of the
training time required by the current SOTA method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuhao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1&quot;&gt;Tong Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chunliang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jingbo Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03830">
<title>Reducing Spatial Fitting Error in Distillation of Denoising Diffusion Models. (arXiv:2311.03830v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03830</link>
<description rdf:parseType="Literal">&lt;p&gt;Denoising Diffusion models have exhibited remarkable capabilities in image
generation. However, generating high-quality samples requires a large number of
iterations. Knowledge distillation for diffusion models is an effective method
to address this limitation with a shortened sampling process but causes
degraded generative quality. Based on our analysis with bias-variance
decomposition and experimental observations, we attribute the degradation to
the spatial fitting error occurring in the training of both the teacher and
student model. Accordingly, we propose $\textbf{S}$patial
$\textbf{F}$itting-$\textbf{E}$rror $\textbf{R}$eduction
$\textbf{D}$istillation model ($\textbf{SFERD}$). SFERD utilizes attention
guidance from the teacher model and a designed semantic gradient predictor to
reduce the student&apos;s fitting error. Empirically, our proposed model facilitates
high-quality sample generation in a few function evaluations. We achieve an FID
of 5.31 on CIFAR-10 and 9.39 on ImageNet 64$\times$64 with only one step,
outperforming existing diffusion methods. Our study provides a new perspective
on diffusion distillation by highlighting the intrinsic denoising ability of
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shengzhe Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Z/0/1/0/all/0/1&quot;&gt;Zejian Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shengyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1&quot;&gt;Lefan Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Changyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Lingyun Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03839">
<title>Aspects of human memory and Large Language Models. (arXiv:2311.03839v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.03839</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) are huge artificial neural networks which
primarily serve to generate text, but also provide a very sophisticated
probabilistic model of language use. Since generating a semantically consistent
text requires a form of effective memory, we investigate the memory properties
of LLMs and find surprising similarities with key characteristics of human
memory. This result strongly suggests that the biological features of human
memory leave an imprint on the way that we structure our textual narratives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Janik_R/0/1/0/all/0/1&quot;&gt;Romuald A. Janik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03865">
<title>FD-MIA: Efficient Attacks on Fairness-enhanced Models. (arXiv:2311.03865v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.03865</link>
<description rdf:parseType="Literal">&lt;p&gt;Previous studies have developed fairness methods for biased models that
exhibit discriminatory behaviors towards specific subgroups. While these models
have shown promise in achieving fair predictions, recent research has
identified their potential vulnerability to score-based membership inference
attacks (MIAs). In these attacks, adversaries can infer whether a particular
data sample was used during training by analyzing the model&apos;s prediction
scores. However, our investigations reveal that these score-based MIAs are
ineffective when targeting fairness-enhanced models in binary classifications.
The attack models trained to launch the MIAs degrade into simplistic threshold
models, resulting in lower attack performance. Meanwhile, we observe that
fairness methods often lead to prediction performance degradation for the
majority subgroups of the training data. This raises the barrier to successful
attacks and widens the prediction gaps between member and non-member data.
Building upon these insights, we propose an efficient MIA method against
fairness-enhanced models based on fairness discrepancy results (FD-MIA). It
leverages the difference in the predictions from both the original and
fairness-enhanced models and exploits the observed prediction gaps as attack
clues. We also explore potential strategies for mitigating privacy leakages.
Extensive experiments validate our findings and demonstrate the efficacy of the
proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1&quot;&gt;Huan Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guangsheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1&quot;&gt;Tianqing Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1&quot;&gt;Ming Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wanlei Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03873">
<title>Mini but Mighty: Finetuning ViTs with Mini Adapters. (arXiv:2311.03873v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.03873</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformers (ViTs) have become one of the dominant architectures in
computer vision, and pre-trained ViT models are commonly adapted to new tasks
via fine-tuning. Recent works proposed several parameter-efficient transfer
learning methods, such as adapters, to avoid the prohibitive training and
storage cost of finetuning. In this work, we observe that adapters perform
poorly when the dimension of adapters is small, and we propose MiMi, a training
framework that addresses this issue. We start with large adapters which can
reach high performance, and iteratively reduce their size. To enable automatic
estimation of the hidden dimension of every adapter, we also introduce a new
scoring function, specifically designed for adapters, that compares the neuron
importance across layers. Our method outperforms existing methods in finding
the best trade-off between accuracy and trained parameters across the three
dataset benchmarks DomainNet, VTAB, and Multi-task, for a total of 29 datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marouf_I/0/1/0/all/0/1&quot;&gt;Imad Eddine Marouf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tartaglione_E/0/1/0/all/0/1&quot;&gt;Enzo Tartaglione&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lathuiliere_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Lathuili&amp;#xe8;re&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03886">
<title>Formulating Discrete Probability Flow Through Optimal Transport. (arXiv:2311.03886v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.03886</link>
<description rdf:parseType="Literal">&lt;p&gt;Continuous diffusion models are commonly acknowledged to display a
deterministic probability flow, whereas discrete diffusion models do not. In
this paper, we aim to establish the fundamental theory for the probability flow
of discrete diffusion models. Specifically, we first prove that the continuous
probability flow is the Monge optimal transport map under certain conditions,
and also present an equivalent evidence for discrete cases. In view of these
findings, we are then able to define the discrete probability flow in line with
the principles of optimal transport. Finally, drawing upon our newly
established definitions, we propose a novel sampling method that surpasses
previous discrete diffusion models in its ability to generate more certain
outcomes. Extensive experiments on the synthetic toy dataset and the CIFAR-10
dataset have validated the effectiveness of our proposed discrete probability
flow. Code is released at:
https://github.com/PangzeCheung/Discrete-Probability-Flow.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Pengze Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1&quot;&gt;Hubery Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiaohua Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03893">
<title>Understanding Tool Discovery and Tool Innovation Using Active Inference. (arXiv:2311.03893v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.03893</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to invent new tools has been identified as an important facet of
our ability as a species to problem solve in dynamic and novel environments.
While the use of tools by artificial agents presents a challenging task and has
been widely identified as a key goal in the field of autonomous robotics, far
less research has tackled the invention of new tools by agents. In this paper,
(1) we articulate the distinction between tool discovery and tool innovation by
providing a minimal description of the two concepts under the formalism of
active inference. We then (2) apply this description to construct a toy model
of tool innovation by introducing the notion of tool affordances into the
hidden states of the agent&apos;s probabilistic generative model. This particular
state factorisation facilitates the ability to not just discover tools but
invent them through the offline induction of an appropriate tool property. We
discuss the implications of these preliminary results and outline future
directions of research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Collis_P/0/1/0/all/0/1&quot;&gt;Poppy Collis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kinghorn_P/0/1/0/all/0/1&quot;&gt;Paul F Kinghorn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buckley_C/0/1/0/all/0/1&quot;&gt;Christopher L Buckley&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03897">
<title>Temporal Graph Representation Learning with Adaptive Augmentation Contrastive. (arXiv:2311.03897v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.03897</link>
<description rdf:parseType="Literal">&lt;p&gt;Temporal graph representation learning aims to generate low-dimensional
dynamic node embeddings to capture temporal information as well as structural
and property information. Current representation learning methods for temporal
networks often focus on capturing fine-grained information, which may lead to
the model capturing random noise instead of essential semantic information.
While graph contrastive learning has shown promise in dealing with noise, it
only applies to static graphs or snapshots and may not be suitable for handling
time-dependent noise. To alleviate the above challenge, we propose a novel
Temporal Graph representation learning with Adaptive augmentation Contrastive
(TGAC) model. The adaptive augmentation on the temporal graph is made by
combining prior knowledge with temporal information, and the contrastive
objective function is constructed by defining the augmented inter-view contrast
and intra-view contrast. To complement TGAC, we propose three adaptive
augmentation strategies that modify topological features to reduce noise from
the network. Our extensive experiments on various real networks demonstrate
that the proposed model outperforms other temporal graph representation
learning methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hongjiang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_P/0/1/0/all/0/1&quot;&gt;Pengfei Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Huijun Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Huaming Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03942">
<title>The Music Meta Ontology: a flexible semantic model for the interoperability of music metadata. (arXiv:2311.03942v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2311.03942</link>
<description rdf:parseType="Literal">&lt;p&gt;The semantic description of music metadata is a key requirement for the
creation of music datasets that can be aligned, integrated, and accessed for
information retrieval and knowledge discovery. It is nonetheless an open
challenge due to the complexity of musical concepts arising from different
genres, styles, and periods -- standing to benefit from a lingua franca to
accommodate various stakeholders (musicologists, librarians, data engineers,
etc.). To initiate this transition, we introduce the Music Meta ontology, a
rich and flexible semantic model to describe music metadata related to artists,
compositions, performances, recordings, and links. We follow eXtreme Design
methodologies and best practices for data engineering, to reflect the
perspectives and the requirements of various stakeholders into the design of
the model, while leveraging ontology design patterns and accounting for
provenance at different levels (claims, links). After presenting the main
features of Music Meta, we provide a first evaluation of the model, alignments
to other schema (Music Ontology, DOREMUS, Wikidata), and support for data
transformation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berardinis_J/0/1/0/all/0/1&quot;&gt;Jacopo de Berardinis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carriero_V/0/1/0/all/0/1&quot;&gt;Valentina Anita Carriero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merono_Penuela_A/0/1/0/all/0/1&quot;&gt;Albert Mero&amp;#xf1;o-Pe&amp;#xf1;uela&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poltronieri_A/0/1/0/all/0/1&quot;&gt;Andrea Poltronieri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Presutti_V/0/1/0/all/0/1&quot;&gt;Valentina Presutti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03955">
<title>Elastic Information Bottleneck. (arXiv:2311.03955v1 [cs.IT])</title>
<link>http://arxiv.org/abs/2311.03955</link>
<description rdf:parseType="Literal">&lt;p&gt;Information bottleneck is an information-theoretic principle of
representation learning that aims to learn a maximally compressed
representation that preserves as much information about labels as possible.
Under this principle, two different methods have been proposed, i.e.,
information bottleneck (IB) and deterministic information bottleneck (DIB), and
have gained significant progress in explaining the representation mechanisms of
deep learning algorithms. However, these theoretical and empirical successes
are only valid with the assumption that training and test data are drawn from
the same distribution, which is clearly not satisfied in many real-world
applications. In this paper, we study their generalization abilities within a
transfer learning scenario, where the target error could be decomposed into
three components, i.e., source empirical error, source generalization gap (SG),
and representation discrepancy (RD). Comparing IB and DIB on these terms, we
prove that DIB&apos;s SG bound is tighter than IB&apos;s while DIB&apos;s RD is larger than
IB&apos;s. Therefore, it is difficult to tell which one is better. To balance the
trade-off between SG and the RD, we propose an elastic information bottleneck
(EIB) to interpolate between the IB and DIB regularizers, which guarantees a
Pareto frontier within the IB framework. Additionally, simulations and real
data experiments show that EIB has the ability to achieve better domain
adaptation results than IB and DIB, which validates the correctness of our
theories.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1&quot;&gt;Yuyan Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1&quot;&gt;Yanyan Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;Ao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zhiming Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03963">
<title>An Expectation-Realization Model for Metaphor Detection. (arXiv:2311.03963v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.03963</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a metaphor detection architecture that is structured around two
main modules: an expectation component that estimates representations of
literal word expectations given a context, and a realization component that
computes representations of actual word meanings in context. The overall
architecture is trained to learn expectation-realization (ER) patterns that
characterize metaphorical uses of words. When evaluated on three metaphor
datasets for within distribution, out of distribution, and novel metaphor
generalization, the proposed method is shown to obtain results that are
competitive or better than state-of-the art. Further increases in metaphor
detection accuracy are obtained through ensembling of ER models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uduehi_O/0/1/0/all/0/1&quot;&gt;Oseremen O. Uduehi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bunescu_R/0/1/0/all/0/1&quot;&gt;Razvan C. Bunescu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03976">
<title>Its All Graph To Me: Foundational Topology Models with Contrastive Learning on Multiple Domains. (arXiv:2311.03976v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.03976</link>
<description rdf:parseType="Literal">&lt;p&gt;Representations and embeddings of graph data have been essential in many
domains of research.
&lt;/p&gt;
&lt;p&gt;The principle benefit of learning such representations is that the
pre-trained model can be fine-tuned on smaller datasets where data or labels
are scarse.
&lt;/p&gt;
&lt;p&gt;Existing models, however, are domain specific; for example a model trained on
molecular graphs is fine-tuned on other molecular graphs.
&lt;/p&gt;
&lt;p&gt;This means that in many application cases the choice of pre-trained model can
be arbitrary, and novel domains may lack an appropriate pre-trained model.
&lt;/p&gt;
&lt;p&gt;This is of particular issue where data is scarse, precluding traditional
supervised methods.
&lt;/p&gt;
&lt;p&gt;In this work we use adversarial contrastive learning to present a \method, a
model pre-trained on many graph domains.
&lt;/p&gt;
&lt;p&gt;We train the model only on topologies but include node labels in evaluation.
&lt;/p&gt;
&lt;p&gt;We evaluate the efficacy of its learnt representations on various downstream
tasks.
&lt;/p&gt;
&lt;p&gt;Against baseline models pre-trained on single domains, as well as un-trained
models and non-transferred models, we show that performance is equal or better
using our single model.
&lt;/p&gt;
&lt;p&gt;This includes when node labels are used in evaluation, where performance is
consistently superior to single-domain or non-pre-trained models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davies_A/0/1/0/all/0/1&quot;&gt;Alex O. Davies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Green_R/0/1/0/all/0/1&quot;&gt;Riku W. Green&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ajmeri_N/0/1/0/all/0/1&quot;&gt;Nirav S. Ajmeri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Filho_T/0/1/0/all/0/1&quot;&gt;Telmo M. Silva Filho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03989">
<title>Learned Causal Method Prediction. (arXiv:2311.03989v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.03989</link>
<description rdf:parseType="Literal">&lt;p&gt;For a given causal question, it is important to efficiently decide which
causal inference method to use for a given dataset. This is challenging because
causal methods typically rely on complex and difficult-to-verify assumptions,
and cross-validation is not applicable since ground truth causal quantities are
unobserved.In this work, we propose CAusal Method Predictor (CAMP), a framework
for predicting the best method for a given dataset. To this end, we generate
datasets from a diverse set of synthetic causal models, score the candidate
methods, and train a model to directly predict the highest-scoring method for
that dataset. Next, by formulating a self-supervised pre-training objective
centered on dataset assumptions relevant for causal inference, we significantly
reduce the need for costly labeled data and enhance training efficiency. Our
strategy learns to map implicit dataset properties to the best method in a
data-driven manner. In our experiments, we focus on method prediction for
causal discovery. CAMP outperforms selecting any individual candidate method
and demonstrates promising generalization to unseen semi-synthetic and
real-world benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1&quot;&gt;Shantanu Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Cheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hilmkil_A/0/1/0/all/0/1&quot;&gt;Agrin Hilmkil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2004.14254">
<title>Hierarchical Reinforcement Learning for Automatic Disease Diagnosis. (arXiv:2004.14254v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2004.14254</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivation: Disease diagnosis oriented dialogue system models the interactive
consultation procedure as Markov Decision Process and reinforcement learning
algorithms are used to solve the problem. Existing approaches usually employ a
flat policy structure that treat all symptoms and diseases equally for action
making. This strategy works well in the simple scenario when the action space
is small, however, its efficiency will be challenged in the real environment.
Inspired by the offline consultation process, we propose to integrate a
hierarchical policy structure of two levels into the dialogue systemfor policy
learning. The high-level policy consists of amastermodel that is responsible
for triggering a low-levelmodel, the lowlevel policy consists of several
symptom checkers and a disease classifier. The proposed policy structure is
capable to deal with diagnosis problem including large number of diseases and
symptoms.
&lt;/p&gt;
&lt;p&gt;Results: Experimental results on three real-world datasets and a synthetic
dataset demonstrate that our hierarchical framework achieves higher accuracy
and symptom recall in disease diagnosis compared with existing systems. We
construct a benchmark including datasets and implementation of existing
algorithms to encourage follow-up researches.
&lt;/p&gt;
&lt;p&gt;Availability: The code and data is available from
https://github.com/FudanDISC/DISCOpen-MedBox-DialoDiagnosis
&lt;/p&gt;
&lt;p&gt;Contact: 21210980124@m.fudan.edu.cn
&lt;/p&gt;
&lt;p&gt;Supplementary information: Supplementary data are available at Bioinformatics
online.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1&quot;&gt;Cheng Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_K/0/1/0/all/0/1&quot;&gt;Kangenbei Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qianlong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1&quot;&gt;Baolin Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xuanjing Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jiajie Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1&quot;&gt;Zhongyu Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2012.08026">
<title>Classification of Smoking and Calling using Deep Learning. (arXiv:2012.08026v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2012.08026</link>
<description rdf:parseType="Literal">&lt;p&gt;Since 2014, very deep convolutional neural networks have been proposed and
become the must-have weapon for champions in all kinds of competition. In this
report, a pipeline is introduced to perform the classification of smoking and
calling by modifying the pretrained inception V3. Brightness enhancing based on
deep learning is implemented to improve the classification of this
classification task along with other useful training tricks. Based on the
quality and quantity results, it can be concluded that this pipeline with small
biased samples is practical and useful with high accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Miaowei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohacey_A/0/1/0/all/0/1&quot;&gt;Alexander William Mohacey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Apfel_J/0/1/0/all/0/1&quot;&gt;James Apfel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.11048">
<title>K-Lane: Lidar Lane Dataset and Benchmark for Urban Roads and Highways. (arXiv:2110.11048v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2110.11048</link>
<description rdf:parseType="Literal">&lt;p&gt;Lane detection is a critical function for autonomous driving. With the recent
development of deep learning and the publication of camera lane datasets and
benchmarks, camera lane detection networks (CLDNs) have been remarkably
developed. Unfortunately, CLDNs rely on camera images which are often distorted
near the vanishing line and prone to poor lighting condition. This is in
contrast with Lidar lane detection networks (LLDNs), which can directly extract
the lane lines on the bird&apos;s eye view (BEV) for motion planning and operate
robustly under various lighting conditions. However, LLDNs have not been
actively studied, mostly due to the absence of large public lidar lane
datasets. In this paper, we introduce KAIST-Lane (K-Lane), the world&apos;s first
and the largest public urban road and highway lane dataset for Lidar. K-Lane
has more than 15K frames and contains annotations of up to six lanes under
various road and traffic conditions, e.g., occluded roads of multiple occlusion
levels, roads at day and night times, merging (converging and diverging) and
curved lanes. We also provide baseline networks we term Lidar lane detection
networks utilizing global feature correlator (LLDN-GFC). LLDN-GFC exploits the
spatial characteristics of lane lines on the point cloud, which are sparse,
thin, and stretched along the entire ground plane of the point cloud. From
experimental results, LLDN-GFC achieves the state-of-the-art performance with
an F1- score of 82.1%, on the K-Lane. Moreover, LLDN-GFC shows strong
performance under various lighting conditions, which is unlike CLDNs, and also
robust even in the case of severe occlusions, unlike LLDNs using the
conventional CNN. The K-Lane, LLDN-GFC training code, pre-trained models, and
complete development kits including evaluation, visualization and annotation
tools are available at https://github.com/kaist-avelab/k-lane.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paek_D/0/1/0/all/0/1&quot;&gt;Donghee Paek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_S/0/1/0/all/0/1&quot;&gt;Seung-Hyun Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wijaya_K/0/1/0/all/0/1&quot;&gt;Kevin Tirta Wijaya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2111.11104">
<title>Hierarchical Text Classification As Sub-Hierarchy Sequence Generation. (arXiv:2111.11104v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2111.11104</link>
<description rdf:parseType="Literal">&lt;p&gt;Hierarchical text classification (HTC) is essential for various real
applications. However, HTC models are challenging to develop because they often
require processing a large volume of documents and labels with hierarchical
taxonomy. Recent HTC models based on deep learning have attempted to
incorporate hierarchy information into a model structure. Consequently, these
models are challenging to implement when the model parameters increase for a
large-scale hierarchy because the model structure depends on the hierarchy
size. To solve this problem, we formulate HTC as a sub-hierarchy sequence
generation to incorporate hierarchy information into a target label sequence
instead of the model structure. Subsequently, we propose the Hierarchy DECoder
(HiDEC), which decodes a text sequence into a sub-hierarchy sequence using
recursive hierarchy decoding, classifying all parents at the same level into
children at once. In addition, HiDEC is trained to use hierarchical path
information from a root to each leaf in a sub-hierarchy composed of the labels
of a target document via an attention mechanism and hierarchy-aware masking.
HiDEC achieved state-of-the-art performance with significantly fewer model
parameters than existing models on benchmark datasets, such as RCV1-v2, NYT,
and EURLEX57K.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Im_S/0/1/0/all/0/1&quot;&gt;SangHun Im&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1&quot;&gt;Gibaeg Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_H/0/1/0/all/0/1&quot;&gt;Heung-Seon Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jo_S/0/1/0/all/0/1&quot;&gt;Seongung Jo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Donghwan Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.10581">
<title>FAIR4Cov: Fused Audio Instance and Representation for COVID-19 Detection. (arXiv:2204.10581v3 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2204.10581</link>
<description rdf:parseType="Literal">&lt;p&gt;Audio-based classification techniques on body sounds have long been studied
to aid in the diagnosis of respiratory diseases. While most research is
centered on the use of cough as the main biomarker, other body sounds also have
the potential to detect respiratory diseases. Recent studies on COVID-19 have
shown that breath and speech sounds, in addition to cough, correlate with the
disease. Our study proposes Fused Audio Instance and Representation (FAIR) as a
method for respiratory disease detection. FAIR relies on constructing a joint
feature vector from various body sounds represented in waveform and spectrogram
form. We conducted experiments on the use case of COVID-19 detection by
combining waveform and spectrogram representation of body sounds. Our findings
show that the use of self-attention to combine extracted features from cough,
breath, and speech sounds leads to the best performance with an Area Under the
Receiver Operating Characteristic Curve (AUC) score of 0.8658, a sensitivity of
0.8057, and a specificity of 0.7958. Compared to models trained solely on
spectrograms or waveforms, the use of both representations results in an
improved AUC score, demonstrating that combining spectrogram and waveform
representation helps to enrich the extracted features and outperforms the
models that use only one representation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1&quot;&gt;Tuan Truong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lenga_M/0/1/0/all/0/1&quot;&gt;Matthias Lenga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serrurier_A/0/1/0/all/0/1&quot;&gt;Antoine Serrurier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohammadi_S/0/1/0/all/0/1&quot;&gt;Sadegh Mohammadi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.08171">
<title>K-Radar: 4D Radar Object Detection for Autonomous Driving in Various Weather Conditions. (arXiv:2206.08171v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.08171</link>
<description rdf:parseType="Literal">&lt;p&gt;Unlike RGB cameras that use visible light bands (384$\sim$769 THz) and Lidars
that use infrared bands (361$\sim$331 THz), Radars use relatively longer
wavelength radio bands (77$\sim$81 GHz), resulting in robust measurements in
adverse weathers. Unfortunately, existing Radar datasets only contain a
relatively small number of samples compared to the existing camera and Lidar
datasets. This may hinder the development of sophisticated data-driven deep
learning techniques for Radar-based perception. Moreover, most of the existing
Radar datasets only provide 3D Radar tensor (3DRT) data that contain power
measurements along the Doppler, range, and azimuth dimensions. As there is no
elevation information, it is challenging to estimate the 3D bounding box of an
object from 3DRT. In this work, we introduce KAIST-Radar (K-Radar), a novel
large-scale object detection dataset and benchmark that contains 35K frames of
4D Radar tensor (4DRT) data with power measurements along the Doppler, range,
azimuth, and elevation dimensions, together with carefully annotated 3D
bounding box labels of objects on the roads. K-Radar includes challenging
driving conditions such as adverse weathers (fog, rain, and snow) on various
road structures (urban, suburban roads, alleyways, and highways). In addition
to the 4DRT, we provide auxiliary measurements from carefully calibrated
high-resolution Lidars, surround stereo cameras, and RTK-GPS. We also provide
4DRT-based object detection baseline neural networks (baseline NNs) and show
that the height information is crucial for 3D object detection. And by
comparing the baseline NN with a similarly-structured Lidar-based neural
network, we demonstrate that 4D Radar is a more robust sensor for adverse
weather conditions. All codes are available at
https://github.com/kaist-avelab/k-radar.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paek_D/0/1/0/all/0/1&quot;&gt;Dong-Hee Paek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_S/0/1/0/all/0/1&quot;&gt;Seung-Hyun Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wijaya_K/0/1/0/all/0/1&quot;&gt;Kevin Tirta Wijaya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.10540">
<title>Rethinking Symbolic Regression Datasets and Benchmarks for Scientific Discovery. (arXiv:2206.10540v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.10540</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper revisits datasets and evaluation criteria for Symbolic Regression
(SR), specifically focused on its potential for scientific discovery. Focused
on a set of formulas used in the existing datasets based on Feynman Lectures on
Physics, we recreate 120 datasets to discuss the performance of symbolic
regression for scientific discovery (SRSD). For each of the 120 SRSD datasets,
we carefully review the properties of the formula and its variables to design
reasonably realistic sampling ranges of values so that our new SRSD datasets
can be used for evaluating the potential of SRSD such as whether or not an SR
method can (re)discover physical laws from such datasets. We also create
another 120 datasets that contain dummy variables to examine whether SR methods
can choose necessary variables only. Besides, we propose to use normalized edit
distances (NED) between a predicted equation and the true equation trees for
addressing a critical issue that existing SR metrics are either binary or
errors between the target values and an SR model&apos;s predicted values for a given
input. We conduct benchmark experiments on our new SRSD datasets using various
representative SR methods. The experimental results show that we provide a more
realistic performance evaluation, and our user study shows that the NED
correlates with human judges significantly more than an existing SR metric.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matsubara_Y/0/1/0/all/0/1&quot;&gt;Yoshitomo Matsubara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiba_N/0/1/0/all/0/1&quot;&gt;Naoya Chiba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Igarashi_R/0/1/0/all/0/1&quot;&gt;Ryo Igarashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ushiku_Y/0/1/0/all/0/1&quot;&gt;Yoshitaka Ushiku&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.12850">
<title>SSIVD-Net: A Novel Salient Super Image Classification &amp; Detection Technique for Weaponized Violence. (arXiv:2207.12850v8 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.12850</link>
<description rdf:parseType="Literal">&lt;p&gt;Detection of violence and weaponized violence in closed-circuit television
(CCTV) footage requires a comprehensive approach. In this work, we introduce
the \emph{Smart-City CCTV Violence Detection (SCVD)} dataset, specifically
designed to facilitate the learning of weapon distribution in surveillance
videos. To tackle the complexities of analyzing 3D surveillance video for
violence recognition tasks, we propose a novel technique called
\emph{SSIVD-Net} (\textbf{S}alient-\textbf{S}uper-\textbf{I}mage for
\textbf{V}iolence \textbf{D}etection). Our method reduces 3D video data
complexity, dimensionality, and information loss while improving inference,
performance, and explainability through salient-super-Image representations.
Considering the scalability and sustainability requirements of futuristic smart
cities, the authors introduce the \emph{Salient-Classifier}, a novel
architecture combining a kernelized approach with a residual learning strategy.
We evaluate variations of SSIVD-Net and Salient Classifier on our SCVD dataset
and benchmark against state-of-the-art (SOTA) models commonly employed in
violence detection. Our approach exhibits significant improvements in detecting
both weaponized and non-weaponized violence instances. By advancing the SOTA in
violence detection, our work offers a practical and scalable solution suitable
for real-world applications. The proposed methodology not only addresses the
challenges of violence detection in CCTV footage but also contributes to the
understanding of weapon distribution in smart surveillance. Ultimately, our
research findings should enable smarter and more secure cities, as well as
enhance public safety measures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aremu_T/0/1/0/all/0/1&quot;&gt;Toluwani Aremu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhiyuan_L/0/1/0/all/0/1&quot;&gt;Li Zhiyuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alameeri_R/0/1/0/all/0/1&quot;&gt;Reem Alameeri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Mustaqeem Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saddik_A/0/1/0/all/0/1&quot;&gt;Abdulmotaleb El Saddik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.06318">
<title>Towards Code Summarization of APIs Based on Unofficial Documentation Using NLP Techniques. (arXiv:2208.06318v3 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2208.06318</link>
<description rdf:parseType="Literal">&lt;p&gt;Each programming language comes with official documentation to guide
developers with APIs, methods, and classes. However, in some cases, official
documentation is not an efficient way to get the needed information. As a
result, developers may consult other sources (e.g., Stack Overflow, GitHub) to
learn more about an API, its implementation, usage, and other information that
official documentation may not provide. In this research, we propose an
automatic approach to generate summaries for APIs and methods by leveraging
unofficial documentation using NLP techniques. Our findings demonstrate that
the generated summaries are competitive, and can be used as a complementary
source for guiding developers in software development and maintenance tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naghshzan_A/0/1/0/all/0/1&quot;&gt;AmirHossein Naghshzan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.14741">
<title>Failed Goal Aware Hindsight Experience Replay. (arXiv:2208.14741v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2208.14741</link>
<description rdf:parseType="Literal">&lt;p&gt;In multi-goal reinforcement learning for a given environment, agents learn
policies to achieve multiple goals by using experiences gained from
interactions with the environment. One of the key challenges in this setting is
training agents using sparse binary rewards, which can be difficult due to a
lack of successful experiences. To address this challenge, hindsight experience
replay (HER) generates successful experiences from unsuccessful experiences.
However, the process of generating successful experiences from uniformly
sampled ones can be inefficient. In this paper, a novel approach called Failed
goal Aware HER (FAHER) is proposed to enhance the sampling efficiency. The
approach exploits the property of achieved goals in relation to failed goals
that are defined as the original goals not achieved. The proposed method
involves clustering episodes with different achieved goals using a cluster
model and subsequently sampling experiences in the manner of HER. The cluster
model is generated by applying a clustering algorithm to failed goals. The
proposed method is validated by experiments with three robotic control tasks of
the OpenAI gym. The results of experiments demonstrate that the proposed method
is more sample efficient and achieves improved performance over baseline
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Taeyoung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Har_D/0/1/0/all/0/1&quot;&gt;Dongsoo Har&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.15042">
<title>Generalizability of Adversarial Robustness Under Distribution Shifts. (arXiv:2209.15042v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.15042</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent progress in empirical and certified robustness promises to deliver
reliable and deployable Deep Neural Networks (DNNs). Despite that success, most
existing evaluations of DNN robustness have been done on images sampled from
the same distribution on which the model was trained. However, in the real
world, DNNs may be deployed in dynamic environments that exhibit significant
distribution shifts. In this work, we take a first step towards thoroughly
investigating the interplay between empirical and certified adversarial
robustness on one hand and domain generalization on another. To do so, we train
robust models on multiple domains and evaluate their accuracy and robustness on
an unseen domain. We observe that: (1) both empirical and certified robustness
generalize to unseen domains, and (2) the level of generalizability does not
correlate well with input visual similarity, measured by the FID between source
and target domains. We also extend our study to cover a real-world medical
application, in which adversarial augmentation significantly boosts the
generalization of robustness with minimal effect on clean data accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alhamoud_K/0/1/0/all/0/1&quot;&gt;Kumail Alhamoud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hammoud_H/0/1/0/all/0/1&quot;&gt;Hasan Abed Al Kader Hammoud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alfarra_M/0/1/0/all/0/1&quot;&gt;Motasem Alfarra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1&quot;&gt;Bernard Ghanem&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.01984">
<title>Manipulation and Peer Mechanisms: A Survey. (arXiv:2210.01984v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2210.01984</link>
<description rdf:parseType="Literal">&lt;p&gt;In peer mechanisms, the competitors for a prize also determine who wins. Each
competitor may be asked to rank, grade, or nominate peers for the prize. Since
the prize can be valuable, such as financial aid, course grades, or an award at
a conference, competitors may be tempted to manipulate the mechanism. We survey
approaches to prevent or discourage the manipulation of peer mechanisms. We
conclude our survey by identifying several important research challenges.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olckers_M/0/1/0/all/0/1&quot;&gt;Matthew Olckers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walsh_T/0/1/0/all/0/1&quot;&gt;Toby Walsh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.13709">
<title>Undesirable biases in NLP: Addressing challenges of measurement. (arXiv:2211.13709v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2211.13709</link>
<description rdf:parseType="Literal">&lt;p&gt;As Large Language Models and Natural Language Processing (NLP) technology
rapidly develop and spread into daily life, it becomes crucial to anticipate
how their use could harm people. One problem that has received a lot of
attention in recent years is that this technology has displayed harmful biases,
from generating derogatory stereotypes to producing disparate outcomes for
different social groups. Although a lot of effort has been invested in
assessing and mitigating these biases, our methods of measuring the biases of
NLP models have serious problems and it is often unclear what they actually
measure. In this paper, we provide an interdisciplinary approach to discussing
the issue of NLP model bias by adopting the lens of psychometrics -- a field
specialized in the measurement of concepts like bias that are not directly
observable. In particular, we will explore two central notions from
psychometrics, the \emph{construct validity} and the \emph{reliability} of
measurement tools, and discuss how they can be applied in the context of
measuring model bias. Our goal is to provide NLP practitioners with
methodological tools for designing better bias measures, and to inspire them
more generally to explore tools from psychometrics when working on bias
measurement tools.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wal_O/0/1/0/all/0/1&quot;&gt;Oskar van der Wal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bachmann_D/0/1/0/all/0/1&quot;&gt;Dominik Bachmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leidinger_A/0/1/0/all/0/1&quot;&gt;Alina Leidinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maanen_L/0/1/0/all/0/1&quot;&gt;Leendert van Maanen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuidema_W/0/1/0/all/0/1&quot;&gt;Willem Zuidema&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulz_K/0/1/0/all/0/1&quot;&gt;Katrin Schulz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.01346">
<title>Guaranteed Conformance of Neurosymbolic Models to Natural Constraints. (arXiv:2212.01346v8 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.01346</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks have emerged as the workhorse for a large section of
robotics and control applications, especially as models for dynamical systems.
Such data-driven models are in turn used for designing and verifying autonomous
systems. They are particularly useful in modeling medical systems where data
can be leveraged to individualize treatment. In safety-critical applications,
it is important that the data-driven model is conformant to established
knowledge from the natural sciences. Such knowledge is often available or can
often be distilled into a (possibly black-box) model. For instance, an F1
racing car should conform to Newton&apos;s laws (which are encoded within a unicycle
model). In this light, we consider the following problem - given a model $M$
and a state transition dataset, we wish to best approximate the system model
while being a bounded distance away from $M$. We propose a method to guarantee
this conformance. Our first step is to distill the dataset into a few
representative samples called memories, using the idea of a growing neural gas.
Next, using these memories we partition the state space into disjoint subsets
and compute bounds that should be respected by the neural network in each
subset. This serves as a symbolic wrapper for guaranteed conformance. We argue
theoretically that this only leads to a bounded increase in approximation
error; which can be controlled by increasing the number of memories. We
experimentally show that on three case studies (Car Model, Drones, and
Artificial Pancreas), our constrained neurosymbolic models conform to specified
models (each encoding various constraints) with order-of-magnitude improvements
compared to the augmented Lagrangian and vanilla training methods. Our code can
be found at: https://github.com/kaustubhsridhar/Constrained_Models
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sridhar_K/0/1/0/all/0/1&quot;&gt;Kaustubh Sridhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1&quot;&gt;Souradeep Dutta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weimer_J/0/1/0/all/0/1&quot;&gt;James Weimer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1&quot;&gt;Insup Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.10884">
<title>Break It Down: Evidence for Structural Compositionality in Neural Networks. (arXiv:2301.10884v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2301.10884</link>
<description rdf:parseType="Literal">&lt;p&gt;Though modern neural networks have achieved impressive performance in both
vision and language tasks, we know little about the functions that they
implement. One possibility is that neural networks implicitly break down
complex tasks into subroutines, implement modular solutions to these
subroutines, and compose them into an overall solution to a task - a property
we term structural compositionality. Another possibility is that they may
simply learn to match new inputs to learned templates, eliding task
decomposition entirely. Here, we leverage model pruning techniques to
investigate this question in both vision and language across a variety of
architectures, tasks, and pretraining regimens. Our results demonstrate that
models often implement solutions to subroutines via modular subnetworks, which
can be ablated while maintaining the functionality of other subnetworks. This
suggests that neural networks may be able to learn compositionality, obviating
the need for specialized symbolic mechanisms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lepori_M/0/1/0/all/0/1&quot;&gt;Michael A. Lepori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serre_T/0/1/0/all/0/1&quot;&gt;Thomas Serre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1&quot;&gt;Ellie Pavlick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.04178">
<title>DynGFN: Towards Bayesian Inference of Gene Regulatory Networks with GFlowNets. (arXiv:2302.04178v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.04178</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the grand challenges of cell biology is inferring the gene regulatory
network (GRN) which describes interactions between genes and their products
that control gene expression and cellular function. We can treat this as a
causal discovery problem but with two non-standard challenges: (1) regulatory
networks are inherently cyclic so we should not model a GRN as a directed
acyclic graph (DAG), and (2) observations have significant measurement noise,
so for typical sample sizes there will always be a large equivalence class of
graphs that are likely given the data, and we want methods that capture this
uncertainty. Existing methods either focus on challenge (1), identifying cyclic
structure from dynamics, or on challenge (2) learning complex Bayesian
posteriors over DAGs, but not both. In this paper we leverage the fact that it
is possible to estimate the &quot;velocity&quot; of gene expression with RNA velocity
techniques to develop an approach that addresses both challenges. Because we
have access to velocity information, we can treat the Bayesian structure
learning problem as a problem of sparse identification of a dynamical system,
capturing cyclic feedback loops through time. Since our objective is to model
uncertainty over discrete structures, we leverage Generative Flow Networks
(GFlowNets) to estimate the posterior distribution over the combinatorial space
of possible sparse dependencies. Our results indicate that our method learns
posteriors that better encapsulate the distributions of cyclic structures
compared to counterpart state-of-the-art Bayesian structure learning
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atanackovic_L/0/1/0/all/0/1&quot;&gt;Lazar Atanackovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_A/0/1/0/all/0/1&quot;&gt;Alexander Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1&quot;&gt;Leo J. Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hartford_J/0/1/0/all/0/1&quot;&gt;Jason Hartford&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.12000">
<title>Graph Construction using Principal Axis Trees for Simple Graph Convolution. (arXiv:2302.12000v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.12000</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) are increasingly becoming the favorite method
for graph learning. They exploit the semi-supervised nature of deep learning,
and they bypass computational bottlenecks associated with traditional graph
learning methods. In addition to the feature matrix $X$, GNNs need an adjacency
matrix $A$ to perform feature propagation. In many cases, the adjacency matrix
$A$ is missing. We introduce a graph construction scheme that constructs the
adjacency matrix $A$ using unsupervised and supervised information.
Unsupervised information characterizes the neighborhood around points. We used
Principal Axis trees (PA-trees) as a source for unsupervised information, where
we create edges between points falling onto the same leaf node. For supervised
information, we used the concept of penalty and intrinsic graphs. A penalty
graph connects points with different class labels, whereas an intrinsic graph
connects points with the same class labels. We used the penalty and intrinsic
graphs to remove or add edges to the graph constructed via PA-tree. We tested
this graph construction scheme on two well-known GNNs: 1) Graph Convolutional
Network (GCN) and 2) Simple Graph Convolution (SGC). The experiments show that
it is better to use SGC because it is faster and delivers better or the same
results as GCN. We also test the effect of oversmoothing on both GCN and SGC.
We found out that the level of smoothing has to be carefully selected for SGC
to avoid oversmoothing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alshammari_M/0/1/0/all/0/1&quot;&gt;Mashaan Alshammari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stavrakakis_J/0/1/0/all/0/1&quot;&gt;John Stavrakakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1&quot;&gt;Adel F. Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takatsuka_M/0/1/0/all/0/1&quot;&gt;Masahiro Takatsuka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.09354">
<title>The NCI Imaging Data Commons as a platform for reproducible research in computational pathology. (arXiv:2303.09354v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.09354</link>
<description rdf:parseType="Literal">&lt;p&gt;Background and Objectives: Reproducibility is a major challenge in developing
machine learning (ML)-based solutions in computational pathology (CompPath).
The NCI Imaging Data Commons (IDC) provides &amp;gt;120 cancer image collections
according to the FAIR principles and is designed to be used with cloud ML
services. Here, we explore its potential to facilitate reproducibility in
CompPath research.
&lt;/p&gt;
&lt;p&gt;Methods: Using the IDC, we implemented two experiments in which a
representative ML-based method for classifying lung tumor tissue was trained
and/or evaluated on different datasets. To assess reproducibility, the
experiments were run multiple times with separate but identically configured
instances of common ML services.
&lt;/p&gt;
&lt;p&gt;Results: The AUC values of different runs of the same experiment were
generally consistent. However, we observed small variations in AUC values of up
to 0.045, indicating a practical limit to reproducibility.
&lt;/p&gt;
&lt;p&gt;Conclusions: We conclude that the IDC facilitates approaching the
reproducibility limit of CompPath research (i) by enabling researchers to reuse
exactly the same datasets and (ii) by integrating with cloud ML services so
that experiments can be run in identically configured computing environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schacherer_D/0/1/0/all/0/1&quot;&gt;Daniela P. Schacherer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herrmann_M/0/1/0/all/0/1&quot;&gt;Markus D. Herrmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clunie_D/0/1/0/all/0/1&quot;&gt;David A. Clunie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hofener_H/0/1/0/all/0/1&quot;&gt;Henning H&amp;#xf6;fener&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clifford_W/0/1/0/all/0/1&quot;&gt;William Clifford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Longabaugh_W/0/1/0/all/0/1&quot;&gt;William J.R. Longabaugh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pieper_S/0/1/0/all/0/1&quot;&gt;Steve Pieper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kikinis_R/0/1/0/all/0/1&quot;&gt;Ron Kikinis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fedorov_A/0/1/0/all/0/1&quot;&gt;Andrey Fedorov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Homeyer_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Homeyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.10093">
<title>Investigating the Role of Attribute Context in Vision-Language Models for Object Recognition and Detection. (arXiv:2303.10093v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.10093</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision-language alignment learned from image-caption pairs has been shown to
benefit tasks like object recognition and detection. Methods are mostly
evaluated in terms of how well object class names are learned, but captions
also contain rich attribute context that should be considered when learning
object alignment. It is unclear how methods use this context in learning, as
well as whether models succeed when tasks require attribute and object
understanding. To address this gap, we conduct extensive analysis of the role
of attributes in vision-language models. We specifically measure model
sensitivity to the presence and meaning of attribute context, gauging influence
on object embeddings through unsupervised phrase grounding and classification
via description methods. We further evaluate the utility of attribute context
in training for open-vocabulary object detection, fine-grained text-region
retrieval, and attribution tasks. Our results show that attribute context can
be wasted when learning alignment for detection, attribute meaning is not
adequately considered in embeddings, and describing classes by only their
attributes is ineffective. A viable strategy that we find to increase benefits
from attributes is contrastive training with adjective-based negative captions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buettner_K/0/1/0/all/0/1&quot;&gt;Kyle Buettner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kovashka_A/0/1/0/all/0/1&quot;&gt;Adriana Kovashka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.12040">
<title>Roots and Requirements for Collaborative AIs. (arXiv:2303.12040v5 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2303.12040</link>
<description rdf:parseType="Literal">&lt;p&gt;The vision of AI collaborators is a staple of mythology and science fiction,
where artificial agents with special talents assist human partners and teams.
In this dream, sophisticated AIs understand nuances of collaboration and human
communication. The AI as collaborator dream is different from computer tools
that augment human intelligence (IA) or intermediate human collaboration. Such
tools have their roots in the 1960s and helped to drive an information
technology revolution. They can be useful but they are not intelligent and do
not collaborate as effectively as skilled people. With the increase of hybrid
and remote work since the COVID pandemic, the benefits and requirements for
better coordination, collaboration, and communication are becoming a hot topic
in the workplace. Employers and workers face choices and trade-offs as they
negotiate the options for working from home versus working at the office. Many
factors such as the high costs of homes near employers are impeding a mass
return to the office. Government advisory groups and leaders in AI have
advocated for years that AIs should be transparent and effective collaborators.
Nonetheless, robust AIs that collaborate like talented people remain out of
reach. Are AI teammates part of a solution? How artificially intelligent (AI)
could and should they be? This position paper reviews the arc of technology and
public calls for human-machine teaming. It draws on earlier research in
psychology and the social sciences about what human-like collaboration
requires. This paper sets a context for a second science-driven paper that
advocates a radical shift in technology and methodology for creating resilient,
intelligent, and human-compatible AIs (Stefik &amp;amp; Price, 2023). The aspirational
goal is that such AIs would learn, share what they learn, and collaborate to
achieve high capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stefik_M/0/1/0/all/0/1&quot;&gt;Mark Stefik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.15714">
<title>Explicit Planning Helps Language Models in Logical Reasoning. (arXiv:2303.15714v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2303.15714</link>
<description rdf:parseType="Literal">&lt;p&gt;Language models have been shown to perform remarkably well on a wide range of
natural language processing tasks. In this paper, we propose LEAP, a novel
system that uses language models to perform multi-step logical reasoning and
incorporates explicit planning into the inference procedure. Explicit planning
enables the system to make more informed reasoning decisions at each step by
looking ahead into their future effects. Moreover, we propose a training
strategy that safeguards the planning process from being led astray by spurious
features. Our full system significantly outperforms other competing methods on
multiple standard datasets. When using small T5 models as its core selection
and deduction components, our system performs competitively compared to GPT-3
despite having only about 1B parameters (i.e., 175 times smaller than GPT-3).
When using GPT-3.5, it significantly outperforms chain-of-thought prompting on
the challenging PrOntoQA dataset. We have conducted extensive empirical studies
to demonstrate that explicit planning plays a crucial role in the system&apos;s
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hongyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kangrui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1&quot;&gt;Mo Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1&quot;&gt;Hongyuan Mei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.16258">
<title>Optimisation via encodings: a renormalisation group perspective. (arXiv:2303.16258v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2303.16258</link>
<description rdf:parseType="Literal">&lt;p&gt;Difficult, in particular NP-complete, optimization problems are traditionally
solved approximately using search heuristics. These are usually slowed down by
the rugged landscapes encountered, because local minima arrest the search
process. Cover-encoding maps were devised to circumvent this problem by
transforming the original landscape to one that is free of local minima and
enriched in near-optimal solutions. By definition, these involve the mapping of
the original (larger) search space into smaller subspaces, by processes that
typically amount to a form of coarse-graining. In this paper, we explore the
details of this coarse-graining using formal arguments, as well as concrete
examples of cover-encoding maps, that are investigated analytically as well as
computationally. Our results strongly suggest that the coarse-graining involved
in cover-encoding maps bears a strong resemblance to that encountered in
renormalisation group schemes. Given the apparently disparate nature of these
two formalisms, these strong similarities are rather startling, and suggest
deep mathematical underpinnings that await further exploration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klemm_K/0/1/0/all/0/1&quot;&gt;Konstantin Klemm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehta_A/0/1/0/all/0/1&quot;&gt;Anita Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stadler_P/0/1/0/all/0/1&quot;&gt;Peter F. Stadler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.10749">
<title>Multi-scale Evolutionary Neural Architecture Search for Deep Spiking Neural Networks. (arXiv:2304.10749v5 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2304.10749</link>
<description rdf:parseType="Literal">&lt;p&gt;Spiking Neural Networks (SNNs) have received considerable attention not only
for their superiority in energy efficiency with discrete signal processing but
also for their natural suitability to integrate multi-scale biological
plasticity. However, most SNNs directly adopt the structure of the
well-established Deep Neural Networks (DNNs), and rarely automatically design
Neural Architecture Search (NAS) for SNNs. The neural motifs topology, modular
regional structure and global cross-brain region connection of the human brain
are the product of natural evolution and can serve as a perfect reference for
designing brain-inspired SNN architecture. In this paper, we propose a
Multi-Scale Evolutionary Neural Architecture Search (MSE-NAS) for SNN,
simultaneously considering micro-, meso- and macro-scale brain topologies as
the evolutionary search space. MSE-NAS evolves individual neuron operation,
self-organized integration of multiple circuit motifs, and global connectivity
across motifs through a brain-inspired indirect evaluation function,
Representational Dissimilarity Matrices (RDMs). This training-free fitness
function could greatly reduce computational consumption and NAS&apos;s time, and its
task-independent property enables the searched SNNs to exhibit excellent
transferability on multiple datasets. Furthermore, MSE-NAS show robustness
against the training method and noise. Extensive experiments demonstrate that
the proposed algorithm achieves state-of-the-art (SOTA) performance with
shorter simulation steps on static datasets (CIFAR10, CIFAR100) and
neuromorphic datasets (CIFAR10-DVS and DVS128-Gesture). The thorough analysis
also illustrates the significant performance improvement and consistent
bio-interpretability deriving from the topological evolution at different
scales and the RDMs fitness function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1&quot;&gt;Wenxuan Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1&quot;&gt;Feifei Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_G/0/1/0/all/0/1&quot;&gt;Guobin Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1&quot;&gt;Yi Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.11235">
<title>Spatial-Language Attention Policies for Efficient Robot Learning. (arXiv:2304.11235v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2304.11235</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite great strides in language-guided manipulation, existing work has been
constrained to table-top settings. Table-tops allow for perfect and consistent
camera angles, properties are that do not hold in mobile manipulation. Task
plans that involve moving around the environment must be robust to egocentric
views and changes in the plane and angle of grasp. A further challenge is
ensuring this is all true while still being able to learn skills efficiently
from limited data. We propose Spatial-Language Attention Policies (SLAP) as a
solution. SLAP uses three-dimensional tokens as the input representation to
train a single multi-task, language-conditioned action prediction policy. Our
method shows an 80% success rate in the real world across eight tasks with a
single model, and a 47.5% success rate when unseen clutter and unseen object
configurations are introduced, even with only a handful of examples per task.
This represents an improvement of 30% over prior work (20% given unseen
distractors and configurations). We see a 4x improvement over baseline in
mobile manipulation setting. In addition, we show how SLAPs robustness allows
us to execute Task Plans from open-vocabulary instructions using a large
language model for multi-step mobile manipulation. For videos, see the website:
https://robotslap.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parashar_P/0/1/0/all/0/1&quot;&gt;Priyam Parashar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1&quot;&gt;Vidhi Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaohan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vakil_J/0/1/0/all/0/1&quot;&gt;Jay Vakil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Powers_S/0/1/0/all/0/1&quot;&gt;Sam Powers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1&quot;&gt;Yonatan Bisk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1&quot;&gt;Chris Paxton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.13105">
<title>Attention-Enhanced Deep Learning for Device-Free Through-the-Wall Presence Detection Using Indoor WiFi System. (arXiv:2304.13105v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.13105</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate detection of human presence in indoor environments is important for
various applications, such as energy management and security. In this paper, we
propose a novel system for human presence detection using the channel state
information (CSI) of WiFi signals. Our system named attention-enhanced deep
learning for presence detection (ALPD) employs an attention mechanism to
automatically select informative subcarriers from the CSI data and a
bidirectional long short-term memory (LSTM) network to capture temporal
dependencies in CSI. Additionally, we utilize a static feature to improve the
accuracy of human presence detection in static states. We evaluate the proposed
ALPD system by deploying a pair of WiFi access points (APs) for collecting CSI
dataset, which is further compared with several benchmarks. The results
demonstrate that our ALPD system outperforms the benchmarks in terms of
accuracy, especially in the presence of interference. Moreover, bidirectional
transmission data is beneficial to training improving stability and accuracy,
as well as reducing the costs of data collection for training. Overall, our
proposed ALPD system shows promising results for human presence detection using
WiFi CSI signals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Li-Hsiang Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1&quot;&gt;Kuan-I Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsiao_A/0/1/0/all/0/1&quot;&gt;An-Hung Hsiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_K/0/1/0/all/0/1&quot;&gt;Kai-Ten Feng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.03353">
<title>MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic. (arXiv:2305.03353v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.03353</link>
<description rdf:parseType="Literal">&lt;p&gt;Theory of Mind (ToM) is a critical component of intelligence but its
assessment remains the subject of heated debates. Prior research applied human
ToM assessments to natural language processing models using either
human-created standardized tests or rule-based templates. However, these
methods primarily focus on simplistic reasoning and require further validation.
Here, we leverage dynamic epistemic logic to isolate a particular component of
ToM and to generate controlled problems. We also introduce new verbalization
techniques to express these problems in English natural language. Our findings
indicate that some language model scaling (from 70M to 6B and 350M to 174B)
does not consistently yield results better than random chance. While GPT-4
demonstrates superior epistemic reasoning capabilities, there is still room for
improvement. Our code and datasets are publicly available
(https://huggingface.co/datasets/sileod/mindgames ,
https://github.com/sileod/llm-theory-of-mind )
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sileo_D/0/1/0/all/0/1&quot;&gt;Damien Sileo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lernould_A/0/1/0/all/0/1&quot;&gt;Antoine Lernould&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14517">
<title>CongFu: Conditional Graph Fusion for Drug Synergy Prediction. (arXiv:2305.14517v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14517</link>
<description rdf:parseType="Literal">&lt;p&gt;Drug synergy, characterized by the amplified combined effect of multiple
drugs, is critically important for optimizing therapeutic outcomes. Limited
data on drug synergy, arising from the vast number of possible drug
combinations and testing costs, motivate the need for predictive methods. In
this work, we introduce CongFu, a novel Conditional Graph Fusion Layer,
designed to predict drug synergy. CongFu employs an attention mechanism and a
bottleneck to extract local graph contexts and conditionally fuse graph data
within a global context. Its modular architecture enables flexible replacement
of layer modules, including readouts and graph encoders, facilitating
customization for diverse applications. To evaluate the performance of CongFu,
we conduct comprehensive experiments on four datasets, encompassing three
distinct setups for drug synergy prediction. CongFu achieves state-of-the-art
results on 11 out of 12 benchmark datasets, demonstrating its ability to
capture intricate patterns of drug synergy. Through ablation studies, we
validate the significance of individual layer components, affirming their
contributions to overall predictive performance. Finally, we propose an
explainability strategy for elucidating the effect of drugs on genes. By
addressing the challenge of predicting drug synergy in untested drug pairs and
utilizing our proposed explainability approach, CongFu opens new avenues for
optimizing drug combinations and advancing personalized medicine.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsepa_O/0/1/0/all/0/1&quot;&gt;Oleksii Tsepa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naida_B/0/1/0/all/0/1&quot;&gt;Bohdan Naida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldenberg_A/0/1/0/all/0/1&quot;&gt;Anna Goldenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bo Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19466">
<title>The Impact of Positional Encoding on Length Generalization in Transformers. (arXiv:2305.19466v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19466</link>
<description rdf:parseType="Literal">&lt;p&gt;Length generalization, the ability to generalize from small training context
sizes to larger ones, is a critical challenge in the development of
Transformer-based language models. Positional encoding (PE) has been identified
as a major factor influencing length generalization, but the exact impact of
different PE schemes on extrapolation in downstream tasks remains unclear. In
this paper, we conduct a systematic empirical study comparing the length
generalization performance of decoder-only Transformers with five different
position encoding approaches including Absolute Position Embedding (APE), T5&apos;s
Relative PE, ALiBi, and Rotary, in addition to Transformers without positional
encoding (NoPE). Our evaluation encompasses a battery of reasoning and
mathematical tasks. Our findings reveal that the most commonly used positional
encoding methods, such as ALiBi, Rotary, and APE, are not well suited for
length generalization in downstream tasks. More importantly, NoPE outperforms
other explicit positional encoding methods while requiring no additional
computation. We theoretically demonstrate that NoPE can represent both absolute
and relative PEs, but when trained with SGD, it mostly resembles T5&apos;s relative
PE attention patterns. Finally, we find that scratchpad is not always helpful
to solve length generalization and its format highly impacts the model&apos;s
performance. Overall, our work suggests that explicit position embeddings are
not essential for decoder-only Transformers to generalize well to longer
sequences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kazemnejad_A/0/1/0/all/0/1&quot;&gt;Amirhossein Kazemnejad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Padhi_I/0/1/0/all/0/1&quot;&gt;Inkit Padhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramamurthy_K/0/1/0/all/0/1&quot;&gt;Karthikeyan Natesan Ramamurthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1&quot;&gt;Payel Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1&quot;&gt;Siva Reddy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.02866">
<title>Learning Probabilistic Symmetrization for Architecture Agnostic Equivariance. (arXiv:2306.02866v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.02866</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel framework to overcome the limitations of equivariant
architectures in learning functions with group symmetries. In contrary to
equivariant architectures, we use an arbitrary base model such as an MLP or a
transformer and symmetrize it to be equivariant to the given group by employing
a small equivariant network that parameterizes the probabilistic distribution
underlying the symmetrization. The distribution is end-to-end trained with the
base model which can maximize performance while reducing sample complexity of
symmetrization. We show that this approach ensures not only equivariance to
given group but also universal approximation capability in expectation. We
implement our method on various base models, including patch-based transformers
that can be initialized from pretrained vision transformers, and test them for
a wide range of symmetry groups including permutation and Euclidean groups and
their combinations. Empirical tests show competitive results against tailored
equivariant architectures, suggesting the potential for learning equivariant
functions for diverse groups using a non-equivariant universal base
architecture. We further show evidence of enhanced learning in symmetric
modalities, like graphs, when pretrained from non-symmetric modalities, like
vision. Code is available at https://github.com/jw9730/lps.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jinwoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tien Dat Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suleymanzade_A/0/1/0/all/0/1&quot;&gt;Ayhan Suleymanzade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_H/0/1/0/all/0/1&quot;&gt;Hyeokjun An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1&quot;&gt;Seunghoon Hong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04061">
<title>Deploying a Robust Active Preference Elicitation Algorithm on MTurk: Experiment Design, Interface, and Evaluation for COVID-19 Patient Prioritization. (arXiv:2306.04061v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04061</link>
<description rdf:parseType="Literal">&lt;p&gt;Preference elicitation leverages AI or optimization to learn stakeholder
preferences in settings ranging from marketing to public policy. The online
robust preference elicitation procedure of &lt;a href=&quot;/abs/2003.01899&quot;&gt;arXiv:2003.01899&lt;/a&gt; has been shown in
simulation to outperform various other elicitation procedures in terms of
effectively learning individuals&apos; true utilities. However, as with any
simulation, the method makes a series of assumptions that cannot easily be
verified to hold true beyond simulation. Thus, we propose to validate the
robust method&apos;s performance using real users, focusing on the particular
challenge of selecting policies for prioritizing COVID-19 patients for scarce
hospital resources during the pandemic. To this end, we develop an online
platform for preference elicitation where users report their preferences
between alternatives over a moderate number of pairwise comparisons chosen by a
particular elicitation procedure. We recruit 193 Amazon Mechanical Turk (MTurk)
workers to report their preferences and demonstrate that the robust method
outperforms asking random queries by 21%, the next best performing method in
the simulated results of &lt;a href=&quot;/abs/2003.01899&quot;&gt;arXiv:2003.01899&lt;/a&gt;, in terms of recommending policies
with a higher utility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johnston_C/0/1/0/all/0/1&quot;&gt;Caroline M. Johnston&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vossler_P/0/1/0/all/0/1&quot;&gt;Patrick Vossler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blessenohl_S/0/1/0/all/0/1&quot;&gt;Simon Blessenohl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vayanos_P/0/1/0/all/0/1&quot;&gt;Phebe Vayanos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05284">
<title>Simple and Controllable Music Generation. (arXiv:2306.05284v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05284</link>
<description rdf:parseType="Literal">&lt;p&gt;We tackle the task of conditional music generation. We introduce MusicGen, a
single Language Model (LM) that operates over several streams of compressed
discrete music representation, i.e., tokens. Unlike prior work, MusicGen is
comprised of a single-stage transformer LM together with efficient token
interleaving patterns, which eliminates the need for cascading several models,
e.g., hierarchically or upsampling. Following this approach, we demonstrate how
MusicGen can generate high-quality samples, both mono and stereo, while being
conditioned on textual description or melodic features, allowing better
controls over the generated output. We conduct extensive empirical evaluation,
considering both automatic and human studies, showing the proposed approach is
superior to the evaluated baselines on a standard text-to-music benchmark.
Through ablation studies, we shed light over the importance of each of the
components comprising MusicGen. Music samples, code, and models are available
at https://github.com/facebookresearch/audiocraft
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Copet_J/0/1/0/all/0/1&quot;&gt;Jade Copet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kreuk_F/0/1/0/all/0/1&quot;&gt;Felix Kreuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gat_I/0/1/0/all/0/1&quot;&gt;Itai Gat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1&quot;&gt;Tal Remez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kant_D/0/1/0/all/0/1&quot;&gt;David Kant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1&quot;&gt;Gabriel Synnaeve&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1&quot;&gt;Yossi Adi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Defossez_A/0/1/0/all/0/1&quot;&gt;Alexandre D&amp;#xe9;fossez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11412">
<title>Size Matters: Large Graph Generation with HiGGs. (arXiv:2306.11412v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11412</link>
<description rdf:parseType="Literal">&lt;p&gt;Large graphs are present in a variety of domains, including social networks,
civil infrastructure, and the physical sciences to name a few. Graph generation
is similarly widespread, with applications in drug discovery, network analysis
and synthetic datasets among others. While GNN (Graph Neural Network) models
have been applied in these domains their high in-memory costs restrict them to
small graphs. Conversely less costly rule-based methods struggle to reproduce
complex structures. We propose HIGGS (Hierarchical Generation of Graphs) as a
model-agnostic framework of producing large graphs with realistic local
structures. HIGGS uses GNN models with conditional generation capabilities to
sample graphs in hierarchies of resolution. As a result HIGGS has the capacity
to extend the scale of generated graphs from a given GNN model by quadratic
order. As a demonstration we implement HIGGS using DiGress, a recent
graph-diffusion model, including a novel edge-predictive-diffusion variant
edge-DiGress. We use this implementation to generate categorically attributed
graphs with tens of thousands of nodes. These HIGGS generated graphs are far
larger than any previously produced using GNNs. Despite this jump in scale we
demonstrate that the graphs produced by HIGGS are, on the local scale, more
realistic than those from the rule-based model BTER.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davies_A/0/1/0/all/0/1&quot;&gt;Alex O. Davies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ajmeri_N/0/1/0/all/0/1&quot;&gt;Nirav S. Ajmeri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Filho_T/0/1/0/all/0/1&quot;&gt;Telmo M. Silva Filho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.13258">
<title>Fast Maximum $k$-Plex Algorithms Parameterized by Small Degeneracy Gaps. (arXiv:2306.13258v2 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/2306.13258</link>
<description rdf:parseType="Literal">&lt;p&gt;Given a graph, a $k$-plex is a set of vertices in which each vertex is not
adjacent to at most $k-1$ other vertices in the set. The maximum $k$-plex
problem, which asks for the largest $k$-plex from the given graph, is an
important but computationally challenging problem in applications such as graph
mining and community detection. So far, there are many practical algorithms,
but without providing theoretical explanations on their efficiency. We define a
novel parameter of the input instance, $g_k(G)$, the gap between the degeneracy
bound and the size of the maximum $k$-plex in the given graph, and present an
exact algorithm parameterized by this $g_k(G)$, which has a worst-case running
time polynomial in the size of the input graph and exponential in $g_k(G)$. In
real-world inputs, $g_k(G)$ is very small, usually bounded by $O(\log{(|V|)})$,
indicating that the algorithm runs in polynomial time. We further extend our
discussion to an even smaller parameter $cg_k(G)$, the gap between the
community-degeneracy bound and the size of the maximum $k$-plex, and show that
without much modification, our algorithm can also be parameterized by
$cg_k(G)$. To verify the empirical performance of these algorithms, we carry
out extensive experiments to show that these algorithms are competitive with
the state-of-the-art algorithms. In particular, for large $k$ values such as
$15$ and $20$, our algorithms dominate the existing algorithms. Finally,
empirical analysis is performed to illustrate the effectiveness of the
parameters and other key components in the implementation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhengren Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1&quot;&gt;Chunyu Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1&quot;&gt;Mingyu Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1&quot;&gt;Jin-Kao Hao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14351">
<title>Comparing Causal Frameworks: Potential Outcomes, Structural Models, Graphs, and Abstractions. (arXiv:2306.14351v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14351</link>
<description rdf:parseType="Literal">&lt;p&gt;The aim of this paper is to make clear and precise the relationship between
the Rubin causal model (RCM) and structural causal model (SCM) frameworks for
causal inference. Adopting a neutral logical perspective, and drawing on
previous work, we show what is required for an RCM to be representable by an
SCM. A key result then shows that every RCM -- including those that violate
algebraic principles implied by the SCM framework -- emerges as an abstraction
of some representable RCM. Finally, we illustrate the power of this
conciliatory perspective by pinpointing an important role for SCM principles in
classic applications of RCMs; conversely, we offer a characterization of the
algebraic constraints implied by a graph, helping to substantiate further
comparisons between the two frameworks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ibeling_D/0/1/0/all/0/1&quot;&gt;Duligur Ibeling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Icard_T/0/1/0/all/0/1&quot;&gt;Thomas Icard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14770">
<title>ProtoDiff: Learning to Learn Prototypical Networks by Task-Guided Diffusion. (arXiv:2306.14770v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14770</link>
<description rdf:parseType="Literal">&lt;p&gt;Prototype-based meta-learning has emerged as a powerful technique for
addressing few-shot learning challenges. However, estimating a deterministic
prototype using a simple average function from a limited number of examples
remains a fragile process. To overcome this limitation, we introduce ProtoDiff,
a novel framework that leverages a task-guided diffusion model during the
meta-training phase to gradually generate prototypes, thereby providing
efficient class representations. Specifically, a set of prototypes is optimized
to achieve per-task prototype overfitting, enabling accurately obtaining the
overfitted prototypes for individual tasks. Furthermore, we introduce a
task-guided diffusion process within the prototype space, enabling the
meta-learning of a generative process that transitions from a vanilla prototype
to an overfitted prototype. ProtoDiff gradually generates task-specific
prototypes from random noise during the meta-test stage, conditioned on the
limited samples available for the new task. Furthermore, to expedite training
and enhance ProtoDiff&apos;s performance, we propose the utilization of residual
prototype learning, which leverages the sparsity of the residual prototype. We
conduct thorough ablation studies to demonstrate its ability to accurately
capture the underlying prototype distribution and enhance generalization. The
new state-of-the-art performance on within-domain, cross-domain, and few-task
few-shot classification further substantiates the benefit of ProtoDiff.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yingjun Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1&quot;&gt;Zehao Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1&quot;&gt;Shengcai Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1&quot;&gt;Cees Snoek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16819">
<title>Graph Denoising Diffusion for Inverse Protein Folding. (arXiv:2306.16819v2 [q-bio.QM] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16819</link>
<description rdf:parseType="Literal">&lt;p&gt;Inverse protein folding is challenging due to its inherent one-to-many
mapping characteristic, where numerous possible amino acid sequences can fold
into a single, identical protein backbone. This task involves not only
identifying viable sequences but also representing the sheer diversity of
potential solutions. However, existing discriminative models, such as
transformer-based auto-regressive models, struggle to encapsulate the diverse
range of plausible solutions. In contrast, diffusion probabilistic models, as
an emerging genre of generative approaches, offer the potential to generate a
diverse set of sequence candidates for determined protein backbones. We propose
a novel graph denoising diffusion model for inverse protein folding, where a
given protein backbone guides the diffusion process on the corresponding amino
acid residue types. The model infers the joint distribution of amino acids
conditioned on the nodes&apos; physiochemical properties and local environment.
Moreover, we utilize amino acid replacement matrices for the diffusion forward
process, encoding the biologically-meaningful prior knowledge of amino acids
from their spatial and sequential neighbors as well as themselves, which
reduces the sampling space of the generative process. Our model achieves
state-of-the-art performance over a set of popular baseline methods in sequence
recovery and exhibits great potential in generating diverse protein sequences
for a determined protein backbone structure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Yi_K/0/1/0/all/0/1&quot;&gt;Kai Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Bingxin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yiqing Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lio_P/0/1/0/all/0/1&quot;&gt;Pietro Li&amp;#xf2;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu Guang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04841">
<title>Loss Dynamics of Temporal Difference Reinforcement Learning. (arXiv:2307.04841v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04841</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning has been successful across several applications in
which agents have to learn to act in environments with sparse feedback.
However, despite this empirical success there is still a lack of theoretical
understanding of how the parameters of reinforcement learning models and the
features used to represent states interact to control the dynamics of learning.
In this work, we use concepts from statistical physics, to study the typical
case learning curves for temporal difference learning of a value function with
linear function approximators. Our theory is derived under a Gaussian
equivalence hypothesis where averages over the random trajectories are replaced
with temporally correlated Gaussian feature averages and we validate our
assumptions on small scale Markov Decision Processes. We find that the
stochastic semi-gradient noise due to subsampling the space of possible
episodes leads to significant plateaus in the value error, unlike in
traditional gradient descent dynamics. We study how learning dynamics and
plateaus depend on feature structure, learning rate, discount factor, and
reward function. We then analyze how strategies like learning rate annealing
and reward shaping can favorably alter learning dynamics and plateaus. To
conclude, our work introduces new tools to open a new direction towards
developing a theory of learning dynamics in reinforcement learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bordelon_B/0/1/0/all/0/1&quot;&gt;Blake Bordelon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Masset_P/0/1/0/all/0/1&quot;&gt;Paul Masset&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kuo_H/0/1/0/all/0/1&quot;&gt;Henry Kuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pehlevan_C/0/1/0/all/0/1&quot;&gt;Cengiz Pehlevan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05857">
<title>FAIRO: Fairness-aware Adaptation in Sequential-Decision Making for Human-in-the-Loop Systems. (arXiv:2307.05857v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05857</link>
<description rdf:parseType="Literal">&lt;p&gt;Achieving fairness in sequential-decision making systems within
Human-in-the-Loop (HITL) environments is a critical concern, especially when
multiple humans with different behavior and expectations are affected by the
same adaptation decisions in the system. This human variability factor adds
more complexity since policies deemed fair at one point in time may become
discriminatory over time due to variations in human preferences resulting from
inter- and intra-human variability. This paper addresses the fairness problem
from an equity lens, considering human behavior variability, and the changes in
human preferences over time. We propose FAIRO, a novel algorithm for
fairness-aware sequential-decision making in HITL adaptation, which
incorporates these notions into the decision-making process. In particular,
FAIRO decomposes this complex fairness task into adaptive sub-tasks based on
individual human preferences through leveraging the Options reinforcement
learning framework. We design FAIRO to generalize to three types of HITL
application setups that have the shared adaptation decision problem.
Furthermore, we recognize that fairness-aware policies can sometimes conflict
with the application&apos;s utility. To address this challenge, we provide a
fairness-utility tradeoff in FAIRO, allowing system designers to balance the
objectives of fairness and utility based on specific application requirements.
Extensive evaluations of FAIRO on the three HITL applications demonstrate its
generalizability and effectiveness in promoting fairness while accounting for
human variability. On average, FAIRO can improve fairness compared with other
methods across all three applications by 35.36%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tianyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taherisadr_M/0/1/0/all/0/1&quot;&gt;Mojtaba Taherisadr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elmalaki_S/0/1/0/all/0/1&quot;&gt;Salma Elmalaki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07870">
<title>Large Language Models as Superpositions of Cultural Perspectives. (arXiv:2307.07870v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07870</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) are often misleadingly recognized as having a
personality or a set of values. We argue that an LLM can be seen as a
superposition of perspectives with different values and personality traits.
LLMs exhibit context-dependent values and personality traits that change based
on the induced perspective (as opposed to humans, who tend to have more
coherent values and personality traits across contexts). We introduce the
concept of perspective controllability, which refers to a model&apos;s affordance to
adopt various perspectives with differing values and personality traits. In our
experiments, we use questionnaires from psychology (PVQ, VSM, IPIP) to study
how exhibited values and personality traits change based on different
perspectives. Through qualitative experiments, we show that LLMs express
different values when those are (implicitly or explicitly) implied in the
prompt, and that LLMs express different values even when those are not
obviously implied (demonstrating their context-dependent nature). We then
conduct quantitative experiments to study the controllability of different
models (GPT-4, GPT-3.5, OpenAssistant, StableVicuna, StableLM), the
effectiveness of various methods for inducing perspectives, and the smoothness
of the models&apos; drivability. We conclude by examining the broader implications
of our work and outline a variety of associated scientific questions. The
project website is available at
https://sites.google.com/view/llm-superpositions .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kovac_G/0/1/0/all/0/1&quot;&gt;Grgur Kova&amp;#x10d;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sawayama_M/0/1/0/all/0/1&quot;&gt;Masataka Sawayama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Portelas_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;my Portelas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colas_C/0/1/0/all/0/1&quot;&gt;C&amp;#xe9;dric Colas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dominey_P/0/1/0/all/0/1&quot;&gt;Peter Ford Dominey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1&quot;&gt;Pierre-Yves Oudeyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08304">
<title>Efficient Computation of Counterfactual Bounds. (arXiv:2307.08304v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08304</link>
<description rdf:parseType="Literal">&lt;p&gt;We assume to be given structural equations over discrete variables inducing a
directed acyclic graph, namely, a structural causal model, together with data
about its internal nodes. The question we want to answer is how we can compute
bounds for partially identifiable counterfactual queries from such an input. We
start by giving a map from structural casual models to credal networks. This
allows us to compute exact counterfactual bounds via algorithms for credal nets
on a subclass of structural causal models. Exact computation is going to be
inefficient in general given that, as we show, causal inference is NP-hard even
on polytrees. We target then approximate bounds via a causal EM scheme. We
evaluate their accuracy by providing credible intervals on the quality of the
approximation; we show through a synthetic benchmark that the EM scheme
delivers accurate results in a fair number of runs. In the course of the
discussion, we also point out what seems to be a neglected limitation to the
trending idea that counterfactual bounds can be computed without knowledge of
the structural equations. We also present a real case study on palliative care
to show how our algorithms can readily be used for practical purposes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaffalon_M/0/1/0/all/0/1&quot;&gt;Marco Zaffalon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antonucci_A/0/1/0/all/0/1&quot;&gt;Alessandro Antonucci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cabanas_R/0/1/0/all/0/1&quot;&gt;Rafael Caba&amp;#xf1;as&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huber_D/0/1/0/all/0/1&quot;&gt;David Huber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azzimonti_D/0/1/0/all/0/1&quot;&gt;Dario Azzimonti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13214">
<title>FedMEKT: Distillation-based Embedding Knowledge Transfer for Multimodal Federated Learning. (arXiv:2307.13214v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13214</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) enables a decentralized machine learning paradigm for
multiple clients to collaboratively train a generalized global model without
sharing their private data. Most existing works simply propose typical FL
systems for single-modal data, thus limiting its potential on exploiting
valuable multimodal data for future personalized applications. Furthermore, the
majority of FL approaches still rely on the labeled data at the client side,
which is limited in real-world applications due to the inability of
self-annotation from users. In light of these limitations, we propose a novel
multimodal FL framework that employs a semi-supervised learning approach to
leverage the representations from different modalities. Bringing this concept
into a system, we develop a distillation-based multimodal embedding knowledge
transfer mechanism, namely FedMEKT, which allows the server and clients to
exchange the joint knowledge of their learning models extracted from a small
multimodal proxy dataset. Our FedMEKT iteratively updates the generalized
global encoders with the joint embedding knowledge from the participating
clients. Thereby, to address the modality discrepancy and labeled data
constraint in existing FL systems, our proposed FedMEKT comprises local
multimodal autoencoder learning, generalized multimodal autoencoder
construction, and generalized classifier learning. Through extensive
experiments on three multimodal human activity recognition datasets, we
demonstrate that FedMEKT achieves superior global encoder performance on linear
evaluation and guarantees user privacy for personal data and model parameters
while demanding less communication cost than other baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1&quot;&gt;Huy Q. Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1&quot;&gt;Minh N. H. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thwal_C/0/1/0/all/0/1&quot;&gt;Chu Myaet Thwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chaoning Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1&quot;&gt;Choong Seon Hong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13813">
<title>How to Scale Your EMA. (arXiv:2307.13813v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13813</link>
<description rdf:parseType="Literal">&lt;p&gt;Preserving training dynamics across batch sizes is an important tool for
practical machine learning as it enables the trade-off between batch size and
wall-clock time. This trade-off is typically enabled by a scaling rule, for
example, in stochastic gradient descent, one should scale the learning rate
linearly with the batch size. Another important machine learning tool is the
model EMA, a functional copy of a target model, whose parameters move towards
those of its target model according to an Exponential Moving Average (EMA) at a
rate parameterized by a momentum hyperparameter. This model EMA can improve the
robustness and generalization of supervised learning, stabilize
pseudo-labeling, and provide a learning signal for Self-Supervised Learning
(SSL). Prior works have not considered the optimization of the model EMA when
performing scaling, leading to different training dynamics across batch sizes
and lower model performance. In this work, we provide a scaling rule for
optimization in the presence of a model EMA and demonstrate the rule&apos;s validity
across a range of architectures, optimizers, and data modalities. We also show
the rule&apos;s validity where the model EMA contributes to the optimization of the
target model, enabling us to train EMA-based pseudo-labeling and SSL methods at
small and large batch sizes. For SSL, we enable training of BYOL up to batch
size 24,576 without sacrificing performance, a 6$\times$ wall-clock time
reduction under idealized hardware settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Busbridge_D/0/1/0/all/0/1&quot;&gt;Dan Busbridge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ramapuram_J/0/1/0/all/0/1&quot;&gt;Jason Ramapuram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ablin_P/0/1/0/all/0/1&quot;&gt;Pierre Ablin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Likhomanenko_T/0/1/0/all/0/1&quot;&gt;Tatiana Likhomanenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dhekane_E/0/1/0/all/0/1&quot;&gt;Eeshan Gunesh Dhekane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Suau_X/0/1/0/all/0/1&quot;&gt;Xavier Suau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Webb_R/0/1/0/all/0/1&quot;&gt;Russ Webb&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03427">
<title>TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage. (arXiv:2308.03427v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.03427</link>
<description rdf:parseType="Literal">&lt;p&gt;With recent advancements in natural language processing, Large Language
Models (LLMs) have emerged as powerful tools for various real-world
applications. Despite their prowess, the intrinsic generative abilities of LLMs
may prove insufficient for handling complex tasks which necessitate a
combination of task planning and the usage of external tools. In this paper, we
first propose a structured framework tailored for LLM-based AI Agents and
discuss the crucial capabilities necessary for tackling intricate problems.
Within this framework, we design two distinct types of agents (i.e., one-step
agent and sequential agent) to execute the inference process. Subsequently, we
instantiate the framework using various LLMs and evaluate their Task Planning
and Tool Usage (TPTU) abilities on typical tasks. By highlighting key findings
and challenges, our goal is to provide a helpful resource for researchers and
practitioners to leverage the power of LLMs in their AI applications. Our study
emphasizes the substantial potential of these models, while also identifying
areas that need more investigation and improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruan_J/0/1/0/all/0/1&quot;&gt;Jingqing Ruan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yihong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_T/0/1/0/all/0/1&quot;&gt;Tianpeng Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_G/0/1/0/all/0/1&quot;&gt;Guoqing Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1&quot;&gt;Shiwei Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1&quot;&gt;Hangyu Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Ziyue Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1&quot;&gt;Xingyu Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1&quot;&gt;Rui Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11849">
<title>A Mobile Data-Driven Hierarchical Deep Reinforcement Learning Approach for Real-time Demand-Responsive Railway Rescheduling and Station Overcrowding Mitigation. (arXiv:2308.11849v2 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11849</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-time railway rescheduling is an important technique to enable
operational recovery in response to unexpected and dynamic conditions in a
timely and flexible manner. Current research relies mostly on OD based data and
model-based methods for estimating train passenger demands. These approaches
primarily focus on averaged disruption patterns, often overlooking the
immediate uneven distribution of demand over time. In reality, passenger demand
deviates significantly from predictions, especially during a disaster.
Disastrous situations such as flood in Zhengzhou, China in 2022 has created not
only unprecedented effect on Zhengzhou railway station itself, which is a major
railway hub in China, but also other major hubs connected to Zhengzhou, e.g.,
Xi&apos;an, the closest hub west of Zhengzhou. In this study, we define a real-time
demand-responsive (RTDR) railway rescheduling problem focusing two specific
aspects, namely, volatility of the demand, and management of station
crowdedness. For the first time, we propose a data-driven approach using
real-time mobile data (MD) to deal with this RTDR problem. A hierarchical deep
reinforcement learning (HDRL) framework is designed to perform real-time
rescheduling in a demand-responsive manner. The use of MD has enabled the
modelling of passenger dynamics in response to train delays and station
crowdedness, and a real-time optimisation for rescheduling of train services in
view of the change in demand as a result of passengers&apos; behavioural response to
disruption. Results show that the agent can steadily satisfy over 62% of the
demand with only 61% of the original rolling stock, ensuring continuous
operations without overcrowding. Moreover, the agent exhibits adaptability when
transferred to a new environment with increased demand, highlighting its
effectiveness in addressing unforeseen disruptions in real-time settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_E/0/1/0/all/0/1&quot;&gt;Enze Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Judith Y.T. Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.14132">
<title>Detecting Language Model Attacks with Perplexity. (arXiv:2308.14132v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.14132</link>
<description rdf:parseType="Literal">&lt;p&gt;A novel hack involving Large Language Models (LLMs) has emerged, exploiting
adversarial suffixes to deceive models into generating perilous responses. Such
jailbreaks can trick LLMs into providing intricate instructions to a malicious
user for creating explosives, orchestrating a bank heist, or facilitating the
creation of offensive content. By evaluating the perplexity of queries with
adversarial suffixes using an open-source LLM (GPT-2), we found that they have
exceedingly high perplexity values. As we explored a broad range of regular
(non-adversarial) prompt varieties, we concluded that false positives are a
significant challenge for plain perplexity filtering. A Light-GBM trained on
perplexity and token length resolved the false positives and correctly detected
most adversarial attacks in the test set.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alon_G/0/1/0/all/0/1&quot;&gt;Gabriel Alon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamfonas_M/0/1/0/all/0/1&quot;&gt;Michael Kamfonas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.00317">
<title>A Text-based Approach For Link Prediction on Wikipedia Articles. (arXiv:2309.00317v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2309.00317</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper present our work in the DSAA 2023 Challenge about Link Prediction
for Wikipedia Articles. We use traditional machine learning models with POS
tags (part-of-speech tags) features extracted from text to train the
classification model for predicting whether two nodes has the link. Then, we
use these tags to test on various machine learning models. We obtained the
results by F1 score at 0.99999 and got 7th place in the competition. Our source
code is publicly available at this link:
https://github.com/Tam1032/DSAA2023-Challenge-Link-prediction-DS-UIT_SAT
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_A/0/1/0/all/0/1&quot;&gt;Anh Hoang Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tam Minh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luu_S/0/1/0/all/0/1&quot;&gt;Son T. Luu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.00543">
<title>Curating Naturally Adversarial Datasets for Learning-Enabled Medical Cyber-Physical Systems. (arXiv:2309.00543v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.00543</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models have shown promising predictive accuracy for time-series
healthcare applications. However, ensuring the robustness of these models is
vital for building trustworthy AI systems. Existing research predominantly
focuses on robustness to synthetic adversarial examples, crafted by adding
imperceptible perturbations to clean input data. However, these synthetic
adversarial examples do not accurately reflect the most challenging real-world
scenarios, especially in the context of healthcare data. Consequently,
robustness to synthetic adversarial examples may not necessarily translate to
robustness against naturally occurring adversarial examples, which is highly
desirable for trustworthy AI. We propose a method to curate datasets comprised
of natural adversarial examples to evaluate model robustness. The method relies
on probabilistic labels obtained from automated weakly-supervised labeling that
combines noisy and cheap-to-obtain labeling heuristics. Based on these labels,
our method adversarially orders the input data and uses this ordering to
construct a sequence of increasingly adversarial datasets. Our evaluation on
six medical case studies and three non-medical case studies demonstrates the
efficacy and statistical validity of our approach to generating naturally
adversarial datasets
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pugh_S/0/1/0/all/0/1&quot;&gt;Sydney Pugh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruchkin_I/0/1/0/all/0/1&quot;&gt;Ivan Ruchkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1&quot;&gt;Insup Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weimer_J/0/1/0/all/0/1&quot;&gt;James Weimer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02641">
<title>Deformation-Invariant Neural Network and Its Applications in Distorted Image Restoration and Analysis. (arXiv:2310.02641v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02641</link>
<description rdf:parseType="Literal">&lt;p&gt;Images degraded by geometric distortions pose a significant challenge to
imaging and computer vision tasks such as object recognition. Deep
learning-based imaging models usually fail to give accurate performance for
geometrically distorted images. In this paper, we propose the
deformation-invariant neural network (DINN), a framework to address the problem
of imaging tasks for geometrically distorted images. The DINN outputs
consistent latent features for images that are geometrically distorted but
represent the same underlying object or scene. The idea of DINN is to
incorporate a simple component, called the quasiconformal transformer network
(QCTN), into other existing deep networks for imaging tasks. The QCTN is a deep
neural network that outputs a quasiconformal map, which can be used to
transform a geometrically distorted image into an improved version that is
closer to the distribution of natural or good images. It first outputs a
Beltrami coefficient, which measures the quasiconformality of the output
deformation map. By controlling the Beltrami coefficient, the local geometric
distortion under the quasiconformal mapping can be controlled. The QCTN is
lightweight and simple, which can be readily integrated into other existing
deep neural networks to enhance their performance. Leveraging our framework, we
have developed an image classification network that achieves accurate
classification of distorted images. Our proposed framework has been applied to
restore geometrically distorted images by atmospheric turbulence and water
turbulence. DINN outperforms existing GAN-based restoration methods under these
scenarios, demonstrating the effectiveness of the proposed framework.
Additionally, we apply our proposed framework to the 1-1 verification of human
face images under atmospheric turbulence and achieve satisfactory performance,
further demonstrating the efficacy of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Han Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qiguang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lui_L/0/1/0/all/0/1&quot;&gt;Lok Ming Lui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05309">
<title>Optimizing Solution-Samplers for Combinatorial Problems: The Landscape of Policy-Gradient Methods. (arXiv:2310.05309v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05309</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Neural Networks and Reinforcement Learning methods have empirically
shown great promise in tackling challenging combinatorial problems. In those
methods a deep neural network is used as a solution generator which is then
trained by gradient-based methods (e.g., policy gradient) to successively
obtain better solution distributions. In this work we introduce a novel
theoretical framework for analyzing the effectiveness of such methods. We ask
whether there exist generative models that (i) are expressive enough to
generate approximately optimal solutions; (ii) have a tractable, i.e,
polynomial in the size of the input, number of parameters; (iii) their
optimization landscape is benign in the sense that it does not contain
sub-optimal stationary points. Our main contribution is a positive answer to
this question. Our result holds for a broad class of combinatorial problems
including Max- and Min-Cut, Max-$k$-CSP, Maximum-Weight-Bipartite-Matching, and
the Traveling Salesman Problem. As a byproduct of our analysis we introduce a
novel regularization process over vanilla gradient descent and provide
theoretical and experimental evidence that it helps address vanishing-gradient
issues and escape bad stationary points.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caramanis_C/0/1/0/all/0/1&quot;&gt;Constantine Caramanis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fotakis_D/0/1/0/all/0/1&quot;&gt;Dimitris Fotakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalavasis_A/0/1/0/all/0/1&quot;&gt;Alkis Kalavasis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kontonis_V/0/1/0/all/0/1&quot;&gt;Vasilis Kontonis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tzamos_C/0/1/0/all/0/1&quot;&gt;Christos Tzamos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10149">
<title>Recursive Segmentation Living Image: An eXplainable AI (XAI) Approach for Computing Structural Beauty of Images or the Livingness of Space. (arXiv:2310.10149v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10149</link>
<description rdf:parseType="Literal">&lt;p&gt;This study introduces the concept of &quot;structural beauty&quot; as an objective
computational approach for evaluating the aesthetic appeal of images. Through
the utilization of the Segment anything model (SAM), we propose a method that
leverages recursive segmentation to extract finer-grained substructures.
Additionally, by reconstructing the hierarchical structure, we obtain a more
accurate representation of substructure quantity and hierarchy. This approach
reproduces and extends our previous research, allowing for the simultaneous
assessment of Livingness in full-color images without the need for grayscale
conversion or separate computations for foreground and background Livingness.
Furthermore, the application of our method to the Scenic or Not dataset, a
repository of subjective scenic ratings, demonstrates a high degree of
consistency with subjective ratings in the 0-6 score range. This underscores
that structural beauty is not solely a subjective perception, but a
quantifiable attribute accessible through objective computation. Through our
case studies, we have arrived at three significant conclusions. 1) our method
demonstrates the capability to accurately segment meaningful objects, including
trees, buildings, and windows, as well as abstract substructures within
paintings. 2) we observed that the clarity of an image impacts our
computational results; clearer images tend to yield higher Livingness scores.
However, for equally blurry images, Livingness does not exhibit a significant
reduction, aligning with human visual perception. 3) our approach fundamentally
differs from methods employing Convolutional Neural Networks (CNNs) for
predicting image scores. Our method not only provides computational results but
also offers transparency and interpretability, positioning it as a novel avenue
in the realm of Explainable AI (XAI).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qianxiang_Y/0/1/0/all/0/1&quot;&gt;Yao Qianxiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1&quot;&gt;Bin Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.11807">
<title>Learning Global Quantum Properties from Local Measurements with Neural Networks. (arXiv:2310.11807v2 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2310.11807</link>
<description rdf:parseType="Literal">&lt;p&gt;Characterizing the properties of multiparticle quantum systems is a crucial
task for quantum computing and many-body quantum physics. The task, however,
becomes extremely challenging when the system size becomes large and when the
properties of interest involve global measurements on a large number of sites.
Here we develop a multi-task neural network model that can accurately predict
global properties of many-body quantum systems, like string order parameters
and many-body topological invariants, using only limited measurement data
gathered from few neighbouring sites. The model can simultaneously predict
multiple quantum properties, including not only expectation values of quantum
observables, but also general nonlinear functions of the quantum state, such as
entanglement entropies. Remarkably, we find that multi-task training over a
given set of quantum properties enables our model to discover new properties
outside the original set. Without any labeled data, the model can perform
unsupervised classification of quantum phases of matter and uncover unknown
boundaries between different phases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Ya-Dong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuexuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Chiribella_G/0/1/0/all/0/1&quot;&gt;Giulio Chiribella&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12425">
<title>Automated Repair of Declarative Software Specifications in the Era of Large Language Models. (arXiv:2310.12425v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2310.12425</link>
<description rdf:parseType="Literal">&lt;p&gt;The growing adoption of declarative software specification languages, coupled
with their inherent difficulty in debugging, has underscored the need for
effective and automated repair techniques applicable to such languages.
Researchers have recently explored various methods to automatically repair
declarative software specifications, such as template-based repair,
feedback-driven iterative repair, and bounded exhaustive approaches. The latest
developments in large language models provide new opportunities for the
automatic repair of declarative specifications. In this study, we assess the
effectiveness of utilizing OpenAI&apos;s ChatGPT to repair software specifications
written in the Alloy declarative language. Unlike imperative languages,
specifications in Alloy are not executed but rather translated into logical
formulas and evaluated using backend constraint solvers to identify
specification instances and counterexamples to assertions. Our evaluation
focuses on ChatGPT&apos;s ability to improve the correctness and completeness of
Alloy declarative specifications through automatic repairs. We analyze the
results produced by ChatGPT and compare them with those of leading automatic
Alloy repair methods. Our study revealed that while ChatGPT falls short in
comparison to existing techniques, it was able to successfully repair bugs that
no other technique could address. Our analysis also identified errors in
ChatGPT&apos;s generated repairs, including improper operator usage, type errors,
higher-order logic misuse, and relational arity mismatches. Additionally, we
observed instances of hallucinations in ChatGPT-generated repairs and
inconsistency in its results. Our study provides valuable insights for software
practitioners, researchers, and tool builders considering ChatGPT for
declarative specification repairs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1&quot;&gt;Md Rashedul Hasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiawei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_I/0/1/0/all/0/1&quot;&gt;Iftekhar Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagheri_H/0/1/0/all/0/1&quot;&gt;Hamid Bagheri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12567">
<title>Safety-Gymnasium: A Unified Safe Reinforcement Learning Benchmark. (arXiv:2310.12567v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.12567</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence (AI) systems possess significant potential to drive
societal progress. However, their deployment often faces obstacles due to
substantial safety concerns. Safe reinforcement learning (SafeRL) emerges as a
solution to optimize policies while simultaneously adhering to multiple
constraints, thereby addressing the challenge of integrating reinforcement
learning in safety-critical scenarios. In this paper, we present an environment
suite called Safety-Gymnasium, which encompasses safety-critical tasks in both
single and multi-agent scenarios, accepting vector and vision-only input.
Additionally, we offer a library of algorithms named Safe Policy Optimization
(SafePO), comprising 16 state-of-the-art SafeRL algorithms. This comprehensive
library can serve as a validation tool for the research community. By
introducing this benchmark, we aim to facilitate the evaluation and comparison
of safety performance, thus fostering the development of reinforcement learning
for safer, more reliable, and responsible real-world applications. The website
of this project can be accessed at
https://sites.google.com/view/safety-gymnasium.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1&quot;&gt;Jiaming Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Borong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jiayi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1&quot;&gt;Xuehai Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Weidong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1&quot;&gt;Ruiyang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1&quot;&gt;Yiran Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yifan Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1&quot;&gt;Juntao Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yaodong Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18725">
<title>The Evolution of the Interplay Between Input Distributions and Linear Regions in Networks. (arXiv:2310.18725v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18725</link>
<description rdf:parseType="Literal">&lt;p&gt;It is commonly recognized that the expressiveness of deep neural networks is
contingent upon a range of factors, encompassing their depth, width, and other
relevant considerations. Currently, the practical performance of the majority
of deep neural networks remains uncertain. For ReLU (Rectified Linear Unit)
networks with piecewise linear activations, the number of linear convex regions
serves as a natural metric to gauge the network&apos;s expressivity. In this paper,
we count the number of linear convex regions in deep neural networks based on
ReLU. In particular, we prove that for any one-dimensional input, there exists
a minimum threshold for the number of neurons required to express it. We also
empirically observe that for the same network, intricate inputs hinder its
capacity to express linear regions. Furthermore, we unveil the iterative
refinement process of decision boundaries in ReLU networks during training. We
aspire for our research to serve as an inspiration for network optimization
endeavors and aids in the exploration and analysis of the behaviors exhibited
by deep networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1&quot;&gt;Xuan Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yi Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19109">
<title>Dynamic Task and Weight Prioritization Curriculum Learning for Multimodal Imagery. (arXiv:2310.19109v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19109</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores post-disaster analytics using multimodal deep learning
models trained with curriculum learning method. Studying post-disaster
analytics is important as it plays a crucial role in mitigating the impact of
disasters by providing timely and accurate insights into the extent of damage
and the allocation of resources. We propose a curriculum learning strategy to
enhance the performance of multimodal deep learning models. Curriculum learning
emulates the progressive learning sequence in human education by training deep
learning models on increasingly complex data. Our primary objective is to
develop a curriculum-trained multimodal deep learning model, with a particular
focus on visual question answering (VQA) capable of jointly processing image
and text data, in conjunction with semantic segmentation for disaster analytics
using the
FloodNet\footnote{https://github.com/BinaLab/FloodNet-Challenge-EARTHVISION2021}
dataset. To achieve this, U-Net model is used for semantic segmentation and
image encoding. A custom built text classifier is used for visual question
answering. Existing curriculum learning methods rely on manually defined
difficulty functions. We introduce a novel curriculum learning approach termed
Dynamic Task and Weight Prioritization (DATWEP), which leverages a
gradient-based method to automatically decide task difficulty during curriculum
learning training, thereby eliminating the need for explicit difficulty
computation. The integration of DATWEP into our multimodal model shows
improvement on VQA performance. Source code is available at
https://github.com/fualsan/DATWEP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alsan_H/0/1/0/all/0/1&quot;&gt;Huseyin Fuat Alsan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arsan_T/0/1/0/all/0/1&quot;&gt;Taner Arsan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.20246">
<title>Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations. (arXiv:2310.20246v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.20246</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing research predominantly focuses on developing powerful language
learning models (LLMs) for mathematical reasoning within monolingual languages,
with few explorations in preserving efficacy in a multilingual context. To
bridge this gap, this paper pioneers exploring and training powerful
Multilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we
construct the first multilingual math reasoning instruction dataset,
MGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue
of training data scarcity in xMR tasks. Based on the collected dataset, we
propose different training strategies to build powerful xMR LLMs, named
MathOctopus, notably outperform conventional open-source LLMs and exhibit
superiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B
reaches 47.6% accuracy which exceeds ChatGPT 46.3% on MGSM testset. Beyond
remarkable results, we unearth several pivotal observations and insights from
extensive experiments: (1) When extending the rejection sampling strategy to
the multilingual context, it proves effective for model performances, albeit
limited. (2) Employing parallel corpora for math Supervised Fine-Tuning (SFT)
across multiple languages not only significantly enhances model performance
multilingually but also elevates their monolingual performance. This indicates
that crafting multilingual corpora can be regarded as a vital strategy for
enhancing model performance in a specific language, especially in mathematical
reasoning tasks. For instance, MathOctopus-7B improves its counterparts that
trained on English from 42.2% to 50.8% on GSM8K testset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1&quot;&gt;Nuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1&quot;&gt;Zinan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_N/0/1/0/all/0/1&quot;&gt;Ning Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1&quot;&gt;Ming Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yangqiu Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dongmei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jia Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.20327">
<title>Improving Entropy-Based Test-Time Adaptation from a Clustering View. (arXiv:2310.20327v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.20327</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain shift is a common problem in the realistic world, where training data
and test data follow different data distributions. To deal with this problem,
fully test-time adaptation (TTA) leverages the unlabeled data encountered
during test time to adapt the model. In particular, Entropy-Based TTA (EBTTA)
methods, which minimize the prediction&apos;s entropy on test samples, have shown
great success. In this paper, we introduce a new perspective on the EBTTA,
which interprets these methods from a view of clustering. It is an iterative
algorithm: 1) in the assignment step, the forward process of the EBTTA models
is the assignment of labels for these test samples, and 2) in the updating
step, the backward process is the update of the model via the assigned samples.
Based on the interpretation, we can gain a deeper understanding of EBTTA, where
we show that the entropy loss would further increase the largest probability.
Accordingly, we offer an alternative explanation for why existing EBTTA methods
are sensitive to initial assignments, outliers, and batch size. This
observation can guide us to put forward the improvement of EBTTA. We propose
robust label assignment, weight adjustment, and gradient accumulation to
alleviate the above problems. Experimental results demonstrate that our method
can achieve consistent improvements on various datasets. Code is provided in
the supplementary material.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1&quot;&gt;Guoliang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_H/0/1/0/all/0/1&quot;&gt;Hanjiang Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yan Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1&quot;&gt;Jian Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00729">
<title>ZEETAD: Adapting Pretrained Vision-Language Model for Zero-Shot End-to-End Temporal Action Detection. (arXiv:2311.00729v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00729</link>
<description rdf:parseType="Literal">&lt;p&gt;Temporal action detection (TAD) involves the localization and classification
of action instances within untrimmed videos. While standard TAD follows fully
supervised learning with closed-set setting on large training data, recent
zero-shot TAD methods showcase the promising open-set setting by leveraging
large-scale contrastive visual-language (ViL) pretrained models. However,
existing zero-shot TAD methods have limitations on how to properly construct
the strong relationship between two interdependent tasks of localization and
classification and adapt ViL model to video understanding. In this work, we
present ZEETAD, featuring two modules: dual-localization and zero-shot proposal
classification. The former is a Transformer-based module that detects action
events while selectively collecting crucial semantic embeddings for later
recognition. The latter one, CLIP-based module, generates semantic embeddings
from text and frame inputs for each temporal unit. Additionally, we enhance
discriminative capability on unseen classes by minimally updating the frozen
CLIP encoder with lightweight adapters. Extensive experiments on THUMOS14 and
ActivityNet-1.3 datasets demonstrate our approach&apos;s superior performance in
zero-shot TAD and effective knowledge transfer from ViL models to unseen action
categories.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phan_T/0/1/0/all/0/1&quot;&gt;Thinh Phan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vo_K/0/1/0/all/0/1&quot;&gt;Khoa Vo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1&quot;&gt;Duy Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doretto_G/0/1/0/all/0/1&quot;&gt;Gianfranco Doretto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adjeroh_D/0/1/0/all/0/1&quot;&gt;Donald Adjeroh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1&quot;&gt;Ngan Le&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00855">
<title>A Multi-Agent Reinforcement Learning Framework for Evaluating the U.S. Ending the HIV Epidemic Plan. (arXiv:2311.00855v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00855</link>
<description rdf:parseType="Literal">&lt;p&gt;Human immunodeficiency virus (HIV) is a major public health concern in the
United States, with about 1.2 million people living with HIV and 35,000 newly
infected each year. There are considerable geographical disparities in HIV
burden and care access across the U.S. The 2019 Ending the HIV Epidemic (EHE)
initiative aims to reduce new infections by 90% by 2030, by improving coverage
of diagnoses, treatment, and prevention interventions and prioritizing
jurisdictions with high HIV prevalence. Identifying optimal scale-up of
intervention combinations will help inform resource allocation. Existing HIV
decision analytic models either evaluate specific cities or the overall
national population, thus overlooking jurisdictional interactions or
differences. In this paper, we propose a multi-agent reinforcement learning
(MARL) model, that enables jurisdiction-specific decision analyses but in an
environment with cross-jurisdictional epidemiological interactions. In
experimental analyses, conducted on jurisdictions within California and
Florida, optimal policies from MARL were significantly different than those
generated from single-agent RL, highlighting the influence of jurisdictional
variations and interactions. By using comprehensive modeling of HIV and
formulations of state space, action space, and reward functions, this work
helps demonstrate the strengths and applicability of MARL for informing public
health policies, and provides a framework for expanding to the national-level
to inform the EHE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1&quot;&gt;Dinesh Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1&quot;&gt;Ankit Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gopalappa_C/0/1/0/all/0/1&quot;&gt;Chaitra Gopalappa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01305">
<title>AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models. (arXiv:2311.01305v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01305</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models(LLMs) exhibit excellent performance across a variety of
tasks, but they come with significant computational and storage costs.
Quantizing these models is an effective way to alleviate this issue. However,
existing methods struggle to strike a balance between model accuracy and
hardware efficiency. This is where we introduce AWEQ, a post-training method
that requires no additional training overhead. AWEQ excels in both
ultra-low-bit quantization and 8-bit weight and activation (W8A8) quantization.
There is an observation that weight quantization is less challenging than
activation quantization. AWEQ transfers the difficulty of activation
quantization to weights using channel equalization, achieving a balance between
the quantization difficulties of both, and thereby maximizing performance. We
have further refined the equalization method to mitigate quantization bias
error, ensuring the robustness of the model. Extensive experiments on popular
models such as LLaMA and OPT demonstrate that AWEQ outperforms all existing
post-training quantization methods for large models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Baisong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xingwang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Haixiao Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02117">
<title>Cooperative Network Learning for Large-Scale and Decentralized Graphs. (arXiv:2311.02117v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02117</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph research, the systematic study of interconnected data points
represented as graphs, plays a vital role in capturing intricate relationships
within networked systems. However, in the real world, as graphs scale up,
concerns about data security among different data-owning agencies arise,
hindering information sharing and, ultimately, the utilization of graph data.
Therefore, establishing a mutual trust mechanism among graph agencies is
crucial for unlocking the full potential of graphs. Here, we introduce a
Cooperative Network Learning (CNL) framework to ensure secure graph computing
for various graph tasks. Essentially, this CNL framework unifies the local and
global perspectives of GNN computing with distributed data for an agency by
virtually connecting all participating agencies as a global graph without a
fixed central coordinator. Inter-agency computing is protected by various
technologies inherent in our framework, including homomorphic encryption and
secure transmission. Moreover, each agency has a fair right to design or employ
various graph learning models from its local or global perspective. Thus, CNL
can collaboratively train GNN models based on decentralized graphs inferred
from local and global graphs. Experiments on contagion dynamics prediction and
traditional graph tasks (i.e., node classification and link prediction)
demonstrate that our CNL architecture outperforms state-of-the-art GNNs
developed at individual sites, revealing that CNL can provide a reliable, fair,
secure, privacy-preserving, and global perspective to build effective and
personalized models for network applications. We hope this framework will
address privacy concerns in graph-related research and integrate decentralized
graph data structures to benefit the network research community in cooperation
and innovation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qiang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yiming Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1&quot;&gt;Yujie Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teng_Y/0/1/0/all/0/1&quot;&gt;Yijie Teng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1&quot;&gt;Fang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1&quot;&gt;Linyuan L&amp;#xfc;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02171">
<title>Emergence of Abstract State Representations in Embodied Sequence Modeling. (arXiv:2311.02171v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02171</link>
<description rdf:parseType="Literal">&lt;p&gt;Decision making via sequence modeling aims to mimic the success of language
models, where actions taken by an embodied agent are modeled as tokens to
predict. Despite their promising performance, it remains unclear if embodied
sequence modeling leads to the emergence of internal representations that
represent the environmental state information. A model that lacks abstract
state representations would be liable to make decisions based on surface
statistics which fail to generalize. We take the BabyAI environment, a grid
world in which language-conditioned navigation tasks are performed, and build a
sequence modeling Transformer, which takes a language instruction, a sequence
of actions, and environmental observations as its inputs. In order to
investigate the emergence of abstract state representations, we design a
&quot;blindfolded&quot; navigation task, where only the initial environmental layout, the
language instruction, and the action sequence to complete the task are
available for training. Our probing results show that intermediate
environmental layouts can be reasonably reconstructed from the internal
activations of a trained model, and that language instructions play a role in
the reconstruction accuracy. Our results suggest that many key features of
state representations can emerge via embodied sequence modeling, supporting an
optimistic outlook for applications of sequence modeling objectives to more
complex embodied decision-making domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_T/0/1/0/all/0/1&quot;&gt;Tian Yun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1&quot;&gt;Zilai Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Handa_K/0/1/0/all/0/1&quot;&gt;Kunal Handa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thapliyal_A/0/1/0/all/0/1&quot;&gt;Ashish V. Thapliyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1&quot;&gt;Bo Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1&quot;&gt;Ellie Pavlick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1&quot;&gt;Chen Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02849">
<title>Co-training and Co-distillation for Quality Improvement and Compression of Language Models. (arXiv:2311.02849v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02849</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge Distillation (KD) compresses computationally expensive pre-trained
language models (PLMs) by transferring their knowledge to smaller models,
allowing their use in resource-constrained or real-time settings. However, most
smaller models fail to surpass the performance of the original larger model,
resulting in sacrificing performance to improve inference speed. To address
this issue, we propose Co-Training and Co-Distillation (CTCD), a novel
framework that improves performance and inference speed together by co-training
two models while mutually distilling knowledge. The CTCD framework successfully
achieves this based on two significant findings: 1) Distilling knowledge from
the smaller model to the larger model during co-training improves the
performance of the larger model. 2) The enhanced performance of the larger
model further boosts the performance of the smaller model. The CTCD framework
shows promise as it can be combined with existing techniques like architecture
design or data augmentation, replacing one-way KD methods, to achieve further
performance improvement. Extensive ablation studies demonstrate the
effectiveness of CTCD, and the small model distilled by CTCD outperforms the
original larger model by a significant margin of 1.66 on the GLUE benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hayeon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_R/0/1/0/all/0/1&quot;&gt;Rui Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jongpil Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1&quot;&gt;Davis Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongbo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1&quot;&gt;Sung Ju Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_A/0/1/0/all/0/1&quot;&gt;Alexander Min&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03076">
<title>SugarViT -- Multi-objective Regression of UAV Images with Vision Transformers and Deep Label Distribution Learning Demonstrated on Disease Severity Prediction in Sugar Beet. (arXiv:2311.03076v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03076</link>
<description rdf:parseType="Literal">&lt;p&gt;Remote sensing and artificial intelligence are pivotal technologies of
precision agriculture nowadays. The efficient retrieval of large-scale field
imagery combined with machine learning techniques shows success in various
tasks like phenotyping, weeding, cropping, and disease control. This work will
introduce a machine learning framework for automatized large-scale
plant-specific trait annotation for the use case disease severity scoring for
Cercospora Leaf Spot (CLS) in sugar beet. With concepts of Deep Label
Distribution Learning (DLDL), special loss functions, and a tailored model
architecture, we develop an efficient Vision Transformer based model for
disease severity scoring called SugarViT. One novelty in this work is the
combination of remote sensing data with environmental parameters of the
experimental sites for disease severity prediction. Although the model is
evaluated on this special use case, it is held as generic as possible to also
be applicable to various image-based classification and regression tasks. With
our framework, it is even possible to learn models on multi-objective problems
as we show by a pretraining on environmental metadata.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunder_M/0/1/0/all/0/1&quot;&gt;Maurice G&amp;#xfc;nder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamati_F/0/1/0/all/0/1&quot;&gt;Facundo Ram&amp;#xf3;n Ispizua Yamati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alcantara_A/0/1/0/all/0/1&quot;&gt;Abel Andree Barreto Alc&amp;#xe1;ntara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahlein_A/0/1/0/all/0/1&quot;&gt;Anne-Katrin Mahlein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sifa_R/0/1/0/all/0/1&quot;&gt;Rafet Sifa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bauckhage_C/0/1/0/all/0/1&quot;&gt;Christian Bauckhage&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03285">
<title>S-LoRA: Serving Thousands of Concurrent LoRA Adapters. (arXiv:2311.03285v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03285</link>
<description rdf:parseType="Literal">&lt;p&gt;The &quot;pretrain-then-finetune&quot; paradigm is commonly adopted in the deployment
of large language models. Low-Rank Adaptation (LoRA), a parameter-efficient
fine-tuning method, is often employed to adapt a base model to a multitude of
tasks, resulting in a substantial collection of LoRA adapters derived from one
base model. We observe that this paradigm presents significant opportunities
for batched inference during serving. To capitalize on these opportunities, we
present S-LoRA, a system designed for the scalable serving of many LoRA
adapters. S-LoRA stores all adapters in the main memory and fetches the
adapters used by the currently running queries to the GPU memory. To
efficiently use the GPU memory and reduce fragmentation, S-LoRA proposes
Unified Paging. Unified Paging uses a unified memory pool to manage dynamic
adapter weights with different ranks and KV cache tensors with varying sequence
lengths. Additionally, S-LoRA employs a novel tensor parallelism strategy and
highly optimized custom CUDA kernels for heterogeneous batching of LoRA
computation. Collectively, these features enable S-LoRA to serve thousands of
LoRA adapters on a single GPU or across multiple GPUs with a small overhead.
Compared to state-of-the-art libraries such as HuggingFace PEFT and vLLM (with
naive support of LoRA serving), S-LoRA can improve the throughput by up to 4
times and increase the number of served adapters by several orders of
magnitude. As a result, S-LoRA enables scalable serving of many task-specific
fine-tuned models and offers the potential for large-scale customized
fine-tuning services. The code is available at https://github.com/S-LoRA/S-LoRA
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_Y/0/1/0/all/0/1&quot;&gt;Ying Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1&quot;&gt;Shiyi Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dacheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hooper_C/0/1/0/all/0/1&quot;&gt;Coleman Hooper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1&quot;&gt;Nicholas Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shuo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chou_C/0/1/0/all/0/1&quot;&gt;Christopher Chou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1&quot;&gt;Banghua Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1&quot;&gt;Lianmin Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1&quot;&gt;Kurt Keutzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1&quot;&gt;Joseph E. Gonzalez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1&quot;&gt;Ion Stoica&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>