<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-11-05T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01463" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01472" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01473" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01483" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01490" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01530" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01534" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01540" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01550" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01571" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01573" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01580" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01588" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01602" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01609" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01617" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01623" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01634" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01635" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01650" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01661" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01676" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01677" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01681" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01689" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01713" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01723" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01727" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01743" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01753" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01757" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01770" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01786" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01792" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01811" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01842" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01860" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01864" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01866" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01870" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01875" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01918" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01927" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01937" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01939" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01956" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01961" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01964" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01967" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01977" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01994" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02013" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02017" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02018" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02026" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02041" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02049" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2106.12915" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2111.08452" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.08369" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.09698" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.00282" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.09090" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.04873" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.05062" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.12247" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.07104" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.12040" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.03843" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.04370" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.11015" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.03515" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10050" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13484" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13669" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16397" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16671" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18381" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19234" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00265" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.02231" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10763" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15136" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02345" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08873" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08964" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14283" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01923" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02618" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.15457" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.01865" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.02553" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.04579" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03940" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05058" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05280" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05619" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.09219" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18144" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19180" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.20033" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.20381" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.20414" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.20550" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00687" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01057" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01240" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17688" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2311.01463">
<title>Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI. (arXiv:2311.01463v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.01463</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models have proliferated across multiple domains in as short
period of time. There is however hesitation in the medical and healthcare
domain towards their adoption because of issues like factuality, coherence, and
hallucinations. Give the high stakes nature of healthcare, many researchers
have even cautioned against its usage until these issues are resolved. The key
to the implementation and deployment of LLMs in healthcare is to make these
models trustworthy, transparent (as much possible) and explainable. In this
paper we describe the key elements in creating reliable, trustworthy, and
unbiased models as a necessary condition for their adoption in healthcare.
Specifically we focus on the quantification, validation, and mitigation of
hallucinations in the context in healthcare. Lastly, we discuss how the future
of LLMs in healthcare may look like.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmad_M/0/1/0/all/0/1&quot;&gt;Muhammad Aurangzeb Ahmad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yaramis_I/0/1/0/all/0/1&quot;&gt;Ilker Yaramis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_T/0/1/0/all/0/1&quot;&gt;Taposh Dutta Roy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01472">
<title>Relation Extraction from News Articles (RENA): A Tool for Epidemic Surveillance. (arXiv:2311.01472v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.01472</link>
<description rdf:parseType="Literal">&lt;p&gt;Relation Extraction from News Articles (RENA) is a browser-based tool
designed to extract key entities and their semantic relationships in English
language news articles related to infectious diseases. Constructed using the
React framework, this system presents users with an elegant and user-friendly
interface. It enables users to input a news article and select from a choice of
two models to generate a comprehensive list of relations within the provided
text. As a result, RENA allows real-time parsing of news articles to extract
key information for epidemic surveillance, contributing to EPIWATCH, an
open-source intelligence-based epidemic warning system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1&quot;&gt;Jaeff Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dung_D/0/1/0/all/0/1&quot;&gt;Duong Dung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hutchinson_D/0/1/0/all/0/1&quot;&gt;Danielle Hutchinson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akhtar_Z/0/1/0/all/0/1&quot;&gt;Zubair Akhtar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Rosalie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dawson_R/0/1/0/all/0/1&quot;&gt;Rebecca Dawson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1&quot;&gt;Aditya Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1&quot;&gt;Samsung Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+MacIntyre_C/0/1/0/all/0/1&quot;&gt;C Raina MacIntyre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurdasani_D/0/1/0/all/0/1&quot;&gt;Deepti Gurdasani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01473">
<title>Adversarial Examples in the Physical World: A Survey. (arXiv:2311.01473v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01473</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) have demonstrated high vulnerability to
adversarial examples. Besides the attacks in the digital world, the practical
implications of adversarial examples in the physical world present significant
challenges and safety concerns. However, current research on physical
adversarial examples (PAEs) lacks a comprehensive understanding of their unique
characteristics, leading to limited significance and understanding. In this
paper, we address this gap by thoroughly examining the characteristics of PAEs
within a practical workflow encompassing training, manufacturing, and
re-sampling processes. By analyzing the links between physical adversarial
attacks, we identify manufacturing and re-sampling as the primary sources of
distinct attributes and particularities in PAEs. Leveraging this knowledge, we
develop a comprehensive analysis and classification framework for PAEs based on
their specific characteristics, covering over 100 studies on physical-world
adversarial examples. Furthermore, we investigate defense strategies against
PAEs and identify open challenges and opportunities for future research. We aim
to provide a fresh, thorough, and systematic understanding of PAEs, thereby
promoting the development of robust adversarial learning and its application in
open-world scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiakai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Donghua Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jin Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Siyang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1&quot;&gt;Tingsong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1&quot;&gt;Wen Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;Aishan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xianglong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01483">
<title>FedSN: A General Federated Learning Framework over LEO Satellite Networks. (arXiv:2311.01483v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.01483</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, a large number of Low Earth Orbit (LEO) satellites have been
launched and deployed successfully in space by commercial companies, such as
SpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve
not only for communication but also for various machine learning applications,
such as space modulation recognition, remote sensing image classification, etc.
However, the ground station (GS) may be incapable of downloading such a large
volume of raw sensing data for centralized model training due to the limited
contact time with LEO satellites (e.g. 5 minutes). Therefore, federated
learning (FL) has emerged as the promising solution to address this problem via
on-device training. Unfortunately, to enable FL on LEO satellites, we still
face three critical challenges that are i) heterogeneous computing and memory
capabilities, ii) limited uplink rate, and iii) model staleness. To this end,
we propose FedSN as a general FL framework to tackle the above challenges, and
fully explore data diversity on LEO satellites. Specifically, we first present
a novel sub-structure scheme to enable heterogeneous local model training
considering different computing, memory, and communication constraints on LEO
satellites. Additionally, we propose a pseudo-synchronous model aggregation
strategy to dynamically schedule model aggregation for compensating model
staleness. To further demonstrate the effectiveness of the FedSN, we evaluate
it using space modulation recognition and remote sensing image classification
tasks by leveraging the data from real-world satellite networks. Extensive
experimental results demonstrate that FedSN framework achieves higher accuracy,
lower computing, and communication overhead than the state-of-the-art
benchmarks and the effectiveness of each components in FedSN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zheng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhe Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1&quot;&gt;Zihan Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xianhao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yue Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01490">
<title>The Behavior of Large Language Models When Prompted to Generate Code Explanations. (arXiv:2311.01490v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2311.01490</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper systematically explores how Large Language Models (LLMs) generate
explanations of code examples of the type used in intro-to-programming courses.
As we show, the nature of code explanations generated by LLMs varies
considerably based on the wording of the prompt, the target code examples being
explained, the programming language, the temperature parameter, and the version
of the LLM. Nevertheless, they are consistent in two major respects for Java
and Python: the readability level, which hovers around 7-8 grade, and lexical
density, i.e., the relative size of the meaningful words with respect to the
total explanation size. Furthermore, the explanations score very high in
correctness but less on three other metrics: completeness, conciseness, and
contextualization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oli_P/0/1/0/all/0/1&quot;&gt;Priti Oli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banjade_R/0/1/0/all/0/1&quot;&gt;Rabin Banjade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chapagain_J/0/1/0/all/0/1&quot;&gt;Jeevan Chapagain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rus_V/0/1/0/all/0/1&quot;&gt;Vasile Rus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01530">
<title>NOD-TAMP: Multi-Step Manipulation Planning with Neural Object Descriptors. (arXiv:2311.01530v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.01530</link>
<description rdf:parseType="Literal">&lt;p&gt;Developing intelligent robots for complex manipulation tasks in household and
factory settings remains challenging due to long-horizon tasks, contact-rich
manipulation, and the need to generalize across a wide variety of object shapes
and scene layouts. While Task and Motion Planning (TAMP) offers a promising
solution, its assumptions such as kinodynamic models limit applicability in
novel contexts. Neural object descriptors (NODs) have shown promise in object
and scene generalization but face limitations in addressing broader tasks. Our
proposed TAMP-based framework, NOD-TAMP, extracts short manipulation
trajectories from a handful of human demonstrations, adapts these trajectories
using NOD features, and composes them to solve broad long-horizon tasks.
Validated in a simulation environment, NOD-TAMP effectively tackles varied
challenges and outperforms existing methods, establishing a cohesive framework
for manipulation planning. For videos and other supplemental material, see the
project website: https://sites.google.com/view/nod-tamp/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;Shuo Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garrett_C/0/1/0/all/0/1&quot;&gt;Caelan Garrett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mandlekar_A/0/1/0/all/0/1&quot;&gt;Ajay Mandlekar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Danfei Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01534">
<title>Approximate Multiagent Reinforcement Learning for On-Demand Urban Mobility Problem on a Large Map (extended version). (arXiv:2311.01534v1 [cs.MA])</title>
<link>http://arxiv.org/abs/2311.01534</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we focus on the autonomous multiagent taxi routing problem for
a large urban environment where the location and number of future ride requests
are unknown a-priori, but follow an estimated empirical distribution. Recent
theory has shown that if a base policy is stable then a rollout-based algorithm
with such a base policy produces a near-optimal stable policy. Although,
rollout-based approaches are well-suited for learning cooperative multiagent
policies with considerations for future demand, applying such methods to a
large urban environment can be computationally expensive. Large environments
tend to have a large volume of requests, and hence require a large fleet of
taxis to guarantee stability. In this paper, we aim to address the
computational bottleneck of multiagent (one-at-a-time) rollout, where the
computational complexity grows linearly in the number of agents. We propose an
approximate one-at-a-time rollout-based two-phase algorithm that reduces the
computational cost, while still achieving a stable near-optimal policy. Our
approach partitions the graph into sectors based on the predicted demand and an
user-defined maximum number of agents that can be planned for using the
one-at-a-time rollout approach. The algorithm then applies instantaneous
assignment (IA) for re-balancing taxis across sectors and a sector-wide
one-at-a-time rollout algorithm that is executed in parallel for each sector.
We characterize the number of taxis $m$ that is sufficient for IA base policy
to be stable, and derive a necessary condition on $m$ as time goes to infinity.
Our numerical results show that our approach achieves stability for an $m$ that
satisfies the theoretical conditions. We also empirically demonstrate that our
proposed two-phase algorithm has comparable performance to the one-at-a-time
rollout over the entire map, but with significantly lower runtimes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garces_D/0/1/0/all/0/1&quot;&gt;Daniel Garces&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1&quot;&gt;Sushmita Bhattacharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertsekas_D/0/1/0/all/0/1&quot;&gt;Dimitri Bertsekas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gil_S/0/1/0/all/0/1&quot;&gt;Stephanie Gil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01540">
<title>Open-Set Object Recognition Using Mechanical Properties During Interaction. (arXiv:2311.01540v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.01540</link>
<description rdf:parseType="Literal">&lt;p&gt;while most of the tactile robots are operated in close-set conditions, it is
challenging for them to operate in open-set conditions where test objects are
beyond the robots&apos; knowledge. We proposed an open-set recognition framework
using mechanical properties to recongise known objects and incrementally label
novel objects. The main contribution is a clustering algorithm that exploits
knowledge of known objects to estimate cluster centre and sizes, unlike a
typical algorithm that randomly selects them. The framework is validated with
the mechanical properties estimated from a real object during interaction. The
results show that the framework could recognise objects better than alternative
methods contributed by the novelty detector. Importantly, our clustering
algorithm yields better clustering performance than other methods. Furthermore,
the hyperparameters studies show that cluster size is important to clustering
results and needed to be tuned properly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uttayopas_P/0/1/0/all/0/1&quot;&gt;Pakorn Uttayopas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burdet_E/0/1/0/all/0/1&quot;&gt;Etienne Burdet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01550">
<title>Market Concentration Implications of Foundation Models. (arXiv:2311.01550v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.01550</link>
<description rdf:parseType="Literal">&lt;p&gt;We analyze the structure of the market for foundation models, i.e., large AI
models such as those that power ChatGPT and that are adaptable to downstream
uses, and we examine the implications for competition policy and regulation. We
observe that the most capable models will have a tendency towards natural
monopoly and may have potentially vast markets. This calls for a two-pronged
regulatory response: (i) Antitrust authorities need to ensure the
contestability of the market by tackling strategic behavior, in particular by
ensuring that monopolies do not propagate vertically to downstream uses, and
(ii) given the diminished potential for market discipline, there is a role for
regulators to ensure that the most capable models meet sufficient quality
standards (including safety, privacy, non-discrimination, reliability and
interoperability standards) to maximally contribute to social welfare.
Regulators should also ensure a level regulatory playing field between AI and
non-AI applications in all sectors of the economy. For models that are behind
the frontier, we expect competition to be quite intense, implying a more
limited role for competition policy, although a role for regulation remains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vipra_J/0/1/0/all/0/1&quot;&gt;Jai Vipra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korinek_A/0/1/0/all/0/1&quot;&gt;Anton Korinek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01571">
<title>Preserving the knowledge of long clinical texts using aggregated ensembles of large language models. (arXiv:2311.01571v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.01571</link>
<description rdf:parseType="Literal">&lt;p&gt;Clinical texts, such as admission notes, discharge summaries, and progress
notes, contain rich and valuable information that can be used for various
clinical outcome prediction tasks. However, applying large language models,
such as BERT-based models, to clinical texts poses two major challenges: the
limitation of input length and the diversity of data sources. This paper
proposes a novel method to preserve the knowledge of long clinical texts using
aggregated ensembles of large language models. Unlike previous studies which
use model ensembling or text aggregation methods separately, we combine
ensemble learning with text aggregation and train multiple large language
models on two clinical outcome tasks: mortality prediction and length of stay
prediction. We show that our method can achieve better results than baselines,
ensembling, and aggregation individually, and can improve the performance of
large language models while handling long inputs and diverse datasets. We
conduct extensive experiments on the admission notes from the MIMIC-III
clinical database by combining multiple unstructured and high-dimensional
datasets, demonstrating our method&apos;s effectiveness and superiority over
existing approaches. We also provide a comprehensive analysis and discussion of
our results, highlighting our method&apos;s applications and limitations for future
research in the domain of clinical healthcare. The results and analysis of this
study is supportive of our method assisting in clinical healthcare systems by
enabling clinical decision-making with robust performance overcoming the
challenges of long text inputs and varied datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1&quot;&gt;Mohammad Junayed Hasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noor_S/0/1/0/all/0/1&quot;&gt;Suhra Noor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Mohammad Ashrafuzzaman Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01573">
<title>Improving Fairness using Vision-Language Driven Image Augmentation. (arXiv:2311.01573v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01573</link>
<description rdf:parseType="Literal">&lt;p&gt;Fairness is crucial when training a deep-learning discriminative model,
especially in the facial domain. Models tend to correlate specific
characteristics (such as age and skin color) with unrelated attributes
(downstream tasks), resulting in biases which do not correspond to reality. It
is common knowledge that these correlations are present in the data and are
then transferred to the models during training. This paper proposes a method to
mitigate these correlations to improve fairness. To do so, we learn
interpretable and meaningful paths lying in the semantic space of a pre-trained
diffusion model (DiffAE) -- such paths being supervised by contrastive text
dipoles. That is, we learn to edit protected characteristics (age and skin
color). These paths are then applied to augment images to improve the fairness
of a given dataset. We test the proposed method on CelebA-HQ and UTKFace on
several downstream tasks with age and skin color as protected characteristics.
As a proxy for fairness, we compute the difference in accuracy with respect to
the protected characteristics. Quantitative results show how the augmented
images help the model improve the overall accuracy, the aforementioned metric,
and the disparity of equal opportunity. Code is available at:
https://github.com/Moreno98/Vision-Language-Bias-Control.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DInca_M/0/1/0/all/0/1&quot;&gt;Moreno D&amp;#x27;Inc&amp;#xe0;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tzelepis_C/0/1/0/all/0/1&quot;&gt;Christos Tzelepis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patras_I/0/1/0/all/0/1&quot;&gt;Ioannis Patras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1&quot;&gt;Nicu Sebe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01580">
<title>MetaReVision: Meta-Learning with Retrieval for Visually Grounded Compositional Concept Acquisition. (arXiv:2311.01580v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.01580</link>
<description rdf:parseType="Literal">&lt;p&gt;Humans have the ability to learn novel compositional concepts by recalling
and generalizing primitive concepts acquired from past experiences. Inspired by
this observation, in this paper, we propose MetaReVision, a retrieval-enhanced
meta-learning model to address the visually grounded compositional concept
learning problem. The proposed MetaReVision consists of a retrieval module and
a meta-learning module which are designed to incorporate retrieved primitive
concepts as a supporting set to meta-train vision-anguage models for grounded
compositional concept recognition. Through meta-learning from episodes
constructed by the retriever, MetaReVision learns a generic compositional
representation that can be fast updated to recognize novel compositional
concepts. We create CompCOCO and CompFlickr to benchmark the grounded
compositional concept learning. Our experimental results show that MetaReVision
outperforms other competitive baselines and the retrieval module plays an
important role in this compositional learning process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1&quot;&gt;Guangyue Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kordjamshidi_P/0/1/0/all/0/1&quot;&gt;Parisa Kordjamshidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1&quot;&gt;Joyce Chai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01588">
<title>Domain Adaptive Graph Neural Networks for Constraining Cosmological Parameters Across Multiple Data Sets. (arXiv:2311.01588v1 [astro-ph.CO])</title>
<link>http://arxiv.org/abs/2311.01588</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models have been shown to outperform methods that rely on
summary statistics, like the power spectrum, in extracting information from
complex cosmological data sets. However, due to differences in the subgrid
physics implementation and numerical approximations across different simulation
suites, models trained on data from one cosmological simulation show a drop in
performance when tested on another. Similarly, models trained on any of the
simulations would also likely experience a drop in performance when applied to
observational data. Training on data from two different suites of the CAMELS
hydrodynamic cosmological simulations, we examine the generalization
capabilities of Domain Adaptive Graph Neural Networks (DA-GNNs). By utilizing
GNNs, we capitalize on their capacity to capture structured scale-free
cosmological information from galaxy distributions. Moreover, by including
unsupervised domain adaptation via Maximum Mean Discrepancy (MMD), we enable
our models to extract domain-invariant features. We demonstrate that DA-GNN
achieves higher accuracy and robustness on cross-dataset tasks (up to $28\%$
better relative error and up to almost an order of magnitude better $\chi^2$).
Using data visualizations, we show the effects of domain adaptation on proper
latent space data alignment. This shows that DA-GNNs are a promising method for
extracting domain-independent cosmological information, a vital step toward
robust deep learning for real cosmic survey data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Roncoli_A/0/1/0/all/0/1&quot;&gt;Andrea Roncoli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Ciprijanovic_A/0/1/0/all/0/1&quot;&gt;Aleksandra &amp;#x106;iprijanovi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Voetberg_M/0/1/0/all/0/1&quot;&gt;Maggie Voetberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Villaescusa_Navarro_F/0/1/0/all/0/1&quot;&gt;Francisco Villaescusa-Navarro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Nord_B/0/1/0/all/0/1&quot;&gt;Brian Nord&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01602">
<title>DRNet: A Decision-Making Method for Autonomous Lane Changingwith Deep Reinforcement Learning. (arXiv:2311.01602v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.01602</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning techniques have outperformed numerous rule-based methods for
decision-making in autonomous vehicles. Despite recent efforts, lane changing
remains a major challenge, due to the complex driving scenarios and changeable
social behaviors of surrounding vehicles. To help improve the state of the art,
we propose to leveraging the emerging \underline{D}eep
\underline{R}einforcement learning (DRL) approach for la\underline{NE} changing
at the \underline{T}actical level. To this end, we present &quot;DRNet&quot;, a novel and
highly efficient DRL-based framework that enables a DRL agent to learn to drive
by executing reasonable lane changing on simulated highways with an arbitrary
number of lanes, and considering driving style of surrounding vehicles to make
better decisions. Furthermore, to achieve a safe policy for decision-making,
DRNet incorporates ideas from safety verification, the most important component
of autonomous driving, to ensure that only safe actions are chosen at any time.
The setting of our state representation and reward function enables the trained
agent to take appropriate actions in a real-world-like simulator. Our DRL agent
has the ability to learn the desired task without causing collisions and
outperforms DDQN and other baseline models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kunpeng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lifei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shengrui Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01609">
<title>Responsible Emergent Multi-Agent Behavior. (arXiv:2311.01609v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.01609</link>
<description rdf:parseType="Literal">&lt;p&gt;Responsible AI has risen to the forefront of the AI research community. As
neural network-based learning algorithms continue to permeate real-world
applications, the field of Responsible AI has played a large role in ensuring
that such systems maintain a high-level of human-compatibility. Despite this
progress, the state of the art in Responsible AI has ignored one crucial point:
human problems are multi-agent problems. Predominant approaches largely
consider the performance of a single AI system in isolation, but human problems
are, by their very nature, multi-agent. From driving in traffic to negotiating
economic policy, human problem-solving involves interaction and the interplay
of the actions and motives of multiple individuals.
&lt;/p&gt;
&lt;p&gt;This dissertation develops the study of responsible emergent multi-agent
behavior, illustrating how researchers and practitioners can better understand
and shape multi-agent learning with respect to three pillars of Responsible AI:
interpretability, fairness, and robustness. First, I investigate multi-agent
interpretability, presenting novel techniques for understanding emergent
multi-agent behavior at multiple levels of granularity. With respect to
low-level interpretability, I examine the extent to which implicit
communication emerges as an aid to coordination in multi-agent populations. I
introduce a novel curriculum-driven method for learning high-performing
policies in difficult, sparse reward environments and show through a measure of
position-based social influence that multi-agent teams that learn sophisticated
coordination strategies exchange significantly more information through
implicit signals than lesser-coordinated agents. Then, at a high-level, I study
concept-based interpretability in the context of multi-agent learning. I
propose a novel method for learning intrinsically interpretable, concept-based
policies and show that it enables...
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grupen_N/0/1/0/all/0/1&quot;&gt;Niko A. Grupen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01617">
<title>Look-Ahead Selective Plasticity for Continual Learning of Visual Tasks. (arXiv:2311.01617v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01617</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive representation learning has emerged as a promising technique for
continual learning as it can learn representations that are robust to
catastrophic forgetting and generalize well to unseen future tasks. Previous
work in continual learning has addressed forgetting by using previous task data
and trained models. Inspired by event models created and updated in the brain,
we propose a new mechanism that takes place during task boundaries, i.e., when
one task finishes and another starts. By observing the redundancy-inducing
ability of contrastive loss on the output of a neural network, our method
leverages the first few samples of the new task to identify and retain
parameters contributing most to the transfer ability of the neural network,
freeing up the remaining parts of the network to learn new features. We
evaluate the proposed methods on benchmark computer vision datasets including
CIFAR10 and TinyImagenet and demonstrate state-of-the-art performance in the
task-incremental, class-incremental, and domain-incremental continual learning
scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meshkinnejad_R/0/1/0/all/0/1&quot;&gt;Rouzbeh Meshkinnejad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1&quot;&gt;Jie Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lizotte_D/0/1/0/all/0/1&quot;&gt;Daniel Lizotte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohsenzadeh_Y/0/1/0/all/0/1&quot;&gt;Yalda Mohsenzadeh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01623">
<title>VQPy: An Object-Oriented Approach to Modern Video Analytics. (arXiv:2311.01623v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01623</link>
<description rdf:parseType="Literal">&lt;p&gt;Video analytics is widely used in contemporary systems and services. At the
forefront of video analytics are video queries that users develop to find
objects of particular interest. Building upon the insight that video objects
(e.g., human, animals, cars, etc.), the center of video analytics, are similar
in spirit to objects modeled by traditional object-oriented languages, we
propose to develop an object-oriented approach to video analytics. This
approach, named VQPy, consists of a frontend$\unicode{x2015}$a Python variant
with constructs that make it easy for users to express video objects and their
interactions$\unicode{x2015}$as well as an extensible backend that can
automatically construct and optimize pipelines based on video objects. We have
implemented and open-sourced VQPy, which has been productized in Cisco as part
of its DeepVision framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Shan Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhenting Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hanchen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1&quot;&gt;Pengzhan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Padmanabhan_A/0/1/0/all/0/1&quot;&gt;Arthi Padmanabhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Latapie_H/0/1/0/all/0/1&quot;&gt;Hugo Latapie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Harry Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01634">
<title>&quot;Close...but not as good as an educator.&quot; -- Using ChatGPT to provide formative feedback in large-class collaborative learning. (arXiv:2311.01634v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2311.01634</link>
<description rdf:parseType="Literal">&lt;p&gt;Delivering personalised, formative feedback to multiple problem-based
learning groups in a short time period can be almost impossible. We employed
ChatGPT to provide personalised formative feedback in a one-hour Zoom break-out
room activity that taught practicing health professionals how to formulate
evaluation plans for digital health initiatives. Learners completed an
evaluation survey that included Likert scales and open-ended questions that
were analysed. Half of the 44 survey respondents had never used ChatGPT before.
Overall, respondents found the feedback favourable, described a wide range of
group dynamics, and had adaptive responses to the feedback, yet only three
groups used the feedback loop to improve their evaluation plans. Future
educators can learn from our experience including engineering prompts,
providing instructions on how to use ChatGPT, and scaffolding optimal group
interactions with ChatGPT. Future researchers should explore the influence of
ChatGPT on group dynamics and derive design principles for the use of ChatGPT
in collaborative learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ponte_C/0/1/0/all/0/1&quot;&gt;Cory Dal Ponte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dushyanthen_S/0/1/0/all/0/1&quot;&gt;Sathana Dushyanthen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyons_K/0/1/0/all/0/1&quot;&gt;Kayley Lyons&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01635">
<title>RTP: Rethinking Tensor Parallelism with Memory Deduplication. (arXiv:2311.01635v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2311.01635</link>
<description rdf:parseType="Literal">&lt;p&gt;In the evolving landscape of neural network models, one prominent challenge
stand out: the significant memory overheads associated with training expansive
models. Addressing this challenge, this study delves deep into the Rotated
Tensor Parallelism (RTP). RTP is an innovative approach that strategically
focuses on memory deduplication in distributed training environments. It boasts
of unique features like a customized communication primitive and the Flyweight
Pattern initialization. Furthermore, RTP ensures a seamless overlap between
partition computation and partition weight communication, optimizing the
training process. Our empirical evaluations underscore RTP&apos;s efficiency,
revealing that its memory consumption during distributed system training is
remarkably close to the optimal - distributing the memory overhead of a single
machine equitably among multiple machines. The experimental results demonstrate
that RTP is capable of achieving comparable performance to Distributed Data
Parallel while providing support for significantly larger models with
near-linear scalability in terms of memory. Code of RTP is available at
https://github.com/wdlctc/rtp.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1&quot;&gt;Cheng Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_T/0/1/0/all/0/1&quot;&gt;Tianle Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fox_G/0/1/0/all/0/1&quot;&gt;Geoffrey Fox&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01650">
<title>MARRS: Multimodal Reference Resolution System. (arXiv:2311.01650v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.01650</link>
<description rdf:parseType="Literal">&lt;p&gt;Successfully handling context is essential for any dialog understanding task.
This context maybe be conversational (relying on previous user queries or
system responses), visual (relying on what the user sees, for example, on their
screen), or background (based on signals such as a ringing alarm or playing
music). In this work, we present an overview of MARRS, or Multimodal Reference
Resolution System, an on-device framework within a Natural Language
Understanding system, responsible for handling conversational, visual and
background context. In particular, we present different machine learning models
to enable handing contextual queries; specifically, one to enable reference
resolution, and one to handle context via query rewriting. We also describe how
these models complement each other to form a unified, coherent, lightweight
system that can understand context while preserving user privacy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ates_H/0/1/0/all/0/1&quot;&gt;Halim Cagri Ates&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhargava_S/0/1/0/all/0/1&quot;&gt;Shruti Bhargava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Site Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiarui Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maddula_S/0/1/0/all/0/1&quot;&gt;Siddhardha Maddula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moniz_J/0/1/0/all/0/1&quot;&gt;Joel Ruben Antony Moniz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nalamalapu_A/0/1/0/all/0/1&quot;&gt;Anil Kumar Nalamalapu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_R/0/1/0/all/0/1&quot;&gt;Roman Hoang Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozyildirim_M/0/1/0/all/0/1&quot;&gt;Melis Ozyildirim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1&quot;&gt;Alkesh Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piraviperumal_D/0/1/0/all/0/1&quot;&gt;Dhivya Piraviperumal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Renkens_V/0/1/0/all/0/1&quot;&gt;Vincent Renkens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samal_A/0/1/0/all/0/1&quot;&gt;Ankit Samal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1&quot;&gt;Thy Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tseng_B/0/1/0/all/0/1&quot;&gt;Bo-Hsiang Tseng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_R/0/1/0/all/0/1&quot;&gt;Rong Zou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01661">
<title>Deep Learning-driven Community Resilience Rating based on Intertwined Socio-Technical Systems Features. (arXiv:2311.01661v1 [cs.SI])</title>
<link>http://arxiv.org/abs/2311.01661</link>
<description rdf:parseType="Literal">&lt;p&gt;Community resilience is a complex and muti-faceted phenomenon that emerges
from complex and nonlinear interactions among different socio-technical systems
and their resilience properties. However, present studies on community
resilience focus primarily on vulnerability assessment and utilize index-based
approaches, with limited ability to capture heterogeneous features within
community socio-technical systems and their nonlinear interactions in shaping
robustness, redundancy, and resourcefulness components of resilience. To
address this gap, this paper presents an integrated three-layer deep learning
model for community resilience rating (called Resili-Net). Twelve measurable
resilience features are specified and computed within community socio-technical
systems (i.e., facilities, infrastructures, and society) related to three
resilience components of robustness, redundancy, and resourcefulness. Using
publicly accessible data from multiple metropolitan statistical areas in the
United States, Resili-Net characterizes the resilience levels of spatial areas
into five distinct levels. The interpretability of the model outcomes enables
feature analysis for specifying the determinants of resilience in areas within
each resilience level, allowing for the identification of specific resilience
enhancement strategies. Changes in community resilience profiles under urban
development patterns are further examined by changing the value of related
socio-technical systems features. Accordingly, the outcomes provide novel
perspectives for community resilience assessment by harnessing machine
intelligence and heterogeneous urban big data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1&quot;&gt;Kai Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mostafavi_A/0/1/0/all/0/1&quot;&gt;Ali Mostafavi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01676">
<title>MineSegSAT: An automated system to evaluate mining disturbed area extents from Sentinel-2 imagery. (arXiv:2311.01676v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01676</link>
<description rdf:parseType="Literal">&lt;p&gt;Assessing the environmental impact of the mineral extraction industry plays a
critical role in understanding and mitigating the ecological consequences of
extractive activities. This paper presents MineSegSAT, a model that presents a
novel approach to predicting environmentally impacted areas of mineral
extraction sites using the SegFormer deep learning segmentation architecture
trained on Sentinel-2 data. The data was collected from non-overlapping regions
over Western Canada in 2021 containing areas of land that have been
environmentally impacted by mining activities that were identified from
high-resolution satellite imagery in 2021. The SegFormer architecture, a
state-of-the-art semantic segmentation framework, is employed to leverage its
advanced spatial understanding capabilities for accurate land cover
classification. We investigate the efficacy of loss functions including Dice,
Tversky, and Lovasz loss respectively. The trained model was utilized for
inference over the test region in the ensuing year to identify potential areas
of expansion or contraction over these same periods. The Sentinel-2 data is
made available on Amazon Web Services through a collaboration with Earth Daily
Analytics which provides corrected and tiled analytics-ready data on the AWS
platform. The model and ongoing API to access the data on AWS allow the
creation of an automated tool to monitor the extent of disturbed areas
surrounding known mining sites to ensure compliance with their environmental
impact goals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+MacDonald_E/0/1/0/all/0/1&quot;&gt;Ezra MacDonald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacoby_D/0/1/0/all/0/1&quot;&gt;Derek Jacoby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coady_Y/0/1/0/all/0/1&quot;&gt;Yvonne Coady&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01677">
<title>DialogBench: Evaluating LLMs as Human-like Dialogue Systems. (arXiv:2311.01677v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.01677</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have achieved remarkable breakthroughs in new
dialogue capabilities, refreshing human&apos;s impressions on dialogue systems. The
long-standing goal of dialogue systems is to be human-like enough to establish
long-term connections with users by satisfying the need for communication,
affection and social belonging. Therefore, there has been an urgent need to
evaluate LLMs as human-like dialogue systems. In this paper, we propose
DialogBench, a dialogue evaluation benchmark that currently contains $12$
dialogue tasks to assess the capabilities of LLMs as human-like dialogue
systems should have. Specifically, we prompt GPT-4 to generate evaluation
instances for each task. We first design the basic prompt based on widely-used
design principles and further mitigate the existing biases to generate
higher-quality evaluation instances. Our extensive test over $28$ LLMs
(including pre-trained and supervised instruction-tuning) shows that
instruction fine-tuning benefits improve the human likeness of LLMs to a
certain extent, but there is still much room to improve those capabilities for
most LLMs as human-like dialogue systems. In addition, experimental results
also indicate that LLMs perform differently in various abilities that
human-like dialogue systems should have. We will publicly release DialogBench,
along with the associated evaluation code for the broader research community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ou_J/0/1/0/all/0/1&quot;&gt;Jiao Ou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Junda Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Che Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yihong Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fuzheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Di Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhongyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gai_K/0/1/0/all/0/1&quot;&gt;Kun Gai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01681">
<title>The R.O.A.D. to precision medicine. (arXiv:2311.01681v1 [stat.AP])</title>
<link>http://arxiv.org/abs/2311.01681</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a prognostic stratum matching framework that addresses the
deficiencies of Randomized trial data subgroup analysis and transforms
ObservAtional Data to be used as if they were randomized, thus paving the road
for precision medicine. Our approach counters the effects of unobserved
confounding in observational data by correcting the estimated probabilities of
the outcome under a treatment through a novel two-step process. These
probabilities are then used to train Optimal Policy Trees (OPTs), which are
decision trees that optimally assign treatments to subgroups of patients based
on their characteristics. This facilitates the creation of clinically intuitive
treatment recommendations. We applied our framework to observational data of
patients with gastrointestinal stromal tumors (GIST) and validated the OPTs in
an external cohort using the sensitivity and specificity metrics. We show that
these recommendations outperformed those of experts in GIST. We further applied
the same framework to randomized clinical trial (RCT) data of patients with
extremity sarcomas. Remarkably, despite the initial trial results suggesting
that all patients should receive treatment, our framework, after addressing
imbalances in patient distribution due to the trial&apos;s small sample size,
identified through the OPTs a subset of patients with unique characteristics
who may not require treatment. Again, we successfully validated our
recommendations in an external cohort.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bertsimas_D/0/1/0/all/0/1&quot;&gt;Dimitris Bertsimas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Koulouras_A/0/1/0/all/0/1&quot;&gt;Angelos G. Koulouras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Margonis_G/0/1/0/all/0/1&quot;&gt;Georgios Antonios Margonis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01689">
<title>Data-Free Distillation of Language Model by Text-to-Text Transfer. (arXiv:2311.01689v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.01689</link>
<description rdf:parseType="Literal">&lt;p&gt;Data-Free Knowledge Distillation (DFKD) plays a vital role in compressing the
model when original training data is unavailable. Previous works for DFKD in
NLP mainly focus on distilling encoder-only structures like BERT on
classification tasks, which overlook the notable progress of generative
language modeling. In this work, we propose a novel DFKD framework, namely
DFKD-T$^{3}$, where the pretrained generative language model can also serve as
a controllable data generator for model compression. This novel framework
DFKD-T$^{3}$ leads to an end-to-end learnable text-to-text framework to
transform the general domain corpus to compression-friendly task data,
targeting to improve both the \textit{specificity} and \textit{diversity}.
Extensive experiments show that our method can boost the distillation
performance in various downstream tasks such as sentiment analysis, linguistic
acceptability, and information extraction. Furthermore, we show that the
generated texts can be directly used for distilling other language models and
outperform the SOTA methods, making our method more appealing in a general DFKD
setting. Our code is available at
https://gitee.com/mindspore/models/tree/master/research/nlp/DFKD\_T3.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Z/0/1/0/all/0/1&quot;&gt;Zheyuan Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinduo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Hailin Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1&quot;&gt;Tianyu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qinghua Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunhe Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01713">
<title>An Empirical Study of Benchmarking Chinese Aspect Sentiment Quad Prediction. (arXiv:2311.01713v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.01713</link>
<description rdf:parseType="Literal">&lt;p&gt;Aspect sentiment quad prediction (ASQP) is a critical subtask of aspect-level
sentiment analysis. Current ASQP datasets are characterized by their small size
and low quadruple density, which hinders technical development. To expand
capacity, we construct two large Chinese ASQP datasets crawled from multiple
online platforms. The datasets hold several significant characteristics: larger
size (each with 10,000+ samples) and rich aspect categories, more words per
sentence, and higher density than existing ASQP datasets. Moreover, we are the
first to evaluate the performance of Generative Pre-trained Transformer (GPT)
series models on ASQP and exhibit potential issues. The experiments with
state-of-the-art ASQP baselines underscore the need to explore additional
techniques to address ASQP, as well as the importance of further investigation
into methods to improve the performance of GPTs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Junxian Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Haiqin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Junpeng_Y/0/1/0/all/0/1&quot;&gt;Ye Junpeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yuxuan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mou_H/0/1/0/all/0/1&quot;&gt;Hao Mou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01723">
<title>Towards Calibrated Robust Fine-Tuning of Vision-Language Models. (arXiv:2311.01723v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01723</link>
<description rdf:parseType="Literal">&lt;p&gt;While fine-tuning unleashes the potential of a pre-trained model to a
specific task, it trades off the model&apos;s generalization capability on
out-of-distribution (OOD) datasets. To mitigate this, robust fine-tuning aims
to ensure performance on OOD datasets as well as an in-distribution (ID)
dataset for which the model is being tuned. However, another criterion for
reliable machine learning (ML), confidence calibration, has been overlooked
despite its increasing demand for real-world high-stakes ML applications (e.g.,
autonomous driving and medical diagnosis). For the first time, we raise
concerns about the calibration of fine-tuned vision-language models (VLMs)
under distribution shift by showing that naive fine-tuning and even
state-of-the-art robust fine-tuning methods hurt the calibration of pre-trained
VLMs, especially on OOD datasets. To address this, we provide a simple
approach, called a calibrated robust fine-tuning (CaRot) that incentivizes the
calibration and robustness on both ID and OOD datasets. Empirical results on
ImageNet-1K distribution shift evaluation verify the effectiveness of our
method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_C/0/1/0/all/0/1&quot;&gt;Changdae Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Mijoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1&quot;&gt;Hyesu Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Junhyeok Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_E/0/1/0/all/0/1&quot;&gt;Euiseog Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1&quot;&gt;Zhi-Qi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1&quot;&gt;Kyungwoo Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01727">
<title>Flexible Error Mitigation of Quantum Processes with Data Augmentation Empowered Neural Model. (arXiv:2311.01727v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2311.01727</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks have shown their effectiveness in various tasks in the realm
of quantum computing. However, their application in quantum error mitigation, a
crucial step towards realizing practical quantum advancements, has been
restricted by reliance on noise-free statistics. To tackle this critical
challenge, we propose a data augmentation empowered neural model for error
mitigation (DAEM). Our model does not require any prior knowledge about the
specific noise type and measurement settings and can estimate noise-free
statistics solely from the noisy measurement results of the target quantum
process, rendering it highly suitable for practical implementation. In
numerical experiments, we show the model&apos;s superior performance in mitigating
various types of noise, including Markovian noise and Non-Markovian noise,
compared with previous error mitigation methods. We further demonstrate its
versatility by employing the model to mitigate errors in diverse types of
quantum processes, including those involving large-scale quantum systems and
continuous-variable quantum states. This powerful data augmentation-empowered
neural model for error mitigation establishes a solid foundation for realizing
more reliable and robust quantum technologies in practical applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Liao_M/0/1/0/all/0/1&quot;&gt;Manwen Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Chiribella_G/0/1/0/all/0/1&quot;&gt;Giulio Chiribella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuxiang Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01743">
<title>Energy Efficiency Optimization for Subterranean LoRaWAN Using A Reinforcement Learning Approach: A Direct-to-Satellite Scenario. (arXiv:2311.01743v1 [cs.IT])</title>
<link>http://arxiv.org/abs/2311.01743</link>
<description rdf:parseType="Literal">&lt;p&gt;The integration of subterranean LoRaWAN and non-terrestrial networks (NTN)
delivers substantial economic and societal benefits in remote agriculture and
disaster rescue operations. The LoRa modulation leverages quasi-orthogonal
spreading factors (SFs) to optimize data rates, airtime, coverage and energy
consumption. However, it is still challenging to effectively assign SFs to end
devices for minimizing co-SF interference in massive subterranean LoRaWAN NTN.
To address this, we investigate a reinforcement learning (RL)-based SFs
allocation scheme to optimize the system&apos;s energy efficiency (EE). To
efficiently capture the device-to-environment interactions in dense networks,
we proposed an SFs allocation technique using the multi-agent dueling double
deep Q-network (MAD3QN) and the multi-agent advantage actor-critic (MAA2C)
algorithms based on an analytical reward mechanism. Our proposed RL-based SFs
allocation approach evinces better performance compared to four benchmarks in
the extreme underground direct-to-satellite scenario. Remarkably, MAD3QN shows
promising potentials in surpassing MAA2C in terms of convergence rate and EE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1&quot;&gt;Kaiqiang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ullah_M/0/1/0/all/0/1&quot;&gt;Muhammad Asad Ullah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alves_H/0/1/0/all/0/1&quot;&gt;Hirley Alves&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mikhaylov_K/0/1/0/all/0/1&quot;&gt;Konstantin Mikhaylov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_T/0/1/0/all/0/1&quot;&gt;Tong Hao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01753">
<title>RiskQ: Risk-sensitive Multi-Agent Reinforcement Learning Value Factorization. (arXiv:2311.01753v1 [cs.MA])</title>
<link>http://arxiv.org/abs/2311.01753</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-agent systems are characterized by environmental uncertainty, varying
policies of agents, and partial observability, which result in significant
risks. In the context of Multi-Agent Reinforcement Learning (MARL), learning
coordinated and decentralized policies that are sensitive to risk is
challenging. To formulate the coordination requirements in risk-sensitive MARL,
we introduce the Risk-sensitive Individual-Global-Max (RIGM) principle as a
generalization of the Individual-Global-Max (IGM) and Distributional IGM (DIGM)
principles. This principle requires that the collection of risk-sensitive
action selections of each agent should be equivalent to the risk-sensitive
action selection of the central policy. Current MARL value factorization
methods do not satisfy the RIGM principle for common risk metrics such as the
Value at Risk (VaR) metric or distorted risk measurements. Therefore, we
propose RiskQ to address this limitation, which models the joint return
distribution by modeling quantiles of it as weighted quantile mixtures of
per-agent return distribution utilities. RiskQ satisfies the RIGM principle for
the VaR and distorted risk metrics. We show that RiskQ can obtain promising
performance through extensive experiments. The source code of RiskQ is
available in https://github.com/xmu-rl-3dv/RiskQ.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1&quot;&gt;Siqi Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chennan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Weiquan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yongquan Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_S/0/1/0/all/0/1&quot;&gt;Songzhu Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinwang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Cheng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01757">
<title>Indo LEGO-ABSA: A Multitask Generative Aspect Based Sentiment Analysis for Indonesian Language. (arXiv:2311.01757v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.01757</link>
<description rdf:parseType="Literal">&lt;p&gt;Aspect-based sentiment analysis is a method in natural language processing
aimed at identifying and understanding sentiments related to specific aspects
of an entity. Aspects are words or phrases that represent an aspect or
attribute of a particular entity. Previous research has utilized generative
pre-trained language models to perform aspect-based sentiment analysis.
LEGO-ABSA is one framework that has successfully employed generative
pre-trained language models in aspect-based sentiment analysis, particularly in
English. LEGO-ABSA uses a multitask learning and prompting approach to enhance
model performance. However, the application of this approach has not been done
in the context of Bahasa Indonesia. Therefore, this research aims to implement
the multitask learning and prompting approach in aspect-based sentiment
analysis for Bahasa Indonesia using generative pre-trained language models. In
this study, the Indo LEGO-ABSA model is developed, which is an aspect-based
sentiment analysis model utilizing generative pre-trained language models and
trained with multitask learning and prompting. Indo LEGO-ABSA is trained with a
hotel domain dataset in the Indonesian language. The obtained results include
an f1-score of 79.55% for the Aspect Sentiment Triplet Extraction task, 86.09%
for Unified Aspect-based Sentiment Analysis, 79.85% for Aspect Opinion Pair
Extraction, 87.45% for Aspect Term Extraction, and 88.09% for Opinion Term
Extraction. Indo LEGO-ABSA adopts the LEGO-ABSA framework that employs the T5
model, specifically mT5, by applying multitask learning to train all tasks
within aspect-based sentiment analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suchrady_R/0/1/0/all/0/1&quot;&gt;Randy Zakya Suchrady&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Purwarianti_A/0/1/0/all/0/1&quot;&gt;Ayu Purwarianti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01770">
<title>Modeling the Uncertainty with Maximum Discrepant Students for Semi-supervised 2D Pose Estimation. (arXiv:2311.01770v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01770</link>
<description rdf:parseType="Literal">&lt;p&gt;Semi-supervised pose estimation is a practically challenging task for
computer vision. Although numerous excellent semi-supervised classification
methods have emerged, these methods typically use confidence to evaluate the
quality of pseudo-labels, which is difficult to achieve in pose estimation
tasks. For example, in pose estimation, confidence represents only the
possibility that a position of the heatmap is a keypoint, not the quality of
that prediction. In this paper, we propose a simple yet efficient framework to
estimate the quality of pseudo-labels in semi-supervised pose estimation tasks
from the perspective of modeling the uncertainty of the pseudo-labels.
Concretely, under the dual mean-teacher framework, we construct the two maximum
discrepant students (MDSs) to effectively push two teachers to generate
different decision boundaries for the same sample. Moreover, we create multiple
uncertainties to assess the quality of the pseudo-labels. Experimental results
demonstrate that our method improves the performance of semi-supervised pose
estimation on three datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiaqi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1&quot;&gt;Junbiao Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qingming Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01786">
<title>TCM-GPT: Efficient Pre-training of Large Language Models for Domain Adaptation in Traditional Chinese Medicine. (arXiv:2311.01786v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.01786</link>
<description rdf:parseType="Literal">&lt;p&gt;Pre-training and fine-tuning have emerged as a promising paradigm across
various natural language processing (NLP) tasks. The effectiveness of
pretrained large language models (LLM) has witnessed further enhancement,
holding potential for applications in the field of medicine, particularly in
the context of Traditional Chinese Medicine (TCM). However, the application of
these general models to specific domains often yields suboptimal results,
primarily due to challenges like lack of domain knowledge, unique objectives,
and computational efficiency. Furthermore, their effectiveness in specialized
domains, such as Traditional Chinese Medicine, requires comprehensive
evaluation. To address the above issues, we propose a novel domain specific
TCMDA (TCM Domain Adaptation) approach, efficient pre-training with
domain-specific corpus. Specifically, we first construct a large TCM-specific
corpus, TCM-Corpus-1B, by identifying domain keywords and retreving from
general corpus. Then, our TCMDA leverages the LoRA which freezes the pretrained
model&apos;s weights and uses rank decomposition matrices to efficiently train
specific dense layers for pre-training and fine-tuning, efficiently aligning
the model with TCM-related tasks, namely TCM-GPT-7B. We further conducted
extensive experiments on two TCM tasks, including TCM examination and TCM
diagnosis. TCM-GPT-7B archived the best performance across both datasets,
outperforming other models by relative increments of 17% and 12% in accuracy,
respectively. To the best of our knowledge, our study represents the pioneering
validation of domain adaptation of a large language model with 7 billion
parameters in TCM domain. We will release both TCMCorpus-1B and TCM-GPT-7B
model once accepted to facilitate interdisciplinary development in TCM and NLP,
serving as the foundation for further study.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guoxing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jianyu Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaohong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guangyu Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01792">
<title>AFPQ: Asymmetric Floating Point Quantization for LLMs. (arXiv:2311.01792v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.01792</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) show great performance in various tasks, but
face deployment challenges from limited memory capacity and bandwidth. Low-bit
weight quantization can save memory and accelerate inference. Although
floating-point (FP) formats show good performance in LLM quantization, they
tend to perform poorly with small group sizes or sub-4 bits. We find the reason
is that the absence of asymmetry in previous FP quantization makes it
unsuitable for handling asymmetric value distribution of LLM weight tensors. In
this work, we propose asymmetric FP quantization (AFPQ), which sets separate
scales for positive and negative values. Our method leads to large accuracy
improvements and can be easily plugged into other quantization methods,
including GPTQ and AWQ, for better performance. Besides, no additional storage
is needed compared with asymmetric integer (INT) quantization. The code is
available at https://github.com/zhangsichengsjtu/AFPQ.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yijia Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Sicheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1&quot;&gt;Shijie Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_D/0/1/0/all/0/1&quot;&gt;Dayou Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1&quot;&gt;Jianyu Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1&quot;&gt;Ting Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1&quot;&gt;Ningyi Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01811">
<title>DiffDub: Person-generic Visual Dubbing Using Inpainting Renderer with Diffusion Auto-encoder. (arXiv:2311.01811v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01811</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating high-quality and person-generic visual dubbing remains a
challenge. Recent innovation has seen the advent of a two-stage paradigm,
decoupling the rendering and lip synchronization process facilitated by
intermediate representation as a conduit. Still, previous methodologies rely on
rough landmarks or are confined to a single speaker, thus limiting their
performance. In this paper, we propose DiffDub: Diffusion-based dubbing. We
first craft the Diffusion auto-encoder by an inpainting renderer incorporating
a mask to delineate editable zones and unaltered regions. This allows for
seamless filling of the lower-face region while preserving the remaining parts.
Throughout our experiments, we encountered several challenges. Primarily, the
semantic encoder lacks robustness, constricting its ability to capture
high-level features. Besides, the modeling ignored facial positioning, causing
mouth or nose jitters across frames. To tackle these issues, we employ
versatile strategies, including data augmentation and supplementary eye
guidance. Moreover, we encapsulated a conformer-based reference encoder and
motion generator fortified by a cross-attention mechanism. This enables our
model to learn person-specific textures with varying references and reduces
reliance on paired audio-visual data. Our rigorous experiments comprehensively
highlight that our ground-breaking approach outpaces existing methods with
considerable margins and delivers seamless, intelligible videos in
person-generic and multilingual scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1&quot;&gt;Chenpeng Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1&quot;&gt;Shuai Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1&quot;&gt;Feilong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1&quot;&gt;Kai Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01842">
<title>A Neural Radiance Field-Based Architecture for Intelligent Multilayered View Synthesis. (arXiv:2311.01842v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2311.01842</link>
<description rdf:parseType="Literal">&lt;p&gt;A mobile ad hoc network is made up of a number of wireless portable nodes
that spontaneously come together en route for establish a transitory network
with no need for any central management. A mobile ad hoc network (MANET) is
made up of a sizable and reasonably dense community of mobile nodes that travel
across any terrain and rely solely on wireless interfaces for communication,
not on any well before centralized management. Furthermore, routing be supposed
to offer a method for instantly delivering data across a network between any
two nodes. Finding the best packet routing from across infrastructure is the
major issue, though. The proposed protocol&apos;s major goal is to identify the
least-expensive nominal capacity acquisition that assures the transportation of
realistic transport that ensures its durability in the event of any node
failure. This study suggests the Optimized Route Selection via Red Imported
Fire Ants (RIFA) Strategy as a way to improve on-demand source routing systems.
Predicting Route Failure and energy Utilization is used to pick the path during
the routing phase. Proposed work assess the results of the comparisons based on
performance parameters like as energy usage, packet delivery rate (PDR), and
end-to-end (E2E) delay. The outcome demonstrates that the proposed strategy is
preferable and increases network lifetime while lowering node energy
consumption and typical E2E delay under the majority of network performance
measures and factors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhinakaran_D/0/1/0/all/0/1&quot;&gt;D. Dhinakaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sankar_S/0/1/0/all/0/1&quot;&gt;S. M. Udhaya Sankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elumalai_G/0/1/0/all/0/1&quot;&gt;G. Elumalai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+kumar_N/0/1/0/all/0/1&quot;&gt;N. Jagadish kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01860">
<title>FAME: Flexible, Scalable Analogy Mappings Engine. (arXiv:2311.01860v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.01860</link>
<description rdf:parseType="Literal">&lt;p&gt;Analogy is one of the core capacities of human cognition; when faced with new
situations, we often transfer prior experience from other domains. Most work on
computational analogy relies heavily on complex, manually crafted input. In
this work, we relax the input requirements, requiring only names of entities to
be mapped. We automatically extract commonsense representations and use them to
identify a mapping between the entities. Unlike previous works, our framework
can handle partial analogies and suggest new entities to be added. Moreover,
our method&apos;s output is easily interpretable, allowing for users to understand
why a specific mapping was chosen.
&lt;/p&gt;
&lt;p&gt;Experiments show that our model correctly maps 81.2% of classical 2x2 analogy
problems (guess level=50%). On larger problems, it achieves 77.8% accuracy
(mean guess level=13.1%). In another experiment, we show our algorithm
outperforms human performance, and the automatic suggestions of new entities
resemble those suggested by humans. We hope this work will advance
computational analogy by paving the way to more flexible, realistic input
requirements, with broader applicability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacob_S/0/1/0/all/0/1&quot;&gt;Shahar Jacob&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shani_C/0/1/0/all/0/1&quot;&gt;Chen Shani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahaf_D/0/1/0/all/0/1&quot;&gt;Dafna Shahaf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01864">
<title>SortNet: Learning To Rank By a Neural-Based Sorting Algorithm. (arXiv:2311.01864v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.01864</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of relevance ranking consists of sorting a set of objects with
respect to a given criterion. Since users may prefer different relevance
criteria, the ranking algorithms should be adaptable to the user needs. Two
main approaches exist in literature for the task of learning to rank: 1) a
score function, learned by examples, which evaluates the properties of each
object yielding an absolute relevance value that can be used to order the
objects or 2) a pairwise approach, where a &quot;preference function&quot; is learned
using pairs of objects to define which one has to be ranked first. In this
paper, we present SortNet, an adaptive ranking algorithm which orders objects
using a neural network as a comparator. The neural network training set
provides examples of the desired ordering between pairs of items and it is
constructed by an iterative procedure which, at each iteration, adds the most
informative training examples. Moreover, the comparator adopts a connectionist
architecture that is particularly suited for implementing a preference
function. We also prove that such an architecture has the universal
approximation property and can implement a wide class of functions. Finally,
the proposed algorithm is evaluated on the LETOR dataset showing promising
performances in comparison with other state of the art algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rigutini_L/0/1/0/all/0/1&quot;&gt;Leonardo Rigutini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papini_T/0/1/0/all/0/1&quot;&gt;Tiziano Papini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maggini_M/0/1/0/all/0/1&quot;&gt;Marco Maggini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scarselli_F/0/1/0/all/0/1&quot;&gt;Franco Scarselli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01866">
<title>Towards Concept-Aware Large Language Models. (arXiv:2311.01866v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.01866</link>
<description rdf:parseType="Literal">&lt;p&gt;Concepts play a pivotal role in various human cognitive functions, including
learning, reasoning and communication. However, there is very little work on
endowing machines with the ability to form and reason with concepts. In
particular, state-of-the-art large language models (LLMs) work at the level of
tokens, not concepts.
&lt;/p&gt;
&lt;p&gt;In this work, we analyze how well contemporary LLMs capture human concepts
and their structure. We then discuss ways to develop concept-aware LLMs, taking
place at different stages of the pipeline. We sketch a method for pretraining
LLMs using concepts, and also explore the simpler approach that uses the output
of existing LLMs. Despite its simplicity, our proof-of-concept is shown to
better match human intuition, as well as improve the robustness of predictions.
These preliminary results underscore the promise of concept-aware LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shani_C/0/1/0/all/0/1&quot;&gt;Chen Shani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vreeken_J/0/1/0/all/0/1&quot;&gt;Jilles Vreeken&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahaf_D/0/1/0/all/0/1&quot;&gt;Dafna Shahaf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01870">
<title>Multi-EuP: The Multilingual European Parliament Dataset for Analysis of Bias in Information Retrieval. (arXiv:2311.01870v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.01870</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Multi-EuP, a new multilingual benchmark dataset, comprising 22K
multi-lingual documents collected from the European Parliament, spanning 24
languages. This dataset is designed to investigate fairness in a multilingual
information retrieval (IR) context to analyze both language and demographic
bias in a ranking context. It boasts an authentic multilingual corpus,
featuring topics translated into all 24 languages, as well as cross-lingual
relevance judgments. Furthermore, it offers rich demographic information
associated with its documents, facilitating the study of demographic bias. We
report the effectiveness of Multi-EuP for benchmarking both monolingual and
multilingual IR. We also conduct a preliminary experiment on language bias
caused by the choice of tokenization strategy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jinrui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1&quot;&gt;Timothy Baldwin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1&quot;&gt;Trevor Cohn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01875">
<title>Enhancing Functional Data Analysis with Sequential Neural Networks: Advantages and Comparative Study. (arXiv:2311.01875v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.01875</link>
<description rdf:parseType="Literal">&lt;p&gt;Functional Data Analysis (FDA) is a statistical domain developed to handle
functional data characterized by high dimensionality and complex data
structures. Sequential Neural Networks (SNNs) are specialized neural networks
capable of processing sequence data, a fundamental aspect of functional data.
Despite their great flexibility in modeling functional data, SNNs have been
inadequately employed in the FDA community. One notable advantage of SNNs is
the ease of implementation, making them accessible to a broad audience beyond
academia. Conversely, FDA-based methodologies present challenges, particularly
for practitioners outside the field, due to their intricate complexity. In
light of this, we propose utilizing SNNs in FDA applications and demonstrate
their effectiveness through comparative analyses against popular FDA regression
models based on numerical experiments and real-world data analysis. SNN
architectures allow us to surpass the limitations of traditional FDA methods,
offering scalability, flexibility, and improved analytical performance. Our
findings highlight the potential of SNN-based methodologies as powerful tools
for data applications involving functional data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;J. Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;J. Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;M. Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jadhav_S/0/1/0/all/0/1&quot;&gt;S. Jadhav&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01918">
<title>Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review. (arXiv:2311.01918v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.01918</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rapid development of artificial intelligence, large language models
(LLMs) have shown promising capabilities in mimicking human-level language
comprehension and reasoning. This has sparked significant interest in applying
LLMs to enhance various aspects of healthcare, ranging from medical education
to clinical decision support. However, medicine involves multifaceted data
modalities and nuanced reasoning skills, presenting challenges for integrating
LLMs. This paper provides a comprehensive review on the applications and
implications of LLMs in medicine. It begins by examining the fundamental
applications of general-purpose and specialized LLMs, demonstrating their
utilities in knowledge retrieval, research support, clinical workflow
automation, and diagnostic assistance. Recognizing the inherent multimodality
of medicine, the review then focuses on multimodal LLMs, investigating their
ability to process diverse data types like medical imaging and EHRs to augment
diagnostic accuracy. To address LLMs&apos; limitations regarding personalization and
complex clinical reasoning, the paper explores the emerging development of
LLM-powered autonomous agents for healthcare. Furthermore, it summarizes the
evaluation methodologies for assessing LLMs&apos; reliability and safety in medical
contexts. Overall, this review offers an extensive analysis on the
transformative potential of LLMs in modern medicine. It also highlights the
pivotal need for continuous optimizations and ethical oversight before these
models can be effectively integrated into clinical practice. Visit
https://github.com/mingze-yuan/Awesome-LLM-Healthcare for an accompanying
GitHub repository containing latest papers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1&quot;&gt;Mingze Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_P/0/1/0/all/0/1&quot;&gt;Peng Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1&quot;&gt;Jiajia Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yunhao Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zifan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yi Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jie Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Li Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Lin Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1&quot;&gt;Bin Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01927">
<title>GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling. (arXiv:2311.01927v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.01927</link>
<description rdf:parseType="Literal">&lt;p&gt;Linear Recurrence has proven to be a powerful tool for modeling long
sequences efficiently. In this work, we show that existing models fail to take
full advantage of its potential. Motivated by this finding, we develop
GateLoop, a foundational sequence model that generalizes linear recurrent
models such as S4, S5, LRU and RetNet, by employing data-controlled state
transitions. Utilizing this theoretical advance, GateLoop empirically
outperforms existing models for auto-regressive language modeling. Our method
comes with a low-cost $O(l)$ recurrent mode and an efficient $O(l \log_{2} l)$
parallel mode making use of highly optimized associative scan implementations.
Furthermore, we derive an $O(l^2)$ surrogate attention mode, revealing
remarkable implications for Transformer and recently proposed architectures.
Specifically, we prove that our approach can be interpreted as providing
data-controlled relative-positional information to Attention. While many
existing models solely rely on data-controlled cumulative sums for context
aggregation, our findings suggest that incorporating data-controlled complex
cumulative products may be a crucial step towards more powerful sequence
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katsch_T/0/1/0/all/0/1&quot;&gt;Tobias Katsch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01937">
<title>Supermind Ideator: Exploring generative AI to support creative problem-solving. (arXiv:2311.01937v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.01937</link>
<description rdf:parseType="Literal">&lt;p&gt;Previous efforts to support creative problem-solving have included (a)
techniques (such as brainstorming and design thinking) to stimulate creative
ideas, and (b) software tools to record and share these ideas. Now, generative
AI technologies can suggest new ideas that might never have occurred to the
users, and users can then select from these ideas or use them to stimulate even
more ideas. Here, we describe such a system, Supermind Ideator. The system uses
a large language model (GPT 3.5) and adds prompting, fine tuning, and a user
interface specifically designed to help people use creative problem-solving
techniques. Some of these techniques can be applied to any problem; others are
specifically intended to help generate innovative ideas about how to design
groups of people and/or computers (&quot;superminds&quot;). We also describe our early
experiences with using this system and suggest ways it could be extended to
support additional techniques for other specific problem-solving domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rick_S/0/1/0/all/0/1&quot;&gt;Steven R. Rick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giacomelli_G/0/1/0/all/0/1&quot;&gt;Gianni Giacomelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1&quot;&gt;Haoran Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laubacher_R/0/1/0/all/0/1&quot;&gt;Robert J. Laubacher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taubenslag_N/0/1/0/all/0/1&quot;&gt;Nancy Taubenslag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heyman_J/0/1/0/all/0/1&quot;&gt;Jennifer L. Heyman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knicker_M/0/1/0/all/0/1&quot;&gt;Max Sina Knicker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeddi_Y/0/1/0/all/0/1&quot;&gt;Younes Jeddi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maier_H/0/1/0/all/0/1&quot;&gt;Hendrik Maier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dwyer_S/0/1/0/all/0/1&quot;&gt;Stephen Dwyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ragupathy_P/0/1/0/all/0/1&quot;&gt;Pranav Ragupathy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malone_T/0/1/0/all/0/1&quot;&gt;Thomas W. Malone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01939">
<title>A Quantitative Autonomy Quantification Framework for Fully Autonomous Robotic Systems. (arXiv:2311.01939v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.01939</link>
<description rdf:parseType="Literal">&lt;p&gt;Although autonomous functioning facilitates deployment of robotic systems in
domains that admit limited human oversight on our planet and beyond, finding
correspondence between task requirements and autonomous capability is still an
open challenge. Consequently, a number of methods for quantifying autonomy have
been proposed over the last three decades, but to our knowledge all these have
no discernment of sub-mode features of variation of autonomy and some are based
on metrics that violet the Goodhart&apos;s law. This paper focuses on the full
autonomous mode and proposes a task-requirements based autonomy assessment
framework. The framework starts by establishing robot task characteristics from
which three autonomy metrics, namely requisite capability, reliability and
responsiveness, and functions for determining autonomy as a two-part measure,
namely of level of autonomy and degree of autonomy are derived. These
characteristics are founded on the realization that robots ultimately replace
human skilled workers, to find a mapping between human job and robot task
characteristics. The distinction between level and degree of autonomy stemmed
from the acknowledgment that autonomy is not just a question of existence, but
also one of performance of requisite capability. When continuously monitored,
the proposed metrics provide a means of monitoring the integrity of a system.
The framework has been demonstrated on two case studies, namely autonomous
vehicle at an on-road dynamic driving task and the DARPA subT challenge rules
analysis. The framework provides not only a tool for quantifying autonomy, but
also a regulatory interface and common language for autonomous systems
developers and users.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gyagenda_N/0/1/0/all/0/1&quot;&gt;Nasser Gyagenda&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roth_H/0/1/0/all/0/1&quot;&gt;Hubert Roth&lt;/a&gt; (1) ((1) University of Siegen)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01956">
<title>Architecture of Smart Certificates for Web3 Applications Against Cyberthreats in Financial Industry. (arXiv:2311.01956v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2311.01956</link>
<description rdf:parseType="Literal">&lt;p&gt;This study addresses the security challenges associated with the current
internet transformations, specifically focusing on emerging technologies such
as blockchain and decentralized storage. It also investigates the role of Web3
applications in shaping the future of the internet. The primary objective is to
propose a novel design for &apos;smart certificates,&apos; which are digital certificates
that can be programmatically enforced. Utilizing such certificates, an
enterprise can better protect itself from cyberattacks and ensure the security
of its data and systems. Web3 recent security solutions by companies and
projects like Certik, Forta, Slither, and Securify are the equivalent of code
scanning tool that were originally developed for Web1 and Web2 applications,
and definitely not like certificates to help enterprises feel safe against
cyberthreats. We aim to improve the resilience of enterprises&apos; digital
infrastructure by building on top of Web3 application and put methodologies in
place for vulnerability analysis and attack correlation, focusing on
architecture of different layers, Wallet/Client, Application and Smart
Contract, where specific components are provided to identify and predict
threats and risks. Furthermore, Certificate Transparency is used for enhancing
the security, trustworthiness and decentralized management of the certificates,
and detecting misuses, compromises, and malfeasances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Behfar_S/0/1/0/all/0/1&quot;&gt;Stefan Kambiz Behfar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crowcroft_J/0/1/0/all/0/1&quot;&gt;Jon Crowcroft&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01961">
<title>Assessing Fidelity in XAI post-hoc techniques: A Comparative Study with Ground Truth Explanations Datasets. (arXiv:2311.01961v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01961</link>
<description rdf:parseType="Literal">&lt;p&gt;The evaluation of the fidelity of eXplainable Artificial Intelligence (XAI)
methods to their underlying models is a challenging task, primarily due to the
absence of a ground truth for explanations. However, assessing fidelity is a
necessary step for ensuring a correct XAI methodology. In this study, we
conduct a fair and objective comparison of the current state-of-the-art XAI
methods by introducing three novel image datasets with reliable ground truth
for explanations. The primary objective of this comparison is to identify
methods with low fidelity and eliminate them from further research, thereby
promoting the development of more trustworthy and effective XAI techniques. Our
results demonstrate that XAI methods based on the backpropagation of output
information to input yield higher accuracy and reliability compared to methods
relying on sensitivity analysis or Class Activation Maps (CAM). However, the
backpropagation method tends to generate more noisy saliency maps. These
findings have significant implications for the advancement of XAI methods,
enabling the elimination of erroneous explanations and fostering the
development of more robust and reliable XAI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miro_Nicolau_M/0/1/0/all/0/1&quot;&gt;M. Mir&amp;#xf3;-Nicolau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaume_i_Capo_A/0/1/0/all/0/1&quot;&gt;A. Jaume-i-Cap&amp;#xf3;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moya_Alcover_G/0/1/0/all/0/1&quot;&gt;G. Moy&amp;#xe0;-Alcover&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01964">
<title>Don&apos;t Make Your LLM an Evaluation Benchmark Cheater. (arXiv:2311.01964v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.01964</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models~(LLMs) have greatly advanced the frontiers of
artificial intelligence, attaining remarkable improvement in model capacity. To
assess the model performance, a typical approach is to construct evaluation
benchmarks for measuring the ability level of LLMs in different aspects.
Despite that a number of high-quality benchmarks have been released, the
concerns about the appropriate use of these benchmarks and the fair comparison
of different models are increasingly growing. Considering these concerns, in
this paper, we discuss the potential risk and impact of inappropriately using
evaluation benchmarks and misleadingly interpreting the evaluation results.
Specially, we focus on a special issue that would lead to inappropriate
evaluation, \ie \emph{benchmark leakage}, referring that the data related to
evaluation sets is occasionally used for model training. This phenomenon now
becomes more common since pre-training data is often prepared ahead of model
test. We conduct extensive experiments to study the effect of benchmark
leverage, and find that it can dramatically boost the evaluation results, which
would finally lead to an unreliable assessment of model performance. To improve
the use of existing evaluation benchmarks, we finally present several
guidelines for both LLM developers and benchmark maintainers. We hope this work
can draw attention to appropriate training and evaluation of LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1&quot;&gt;Kun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yutao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhipeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wentong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wayne Xin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yankai Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Ji-Rong Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jiawei Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01967">
<title>The language of prompting: What linguistic properties make a prompt successful?. (arXiv:2311.01967v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.01967</link>
<description rdf:parseType="Literal">&lt;p&gt;The latest generation of LLMs can be prompted to achieve impressive zero-shot
or few-shot performance in many NLP tasks. However, since performance is highly
sensitive to the choice of prompts, considerable effort has been devoted to
crowd-sourcing prompts or designing methods for prompt optimisation. Yet, we
still lack a systematic understanding of how linguistic properties of prompts
correlate with task performance. In this work, we investigate how LLMs of
different sizes, pre-trained and instruction-tuned, perform on prompts that are
semantically equivalent, but vary in linguistic structure. We investigate both
grammatical properties such as mood, tense, aspect and modality, as well as
lexico-semantic variation through the use of synonyms. Our findings contradict
the common assumption that LLMs achieve optimal performance on lower perplexity
prompts that reflect language use in pretraining or instruction-tuning data.
Prompts transfer poorly between datasets or models, and performance cannot
generally be explained by perplexity, word frequency, ambiguity or prompt
length. Based on our results, we put forward a proposal for a more robust and
comprehensive evaluation standard for prompting research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leidinger_A/0/1/0/all/0/1&quot;&gt;Alina Leidinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rooij_R/0/1/0/all/0/1&quot;&gt;Robert van Rooij&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shutova_E/0/1/0/all/0/1&quot;&gt;Ekaterina Shutova&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01977">
<title>RT-Trajectory: Robotic Task Generalization via Hindsight Trajectory Sketches. (arXiv:2311.01977v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.01977</link>
<description rdf:parseType="Literal">&lt;p&gt;Generalization remains one of the most important desiderata for robust robot
learning systems. While recently proposed approaches show promise in
generalization to novel objects, semantic concepts, or visual distribution
shifts, generalization to new tasks remains challenging. For example, a
language-conditioned policy trained on pick-and-place tasks will not be able to
generalize to a folding task, even if the arm trajectory of folding is similar
to pick-and-place. Our key insight is that this kind of generalization becomes
feasible if we represent the task through rough trajectory sketches. We propose
a policy conditioning method using such rough trajectory sketches, which we
call RT-Trajectory, that is practical, easy to specify, and allows the policy
to effectively perform new tasks that would otherwise be challenging to
perform. We find that trajectory sketches strike a balance between being
detailed enough to express low-level motion-centric guidance while being coarse
enough to allow the learned policy to interpret the trajectory sketch in the
context of situational visual observations. In addition, we show how trajectory
sketches can provide a useful interface to communicate with robotic policies:
they can be specified through simple human inputs like drawings or videos, or
through automated methods such as modern image-generating or
waypoint-generating methods. We evaluate RT-Trajectory at scale on a variety of
real-world robotic tasks, and find that RT-Trajectory is able to perform a
wider range of tasks compared to language-conditioned and goal-conditioned
policies, when provided the same training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jiayuan Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirmani_S/0/1/0/all/0/1&quot;&gt;Sean Kirmani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wohlhart_P/0/1/0/all/0/1&quot;&gt;Paul Wohlhart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arenas_M/0/1/0/all/0/1&quot;&gt;Montserrat Gonzalez Arenas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1&quot;&gt;Kanishka Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1&quot;&gt;Wenhao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1&quot;&gt;Chuyuan Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1&quot;&gt;Keerthana Gopalakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhuo Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sundaresan_P/0/1/0/all/0/1&quot;&gt;Priya Sundaresan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Peng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hao Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hausman_K/0/1/0/all/0/1&quot;&gt;Karol Hausman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1&quot;&gt;Chelsea Finn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vuong_Q/0/1/0/all/0/1&quot;&gt;Quan Vuong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1&quot;&gt;Ted Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01994">
<title>Obtaining Explainable Classification Models using Distributionally Robust Optimization. (arXiv:2311.01994v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2311.01994</link>
<description rdf:parseType="Literal">&lt;p&gt;Model explainability is crucial for human users to be able to interpret how a
proposed classifier assigns labels to data based on its feature values. We
study generalized linear models constructed using sets of feature value rules,
which can capture nonlinear dependencies and interactions. An inherent
trade-off exists between rule set sparsity and its prediction accuracy. It is
computationally expensive to find the right choice of sparsity -- e.g., via
cross-validation -- with existing methods. We propose a new formulation to
learn an ensemble of rule sets that simultaneously addresses these competing
factors. Good generalization is ensured while keeping computational costs low
by utilizing distributionally robust optimization. The formulation utilizes
column generation to efficiently search the space of rule sets and constructs a
sparse ensemble of rule sets, in contrast with techniques like random forests
or boosting and their variants. We present theoretical results that motivate
and justify the use of our distributionally robust formulation. Extensive
numerical experiments establish that our method improves over competing methods
-- on a large set of publicly available binary classification problem instances
-- with respect to one or more of the following metrics: generalization
quality, computational cost, and explainability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dash_S/0/1/0/all/0/1&quot;&gt;Sanjeeb Dash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Soumyadip Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Goncalves_J/0/1/0/all/0/1&quot;&gt;Joao Goncalves&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Squillante_M/0/1/0/all/0/1&quot;&gt;Mark S. Squillante&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02013">
<title>Score Models for Offline Goal-Conditioned Reinforcement Learning. (arXiv:2311.02013v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.02013</link>
<description rdf:parseType="Literal">&lt;p&gt;Offline Goal-Conditioned Reinforcement Learning (GCRL) is tasked with
learning to achieve multiple goals in an environment purely from offline
datasets using sparse reward functions. Offline GCRL is pivotal for developing
generalist agents capable of leveraging pre-existing datasets to learn diverse
and reusable skills without hand-engineering reward functions. However,
contemporary approaches to GCRL based on supervised learning and contrastive
learning are often suboptimal in the offline setting. An alternative
perspective on GCRL optimizes for occupancy matching, but necessitates learning
a discriminator, which subsequently serves as a pseudo-reward for downstream
RL. Inaccuracies in the learned discriminator can cascade, negatively
influencing the resulting policy. We present a novel approach to GCRL under a
new lens of mixture-distribution matching, leading to our discriminator-free
method: SMORe. The key insight is combining the occupancy matching perspective
of GCRL with a convex dual formulation to derive a learning objective that can
better leverage suboptimal offline data. SMORe learns scores or unnormalized
densities representing the importance of taking an action at a state for
reaching a particular goal. SMORe is principled and our extensive experiments
on the fully offline GCRL benchmark composed of robot manipulation and
locomotion tasks, including high-dimensional observations, show that SMORe can
outperform state-of-the-art baselines by a significant margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sikchi_H/0/1/0/all/0/1&quot;&gt;Harshit Sikchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chitnis_R/0/1/0/all/0/1&quot;&gt;Rohan Chitnis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Touati_A/0/1/0/all/0/1&quot;&gt;Ahmed Touati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geramifard_A/0/1/0/all/0/1&quot;&gt;Alborz Geramifard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Amy Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1&quot;&gt;Scott Niekum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02017">
<title>DeliverAI: Reinforcement Learning Based Distributed Path-Sharing Network for Food Deliveries. (arXiv:2311.02017v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.02017</link>
<description rdf:parseType="Literal">&lt;p&gt;Delivery of items from the producer to the consumer has experienced
significant growth over the past decade and has been greatly fueled by the
recent pandemic. Amazon Fresh, Shopify, UberEats, InstaCart, and DoorDash are
rapidly growing and are sharing the same business model of consumer items or
food delivery. Existing food delivery methods are sub-optimal because each
delivery is individually optimized to go directly from the producer to the
consumer via the shortest time path. We observe a significant scope for
reducing the costs associated with completing deliveries under the current
model. We model our food delivery problem as a multi-objective optimization,
where consumer satisfaction and delivery costs, both, need to be optimized.
Taking inspiration from the success of ride-sharing in the taxi industry, we
propose DeliverAI - a reinforcement learning-based path-sharing algorithm.
Unlike previous attempts for path-sharing, DeliverAI can provide real-time,
time-efficient decision-making using a Reinforcement learning-enabled agent
system. Our novel agent interaction scheme leverages path-sharing among
deliveries to reduce the total distance traveled while keeping the delivery
completion time under check. We generate and test our methodology vigorously on
a simulation setup using real data from the city of Chicago. Our results show
that DeliverAI can reduce the delivery fleet size by 12\%, the distance
traveled by 13%, and achieve 50% higher fleet utilization compared to the
baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehra_A/0/1/0/all/0/1&quot;&gt;Ashman Mehra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1&quot;&gt;Snehanshu Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raychoudhury_V/0/1/0/all/0/1&quot;&gt;Vaskar Raychoudhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mathur_A/0/1/0/all/0/1&quot;&gt;Archana Mathur&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02018">
<title>Active Reasoning in an Open-World Environment. (arXiv:2311.02018v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.02018</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in vision-language learning have achieved notable success on
complete-information question-answering datasets through the integration of
extensive world knowledge. Yet, most models operate passively, responding to
questions based on pre-stored knowledge. In stark contrast, humans possess the
ability to actively explore, accumulate, and reason using both newfound and
existing information to tackle incomplete-information questions. In response to
this gap, we introduce $Conan$, an interactive open-world environment devised
for the assessment of active reasoning. $Conan$ facilitates active exploration
and promotes multi-round abductive inference, reminiscent of rich, open-world
settings like Minecraft. Diverging from previous works that lean primarily on
single-round deduction via instruction following, $Conan$ compels agents to
actively interact with their surroundings, amalgamating new evidence with prior
knowledge to elucidate events from incomplete observations. Our analysis on
$Conan$ underscores the shortcomings of contemporary state-of-the-art models in
active exploration and understanding complex scenarios. Additionally, we
explore Abduction from Deduction, where agents harness Bayesian rules to recast
the challenge of abduction as a deductive process. Through $Conan$, we aim to
galvanize advancements in active reasoning and set the stage for the next
generation of artificial intelligence agents adept at dynamically engaging in
environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Manjie Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_G/0/1/0/all/0/1&quot;&gt;Guangyuan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1&quot;&gt;Wei Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yixin Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02026">
<title>APRICOT: Acuity Prediction in Intensive Care Unit (ICU): Predicting Stability, Transitions, and Life-Sustaining Therapies. (arXiv:2311.02026v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.02026</link>
<description rdf:parseType="Literal">&lt;p&gt;The acuity state of patients in the intensive care unit (ICU) can quickly
change from stable to unstable, sometimes leading to life-threatening
conditions. Early detection of deteriorating conditions can result in providing
more timely interventions and improved survival rates. Current approaches rely
on manual daily assessments. Some data-driven approaches have been developed,
that use mortality as a proxy of acuity in the ICU. However, these methods do
not integrate acuity states to determine the stability of a patient or the need
for life-sustaining therapies. In this study, we propose APRICOT (Acuity
Prediction in Intensive Care Unit), a Transformer-based neural network to
predict acuity state in real-time in ICU patients. We develop and extensively
validate externally, temporally, and prospectively the APRICOT model on three
large datasets: University of Florida Health (UFH), eICU Collaborative Research
Database (eICU), and Medical Information Mart for Intensive Care (MIMIC)-IV.
The performance of APRICOT shows comparable results to state-of-the-art
mortality prediction models (external AUROC 0.93-0.93, temporal AUROC
0.96-0.98, and prospective AUROC 0.98) as well as acuity prediction models
(external AUROC 0.80-0.81, temporal AUROC 0.77-0.78, and prospective AUROC
0.87). Furthermore, APRICOT can make predictions for the need for
life-sustaining therapies, showing comparable results to state-of-the-art
ventilation prediction models (external AUROC 0.80-0.81, temporal AUROC
0.87-0.88, and prospective AUROC 0.85), and vasopressor prediction models
(external AUROC 0.82-0.83, temporal AUROC 0.73-0.75, prospective AUROC 0.87).
This tool allows for real-time acuity monitoring of a patient and can provide
helpful information to clinicians to make timely interventions. Furthermore,
the model can suggest life-sustaining therapies that the patient might need in
the next hours in the ICU.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Contreras_M/0/1/0/all/0/1&quot;&gt;Miguel Contreras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_B/0/1/0/all/0/1&quot;&gt;Brandon Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shickel_B/0/1/0/all/0/1&quot;&gt;Benjamin Shickel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baslanti_T/0/1/0/all/0/1&quot;&gt;Tezcan Ozrazgat Baslanti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1&quot;&gt;Yuanfang Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_Z/0/1/0/all/0/1&quot;&gt;Ziyuan Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bandyopadhyay_S/0/1/0/all/0/1&quot;&gt;Sabyasachi Bandyopadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khezeli_K/0/1/0/all/0/1&quot;&gt;Kia Khezeli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bihorac_A/0/1/0/all/0/1&quot;&gt;Azra Bihorac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rashidi_P/0/1/0/all/0/1&quot;&gt;Parisa Rashidi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02041">
<title>Quantum circuit synthesis with diffusion models. (arXiv:2311.02041v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2311.02041</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantum computing has recently emerged as a transformative technology. Yet,
its promised advantages rely on efficiently translating quantum operations into
viable physical realizations. In this work, we use generative machine learning
models, specifically denoising diffusion models (DMs), to facilitate this
transformation. Leveraging text-conditioning, we steer the model to produce
desired quantum operations within gate-based quantum circuits. Notably, DMs
allow to sidestep during training the exponential overhead inherent in the
classical simulation of quantum dynamics -- a consistent bottleneck in
preceding ML techniques. We demonstrate the model&apos;s capabilities across two
tasks: entanglement generation and unitary compilation. The model excels at
generating new circuits and supports typical DM extensions such as masking and
editing to, for instance, align the circuit generation to the constraints of
the targeted quantum device. Given their flexibility and generalization
abilities, we envision DMs as pivotal in quantum circuit synthesis, enhancing
both practical applications but also insights into theoretical quantum
computation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Furrutter_F/0/1/0/all/0/1&quot;&gt;Florian F&amp;#xfc;rrutter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Munoz_Gil_G/0/1/0/all/0/1&quot;&gt;Gorka Mu&amp;#xf1;oz-Gil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Briegel_H/0/1/0/all/0/1&quot;&gt;Hans J. Briegel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02049">
<title>Post Turing: Mapping the landscape of LLM Evaluation. (arXiv:2311.02049v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.02049</link>
<description rdf:parseType="Literal">&lt;p&gt;In the rapidly evolving landscape of Large Language Models (LLMs),
introduction of well-defined and standardized evaluation methodologies remains
a crucial challenge. This paper traces the historical trajectory of LLM
evaluations, from the foundational questions posed by Alan Turing to the modern
era of AI research. We categorize the evolution of LLMs into distinct periods,
each characterized by its unique benchmarks and evaluation criteria. As LLMs
increasingly mimic human-like behaviors, traditional evaluation proxies, such
as the Turing test, have become less reliable. We emphasize the pressing need
for a unified evaluation system, given the broader societal implications of
these models. Through an analysis of common evaluation methodologies, we
advocate for a qualitative shift in assessment approaches, underscoring the
importance of standardization and objective criteria. This work serves as a
call for the AI community to collaboratively address the challenges of LLM
evaluation, ensuring their reliability, fairness, and societal benefit.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1&quot;&gt;Alexey Tikhonov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamshchikov_I/0/1/0/all/0/1&quot;&gt;Ivan P. Yamshchikov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2106.12915">
<title>Numerical influence of ReLU&apos;(0) on backpropagation. (arXiv:2106.12915v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2106.12915</link>
<description rdf:parseType="Literal">&lt;p&gt;In theory, the choice of ReLU(0) in [0, 1] for a neural network has a
negligible influence both on backpropagation and training. Yet, in the real
world, 32 bits default precision combined with the size of deep learning
problems makes it a hyperparameter of training methods. We investigate the
importance of the value of ReLU&apos;(0) for several precision levels (16, 32, 64
bits), on various networks (fully connected, VGG, ResNet) and datasets (MNIST,
CIFAR10, SVHN, ImageNet). We observe considerable variations of backpropagation
outputs which occur around half of the time in 32 bits precision. The effect
disappears with double precision, while it is systematic at 16 bits. For
vanilla SGD training, the choice ReLU&apos;(0) = 0 seems to be the most efficient.
For our experiments on ImageNet the gain in test accuracy over ReLU&apos;(0) = 1 was
more than 10 points (two runs). We also evidence that reconditioning approaches
as batch-norm or ADAM tend to buffer the influence of ReLU&apos;(0)&apos;s value.
Overall, the message we convey is that algorithmic differentiation of nonsmooth
problems potentially hides parameters that could be tuned advantageously.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertoin_D/0/1/0/all/0/1&quot;&gt;David Bertoin&lt;/a&gt; (ISAE-SUPAERO), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bolte_J/0/1/0/all/0/1&quot;&gt;J&amp;#xe9;r&amp;#xf4;me Bolte&lt;/a&gt; (TSE-R), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gerchinovitz_S/0/1/0/all/0/1&quot;&gt;S&amp;#xe9;bastien Gerchinovitz&lt;/a&gt; (IMT), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pauwels_E/0/1/0/all/0/1&quot;&gt;Edouard Pauwels&lt;/a&gt; (IRIT-ADRIA)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2111.08452">
<title>On minimizers and convolutional filters: theoretical connections and applications to genome analysis. (arXiv:2111.08452v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2111.08452</link>
<description rdf:parseType="Literal">&lt;p&gt;Minimizers and convolutional neural networks (CNNs) are two quite distinct
popular techniques that have both been employed to analyze categorical
biological sequences. At face value, the methods seem entirely dissimilar.
Minimizers use min-wise hashing on a rolling window to extract a single
important k-mer feature per window. CNNs start with a wide array of randomly
initialized convolutional filters, paired with a pooling operation, and then
multiple additional neural layers to learn both the filters themselves and how
they can be used to classify the sequence.
&lt;/p&gt;
&lt;p&gt;Here, our main result is a careful mathematical analysis of hash function
properties showing that for sequences over a categorical alphabet, random
Gaussian initialization of convolutional filters with max-pooling is equivalent
to choosing a minimizer ordering such that selected k-mers are (in Hamming
distance) far from the k-mers within the sequence but close to other
minimizers. In empirical experiments, we find that this property manifests as
decreased density in repetitive regions, both in simulation and on real human
telomeres. We additionally train from scratch a CNN embedding of synthetic
short-reads from the SARS-CoV-2 genome into 3D Euclidean space that locally
recapitulates the linear sequence distance of the read origins, a modest step
towards building a deep learning assembler, though it is at present too slow to
be practical. In total, this manuscript provides a partial explanation for the
effectiveness of CNNs in categorical sequence analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yun William Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.08369">
<title>Feature-Attending Recurrent Modules for Generalization in Reinforcement Learning. (arXiv:2112.08369v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2112.08369</link>
<description rdf:parseType="Literal">&lt;p&gt;Many important tasks are defined in terms of object. To generalize across
these tasks, a reinforcement learning (RL) agent needs to exploit the structure
that the objects induce. Prior work has either hard-coded object-centric
features, used complex object-centric generative models, or updated state using
local spatial features. However, these approaches have had limited success in
enabling general RL agents. Motivated by this, we introduce &quot;Feature-Attending
Recurrent Modules&quot; (FARM), an architecture for learning state representations
that relies on simple, broadly applicable inductive biases for capturing
spatial and temporal regularities. FARM learns a state representation that is
distributed across multiple modules that each attend to spatiotemporal features
with an expressive feature attention mechanism. We show that this improves an
RL agent&apos;s ability to generalize across object-centric tasks. We study task
suites in both 2D and 3D environments and find that FARM better generalizes
compared to competing architectures that leverage attention or multiple
modules.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carvalho_W/0/1/0/all/0/1&quot;&gt;Wilka Carvalho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lampinen_A/0/1/0/all/0/1&quot;&gt;Andrew Lampinen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikiforou_K/0/1/0/all/0/1&quot;&gt;Kyriacos Nikiforou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1&quot;&gt;Felix Hill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shanahan_M/0/1/0/all/0/1&quot;&gt;Murray Shanahan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.09698">
<title>Graph Neural Diffusion Networks for Semi-supervised Learning. (arXiv:2201.09698v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2201.09698</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Convolutional Networks (GCN) is a pioneering model for graph-based
semi-supervised learning. However, GCN does not perform well on
sparsely-labeled graphs. Its two-layer version cannot effectively propagate the
label information to the whole graph structure (i.e., the under-smoothing
problem) while its deep version over-smoothens and is hard to train (i.e., the
over-smoothing problem). To solve these two issues, we propose a new graph
neural network called GND-Nets (for Graph Neural Diffusion Networks) that
exploits the local and global neighborhood information of a vertex in a single
layer. Exploiting the shallow network mitigates the over-smoothing problem
while exploiting the local and global neighborhood information mitigates the
under-smoothing problem. The utilization of the local and global neighborhood
information of a vertex is achieved by a new graph diffusion method called
neural diffusions, which integrate neural networks into the conventional linear
and nonlinear graph diffusions. The adoption of neural networks makes neural
diffusions adaptable to different datasets. Extensive experiments on various
sparsely-labeled graphs verify the effectiveness and efficiency of GND-Nets
compared to state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1&quot;&gt;Wei Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zexi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1&quot;&gt;Yunqi Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Ambuj Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.00282">
<title>Stabilizing the LIF Neuron Training. (arXiv:2202.00282v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2202.00282</link>
<description rdf:parseType="Literal">&lt;p&gt;Spiking Neuromorphic Computing uses binary activity to improve Artificial
Intelligence energy efficiency. However, the non-smoothness of binary activity
requires approximate gradients, known as Surrogate Gradients (SG), to close the
performance gap with Deep Learning. Several SG have been proposed in the
literature, but it remains unclear how to determine the best SG for a given
task and network. Good performance can be achieved with most SG shapes, after a
costly search of hyper-parameters. Thus, we aim at experimentally and
theoretically define the best SG across different stress tests, to reduce
future need of grid search. To understand the gap for this line of work, we
show that more complex tasks and networks need more careful choice of SG, even
if overall the derivative of the fast sigmoid outperforms other SG across tasks
and networks, for a wide range of learning rates. We therefore design a
stability based theoretical method to choose initialization and SG shape before
training on the most common spiking architecture, the Leaky Integrate and Fire
(LIF). Since our stability method suggests the use of high firing rates at
initialization, which is non-standard in the neuromorphic literature, we show
that high initial firing rates, combined with a sparsity encouraging loss term
introduced gradually, can lead to better generalization, depending on the SG
shape. Our stability based theoretical solution, finds a SG and initialization
that experimentally result in improved accuracy. We show how it can be used to
reduce the need of extensive grid-search of dampening, sharpness and
tail-fatness of the SG.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herranz_Celotti_L/0/1/0/all/0/1&quot;&gt;Luca Herranz-Celotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rouat_J/0/1/0/all/0/1&quot;&gt;Jean Rouat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.09090">
<title>From Understanding Genetic Drift to a Smart-Restart Mechanism for Estimation-of-Distribution Algorithms. (arXiv:2206.09090v5 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2206.09090</link>
<description rdf:parseType="Literal">&lt;p&gt;Estimation-of-distribution algorithms (EDAs) are optimization algorithms that
learn a distribution on the search space from which good solutions can be
sampled easily. A key parameter of most EDAs is the sample size (population
size). If the population size is too small, the update of the probabilistic
model builds on few samples, leading to the undesired effect of genetic drift.
Too large population sizes avoid genetic drift, but slow down the process.
&lt;/p&gt;
&lt;p&gt;Building on a recent quantitative analysis of how the population size leads
to genetic drift, we design a smart-restart mechanism for EDAs. By stopping
runs when the risk for genetic drift is high, it automatically runs the EDA in
good parameter regimes.
&lt;/p&gt;
&lt;p&gt;Via a mathematical runtime analysis, we prove a general performance guarantee
for this smart-restart scheme. This in particular shows that in many situations
where the optimal (problem-specific) parameter values are known, the restart
scheme automatically finds these, leading to the asymptotically optimal
performance.
&lt;/p&gt;
&lt;p&gt;We also conduct an extensive experimental analysis. On four classic benchmark
problems, we clearly observe the critical influence of the population size on
the performance, and we find that the smart-restart scheme leads to a
performance close to the one obtainable with optimal parameter values. Our
results also show that previous theory-based suggestions for the optimal
population size can be far from the optimal ones, leading to a performance
clearly inferior to the one obtained via the smart-restart scheme. We also
conduct experiments with PBIL (cross-entropy algorithm) on two combinatorial
optimization problems from the literature, the max-cut problem and the
bipartition problem. Again, we observe that the smart-restart mechanism finds
much better values for the population size than those suggested in the
literature, leading to a much better performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1&quot;&gt;Weijie Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doerr_B/0/1/0/all/0/1&quot;&gt;Benjamin Doerr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.04873">
<title>Multimodal Prototype-Enhanced Network for Few-Shot Action Recognition. (arXiv:2212.04873v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.04873</link>
<description rdf:parseType="Literal">&lt;p&gt;Current methods for few-shot action recognition mainly fall into the metric
learning framework following ProtoNet, which demonstrates the importance of
prototypes. Although they achieve relatively good performance, the effect of
multimodal information is ignored, e.g. label texts. In this work, we propose a
novel MultimOdal PRototype-ENhanced Network (MORN), which uses the semantic
information of label texts as multimodal information to enhance prototypes. A
CLIP visual encoder and a frozen CLIP text encoder are introduced to obtain
features with good multimodal initialization. Then in the visual flow, visual
prototypes are computed by a Temporal-Relational CrossTransformer (TRX) module
for example. In the text flow, a semantic-enhanced (SE) module and an inflating
operation are used to obtain text prototypes. The final multimodal prototypes
are then computed by a multimodal prototype-enhanced (MPE) module. Besides, we
define a PRototype SImilarity DiffErence (PRIDE) to evaluate the quality of
prototypes, which is used to verify our improvement on the prototype level and
effectiveness of MORN. We conduct extensive experiments on four popular
datasets, and MORN achieves state-of-the-art results on HMDB51, UCF101,
Kinetics and SSv2. When plugging PRIDE into the training stage, the performance
can be further improved.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_X/0/1/0/all/0/1&quot;&gt;Xinzhe Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1&quot;&gt;Hao Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1&quot;&gt;Yatai Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1&quot;&gt;Jing Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yujiu Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.05062">
<title>Tracr: Compiled Transformers as a Laboratory for Interpretability. (arXiv:2301.05062v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.05062</link>
<description rdf:parseType="Literal">&lt;p&gt;We show how to &quot;compile&quot; human-readable programs into standard decoder-only
transformer models. Our compiler, Tracr, generates models with known structure.
This structure can be used to design experiments. For example, we use it to
study &quot;superposition&quot; in transformers that execute multi-step algorithms.
Additionally, the known structure of Tracr-compiled models can serve as
ground-truth for evaluating interpretability methods. Commonly, because the
&quot;programs&quot; learned by transformers are unknown it is unclear whether an
interpretation succeeded. We demonstrate our approach by implementing and
examining programs including computing token frequencies, sorting, and
parenthesis checking. We provide an open-source implementation of Tracr at
https://github.com/google-deepmind/tracr.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lindner_D/0/1/0/all/0/1&quot;&gt;David Lindner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kramar_J/0/1/0/all/0/1&quot;&gt;J&amp;#xe1;nos Kram&amp;#xe1;r&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farquhar_S/0/1/0/all/0/1&quot;&gt;Sebastian Farquhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahtz_M/0/1/0/all/0/1&quot;&gt;Matthew Rahtz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McGrath_T/0/1/0/all/0/1&quot;&gt;Thomas McGrath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mikulik_V/0/1/0/all/0/1&quot;&gt;Vladimir Mikulik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.12247">
<title>SEGA: Instructing Text-to-Image Models using Semantic Guidance. (arXiv:2301.12247v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.12247</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-image diffusion models have recently received a lot of interest for
their astonishing ability to produce high-fidelity images from text only.
However, achieving one-shot generation that aligns with the user&apos;s intent is
nearly impossible, yet small changes to the input prompt often result in very
different images. This leaves the user with little semantic control. To put the
user in control, we show how to interact with the diffusion process to flexibly
steer it along semantic directions. This semantic guidance (SEGA) generalizes
to any generative architecture using classifier-free guidance. More
importantly, it allows for subtle and extensive edits, changes in composition
and style, as well as optimizing the overall artistic conception. We
demonstrate SEGA&apos;s effectiveness on both latent and pixel-based diffusion
models such as Stable Diffusion, Paella, and DeepFloyd-IF using a variety of
tasks, thus providing strong evidence for its versatility, flexibility, and
improvements over existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brack_M/0/1/0/all/0/1&quot;&gt;Manuel Brack&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Friedrich_F/0/1/0/all/0/1&quot;&gt;Felix Friedrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hintersdorf_D/0/1/0/all/0/1&quot;&gt;Dominik Hintersdorf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Struppek_L/0/1/0/all/0/1&quot;&gt;Lukas Struppek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1&quot;&gt;Patrick Schramowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1&quot;&gt;Kristian Kersting&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.07104">
<title>xASTNN: Improved Code Representations for Industrial Practice. (arXiv:2303.07104v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2303.07104</link>
<description rdf:parseType="Literal">&lt;p&gt;The application of deep learning techniques in software engineering becomes
increasingly popular. One key problem is developing high-quality and
easy-to-use source code representations for code-related tasks. The research
community has acquired impressive results in recent years. However, due to the
deployment difficulties and performance bottlenecks, seldom these approaches
are applied to the industry. In this paper, we present xASTNN, an eXtreme
Abstract Syntax Tree (AST)-based Neural Network for source code representation,
aiming to push this technique to industrial practice. The proposed xASTNN has
three advantages. First, xASTNN is completely based on widely-used ASTs and
does not require complicated data pre-processing, making it applicable to
various programming languages and practical scenarios. Second, three
closely-related designs are proposed to guarantee the effectiveness of xASTNN,
including statement subtree sequence for code naturalness, gated recursive unit
for syntactical information, and gated recurrent unit for sequential
information. Third, a dynamic batching algorithm is introduced to significantly
reduce the time complexity of xASTNN. Two code comprehension downstream tasks,
code classification and code clone detection, are adopted for evaluation. The
results demonstrate that our xASTNN can improve the state-of-the-art while
being faster than the baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Min Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xibin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongyu Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.12040">
<title>Roots and Requirements for Collaborative AIs. (arXiv:2303.12040v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2303.12040</link>
<description rdf:parseType="Literal">&lt;p&gt;The vision of AI collaborators has long been a staple of stories and science
fiction, where artificial agents understand nuances of collaboration and human
communication. They assist their human partners and teams and have special
talents. Government advisory groups and leaders in AI have advocated for years
that AIs should be human compatible and effective collaborators. Nonetheless,
robust AIs that collaborate like talented people remain out of reach. The
simpler dream of effective information tools that augment human intelligence
(IA) has its roots in the 1960s and helped to drive an information technology
revolution. With the vast increase in hybrid and remote work since the COVID
pandemic, the benefits and requirements for better coordination, collaboration,
and communication are in focus for the workplace. Many factors (such as the
costs of homes near work) are impeding a mass return to in-person work at the
office. Are human-like AI teammates part of a solution? If we just need better
tools for collaboration, how artificially intelligent (AI) could and should
these tools be? This position paper reviews the arc of technology and calls by
others for human-machine teaming. It draws on earlier research in psychology
and the social sciences about what human-like collaboration actually requires.
This paper argues that current mainstream AI cannot produce robust,
intelligent, and human-compatible collaborators. It sets a context for a second
paper that proposes exploring a radical shift in technology and methodology for
creating resilient, intelligent, and human-compatible AIs (Stefik &amp;amp; Price,
2023). The aspirational goal is that such AIs would learn, share what they
learn, and collaborate to achieve high standards.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stefik_M/0/1/0/all/0/1&quot;&gt;Mark Stefik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.03843">
<title>Why think step by step? Reasoning emerges from the locality of experience. (arXiv:2304.03843v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2304.03843</link>
<description rdf:parseType="Literal">&lt;p&gt;Humans have a powerful and mysterious capacity to reason. Working through a
set of mental steps enables us to make inferences we would not be capable of
making directly even though we get no additional data from the world.
Similarly, when large language models generate intermediate steps (a chain of
thought) before answering a question, they often produce better answers than
they would directly. We investigate why and how chain-of-thought reasoning is
useful in language models, testing the hypothesis that reasoning is effective
when training data consists of overlapping local clusters of variables that
influence each other strongly. These training conditions enable the chaining of
accurate local inferences to estimate relationships between variables that were
not seen together in training. We prove that there will exist a &quot;reasoning
gap&quot;, where reasoning through intermediate variables reduces bias, for the
simple case of an autoregressive density estimator trained on local samples
from a chain-structured probabilistic model. We then test our hypothesis
experimentally in more complex models, training an autoregressive language
model on samples from Bayes nets but only including a subset of variables in
each sample. We test language models&apos; ability to match conditional
probabilities with and without intermediate reasoning steps, finding that
intermediate steps are only helpful when the training data is locally
structured with respect to dependencies between variables. The combination of
locally structured observations and reasoning is much more data-efficient than
training on all variables. Our results illustrate how the effectiveness of
reasoning step by step is rooted in the local statistical structure of the
training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prystawski_B/0/1/0/all/0/1&quot;&gt;Ben Prystawski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Michael Y. Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1&quot;&gt;Noah D. Goodman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.04370">
<title>OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v6 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2304.04370</link>
<description rdf:parseType="Literal">&lt;p&gt;Human Intelligence (HI) excels at combining basic skills to solve complex
tasks. This capability is vital for Artificial Intelligence (AI) and should be
embedded in comprehensive AI Agents, enabling them to harness expert models for
complex task-solving towards Artificial General Intelligence (AGI). Large
Language Models (LLMs) show promising learning and reasoning abilities, and can
effectively use external models, tools, plugins, or APIs to tackle complex
problems. In this work, we introduce OpenAGI, an open-source AGI research and
development platform designed for solving multi-step, real-world tasks.
Specifically, OpenAGI uses a dual strategy, integrating standard benchmark
tasks for benchmarking and evaluation, and open-ended tasks including more
expandable models, tools, plugins, or APIs for creative problem-solving. Tasks
are presented as natural language queries to the LLM, which then selects and
executes appropriate models. We also propose a Reinforcement Learning from Task
Feedback (RLTF) mechanism that uses task results to improve the LLM&apos;s
task-solving ability, which creates a self-improving AI feedback loop. While we
acknowledge that AGI is a broad and multifaceted research challenge with no
singularly defined solution path, the integration of LLMs with domain-specific
expert models, inspired by mirroring the blend of general and specialized
intelligence in humans, offers a promising approach towards AGI. We are
open-sourcing the OpenAGI project&apos;s code, dataset, benchmarks, evaluation
methods, and the UI demo to foster community involvement in AGI advancement:
https://github.com/agiresearch/OpenAGI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yingqiang Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1&quot;&gt;Wenyue Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_K/0/1/0/all/0/1&quot;&gt;Kai Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1&quot;&gt;Jianchao Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1&quot;&gt;Juntao Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shuyuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zelong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongfeng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.11015">
<title>DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction. (arXiv:2304.11015v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2304.11015</link>
<description rdf:parseType="Literal">&lt;p&gt;There is currently a significant gap between the performance of fine-tuned
models and prompting approaches using Large Language Models (LLMs) on the
challenging task of text-to-SQL, as evaluated on datasets such as Spider. To
improve the performance of LLMs in the reasoning process, we study how
decomposing the task into smaller sub-tasks can be effective. In particular, we
show that breaking down the generation problem into sub-problems and feeding
the solutions of those sub-problems into LLMs can be an effective approach for
significantly improving their performance. Our experiments with three LLMs show
that this approach consistently improves their simple few-shot performance by
roughly 10%, pushing the accuracy of LLMs towards SOTA or surpassing it. On the
holdout test set of Spider, the SOTA, in terms of execution accuracy, was 79.9
and the new SOTA at the time of this writing using our approach is 85.3. Our
approach with in-context learning beats many heavily fine-tuned models by at
least 5%. Additionally, when evaluated on the BIRD benchmark, our approach
achieved an execution accuracy of 55.9%, setting a new SOTA on its holdout test
set.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pourreza_M/0/1/0/all/0/1&quot;&gt;Mohammadreza Pourreza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rafiei_D/0/1/0/all/0/1&quot;&gt;Davood Rafiei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.03515">
<title>GradTree: Learning Axis-Aligned Decision Trees with Gradient Descent. (arXiv:2305.03515v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.03515</link>
<description rdf:parseType="Literal">&lt;p&gt;Decision Trees (DTs) are commonly used for many machine learning tasks due to
their high degree of interpretability. However, learning a DT from data is a
difficult optimization problem, as it is non-convex and non-differentiable.
Therefore, common approaches learn DTs using a greedy growth algorithm that
minimizes the impurity locally at each internal node. Unfortunately, this
greedy procedure can lead to inaccurate trees. In this paper, we present a
novel approach for learning hard, axis-aligned DTs with gradient descent. The
proposed method uses backpropagation with a straight-through operator on a
dense DT representation, to jointly optimize all tree parameters. Our approach
outperforms existing methods on binary classification benchmarks and achieves
competitive results for multi-class tasks. The method is available under:
https://github.com/s-marton/GradTree
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marton_S/0/1/0/all/0/1&quot;&gt;Sascha Marton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ludtke_S/0/1/0/all/0/1&quot;&gt;Stefan L&amp;#xfc;dtke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bartelt_C/0/1/0/all/0/1&quot;&gt;Christian Bartelt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stuckenschmidt_H/0/1/0/all/0/1&quot;&gt;Heiner Stuckenschmidt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10050">
<title>The Impact of Missing Data on Causal Discovery: A Multicentric Clinical Study. (arXiv:2305.10050v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10050</link>
<description rdf:parseType="Literal">&lt;p&gt;Causal inference for testing clinical hypotheses from observational data
presents many difficulties because the underlying data-generating model and the
associated causal graph are not usually available. Furthermore, observational
data may contain missing values, which impact the recovery of the causal graph
by causal discovery algorithms: a crucial issue often ignored in clinical
studies. In this work, we use data from a multi-centric study on endometrial
cancer to analyze the impact of different missingness mechanisms on the
recovered causal graph. This is achieved by extending state-of-the-art causal
discovery algorithms to exploit expert knowledge without sacrificing
theoretical soundness. We validate the recovered graph with expert physicians,
showing that our approach finds clinically-relevant solutions. Finally, we
discuss the goodness of fit of our graph and its consistency from a clinical
decision-making perspective using graphical separation to validate causal
pathways.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zanga_A/0/1/0/all/0/1&quot;&gt;Alessio Zanga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bernasconi_A/0/1/0/all/0/1&quot;&gt;Alice Bernasconi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lucas_P/0/1/0/all/0/1&quot;&gt;Peter J.F. Lucas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pijnenborg_H/0/1/0/all/0/1&quot;&gt;Hanny Pijnenborg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Reijnen_C/0/1/0/all/0/1&quot;&gt;Casper Reijnen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Scutari_M/0/1/0/all/0/1&quot;&gt;Marco Scutari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stella_F/0/1/0/all/0/1&quot;&gt;Fabio Stella&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13484">
<title>Flover: A Temporal Fusion Framework for Efficient Autoregressive Model Parallel Inference. (arXiv:2305.13484v3 [cs.DC] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13484</link>
<description rdf:parseType="Literal">&lt;p&gt;Autoregressive models, despite their commendable performance in a myriad of
generative tasks, face challenges stemming from their inherently sequential
structure. Inference on these models, by design, harnesses a temporal
dependency, where the current token&apos;s probability distribution is conditioned
on preceding tokens. This inherent characteristic severely impedes
computational efficiency during inference as a typical inference request can
require more than thousands of tokens, where generating each token requires a
load of entire model weights, making the inference more memory-bound. The large
overhead becomes profound in real deployment where requests arrive randomly,
necessitating various generation lengths. Existing solutions, such as dynamic
batching and concurrent instances, introduce significant response delays and
bandwidth contention, falling short of achieving optimal latency and
throughput. To address these shortcomings, we propose Flover -- a temporal
fusion framework for efficiently inferring multiple requests in parallel. We
deconstruct the general generation pipeline into pre-processing and token
generation, and equip the framework with a dedicated work scheduler for fusing
the generation process temporally across all requests. By orchestrating the
token-level parallelism, Flover exhibits optimal hardware efficiency and
significantly spares the system resources. By further employing a fast buffer
reordering algorithm that allows memory eviction of finished tasks, it brings
over 11x inference speedup on GPT and 16x on LLAMA compared to the cutting-edge
solutions provided by NVIDIA FasterTransformer. Crucially, by leveraging the
advanced tensor parallel technique, Flover proves efficacious across diverse
computational landscapes, from single-GPU setups to distributed scenarios,
thereby offering robust performance optimization that adapts to variable use
cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Jinghan Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alnaasan_N/0/1/0/all/0/1&quot;&gt;Nawras Alnaasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shafi_A/0/1/0/all/0/1&quot;&gt;Aamir Shafi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramoni_H/0/1/0/all/0/1&quot;&gt;Hari Subramoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+K%2E_D/0/1/0/all/0/1&quot;&gt;Dhabaleswar K.&lt;/a&gt; (DK) &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panda/0/1/0/all/0/1&quot;&gt;Panda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13669">
<title>The Knowledge Alignment Problem: Bridging Human and External Knowledge for Large Language Models. (arXiv:2305.13669v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13669</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models often necessitate grounding on external knowledge to
generate faithful and reliable answers. Yet even with the correct groundings in
the reference, they can ignore them and rely on wrong groundings or their
inherent biases to hallucinate when users, being largely unaware of the
specifics of the stored information, pose questions that might not directly
correlate with the retrieved groundings. In this work, we formulate this
knowledge alignment problem and introduce MixAlign, a framework that interacts
with both the human user and the knowledge base to obtain and integrate
clarifications on how the user question relates to the stored information.
MixAlign employs a language model to achieve automatic knowledge alignment and,
if necessary, further enhances this alignment through human user
clarifications. Experimental results highlight the crucial role of knowledge
alignment in boosting model performance and mitigating hallucination, with
improvements noted up to 22.2% and 27.1% respectively. We also demonstrate the
effectiveness of MixAlign in improving knowledge alignment by producing
high-quality, user-centered clarifications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shuo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1&quot;&gt;Liangming Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Junzhou Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;William Yang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16397">
<title>Are Diffusion Models Vision-And-Language Reasoners?. (arXiv:2305.16397v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16397</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-conditioned image generation models have recently shown immense
qualitative success using denoising diffusion processes. However, unlike
discriminative vision-and-language models, it is a non-trivial task to subject
these diffusion-based generative models to automatic fine-grained quantitative
evaluation of high-level phenomena such as compositionality. Towards this goal,
we perform two innovations. First, we transform diffusion-based models (in our
case, Stable Diffusion) for any image-text matching (ITM) task using a novel
method called DiffusionITM. Second, we introduce the Generative-Discriminative
Evaluation Benchmark (GDBench) benchmark with 7 complex vision-and-language
tasks, bias evaluation and detailed analysis. We find that Stable Diffusion +
DiffusionITM is competitive on many tasks and outperforms CLIP on compositional
tasks like like CLEVR and Winoground. We further boost its compositional
performance with a transfer setup by fine-tuning on MS-COCO while retaining
generative capabilities. We also measure the stereotypical bias in diffusion
models, and find that Stable Diffusion 2.1 is, for the most part, less biased
than Stable Diffusion 1.5. Overall, our results point in an exciting direction
bringing discriminative and generative model evaluation closer. We will release
code and benchmark setup soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krojer_B/0/1/0/all/0/1&quot;&gt;Benno Krojer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poole_Dayan_E/0/1/0/all/0/1&quot;&gt;Elinor Poole-Dayan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Voleti_V/0/1/0/all/0/1&quot;&gt;Vikram Voleti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1&quot;&gt;Christopher Pal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1&quot;&gt;Siva Reddy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16671">
<title>A Unified Approach for Maximizing Continuous DR-submodular Functions. (arXiv:2305.16671v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16671</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a unified approach for maximizing continuous
DR-submodular functions that encompasses a range of settings and oracle access
types. Our approach includes a Frank-Wolfe type offline algorithm for both
monotone and non-monotone functions, with different restrictions on the general
convex set. We consider settings where the oracle provides access to either the
gradient of the function or only the function value, and where the oracle
access is either deterministic or stochastic. We determine the number of
required oracle accesses in all cases. Our approach gives new/improved results
for nine out of the sixteen considered cases, avoids computationally expensive
projections in two cases, with the proposed framework matching performance of
state-of-the-art approaches in the remaining five cases. Notably, our approach
for the stochastic function value-based oracle enables the first regret bounds
with bandit feedback for stochastic DR-submodular functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedramfar_M/0/1/0/all/0/1&quot;&gt;Mohammad Pedramfar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quinn_C/0/1/0/all/0/1&quot;&gt;Christopher John Quinn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aggarwal_V/0/1/0/all/0/1&quot;&gt;Vaneet Aggarwal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18381">
<title>Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection. (arXiv:2305.18381v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18381</link>
<description rdf:parseType="Literal">&lt;p&gt;Data-efficient learning has drawn significant attention, especially given the
current trend of large multi-modal models, where dataset distillation can be an
effective solution. However, the dataset distillation process itself is still
very inefficient. In this work, we model the distillation problem with
reference to information transport. Observing that severe data redundancy
exists in dataset distillation, we argue to put more emphasis on the utility of
the training samples. We propose a family of methods to exploit the most
valuable samples, which is validated by our comprehensive analysis of the
optimal data selection. The new strategy significantly reduces the training
cost and extends a variety of existing distillation algorithms to larger and
more diversified datasets, e.g., in some cases only 0.04% training data is
sufficient for comparable distillation performance. Moreover, our strategy
consistently enhances the performance, which may open up new analyses on the
dynamics of distillation and networks. Our method is able to extend the
distillation algorithms to much larger-scale datasets and more heterogeneous
datasets, e.g., ImageNet-1K and Kinetics-400. Our code is available on
https://github.com/silicx/GoldFromOres.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yue Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yong-Lu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_K/0/1/0/all/0/1&quot;&gt;Kaitong Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Cewu Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1&quot;&gt;Yu-Wing Tai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1&quot;&gt;Chi-Keung Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19234">
<title>Grammar Prompting for Domain-Specific Language Generation with Large Language Models. (arXiv:2305.19234v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19234</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) can learn to perform a wide range of natural
language tasks from just a handful of in-context examples. However, for
generating strings from highly structured languages (e.g., semantic parsing to
complex domain-specific languages), it is challenging for the LLM to generalize
from just a few exemplars. We propose \emph{grammar prompting}, a simple
approach to enable LLMs to use external knowledge and domain-specific
constraints, expressed through a grammar in Backus--Naur Form (BNF), during
in-context learning. Grammar prompting augments each demonstration example with
a specialized grammar that is minimally sufficient for generating the
particular output example, where the specialized grammar is a subset of the
full DSL grammar. For inference, the LLM first predicts a BNF grammar given a
test input, and then generates the output according to the rules of the
grammar. Experiments demonstrate that grammar prompting can enable LLMs to
perform competitively on a diverse set of DSL generation tasks, including
semantic parsing (SMCalFlow, Overnight, GeoQuery), PDDL planning, and
SMILES-based molecule generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bailin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xuezhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yuan Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saurous_R/0/1/0/all/0/1&quot;&gt;Rif A. Saurous&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yoon Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00265">
<title>Doubly Robust Self-Training. (arXiv:2306.00265v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00265</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-training is an important technique for solving semi-supervised learning
problems. It leverages unlabeled data by generating pseudo-labels and combining
them with a limited labeled dataset for training. The effectiveness of
self-training heavily relies on the accuracy of these pseudo-labels. In this
paper, we introduce doubly robust self-training, a novel semi-supervised
algorithm that provably balances between two extremes. When the pseudo-labels
are entirely incorrect, our method reduces to a training process solely using
labeled data. Conversely, when the pseudo-labels are completely accurate, our
method transforms into a training process utilizing all pseudo-labeled data and
labeled data, thus increasing the effective sample size. Through empirical
evaluations on both the ImageNet dataset for image classification and the
nuScenes autonomous driving dataset for 3D object detection, we demonstrate the
superiority of the doubly robust loss over the standard self-training baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1&quot;&gt;Banghua Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1&quot;&gt;Mingyu Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacobson_P/0/1/0/all/0/1&quot;&gt;Philip Jacobson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Ming Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1&quot;&gt;Wei Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1&quot;&gt;Michael Jordan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1&quot;&gt;Jiantao Jiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.02231">
<title>Fine-Tuning Language Models with Advantage-Induced Policy Alignment. (arXiv:2306.02231v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.02231</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning from human feedback (RLHF) has emerged as a reliable
approach to aligning large language models (LLMs) to human preferences. Among
the plethora of RLHF techniques, proximal policy optimization (PPO) is of the
most widely used methods. Despite its popularity, however, PPO may suffer from
mode collapse, instability, and poor sample efficiency. We show that these
issues can be alleviated by a novel algorithm that we refer to as
Advantage-Induced Policy Alignment (APA), which leverages a squared error loss
function based on the estimated advantages. We demonstrate empirically that APA
consistently outperforms PPO in language tasks by a large margin, when a
separate reward model is employed as the evaluator. In addition, compared with
PPO, APA offers a more stable form of control over the deviation from the
model&apos;s initial policy, ensuring that the model improves its performance
without collapsing to deterministic output. In addition to empirical results,
we also provide a theoretical justification supporting the design of our loss
function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1&quot;&gt;Banghua Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_H/0/1/0/all/0/1&quot;&gt;Hiteshi Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frujeri_F/0/1/0/all/0/1&quot;&gt;Felipe Vieira Frujeri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1&quot;&gt;Shi Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Chenguang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1&quot;&gt;Michael I. Jordan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1&quot;&gt;Jiantao Jiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10763">
<title>Guiding Language Models of Code with Global Context using Monitors. (arXiv:2306.10763v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10763</link>
<description rdf:parseType="Literal">&lt;p&gt;Language models of code (LMs) work well when the surrounding code provides
sufficient context. This is not true when it becomes necessary to use types,
functionality or APIs defined elsewhere in the repository or a linked library,
especially those not seen during training. LMs suffer from limited awareness of
such global context and end up hallucinating.
&lt;/p&gt;
&lt;p&gt;Integrated development environments (IDEs) assist developers in understanding
repository context using static analysis. We extend this assistance, enjoyed by
developers, to LMs. We propose monitor-guided decoding (MGD) where a monitor
uses static analysis to guide the decoding. We construct a repository-level
dataset PragmaticCode for method-completion in Java and evaluate MGD on it. On
models of varying parameter scale, by monitoring for type-consistent object
dereferences, MGD consistently improves compilation rates and agreement with
ground truth. Further, LMs with fewer parameters, when augmented with MGD, can
outperform larger LMs. With MGD, SantaCoder-1.1B achieves better compilation
rate and next-identifier match than the much larger text-davinci-003 model.
&lt;/p&gt;
&lt;p&gt;We also conduct a generalizability study to evaluate the ability of MGD to
generalize to multiple programming languages (Java, C# and Rust), coding
scenarios (e.g., correct number of arguments to method calls), and to enforce
richer semantic constraints (e.g., stateful API protocols). Our data and
implementation are available at https://github.com/microsoft/monitors4codegen .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_L/0/1/0/all/0/1&quot;&gt;Lakshya A Agrawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanade_A/0/1/0/all/0/1&quot;&gt;Aditya Kanade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1&quot;&gt;Navin Goyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lahiri_S/0/1/0/all/0/1&quot;&gt;Shuvendu K. Lahiri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajamani_S/0/1/0/all/0/1&quot;&gt;Sriram K. Rajamani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15136">
<title>What Truly Matters in Trajectory Prediction for Autonomous Driving?. (arXiv:2306.15136v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15136</link>
<description rdf:parseType="Literal">&lt;p&gt;Trajectory prediction plays a vital role in the performance of autonomous
driving systems, and prediction accuracy, such as average displacement error
(ADE) or final displacement error (FDE), is widely used as a performance
metric. However, a significant disparity exists between the accuracy of
predictors on fixed datasets and driving performance when the predictors are
used downstream for vehicle control, because of a dynamics gap. In the real
world, the prediction algorithm influences the behavior of the ego vehicle,
which, in turn, influences the behaviors of other vehicles nearby. This
interaction results in predictor-specific dynamics that directly impacts
prediction results. In fixed datasets, since other vehicles&apos; responses are
predetermined, this interaction effect is lost, leading to a significant
dynamics gap. This paper studies the overlooked significance of this dynamics
gap. We also examine several other factors contributing to the disparity
between prediction performance and driving performance. The findings highlight
the trade-off between the predictor&apos;s computational efficiency and prediction
accuracy in determining real-world driving performance. In summary, an
interactive, task-driven evaluation protocol for trajectory prediction is
crucial to capture its effectiveness for autonomous driving. Source code along
with experimental settings is available online.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_P/0/1/0/all/0/1&quot;&gt;Phong Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Haoran Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Cunjun Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1&quot;&gt;Panpan Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Sifa Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_D/0/1/0/all/0/1&quot;&gt;David Hsu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02345">
<title>LLQL: Logistic Likelihood Q-Learning for Reinforcement Learning. (arXiv:2307.02345v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02345</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern reinforcement learning (RL) can be categorized into online and offline
variants. As a pivotal aspect of both online and offline RL, current research
on the Bellman equation revolves primarily around optimization techniques and
performance enhancement rather than exploring the inherent structural
properties of the Bellman error, such as its distribution characteristics. This
study investigates the distribution of the Bellman approximation error in both
online and offline settings through iterative exploration of the Bellman
equation. We observed that both in online RL and offline RL, the Bellman error
conforms to a Logistic distribution. Building upon this discovery, this study
employed the Logistics maximum likelihood function (LLoss) as an alternative to
the commonly used MSE Loss, assuming that Bellman errors adhere to a normal
distribution. We validated our hypotheses through extensive numerical
experiments across diverse online and offline environments. In particular, we
applied corrections to the loss function across various baseline algorithms and
consistently observed that the loss function with Logistic corrections
outperformed the MSE counterpart significantly. Additionally, we conducted
Kolmogorov-Smirnov tests to confirm the reliability of the Logistic
distribution. This study&apos;s theoretical and empirical insights provide valuable
groundwork for future investigations and enhancements centered on the
distribution of Bellman errors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_O/0/1/0/all/0/1&quot;&gt;Outongyi Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Bingxin Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08873">
<title>An Alternative to Variance: Gini Deviation for Risk-averse Policy Gradient. (arXiv:2307.08873v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08873</link>
<description rdf:parseType="Literal">&lt;p&gt;Restricting the variance of a policy&apos;s return is a popular choice in
risk-averse Reinforcement Learning (RL) due to its clear mathematical
definition and easy interpretability. Traditional methods directly restrict the
total return variance. Recent methods restrict the per-step reward variance as
a proxy. We thoroughly examine the limitations of these variance-based methods,
such as sensitivity to numerical scale and hindering of policy learning, and
propose to use an alternative risk measure, Gini deviation, as a substitute. We
study various properties of this new risk measure and derive a policy gradient
algorithm to minimize it. Empirical evaluation in domains where risk-aversion
can be clearly defined, shows that our algorithm can mitigate the limitations
of variance-based risk measures and achieves high return with low risk in terms
of variance and Gini deviation when others fail to learn a reasonable policy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yudong Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guiliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poupart_P/0/1/0/all/0/1&quot;&gt;Pascal Poupart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yangchen Pan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08964">
<title>Landscape Surrogate: Learning Decision Losses for Mathematical Optimization Under Partial Information. (arXiv:2307.08964v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08964</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent works in learning-integrated optimization have shown promise in
settings where the optimization problem is only partially observed or where
general-purpose optimizers perform poorly without expert tuning. By learning an
optimizer $\mathbf{g}$ to tackle these challenging problems with $f$ as the
objective, the optimization process can be substantially accelerated by
leveraging past experience. The optimizer can be trained with supervision from
known optimal solutions or implicitly by optimizing the compound function
$f\circ \mathbf{g}$. The implicit approach may not require optimal solutions as
labels and is capable of handling problem uncertainty; however, it is slow to
train and deploy due to frequent calls to optimizer $\mathbf{g}$ during both
training and testing. The training is further challenged by sparse gradients of
$\mathbf{g}$, especially for combinatorial solvers. To address these
challenges, we propose using a smooth and learnable Landscape Surrogate $M$ as
a replacement for $f\circ \mathbf{g}$. This surrogate, learnable by neural
networks, can be computed faster than the solver $\mathbf{g}$, provides dense
and smooth gradients during training, can generalize to unseen optimization
problems, and is efficiently learned via alternating optimization. We test our
approach on both synthetic problems, including shortest path and
multidimensional knapsack, and real-world problems such as portfolio
optimization, achieving comparable or superior objective values compared to
state-of-the-art baselines while reducing the number of calls to $\mathbf{g}$.
Notably, our approach outperforms existing methods for computationally
expensive high-dimensional problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zharmagambetov_A/0/1/0/all/0/1&quot;&gt;Arman Zharmagambetov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amos_B/0/1/0/all/0/1&quot;&gt;Brandon Amos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferber_A/0/1/0/all/0/1&quot;&gt;Aaron Ferber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Taoan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dilkina_B/0/1/0/all/0/1&quot;&gt;Bistra Dilkina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yuandong Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14283">
<title>General Purpose Artificial Intelligence Systems (GPAIS): Properties, Definition, Taxonomy, Societal Implications and Responsible Governance. (arXiv:2307.14283v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.14283</link>
<description rdf:parseType="Literal">&lt;p&gt;Most applications of Artificial Intelligence (AI) are designed for a confined
and specific task. However, there are many scenarios that call for a more
general AI, capable of solving a wide array of tasks without being specifically
designed for them. The term General-Purpose Artificial Intelligence Systems
(GPAIS) has been defined to refer to these AI systems. To date, the possibility
of an Artificial General Intelligence, powerful enough to perform any
intellectual task as if it were human, or even improve it, has remained an
aspiration, fiction, and considered a risk for our society. Whilst we might
still be far from achieving that, GPAIS is a reality and sitting at the
forefront of AI research. This work discusses existing definitions for GPAIS
and proposes a new definition that allows for a gradual differentiation among
types of GPAIS according to their properties and limitations. We distinguish
between closed-world and open-world GPAIS, characterising their degree of
autonomy and ability based on several factors such as adaptation to new tasks,
competence in domains not intentionally trained for, ability to learn from few
data, or proactive acknowledgment of their own limitations. We propose a
taxonomy of approaches to realise GPAIS, describing research trends such as the
use of AI techniques to improve another AI (AI-powered AI) or (single)
foundation models. As a prime example, we delve into GenAI, aligning them with
the concepts presented in the taxonomy. We explore multi-modality, which
involves fusing various types of data sources to expand the capabilities of
GPAIS. Through the proposed definition and taxonomy, our aim is to facilitate
research collaboration across different areas that are tackling general purpose
tasks, as they share many common aspects. Finally, we discuss the state of
GPAIS, prospects, societal implications, and the need for regulation and
governance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Triguero_I/0/1/0/all/0/1&quot;&gt;Isaac Triguero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molina_D/0/1/0/all/0/1&quot;&gt;Daniel Molina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poyatos_J/0/1/0/all/0/1&quot;&gt;Javier Poyatos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ser_J/0/1/0/all/0/1&quot;&gt;Javier Del Ser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herrera_F/0/1/0/all/0/1&quot;&gt;Francisco Herrera&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01923">
<title>Fairness Improvement with Multiple Protected Attributes: How Far Are We?. (arXiv:2308.01923v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.01923</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing research mostly improves the fairness of Machine Learning (ML)
software regarding a single protected attribute at a time, but this is
unrealistic given that many users have multiple protected attributes. This
paper conducts an extensive study of fairness improvement regarding multiple
protected attributes, covering 11 state-of-the-art fairness improvement
methods. We analyze the effectiveness of these methods with different datasets,
metrics, and ML models when considering multiple protected attributes. The
results reveal that improving fairness for a single protected attribute can
largely decrease fairness regarding unconsidered protected attributes. This
decrease is observed in up to 88.3% of scenarios (57.5% on average). More
surprisingly, we find little difference in accuracy loss when considering
single and multiple protected attributes, indicating that accuracy can be
maintained in the multiple-attribute paradigm. However, the effect on precision
and recall when handling multiple protected attributes is about 5 times and 8
times that of a single attribute. This has important implications for future
fairness research: reporting only accuracy as the ML performance metric, which
is currently common in the literature, is inadequate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhenpeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie M. Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarro_F/0/1/0/all/0/1&quot;&gt;Federica Sarro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harman_M/0/1/0/all/0/1&quot;&gt;Mark Harman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02618">
<title>ChatGPT for GTFS: Benchmarking LLMs on GTFS Understanding and Retrieval. (arXiv:2308.02618v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2308.02618</link>
<description rdf:parseType="Literal">&lt;p&gt;The General Transit Feed Specification (GTFS) standard for publishing transit
data is ubiquitous. GTFS being tabular data, with information spread across
different files, necessitates specialized tools or packages to retrieve
information. Concurrently, the use of Large Language Models(LLMs) for text and
information retrieval is growing. The idea of this research is to see if the
current widely adopted LLMs (ChatGPT) are able to understand GTFS and retrieve
information from GTFS using natural language instructions without explicitly
providing information. In this research, we benchmark OpenAI&apos;s GPT-3.5-Turbo
and GPT-4 LLMs which are the backbone of ChatGPT. ChatGPT demonstrates a
reasonable understanding of GTFS by answering 59.7% (GPT-3.5-Turbo) and 73.3%
(GPT-4) of our multiple-choice questions (MCQ) correctly. Furthermore, we
evaluated the LLMs on information extraction tasks using a filtered GTFS feed
containing four routes. We found that program synthesis techniques outperformed
zero-shot approaches, achieving up to 93% (90%) accuracy for simple queries and
61% (41%) for complex ones using GPT-4 (GPT-3.5-Turbo).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Devunuri_S/0/1/0/all/0/1&quot;&gt;Saipraneeth Devunuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiam_S/0/1/0/all/0/1&quot;&gt;Shirin Qiam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lehe_L/0/1/0/all/0/1&quot;&gt;Lewis Lehe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.15457">
<title>From SMOTE to Mixup for Deep Imbalanced Classification. (arXiv:2308.15457v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.15457</link>
<description rdf:parseType="Literal">&lt;p&gt;Given imbalanced data, it is hard to train a good classifier using deep
learning because of the poor generalization of minority classes. Traditionally,
the well-known synthetic minority oversampling technique (SMOTE) for data
augmentation, a data mining approach for imbalanced learning, has been used to
improve this generalization. However, it is unclear whether SMOTE also benefits
deep learning. In this work, we study why the original SMOTE is insufficient
for deep learning, and enhance SMOTE using soft labels. Connecting the
resulting soft SMOTE with Mixup, a modern data augmentation technique, leads to
a unified framework that puts traditional and modern data augmentation
techniques under the same umbrella. A careful study within this framework shows
that Mixup improves generalization by implicitly achieving uneven margins
between majority and minority classes. We then propose a novel margin-aware
Mixup technique that more explicitly achieves uneven margins. Extensive
experimental results demonstrate that our proposed technique yields
state-of-the-art performance on deep imbalanced classification while achieving
superior performance on extremely imbalanced data. The code is open-sourced in
our developed package https://github.com/ntucllab/imbalanced-DL to foster
future research in this direction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1&quot;&gt;Wei-Chao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mai_T/0/1/0/all/0/1&quot;&gt;Tan-Ha Mai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1&quot;&gt;Hsuan-Tien Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.01865">
<title>BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior. (arXiv:2309.01865v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.01865</link>
<description rdf:parseType="Literal">&lt;p&gt;Light-sheet fluorescence microscopy (LSFM), a planar illumination technique
that enables high-resolution imaging of samples, experiences defocused image
quality caused by light scattering when photons propagate through thick
tissues. To circumvent this issue, dualview imaging is helpful. It allows
various sections of the specimen to be scanned ideally by viewing the sample
from opposing orientations. Recent image fusion approaches can then be applied
to determine in-focus pixels by comparing image qualities of two views locally
and thus yield spatially inconsistent focus measures due to their limited
field-of-view. Here, we propose BigFUSE, a global context-aware image fuser
that stabilizes image fusion in LSFM by considering the global impact of photon
propagation in the specimen while determining focus-defocus based on local
image qualities. Inspired by the image formation prior in dual-view LSFM, image
fusion is considered as estimating a focus-defocus boundary using Bayes
Theorem, where (i) the effect of light scattering onto focus measures is
included within Likelihood; and (ii) the spatial consistency regarding
focus-defocus is imposed in Prior. The expectation-maximum algorithm is then
adopted to estimate the focus-defocus boundary. Competitive experimental
results show that BigFUSE is the first dual-view LSFM fuser that is able to
exclude structured artifacts when fusing information, highlighting its
abilities of automatic image fusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Muller_G/0/1/0/all/0/1&quot;&gt;Gesine Muller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Navab_N/0/1/0/all/0/1&quot;&gt;Nassir Navab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Marr_C/0/1/0/all/0/1&quot;&gt;Carsten Marr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huisken_J/0/1/0/all/0/1&quot;&gt;Jan Huisken&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Peng_T/0/1/0/all/0/1&quot;&gt;Tingying Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.02553">
<title>Automating Behavioral Testing in Machine Translation. (arXiv:2309.02553v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.02553</link>
<description rdf:parseType="Literal">&lt;p&gt;Behavioral testing in NLP allows fine-grained evaluation of systems by
examining their linguistic capabilities through the analysis of input-output
behavior. Unfortunately, existing work on behavioral testing in Machine
Translation (MT) is currently restricted to largely handcrafted tests covering
a limited range of capabilities and languages. To address this limitation, we
propose to use Large Language Models (LLMs) to generate a diverse set of source
sentences tailored to test the behavior of MT models in a range of situations.
We can then verify whether the MT model exhibits the expected behavior through
matching candidate sets that are also generated using LLMs. Our approach aims
to make behavioral testing of MT systems practical while requiring only minimal
human effort. In our experiments, we apply our proposed evaluation framework to
assess multiple available MT systems, revealing that while in general
pass-rates follow the trends observable from traditional accuracy-based
metrics, our method was able to uncover several important differences and
potential bugs that go unnoticed when relying only on accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferrando_J/0/1/0/all/0/1&quot;&gt;Javier Ferrando&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sperber_M/0/1/0/all/0/1&quot;&gt;Matthias Sperber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Setiawan_H/0/1/0/all/0/1&quot;&gt;Hendra Setiawan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Telaar_D/0/1/0/all/0/1&quot;&gt;Dominic Telaar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_S/0/1/0/all/0/1&quot;&gt;Sa&amp;#x161;a Hasan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.04579">
<title>EGOFALLS: A visual-audio dataset and benchmark for fall detection using egocentric cameras. (arXiv:2309.04579v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.04579</link>
<description rdf:parseType="Literal">&lt;p&gt;Falls are significant and often fatal for vulnerable populations such as the
elderly. Previous works have addressed the detection of falls by relying on
data capture by a single sensor, images or accelerometers. In this work, we
rely on multimodal descriptors extracted from videos captured by egocentric
cameras. Our proposed method includes a late decision fusion layer that builds
on top of the extracted descriptors. Furthermore, we collect a new dataset on
which we assess our proposed approach. We believe this is the first public
dataset of its kind. The dataset comprises 10,948 video samples by 14 subjects.
We conducted ablation experiments to assess the performance of individual
feature extractors, fusion of visual information, and fusion of both visual and
audio information. Moreover, we experimented with internal and external
cross-validation. Our results demonstrate that the fusion of audio and visual
information through late decision fusion improves detection performance, making
it a promising tool for fall prevention and mitigation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xueyi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03940">
<title>Hard View Selection for Contrastive Learning. (arXiv:2310.03940v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03940</link>
<description rdf:parseType="Literal">&lt;p&gt;Many Contrastive Learning (CL) methods train their models to be invariant to
different &quot;views&quot; of an image input for which a good data augmentation pipeline
is crucial. While considerable efforts were directed towards improving pre-text
tasks, architectures, or robustness (e.g., Siamese networks or teacher-softmax
centering), the majority of these methods remain strongly reliant on the random
sampling of operations within the image augmentation pipeline, such as the
random resized crop or color distortion operation. In this paper, we argue that
the role of the view generation and its effect on performance has so far
received insufficient attention. To address this, we propose an easy,
learning-free, yet powerful Hard View Selection (HVS) strategy designed to
extend the random view generation to expose the pretrained model to harder
samples during CL training. It encompasses the following iterative steps: 1)
randomly sample multiple views and create pairs of two views, 2) run forward
passes for each view pair on the currently trained model, 3) adversarially
select the pair yielding the worst loss, and 4) run the backward pass with the
selected pair. In our empirical analysis we show that under the hood, HVS
increases task difficulty by controlling the Intersection over Union of views
during pretraining. With only 300-epoch pretraining, HVS is able to closely
rival the 800-epoch DINO baseline which remains very favorable even when
factoring in the slowdown induced by the additional forwards of HVS.
Additionally, HVS consistently achieves accuracy improvements on ImageNet
between 0.4% and 1.9% on linear evaluation and similar improvements on transfer
tasks across multiple CL methods, such as DINO, SimSiam, and SimCLR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferreira_F/0/1/0/all/0/1&quot;&gt;Fabio Ferreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rapant_I/0/1/0/all/0/1&quot;&gt;Ivo Rapant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1&quot;&gt;Frank Hutter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05058">
<title>Learning Separable Hidden Unit Contributions for Speaker-Adaptive Lip-Reading. (arXiv:2310.05058v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05058</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel method for speaker adaptation in lip
reading, motivated by two observations. Firstly, a speaker&apos;s own
characteristics can always be portrayed well by his/her few facial images or
even a single image with shallow networks, while the fine-grained dynamic
features associated with speech content expressed by the talking face always
need deep sequential networks to represent accurately. Therefore, we treat the
shallow and deep layers differently for speaker adaptive lip reading. Secondly,
we observe that a speaker&apos;s unique characteristics ( e.g. prominent oral cavity
and mandible) have varied effects on lip reading performance for different
words and pronunciations, necessitating adaptive enhancement or suppression of
the features for robust lip reading. Based on these two observations, we
propose to take advantage of the speaker&apos;s own characteristics to automatically
learn separable hidden unit contributions with different targets for shallow
layers and deep layers respectively. For shallow layers where features related
to the speaker&apos;s characteristics are stronger than the speech content related
features, we introduce speaker-adaptive features to learn for enhancing the
speech content features. For deep layers where both the speaker&apos;s features and
the speech content features are all expressed well, we introduce the
speaker-adaptive features to learn for suppressing the speech content
irrelevant noise for robust lip reading. Our approach consistently outperforms
existing methods, as confirmed by comprehensive analysis and comparison across
different settings. Besides the evaluation on the popular LRW-ID and GRID
datasets, we also release a new dataset for evaluation, CAS-VSR-S68h, to
further assess the performance in an extreme setting where just a few speakers
are available but the speech content covers a large and diversified range.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1&quot;&gt;Songtao Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shuang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1&quot;&gt;Shiguang Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xilin Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05280">
<title>Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems. (arXiv:2310.05280v5 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05280</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in Large Language Models empower them to follow freeform
instructions, including imitating generic or specific demographic personas in
conversations. We define generic personas to represent demographic groups, such
as &quot;an Asian person&quot;, whereas specific personas may take the form of specific
popular Asian names like &quot;Yumi&quot;. While the adoption of personas enriches user
experiences by making dialogue systems more engaging and approachable, it also
casts a shadow of potential risk by exacerbating social biases within model
responses, thereby causing societal harm through interactions with users. In
this paper, we systematically study &quot;persona biases&quot;, which we define to be the
sensitivity of dialogue models&apos; harmful behaviors contingent upon the personas
they adopt. We categorize persona biases into biases in harmful expression and
harmful agreement, and establish a comprehensive evaluation framework to
measure persona biases in five aspects: Offensiveness, Toxic Continuation,
Regard, Stereotype Agreement, and Toxic Agreement. Additionally, we propose to
investigate persona biases by experimenting with UNIVERSALPERSONA, a
systematically constructed persona dataset encompassing various types of both
generic and specific model personas. Through benchmarking on four different
models -- including Blender, ChatGPT, Alpaca, and Vicuna -- our study uncovers
significant persona biases in dialogue systems. Our findings also underscore
the pressing need to revisit the use of personas in dialogue agents to ensure
safe application.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1&quot;&gt;Yixin Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jieyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1&quot;&gt;Aman Chadha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1&quot;&gt;Nanyun Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1&quot;&gt;Kai-Wei Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05619">
<title>Dynamic Top-k Estimation Consolidates Disagreement between Feature Attribution Methods. (arXiv:2310.05619v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05619</link>
<description rdf:parseType="Literal">&lt;p&gt;Feature attribution scores are used for explaining the prediction of a text
classifier to users by highlighting a k number of tokens. In this work, we
propose a way to determine the number of optimal k tokens that should be
displayed from sequential properties of the attribution scores. Our approach is
dynamic across sentences, method-agnostic, and deals with sentence length bias.
We compare agreement between multiple methods and humans on an NLI task, using
fixed k and dynamic k. We find that perturbation-based methods and Vanilla
Gradient exhibit highest agreement on most method--method and method--human
agreement metrics with a static k. Their advantage over other methods
disappears with dynamic ks which mainly improve Integrated Gradient and
GradientXInput. To our knowledge, this is the first evidence that sequential
properties of attribution scores are informative for consolidating attribution
signals for human interpretation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamp_J/0/1/0/all/0/1&quot;&gt;Jonathan Kamp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beinborn_L/0/1/0/all/0/1&quot;&gt;Lisa Beinborn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fokkens_A/0/1/0/all/0/1&quot;&gt;Antske Fokkens&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.09219">
<title>&quot;Kelly is a Warm Person, Joseph is a Role Model&quot;: Gender Biases in LLM-Generated Reference Letters. (arXiv:2310.09219v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.09219</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have recently emerged as an effective tool to
assist individuals in writing various types of content, including professional
documents such as recommendation letters. Though bringing convenience, this
application also introduces unprecedented fairness concerns. Model-generated
reference letters might be directly used by users in professional scenarios. If
underlying biases exist in these model-constructed letters, using them without
scrutinization could lead to direct societal harms, such as sabotaging
application success rates for female applicants. In light of this pressing
issue, it is imminent and necessary to comprehensively study fairness issues
and associated harms in this real-world use case. In this paper, we critically
examine gender biases in LLM-generated reference letters. Drawing inspiration
from social science findings, we design evaluation methods to manifest biases
through 2 dimensions: (1) biases in language style and (2) biases in lexical
content. We further investigate the extent of bias propagation by analyzing the
hallucination bias of models, a term that we define to be bias exacerbation in
model-hallucinated contents. Through benchmarking evaluation on 2 popular LLMs-
ChatGPT and Alpaca, we reveal significant gender biases in LLM-generated
recommendation letters. Our findings not only warn against using LLMs for this
application without scrutinization, but also illuminate the importance of
thoroughly studying hidden biases and harms in LLM-generated professional
documents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1&quot;&gt;Yixin Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pu_G/0/1/0/all/0/1&quot;&gt;George Pu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garimella_A/0/1/0/all/0/1&quot;&gt;Aparna Garimella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1&quot;&gt;Kai-Wei Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1&quot;&gt;Nanyun Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18144">
<title>Improving Intrinsic Exploration by Creating Stationary Objectives. (arXiv:2310.18144v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18144</link>
<description rdf:parseType="Literal">&lt;p&gt;Exploration bonuses in reinforcement learning guide long-horizon exploration
by defining custom intrinsic objectives. Count-based methods use the frequency
of state visits to derive an exploration bonus. In this paper, we identify that
any intrinsic reward function derived from count-based methods is
non-stationary and hence induces a difficult objective to optimize for the
agent. The key contribution of our work lies in transforming the original
non-stationary rewards into stationary rewards through an augmented state
representation. For this purpose, we introduce the Stationary Objectives For
Exploration (SOFE) framework. SOFE requires identifying sufficient statistics
for different exploration bonuses and finding an efficient encoding of these
statistics to use as input to a deep network. SOFE is based on proposing state
augmentations that expand the state space but hold the promise of simplifying
the optimization of the agent&apos;s objective. Our experiments show that SOFE
improves the agents&apos; performance in challenging exploration problems, including
sparse-reward tasks, pixel-based observations, 3D navigation, and procedurally
generated environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castanyer_R/0/1/0/all/0/1&quot;&gt;Roger Creus Castanyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romoff_J/0/1/0/all/0/1&quot;&gt;Joshua Romoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berseth_G/0/1/0/all/0/1&quot;&gt;Glen Berseth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19180">
<title>JEN-1 Composer: A Unified Framework for High-Fidelity Multi-Track Music Generation. (arXiv:2310.19180v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19180</link>
<description rdf:parseType="Literal">&lt;p&gt;With rapid advances in generative artificial intelligence, the text-to-music
synthesis task has emerged as a promising direction for music generation from
scratch. However, finer-grained control over multi-track generation remains an
open challenge. Existing models exhibit strong raw generation capability but
lack the flexibility to compose separate tracks and combine them in a
controllable manner, differing from typical workflows of human composers. To
address this issue, we propose JEN-1 Composer, a unified framework to
efficiently model marginal, conditional, and joint distributions over
multi-track music via a single model. JEN-1 Composer framework exhibits the
capacity to seamlessly incorporate any diffusion-based music generation system,
\textit{e.g.} Jen-1, enhancing its capacity for versatile multi-track music
generation. We introduce a curriculum training strategy aimed at incrementally
instructing the model in the transition from single-track generation to the
flexible generation of multi-track combinations. During the inference, users
have the ability to iteratively produce and choose music tracks that meet their
preferences, subsequently creating an entire musical composition incrementally
following the proposed Human-AI co-composition workflow. Quantitative and
qualitative assessments demonstrate state-of-the-art performance in
controllable and high-fidelity multi-track music synthesis. The proposed JEN-1
Composer represents a significant advance toward interactive AI-facilitated
music creation and composition. Demos will be available at
https://www.jenmusic.ai/audio-demos.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yao Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Peike Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Boyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1&quot;&gt;Alex Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.20033">
<title>Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization. (arXiv:2310.20033v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.20033</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) like the GPT and LLaMA families have
demonstrated exceptional capabilities in capturing and condensing critical
contextual information and achieving state-of-the-art performance in the
summarization task. However, community concerns about these models&apos;
hallucination issues continue to rise. LLMs sometimes generate factually
hallucinated summaries, which can be extremely harmful in the clinical domain
NLP tasks (e.g., clinical note summarization), where factually incorrect
statements can lead to critically erroneous diagnoses. Fine-tuning LLMs using
human feedback has shown the promise of aligning LLMs to be factually
consistent during generation, but such training procedure requires high-quality
human-annotated data, which can be extremely expensive to get in the clinical
domain. In this work, we propose a new pipeline using ChatGPT instead of human
experts to generate high-quality feedback data for improving factual
consistency in the clinical note summarization task. We focus specifically on
edit feedback because recent work discusses the shortcomings of human alignment
via preference feedback in complex situations (such as clinical NLP tasks that
require extensive expert knowledge), as well as some advantages of collecting
edit feedback from domain experts. In addition, although GPT has reached the
expert level in many clinical NLP tasks (e.g., USMLE QA), there is not much
previous work discussing whether GPT can generate expert-level edit feedback
for LMs in the clinical note summarization task. We hope to fill this gap.
Finally, our evaluations demonstrate the potential use of GPT edits in human
alignment, especially from a factuality perspective.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_P/0/1/0/all/0/1&quot;&gt;Prakamya Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1&quot;&gt;Zonghai Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shuwei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Beining Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mittal_R/0/1/0/all/0/1&quot;&gt;Rohan Mittal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hong Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.20381">
<title>A Comprehensive Study of GPT-4V&apos;s Multimodal Capabilities in Medical Imaging. (arXiv:2310.20381v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.20381</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a comprehensive evaluation of GPT-4V&apos;s capabilities
across diverse medical imaging tasks, including Radiology Report Generation,
Medical Visual Question Answering (VQA), and Visual Grounding. While prior
efforts have explored GPT-4V&apos;s performance in medical image anaylsis, to the
best of our knowledge, our study represents the first quantitative evaluation
on publicly available benchmarks. Our findings highlight GPT-4V&apos;s potential in
generating descriptive reports for chest X-ray images, particularly when guided
by well-structured prompts. Meanwhile, its performance on the MIMIC-CXR dataset
benchmark reveals areas for improvement in certain evaluation metrics, such as
CIDEr. In the domain of Medical VQA, GPT-4V demonstrates proficiency in
distinguishing between question types but falls short of the VQA-RAD benchmark
in terms of accuracy. Furthermore, our analysis finds the limitations of
conventional evaluation metrics like the BLEU score, advocating for the
development of more semantically robust assessment methods. In the field of
Visual Grounding, GPT-4V exhibits preliminary promise in recognizing bounding
boxes, but its precision is lacking, especially in identifying specific medical
organs and signs. Our evaluation underscores the significant potential of
GPT-4V in the medical imaging domain, while also emphasizing the need for
targeted refinements to fully unlock its capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yingshu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yunyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhanyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xinyu Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lingqiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1&quot;&gt;Leyang Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1&quot;&gt;Zhaopeng Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Longyue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Luping Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.20414">
<title>Meta Learning for Multi-View Visuomotor Systems. (arXiv:2310.20414v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2310.20414</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a new approach for quickly adapting a multi-view
visuomotor system for robots to varying camera configurations from the baseline
setup. It utilises meta-learning to fine-tune the perceptual network while
keeping the policy network fixed. Experimental results demonstrate a
significant reduction in the number of new training episodes needed to attain
baseline performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alwis_B/0/1/0/all/0/1&quot;&gt;Benji Alwis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.20550">
<title>CapsFusion: Rethinking Image-Text Data at Scale. (arXiv:2310.20550v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.20550</link>
<description rdf:parseType="Literal">&lt;p&gt;Large multimodal models demonstrate remarkable generalist ability to perform
diverse multimodal tasks in a zero-shot manner. Large-scale web-based
image-text pairs contribute fundamentally to this success, but suffer from
excessive noise. Recent studies use alternative captions synthesized by
captioning models and have achieved notable benchmark performance. However, our
experiments reveal significant Scalability Deficiency and World Knowledge Loss
issues in models trained with synthetic captions, which have been largely
obscured by their initial benchmark success. Upon closer examination, we
identify the root cause as the overly-simplified language structure and lack of
knowledge details in existing synthetic captions. To provide higher-quality and
more scalable multimodal pretraining data, we propose CapsFusion, an advanced
framework that leverages large language models to consolidate and refine
information from both web-based image-text pairs and synthetic captions.
Extensive experiments show that CapsFusion captions exhibit remarkable
all-round superiority over existing captions in terms of model performance
(e.g., 18.8 and 18.3 improvements in CIDEr score on COCO and NoCaps), sample
efficiency (requiring 11-16 times less computation than baselines), world
knowledge depth, and scalability. These effectiveness, efficiency and
scalability advantages position CapsFusion as a promising candidate for future
scaling of LMM training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1&quot;&gt;Qiying Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1&quot;&gt;Quan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaosong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1&quot;&gt;Yufeng Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yue Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinlong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jingjing Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00687">
<title>Improving Interpersonal Communication by Simulating Audiences with Language Models. (arXiv:2311.00687v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00687</link>
<description rdf:parseType="Literal">&lt;p&gt;How do we communicate with others to achieve our goals? We use our prior
experience or advice from others, or construct a candidate utterance by
predicting how it will be received. However, our experiences are limited and
biased, and reasoning about potential outcomes can be difficult and cognitively
challenging. In this paper, we explore how we can leverage Large Language Model
(LLM) simulations to help us communicate better. We propose the
Explore-Generate-Simulate (EGS) framework, which takes as input any scenario
where an individual is communicating to an audience with a goal they want to
achieve. EGS (1) explores the solution space by producing a diverse set of
advice relevant to the scenario, (2) generates communication candidates
conditioned on subsets of the advice, and (3) simulates the reactions from
various audiences to determine both the best candidate and advice to use. We
evaluate the framework on eight scenarios spanning the ten fundamental
processes of interpersonal communication. For each scenario, we collect a
dataset of human evaluations across candidates and baselines, and showcase that
our framework&apos;s chosen candidate is preferred over popular generation
mechanisms including Chain-of-Thought. We also find that audience simulations
achieve reasonably high agreement with human raters across 5 of the 8
scenarios. Finally, we demonstrate the generality of our framework by applying
it to real-world scenarios described by users on web forums. Through
evaluations and demonstrations, we show that EGS enhances the effectiveness and
outcomes of goal-oriented communication across a variety of situations, thus
opening up new possibilities for the application of large language models in
revolutionizing communication and decision-making processes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Ryan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yen_H/0/1/0/all/0/1&quot;&gt;Howard Yen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marjieh_R/0/1/0/all/0/1&quot;&gt;Raja Marjieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1&quot;&gt;Thomas L. Griffiths&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1&quot;&gt;Ranjay Krishna&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01057">
<title>Ultra-Efficient On-Device Object Detection on AI-Integrated Smart Glasses with TinyissimoYOLO. (arXiv:2311.01057v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01057</link>
<description rdf:parseType="Literal">&lt;p&gt;Smart glasses are rapidly gaining advanced functionality thanks to
cutting-edge computing technologies, accelerated hardware architectures, and
tiny AI algorithms. Integrating AI into smart glasses featuring a small form
factor and limited battery capacity is still challenging when targeting
full-day usage for a satisfactory user experience. This paper illustrates the
design and implementation of tiny machine-learning algorithms exploiting novel
low-power processors to enable prolonged continuous operation in smart glasses.
We explore the energy- and latency-efficient of smart glasses in the case of
real-time object detection. To this goal, we designed a smart glasses prototype
as a research platform featuring two microcontrollers, including a novel
milliwatt-power RISC-V parallel processor with a hardware accelerator for
visual AI, and a Bluetooth low-power module for communication. The smart
glasses integrate power cycling mechanisms, including image and audio sensing
interfaces. Furthermore, we developed a family of novel tiny deep-learning
models based on YOLO with sub-million parameters customized for
microcontroller-based inference dubbed TinyissimoYOLO v1.3, v5, and v8, aiming
at benchmarking object detection with smart glasses for energy and latency.
Evaluations on the prototype of the smart glasses demonstrate TinyissimoYOLO&apos;s
17ms inference latency and 1.59mJ energy consumption per inference while
ensuring acceptable detection accuracy. Further evaluation reveals an
end-to-end latency from image capturing to the algorithm&apos;s prediction of 56ms
or equivalently 18 fps, with a total power consumption of 62.9mW, equivalent to
a 9.3 hours of continuous run time on a 154mAh battery. These results
outperform MCUNet (TinyNAS+TinyEngine), which runs a simpler task (image
classification) at just 7.3 fps per second.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moosmann_J/0/1/0/all/0/1&quot;&gt;Julian Moosmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonazzi_P/0/1/0/all/0/1&quot;&gt;Pietro Bonazzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yawei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_S/0/1/0/all/0/1&quot;&gt;Sizhen Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mayer_P/0/1/0/all/0/1&quot;&gt;Philipp Mayer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1&quot;&gt;Luca Benini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magno_M/0/1/0/all/0/1&quot;&gt;Michele Magno&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01240">
<title>FacadeNet: Conditional Facade Synthesis via Selective Editing. (arXiv:2311.01240v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01240</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce FacadeNet, a deep learning approach for synthesizing building
facade images from diverse viewpoints. Our method employs a conditional GAN,
taking a single view of a facade along with the desired viewpoint information
and generates an image of the facade from the distinct viewpoint. To precisely
modify view-dependent elements like windows and doors while preserving the
structure of view-independent components such as walls, we introduce a
selective editing module. This module leverages image embeddings extracted from
a pre-trained vision transformer. Our experiments demonstrated state-of-the-art
performance on building facade generation, surpassing alternative methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Georgiou_Y/0/1/0/all/0/1&quot;&gt;Yiangos Georgiou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loizou_M/0/1/0/all/0/1&quot;&gt;Marios Loizou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kelly_T/0/1/0/all/0/1&quot;&gt;Tom Kelly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Averkiou_M/0/1/0/all/0/1&quot;&gt;Melinos Averkiou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17688">
<title>Managing AI Risks in an Era of Rapid Progress. (arXiv:2310.17688v1 [cs.CY] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2310.17688</link>
<description rdf:parseType="Literal">&lt;p&gt;In this short consensus paper, we outline risks from upcoming, advanced AI
systems. We examine large-scale social harms and malicious uses, as well as an
irreversible loss of human control over autonomous AI systems. In light of
rapid and continuing AI progress, we propose priorities for AI R&amp;amp;D and
governance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hinton_G/0/1/0/all/0/1&quot;&gt;Geoffrey Hinton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1&quot;&gt;Andrew Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1&quot;&gt;Dawn Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1&quot;&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harari_Y/0/1/0/all/0/1&quot;&gt;Yuval Noah Harari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Ya-Qin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1&quot;&gt;Lan Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shalev_Shwartz_S/0/1/0/all/0/1&quot;&gt;Shai Shalev-Shwartz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hadfield_G/0/1/0/all/0/1&quot;&gt;Gillian Hadfield&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clune_J/0/1/0/all/0/1&quot;&gt;Jeff Clune&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maharaj_T/0/1/0/all/0/1&quot;&gt;Tegan Maharaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1&quot;&gt;Frank Hutter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baydin_A/0/1/0/all/0/1&quot;&gt;At&amp;#x131;l&amp;#x131;m G&amp;#xfc;ne&amp;#x15f; Baydin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McIlraith_S/0/1/0/all/0/1&quot;&gt;Sheila McIlraith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1&quot;&gt;Qiqi Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1&quot;&gt;Ashwin Acharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krueger_D/0/1/0/all/0/1&quot;&gt;David Krueger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1&quot;&gt;Anca Dragan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1&quot;&gt;Philip Torr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russell_S/0/1/0/all/0/1&quot;&gt;Stuart Russell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kahneman_D/0/1/0/all/0/1&quot;&gt;Daniel Kahneman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brauner_J/0/1/0/all/0/1&quot;&gt;Jan Brauner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mindermann_S/0/1/0/all/0/1&quot;&gt;S&amp;#xf6;ren Mindermann&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>