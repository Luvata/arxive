<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2024-01-24T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12983" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12986" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12988" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12996" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12998" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12999" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13001" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13002" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13006" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13034" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13049" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13060" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13068" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13081" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13085" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13086" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13097" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13098" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13099" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13110" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13112" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13136" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13138" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13157" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13171" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13178" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13192" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13193" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13201" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13205" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13212" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13214" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13219" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13223" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13227" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13229" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13256" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13262" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13270" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13275" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13282" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13298" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13311" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13315" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13324" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13334" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13335" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13346" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13408" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13432" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13444" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13460" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13462" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13481" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13486" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13498" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13555" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13586" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13588" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13594" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13604" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13611" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13613" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13641" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13652" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13657" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13662" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.03203" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.07467" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.13883" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.06009" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.14359" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.10227" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.14648" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.03468" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15936" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08013" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00527" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02637" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06082" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.12547" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.16706" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12539" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.15812" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.08565" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14808" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.17249" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08535" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13361" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13541" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06717" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08917" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09442" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10369" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00496" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02981" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05535" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05975" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06373" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06461" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.07448" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08396" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08517" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08534" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10841" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11120" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11648" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11792" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11798" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12435" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2401.12983">
<title>Assessing Large Language Models in Mechanical Engineering Education: A Study on Mechanics-Focused Conceptual Understanding. (arXiv:2401.12983v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.12983</link>
<description rdf:parseType="Literal">&lt;p&gt;This study is a pioneering endeavor to investigate the capabilities of Large
Language Models (LLMs) in addressing conceptual questions within the domain of
mechanical engineering with a focus on mechanics. Our examination involves a
manually crafted exam encompassing 126 multiple-choice questions, spanning
various aspects of mechanics courses, including Fluid Mechanics, Mechanical
Vibration, Engineering Statics and Dynamics, Mechanics of Materials, Theory of
Elasticity, and Continuum Mechanics. Three LLMs, including ChatGPT (GPT-3.5),
ChatGPT (GPT-4), and Claude (Claude-2.1), were subjected to evaluation against
engineering faculties and students with or without mechanical engineering
background. The findings reveal GPT-4&apos;s superior performance over the other two
LLMs and human cohorts in answering questions across various mechanics topics,
except for Continuum Mechanics. This signals the potential future improvements
for GPT models in handling symbolic calculations and tensor analyses. The
performances of LLMs were all significantly improved with explanations prompted
prior to direct responses, underscoring the crucial role of prompt engineering.
Interestingly, GPT-3.5 demonstrates improved performance with prompts covering
a broader domain, while GPT-4 excels with prompts focusing on specific
subjects. Finally, GPT-4 exhibits notable advancements in mitigating input
bias, as evidenced by guessing preferences for humans. This study unveils the
substantial potential of LLMs as highly knowledgeable assistants in both
mechanical pedagogy and scientific research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1&quot;&gt;Jie Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1&quot;&gt;Jixin Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zihao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_P/0/1/0/all/0/1&quot;&gt;Peng Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhengliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1&quot;&gt;Yujie Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_B/0/1/0/all/0/1&quot;&gt;Beikang Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Filla_N/0/1/0/all/0/1&quot;&gt;Nicholas Filla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yiwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Ning Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xianyan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1&quot;&gt;Keke Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tianming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xianqiao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12986">
<title>Crowdsourced Adaptive Surveys. (arXiv:2401.12986v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.12986</link>
<description rdf:parseType="Literal">&lt;p&gt;Public opinion surveys are vital for informing democratic decision-making,
but responding to rapidly changing information environments and measuring
beliefs within niche communities can be challenging for traditional survey
methods. This paper introduces a crowdsourced adaptive survey methodology
(CSAS) that unites advances in natural language processing and adaptive
algorithms to generate question banks that evolve with user input. The CSAS
method converts open-ended text provided by participants into Likert-style
items and applies a multi-armed bandit algorithm to determine user-provided
questions that should be prioritized in the survey. The method&apos;s adaptive
nature allows for the exploration of new survey questions, while imposing
minimal costs in survey length. Applications in the domains of Latino
information environments and issue importance showcase CSAS&apos;s ability to
identify claims or issues that might otherwise be difficult to track using
standard approaches. I conclude by discussing the method&apos;s potential for
studying topics where participant-generated content might improve our
understanding of public opinion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Velez_Y/0/1/0/all/0/1&quot;&gt;Yamil Velez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12988">
<title>Few-Shot Learning for Chronic Disease Management: Leveraging Large Language Models and Multi-Prompt Engineering with Medical Knowledge Injection. (arXiv:2401.12988v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.12988</link>
<description rdf:parseType="Literal">&lt;p&gt;This study harnesses state-of-the-art AI technology for chronic disease
management, specifically in detecting various mental disorders through
user-generated textual content. Existing studies typically rely on fully
supervised machine learning, which presents challenges such as the
labor-intensive manual process of annotating extensive training data for each
disease and the need to design specialized deep learning architectures for each
problem. To address such challenges, we propose a novel framework that
leverages advanced AI techniques, including large language models and
multi-prompt engineering. Specifically, we address two key technical challenges
in data-driven chronic disease management: (1) developing personalized prompts
to represent each user&apos;s uniqueness and (2) incorporating medical knowledge
into prompts to provide context for chronic disease detection, instruct
learning objectives, and operationalize prediction goals. We evaluate our
method using four mental disorders, which are prevalent chronic diseases
worldwide, as research cases. On the depression detection task, our method (F1
= 0.975~0.978) significantly outperforms traditional supervised learning
paradigms, including feature engineering (F1 = 0.760) and architecture
engineering (F1 = 0.756). Meanwhile, our approach demonstrates success in
few-shot learning, i.e., requiring only a minimal number of training examples
to detect chronic diseases based on user-generated textual content (i.e., only
2, 10, or 100 subjects). Moreover, our method can be generalized to other
mental disorder detection tasks, including anorexia, pathological gambling, and
self-harm (F1 = 0.919~0.978).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haoxin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenli Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1&quot;&gt;Jiaheng Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1&quot;&gt;Buomsoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1&quot;&gt;Yidong Chai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12996">
<title>A Comparison of Veterans with Problematic Opioid Use Identified through Natural Language Processing of Clinical Notes versus Using Diagnostic Codes. (arXiv:2401.12996v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.12996</link>
<description rdf:parseType="Literal">&lt;p&gt;Background: Electronic health records (EHRs) are a data source for opioid
research. Opioid use disorder is known to be under-coded as a diagnosis, yet
problematic opioid use can be documented in clinical notes.
&lt;/p&gt;
&lt;p&gt;Objectives: Our goals were 1) to identify problematic opioid use from a full
range of clinical notes; and 2) to compare the characteristics of patients
identified as having problematic opioid use, exclusively documented in clinical
notes, to those having documented ICD opioid use disorder diagnostic codes.
&lt;/p&gt;
&lt;p&gt;Materials and Methods: We developed and applied a natural language processing
(NLP) tool to the clinical notes of a patient cohort (n=222,371) from two
Veteran Affairs service regions to identify patients with problematic opioid
use. We also used a set of ICD diagnostic codes to identify patients with
opioid use disorder from the same cohort. We compared the demographic and
clinical characteristics of patients identified only through NLP, to those of
patients identified through ICD codes.
&lt;/p&gt;
&lt;p&gt;Results: NLP exclusively identified 57,331 patients; 6,997 patients had
positive ICD code identifications. Patients exclusively identified through NLP
were more likely to be women. Those identified through ICD codes were more
likely to be male, younger, have concurrent benzodiazepine prescriptions, more
comorbidities, more care encounters, and less likely to be married. Patients in
the NLP and ICD groups had substantially elevated comorbidity levels compared
to patients not documented as experiencing problematic opioid use.
&lt;/p&gt;
&lt;p&gt;Conclusions: NLP is a feasible approach for identifying problematic opioid
use not otherwise recorded by ICD codes. Clinicians may be reluctant to code
for opioid use disorder. It is therefore incumbent on the healthcare team to
search for documentation of opioid concerns within clinical notes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Workman_T/0/1/0/all/0/1&quot;&gt;Terri Elizabeth Workman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kupersmith_J/0/1/0/all/0/1&quot;&gt;Joel Kupersmith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1&quot;&gt;Phillip Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spevak_C/0/1/0/all/0/1&quot;&gt;Christopher Spevak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sandbrink_F/0/1/0/all/0/1&quot;&gt;Friedhelm Sandbrink&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Treitler_Y/0/1/0/all/0/1&quot;&gt;Yan Cheng Qing Zeng-Treitler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12998">
<title>Evaluating and Enhancing Large Language Models Performance in Domain-specific Medicine: Osteoarthritis Management with DocOA. (arXiv:2401.12998v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.12998</link>
<description rdf:parseType="Literal">&lt;p&gt;The efficacy of large language models (LLMs) in domain-specific medicine,
particularly for managing complex diseases such as osteoarthritis (OA), remains
largely unexplored. This study focused on evaluating and enhancing the clinical
capabilities of LLMs in specific domains, using osteoarthritis (OA) management
as a case study. A domain specific benchmark framework was developed, which
evaluate LLMs across a spectrum from domain-specific knowledge to clinical
applications in real-world clinical scenarios. DocOA, a specialized LLM
tailored for OA management that integrates retrieval-augmented generation (RAG)
and instruction prompts, was developed. The study compared the performance of
GPT-3.5, GPT-4, and a specialized assistant, DocOA, using objective and human
evaluations. Results showed that general LLMs like GPT-3.5 and GPT-4 were less
effective in the specialized domain of OA management, particularly in providing
personalized treatment recommendations. However, DocOA showed significant
improvements. This study introduces a novel benchmark framework which assesses
the domain-specific abilities of LLMs in multiple aspects, highlights the
limitations of generalized LLMs in clinical contexts, and demonstrates the
potential of tailored approaches for developing domain-specific medical LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_M/0/1/0/all/0/1&quot;&gt;MingKe You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Li Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;WeiZhi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yu Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jie Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shaoting Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Gang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jian Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12999">
<title>Quantum-Inspired Machine Learning for Molecular Docking. (arXiv:2401.12999v1 [physics.chem-ph])</title>
<link>http://arxiv.org/abs/2401.12999</link>
<description rdf:parseType="Literal">&lt;p&gt;Molecular docking is an important tool for structure-based drug design,
accelerating the efficiency of drug development. Complex and dynamic binding
processes between proteins and small molecules require searching and sampling
over a wide spatial range. Traditional docking by searching for possible
binding sites and conformations is computationally complex and results poorly
under blind docking. Quantum-inspired algorithms combining quantum properties
and annealing show great advantages in solving combinatorial optimization
problems. Inspired by this, we achieve an improved in blind docking by using
quantum-inspired combined with gradients learned by deep learning in the
encoded molecular space. Numerical simulation shows that our method outperforms
traditional docking algorithms and deep learning-based algorithms over 10\%.
Compared to the current state-of-the-art deep learning-based docking algorithm
DiffDock, the success rate of Top-1 (RMSD&amp;lt;2) achieves an improvement from 33\%
to 35\% in our same setup. In particular, a 6\% improvement is realized in the
high-precision region(RMSD&amp;lt;1) on molecules data unseen in DiffDock, which
demonstrates the well-generalized of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Shu_R/0/1/0/all/0/1&quot;&gt;Runqiu Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bowen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Xiong_Z/0/1/0/all/0/1&quot;&gt;Zhaoping Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Cui_X/0/1/0/all/0/1&quot;&gt;Xiaopeng Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunting Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Cui_W/0/1/0/all/0/1&quot;&gt;Wei Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Yung_M/0/1/0/all/0/1&quot;&gt;Man-Hong Yung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Qiao_N/0/1/0/all/0/1&quot;&gt;Nan Qiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13001">
<title>PatternPortrait: Draw Me Like One of Your Scribbles. (arXiv:2401.13001v1 [cs.GR])</title>
<link>http://arxiv.org/abs/2401.13001</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a process for generating abstract portrait drawings
from pictures. Their unique style is created by utilizing single freehand
pattern sketches as references to generate unique patterns for shading. The
method involves extracting facial and body features from images and
transforming them into vector lines. A key aspect of the research is the
development of a graph neural network architecture designed to learn sketch
stroke representations in vector form, enabling the generation of diverse
stroke variations. The combination of these two approaches creates joyful
abstract drawings that are realized via a pen plotter. The presented process
garnered positive feedback from an audience of approximately 280 participants.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wieluch_S/0/1/0/all/0/1&quot;&gt;Sabine Wieluch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwenker_F/0/1/0/all/0/1&quot;&gt;Friedhelm Schwenker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13002">
<title>Theorem Discovery Amongst Cyclic Polygons. (arXiv:2401.13002v1 [cs.CG])</title>
<link>http://arxiv.org/abs/2401.13002</link>
<description rdf:parseType="Literal">&lt;p&gt;We examine a class of geometric theorems on cyclic 2n-gons. We prove that if
we take n disjoint pairs of sides, each pair separated by an even number of
polygon sides, then there is a linear combination of the angles between those
sides which is constant. We present a formula for the linear combination, which
provides a theorem statement in terms of those angles. We describe a program
which uses this result to generate new geometry proof problems and their
solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Todd_P/0/1/0/all/0/1&quot;&gt;Philip Todd&lt;/a&gt; (Saltire Software)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13006">
<title>CIMGEN: Controlled Image Manipulation by Finetuning Pretrained Generative Models on Limited Data. (arXiv:2401.13006v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.13006</link>
<description rdf:parseType="Literal">&lt;p&gt;Content creation and image editing can benefit from flexible user controls. A
common intermediate representation for conditional image generation is a
semantic map, that has information of objects present in the image. When
compared to raw RGB pixels, the modification of semantic map is much easier.
One can take a semantic map and easily modify the map to selectively insert,
remove, or replace objects in the map. The method proposed in this paper takes
in the modified semantic map and alter the original image in accordance to the
modified map. The method leverages traditional pre-trained image-to-image
translation GANs, such as CycleGAN or Pix2Pix GAN, that are fine-tuned on a
limited dataset of reference images associated with the semantic maps. We
discuss the qualitative and quantitative performance of our technique to
illustrate its capacity and possible applications in the fields of image
forgery and image editing. We also demonstrate the effectiveness of the
proposed image forgery technique in thwarting the numerous deep learning-based
image forensic techniques, highlighting the urgent need to develop robust and
generalizable image forensic tools in the fight against the spread of fake
media.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gudavalli_C/0/1/0/all/0/1&quot;&gt;Chandrakanth Gudavalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosten_E/0/1/0/all/0/1&quot;&gt;Erik Rosten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nataraj_L/0/1/0/all/0/1&quot;&gt;Lakshmanan Nataraj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandrasekaran_S/0/1/0/all/0/1&quot;&gt;Shivkumar Chandrasekaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manjunath_B/0/1/0/all/0/1&quot;&gt;B. S. Manjunath&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13034">
<title>Locality Sensitive Sparse Encoding for Learning World Models Online. (arXiv:2401.13034v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.13034</link>
<description rdf:parseType="Literal">&lt;p&gt;Acquiring an accurate world model online for model-based reinforcement
learning (MBRL) is challenging due to data nonstationarity, which typically
causes catastrophic forgetting for neural networks (NNs). From the online
learning perspective, a Follow-The-Leader (FTL) world model is desirable, which
optimally fits all previous experiences at each round. Unfortunately, NN-based
models need re-training on all accumulated data at every interaction step to
achieve FTL, which is computationally expensive for lifelong agents. In this
paper, we revisit models that can achieve FTL with incremental updates.
Specifically, our world model is a linear regression model supported by
nonlinear random features. The linear part ensures efficient FTL update while
the nonlinear random feature empowers the fitting of complex environments. To
best trade off model capacity and computation efficiency, we introduce a
locality sensitive sparse encoding, which allows us to conduct efficient sparse
updates even with very high dimensional nonlinear features. We validate the
representation power of our encoding and verify that it allows efficient online
learning under data covariate shift. We also show, in the Dyna MBRL setting,
that our world models learned online using a single pass of trajectory data
either surpass or match the performance of deep world models trained with
replay and other continual learning methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zichen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1&quot;&gt;Chao Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1&quot;&gt;Wee Sun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1&quot;&gt;Min Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13049">
<title>CIS-UNet: Multi-Class Segmentation of the Aorta in Computed Tomography Angiography via Context-Aware Shifted Window Self-Attention. (arXiv:2401.13049v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.13049</link>
<description rdf:parseType="Literal">&lt;p&gt;Advancements in medical imaging and endovascular grafting have facilitated
minimally invasive treatments for aortic diseases. Accurate 3D segmentation of
the aorta and its branches is crucial for interventions, as inaccurate
segmentation can lead to erroneous surgical planning and endograft
construction. Previous methods simplified aortic segmentation as a binary image
segmentation problem, overlooking the necessity of distinguishing between
individual aortic branches. In this paper, we introduce Context Infused
Swin-UNet (CIS-UNet), a deep learning model designed for multi-class
segmentation of the aorta and thirteen aortic branches. Combining the strengths
of Convolutional Neural Networks (CNNs) and Swin transformers, CIS-UNet adopts
a hierarchical encoder-decoder structure comprising a CNN encoder, symmetric
decoder, skip connections, and a novel Context-aware Shifted Window
Self-Attention (CSW-SA) as the bottleneck block. Notably, CSW-SA introduces a
unique utilization of the patch merging layer, distinct from conventional Swin
transformers. It efficiently condenses the feature map, providing a global
spatial context and enhancing performance when applied at the bottleneck layer,
offering superior computational efficiency and segmentation accuracy compared
to the Swin transformers. We trained our model on computed tomography (CT)
scans from 44 patients and tested it on 15 patients. CIS-UNet outperformed the
state-of-the-art SwinUNetR segmentation model, which is solely based on Swin
transformers, by achieving a superior mean Dice coefficient of 0.713 compared
to 0.697, and a mean surface distance of 2.78 mm compared to 3.39 mm.
CIS-UNet&apos;s superior 3D aortic segmentation offers improved precision and
optimization for planning endovascular treatments. Our dataset and code will be
publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Imran_M/0/1/0/all/0/1&quot;&gt;Muhammad Imran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Krebs_J/0/1/0/all/0/1&quot;&gt;Jonathan R Krebs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gopu_V/0/1/0/all/0/1&quot;&gt;Veera Rajasekhar Reddy Gopu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fazzone_B/0/1/0/all/0/1&quot;&gt;Brian Fazzone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sivaraman_V/0/1/0/all/0/1&quot;&gt;Vishal Balaji Sivaraman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Amarjeet Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Viscardi_C/0/1/0/all/0/1&quot;&gt;Chelsea Viscardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Heithaus_R/0/1/0/all/0/1&quot;&gt;Robert Evans Heithaus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shickel_B/0/1/0/all/0/1&quot;&gt;Benjamin Shickel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuyin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cooper_M/0/1/0/all/0/1&quot;&gt;Michol A Cooper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shao_W/0/1/0/all/0/1&quot;&gt;Wei Shao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13060">
<title>TCE at Qur&apos;an QA 2023 Shared Task: Low Resource Enhanced Transformer-based Ensemble Approach for Qur&apos;anic QA. (arXiv:2401.13060v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.13060</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present our approach to tackle Qur&apos;an QA 2023 shared tasks
A and B. To address the challenge of low-resourced training data, we rely on
transfer learning together with a voting ensemble to improve prediction
stability across multiple runs. Additionally, we employ different architectures
and learning mechanisms for a range of Arabic pre-trained transformer-based
models for both tasks. To identify unanswerable questions, we propose using a
thresholding mechanism. Our top-performing systems greatly surpass the baseline
performance on the hidden split, achieving a MAP score of 25.05% for task A and
a partial Average Precision (pAP) of 57.11% for task B.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elkomy_M/0/1/0/all/0/1&quot;&gt;Mohammed Alaa Elkomy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarhan_A/0/1/0/all/0/1&quot;&gt;Amany Sarhan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13068">
<title>Local Background Estimation for Improved Gas Plume Identification in Hyperspectral Images. (arXiv:2401.13068v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13068</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning identification models have shown promise for identifying gas
plumes in Longwave IR hyperspectral images of urban scenes, particularly when a
large library of gases are being considered. Because many gases have similar
spectral signatures, it is important to properly estimate the signal from a
detected plume. Typically, a scene&apos;s global mean spectrum and covariance matrix
are estimated to whiten the plume&apos;s signal, which removes the background&apos;s
signature from the gas signature. However, urban scenes can have many different
background materials that are spatially and spectrally heterogeneous. This can
lead to poor identification performance when the global background estimate is
not representative of a given local background material. We use image
segmentation, along with an iterative background estimation algorithm, to
create local estimates for the various background materials that reside
underneath a gas plume. Our method outperforms global background estimation on
a set of simulated and real gas plumes. This method shows promise in increasing
deep learning identification confidence, while being simple and easy to tune
when considering diverse plumes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jarman_S/0/1/0/all/0/1&quot;&gt;Scout Jarman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hampel_Arias_Z/0/1/0/all/0/1&quot;&gt;Zigfried Hampel-Arias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carr_A/0/1/0/all/0/1&quot;&gt;Adra Carr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_K/0/1/0/all/0/1&quot;&gt;Kevin R. Moon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13081">
<title>Free Form Medical Visual Question Answering in Radiology. (arXiv:2401.13081v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13081</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual Question Answering (VQA) in the medical domain presents a unique,
interdisciplinary challenge, combining fields such as Computer Vision, Natural
Language Processing, and Knowledge Representation. Despite its importance,
research in medical VQA has been scant, only gaining momentum since 2018.
Addressing this gap, our research delves into the effective representation of
radiology images and the joint learning of multimodal representations,
surpassing existing methods. We innovatively augment the SLAKE dataset,
enabling our model to respond to a more diverse array of questions, not limited
to the immediate content of radiology or pathology images. Our model achieves a
top-1 accuracy of 79.55\% with a less complex architecture, demonstrating
comparable performance to current state-of-the-art models. This research not
only advances medical VQA but also opens avenues for practical applications in
diagnostic settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narayanan_A/0/1/0/all/0/1&quot;&gt;Abhishek Narayanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Musthyala_R/0/1/0/all/0/1&quot;&gt;Rushabh Musthyala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sankar_R/0/1/0/all/0/1&quot;&gt;Rahul Sankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nistala_A/0/1/0/all/0/1&quot;&gt;Anirudh Prasad Nistala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1&quot;&gt;Pranav Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cirrone_J/0/1/0/all/0/1&quot;&gt;Jacopo Cirrone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13085">
<title>IndiText Boost: Text Augmentation for Low Resource India Languages. (arXiv:2401.13085v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.13085</link>
<description rdf:parseType="Literal">&lt;p&gt;Text Augmentation is an important task for low-resource languages. It helps
deal with the problem of data scarcity. A data augmentation strategy is used to
deal with the problem of data scarcity. Through the years, much work has been
done on data augmentation for the English language. In contrast, very less work
has been done on Indian languages. This is contrary to the fact that data
augmentation is used to deal with data scarcity. In this work, we focus on
implementing techniques like Easy Data Augmentation, Back Translation,
Paraphrasing, Text Generation using LLMs, and Text Expansion using LLMs for
text classification on different languages. We focus on 6 Indian languages
namely: Sindhi, Marathi, Hindi, Gujarati, Telugu, and Sanskrit. According to
our knowledge, no such work exists for text augmentation on Indian languages.
We carry out binary as well as multi-class text classification to make our
results more comparable. We get surprising results as basic data augmentation
techniques surpass LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Litake_O/0/1/0/all/0/1&quot;&gt;Onkar Litake&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yagnik_N/0/1/0/all/0/1&quot;&gt;Niraj Yagnik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Labhsetwar_S/0/1/0/all/0/1&quot;&gt;Shreyas Labhsetwar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13086">
<title>Towards Trustable Language Models: Investigating Information Quality of Large Language Models. (arXiv:2401.13086v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.13086</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLM) are generating information at a rapid pace,
requiring users to increasingly rely and trust the data. Despite remarkable
advances of LLM, Information generated by LLM is not completely trustworthy,
due to challenges in information quality. Specifically, integrity of
Information quality decreases due to unreliable, biased, tokenization during
pre-training of LLM. Moreover, due to decreased information quality issues, has
led towards hallucination, fabricated information. Unreliable information can
lead towards flawed decisions in businesses, which impacts economic activity.
In this work, we introduce novel mathematical information quality evaluation of
LLM, we furthermore analyze and highlight information quality challenges,
scaling laws to systematically scale language models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rejeleene_R/0/1/0/all/0/1&quot;&gt;Rick Rejeleene&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaowei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talburt_J/0/1/0/all/0/1&quot;&gt;John Talburt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13097">
<title>Digital Divides in Scene Recognition: Uncovering Socioeconomic Biases in Deep Learning Systems. (arXiv:2401.13097v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13097</link>
<description rdf:parseType="Literal">&lt;p&gt;Computer-based scene understanding has influenced fields ranging from urban
planning to autonomous vehicle performance, yet little is known about how well
these technologies work across social differences. We investigate the biases of
deep convolutional neural networks (dCNNs) in scene classification, using
nearly one million images from global and US sources, including user-submitted
home photographs and Airbnb listings. We applied statistical models to quantify
the impact of socioeconomic indicators such as family income, Human Development
Index (HDI), and demographic factors from public data sources (CIA and US
Census) on dCNN performance. Our analyses revealed significant socioeconomic
bias, where pretrained dCNNs demonstrated lower classification accuracy, lower
classification confidence, and a higher tendency to assign labels that could be
offensive when applied to homes (e.g., &quot;ruin&quot;, &quot;slum&quot;), especially in images
from homes with lower socioeconomic status (SES). This trend is consistent
across two datasets of international images and within the diverse economic and
racial landscapes of the United States. This research contributes to
understanding biases in computer vision, emphasizing the need for more
inclusive and representative training datasets. By mitigating the bias in the
computer vision pipelines, we can ensure fairer and more equitable outcomes for
applied computer vision, including home valuation and smart home security
systems. There is urgency in addressing these biases, which can significantly
impact critical decisions in urban development and resource allocation. Our
findings also motivate the development of AI systems that better understand and
serve diverse communities, moving towards technology that equitably benefits
all sectors of society.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Greene_M/0/1/0/all/0/1&quot;&gt;Michelle R. Greene&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Josyula_M/0/1/0/all/0/1&quot;&gt;Mariam Josyula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Si_W/0/1/0/all/0/1&quot;&gt;Wentao Si&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hart_J/0/1/0/all/0/1&quot;&gt;Jennifer A. Hart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13098">
<title>Gravity-Informed Deep Learning Framework for Predicting Ship Traffic Flow and Invasion Risk of Non-Indigenous Species via Ballast Water Discharge. (arXiv:2401.13098v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.13098</link>
<description rdf:parseType="Literal">&lt;p&gt;Invasive species in water bodies pose a major threat to the environment and
biodiversity globally. Due to increased transportation and trade, non-native
species have been introduced to new environments, causing damage to ecosystems
and leading to economic losses in agriculture, forestry, and fisheries.
Therefore, there is a pressing need for risk assessment and management
techniques to mitigate the impact of these invasions. This study aims to
develop a new physics-inspired model to forecast maritime shipping traffic and
thus inform risk assessment of invasive species spread through global
transportation networks. Inspired by the gravity model for international
trades, our model considers various factors that influence the likelihood and
impact of vessel activities, such as shipping flux density, distance between
ports, trade flow, and centrality measures of transportation hubs.
Additionally, by analyzing the risk network of invasive species, we provide a
comprehensive framework for assessing the invasion threat level given a pair of
origin and destination. Accordingly, this paper introduces transformers to
gravity models to rebuild the short- and long-term dependencies that make the
risk analysis feasible. Thus, we introduce a physics-inspired framework that
achieves an 89% segmentation accuracy for existing and non-existing
trajectories and an 84.8% accuracy for the number of vessels flowing between
key port areas, representing more than 10% improvement over the traditional
deep-gravity model. Along these lines, this research contributes to a better
understanding of invasive species risk assessment. It allows policymakers,
conservationists, and stakeholders to prioritize management actions by
identifying high-risk invasion pathways. Besides, our model is versatile and
can include new data sources, making it suitable for assessing species invasion
risks in a changing global landscape.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1&quot;&gt;Ruixin Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spadon_G/0/1/0/all/0/1&quot;&gt;Gabriel Spadon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bailey_S/0/1/0/all/0/1&quot;&gt;Sarah Bailey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pelot_R/0/1/0/all/0/1&quot;&gt;Ronald Pelot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matwin_S/0/1/0/all/0/1&quot;&gt;Stan Matwin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soares_A/0/1/0/all/0/1&quot;&gt;Amilcar Soares&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13099">
<title>Sparse identification of nonlinear dynamics in the presence of library and system uncertainty. (arXiv:2401.13099v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.13099</link>
<description rdf:parseType="Literal">&lt;p&gt;The SINDy algorithm has been successfully used to identify the governing
equations of dynamical systems from time series data. However, SINDy assumes
the user has prior knowledge of the variables in the system and of a function
library that can act as a basis for the system. In this paper, we demonstrate
on real world data how the Augmented SINDy algorithm outperforms SINDy in the
presence of system variable uncertainty. We then show SINDy can be further
augmented to perform robustly when both kinds of uncertainty are present.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OBrien_A/0/1/0/all/0/1&quot;&gt;Andrew O&amp;#x27;Brien&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13110">
<title>XAI for All: Can Large Language Models Simplify Explainable AI?. (arXiv:2401.13110v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.13110</link>
<description rdf:parseType="Literal">&lt;p&gt;The field of Explainable Artificial Intelligence (XAI) often focuses on users
with a strong technical background, making it challenging for non-experts to
understand XAI methods. This paper presents &quot;x-[plAIn]&quot;, a new approach to make
XAI more accessible to a wider audience through a custom Large Language Model
(LLM), developed using ChatGPT Builder. Our goal was to design a model that can
generate clear, concise summaries of various XAI methods, tailored for
different audiences, including business professionals and academics. The key
feature of our model is its ability to adapt explanations to match each
audience group&apos;s knowledge level and interests. Our approach still offers
timely insights, facilitating the decision-making process by the end users.
Results from our use-case studies show that our model is effective in providing
easy-to-understand, audience-specific explanations, regardless of the XAI
method used. This adaptability improves the accessibility of XAI, bridging the
gap between complex AI technologies and their practical applications. Our
findings indicate a promising direction for LLMs in making advanced AI concepts
more accessible to a diverse range of users.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mavrepis_P/0/1/0/all/0/1&quot;&gt;Philip Mavrepis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Makridis_G/0/1/0/all/0/1&quot;&gt;Georgios Makridis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fatouros_G/0/1/0/all/0/1&quot;&gt;Georgios Fatouros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koukos_V/0/1/0/all/0/1&quot;&gt;Vasileios Koukos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Separdani_M/0/1/0/all/0/1&quot;&gt;Maria Margarita Separdani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kyriazis_D/0/1/0/all/0/1&quot;&gt;Dimosthenis Kyriazis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13112">
<title>DISCOUNT: Distributional Counterfactual Explanation With Optimal Transport. (arXiv:2401.13112v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.13112</link>
<description rdf:parseType="Literal">&lt;p&gt;Counterfactual Explanations (CE) is the de facto method for providing insight
and interpretability in black-box decision-making models by identifying
alternative input instances that lead to different outcomes. This paper extends
the concept of CEs to a distributional context, broadening the scope from
individual data points to entire input and output distributions, named
Distributional Counterfactual Explanation (DCE). In DCE, our focus shifts to
analyzing the distributional properties of the factual and counterfactual,
drawing parallels to the classical approach of assessing individual instances
and their resulting decisions. We leverage Optimal Transport (OT) to frame a
chance-constrained optimization problem, aiming to derive a counterfactual
distribution that closely aligns with its factual counterpart, substantiated by
statistical confidence. Our proposed optimization method, DISCOUNT,
strategically balances this confidence across both input and output
distributions. This algorithm is accompanied by an analysis of its convergence
rate. The efficacy of our proposed method is substantiated through a series of
illustrative case studies, highlighting its potential in providing deep
insights into decision-making models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_L/0/1/0/all/0/1&quot;&gt;Lei You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1&quot;&gt;Lele Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nilsson_M/0/1/0/all/0/1&quot;&gt;Mattias Nilsson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13136">
<title>The Language Barrier: Dissecting Safety Challenges of LLMs in Multilingual Contexts. (arXiv:2401.13136v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.13136</link>
<description rdf:parseType="Literal">&lt;p&gt;As the influence of large language models (LLMs) spans across global
communities, their safety challenges in multilingual settings become paramount
for alignment research. This paper examines the variations in safety challenges
faced by LLMs across different languages and discusses approaches to
alleviating such concerns. By comparing how state-of-the-art LLMs respond to
the same set of malicious prompts written in higher- vs. lower-resource
languages, we observe that (1) LLMs tend to generate unsafe responses much more
often when a malicious prompt is written in a lower-resource language, and (2)
LLMs tend to generate more irrelevant responses to malicious prompts in
lower-resource languages. To understand where the discrepancy can be
attributed, we study the effect of instruction tuning with reinforcement
learning from human feedback (RLHF) or supervised finetuning (SFT) on the
HH-RLHF dataset. Surprisingly, while training with high-resource languages
improves model alignment, training in lower-resource languages yields minimal
improvement. This suggests that the bottleneck of cross-lingual alignment is
rooted in the pretraining stage. Our findings highlight the challenges in
cross-lingual LLM safety, and we hope they inform future research in this
direction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Lingfeng Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1&quot;&gt;Weiting Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Sihao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yunmo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jingyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Haoran Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1&quot;&gt;Boyuan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1&quot;&gt;Philipp Koehn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1&quot;&gt;Daniel Khashabi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13138">
<title>Visibility into AI Agents. (arXiv:2401.13138v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2401.13138</link>
<description rdf:parseType="Literal">&lt;p&gt;Increased delegation of commercial, scientific, governmental, and personal
activities to AI agents -- systems capable of pursuing complex goals with
limited supervision -- may exacerbate existing societal risks and introduce new
risks. Understanding and mitigating these risks involves critically evaluating
existing governance structures, revising and adapting these structures where
needed, and ensuring accountability of key stakeholders. Information about
where, why, how, and by whom certain AI agents are used, which we refer to as
\textbf{visibility}, is critical to these objectives. In this paper, we assess
three categories of measures to increase visibility into AI agents:
\textbf{agent identifiers}, \textbf{real-time monitoring}, and \textbf{activity
logging}. For each, we outline potential implementations that vary in
intrusiveness and informativeness. We analyze how the measures apply across a
spectrum of centralized through decentralized deployment contexts, accounting
for various actors in the supply chain including hardware and software service
providers. Finally, we discuss the implications of our measures for privacy and
concentration of power. Further work into understanding the measures and
mitigating their negative impacts can help to build a foundation for the
governance of AI agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1&quot;&gt;Alan Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ezell_C/0/1/0/all/0/1&quot;&gt;Carson Ezell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaufmann_M/0/1/0/all/0/1&quot;&gt;Max Kaufmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1&quot;&gt;Kevin Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hammond_L/0/1/0/all/0/1&quot;&gt;Lewis Hammond&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bradley_H/0/1/0/all/0/1&quot;&gt;Herbie Bradley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bluemke_E/0/1/0/all/0/1&quot;&gt;Emma Bluemke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajkumar_N/0/1/0/all/0/1&quot;&gt;Nitarshan Rajkumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krueger_D/0/1/0/all/0/1&quot;&gt;David Krueger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolt_N/0/1/0/all/0/1&quot;&gt;Noam Kolt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heim_L/0/1/0/all/0/1&quot;&gt;Lennart Heim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anderljung_M/0/1/0/all/0/1&quot;&gt;Markus Anderljung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13157">
<title>Time-Aware Knowledge Representations of Dynamic Objects with Multidimensional Persistence. (arXiv:2401.13157v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.13157</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning time-evolving objects such as multivariate time series and dynamic
networks requires the development of novel knowledge representation mechanisms
and neural network architectures, which allow for capturing implicit
time-dependent information contained in the data. Such information is typically
not directly observed but plays a key role in the learning task performance. In
turn, lack of time dimension in knowledge encoding mechanisms for
time-dependent data leads to frequent model updates, poor learning performance,
and, as a result, subpar decision-making. Here we propose a new approach to a
time-aware knowledge representation mechanism that notably focuses on implicit
time-dependent topological information along multiple geometric dimensions. In
particular, we propose a new approach, named \textit{Temporal MultiPersistence}
(TMP), which produces multidimensional topological fingerprints of the data by
using the existing single parameter topological summaries. The main idea behind
TMP is to merge the two newest directions in topological representation
learning, that is, multi-persistence which simultaneously describes data shape
evolution along multiple key parameters, and zigzag persistence to enable us to
extract the most salient data shape information over time. We derive
theoretical guarantees of TMP vectorizations and show its utility, in
application to forecasting on benchmark traffic flow, Ethereum blockchain, and
electrocardiogram datasets, demonstrating the competitive performance,
especially, in scenarios of limited data records. In addition, our TMP method
improves the computational efficiency of the state-of-the-art multipersistence
summaries up to 59.5 times.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coskunuzer_B/0/1/0/all/0/1&quot;&gt;Baris Coskunuzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Segovia_Dominguez_I/0/1/0/all/0/1&quot;&gt;Ignacio Segovia-Dominguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuzhou Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gel_Y/0/1/0/all/0/1&quot;&gt;Yulia R. Gel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13171">
<title>Compositional Generative Inverse Design. (arXiv:2401.13171v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.13171</link>
<description rdf:parseType="Literal">&lt;p&gt;Inverse design, where we seek to design input variables in order to optimize
an underlying objective function, is an important problem that arises across
fields such as mechanical engineering to aerospace engineering. Inverse design
is typically formulated as an optimization problem, with recent works
leveraging optimization across learned dynamics models. However, as models are
optimized they tend to fall into adversarial modes, preventing effective
sampling. We illustrate that by instead optimizing over the learned energy
function captured by the diffusion model, we can avoid such adversarial
examples and significantly improve design performance. We further illustrate
how such a design system is compositional, enabling us to combine multiple
different diffusion models representing subcomponents of our desired system to
design systems with every specified component. In an N-body interaction task
and a challenging 2D multi-airfoil design task, we demonstrate that by
composing the learned diffusion model at test time, our method allows us to
design initial states and boundary shapes that are more complex than those in
the training data. Our method outperforms state-of-the-art neural inverse
design method by an average of 41.5% in prediction MAE and 14.3% in design
objective for the N-body dataset and discovers formation flying to minimize
drag in the multi-airfoil design task. Project website and code can be found at
https://github.com/AI4Science-WestlakeU/cindm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tailin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maruyama_T/0/1/0/all/0/1&quot;&gt;Takashi Maruyama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1&quot;&gt;Long Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yilun Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iaccarino_G/0/1/0/all/0/1&quot;&gt;Gianluca Iaccarino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1&quot;&gt;Jure Leskovec&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13178">
<title>AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents. (arXiv:2401.13178v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.13178</link>
<description rdf:parseType="Literal">&lt;p&gt;Evaluating large language models (LLMs) as general-purpose agents is
essential for understanding their capabilities and facilitating their
integration into practical applications. However, the evaluation process
presents substantial challenges. A primary obstacle is the benchmarking of
agent performance across diverse scenarios within a unified framework,
especially in maintaining partially-observable environments and ensuring
multi-round interactions. Moreover, current evaluation frameworks mostly focus
on the final success rate, revealing few insights during the process and
failing to provide a deep understanding of the model abilities. To address
these challenges, we introduce AgentBoard, a pioneering comprehensive benchmark
and accompanied open-source evaluation framework tailored to analytical
evaluation of LLM agents. AgentBoard offers a fine-grained progress rate metric
that captures incremental advancements as well as a comprehensive evaluation
toolkit that features easy assessment of agents for multi-faceted analysis
through interactive visualization. This not only sheds light on the
capabilities and limitations of LLM agents but also propels the
interpretability of their performance to the forefront. Ultimately, AgentBoard
serves as a significant step towards demystifying agent behaviors and
accelerating the development of stronger LLM agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Junlei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhihao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Cheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yujiu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yaohui Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1&quot;&gt;Zhenzhong Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1&quot;&gt;Lingpeng Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Junxian He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13192">
<title>Generative Design of Crystal Structures by Point Cloud Representations and Diffusion Model. (arXiv:2401.13192v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.13192</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficiently generating energetically stable crystal structures has long been
a challenge in material design, primarily due to the immense arrangement of
atoms in a crystal lattice. To facilitate the discovery of stable material, we
present a framework for the generation of synthesizable materials, leveraging a
point cloud representation to encode intricate structural information. At the
heart of this framework lies the introduction of a diffusion model as its
foundational pillar. To gauge the efficacy of our approach, we employ it to
reconstruct input structures from our training datasets, rigorously validating
its high reconstruction performance. Furthermore, we demonstrate the profound
potential of Point Cloud-Based Crystal Diffusion (PCCD) by generating entirely
new materials, emphasizing their synthesizability. Our research stands as a
noteworthy contribution to the advancement of materials design and synthesis
through the cutting-edge avenue of generative design instead of the
conventional substitution or experience-based discovery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhelin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mrad_R/0/1/0/all/0/1&quot;&gt;Rami Mrad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_R/0/1/0/all/0/1&quot;&gt;Runxian Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1&quot;&gt;Guan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_J/0/1/0/all/0/1&quot;&gt;Jun Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_S/0/1/0/all/0/1&quot;&gt;Shibing Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuanping Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13193">
<title>Catch-Up Mix: Catch-Up Class for Struggling Filters in CNN. (arXiv:2401.13193v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13193</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has made significant advances in computer vision, particularly
in image classification tasks. Despite their high accuracy on training data,
deep learning models often face challenges related to complexity and
overfitting. One notable concern is that the model often relies heavily on a
limited subset of filters for making predictions. This dependency can result in
compromised generalization and an increased vulnerability to minor variations.
While regularization techniques like weight decay, dropout, and data
augmentation are commonly used to address this issue, they may not directly
tackle the reliance on specific filters. Our observations reveal that the heavy
reliance problem gets severe when slow-learning filters are deprived of
learning opportunities due to fast-learning filters. Drawing inspiration from
image augmentation research that combats over-reliance on specific image
regions by removing and replacing parts of images, our idea is to mitigate the
problem of over-reliance on strong filters by substituting highly activated
features. To this end, we present a novel method called Catch-up Mix, which
provides learning opportunities to a wide range of filters during training,
focusing on filters that may lag behind. By mixing activation maps with
relatively lower norms, Catch-up Mix promotes the development of more diverse
representations and reduces reliance on a small subset of filters. Experimental
results demonstrate the superiority of our method in various vision
classification datasets, providing enhanced robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1&quot;&gt;Minsoo Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1&quot;&gt;Minkoo Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Suhyun Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13201">
<title>MLLMReID: Multimodal Large Language Model-based Person Re-identification. (arXiv:2401.13201v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13201</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal large language models (MLLM) have achieved satisfactory results in
many tasks. However, their performance in the task of person re-identification
(ReID) has not been explored to date. This paper will investigate how to adapt
them for the task of ReID. An intuitive idea is to fine-tune MLLM with ReID
image-text datasets, and then use their visual encoder as a backbone for ReID.
However, there still exist two apparent issues: (1) Designing instructions for
ReID, MLLMs may overfit specific instructions, and designing a variety of
instructions will lead to higher costs. (2) Latent image feature vectors from
LLMs are not involved in loss computation. Instructional learning, aligning
image-text features, results in indirect optimization and a learning objective
that inadequately utilizes features, limiting effectiveness in person feature
learning. To address these problems, this paper proposes MLLMReID: Multimodal
Large Language Model-based ReID. Firstly, we proposed Common Instruction, a
simple approach that leverages the essence ability of LLMs to continue writing,
avoiding complex and diverse instruction design. Secondly, we proposed
DirectReID, which effectively employs the latent image feature vectors of
images outputted by LLMs in ReID tasks. The experimental results demonstrate
the superiority of our method. We will open-source the code on GitHub.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongfei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13205">
<title>Boosting the Transferability of Adversarial Examples via Local Mixup and Adaptive Step Size. (arXiv:2401.13205v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13205</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial examples are one critical security threat to various visual
applications, where injected human-imperceptible perturbations can confuse the
output.Generating transferable adversarial examples in the black-box setting is
crucial but challenging in practice. Existing input-diversity-based methods
adopt different image transformations, but may be inefficient due to
insufficient input diversity and an identical perturbation step size. Motivated
by the fact that different image regions have distinctive weights in
classification, this paper proposes a black-box adversarial generative
framework by jointly designing enhanced input diversity and adaptive step
sizes. We design local mixup to randomly mix a group of transformed adversarial
images, strengthening the input diversity. For precise adversarial generation,
we project the perturbation into the $tanh$ space to relax the boundary
constraint. Moreover, the step sizes of different regions can be dynamically
adjusted by integrating a second-order momentum.Extensive experiments on
ImageNet validate that our framework can achieve superior transferability
compared to state-of-the-art baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Junlin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1&quot;&gt;Xinchen Lyu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13212">
<title>AdCorDA: Classifier Refinement via Adversarial Correction and Domain Adaptation. (arXiv:2401.13212v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13212</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper describes a simple yet effective technique for refining a
pretrained classifier network. The proposed AdCorDA method is based on
modification of the training set and making use of the duality between network
weights and layer inputs. We call this input space training. The method
consists of two stages - adversarial correction followed by domain adaptation.
Adversarial correction uses adversarial attacks to correct incorrect
training-set classifications. The incorrectly classified samples of the
training set are removed and replaced with the adversarially corrected samples
to form a new training set, and then, in the second stage, domain adaptation is
performed back to the original training set. Extensive experimental validations
show significant accuracy boosts of over 5% on the CIFAR-100 dataset. The
technique can be straightforwardly applied to refinement of weight-quantized
neural networks, where experiments show substantial enhancement in performance
over the baseline. The adversarial correction technique also results in
enhanced robustness to adversarial attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Lulan Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Edalati_A/0/1/0/all/0/1&quot;&gt;Ali Edalati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meyer_B/0/1/0/all/0/1&quot;&gt;Brett Meyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gross_W/0/1/0/all/0/1&quot;&gt;Warren Gross&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1&quot;&gt;James J. Clark&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13214">
<title>AMANet: Advancing SAR Ship Detection with Adaptive Multi-Hierarchical Attention Network. (arXiv:2401.13214v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13214</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, methods based on deep learning have been successfully applied to
ship detection for synthetic aperture radar (SAR) images. Despite the
development of numerous ship detection methodologies, detecting small and
coastal ships remains a significant challenge due to the limited features and
clutter in coastal environments. For that, a novel adaptive multi-hierarchical
attention module (AMAM) is proposed to learn multi-scale features and
adaptively aggregate salient features from various feature layers, even in
complex environments. Specifically, we first fuse information from adjacent
feature layers to enhance the detection of smaller targets, thereby achieving
multi-scale feature enhancement. Then, to filter out the adverse effects of
complex backgrounds, we dissect the previously fused multi-level features on
the channel, individually excavate the salient regions, and adaptively
amalgamate features originating from different channels. Thirdly, we present a
novel adaptive multi-hierarchical attention network (AMANet) by embedding the
AMAM between the backbone network and the feature pyramid network (FPN).
Besides, the AMAM can be readily inserted between different frameworks to
improve object detection. Lastly, extensive experiments on two large-scale SAR
ship detection datasets demonstrate that our AMANet method is superior to
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiaolin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Junkai Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1&quot;&gt;Aihua Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuhua Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhilong Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13219">
<title>TEPI: Taxonomy-aware Embedding and Pseudo-Imaging for Scarcely-labeled Zero-shot Genome Classification. (arXiv:2401.13219v1 [q-bio.GN])</title>
<link>http://arxiv.org/abs/2401.13219</link>
<description rdf:parseType="Literal">&lt;p&gt;A species&apos; genetic code or genome encodes valuable evolutionary, biological,
and phylogenetic information that aids in species recognition, taxonomic
classification, and understanding genetic predispositions like drug resistance
and virulence. However, the vast number of potential species poses significant
challenges in developing a general-purpose whole genome classification tool.
Traditional bioinformatics tools have made notable progress but lack
scalability and are computationally expensive. Machine learning-based
frameworks show promise but must address the issue of large classification
vocabularies with long-tail distributions. In this study, we propose addressing
this problem through zero-shot learning using TEPI, Taxonomy-aware Embedding
and Pseudo-Imaging. We represent each genome as pseudo-images and map them to a
taxonomy-aware embedding space for reasoning and classification. This embedding
space captures compositional and phylogenetic relationships of species,
enabling predictions in extensive search spaces. We evaluate TEPI using two
rigorous zero-shot settings and demonstrate its generalization capabilities
qualitatively on curated, large-scale, publicly sourced data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Aakur_S/0/1/0/all/0/1&quot;&gt;Sathyanarayanan Aakur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Laguduva_V/0/1/0/all/0/1&quot;&gt;Vishalini R. Laguduva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ramamurthy_P/0/1/0/all/0/1&quot;&gt;Priyadharsini Ramamurthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ramachandran_A/0/1/0/all/0/1&quot;&gt;Akhilesh Ramachandran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13223">
<title>TAT-LLM: A Specialized Language Model for Discrete Reasoning over Tabular and Textual Data. (arXiv:2401.13223v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.13223</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we address question answering (QA) over a hybrid of tabular and
textual data that are very common content on the Web (e.g. SEC filings), where
discrete reasoning capabilities are often required. Recently, large language
models (LLMs) like GPT-4 have demonstrated strong multi-step reasoning
capabilities. We then consider harnessing the amazing power of LLMs to solve
our task. We abstract a Step-wise Pipeline for tabular and textual QA, which
consists of three key steps, including Extractor, Reasoner and Executor, and
initially design an instruction to instantiate the pipeline and validate that
GPT-4 outperforms all existing methods. However, utilizing an online LLM like
GPT-4 holds various challenges in terms of cost, latency, and data security
risk, which motivates us to specialize smaller LLMs in this task. We develop a
TAT-LLM language model by fine-tuning LLaMA 2 with the training data generated
automatically from existing expert-annotated datasets following the Step-wise
Pipeline. The experimental results have verified that our TAT-LLM model can
outperform all baseline models, including the previous best fine-tuned models
and very large-scale LLMs like GPT-4 on FinQA, TAT-QA and TAT-DQA benchmarks.
We hope our work can serve as a pioneering example of specializing smaller
language models for specific tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1&quot;&gt;Fengbin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1&quot;&gt;Fuli Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Moxin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1&quot;&gt;Tat-Seng Chua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13227">
<title>Scalable Link Prediction on Large-Scale Heterogeneous Graphs with Large Language Models. (arXiv:2401.13227v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.13227</link>
<description rdf:parseType="Literal">&lt;p&gt;Exploring the application of large-scale language models to graph learning is
a novel endeavor. However, the vast amount of information inherent in large
graphs poses significant challenges to this process. This paper focuses on the
link prediction task and introduces LPNL (Link Prediction via Natural
Language), a framework based on a large language model designed for scalable
link prediction on large-scale heterogeneous graphs.We design novel prompts for
link prediction that articulate graph details in natural language. We propose a
two-stage sampling pipeline to extract crucial information from large-scale
heterogeneous graphs, and a divide-and-conquer strategy to control the input
token count within predefined limits, addressing the challenge of overwhelming
information. We fine-tune a T5 model based on our self-supervised learning
designed for for link prediction. Extensive experiments on a large public
heterogeneous graphs demonstrate that LPNL outperforms various advanced
baselines, highlighting its remarkable performance in link prediction tasks on
large-scale graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_B/0/1/0/all/0/1&quot;&gt;Baolong Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shenghua Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yiwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_L/0/1/0/all/0/1&quot;&gt;Lingrui Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xueqi Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13229">
<title>From Random to Informed Data Selection: A Diversity-Based Approach to Optimize Human Annotation and Few-Shot Learning. (arXiv:2401.13229v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.13229</link>
<description rdf:parseType="Literal">&lt;p&gt;A major challenge in Natural Language Processing is obtaining annotated data
for supervised learning. An option is the use of crowdsourcing platforms for
data annotation. However, crowdsourcing introduces issues related to the
annotator&apos;s experience, consistency, and biases. An alternative is to use
zero-shot methods, which in turn have limitations compared to their few-shot or
fully supervised counterparts. Recent advancements driven by large language
models show potential, but struggle to adapt to specialized domains with
severely limited data. The most common approaches therefore involve the human
itself randomly annotating a set of datapoints to build initial datasets. But
randomly sampling data to be annotated is often inefficient as it ignores the
characteristics of the data and the specific needs of the model. The situation
worsens when working with imbalanced datasets, as random sampling tends to
heavily bias towards the majority classes, leading to excessive annotated data.
To address these issues, this paper contributes an automatic and informed data
selection architecture to build a small dataset for few-shot learning. Our
proposal minimizes the quantity and maximizes diversity of data selected for
human annotation, while improving model performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alcoforado_A/0/1/0/all/0/1&quot;&gt;Alexandre Alcoforado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferraz_T/0/1/0/all/0/1&quot;&gt;Thomas Palmeira Ferraz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okamura_L/0/1/0/all/0/1&quot;&gt;Lucas Hideki Okamura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fama_I/0/1/0/all/0/1&quot;&gt;Israel Campos Fama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lavado_A/0/1/0/all/0/1&quot;&gt;Arnold Moya Lavado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bueno_B/0/1/0/all/0/1&quot;&gt;B&amp;#xe1;rbara Dias Bueno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veloso_B/0/1/0/all/0/1&quot;&gt;Bruno Veloso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Costa_A/0/1/0/all/0/1&quot;&gt;Anna Helena Reali Costa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13256">
<title>UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems. (arXiv:2401.13256v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.13256</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) has shown exceptional capabilities in many
natual language understanding and generation tasks. However, the
personalization issue still remains a much-coveted property, especially when it
comes to the multiple sources involved in the dialogue system. To better plan
and incorporate the use of multiple sources in generating personalized
response, we firstly decompose it into three sub-tasks: Knowledge Source
Selection, Knowledge Retrieval, and Response Generation. We then propose a
novel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG)
Specifically, we unify these three sub-tasks with different formulations into
the same sequence-to-sequence paradigm during the training, to adaptively
retrieve evidences and evaluate the relevance on-demand using special tokens,
called acting tokens and evaluation tokens. Enabling language models to
generate acting tokens facilitates interaction with various knowledge sources,
allowing them to adapt their behavior to diverse task requirements. Meanwhile,
evaluation tokens gauge the relevance score between the dialogue context and
the retrieved evidence. In addition, we carefully design a self-refinement
mechanism to iteratively refine the generated response considering 1) the
consistency scores between the generated response and retrieved evidence; and
2) the relevance scores. Experiments on two personalized datasets (DuLeMon and
KBP) show that UniMS-RAG achieves state-of-the-art performance on the knowledge
source selection and response generation task with itself as a retriever in a
unified manner. Extensive analyses and discussions are provided for shedding
some new perspectives for personalized dialogue systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongru Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wenyu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yang Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zezhong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yufei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1&quot;&gt;Fei Mi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Jeff Z. Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1&quot;&gt;Kam-Fai Wong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13262">
<title>Designing Redistribution Mechanisms for Reducing Transaction Fees in Blockchains. (arXiv:2401.13262v1 [cs.GT])</title>
<link>http://arxiv.org/abs/2401.13262</link>
<description rdf:parseType="Literal">&lt;p&gt;Blockchains deploy Transaction Fee Mechanisms (TFMs) to determine which user
transactions to include in blocks and determine their payments (i.e.,
transaction fees). Increasing demand and scarce block resources have led to
high user transaction fees. As these blockchains are a public resource, it may
be preferable to reduce these transaction fees. To this end, we introduce
Transaction Fee Redistribution Mechanisms (TFRMs) -- redistributing VCG
payments collected from such TFM as rebates to minimize transaction fees.
Classic redistribution mechanisms (RMs) achieve this while ensuring Allocative
Efficiency (AE) and User Incentive Compatibility (UIC). Our first result shows
the non-triviality of applying RM in TFMs. More concretely, we prove that it is
impossible to reduce transaction fees when (i) transactions that are not
confirmed do not receive rebates and (ii) the miner can strategically
manipulate the mechanism. Driven by this, we propose \emph{Robust} TFRM
(\textsf{R-TFRM}): a mechanism that compromises on an honest miner&apos;s individual
rationality to guarantee strictly positive rebates to the users. We then
introduce \emph{robust} and \emph{rational} TFRM (\textsf{R}$^2$\textsf{-TFRM})
that uses trusted on-chain randomness that additionally guarantees miner&apos;s
individual rationality (in expectation) and strictly positive rebates. Our
results show that TFRMs provide a promising new direction for reducing
transaction fees in public blockchains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damle_S/0/1/0/all/0/1&quot;&gt;Sankarshan Damle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Padala_M/0/1/0/all/0/1&quot;&gt;Manisha Padala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gujar_S/0/1/0/all/0/1&quot;&gt;Sujit Gujar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13270">
<title>Audio-Infused Automatic Image Colorization by Exploiting Audio Scene Semantics. (arXiv:2401.13270v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13270</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic image colorization is inherently an ill-posed problem with
uncertainty, which requires an accurate semantic understanding of scenes to
estimate reasonable colors for grayscale images. Although recent
interaction-based methods have achieved impressive performance, it is still a
very difficult task to infer realistic and accurate colors for automatic
colorization. To reduce the difficulty of semantic understanding of grayscale
scenes, this paper tries to utilize corresponding audio, which naturally
contains extra semantic information about the same scene. Specifically, a novel
audio-infused automatic image colorization (AIAIC) network is proposed, which
consists of three stages. First, we take color image semantics as a bridge and
pretrain a colorization network guided by color image semantics. Second, the
natural co-occurrence of audio and video is utilized to learn the color
semantic correlations between audio and visual scenes. Third, the implicit
audio semantic representation is fed into the pretrained network to finally
realize the audio-guided colorization. The whole process is trained in a
self-supervised manner without human annotation. In addition, an audiovisual
colorization dataset is established for training and testing. Experiments
demonstrate that audio guidance can effectively improve the performance of
automatic colorization, especially for some scenes that are difficult to
understand only from visual modality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1&quot;&gt;Pengcheng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yanxiang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1&quot;&gt;Wei Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ronggang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1&quot;&gt;Richang Hong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13275">
<title>Can AI Assistants Know What They Don&apos;t Know?. (arXiv:2401.13275v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.13275</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, AI assistants based on large language models (LLMs) show surprising
performance in many tasks, such as dialogue, solving math problems, writing
code, and using tools. Although LLMs possess intensive world knowledge, they
still make factual errors when facing some knowledge intensive tasks, like
open-domain question answering. These untruthful responses from the AI
assistant may cause significant risks in practical applications. We believe
that an AI assistant&apos;s refusal to answer questions it does not know is a
crucial method for reducing hallucinations and making the assistant truthful.
Therefore, in this paper, we ask the question &quot;Can AI assistants know what they
don&apos;t know and express them through natural language?&quot; To answer this question,
we construct a model-specific &quot;I don&apos;t know&quot; (Idk) dataset for an assistant,
which contains its known and unknown questions, based on existing open-domain
question answering datasets. Then we align the assistant with its corresponding
Idk dataset and observe whether it can refuse to answer its unknown questions
after alignment. Experimental results show that after alignment with Idk
datasets, the assistant can refuse to answer most its unknown questions. For
questions they attempt to answer, the accuracy is significantly higher than
before the alignment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1&quot;&gt;Qinyuan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1&quot;&gt;Tianxiang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiangyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Zhangyue Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shimin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Linyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1&quot;&gt;Xipeng Qiu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13282">
<title>RefreshNet: Learning Multiscale Dynamics through Hierarchical Refreshing. (arXiv:2401.13282v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.13282</link>
<description rdf:parseType="Literal">&lt;p&gt;Forecasting complex system dynamics, particularly for long-term predictions,
is persistently hindered by error accumulation and computational burdens. This
study presents RefreshNet, a multiscale framework developed to overcome these
challenges, delivering an unprecedented balance between computational
efficiency and predictive accuracy. RefreshNet incorporates convolutional
autoencoders to identify a reduced order latent space capturing essential
features of the dynamics, and strategically employs multiple recurrent neural
network (RNN) blocks operating at varying temporal resolutions within the
latent space, thus allowing the capture of latent dynamics at multiple temporal
scales. The unique &quot;refreshing&quot; mechanism in RefreshNet allows coarser blocks
to reset inputs of finer blocks, effectively controlling and alleviating error
accumulation. This design demonstrates superiority over existing techniques
regarding computational efficiency and predictive accuracy, especially in
long-term forecasting. The framework is validated using three benchmark
applications: the FitzHugh-Nagumo system, the Reaction-Diffusion equation, and
Kuramoto-Sivashinsky dynamics. RefreshNet significantly outperforms
state-of-the-art methods in long-term forecasting accuracy and speed, marking a
significant advancement in modeling complex systems and opening new avenues in
understanding and predicting their behavior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farooq_J/0/1/0/all/0/1&quot;&gt;Junaid Farooq&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rafiq_D/0/1/0/all/0/1&quot;&gt;Danish Rafiq&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vlachas_P/0/1/0/all/0/1&quot;&gt;Pantelis R. Vlachas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bazaz_M/0/1/0/all/0/1&quot;&gt;Mohammad Abid Bazaz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13298">
<title>Towards Explainable Harmful Meme Detection through Multimodal Debate between Large Language Models. (arXiv:2401.13298v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.13298</link>
<description rdf:parseType="Literal">&lt;p&gt;The age of social media is flooded with Internet memes, necessitating a clear
grasp and effective identification of harmful ones. This task presents a
significant challenge due to the implicit meaning embedded in memes, which is
not explicitly conveyed through the surface text and image. However, existing
harmful meme detection methods do not present readable explanations that unveil
such implicit meaning to support their detection decisions. In this paper, we
propose an explainable approach to detect harmful memes, achieved through
reasoning over conflicting rationales from both harmless and harmful positions.
Specifically, inspired by the powerful capacity of Large Language Models (LLMs)
on text generation and reasoning, we first elicit multimodal debate between
LLMs to generate the explanations derived from the contradictory arguments.
Then we propose to fine-tune a small language model as the debate judge for
harmfulness inference, to facilitate multimodal fusion between the harmfulness
rationales and the intrinsic multimodal information within memes. In this way,
our model is empowered to perform dialectical reasoning over intricate and
implicit harm-indicative patterns, utilizing multimodal explanations
originating from both harmless and harmful arguments. Extensive experiments on
three public meme datasets demonstrate that our harmful meme detection approach
achieves much better performance than state-of-the-art methods and exhibits a
superior capacity for explaining the meme harmfulness of the model predictions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1&quot;&gt;Hongzhan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Ziyang Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1&quot;&gt;Wei Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jing Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1&quot;&gt;Ruichao Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13311">
<title>ConTextual: Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models. (arXiv:2401.13311v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13311</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in AI have led to the development of large multimodal
models (LMMs) capable of processing complex tasks involving joint reasoning
over text and visual content in the image (e.g., navigating maps in public
places). This paper introduces ConTextual, a novel benchmark comprising
instructions designed explicitly to evaluate LMMs&apos; ability to perform
context-sensitive text-rich visual reasoning. ConTextual emphasizes diverse
real-world scenarios (e.g., time-reading, navigation, shopping and more)
demanding a deeper understanding of the interactions between textual and visual
elements. Our findings reveal a significant performance gap of 30.8% between
the best-performing LMM, GPT-4V(ision), and human capabilities using human
evaluation indicating substantial room for improvement in context-sensitive
text-rich visual reasoning. Notably, while GPT-4V excelled in abstract
categories like meme and quote interpretation, its overall performance still
lagged behind humans. In addition to human evaluations, we also employed
automatic evaluation metrics using GPT-4, uncovering similar trends in
performance disparities. We also perform a fine-grained evaluation across
diverse visual contexts and provide qualitative analysis which provides a
robust framework for future advancements in the LMM design.
https://con-textual.github.io/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wadhawan_R/0/1/0/all/0/1&quot;&gt;Rohan Wadhawan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_H/0/1/0/all/0/1&quot;&gt;Hritik Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1&quot;&gt;Kai-Wei Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1&quot;&gt;Nanyun Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13315">
<title>Deep Learning for Improved Polyp Detection from Synthetic Narrow-Band Imaging. (arXiv:2401.13315v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.13315</link>
<description rdf:parseType="Literal">&lt;p&gt;To cope with the growing prevalence of colorectal cancer (CRC), screening
programs for polyp detection and removal have proven their usefulness.
Colonoscopy is considered the best-performing procedure for CRC screening. To
ease the examination, deep learning based methods for automatic polyp detection
have been developed for conventional white-light imaging (WLI). Compared with
WLI, narrow-band imaging (NBI) can improve polyp classification during
colonoscopy but requires special equipment. We propose a CycleGAN-based
framework to convert images captured with regular WLI to synthetic NBI (SNBI)
as a pre-processing method for improving object detection on WLI when NBI is
unavailable. This paper first shows that better results for polyp detection can
be achieved on NBI compared to a relatively similar dataset of WLI. Secondly,
experimental results demonstrate that our proposed modality translation can
achieve improved polyp detection on SNBI images generated from WLI compared to
the original WLI. This is because our WLI-to-SNBI translation model can enhance
the observation of polyp surface patterns in the generated SNBI images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Haugland_M/0/1/0/all/0/1&quot;&gt;Mathias Ramm Haugland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qadir_H/0/1/0/all/0/1&quot;&gt;Hemin Ali Qadir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Balasingham_I/0/1/0/all/0/1&quot;&gt;Ilangko Balasingham&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13324">
<title>Information That Matters: Exploring Information Needs of People Affected by Algorithmic Decisions. (arXiv:2401.13324v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.13324</link>
<description rdf:parseType="Literal">&lt;p&gt;Explanations of AI systems rarely address the information needs of people
affected by algorithmic decision-making (ADM). This gap between conveyed
information and information that matters to affected stakeholders can impede
understanding and adherence to regulatory frameworks such as the AI Act. To
address this gap, we present the &quot;XAI Novice Question Bank&quot;: A catalog of
affected stakeholders&apos; information needs in two ADM use cases (employment
prediction and health monitoring), covering the categories data, system
context, system usage, and system specifications. Information needs were
gathered in an interview study where participants received explanations in
response to their inquiries. Participants further reported their understanding
and decision confidence, showing that while confidence tended to increase after
receiving explanations, participants also met understanding challenges, such as
being unable to tell why their understanding felt incomplete. Explanations
further influenced participants&apos; perceptions of the systems&apos; risks and
benefits, which they confirmed or changed depending on the use case. When risks
were perceived as high, participants expressed particular interest in
explanations about intention, such as why and to what end a system was put in
place. With this work, we aim to support the inclusion of affected stakeholders
into explainability by contributing an overview of information and challenges
relevant to them when deciding on the adoption of ADM systems. We close by
summarizing our findings in a list of six key implications that inform the
design of future explanations for affected stakeholder audiences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmude_T/0/1/0/all/0/1&quot;&gt;Timoth&amp;#xe9;e Schmude&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koesten_L/0/1/0/all/0/1&quot;&gt;Laura Koesten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moller_T/0/1/0/all/0/1&quot;&gt;Torsten M&amp;#xf6;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tschiatschek_S/0/1/0/all/0/1&quot;&gt;Sebastian Tschiatschek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13334">
<title>Explainable Bayesian Optimization. (arXiv:2401.13334v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.13334</link>
<description rdf:parseType="Literal">&lt;p&gt;In industry, Bayesian optimization (BO) is widely applied in the human-AI
collaborative parameter tuning of cyber-physical systems. However, BO&apos;s
solutions may deviate from human experts&apos; actual goal due to approximation
errors and simplified objectives, requiring subsequent tuning. The black-box
nature of BO limits the collaborative tuning process because the expert does
not trust the BO recommendations. Current explainable AI (XAI) methods are not
tailored for optimization and thus fall short of addressing this gap. To bridge
this gap, we propose TNTRules (TUNE-NOTUNE Rules), a post-hoc, rule-based
explainability method that produces high quality explanations through
multiobjective optimization. Our evaluation of benchmark optimization problems
and real-world hyperparameter optimization tasks demonstrates TNTRules&apos;
superiority over state-of-the-art XAI methods in generating high quality
explanations. This work contributes to the intersection of BO and XAI,
providing interpretable optimization techniques for real-world applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1&quot;&gt;Tanmay Chakraborty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seifert_C/0/1/0/all/0/1&quot;&gt;Christin Seifert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wirth_C/0/1/0/all/0/1&quot;&gt;Christian Wirth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13335">
<title>Full Bayesian Significance Testing for Neural Networks. (arXiv:2401.13335v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2401.13335</link>
<description rdf:parseType="Literal">&lt;p&gt;Significance testing aims to determine whether a proposition about the
population distribution is the truth or not given observations. However,
traditional significance testing often needs to derive the distribution of the
testing statistic, failing to deal with complex nonlinear relationships. In
this paper, we propose to conduct Full Bayesian Significance Testing for neural
networks, called \textit{n}FBST, to overcome the limitation in relationship
characterization of traditional approaches. A Bayesian neural network is
utilized to fit the nonlinear and multi-dimensional relationships with small
errors and avoid hard theoretical derivation by computing the evidence value.
Besides, \textit{n}FBST can test not only global significance but also local
and instance-wise significance, which previous testing methods don&apos;t focus on.
Moreover, \textit{n}FBST is a general framework that can be extended based on
the measures selected, such as Grad-\textit{n}FBST, LRP-\textit{n}FBST,
DeepLIFT-\textit{n}FBST, LIME-\textit{n}FBST. A range of experiments on both
simulated and real data are conducted to show the advantages of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zehua Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zimeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yue He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13346">
<title>Past, Present, Future: A Comprehensive Exploration of AI Use Cases in the UMBRELLA IoT Testbed. (arXiv:2401.13346v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2401.13346</link>
<description rdf:parseType="Literal">&lt;p&gt;UMBRELLA is a large-scale, open-access Internet of Things (IoT) ecosystem
incorporating over 200 multi-sensor multi-wireless nodes, 20 collaborative
robots, and edge-intelligence-enabled devices. This paper provides a guide to
the implemented and prospective artificial intelligence (AI) capabilities of
UMBRELLA in real-world IoT systems. Four existing UMBRELLA applications are
presented in detail: 1) An automated streetlight monitoring for detecting
issues and triggering maintenance alerts; 2) A Digital twin of building
environments providing enhanced air quality sensing with reduced cost; 3) A
large-scale Federated Learning framework for reducing communication overhead;
and 4) An intrusion detection for containerised applications identifying
malicious activities. Additionally, the potential of UMBRELLA is outlined for
future smart city and multi-robot crowdsensing applications enhanced by
semantic communications and multi-agent planning. Finally, to realise the above
use-cases we discuss the need for a tailored MLOps platform to automate
UMBRELLA model pipelines and establish trust.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Peizheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mavromatis_I/0/1/0/all/0/1&quot;&gt;Ioannis Mavromatis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Aftab Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13408">
<title>Causal Perception. (arXiv:2401.13408v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.13408</link>
<description rdf:parseType="Literal">&lt;p&gt;Perception occurs when two individuals interpret the same information
differently. Despite being a known phenomenon with implications for bias in
decision-making, as individuals&apos; experience determines interpretation,
perception remains largely overlooked in automated decision-making (ADM)
systems. In particular, it can have considerable effects on the fairness or
fair usage of an ADM system, as fairness itself is context-specific and its
interpretation dependent on who is judging. In this work, we formalize
perception under causal reasoning to capture the act of interpretation by an
individual. We also formalize individual experience as additional causal
knowledge that comes with and is used by an individual. Further, we define and
discuss loaded attributes, which are attributes prone to evoke perception.
Sensitive attributes, such as gender and race, are clear examples of loaded
attributes. We define two kinds of causal perception, unfaithful and
inconsistent, based on the causal properties of faithfulness and consistency.
We illustrate our framework through a series of decision-making examples and
discuss relevant fairness applications. The goal of this work is to position
perception as a parameter of interest, useful for extending the standard,
single interpretation ADM problem formulation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1&quot;&gt;Jose M. Alvarez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruggieri_S/0/1/0/all/0/1&quot;&gt;Salvatore Ruggieri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13432">
<title>Semi-Supervised Coupled Thin-Plate Spline Model for Rotation Correction and Beyond. (arXiv:2401.13432v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13432</link>
<description rdf:parseType="Literal">&lt;p&gt;Thin-plate spline (TPS) is a principal warp that allows for representing
elastic, nonlinear transformation with control point motions. With the increase
of control points, the warp becomes increasingly flexible but usually
encounters a bottleneck caused by undesired issues, e.g., content distortion.
In this paper, we explore generic applications of TPS in single-image-based
warping tasks, such as rotation correction, rectangling, and portrait
correction. To break this bottleneck, we propose the coupled thin-plate spline
model (CoupledTPS), which iteratively couples multiple TPS with limited control
points into a more flexible and powerful transformation. Concretely, we first
design an iterative search to predict new control points according to the
current latent condition. Then, we present the warping flow as a bridge for the
coupling of different TPS transformations, effectively eliminating
interpolation errors caused by multiple warps. Besides, in light of the
laborious annotation cost, we develop a semi-supervised learning scheme to
improve warping quality by exploiting unlabeled data. It is formulated through
dual transformation between the searched control points of unlabeled data and
its graphic augmentation, yielding an implicit correction consistency
constraint. Finally, we collect massive unlabeled data to exhibit the benefit
of our semi-supervised scheme in rotation correction. Extensive experiments
demonstrate the superiority and universality of CoupledTPS over the existing
state-of-the-art (SoTA) solutions for rotation correction and beyond. The code
and data will be available at https://github.com/nie-lang/CoupledTPS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1&quot;&gt;Lang Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chunyu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_K/0/1/0/all/0/1&quot;&gt;Kang Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shuaicheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yao Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13444">
<title>Clue-Guided Path Exploration: An Efficient Knowledge Base Question-Answering Framework with Low Computational Resource Consumption. (arXiv:2401.13444v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.13444</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent times, large language models (LLMs) have showcased remarkable
capabilities. However, updating their knowledge poses challenges, potentially
leading to inaccuracies when confronted with unfamiliar queries. While
integrating knowledge graphs with LLMs has been explored, existing approaches
treat LLMs as primary decision-makers, imposing high demands on their
capabilities. This is particularly unsuitable for LLMs with lower computational
costs and relatively poorer performance. In this paper, we introduce a
Clue-Guided Path Exploration framework (CGPE) that efficiently merges a
knowledge base with an LLM, placing less stringent requirements on the model&apos;s
capabilities. Inspired by the method humans use to manually retrieve knowledge,
CGPE employs information from the question as clues to systematically explore
the required knowledge path within the knowledge base. Experiments on
open-source datasets reveal that CGPE outperforms previous methods and is
highly applicable to LLMs with fewer parameters. In some instances, even
ChatGLM3, with its 6 billion parameters, can rival the performance of GPT-4.
Furthermore, the results indicate a minimal invocation frequency of CGPE on
LLMs, suggesting reduced computational overhead. For organizations and
individuals facing constraints in computational resources, our research offers
significant practical value.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dehao Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Feng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yongfeng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1&quot;&gt;Minghu Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13460">
<title>Multi-Agent Diagnostics for Robustness via Illuminated Diversity. (arXiv:2401.13460v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.13460</link>
<description rdf:parseType="Literal">&lt;p&gt;In the rapidly advancing field of multi-agent systems, ensuring robustness in
unfamiliar and adversarial settings is crucial. Notwithstanding their
outstanding performance in familiar environments, these systems often falter in
new situations due to overfitting during the training phase. This is especially
pronounced in settings where both cooperative and competitive behaviours are
present, encapsulating a dual nature of overfitting and generalisation
challenges. To address this issue, we present Multi-Agent Diagnostics for
Robustness via Illuminated Diversity (MADRID), a novel approach for generating
diverse adversarial scenarios that expose strategic vulnerabilities in
pre-trained multi-agent policies. Leveraging the concepts from open-ended
learning, MADRID navigates the vast space of adversarial settings, employing a
target policy&apos;s regret to gauge the vulnerabilities of these settings. We
evaluate the effectiveness of MADRID on the 11vs11 version of Google Research
Football, one of the most complex environments for multi-agent reinforcement
learning. Specifically, we employ MADRID for generating a diverse array of
adversarial settings for TiZero, the state-of-the-art approach which &quot;masters&quot;
the game through 45 days of training on a large-scale distributed
infrastructure. We expose key shortcomings in TiZero&apos;s tactical
decision-making, underlining the crucial importance of rigorous evaluation in
multi-agent systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samvelyan_M/0/1/0/all/0/1&quot;&gt;Mikayel Samvelyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paglieri_D/0/1/0/all/0/1&quot;&gt;Davide Paglieri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1&quot;&gt;Minqi Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parker_Holder_J/0/1/0/all/0/1&quot;&gt;Jack Parker-Holder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1&quot;&gt;Tim Rockt&amp;#xe4;schel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13462">
<title>Growing from Exploration: A self-exploring framework for robots based on foundation models. (arXiv:2401.13462v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.13462</link>
<description rdf:parseType="Literal">&lt;p&gt;Intelligent robot is the ultimate goal in the robotics field. Existing works
leverage learning-based or optimization-based methods to accomplish
human-defined tasks. However, the challenge of enabling robots to explore
various environments autonomously remains unresolved. In this work, we propose
a framework named GExp, which enables robots to explore and learn autonomously
without human intervention. To achieve this goal, we devise modules including
self-exploration, knowledge-base-building, and close-loop feedback based on
foundation models. Inspired by the way that infants interact with the world,
GExp encourages robots to understand and explore the environment with a series
of self-generated tasks. During the process of exploration, the robot will
acquire skills from beneficial experiences that are useful in the future. GExp
provides robots with the ability to solve complex tasks through
self-exploration. GExp work is independent of prior interactive knowledge and
human intervention, allowing it to adapt directly to different scenarios,
unlike previous studies that provided in-context examples as few-shot learning.
In addition, we propose a workflow of deploying the real-world robot system
with self-learned skills as an embodied assistant.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shoujie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1&quot;&gt;Ran Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_J/0/1/0/all/0/1&quot;&gt;JunWen Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiao-Ping Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1&quot;&gt;Wenbo Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13481">
<title>How AI Ideas Affect the Creativity, Diversity, and Evolution of Human Ideas: Evidence From a Large, Dynamic Experiment. (arXiv:2401.13481v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2401.13481</link>
<description rdf:parseType="Literal">&lt;p&gt;Exposure to large language model output is rapidly increasing. How will
seeing AI-generated ideas affect human ideas? We conducted an experiment (800+
participants, 40+ countries) where participants viewed creative ideas that were
from ChatGPT or prior experimental participants and then brainstormed their own
idea. We varied the number of AI-generated examples (none, low, or high
exposure) and if the examples were labeled as &apos;AI&apos; (disclosure). Our dynamic
experiment design -- ideas from prior participants in an experimental condition
are used as stimuli for future participants in the same experimental condition
-- mimics the interdependent process of cultural creation: creative ideas are
built upon prior ideas. Hence, we capture the compounding effects of having
LLMs &apos;in the culture loop&apos;. We find that high AI exposure (but not low AI
exposure) did not affect the creativity of individual ideas but did increase
the average amount and rate of change of collective idea diversity. AI made
ideas different, not better. There were no main effects of disclosure. We also
found that self-reported creative people were less influenced by knowing an
idea was from AI, and that participants were more likely to knowingly adopt AI
ideas when the task was difficult. Our findings suggest that introducing AI
ideas into society may increase collective diversity but not individual
creativity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ashkinaze_J/0/1/0/all/0/1&quot;&gt;Joshua Ashkinaze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mendelsohn_J/0/1/0/all/0/1&quot;&gt;Julia Mendelsohn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiwei_L/0/1/0/all/0/1&quot;&gt;Li Qiwei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Budak_C/0/1/0/all/0/1&quot;&gt;Ceren Budak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilbert_E/0/1/0/all/0/1&quot;&gt;Eric Gilbert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13486">
<title>Separable Physics-Informed Neural Networks for the solution of elasticity problems. (arXiv:2401.13486v1 [math.NA])</title>
<link>http://arxiv.org/abs/2401.13486</link>
<description rdf:parseType="Literal">&lt;p&gt;A method for solving elasticity problems based on separable physics-informed
neural networks (SPINN) in conjunction with the deep energy method (DEM) is
presented. Numerical experiments have been carried out for a number of problems
showing that this method has a significantly higher convergence rate and
accuracy than the vanilla physics-informed neural networks (PINN) and even
SPINN based on a system of partial differential equations (PDEs). In addition,
using the SPINN in the framework of DEM approach it is possible to solve
problems of the linear theory of elasticity on complex geometries, which is
unachievable with the help of PINNs in frames of partial differential
equations. Considered problems are very close to the industrial problems in
terms of geometry, loading, and material parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Eskin_V/0/1/0/all/0/1&quot;&gt;Vasiliy A. Es&amp;#x27;kin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Davydov_D/0/1/0/all/0/1&quot;&gt;Danil V. Davydov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Gureva_J/0/1/0/all/0/1&quot;&gt;Julia V. Gur&amp;#x27;eva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Malkhanov_A/0/1/0/all/0/1&quot;&gt;Alexey O. Malkhanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Smorkalov_M/0/1/0/all/0/1&quot;&gt;Mikhail E. Smorkalov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13498">
<title>Expressive Acoustic Guitar Sound Synthesis with an Instrument-Specific Input Representation and Diffusion Outpainting. (arXiv:2401.13498v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2401.13498</link>
<description rdf:parseType="Literal">&lt;p&gt;Synthesizing performing guitar sound is a highly challenging task due to the
polyphony and high variability in expression. Recently, deep generative models
have shown promising results in synthesizing expressive polyphonic instrument
sounds from music scores, often using a generic MIDI input. In this work, we
propose an expressive acoustic guitar sound synthesis model with a customized
input representation to the instrument, which we call guitarroll. We implement
the proposed approach using diffusion-based outpainting which can generate
audio with long-term consistency. To overcome the lack of MIDI/audio-paired
datasets, we used not only an existing guitar dataset but also collected data
from a high quality sample-based guitar synthesizer. Through quantitative and
qualitative evaluations, we show that our proposed model has higher audio
quality than the baseline model and generates more realistic timbre sounds than
the previous leading work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hounsu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1&quot;&gt;Soonbeom Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nam_J/0/1/0/all/0/1&quot;&gt;Juhan Nam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13555">
<title>Benchmarking the Fairness of Image Upsampling Methods. (arXiv:2401.13555v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13555</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years have witnessed a rapid development of deep generative models for
creating synthetic media, such as images and videos. While the practical
applications of these models in everyday tasks are enticing, it is crucial to
assess the inherent risks regarding their fairness. In this work, we introduce
a comprehensive framework for benchmarking the performance and fairness of
conditional generative models. We develop a set of
metrics$\unicode{x2013}$inspired by their supervised fairness
counterparts$\unicode{x2013}$to evaluate the models on their fairness and
diversity. Focusing on the specific application of image upsampling, we create
a benchmark covering a wide variety of modern upsampling methods. As part of
the benchmark, we introduce UnfairFace, a subset of FairFace that replicates
the racial distribution of common large-scale face datasets. Our empirical
study highlights the importance of using an unbiased training set and reveals
variations in how the algorithms respond to dataset imbalances. Alarmingly, we
find that none of the considered methods produces statistically fair and
diverse results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laszkiewicz_M/0/1/0/all/0/1&quot;&gt;Mike Laszkiewicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daunhawer_I/0/1/0/all/0/1&quot;&gt;Imant Daunhawer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vogt_J/0/1/0/all/0/1&quot;&gt;Julia E. Vogt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fischer_A/0/1/0/all/0/1&quot;&gt;Asja Fischer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lederer_J/0/1/0/all/0/1&quot;&gt;Johannes Lederer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13586">
<title>Prompt Weight Experiments for LLM Instruction Fine-Tuning. (arXiv:2401.13586v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.13586</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a small study analyzing how prompt token classification loss
weighting (PLW) affects the performance of 7B-size LLaMA models fine-tuned on
instruction tasks. We recreated Stanford&apos;s Alpaca experiment with both LLaMA 1
and LLaMA 2 using multiple instruction datasets. We found that models
fine-tuned on our short-completion dataset have a negative quadratic
relationship with PLW while models fine-tuned on long-completion datasets were
unaffected by PLW.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huerta_Enochian_M/0/1/0/all/0/1&quot;&gt;Mathew Huerta-Enochian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13588">
<title>Evaluation of General Large Language Models in Contextually Assessing Semantic Concepts Extracted from Adult Critical Care Electronic Health Record Notes. (arXiv:2401.13588v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.13588</link>
<description rdf:parseType="Literal">&lt;p&gt;The field of healthcare has increasingly turned its focus towards Large
Language Models (LLMs) due to their remarkable performance. However, their
performance in actual clinical applications has been underexplored. Traditional
evaluations based on question-answering tasks don&apos;t fully capture the nuanced
contexts. This gap highlights the need for more in-depth and practical
assessments of LLMs in real-world healthcare settings. Objective: We sought to
evaluate the performance of LLMs in the complex clinical context of adult
critical care medicine using systematic and comprehensible analytic methods,
including clinician annotation and adjudication. Methods: We investigated the
performance of three general LLMs in understanding and processing real-world
clinical notes. Concepts from 150 clinical notes were identified by MetaMap and
then labeled by 9 clinicians. Each LLM&apos;s proficiency was evaluated by
identifying the temporality and negation of these concepts using different
prompts for an in-depth analysis. Results: GPT-4 showed overall superior
performance compared to other LLMs. In contrast, both GPT-3.5 and
text-davinci-003 exhibit enhanced performance when the appropriate prompting
strategies are employed. The GPT family models have demonstrated considerable
efficiency, evidenced by their cost-effectiveness and time-saving capabilities.
Conclusion: A comprehensive qualitative performance evaluation framework for
LLMs is developed and operationalized. This framework goes beyond singular
performance aspects. With expert annotations, this methodology not only
validates LLMs&apos; capabilities in processing complex medical data but also
establishes a benchmark for future LLM evaluations across specialized domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Darren Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1&quot;&gt;Cheng Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bold_D/0/1/0/all/0/1&quot;&gt;Delgersuren Bold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouvier_M/0/1/0/all/0/1&quot;&gt;Monique Bouvier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiaying Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shickel_B/0/1/0/all/0/1&quot;&gt;Benjamin Shickel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jabaley_C/0/1/0/all/0/1&quot;&gt;Craig S. Jabaley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenhui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Soojin Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Young_M/0/1/0/all/0/1&quot;&gt;Michael J. Young&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wainwright_M/0/1/0/all/0/1&quot;&gt;Mark S. Wainwright&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clermont_G/0/1/0/all/0/1&quot;&gt;Gilles Clermont&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rashidi_P/0/1/0/all/0/1&quot;&gt;Parisa Rashidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosenthal_E/0/1/0/all/0/1&quot;&gt;Eric S. Rosenthal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dimisko_L/0/1/0/all/0/1&quot;&gt;Laurie Dimisko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_R/0/1/0/all/0/1&quot;&gt;Ran Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1&quot;&gt;Joo Heung Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Carl Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xiao Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13594">
<title>Graph Guided Question Answer Generation for Procedural Question-Answering. (arXiv:2401.13594v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.13594</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we focus on task-specific question answering (QA). To this
end, we introduce a method for generating exhaustive and high-quality training
data, which allows us to train compact (e.g., run on a mobile device),
task-specific QA models that are competitive against GPT variants. The key
technological enabler is a novel mechanism for automatic question-answer
generation from procedural text which can ingest large amounts of textual
instructions and produce exhaustive in-domain QA training data. While current
QA data generation methods can produce well-formed and varied data, their
non-exhaustive nature is sub-optimal for training a QA model. In contrast, we
leverage the highly structured aspect of procedural text and represent each
step and the overall flow of the procedure as graphs. We then condition on
graph nodes to automatically generate QA pairs in an exhaustive and
controllable manner. Comprehensive evaluations of our method show that: 1)
small models trained with our data achieve excellent performance on the target
QA task, even exceeding that of GPT3 and ChatGPT despite being several orders
of magnitude smaller. 2) semantic coverage is the key indicator for downstream
QA performance. Crucially, while large language models excel at syntactic
diversity, this does not necessarily result in improvements on the end QA
model. In contrast, the higher semantic coverage provided by our method is
critical for QA performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1&quot;&gt;Hai X. Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hadji_I/0/1/0/all/0/1&quot;&gt;Isma Hadji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xinnuo Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Degutyte_Z/0/1/0/all/0/1&quot;&gt;Ziedune Degutyte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rainey_J/0/1/0/all/0/1&quot;&gt;Jay Rainey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kazakos_E/0/1/0/all/0/1&quot;&gt;Evangelos Kazakos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fazly_A/0/1/0/all/0/1&quot;&gt;Afsaneh Fazly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tzimiropoulos_G/0/1/0/all/0/1&quot;&gt;Georgios Tzimiropoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_B/0/1/0/all/0/1&quot;&gt;Brais Martinez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13604">
<title>Stream-based perception for cognitive agents in mobile ecosystems. (arXiv:2401.13604v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.13604</link>
<description rdf:parseType="Literal">&lt;p&gt;Cognitive agent abstractions can help to engineer intelligent systems across
mobile devices. On smartphones, the data obtained from onboard sensors can give
valuable insights into the user&apos;s current situation. Unfortunately, today&apos;s
cognitive agent frameworks cannot cope well with the challenging
characteristics of sensor data. Sensor data is located on a low abstraction
level and the individual data elements are not meaningful when observed in
isolation. In contrast, cognitive agents operate on high-level percepts and
lack the means to effectively detect complex spatio-temporal patterns in
sequences of multiple percepts. In this paper, we present a stream-based
perception approach that enables the agents to perceive meaningful situations
in low-level sensor data streams. We present a crowdshipping case study where
autonomous, self-interested agents collaborate to deliver parcels to their
destinations. We show how situations derived from smartphone sensor data can
trigger and guide auctions, which the agents use to reach agreements.
Experiments with real smartphone data demonstrate the benefits of stream-based
agent perception.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dotterl_J/0/1/0/all/0/1&quot;&gt;Jeremias D&amp;#xf6;tterl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bruns_R/0/1/0/all/0/1&quot;&gt;Ralf Bruns&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dunkel_J/0/1/0/all/0/1&quot;&gt;J&amp;#xfc;rgen Dunkel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ossowski_S/0/1/0/all/0/1&quot;&gt;Sascha Ossowski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13611">
<title>Non-Intrusive Speech Intelligibility Prediction for Hearing-Impaired Users using Intermediate ASR Features and Human Memory Models. (arXiv:2401.13611v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2401.13611</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks have been successfully used for non-intrusive speech
intelligibility prediction. Recently, the use of feature representations
sourced from intermediate layers of pre-trained self-supervised and
weakly-supervised models has been found to be particularly useful for this
task. This work combines the use of Whisper ASR decoder layer representations
as neural network input features with an exemplar-based, psychologically
motivated model of human memory to predict human intelligibility ratings for
hearing-aid users. Substantial performance improvement over an established
intrusive HASPI baseline system is found, including on enhancement systems and
listeners unseen in the training data, with a root mean squared error of 25.3
compared with the baseline of 28.7.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mogridge_R/0/1/0/all/0/1&quot;&gt;Rhiannon Mogridge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Close_G/0/1/0/all/0/1&quot;&gt;George Close&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sutherland_R/0/1/0/all/0/1&quot;&gt;Robert Sutherland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hain_T/0/1/0/all/0/1&quot;&gt;Thomas Hain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barker_J/0/1/0/all/0/1&quot;&gt;Jon Barker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goetze_S/0/1/0/all/0/1&quot;&gt;Stefan Goetze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ragni_A/0/1/0/all/0/1&quot;&gt;Anton Ragni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13613">
<title>Enhancing Image Retrieval : A Comprehensive Study on Photo Search using the CLIP Mode. (arXiv:2401.13613v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13613</link>
<description rdf:parseType="Literal">&lt;p&gt;Photo search, the task of retrieving images based on textual queries, has
witnessed significant advancements with the introduction of CLIP (Contrastive
Language-Image Pretraining) model. CLIP leverages a vision-language pre
training approach, wherein it learns a shared representation space for images
and text, enabling cross-modal understanding. This model demonstrates the
capability to understand the semantic relationships between diverse image and
text pairs, allowing for efficient and accurate retrieval of images based on
natural language queries. By training on a large-scale dataset containing
images and their associated textual descriptions, CLIP achieves remarkable
generalization, providing a powerful tool for tasks such as zero-shot learning
and few-shot classification. This abstract summarizes the foundational
principles of CLIP and highlights its potential impact on advancing the field
of photo search, fostering a seamless integration of natural language
understanding and computer vision for improved information retrieval in
multimedia applications
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lahajal_N/0/1/0/all/0/1&quot;&gt;Naresh Kumar Lahajal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+S_H/0/1/0/all/0/1&quot;&gt;Harini S&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13641">
<title>How Good is ChatGPT at Face Biometrics? A First Look into Recognition, Soft Biometrics, and Explainability. (arXiv:2401.13641v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13641</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) such as GPT developed by OpenAI, have already
shown astonishing results, introducing quick changes in our society. This has
been intensified by the release of ChatGPT which allows anyone to interact in a
simple conversational way with LLMs, without any experience in the field
needed. As a result, ChatGPT has been rapidly applied to many different tasks
such as code- and song-writer, education, virtual assistants, etc., showing
impressive results for tasks for which it was not trained (zero-shot learning).
&lt;/p&gt;
&lt;p&gt;The present study aims to explore the ability of ChatGPT, based on the recent
GPT-4 multimodal LLM, for the task of face biometrics. In particular, we
analyze the ability of ChatGPT to perform tasks such as face verification,
soft-biometrics estimation, and explainability of the results. ChatGPT could be
very valuable to further increase the explainability and transparency of the
automatic decisions in human scenarios. Experiments are carried out in order to
evaluate the performance and robustness of ChatGPT, using popular public
benchmarks and comparing the results with state-of-the-art methods in the
field. The results achieved in this study show the potential of LLMs such as
ChatGPT for face biometrics, especially to enhance explainability. For
reproducibility reasons, we release all the code in GitHub.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DeAndres_Tame_I/0/1/0/all/0/1&quot;&gt;Ivan DeAndres-Tame&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tolosana_R/0/1/0/all/0/1&quot;&gt;Ruben Tolosana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vera_Rodriguez_R/0/1/0/all/0/1&quot;&gt;Ruben Vera-Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1&quot;&gt;Aythami Morales&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1&quot;&gt;Julian Fierrez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortega_Garcia_J/0/1/0/all/0/1&quot;&gt;Javier Ortega-Garcia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13652">
<title>Graph-Informed Neural Networks for Sparse Grid-Based Discontinuity Detectors. (arXiv:2401.13652v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.13652</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a novel approach for detecting the discontinuity
interfaces of a discontinuous function. This approach leverages Graph-Informed
Neural Networks (GINNs) and sparse grids to address discontinuity detection
also in domains of dimension larger than 3. GINNs, trained to identify troubled
points on sparse grids, exploit graph structures built on the grids to achieve
efficient and accurate discontinuity detection performances. We also introduce
a recursive algorithm for general sparse grid-based detectors, characterized by
convergence properties and easy applicability. Numerical experiments on
functions with dimensions n = 2 and n = 4 demonstrate the efficiency and robust
generalization of GINNs in detecting discontinuity interfaces. Notably, the
trained GINNs offer portability and versatility, allowing integration into
various algorithms and sharing among users.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santa_F/0/1/0/all/0/1&quot;&gt;Francesco Della Santa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pieraccini_S/0/1/0/all/0/1&quot;&gt;Sandra Pieraccini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13657">
<title>Inadequacy of common stochastic neural networks for reliable clinical decision support. (arXiv:2401.13657v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.13657</link>
<description rdf:parseType="Literal">&lt;p&gt;Widespread adoption of AI for medical decision making is still hindered due
to ethical and safety-related concerns. For AI-based decision support systems
in healthcare settings it is paramount to be reliable and trustworthy. Common
deep learning approaches, however, have the tendency towards overconfidence
under data shift. Such inappropriate extrapolation beyond evidence-based
scenarios may have dire consequences. This highlights the importance of
reliable estimation of local uncertainty and its communication to the end user.
While stochastic neural networks have been heralded as a potential solution to
these issues, this study investigates their actual reliability in clinical
applications. We centered our analysis on the exemplary use case of mortality
prediction for ICU hospitalizations using EHR from MIMIC3 study. For
predictions on the EHR time series, Encoder-Only Transformer models were
employed. Stochasticity of model functions was achieved by incorporating common
methods such as Bayesian neural network layers and model ensembles. Our models
achieve state of the art performance in terms of discrimination performance
(AUC ROC: 0.868+-0.011, AUC PR: 0.554+-0.034) and calibration on the mortality
prediction benchmark. However, epistemic uncertainty is critically
underestimated by the selected stochastic deep learning methods. A heuristic
proof for the responsible collapse of the posterior distribution is provided.
Our findings reveal the inadequacy of commonly used stochastic deep learning
approaches to reliably recognize OoD samples. In both methods, unsubstantiated
model confidence is not prevented due to strongly biased functional posteriors,
rendering them inappropriate for reliable clinical decision support. This
highlights the need for approaches with more strictly enforced or inherent
distance-awareness to known data points, e.g., using kernel-based techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lindenmeyer_A/0/1/0/all/0/1&quot;&gt;Adrian Lindenmeyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blattmann_M/0/1/0/all/0/1&quot;&gt;Malte Blattmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Franke_S/0/1/0/all/0/1&quot;&gt;Stefan Franke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumuth_T/0/1/0/all/0/1&quot;&gt;Thomas Neumuth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_D/0/1/0/all/0/1&quot;&gt;Daniel Schneider&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13662">
<title>The Definitive Guide to Policy Gradients in Deep Reinforcement Learning: Theory, Algorithms and Implementations. (arXiv:2401.13662v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.13662</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, various powerful policy gradient algorithms have been
proposed in deep reinforcement learning. While all these algorithms build on
the Policy Gradient Theorem, the specific design choices differ significantly
across algorithms. We provide a holistic overview of on-policy policy gradient
algorithms to facilitate the understanding of both their theoretical
foundations and their practical implementations. In this overview, we include a
detailed proof of the continuous version of the Policy Gradient Theorem,
convergence results and a comprehensive discussion of practical algorithms. We
compare the most prominent algorithms on continuous control environments and
provide insights on the benefits of regularization. All code is available at
https://github.com/Matt00n/PolicyGradientsJax.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lehmann_M/0/1/0/all/0/1&quot;&gt;Matthias Lehmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.03203">
<title>A New Sentence Extraction Strategy for Unsupervised Extractive Summarization Methods. (arXiv:2112.03203v5 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2112.03203</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, text summarization methods have attracted much attention
again thanks to the researches on neural network models. Most of the current
text summarization methods based on neural network models are supervised
methods which need large-scale datasets. However, large-scale datasets are
difficult to obtain in practical applications. In this paper, we model the task
of extractive text summarization methods from the perspective of Information
Theory, and then describe the unsupervised extractive methods with a uniform
framework. To improve the feature distribution and to decrease the mutual
information of summarization sentences, we propose a new sentence extraction
strategy which can be applied to existing unsupervised extractive methods.
Experiments are carried out on different datasets, and results show that our
strategy is indeed effective and in line with expectations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dehao Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1&quot;&gt;Yingzhu Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhongliang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yongfeng Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.07467">
<title>AI Ethics Principles in Practice: Perspectives of Designers and Developers. (arXiv:2112.07467v7 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2112.07467</link>
<description rdf:parseType="Literal">&lt;p&gt;As consensus across the various published AI ethics principles is approached,
a gap remains between high-level principles and practical techniques that can
be readily adopted to design and develop responsible AI systems. We examine the
practices and experiences of researchers and engineers from Australia&apos;s
national scientific research agency (CSIRO), who are involved in designing and
developing AI systems for many application areas. Semi-structured interviews
were used to examine how the practices of the participants relate to and align
with a set of high-level AI ethics principles proposed by the Australian
Government. The principles comprise: (1) privacy protection and security, (2)
reliability and safety, (3) transparency and explainability, (4) fairness, (5)
contestability, (6) accountability, (7) human-centred values, (8) human, social
and environmental wellbeing. Discussions on the gained insights from the
interviews include various tensions and trade-offs between the principles, and
provide suggestions for implementing each high-level principle. We also present
suggestions aiming to enhance associated support mechanisms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanderson_C/0/1/0/all/0/1&quot;&gt;Conrad Sanderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Douglas_D/0/1/0/all/0/1&quot;&gt;David Douglas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1&quot;&gt;Qinghua Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schleiger_E/0/1/0/all/0/1&quot;&gt;Emma Schleiger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whittle_J/0/1/0/all/0/1&quot;&gt;Jon Whittle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lacey_J/0/1/0/all/0/1&quot;&gt;Justine Lacey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Newnham_G/0/1/0/all/0/1&quot;&gt;Glenn Newnham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hajkowicz_S/0/1/0/all/0/1&quot;&gt;Stefan Hajkowicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robinson_C/0/1/0/all/0/1&quot;&gt;Cathy Robinson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hansen_D/0/1/0/all/0/1&quot;&gt;David Hansen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.13883">
<title>Multi-modal Misinformation Detection: Approaches, Challenges and Opportunities. (arXiv:2203.13883v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2203.13883</link>
<description rdf:parseType="Literal">&lt;p&gt;As social media platforms are evolving from text-based forums into
multi-modal environments, the nature of misinformation in social media is also
transforming accordingly. Taking advantage of the fact that visual modalities
such as images and videos are more favorable and attractive to the users and
textual contents are sometimes skimmed carelessly, misinformation spreaders
have recently targeted contextual connections between the modalities e.g., text
and image. Hence many researchers have developed automatic techniques for
detecting possible cross-modal discordance in web-based content. We analyze,
categorize and identify existing approaches in addition to challenges and
shortcomings they face in order to unearth new research opportunities in the
field of multi-modal misinformation detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdali_S/0/1/0/all/0/1&quot;&gt;Sara Abdali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+shaham_S/0/1/0/all/0/1&quot;&gt;Sina shaham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnamachari_B/0/1/0/all/0/1&quot;&gt;Bhaskar Krishnamachari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.06009">
<title>Relative Policy-Transition Optimization for Fast Policy Transfer. (arXiv:2206.06009v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.06009</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of policy transfer between two Markov Decision
Processes (MDPs). We introduce a lemma based on existing theoretical results in
reinforcement learning to measure the relativity gap between two arbitrary
MDPs, that is the difference between any two cumulative expected returns
defined on different policies and environment dynamics. Based on this lemma, we
propose two new algorithms referred to as Relative Policy Optimization (RPO)
and Relative Transition Optimization (RTO), which offer fast policy transfer
and dynamics modelling, respectively. RPO transfers the policy evaluated in one
environment to maximize the return in another, while RTO updates the
parameterized dynamics model to reduce the gap between the dynamics of the two
environments. Integrating the two algorithms results in the complete Relative
Policy-Transition Optimization (RPTO) algorithm, in which the policy interacts
with the two environments simultaneously, such that data collections from two
environments, policy and transition updates are completed in one closed loop to
form a principled learning framework for policy transfer. We demonstrate the
effectiveness of RPTO on a set of MuJoCo continuous control tasks by creating
policy transfer problems via variant dynamics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiawei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Cheng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yizheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Baoxiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1&quot;&gt;Lei Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.14359">
<title>TE2Rules: Explaining Tree Ensembles using Rules. (arXiv:2206.14359v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.14359</link>
<description rdf:parseType="Literal">&lt;p&gt;Tree Ensemble (TE) models, such as Gradient Boosted Trees, often achieve
optimal performance on tabular datasets, yet their lack of transparency poses
challenges for comprehending their decision logic. This paper introduces
TE2Rules (Tree Ensemble to Rules), a novel approach for explaining binary
classification tree ensemble models through a list of rules, particularly
focusing on explaining the minority class. Many state-of-the-art explainers
struggle with minority class explanations, making TE2Rules valuable in such
cases. The rules generated by TE2Rules closely approximate the original model,
ensuring high fidelity, providing an accurate and interpretable means to
understand decision-making. Experimental results demonstrate that TE2Rules
scales effectively to tree ensembles with hundreds of trees, achieving higher
fidelity within runtimes comparable to baselines. TE2Rules allows for a
trade-off between runtime and fidelity, enhancing its practical applicability.
The implementation is available here: https://github.com/linkedin/TE2Rules.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lal_G/0/1/0/all/0/1&quot;&gt;G Roshan Lal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaotong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mithal_V/0/1/0/all/0/1&quot;&gt;Varun Mithal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.10227">
<title>Adversarial Detection by Approximation of Ensemble Boundary. (arXiv:2211.10227v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.10227</link>
<description rdf:parseType="Literal">&lt;p&gt;A new method of detecting adversarial attacks is proposed for an ensemble of
Deep Neural Networks (DNNs) solving two-class pattern recognition problems. The
ensemble is combined using Walsh coefficients which are capable of
approximating Boolean functions and thereby controlling the complexity of the
ensemble decision boundary. The hypothesis in this paper is that decision
boundaries with high curvature allow adversarial perturbations to be found, but
change the curvature of the decision boundary, which is then approximated in a
different way by Walsh coefficients compared to the clean images. By observing
the difference in Walsh coefficient approximation between clean and adversarial
images, it is shown experimentally that transferability of attack may be used
for detection. Furthermore, approximating the decision boundary may aid in
understanding the learning and transferability properties of DNNs. While the
experiments here use images, the proposed approach of modelling two-class
ensemble decision boundaries could in principle be applied to any application
area. Code for approximating Boolean functions using Walsh coefficients:
https://doi.org/10.24433/CO.3695905.v1
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Windeatt_T/0/1/0/all/0/1&quot;&gt;T. Windeatt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.14648">
<title>Digital Over-the-Air Federated Learning in Multi-Antenna Systems. (arXiv:2302.14648v2 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/2302.14648</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, the performance optimization of federated learning (FL), when
deployed over a realistic wireless multiple-input multiple-output (MIMO)
communication system with digital modulation and over-the-air computation
(AirComp) is studied. In particular, a MIMO system is considered in which edge
devices transmit their local FL models (trained using their locally collected
data) to a parameter server (PS) using beamforming to maximize the number of
devices scheduled for transmission. The PS, acting as a central controller,
generates a global FL model using the received local FL models and broadcasts
it back to all devices. Due to the limited bandwidth in a wireless network,
AirComp is adopted to enable efficient wireless data aggregation. However,
fading of wireless channels can produce aggregate distortions in an
AirComp-based FL scheme. To tackle this challenge, we propose a modified
federated averaging (FedAvg) algorithm that combines digital modulation with
AirComp to mitigate wireless fading while ensuring the communication
efficiency. This is achieved by a joint transmit and receive beamforming
design, which is formulated as an optimization problem to dynamically adjust
the beamforming matrices based on current FL model parameters so as to minimize
the transmitting error and ensure the FL performance. To achieve this goal, we
first analytically characterize how the beamforming matrices affect the
performance of the FedAvg in different iterations. Based on this relationship,
an artificial neural network (ANN) is used to estimate the local FL models of
all devices and adjust the beamforming matrices at the PS for future model
transmission. The algorithmic advantages and improved performance of the
proposed methodologies are demonstrated through extensive numerical
experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sihua Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mingzhe Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Cong Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1&quot;&gt;Changchuan Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brinton_C/0/1/0/all/0/1&quot;&gt;Christopher G. Brinton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.03468">
<title>Toward Practical Entity Alignment Method Design: Insights from New Highly Heterogeneous Knowledge Graph Datasets. (arXiv:2304.03468v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.03468</link>
<description rdf:parseType="Literal">&lt;p&gt;The flourishing of knowledge graph applications has driven the need for
entity alignment (EA) across KGs. However, the heterogeneity of practical KGs,
characterized by differing scales, structures, and limited overlapping
entities, greatly surpasses that of existing EA datasets. This discrepancy
highlights an oversimplified heterogeneity in current EA datasets, which
obstructs a full understanding of the advancements achieved by recent EA
methods. In this paper, we study the performance of EA methods in practical
settings, specifically focusing on the alignment of highly heterogeneous KGs
(HHKGs). Firstly, we address the oversimplified heterogeneity settings of
current datasets and propose two new HHKG datasets that closely mimic practical
EA scenarios. Then, based on these datasets, we conduct extensive experiments
to evaluate previous representative EA methods. Our findings reveal that, in
aligning HHKGs, valuable structure information can hardly be exploited through
message-passing and aggregation mechanisms. This phenomenon leads to inferior
performance of existing EA methods, especially those based on GNNs. These
findings shed light on the potential problems associated with the conventional
application of GNN-based methods as a panacea for all EA datasets.
Consequently, in light of these observations and to elucidate what EA
methodology is genuinely beneficial in practical scenarios, we undertake an
in-depth analysis by implementing a simple but effective approach: Simple-HHEA.
This method adaptly integrates entity name, structure, and temporal information
to navigate the challenges posed by HHKGs. Our experiment results conclude that
the key to the future EA model design in practice lies in their adaptability
and efficiency to varying information quality conditions, as well as their
capability to capture patterns across HHKGs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xuhui Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chengjin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yinghan Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuanzhuo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_F/0/1/0/all/0/1&quot;&gt;Fenglong Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1&quot;&gt;Fei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zixuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhichao Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jian Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Huawei Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15936">
<title>Learning DAGs from Data with Few Root Causes. (arXiv:2305.15936v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15936</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel perspective and algorithm for learning directed acyclic
graphs (DAGs) from data generated by a linear structural equation model (SEM).
First, we show that a linear SEM can be viewed as a linear transform that, in
prior work, computes the data from a dense input vector of random valued root
causes (as we will call them) associated with the nodes. Instead, we consider
the case of (approximately) few root causes and also introduce noise in the
measurement of the data. Intuitively, this means that the DAG data is produced
by few data-generating events whose effect percolates through the DAG. We prove
identifiability in this new setting and show that the true DAG is the global
minimizer of the $L^0$-norm of the vector of root causes. For data with few
root causes, with and without noise, we show superior performance compared to
prior DAG learning methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Misiakos_P/0/1/0/all/0/1&quot;&gt;Panagiotis Misiakos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wendler_C/0/1/0/all/0/1&quot;&gt;Chris Wendler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puschel_M/0/1/0/all/0/1&quot;&gt;Markus P&amp;#xfc;schel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08013">
<title>TopP&amp;R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models. (arXiv:2306.08013v6 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08013</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a robust and reliable evaluation metric for generative models by
introducing topological and statistical treatments for rigorous support
estimation. Existing metrics, such as Inception Score (IS), Frechet Inception
Distance (FID), and the variants of Precision and Recall (P&amp;amp;R), heavily rely on
supports that are estimated from sample features. However, the reliability of
their estimation has not been seriously discussed (and overlooked) even though
the quality of the evaluation entirely depends on it. In this paper, we propose
Topological Precision and Recall (TopP&amp;amp;R, pronounced &apos;topper&apos;), which provides
a systematic approach to estimating supports, retaining only topologically and
statistically important features with a certain level of confidence. This not
only makes TopP&amp;amp;R strong for noisy features, but also provides statistical
consistency. Our theoretical and experimental results show that TopP&amp;amp;R is
robust to outliers and non-independent and identically distributed (Non-IID)
perturbations, while accurately capturing the true trend of change in samples.
To the best of our knowledge, this is the first evaluation metric focused on
the robust estimation of the support and provides its statistical consistency
under noise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_P/0/1/0/all/0/1&quot;&gt;Pum Jun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1&quot;&gt;Yoojin Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jisu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1&quot;&gt;Jaejun Yoo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00527">
<title>Graph Neural Networks based Log Anomaly Detection and Explanation. (arXiv:2307.00527v3 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00527</link>
<description rdf:parseType="Literal">&lt;p&gt;Event logs are widely used to record the status of high-tech systems, making
log anomaly detection important for monitoring those systems. Most existing log
anomaly detection methods take a log event count matrix or log event sequences
as input, exploiting quantitative and/or sequential relationships between log
events to detect anomalies. Unfortunately, only considering quantitative or
sequential relationships may result in low detection accuracy. To alleviate
this problem, we propose a graph-based method for unsupervised log anomaly
detection, dubbed Logs2Graphs, which first converts event logs into attributed,
directed, and weighted graphs, and then leverages graph neural networks to
perform graph-level anomaly detection. Specifically, we introduce One-Class
Digraph Inception Convolutional Networks, abbreviated as OCDiGCN, a novel graph
neural network model for detecting graph-level anomalies in a collection of
attributed, directed, and weighted graphs. By coupling the graph representation
and anomaly detection steps, OCDiGCN can learn a representation that is
especially suited for anomaly detection, resulting in a high detection
accuracy. Importantly, for each identified anomaly, we additionally provide a
small subset of nodes that play a crucial role in OCDiGCN&apos;s prediction as
explanations, which can offer valuable cues for subsequent root cause
diagnosis. Experiments on five benchmark datasets show that Logs2Graphs
performs at least on par with state-of-the-art log anomaly detection methods on
simple datasets while largely outperforming state-of-the-art log anomaly
detection methods on complicated datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jiayang Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leeuwen_M/0/1/0/all/0/1&quot;&gt;Matthijs van Leeuwen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02637">
<title>Surge Routing: Event-informed Multiagent Reinforcement Learning for Autonomous Rideshare. (arXiv:2307.02637v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02637</link>
<description rdf:parseType="Literal">&lt;p&gt;Large events such as conferences, concerts and sports games, often cause
surges in demand for ride services that are not captured in average demand
patterns, posing unique challenges for routing algorithms. We propose a
learning framework for an autonomous fleet of taxis that leverages event data
from the internet to predict demand surges and generate cooperative routing
policies. We achieve this through a combination of two major components: (i) a
demand prediction framework that uses textual event information in the form of
events&apos; descriptions and reviews to predict event-driven demand surges over
street intersections, and (ii) a scalable multiagent reinforcement learning
framework that leverages demand predictions and uses one-agent-at-a-time
rollout combined with limited sampling certainty equivalence to learn
intersection-level routing policies. For our experimental results we consider
real NYC ride share data for the year 2022 and information for more than 2000
events across 300 unique venues in Manhattan. We test our approach with a fleet
of 100 taxis on a map with 2235 street intersections. Our experimental results
demonstrate that our method learns routing policies that reduce wait time
overhead per serviced request by 25% to 75%, while picking up 1% to 4% more
requests than other model-based RL frameworks and classical methods in
operations research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garces_D/0/1/0/all/0/1&quot;&gt;Daniel Garces&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gil_S/0/1/0/all/0/1&quot;&gt;Stephanie Gil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06082">
<title>VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View. (arXiv:2307.06082v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06082</link>
<description rdf:parseType="Literal">&lt;p&gt;Incremental decision making in real-world environments is one of the most
challenging tasks in embodied artificial intelligence. One particularly
demanding scenario is Vision and Language Navigation~(VLN) which requires
visual and natural language understanding as well as spatial and temporal
reasoning capabilities. The embodied agent needs to ground its understanding of
navigation instructions in observations of a real-world environment like Street
View. Despite the impressive results of LLMs in other research areas, it is an
ongoing problem of how to best connect them with an interactive visual
environment. In this work, we propose VELMA, an embodied LLM agent that uses a
verbalization of the trajectory and of visual environment observations as
contextual prompt for the next action. Visual information is verbalized by a
pipeline that extracts landmarks from the human written navigation instructions
and uses CLIP to determine their visibility in the current panorama view. We
show that VELMA is able to successfully follow navigation instructions in
Street View with only two in-context examples. We further finetune the LLM
agent on a few thousand examples and achieve 25%-30% relative improvement in
task completion over the previous state-of-the-art for two datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schumann_R/0/1/0/all/0/1&quot;&gt;Raphael Schumann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wanrong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1&quot;&gt;Weixi Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_T/0/1/0/all/0/1&quot;&gt;Tsu-Jui Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1&quot;&gt;Stefan Riezler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;William Yang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.12547">
<title>Knapsack: Connectedness, Path, and Shortest-Path. (arXiv:2307.12547v4 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/2307.12547</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the knapsack problem with graph theoretic constraints. That is, we
assume that there exists a graph structure on the set of items of knapsack and
the solution also needs to satisfy certain graph theoretic properties on top of
knapsack constraints. In particular, we need to compute in the connected
knapsack problem a connected subset of items which has maximum value subject to
the size of knapsack constraint. We show that this problem is strongly
NP-complete even for graphs of maximum degree four and NP-complete even for
star graphs. On the other hand, we develop an algorithm running in time
$O\left(2^{tw\log tw}\cdot\text{poly}(\min\{s^2,d^2\})\right)$ where $tw,s,d$
are respectively treewidth of the graph, size, and target value of the
knapsack. We further exhibit a $(1-\epsilon)$ factor approximation algorithm
running in time $O\left(2^{tw\log tw}\cdot\text{poly}(n,1/\epsilon)\right)$ for
every $\epsilon&amp;gt;0$. We show similar results for several other graph theoretic
properties, namely path and shortest-path under the problem names path-knapsack
and shortestpath-knapsack. Our results seems to indicate that
connected-knapsack is computationally hardest followed by path-knapsack and
shortestpath-knapsack.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dey_P/0/1/0/all/0/1&quot;&gt;Palash Dey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolay_S/0/1/0/all/0/1&quot;&gt;Sudeshna Kolay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Sipra Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.16706">
<title>Continuous-Time Distributed Dynamic Programming for Networked Multi-Agent Markov Decision Processes. (arXiv:2307.16706v5 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2307.16706</link>
<description rdf:parseType="Literal">&lt;p&gt;The main goal of this paper is to investigate continuous-time distributed
dynamic programming (DP) algorithms for networked multi-agent Markov decision
problems (MAMDPs). In our study, we adopt a distributed multi-agent framework
where individual agents have access only to their own rewards, lacking insights
into the rewards of other agents. Moreover, each agent has the ability to share
its parameters with neighboring agents through a communication network,
represented by a graph. We first introduce a novel distributed DP, inspired by
the distributed optimization method of Wang and Elia. Next, a new distributed
DP is introduced through a decoupling process. The convergence of the DP
algorithms is proved through systems and control perspectives. The study in
this paper sets the stage for new distributed temporal different learning
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Donghwan Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lim_H/0/1/0/all/0/1&quot;&gt;Han-Dong Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Do Wan Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12539">
<title>CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias. (arXiv:2308.12539v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12539</link>
<description rdf:parseType="Literal">&lt;p&gt;As language models (LMs) become increasingly powerful and widely used, it is
important to quantify them for sociodemographic bias with potential for harm.
Prior measures of bias are sensitive to perturbations in the templates designed
to compare performance across social groups, due to factors such as low
diversity or limited number of templates. Also, most previous work considers
only one NLP task. We introduce Comprehensive Assessment of Language Models
(CALM) for robust measurement of two types of universally relevant
sociodemographic bias, gender and race. CALM integrates sixteen datasets for
question-answering, sentiment analysis and natural language inference. Examples
from each dataset are filtered to produce 224 templates with high diversity
(e.g., length, vocabulary). We assemble 50 highly frequent person names for
each of seven distinct demographic groups to generate 78,400 prompts covering
the three NLP tasks. Our empirical evaluation shows that CALM bias scores are
more robust and far less sensitive than previous bias measurements to
perturbations in the templates, such as synonym substitution, or to random
subset selection of templates. We apply CALM to 20 large language models, and
find that for 2 language model series, larger parameter models tend to be more
biased than smaller ones. The T0 series is the least biased model families, of
the 20 LLMs investigated here. The code is available at
https://github.com/vipulgupta1011/CALM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1&quot;&gt;Vipul Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkit_P/0/1/0/all/0/1&quot;&gt;Pranav Narayanan Venkit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laurencon_H/0/1/0/all/0/1&quot;&gt;Hugo Lauren&amp;#xe7;on&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_S/0/1/0/all/0/1&quot;&gt;Shomir Wilson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Passonneau_R/0/1/0/all/0/1&quot;&gt;Rebecca J. Passonneau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.15812">
<title>Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models. (arXiv:2308.15812v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.15812</link>
<description rdf:parseType="Literal">&lt;p&gt;Aligning large language models (LLMs) with human values and intents
critically involves the use of human or AI feedback. While dense feedback
annotations are expensive to acquire and integrate, sparse feedback presents a
structural design choice between ratings (e.g., score Response A on a scale of
1-7) and rankings (e.g., is Response A better than Response B?). In this work,
we analyze the effect of this design choice for the alignment and evaluation of
LLMs. We uncover an inconsistency problem wherein the preferences inferred from
ratings and rankings significantly disagree 60% for both human and AI
annotators. Our subsequent analysis identifies various facets of annotator
biases that explain this phenomena, such as human annotators would rate denser
responses higher while preferring accuracy during pairwise judgments. To our
surprise, we also observe that the choice of feedback protocol also has a
significant effect on the evaluation of aligned LLMs. In particular, we find
that LLMs that leverage rankings data for alignment (say model X) are preferred
over those that leverage ratings data (say model Y), with a rank-based
evaluation protocol (is X/Y&apos;s response better than reference response?) but not
with a rating-based evaluation protocol (score Rank X/Y&apos;s response on a scale
of 1-7). Our findings thus shed light on critical gaps in methods for
evaluating the real-world utility of language models and their strong
dependence on the feedback protocol used for alignment. Our code and data are
available at https://github.com/Hritikbansal/sparse_feedback.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_H/0/1/0/all/0/1&quot;&gt;Hritik Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dang_J/0/1/0/all/0/1&quot;&gt;John Dang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1&quot;&gt;Aditya Grover&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.08565">
<title>How Transferable are Attribute Controllers on Pretrained Multilingual Translation Models?. (arXiv:2309.08565v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.08565</link>
<description rdf:parseType="Literal">&lt;p&gt;Customizing machine translation models to comply with desired attributes
(e.g., formality or grammatical gender) is a well-studied topic. However, most
current approaches rely on (semi-)supervised data with attribute annotations.
This data scarcity bottlenecks democratizing such customization possibilities
to a wider range of languages, particularly lower-resource ones. This gap is
out of sync with recent progress in pretrained massively multilingual
translation models. In response, we transfer the attribute controlling
capabilities to languages without attribute-annotated data with an NLLB-200
model as a foundation. Inspired by techniques from controllable generation, we
employ a gradient-based inference-time controller to steer the pretrained
model. The controller transfers well to zero-shot conditions, as it operates on
pretrained multilingual representations and is attribute -- rather than
language-specific. With a comprehensive comparison to finetuning-based control,
we demonstrate that, despite finetuning&apos;s clear dominance in supervised
settings, the gap to inference-time control closes when moving to zero-shot
conditions, especially with new and distant target languages. The latter also
shows stronger domain robustness. We further show that our inference-time
control complements finetuning. A human evaluation on a real low-resource
language, Bengali, confirms our findings. Our code is
https://github.com/dannigt/attribute-controller-transfer
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Danni Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niehues_J/0/1/0/all/0/1&quot;&gt;Jan Niehues&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14808">
<title>Revisiting Softmax Masking: Stop Gradient for Enhancing Stability in Replay-based Continual Learning. (arXiv:2309.14808v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14808</link>
<description rdf:parseType="Literal">&lt;p&gt;In replay-based methods for continual learning, replaying input samples in
episodic memory has shown its effectiveness in alleviating catastrophic
forgetting. However, the potential key factor of cross-entropy loss with
softmax in causing catastrophic forgetting has been underexplored. In this
paper, we analyze the effect of softmax and revisit softmax masking with
negative infinity to shed light on its ability to mitigate catastrophic
forgetting. Based on the analyses, it is found that negative infinity masked
softmax is not always compatible with dark knowledge. To improve the
compatibility, we propose a general masked softmax that controls the stability
by adjusting the gradient scale to old and new classes. We demonstrate that
utilizing our method on other replay-based methods results in better
performance, primarily by enhancing model stability in continual learning
benchmarks, even when the buffer size is set to an extremely small value.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hoyong Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_M/0/1/0/all/0/1&quot;&gt;Minchan Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kangil Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.17249">
<title>Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering. (arXiv:2309.17249v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.17249</link>
<description rdf:parseType="Literal">&lt;p&gt;Prompting and in-context learning (ICL) have become efficient learning
paradigms for large language models (LLMs). However, LLMs suffer from prompt
brittleness and various bias factors in the prompt, including but not limited
to the formatting, the choice verbalizers, and the ICL examples. To address
this problem that results in unexpected performance degradation, calibration
methods have been developed to mitigate the effects of these biases while
recovering LLM performance. In this work, we first conduct a systematic
analysis of the existing calibration methods, where we both provide a unified
view and reveal the failure cases. Inspired by these analyses, we propose Batch
Calibration (BC), a simple yet intuitive method that controls the contextual
bias from the batched input, unifies various prior approaches, and effectively
addresses the aforementioned issues. BC is zero-shot, inference-only, and
incurs negligible additional costs. In the few-shot setup, we further extend BC
to allow it to learn the contextual bias from labeled data. We validate the
effectiveness of BC with PaLM 2-(S, M, L) and CLIP models and demonstrate
state-of-the-art performance over previous calibration baselines across more
than 10 natural language understanding and image classification tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Han Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1&quot;&gt;Xingchen Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Proleev_L/0/1/0/all/0/1&quot;&gt;Lev Proleev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mincu_D/0/1/0/all/0/1&quot;&gt;Diana Mincu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jilin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heller_K/0/1/0/all/0/1&quot;&gt;Katherine Heller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1&quot;&gt;Subhrajit Roy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08535">
<title>Formally Specifying the High-Level Behavior of LLM-Based Agents. (arXiv:2310.08535v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08535</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous, goal-driven agents powered by LLMs have recently emerged as
promising tools for solving challenging problems without the need for
task-specific finetuned models that can be expensive to procure. Currently, the
design and implementation of such agents is ad hoc, as the wide variety of
tasks that LLM-based agents may be applied to naturally means there can be no
one-size-fits-all approach to agent design. In this work we aim to alleviate
the difficulty of designing and implementing new agents by proposing a
minimalistic generation framework that simplifies the process of building
agents. The framework we introduce allows the user to define desired agent
behaviors in a high-level, declarative specification that is then used to
construct a decoding monitor which guarantees the LLM will produce an output
exhibiting the desired behavior. Our declarative approach, in which the
behavior is described without concern for how it should be implemented or
enforced, enables rapid design, implementation, and experimentation with
different LLM-based agents. We demonstrate how the proposed framework can be
used to implement recent LLM-based agents (e.g., ReACT), and show how the
flexibility of our approach can be leveraged to define a new agent with more
complex behavior, the Plan-Act-Summarize-Solve (PASS) agent. Lastly, we
demonstrate that our method outperforms other agents on multiple popular
reasoning-centric question-answering benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crouse_M/0/1/0/all/0/1&quot;&gt;Maxwell Crouse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdelaziz_I/0/1/0/all/0/1&quot;&gt;Ibrahim Abdelaziz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Astudillo_R/0/1/0/all/0/1&quot;&gt;Ramon Astudillo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basu_K/0/1/0/all/0/1&quot;&gt;Kinjal Basu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dan_S/0/1/0/all/0/1&quot;&gt;Soham Dan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumaravel_S/0/1/0/all/0/1&quot;&gt;Sadhana Kumaravel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fokoue_A/0/1/0/all/0/1&quot;&gt;Achille Fokoue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kapanipathi_P/0/1/0/all/0/1&quot;&gt;Pavan Kapanipathi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roukos_S/0/1/0/all/0/1&quot;&gt;Salim Roukos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lastras_L/0/1/0/all/0/1&quot;&gt;Luis Lastras&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13361">
<title>Applying Large Language Models to Power Systems: Potential Security Threats. (arXiv:2311.13361v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13361</link>
<description rdf:parseType="Literal">&lt;p&gt;Applying large language models (LLMs) to modern power systems presents a
promising avenue for enhancing decision-making and operational efficiency.
However, this action may also incur potential security threats, which have not
been fully recognized so far. To this end, this article analyzes potential
threats incurred by applying LLMs to power systems, emphasizing the need for
urgent research and development of countermeasures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruan_J/0/1/0/all/0/1&quot;&gt;Jiaqi Ruan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_G/0/1/0/all/0/1&quot;&gt;Gaoqi Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Huan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guolong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xianzhuo Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1&quot;&gt;Jing Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1&quot;&gt;Fushuan Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1&quot;&gt;Zhao Yang Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13541">
<title>Linear Log-Normal Attention with Unbiased Concentration. (arXiv:2311.13541v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13541</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer models have achieved remarkable results in a wide range of
applications. However, their scalability is hampered by the quadratic time and
memory complexity of the self-attention mechanism concerning the sequence
length. This limitation poses a substantial obstacle when dealing with long
documents or high-resolution images. In this work, we study the self-attention
mechanism by analyzing the distribution of the attention matrix and its
concentration ability. Furthermore, we propose instruments to measure these
quantities and introduce a novel self-attention mechanism, Linear Log-Normal
Attention, designed to emulate the distribution and concentration behavior of
the original self-attention. Our experimental results on popular natural
language benchmarks reveal that our proposed Linear Log-Normal Attention
outperforms other linearized attention alternatives, offering a promising
avenue for enhancing the scalability of transformer models. Our code is
available in supplementary materials.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nahshan_Y/0/1/0/all/0/1&quot;&gt;Yury Nahshan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kampeas_J/0/1/0/all/0/1&quot;&gt;Joseph Kampeas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haleva_E/0/1/0/all/0/1&quot;&gt;Emir Haleva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06717">
<title>Privacy Issues in Large Language Models: A Survey. (arXiv:2312.06717v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06717</link>
<description rdf:parseType="Literal">&lt;p&gt;This is the first survey of the active area of AI research that focuses on
privacy issues in Large Language Models (LLMs). Specifically, we focus on work
that red-teams models to highlight privacy risks, attempts to build privacy
into the training or inference process, enables efficient data deletion from
trained models to comply with existing privacy regulations, and tries to
mitigate copyright issues. Our focus is on summarizing technical research that
develops algorithms, proves theorems, and runs empirical evaluations. While
there is an extensive body of legal and policy work addressing these challenges
from a different angle, that is not the focus of our survey. Nevertheless,
these works, along with recent legal developments do inform how these technical
problems are formalized, and so we discuss them briefly in Section 1. While we
have made our best effort to include all the relevant work, due to the fast
moving nature of this research we may have missed some recent work. If we have
missed some of your work please contact us, as we will attempt to keep this
survey relatively up to date. We are maintaining a repository with the list of
papers covered in this survey and any relevant code that was publicly available
at https://github.com/safr-ml-lab/survey-llm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neel_S/0/1/0/all/0/1&quot;&gt;Seth Neel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_P/0/1/0/all/0/1&quot;&gt;Peter Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08917">
<title>An Incremental Unified Framework for Small Defect Inspection. (arXiv:2312.08917v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08917</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial Intelligence (AI)-driven defect inspection is pivotal in
industrial manufacturing. Yet, many methods, tailored to specific pipelines,
grapple with diverse product portfolios and evolving processes. Addressing
this, we present the Incremental Unified Framework (IUF), which can reduce the
feature conflict problem when continuously integrating new objects in the
pipeline, making it advantageous in object-incremental learning scenarios.
Employing a state-of-the-art transformer, we introduce Object-Aware
Self-Attention (OASA) to delineate distinct semantic boundaries. Semantic
Compression Loss (SCL) is integrated to optimize non-primary semantic space,
enhancing network adaptability for novel objects. Additionally, we prioritize
retaining the features of established objects during weight updates.
Demonstrating prowess in both image and pixel-level defect inspection, our
approach achieves state-of-the-art performance, proving indispensable for
dynamic and scalable industrial inspections. Our code will be released at
\url{https://github.com/jqtangust/IUF}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiaqi Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Hao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaogang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1&quot;&gt;Ruizheng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Sixing Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_T/0/1/0/all/0/1&quot;&gt;Tsz Wa Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_M/0/1/0/all/0/1&quot;&gt;Ming Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Ying-Cong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsung_F/0/1/0/all/0/1&quot;&gt;Fugee Tsung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09442">
<title>A Compact LSTM-SVM Fusion Model for Long-Duration Cardiovascular Diseases Detection. (arXiv:2312.09442v2 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/2312.09442</link>
<description rdf:parseType="Literal">&lt;p&gt;Globally, cardiovascular diseases (CVDs) are the leading cause of mortality,
accounting for an estimated 17.9 million deaths annually. One critical clinical
objective is the early detection of CVDs using electrocardiogram (ECG) data, an
area that has received significant attention from the research community.
Recent advancements based on machine learning and deep learning have achieved
great progress in this domain. However, existing methodologies exhibit inherent
limitations, including inappropriate model evaluations and instances of data
leakage. In this study, we present a streamlined workflow paradigm for
preprocessing ECG signals into consistent 10-second durations, eliminating the
need for manual feature extraction/beat detection. We also propose a hybrid
model of Long Short-Term Memory (LSTM) with Support Vector Machine (SVM) for
fraud detection. This architecture consists of two LSTM layers and an SVM
classifier, which achieves a SOTA results with an Average precision score of
0.9402 on the MIT-BIH arrhythmia dataset and 0.9563 on the MIT-BIH atrial
fibrillation dataset. Based on the results, we believe our method can
significantly benefit the early detection and management of CVDs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Siyang Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10369">
<title>Proportional Representation in Metric Spaces and Low-Distortion Committee Selection. (arXiv:2312.10369v2 [cs.GT] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10369</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a novel definition for a small set R of k points being
&quot;representative&quot; of a larger set in a metric space. Given a set V (e.g.,
documents or voters) to represent, and a set C of possible representatives, our
criterion requires that for any subset S comprising a theta fraction of V, the
average distance of S to their best theta*k points in R should not be more than
a factor gamma compared to their average distance to the best theta*k points
among all of C. This definition is a strengthening of proportional fairness and
core fairness, but - different from those notions - requires that large
cohesive clusters be represented proportionally to their size.
&lt;/p&gt;
&lt;p&gt;Since there are instances for which - unless gamma is polynomially large - no
solutions exist, we study this notion in a resource augmentation framework,
implicitly stating the constraints for a set R of size k as though its size
were only k/alpha, for alpha &amp;gt; 1. Furthermore, motivated by the application to
elections, we mostly focus on the &quot;ordinal&quot; model, where the algorithm does not
learn the actual distances; instead, it learns only for each point v in V and
each candidate pairs c, c&apos; which of c, c&apos; is closer to v. Our main result is
that the Expanding Approvals Rule (EAR) of Aziz and Lee is (alpha, gamma)
representative with gamma &amp;lt;= 1 + 6.71 * (alpha)/(alpha-1).
&lt;/p&gt;
&lt;p&gt;Our results lead to three notable byproducts. First, we show that the EAR
achieves constant proportional fairness in the ordinal model, giving the first
positive result on metric proportional fairness with ordinal information.
Second, we show that for the core fairness objective, the EAR achieves the same
asymptotic tradeoff between resource augmentation and approximation as the
recent results of Li et al., which used full knowledge of the metric. Finally,
our results imply a very simple single-winner voting rule with metric
distortion at most 44.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalayci_Y/0/1/0/all/0/1&quot;&gt;Yusuf Hakan Kalayci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kempe_D/0/1/0/all/0/1&quot;&gt;David Kempe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kher_V/0/1/0/all/0/1&quot;&gt;Vikram Kher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00496">
<title>SAR-RARP50: Segmentation of surgical instrumentation and Action Recognition on Robot-Assisted Radical Prostatectomy Challenge. (arXiv:2401.00496v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00496</link>
<description rdf:parseType="Literal">&lt;p&gt;Surgical tool segmentation and action recognition are fundamental building
blocks in many computer-assisted intervention applications, ranging from
surgical skills assessment to decision support systems. Nowadays,
learning-based action recognition and segmentation approaches outperform
classical methods, relying, however, on large, annotated datasets. Furthermore,
action recognition and tool segmentation algorithms are often trained and make
predictions in isolation from each other, without exploiting potential
cross-task relationships. With the EndoVis 2022 SAR-RARP50 challenge, we
release the first multimodal, publicly available, in-vivo, dataset for surgical
action recognition and semantic instrumentation segmentation, containing 50
suturing video segments of Robotic Assisted Radical Prostatectomy (RARP). The
aim of the challenge is twofold. First, to enable researchers to leverage the
scale of the provided dataset and develop robust and highly accurate
single-task action recognition and tool segmentation approaches in the surgical
domain. Second, to further explore the potential of multitask-based learning
approaches and determine their comparative advantage against their single-task
counterparts. A total of 12 teams participated in the challenge, contributing 7
action recognition methods, 9 instrument segmentation techniques, and 4
multitask approaches that integrated both action recognition and instrument
segmentation. The complete SAR-RARP50 dataset is available at:
https://rdr.ucl.ac.uk/projects/SARRARP50_Segmentation_of_surgical_instrumentation_and_Action_Recognition_on_Robot-Assisted_Radical_Prostatectomy_Challenge/191091
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Psychogyios_D/0/1/0/all/0/1&quot;&gt;Dimitrios Psychogyios&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colleoni_E/0/1/0/all/0/1&quot;&gt;Emanuele Colleoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amsterdam_B/0/1/0/all/0/1&quot;&gt;Beatrice Van Amsterdam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chih-Yang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shu-Yu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuchong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_F/0/1/0/all/0/1&quot;&gt;Fucang Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_B/0/1/0/all/0/1&quot;&gt;Baosheng Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guotai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boels_M/0/1/0/all/0/1&quot;&gt;Maxence Boels&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huo_J/0/1/0/all/0/1&quot;&gt;Jiayu Huo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sparks_R/0/1/0/all/0/1&quot;&gt;Rachel Sparks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dasgupta_P/0/1/0/all/0/1&quot;&gt;Prokar Dasgupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Granados_A/0/1/0/all/0/1&quot;&gt;Alejandro Granados&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ourselin_S/0/1/0/all/0/1&quot;&gt;Sebastien Ourselin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Mengya Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1&quot;&gt;An Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yanan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1&quot;&gt;Long Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1&quot;&gt;Hongliang Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamada_A/0/1/0/all/0/1&quot;&gt;Atsushi Yamada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harai_Y/0/1/0/all/0/1&quot;&gt;Yuriko Harai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ishikawa_Y/0/1/0/all/0/1&quot;&gt;Yuto Ishikawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayashi_K/0/1/0/all/0/1&quot;&gt;Kazuyuki Hayashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simoens_J/0/1/0/all/0/1&quot;&gt;Jente Simoens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DeBacker_P/0/1/0/all/0/1&quot;&gt;Pieter DeBacker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cisternino_F/0/1/0/all/0/1&quot;&gt;Francesco Cisternino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furnari_G/0/1/0/all/0/1&quot;&gt;Gabriele Furnari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mottrie_A/0/1/0/all/0/1&quot;&gt;Alex Mottrie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferraguti_F/0/1/0/all/0/1&quot;&gt;Federica Ferraguti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kondo_S/0/1/0/all/0/1&quot;&gt;Satoshi Kondo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasai_S/0/1/0/all/0/1&quot;&gt;Satoshi Kasai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hirasawa_K/0/1/0/all/0/1&quot;&gt;Kousuke Hirasawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Soohee Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seung Hyun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kyu Eun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_H/0/1/0/all/0/1&quot;&gt;Hyoun-Joong Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1&quot;&gt;Kui Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1&quot;&gt;Shan An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krell_S/0/1/0/all/0/1&quot;&gt;Stefanie Krell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bodenstedt_S/0/1/0/all/0/1&quot;&gt;Sebastian Bodenstedt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayobi_N/0/1/0/all/0/1&quot;&gt;Nicolas Ayobi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_A/0/1/0/all/0/1&quot;&gt;Alejandra Perez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_S/0/1/0/all/0/1&quot;&gt;Santiago Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puentes_J/0/1/0/all/0/1&quot;&gt;Juanita Puentes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1&quot;&gt;Pablo Arbelaez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohareri_O/0/1/0/all/0/1&quot;&gt;Omid Mohareri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1&quot;&gt;Danail Stoyanov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02981">
<title>Fine-tuning and Utilization Methods of Domain-specific LLMs. (arXiv:2401.02981v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.02981</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent releases of pre-trained Large Language Models (LLMs) have gained
considerable traction, yet research on fine-tuning and employing
domain-specific LLMs remains scarce. This study investigates approaches for
fine-tuning and leveraging domain-specific LLMs, highlighting trends in LLMs,
foundational models, and methods for domain-specific pre-training. Focusing on
the financial sector, it details dataset selection, preprocessing, model
choice, and considerations crucial for LLM fine-tuning in finance. Addressing
the unique characteristics of financial data, the study explores the
construction of domain-specific vocabularies and considerations for security
and regulatory compliance. In the practical application of LLM fine-tuning, the
study outlines the procedure and implementation for generating domain-specific
LLMs in finance. Various financial cases, including stock price prediction,
sentiment analysis of financial news, automated document processing, research,
information extraction, and customer service enhancement, are exemplified. The
study explores the potential of LLMs in the financial domain, identifies
limitations, and proposes directions for improvement, contributing valuable
insights for future research. Ultimately, it advances natural language
processing technology in business, suggesting proactive LLM utilization in
financial services across industries.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_C/0/1/0/all/0/1&quot;&gt;Cheonsu Jeong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05535">
<title>Improving the Accuracy and Interpretability of Random Forests via Forest Pruning. (arXiv:2401.05535v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2401.05535</link>
<description rdf:parseType="Literal">&lt;p&gt;Decades after their inception, random forests continue to provide
state-of-the-art accuracy in a variety of learning problems, outperforming in
this respect alternative machine learning algorithms such as decision trees or
even neural networks. However, being an ensemble method, the one aspect where
random forests tend to severely underperform decision trees is
interpretability. In the present work, we propose a post-hoc approach that aims
to have the best of both worlds: the accuracy of random forests and the
interpretability of decision trees. To this end, we present two forest-pruning
methods to find an optimal sub-forest within a given random forest, and then,
when applicable, combine the selected trees into one. Our first method relies
on constrained exhaustive search, while our second method is based on an
adaptation of the LASSO methodology. Extensive experiments over synthetic and
real world datasets show that, in the majority of scenarios, at least one of
the two methods proposed is more accurate than the original random forest,
while just using a small fraction of the trees, aiding result interpretability.
Compared to current state-of-the-art forest pruning methods, namely sequential
forward selection and (a variation of) sequential backward selection, our
methods tend to outperform both of them, whether in terms of accuracy, number
of trees employed, or both.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dorador_A/0/1/0/all/0/1&quot;&gt;Albert Dorador&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05975">
<title>Online Differentiable Clustering for Intent Learning in Recommendation. (arXiv:2401.05975v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2401.05975</link>
<description rdf:parseType="Literal">&lt;p&gt;Mining users&apos; intents plays a crucial role in sequential recommendation. The
recent approach, ICLRec, was introduced to extract underlying users&apos; intents
using contrastive learning and clustering. While it has shown effectiveness,
the existing method suffers from complex and cumbersome alternating
optimization, leading to two main issues. Firstly, the separation of
representation learning and clustering optimization within a generalized
expectation maximization (EM) framework often results in sub-optimal
performance. Secondly, performing clustering on the entire dataset hampers
scalability for large-scale industry data. To address these challenges, we
propose a novel intent learning method called \underline{ODCRec}, which
integrates representation learning into an \underline{O}nline
\underline{D}ifferentiable \underline{C}lustering framework for
\underline{Rec}ommendation. Specifically, we encode users&apos; behavior sequences
and initialize the cluster centers as differentiable network parameters.
Additionally, we design a clustering loss that guides the networks to
differentiate between different cluster centers and pull similar samples
towards their respective cluster centers. This allows simultaneous optimization
of recommendation and clustering using mini-batch data. Moreover, we leverage
the learned cluster centers as self-supervision signals for representation
learning, resulting in further enhancement of recommendation performance.
Extensive experiments conducted on open benchmarks and industry data validate
the superiority, effectiveness, and efficiency of our proposed ODCRec method.
Code is available at: https://github.com/yueliu1999/ELCRec.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yue Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Shihao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1&quot;&gt;Jun Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yingwei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jian Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1&quot;&gt;Wenliang Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guannan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kejun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinwang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06373">
<title>How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs. (arXiv:2401.06373v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.06373</link>
<description rdf:parseType="Literal">&lt;p&gt;Most traditional AI safety research has approached AI models as machines and
centered on algorithm-focused attacks developed by security experts. As large
language models (LLMs) become increasingly common and competent, non-expert
users can also impose risks during daily interactions. This paper introduces a
new perspective to jailbreak LLMs as human-like communicators, to explore this
overlooked intersection between everyday language interaction and AI safety.
Specifically, we study how to persuade LLMs to jailbreak them. First, we
propose a persuasion taxonomy derived from decades of social science research.
Then, we apply the taxonomy to automatically generate interpretable persuasive
adversarial prompts (PAP) to jailbreak LLMs. Results show that persuasion
significantly increases the jailbreak performance across all risk categories:
PAP consistently achieves an attack success rate of over $92\%$ on Llama 2-7b
Chat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent algorithm-focused
attacks. On the defense side, we explore various mechanisms against PAP and,
found a significant gap in existing defenses, and advocate for more fundamental
mitigation for highly interactive LLMs
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1&quot;&gt;Yi Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1&quot;&gt;Hongpeng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jingwen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Diyi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1&quot;&gt;Ruoxi Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1&quot;&gt;Weiyan Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06461">
<title>Between Lines of Code: Unraveling the Distinct Patterns of Machine and Human Programmers. (arXiv:2401.06461v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2401.06461</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models have catalyzed an unprecedented wave in code
generation. While achieving significant advances, they blur the distinctions
between machine-and human-authored source code, causing integrity and
authenticity issues of software artifacts. Previous methods such as DetectGPT
have proven effective in discerning machine-generated texts, but they do not
identify and harness the unique patterns of machine-generated code. Thus, its
applicability falters when applied to code. In this paper, we carefully study
the specific patterns that characterize machine and human-authored code.
Through a rigorous analysis of code attributes such as length, lexical
diversity, and naturalness, we expose unique pat-terns inherent to each source.
We particularly notice that the structural segmentation of code is a critical
factor in identifying its provenance. Based on our findings, we propose a novel
machine-generated code detection method called DetectCodeGPT, which improves
DetectGPT by capturing the distinct structural patterns of code. Diverging from
conventional techniques that depend on external LLMs for perturbations,
DetectCodeGPT perturbs the code corpus by strategically inserting spaces and
newlines, ensuring both efficacy and efficiency. Experiment results show that
our approach significantly outperforms state-of-the-art techniques in detecting
machine-generated code.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yuling Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_C/0/1/0/all/0/1&quot;&gt;Chengcheng Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1&quot;&gt;Xiaodong Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.07448">
<title>Formal Logic Enabled Personalized Federated Learning Through Property Inference. (arXiv:2401.07448v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2401.07448</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in federated learning (FL) have greatly facilitated the
development of decentralized collaborative applications, particularly in the
domain of Artificial Intelligence of Things (AIoT). However, a critical aspect
missing from the current research landscape is the ability to enable
data-driven client models with symbolic reasoning capabilities. Specifically,
the inherent heterogeneity of participating client devices poses a significant
challenge, as each client exhibits unique logic reasoning properties. Failing
to consider these device-specific specifications can result in critical
properties being missed in the client predictions, leading to suboptimal
performance. In this work, we propose a new training paradigm that leverages
temporal logic reasoning to address this issue. Our approach involves enhancing
the training process by incorporating mechanically generated logic expressions
for each FL client. Additionally, we introduce the concept of aggregation
clusters and develop a partitioning algorithm to effectively group clients
based on the alignment of their temporal reasoning properties. We evaluate the
proposed method on two tasks: a real-world traffic volume prediction task
consisting of sensory data from fifteen states and a smart city multi-task
prediction utilizing synthetic data. The evaluation results exhibit clear
improvements, with performance accuracy improved by up to 54% across all
sequential prediction models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_Z/0/1/0/all/0/1&quot;&gt;Ziyan An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johnson_T/0/1/0/all/0/1&quot;&gt;Taylor T. Johnson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1&quot;&gt;Meiyi Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08396">
<title>Hidden Flaws Behind Expert-Level Accuracy of GPT-4 Vision in Medicine. (arXiv:2401.08396v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.08396</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies indicate that Generative Pre-trained Transformer 4 with Vision
(GPT-4V) outperforms human physicians in medical challenge tasks. However,
these evaluations primarily focused on the accuracy of multi-choice questions
alone. Our study extends the current scope by conducting a comprehensive
analysis of GPT-4V&apos;s rationales of image comprehension, recall of medical
knowledge, and step-by-step multimodal reasoning when solving New England
Journal of Medicine (NEJM) Image Challenges - an imaging quiz designed to test
the knowledge and diagnostic capabilities of medical professionals. Evaluation
results confirmed that GPT-4V outperforms human physicians regarding
multi-choice accuracy (88.0% vs. 77.0%, p=0.034). GPT-4V also performs well in
cases where physicians incorrectly answer, with over 80% accuracy. However, we
discovered that GPT-4V frequently presents flawed rationales in cases where it
makes the correct final choices (27.3%), most prominent in image comprehension
(21.6%). Regardless of GPT-4V&apos;s high accuracy in multi-choice questions, our
findings emphasize the necessity for further in-depth evaluations of its
rationales before integrating such models into clinical workflows.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1&quot;&gt;Qiao Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1&quot;&gt;Fangyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yiliang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Ziyang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1&quot;&gt;Justin M. Cheung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Robert Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Summers_R/0/1/0/all/0/1&quot;&gt;Ronald M. Summers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rousseau_J/0/1/0/all/0/1&quot;&gt;Justin F. Rousseau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_P/0/1/0/all/0/1&quot;&gt;Peiyun Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Landsman_M/0/1/0/all/0/1&quot;&gt;Marc J Landsman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baxter_S/0/1/0/all/0/1&quot;&gt;Sally L. Baxter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+AlAref_S/0/1/0/all/0/1&quot;&gt;Subhi J. Al&amp;#x27;Aref&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yijia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiang_M/0/1/0/all/0/1&quot;&gt;Michael F. Chiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yifan Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08517">
<title>Supporting Student Decisions on Learning Recommendations: An LLM-Based Chatbot with Knowledge Graph Contextualization for Conversational Explainability and Mentoring. (arXiv:2401.08517v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2401.08517</link>
<description rdf:parseType="Literal">&lt;p&gt;Student commitment towards a learning recommendation is not separable from
their understanding of the reasons it was recommended to them; and their
ability to modify it based on that understanding. Among explainability
approaches, chatbots offer the potential to engage the student in a
conversation, similar to a discussion with a peer or a mentor. The capabilities
of chatbots, however, are still not sufficient to replace a human mentor,
despite the advancements of generative AI (GenAI) and large language models
(LLM). Therefore, we propose an approach to utilize chatbots as mediators of
the conversation and sources of limited and controlled generation of
explanations, to harvest the potential of LLMs while reducing their potential
risks at the same time. The proposed LLM-based chatbot supports students in
understanding learning-paths recommendations. We use a knowledge graph (KG) as
a human-curated source of information, to regulate the LLM&apos;s output through
defining its prompt&apos;s context. A group chat approach is developed to connect
students with human mentors, either on demand or in cases that exceed the
chatbot&apos;s pre-defined tasks. We evaluate the chatbot with a user study, to
provide a proof-of-concept and highlight the potential requirements and
limitations of utilizing chatbots in conversational explainability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abu_Rasheed_H/0/1/0/all/0/1&quot;&gt;Hasan Abu-Rasheed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdulsalam_M/0/1/0/all/0/1&quot;&gt;Mohamad Hussam Abdulsalam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1&quot;&gt;Christian Weber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fathi_M/0/1/0/all/0/1&quot;&gt;Madjid Fathi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08534">
<title>DiConStruct: Causal Concept-based Explanations through Black-Box Distillation. (arXiv:2401.08534v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.08534</link>
<description rdf:parseType="Literal">&lt;p&gt;Model interpretability plays a central role in human-AI decision-making
systems. Ideally, explanations should be expressed using human-interpretable
semantic concepts. Moreover, the causal relations between these concepts should
be captured by the explainer to allow for reasoning about the explanations.
Lastly, explanation methods should be efficient and not compromise the
performance of the predictive task. Despite the rapid advances in AI
explainability in recent years, as far as we know to date, no method fulfills
these three properties. Indeed, mainstream methods for local concept
explainability do not produce causal explanations and incur a trade-off between
explainability and prediction performance. We present DiConStruct, an
explanation method that is both concept-based and causal, with the goal of
creating more interpretable local explanations in the form of structural causal
models and concept attributions. Our explainer works as a distillation model to
any black-box machine learning model by approximating its predictions while
producing the respective explanations. Because of this, DiConStruct generates
explanations efficiently while not impacting the black-box prediction task. We
validate our method on an image dataset and a tabular dataset, showing that
DiConStruct approximates the black-box models with higher fidelity than other
concept explainability baselines, while providing explanations that include the
causal relations between the concepts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moreira_R/0/1/0/all/0/1&quot;&gt;Ricardo Moreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bono_J/0/1/0/all/0/1&quot;&gt;Jacopo Bono&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cardoso_M/0/1/0/all/0/1&quot;&gt;M&amp;#xe1;rio Cardoso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saleiro_P/0/1/0/all/0/1&quot;&gt;Pedro Saleiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Figueiredo_M/0/1/0/all/0/1&quot;&gt;M&amp;#xe1;rio A. T. Figueiredo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bizarro_P/0/1/0/all/0/1&quot;&gt;Pedro Bizarro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10841">
<title>Using LLMs to discover emerging coded antisemitic hate-speech in extremist social media. (arXiv:2401.10841v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.10841</link>
<description rdf:parseType="Literal">&lt;p&gt;Online hate speech proliferation has created a difficult problem for social
media platforms. A particular challenge relates to the use of coded language by
groups interested in both creating a sense of belonging for its users and
evading detection. Coded language evolves quickly and its use varies over time.
This paper proposes a methodology for detecting emerging coded hate-laden
terminology. The methodology is tested in the context of online antisemitic
discourse. The approach considers posts scraped from social media platforms,
often used by extremist users. The posts are scraped using seed expressions
related to previously known discourse of hatred towards Jews. The method begins
by identifying the expressions most representative of each post and calculating
their frequency in the whole corpus. It filters out grammatically incoherent
expressions as well as previously encountered ones so as to focus on emergent
well-formed terminology. This is followed by an assessment of semantic
similarity to known antisemitic terminology using a fine-tuned large language
model, and subsequent filtering out of the expressions that are too distant
from known expressions of hatred. Emergent antisemitic expressions containing
terms clearly relating to Jewish topics are then removed to return only coded
expressions of hatred.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kikkisetti_D/0/1/0/all/0/1&quot;&gt;Dhanush Kikkisetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mustafa_R/0/1/0/all/0/1&quot;&gt;Raza Ul Mustafa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melillo_W/0/1/0/all/0/1&quot;&gt;Wendy Melillo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corizzo_R/0/1/0/all/0/1&quot;&gt;Roberto Corizzo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boukouvalas_Z/0/1/0/all/0/1&quot;&gt;Zois Boukouvalas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gill_J/0/1/0/all/0/1&quot;&gt;Jeff Gill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Japkowicz_N/0/1/0/all/0/1&quot;&gt;Nathalie Japkowicz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11120">
<title>Enhancing Large Language Models for Clinical Decision Support by Incorporating Clinical Practice Guidelines. (arXiv:2401.11120v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.11120</link>
<description rdf:parseType="Literal">&lt;p&gt;Background Large Language Models (LLMs), enhanced with Clinical Practice
Guidelines (CPGs), can significantly improve Clinical Decision Support (CDS).
However, methods for incorporating CPGs into LLMs are not well studied. Methods
We develop three distinct methods for incorporating CPGs into LLMs: Binary
Decision Tree (BDT), Program-Aided Graph Construction (PAGC), and
Chain-of-Thought-Few-Shot Prompting (CoT-FSP). To evaluate the effectiveness of
the proposed methods, we create a set of synthetic patient descriptions and
conduct both automatic and human evaluation of the responses generated by four
LLMs: GPT-4, GPT-3.5 Turbo, LLaMA, and PaLM 2. Zero-Shot Prompting (ZSP) was
used as the baseline method. We focus on CDS for COVID-19 outpatient treatment
as the case study. Results All four LLMs exhibit improved performance when
enhanced with CPGs compared to the baseline ZSP. BDT outperformed both CoT-FSP
and PAGC in automatic evaluation. All of the proposed methods demonstrated high
performance in human evaluation. Conclusion LLMs enhanced with CPGs demonstrate
superior performance, as compared to plain LLMs with ZSP, in providing accurate
recommendations for COVID-19 outpatient treatment, which also highlights the
potential for broader applications beyond the case study.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oniani_D/0/1/0/all/0/1&quot;&gt;David Oniani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xizhi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Visweswaran_S/0/1/0/all/0/1&quot;&gt;Shyam Visweswaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kapoor_S/0/1/0/all/0/1&quot;&gt;Sumit Kapoor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kooragayalu_S/0/1/0/all/0/1&quot;&gt;Shravan Kooragayalu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Polanska_K/0/1/0/all/0/1&quot;&gt;Katelyn Polanska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanshan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11648">
<title>Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal Contrastive EHR Modelling with Hierarchical Regularisation. (arXiv:2401.11648v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.11648</link>
<description rdf:parseType="Literal">&lt;p&gt;Predicting next visit diagnosis using Electronic Health Records (EHR) is an
essential task in healthcare, critical for devising proactive future plans for
both healthcare providers and patients. Nonetheless, many preceding studies
have not sufficiently addressed the heterogeneous and hierarchical
characteristics inherent in EHR data, inevitably leading to sub-optimal
performance. To this end, we propose NECHO, a novel medical code-centric
multimodal contrastive EHR learning framework with hierarchical regularisation.
First, we integrate multifaceted information encompassing medical codes,
demographics, and clinical notes using a tailored network design and a pair of
bimodal contrastive losses, all of which pivot around a medical code
representation. We also regularise modality-specific encoders using a parental
level information in medical ontology to learn hierarchical structure of EHR
data. A series of experiments on MIMIC-III data demonstrates effectiveness of
our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koo_H/0/1/0/all/0/1&quot;&gt;Heejoon Koo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11792">
<title>Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations. (arXiv:2401.11792v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2401.11792</link>
<description rdf:parseType="Literal">&lt;p&gt;An intelligent driving system should be capable of dynamically formulating
appropriate driving strategies based on the current environment and vehicle
status, while ensuring the security and reliability of the system. However,
existing methods based on reinforcement learning and imitation learning suffer
from low safety, poor generalization, and inefficient sampling. Additionally,
they cannot accurately predict future driving trajectories, and the accurate
prediction of future driving trajectories is a precondition for making optimal
decisions. To solve these problems, in this paper, we introduce a Safe and
Generalized end-to-end Autonomous Driving System (SGADS) for complex and
various scenarios. Our SGADS incorporates variational inference with
normalizing flows, enabling the intelligent vehicle to accurately predict
future driving trajectories. Moreover, we propose the formulation of robust
safety constraints. Furthermore, we combine reinforcement learning with
demonstrations to augment search process of the agent. The experimental results
demonstrate that our SGADS can significantly improve safety performance,
exhibit strong generalization, and enhance the training efficiency of
intelligent vehicles in complex urban scenarios compared to existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1&quot;&gt;Zuojin Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;YongQiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jianyu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11798">
<title>Knowledge Distillation on Spatial-Temporal Graph Convolutional Network for Traffic Prediction. (arXiv:2401.11798v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.11798</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficient real-time traffic prediction is crucial for reducing transportation
time. To predict traffic conditions, we employ a spatio-temporal graph neural
network (ST-GNN) to model our real-time traffic data as temporal graphs.
Despite its capabilities, it often encounters challenges in delivering
efficient real-time predictions for real-world traffic data. Recognizing the
significance of timely prediction due to the dynamic nature of real-time data,
we employ knowledge distillation (KD) as a solution to enhance the execution
time of ST-GNNs for traffic prediction. In this paper, We introduce a cost
function designed to train a network with fewer parameters (the student) using
distilled data from a complex network (the teacher) while maintaining its
accuracy close to that of the teacher. We use knowledge distillation,
incorporating spatial-temporal correlations from the teacher network to enable
the student to learn the complex patterns perceived by the teacher. However, a
challenge arises in determining the student network architecture rather than
considering it inadvertently. To address this challenge, we propose an
algorithm that utilizes the cost function to calculate pruning scores,
addressing small network architecture search issues, and jointly fine-tunes the
network resulting from each pruning stage using KD. Ultimately, we evaluate our
proposed ideas on two real-world datasets, PeMSD7 and PeMSD8. The results
indicate that our method can maintain the student&apos;s accuracy close to that of
the teacher, even with the retention of only $3\%$ of network parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Izadi_M/0/1/0/all/0/1&quot;&gt;Mohammad Izadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Safayani_M/0/1/0/all/0/1&quot;&gt;Mehran Safayani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirzaei_A/0/1/0/all/0/1&quot;&gt;Abdolreza Mirzaei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12435">
<title>Quantitative Analysis of Molecular Transport in the Extracellular Space Using Physics-Informed Neural Network. (arXiv:2401.12435v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2401.12435</link>
<description rdf:parseType="Literal">&lt;p&gt;The brain extracellular space (ECS), an irregular, extremely tortuous
nanoscale space located between cells or between cells and blood vessels, is
crucial for nerve cell survival. It plays a pivotal role in high-level brain
functions such as memory, emotion, and sensation. However, the specific form of
molecular transport within the ECS remain elusive. To address this challenge,
this paper proposes a novel approach to quantitatively analyze the molecular
transport within the ECS by solving an inverse problem derived from the
advection-diffusion equation (ADE) using a physics-informed neural network
(PINN). PINN provides a streamlined solution to the ADE without the need for
intricate mathematical formulations or grid settings. Additionally, the
optimization of PINN facilitates the automatic computation of the diffusion
coefficient governing long-term molecule transport and the velocity of
molecules driven by advection. Consequently, the proposed method allows for the
quantitative analysis and identification of the specific pattern of molecular
transport within the ECS through the calculation of the Peclet number.
Experimental validation on two datasets of magnetic resonance images (MRIs)
captured at different time points showcases the effectiveness of the proposed
method. Notably, our simulations reveal identical molecular transport patterns
between datasets representing rats with tracer injected into the same brain
region. These findings highlight the potential of PINN as a promising tool for
comprehensively exploring molecular transport within the ECS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1&quot;&gt;Jiayi Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongfeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jin Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Q/0/1/0/all/0/1&quot;&gt;Qingrui Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1&quot;&gt;Hanbo Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zu_L/0/1/0/all/0/1&quot;&gt;Lingyun Zu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1&quot;&gt;Xiaobo Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1&quot;&gt;Hongbin Han&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>