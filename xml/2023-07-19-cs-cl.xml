<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CL updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-18T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computation and Language</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08714" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08720" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08767" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08813" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08859" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08922" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08925" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08931" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08941" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08945" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09007" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09009" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09014" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09021" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09059" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09084" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09162" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09249" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09254" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09255" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09270" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09274" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09288" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09312" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09378" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09384" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09390" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09416" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09455" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09456" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09474" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09476" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2104.06474" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.02531" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.14348" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.13066" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.13816" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.09775" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.03642" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.09360" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12421" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14279" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15656" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15788" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02839" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03109" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04964" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07924" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.09550" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.08714">
<title>Cross-Lingual NER for Financial Transaction Data in Low-Resource Languages. (arXiv:2307.08714v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.08714</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an efficient modeling framework for cross-lingual named entity
recognition in semi-structured text data. Our approach relies on both knowledge
distillation and consistency training. The modeling framework leverages
knowledge from a large language model (XLMRoBERTa) pre-trained on the source
language, with a student-teacher relationship (knowledge distillation). The
student model incorporates unsupervised consistency training (with KL
divergence loss) on the low-resource target language.
&lt;/p&gt;
&lt;p&gt;We employ two independent datasets of SMSs in English and Arabic, each
carrying semi-structured banking transaction information, and focus on
exhibiting the transfer of knowledge from English to Arabic. With access to
only 30 labeled samples, our model can generalize the recognition of merchants,
amounts, and other fields from English to Arabic. We show that our modeling
approach, while efficient, performs best overall when compared to
state-of-the-art approaches like DistilBERT pre-trained on the target language
or a supervised model directly trained on labeled data in the target language.
&lt;/p&gt;
&lt;p&gt;Our experiments show that it is enough to learn to recognize entities in
English to reach reasonable performance in a low-resource language in the
presence of a few labeled samples of semi-structured data. The proposed
framework has implications for developing multi-lingual applications,
especially in geographies where digital endeavors rely on both English and one
or more low-resource language(s), sometimes mixed with English or employed
singly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Sunisth Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Davide Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boulenger_A/0/1/0/all/0/1&quot;&gt;Alexandre Boulenger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08720">
<title>ivrit.ai: A Comprehensive Dataset of Hebrew Speech for AI Research and Development. (arXiv:2307.08720v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2307.08720</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce &quot;ivrit.ai&quot;, a comprehensive Hebrew speech dataset, addressing
the distinct lack of extensive, high-quality resources for advancing Automated
Speech Recognition (ASR) technology in Hebrew. With over 3,300 speech hours and
a over a thousand diverse speakers, ivrit.ai offers a substantial compilation
of Hebrew speech across various contexts. It is delivered in three forms to
cater to varying research needs: raw unprocessed audio; data post-Voice
Activity Detection, and partially transcribed data. The dataset stands out for
its legal accessibility, permitting use at no cost, thereby serving as a
crucial resource for researchers, developers, and commercial entities. ivrit.ai
opens up numerous applications, offering vast potential to enhance AI
capabilities in Hebrew. Future efforts aim to expand ivrit.ai further, thereby
advancing Hebrew&apos;s standing in AI research and technology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Marmor_Y/0/1/0/all/0/1&quot;&gt;Yanir Marmor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Misgav_K/0/1/0/all/0/1&quot;&gt;Kinneret Misgav&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lifshitz_Y/0/1/0/all/0/1&quot;&gt;Yair Lifshitz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08767">
<title>A mixed policy to improve performance of language models on math problems. (arXiv:2307.08767v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.08767</link>
<description rdf:parseType="Literal">&lt;p&gt;When to solve math problems, most language models take a sampling strategy to
predict next word according conditional probabilities. In the math reasoning
step, it may generate wrong answer. Considering math problems are
deterministic, we propose a mixed policy exploration approach to solve math
problems with reinforcement learning. In peculiar, we propose a two level token
exploration policy: the abstract level explores next token with probability and
the second level is deterministic. Specifically, the abstract level policy will
decide whether the token is operator or operand with probability sampling,
while the second level is deterministic to select next token with the highest
score in a greedy way. We test our method on GSM8K dataset with GPT-2 model,
and demonstrate more than $2\%$ performance gain. Our implementation is
available at https://github.com/vividitytech/math_lm_rl.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Gang Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08813">
<title>Comparative Performance Evaluation of Large Language Models for Extracting Molecular Interactions and Pathway Knowledge. (arXiv:2307.08813v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.08813</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding protein interactions and pathway knowledge is crucial for
unraveling the complexities of living systems and investigating the underlying
mechanisms of biological functions and complex diseases. While existing
databases provide curated biological data from literature and other sources,
they are often incomplete and their maintenance is labor-intensive,
necessitating alternative approaches. In this study, we propose to harness the
capabilities of large language models to address these issues by automatically
extracting such knowledge from the relevant scientific literature. Toward this
goal, in this work, we investigate the effectiveness of different large
language models in tasks that involve recognizing protein interactions,
pathways, and gene regulatory relations. We thoroughly evaluate the performance
of various models, highlight the significant findings, and discuss both the
future opportunities and the remaining challenges associated with this
approach. The code and data are available at:
https://github.com/boxorange/BioIE-LLM
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_G/0/1/0/all/0/1&quot;&gt;Gilchan Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_B/0/1/0/all/0/1&quot;&gt;Byung-Jun Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xihaier Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopez_Marrero_V/0/1/0/all/0/1&quot;&gt;Vanessa L&amp;#xf3;pez-Marrero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johnstone_P/0/1/0/all/0/1&quot;&gt;Patrick Johnstone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1&quot;&gt;Shinjae Yoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alexander_F/0/1/0/all/0/1&quot;&gt;Francis J. Alexander&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08859">
<title>Curriculum Learning for Graph Neural Networks: A Multiview Competence-based Approach. (arXiv:2307.08859v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08859</link>
<description rdf:parseType="Literal">&lt;p&gt;A curriculum is a planned sequence of learning materials and an effective one
can make learning efficient and effective for both humans and machines. Recent
studies developed effective data-driven curriculum learning approaches for
training graph neural networks in language applications. However, existing
curriculum learning approaches often employ a single criterion of difficulty in
their training paradigms. In this paper, we propose a new perspective on
curriculum learning by introducing a novel approach that builds on graph
complexity formalisms (as difficulty criteria) and model competence during
training. The model consists of a scheduling scheme which derives effective
curricula by accounting for different views of sample difficulty and model
competence during training. The proposed solution advances existing research in
curriculum learning for graph neural networks with the ability to incorporate a
fine-grained spectrum of graph difficulty criteria in their training paradigms.
Experimental results on real-world link prediction and node classification
tasks illustrate the effectiveness of the proposed approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vakil_N/0/1/0/all/0/1&quot;&gt;Nidhi Vakil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amiri_H/0/1/0/all/0/1&quot;&gt;Hadi Amiri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08922">
<title>Large Language Models Perform Diagnostic Reasoning. (arXiv:2307.08922v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.08922</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore the extension of chain-of-thought (CoT) prompting to medical
reasoning for the task of automatic diagnosis. Motivated by doctors&apos; underlying
reasoning process, we present Diagnostic-Reasoning CoT (DR-CoT). Empirical
results demonstrate that by simply prompting large language models trained only
on general text corpus with two DR-CoT exemplars, the diagnostic accuracy
improves by 15% comparing to standard prompting. Moreover, the gap reaches a
pronounced 18% in out-domain settings. Our findings suggest expert-knowledge
reasoning in large language models can be elicited through proper promptings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Cheng-Kuang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei-Lin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hsin-Hsi Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08925">
<title>Federated Large Language Model: A Position Paper. (arXiv:2307.08925v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08925</link>
<description rdf:parseType="Literal">&lt;p&gt;Large scale language models (LLM) have received significant attention and
found diverse applications across various domains, but their development
encounters challenges in real-world scenarios. These challenges arise due to
the scarcity of public domain data availability and the need to maintain
privacy with respect to private domain data. To address these issues, federated
learning (FL) has emerged as a promising technology that enables collaborative
training of shared models while preserving decentralized data. We propose the
concept of federated LLM, which comprises three key components, i.e., federated
LLM pre-training, federated LLM fine-tuning, and federated LLM prompt
engineering. For each component, we discuss its advantage over traditional LLM
training methods and propose specific engineering strategies for
implementation. Furthermore, we explore the novel challenges introduced by the
integration of FL and LLM. We analyze existing solutions and identify potential
obstacles faced by these solutions within the context of federated LLM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chaochao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1&quot;&gt;Xiaohua Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1&quot;&gt;Jianwei Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1&quot;&gt;Xiaolin Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08931">
<title>Teach model to answer questions after comprehending the document. (arXiv:2307.08931v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.08931</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-choice Machine Reading Comprehension (MRC) is a challenging extension
of Natural Language Processing (NLP) that requires the ability to comprehend
the semantics and logical relationships between entities in a given text. The
MRC task has traditionally been viewed as a process of answering questions
based on the given text. This single-stage approach has often led the network
to concentrate on generating the correct answer, potentially neglecting the
comprehension of the text itself. As a result, many prevalent models have faced
challenges in performing well on this task when dealing with longer texts. In
this paper, we propose a two-stage knowledge distillation method that teaches
the model to better comprehend the document by dividing the MRC task into two
separate stages. Our experimental results show that the student model, when
equipped with our method, achieves significant improvements, demonstrating the
effectiveness of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1&quot;&gt;Ruiqing Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jian_P/0/1/0/all/0/1&quot;&gt;Ping Jian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08941">
<title>NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning. (arXiv:2307.08941v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08941</link>
<description rdf:parseType="Literal">&lt;p&gt;Fine-tuning a pre-trained language model (PLM) emerges as the predominant
strategy in many natural language processing applications. However, even
fine-tuning the PLMs and doing inference are expensive, especially on edge
devices with low computing power. Some general approaches (e.g. quantization
and distillation) have been widely studied to reduce the compute/memory of PLM
fine-tuning, while very few one-shot compression techniques are explored. In
this paper, we investigate the neural tangent kernel (NTK)--which reveals the
gradient descent dynamics of neural networks--of the multilayer perceptrons
(MLP) modules in a PLM and propose to coin a lightweight PLM through
NTK-approximating MLP fusion. To achieve this, we reconsider the MLP as a
bundle of sub-MLPs, and cluster them into a given number of centroids, which
can then be restored as a compressed MLP and surprisingly shown to well
approximate the NTK of the original PLM. Extensive experiments of PLM
fine-tuning on both natural language understanding (NLU) and generation (NLG)
tasks are provided to verify the effectiveness of the proposed method MLP
fusion. Our code is available at https://github.com/weitianxin/MLP_Fusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1&quot;&gt;Tianxin Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zeming Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yifan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jingrui He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08945">
<title>Mitigating Label Bias via Decoupled Confident Learning. (arXiv:2307.08945v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08945</link>
<description rdf:parseType="Literal">&lt;p&gt;Growing concerns regarding algorithmic fairness have led to a surge in
methodologies to mitigate algorithmic bias. However, such methodologies largely
assume that observed labels in training data are correct. This is problematic
because bias in labels is pervasive across important domains, including
healthcare, hiring, and content moderation. In particular, human-generated
labels are prone to encoding societal biases. While the presence of labeling
bias has been discussed conceptually, there is a lack of methodologies to
address this problem. We propose a pruning method -- Decoupled Confident
Learning (DeCoLe) -- specifically designed to mitigate label bias. After
illustrating its performance on a synthetic dataset, we apply DeCoLe in the
context of hate speech detection, where label bias has been recognized as an
important challenge, and show that it successfully identifies biased labels and
outperforms competing approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+De_Arteaga_M/0/1/0/all/0/1&quot;&gt;Maria De-Arteaga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saar_Tsechansky_M/0/1/0/all/0/1&quot;&gt;Maytal Saar-Tsechansky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09007">
<title>On the (In)Effectiveness of Large Language Models for Chinese Text Correction. (arXiv:2307.09007v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.09007</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the development and progress of Large Language Models (LLMs) have
amazed the entire Artificial Intelligence community. As an outstanding
representative of LLMs and the foundation model that set off this wave of
research on LLMs, ChatGPT has attracted more and more researchers to study its
capabilities and performance on various downstream Natural Language Processing
(NLP) tasks. While marveling at ChatGPT&apos;s incredible performance on kinds of
tasks, we notice that ChatGPT also has excellent multilingual processing
capabilities, such as Chinese. To explore the Chinese processing ability of
ChatGPT, we focus on Chinese Text Correction, a fundamental and challenging
Chinese NLP task. Specifically, we evaluate ChatGPT on the Chinese Grammatical
Error Correction (CGEC) and Chinese Spelling Check (CSC) tasks, which are two
main Chinese Text Correction scenarios. From extensive analyses and comparisons
with previous state-of-the-art fine-tuned models, we empirically find that the
ChatGPT currently has both amazing performance and unsatisfactory behavior for
Chinese Text Correction. We believe our findings will promote the landing and
application of LLMs in the Chinese NLP community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yinghui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Haojing Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Shirong Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yangning Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1&quot;&gt;Feng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Hai-Tao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1&quot;&gt;Qingyu Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09009">
<title>How is ChatGPT&apos;s behavior changing over time?. (arXiv:2307.09009v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.09009</link>
<description rdf:parseType="Literal">&lt;p&gt;GPT-3.5 and GPT-4 are the two most widely used large language model (LLM)
services. However, when and how these models are updated over time is opaque.
Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on
four diverse tasks: 1) solving math problems, 2) answering sensitive/dangerous
questions, 3) generating code and 4) visual reasoning. We find that the
performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time.
For example, GPT-4 (March 2023) was very good at identifying prime numbers
(accuracy 97.6%) but GPT-4 (June 2023) was very poor on these same questions
(accuracy 2.4%). Interestingly GPT-3.5 (June 2023) was much better than GPT-3.5
(March 2023) in this task. GPT-4 was less willing to answer sensitive questions
in June than in March, and both GPT-4 and GPT-3.5 had more formatting mistakes
in code generation in June than in March. Overall, our findings shows that the
behavior of the same LLM service can change substantially in a relatively short
amount of time, highlighting the need for continuous monitoring of LLM quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lingjiao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1&quot;&gt;Matei Zaharia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1&quot;&gt;James Zou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09014">
<title>Exploring acceptance of autonomous vehicle policies using KeyBERT and SNA: Targeting engineering students. (arXiv:2307.09014v1 [cs.SI])</title>
<link>http://arxiv.org/abs/2307.09014</link>
<description rdf:parseType="Literal">&lt;p&gt;This study aims to explore user acceptance of Autonomous Vehicle (AV)
policies with improved text-mining methods. Recently, South Korean policymakers
have viewed Autonomous Driving Car (ADC) and Autonomous Driving Robot (ADR) as
next-generation means of transportation that will reduce the cost of
transporting passengers and goods. They support the construction of V2I and V2V
communication infrastructures for ADC and recognize that ADR is equivalent to
pedestrians to promote its deployment into sidewalks. To fill the gap where
end-user acceptance of these policies is not well considered, this study
applied two text-mining methods to the comments of graduate students in the
fields of Industrial, Mechanical, and Electronics-Electrical-Computer. One is
the Co-occurrence Network Analysis (CNA) based on TF-IWF and Dice coefficient,
and the other is the Contextual Semantic Network Analysis (C-SNA) based on both
KeyBERT, which extracts keywords that contextually represent the comments, and
double cosine similarity. The reason for comparing these approaches is to
balance interest not only in the implications for the AV policies but also in
the need to apply quality text mining to this research domain. Significantly,
the limitation of frequency-based text mining, which does not reflect textual
context, and the trade-off of adjusting thresholds in Semantic Network Analysis
(SNA) were considered. As the results of comparing the two approaches, the
C-SNA provided the information necessary to understand users&apos; voices using
fewer nodes and features than the CNA. The users who pre-emptively understood
the AV policies based on their engineering literacy and the given texts
revealed potential risks of the AV accident policies. This study adds
suggestions to manage these risks to support the successful deployment of AVs
on public roads.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1&quot;&gt;Jinwoo Ha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Dongsoo Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09021">
<title>Towards a Neural Era in Dialogue Management for Collaboration: A Literature Survey. (arXiv:2307.09021v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.09021</link>
<description rdf:parseType="Literal">&lt;p&gt;Dialogue-based human-AI collaboration can revolutionize collaborative
problem-solving, creative exploration, and social support. To realize this
goal, the development of automated agents proficient in skills such as
negotiating, following instructions, establishing common ground, and
progressing shared tasks is essential. This survey begins by reviewing the
evolution of dialogue management paradigms in collaborative dialogue systems,
from traditional handcrafted and information-state based methods to AI
planning-inspired approaches. It then shifts focus to contemporary data-driven
dialogue management techniques, which seek to transfer deep learning successes
from form-filling and open-domain settings to collaborative contexts. The paper
proceeds to analyze a selected set of recent works that apply neural approaches
to collaborative dialogue management, spotlighting prevailing trends in the
field. This survey hopes to provide foundational background for future
advancements in collaborative dialogue management, particularly as the dialogue
systems community continues to embrace the potential of large language models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mannekote_A/0/1/0/all/0/1&quot;&gt;Amogh Mannekote&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09059">
<title>Unleashing the Imagination of Text: A Novel Framework for Text-to-image Person Retrieval via Exploring the Power of Words. (arXiv:2307.09059v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.09059</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of Text-to-image person retrieval is to retrieve person images from
a large gallery that match the given textual descriptions. The main challenge
of this task lies in the significant differences in information representation
between the visual and textual modalities. The textual modality conveys
abstract and precise information through vocabulary and grammatical structures,
while the visual modality conveys concrete and intuitive information through
images. To fully leverage the expressive power of textual representations, it
is essential to accurately map abstract textual descriptions to specific
images.
&lt;/p&gt;
&lt;p&gt;To address this issue, we propose a novel framework to Unleash the
Imagination of Text (UIT) in text-to-image person retrieval, aiming to fully
explore the power of words in sentences. Specifically, the framework employs
the pre-trained full CLIP model as a dual encoder for the images and texts ,
taking advantage of prior cross-modal alignment knowledge. The Text-guided
Image Restoration auxiliary task is proposed with the aim of implicitly mapping
abstract textual entities to specific image regions, facilitating alignment
between textual and visual embeddings. Additionally, we introduce a cross-modal
triplet loss tailored for handling hard samples, enhancing the model&apos;s ability
to distinguish minor differences.
&lt;/p&gt;
&lt;p&gt;To focus the model on the key components within sentences, we propose a novel
text data augmentation technique. Our proposed methods achieve state-of-the-art
results on three popular benchmark datasets, and the source code will be made
publicly available shortly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Delong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haiwen Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09084">
<title>Attention over pre-trained Sentence Embeddings for Long Document Classification. (arXiv:2307.09084v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.09084</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite being the current de-facto models in most NLP tasks, transformers are
often limited to short sequences due to their quadratic attention complexity on
the number of tokens. Several attempts to address this issue were studied,
either by reducing the cost of the self-attention computation or by modeling
smaller sequences and combining them through a recurrence mechanism or using a
new transformer model. In this paper, we suggest to take advantage of
pre-trained sentence transformers to start from semantically meaningful
embeddings of the individual sentences, and then combine them through a small
attention layer that scales linearly with the document length. We report the
results obtained by this simple architecture on three standard document
classification datasets. When compared with the current state-of-the-art models
using standard fine-tuning, the studied method obtains competitive results
(even if there is no clear best model in this configuration). We also showcase
that the studied architecture obtains better results when freezing the
underlying transformers. A configuration that is useful when we need to avoid
complete fine-tuning (e.g. when the same frozen transformer is shared by
different applications). Finally, two additional experiments are provided to
further evaluate the relevancy of the studied architecture over simpler
baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdaoui_A/0/1/0/all/0/1&quot;&gt;Amine Abdaoui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1&quot;&gt;Sourav Dutta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09162">
<title>Unveiling Gender Bias in Terms of Profession Across LLMs: Analyzing and Addressing Sociological Implications. (arXiv:2307.09162v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.09162</link>
<description rdf:parseType="Literal">&lt;p&gt;Gender bias in artificial intelligence (AI) and natural language processing
has garnered significant attention due to its potential impact on societal
perceptions and biases. This research paper aims to analyze gender bias in
Large Language Models (LLMs) with a focus on multiple comparisons between GPT-2
and GPT-3.5, some prominent language models, to better understand its
implications. Through a comprehensive literature review, the study examines
existing research on gender bias in AI language models and identifies gaps in
the current knowledge. The methodology involves collecting and preprocessing
data from GPT-2 and GPT-3.5, and employing in-depth quantitative analysis
techniques to evaluate gender bias in the generated text. The findings shed
light on gendered word associations, language usage, and biased narratives
present in the outputs of these Large Language Models. The discussion explores
the ethical implications of gender bias and its potential consequences on
social perceptions and marginalized communities. Additionally, the paper
presents strategies for reducing gender bias in LLMs, including algorithmic
approaches and data augmentation techniques. The research highlights the
importance of interdisciplinary collaborations and the role of sociological
studies in mitigating gender bias in AI models. By addressing these issues, we
can pave the way for more inclusive and unbiased AI systems that have a
positive impact on society.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thakur_V/0/1/0/all/0/1&quot;&gt;Vishesh Thakur&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09209">
<title>Automated Ableism: An Exploration of Explicit Disability Biases in Sentiment and Toxicity Analysis Models. (arXiv:2307.09209v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.09209</link>
<description rdf:parseType="Literal">&lt;p&gt;We analyze sentiment analysis and toxicity detection models to detect the
presence of explicit bias against people with disability (PWD). We employ the
bias identification framework of Perturbation Sensitivity Analysis to examine
conversations related to PWD on social media platforms, specifically Twitter
and Reddit, in order to gain insight into how disability bias is disseminated
in real-world social settings. We then create the \textit{Bias Identification
Test in Sentiment} (BITS) corpus to quantify explicit disability bias in any
sentiment analysis and toxicity detection models. Our study utilizes BITS to
uncover significant biases in four open AIaaS (AI as a Service) sentiment
analysis tools, namely TextBlob, VADER, Google Cloud Natural Language API,
DistilBERT and two toxicity detection models, namely two versions of
Toxic-BERT. Our findings indicate that all of these models exhibit
statistically significant explicit bias against PWD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkit_P/0/1/0/all/0/1&quot;&gt;Pranav Narayanan Venkit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinath_M/0/1/0/all/0/1&quot;&gt;Mukund Srinath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_S/0/1/0/all/0/1&quot;&gt;Shomir Wilson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09249">
<title>UniTabE: Pretraining a Unified Tabular Encoder for Heterogeneous Tabular Data. (arXiv:2307.09249v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09249</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in Natural Language Processing (NLP) have witnessed the
groundbreaking impact of pretrained models, yielding impressive outcomes across
various tasks. This study seeks to extend the power of pretraining
methodologies to tabular data, a domain traditionally overlooked, yet
inherently challenging due to the plethora of table schemas intrinsic to
different tasks. The primary research questions underpinning this work revolve
around the adaptation to heterogeneous table structures, the establishment of a
universal pretraining protocol for tabular data, the generalizability and
transferability of learned knowledge across tasks, the adaptation to diverse
downstream applications, and the incorporation of incremental columns over
time. In response to these challenges, we introduce UniTabE, a pioneering
method designed to process tables in a uniform manner, devoid of constraints
imposed by specific table structures. UniTabE&apos;s core concept relies on
representing each basic table element with a module, termed TabUnit. This is
subsequently followed by a Transformer encoder to refine the representation.
Moreover, our model is designed to facilitate pretraining and finetuning
through the utilization of free-form prompts. In order to implement the
pretraining phase, we curated an expansive tabular dataset comprising
approximately 13 billion samples, meticulously gathered from the Kaggle
platform. Rigorous experimental testing and analyses were performed under a
myriad of scenarios to validate the effectiveness of our methodology. The
experimental results demonstrate UniTabE&apos;s superior performance against several
baseline models across a multitude of benchmark datasets. This, therefore,
underscores UniTabE&apos;s potential to significantly enhance the semantic
representation of tabular data, thereby marking a significant stride in the
field of tabular data analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yazheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Ledell Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qi Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09254">
<title>PAC Neural Prediction Set Learning to Quantify the Uncertainty of Generative Language Models. (arXiv:2307.09254v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09254</link>
<description rdf:parseType="Literal">&lt;p&gt;Uncertainty learning and quantification of models are crucial tasks to
enhance the trustworthiness of the models. Importantly, the recent surge of
generative language models (GLMs) emphasizes the need for reliable uncertainty
quantification due to the concerns on generating hallucinated facts. In this
paper, we propose to learn neural prediction set models that comes with the
probably approximately correct (PAC) guarantee for quantifying the uncertainty
of GLMs. Unlike existing prediction set models, which are parameterized by a
scalar value, we propose to parameterize prediction sets via neural networks,
which achieves more precise uncertainty quantification but still satisfies the
PAC guarantee. We demonstrate the efficacy of our method on four types of
language datasets and six types of models by showing that our method improves
the quantified uncertainty by $63\%$ on average, compared to a standard
baseline method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Sangdon Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Taesoo Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09255">
<title>Text vectorization via transformer-based language models and n-gram perplexities. (arXiv:2307.09255v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.09255</link>
<description rdf:parseType="Literal">&lt;p&gt;As the probability (and thus perplexity) of a text is calculated based on the
product of the probabilities of individual tokens, it may happen that one
unlikely token significantly reduces the probability (i.e., increase the
perplexity) of some otherwise highly probable input, while potentially
representing a simple typographical error. Also, given that perplexity is a
scalar value that refers to the entire input, information about the probability
distribution within it is lost in the calculation (a relatively good text that
has one unlikely token and another text in which each token is equally likely
they can have the same perplexity value), especially for longer texts. As an
alternative to scalar perplexity this research proposes a simple algorithm used
to calculate vector values based on n-gram perplexities within the input. Such
representations consider the previously mentioned aspects, and instead of a
unique value, the relative perplexity of each text token is calculated, and
these values are combined into a single vector representing the input.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skoric_M/0/1/0/all/0/1&quot;&gt;Mihailo &amp;#x160;kori&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09270">
<title>Linearized Relative Positional Encoding. (arXiv:2307.09270v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.09270</link>
<description rdf:parseType="Literal">&lt;p&gt;Relative positional encoding is widely used in vanilla and linear
transformers to represent positional information. However, existing encoding
methods of a vanilla transformer are not always directly applicable to a linear
transformer, because the latter requires a decomposition of the query and key
representations into separate kernel functions. Nevertheless, principles for
designing encoding methods suitable for linear transformers remain
understudied. In this work, we put together a variety of existing linear
relative positional encoding approaches under a canonical form and further
propose a family of linear relative positional encoding algorithms via unitary
transformation. Our formulation leads to a principled framework that can be
used to develop new relative positional encoding methods that preserve linear
space-time complexity. Equipped with different models, the proposed linearized
relative positional encoding (LRPE) family derives effective encoding for
various applications. Experiments show that compared with existing methods,
LRPE achieves state-of-the-art performance in language modeling, text
classification, and image classification. Meanwhile, it emphasizes a general
paradigm for designing broadly more relative positional encoding methods that
are applicable to linear transformers. The code is available at
https://github.com/OpenNLPLab/Lrpe.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1&quot;&gt;Zhen Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Weixuan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1&quot;&gt;Kaiyue Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1&quot;&gt;Hui Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dongxu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xiaodong Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1&quot;&gt;Yuchao Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1&quot;&gt;Lingpeng Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yiran Zhong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09274">
<title>Improving Text Semantic Similarity Modeling through a 3D Siamese Network. (arXiv:2307.09274v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.09274</link>
<description rdf:parseType="Literal">&lt;p&gt;Siamese networks have gained popularity as a method for modeling text
semantic similarity. Traditional methods rely on pooling operation to compress
the semantic representations from Transformer blocks in encoding, resulting in
two-dimensional semantic vectors and the loss of hierarchical semantic
information from Transformer blocks. Moreover, this limited structure of
semantic vectors is akin to a flattened landscape, which restricts the methods
that can be applied in downstream modeling, as they can only navigate this flat
terrain. To address this issue, we propose a novel 3D Siamese network for text
semantic similarity modeling, which maps semantic information to a
higher-dimensional space. The three-dimensional semantic tensors not only
retains more precise spatial and feature domain information but also provides
the necessary structural condition for comprehensive downstream modeling
strategies to capture them. Leveraging this structural advantage, we introduce
several modules to reinforce this 3D framework, focusing on three aspects:
feature extraction, attention, and feature fusion. Our extensive experiments on
four text semantic similarity benchmarks demonstrate the effectiveness and
efficiency of our 3D Siamese Network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zang_J/0/1/0/all/0/1&quot;&gt;Jianxiang Zang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hui Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09288">
<title>Llama 2: Open Foundation and Fine-Tuned Chat Models. (arXiv:2307.09288v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.09288</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we develop and release Llama 2, a collection of pretrained and
fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70
billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for
dialogue use cases. Our models outperform open-source chat models on most
benchmarks we tested, and based on our human evaluations for helpfulness and
safety, may be a suitable substitute for closed-source models. We provide a
detailed description of our approach to fine-tuning and safety improvements of
Llama 2-Chat in order to enable the community to build on our work and
contribute to the responsible development of LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Touvron_H/0/1/0/all/0/1&quot;&gt;Hugo Touvron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_L/0/1/0/all/0/1&quot;&gt;Louis Martin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stone_K/0/1/0/all/0/1&quot;&gt;Kevin Stone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albert_P/0/1/0/all/0/1&quot;&gt;Peter Albert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Almahairi_A/0/1/0/all/0/1&quot;&gt;Amjad Almahairi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babaei_Y/0/1/0/all/0/1&quot;&gt;Yasmine Babaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bashlykov_N/0/1/0/all/0/1&quot;&gt;Nikolay Bashlykov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Batra_S/0/1/0/all/0/1&quot;&gt;Soumya Batra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhargava_P/0/1/0/all/0/1&quot;&gt;Prajjwal Bhargava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhosale_S/0/1/0/all/0/1&quot;&gt;Shruti Bhosale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bikel_D/0/1/0/all/0/1&quot;&gt;Dan Bikel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blecher_L/0/1/0/all/0/1&quot;&gt;Lukas Blecher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferrer_C/0/1/0/all/0/1&quot;&gt;Cristian Canton Ferrer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Moya Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cucurull_G/0/1/0/all/0/1&quot;&gt;Guillem Cucurull&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Esiobu_D/0/1/0/all/0/1&quot;&gt;David Esiobu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandes_J/0/1/0/all/0/1&quot;&gt;Jude Fernandes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jeremy Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_W/0/1/0/all/0/1&quot;&gt;Wenyin Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fuller_B/0/1/0/all/0/1&quot;&gt;Brian Fuller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Cynthia Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goswami_V/0/1/0/all/0/1&quot;&gt;Vedanuj Goswami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1&quot;&gt;Naman Goyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hartshorn_A/0/1/0/all/0/1&quot;&gt;Anthony Hartshorn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hosseini_S/0/1/0/all/0/1&quot;&gt;Saghar Hosseini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_R/0/1/0/all/0/1&quot;&gt;Rui Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Inan_H/0/1/0/all/0/1&quot;&gt;Hakan Inan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kardas_M/0/1/0/all/0/1&quot;&gt;Marcin Kardas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kerkez_V/0/1/0/all/0/1&quot;&gt;Viktor Kerkez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khabsa_M/0/1/0/all/0/1&quot;&gt;Madian Khabsa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kloumann_I/0/1/0/all/0/1&quot;&gt;Isabel Kloumann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korenev_A/0/1/0/all/0/1&quot;&gt;Artem Korenev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koura_P/0/1/0/all/0/1&quot;&gt;Punit Singh Koura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lachaux_M/0/1/0/all/0/1&quot;&gt;Marie-Anne Lachaux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lavril_T/0/1/0/all/0/1&quot;&gt;Thibaut Lavril&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jenya Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liskovich_D/0/1/0/all/0/1&quot;&gt;Diana Liskovich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yinghai Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1&quot;&gt;Yuning Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinet_X/0/1/0/all/0/1&quot;&gt;Xavier Martinet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mihaylov_T/0/1/0/all/0/1&quot;&gt;Todor Mihaylov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_P/0/1/0/all/0/1&quot;&gt;Pushkar Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molybog_I/0/1/0/all/0/1&quot;&gt;Igor Molybog&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1&quot;&gt;Yixin Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poulton_A/0/1/0/all/0/1&quot;&gt;Andrew Poulton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reizenstein_J/0/1/0/all/0/1&quot;&gt;Jeremy Reizenstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rungta_R/0/1/0/all/0/1&quot;&gt;Rashi Rungta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saladi_K/0/1/0/all/0/1&quot;&gt;Kalyan Saladi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schelten_A/0/1/0/all/0/1&quot;&gt;Alan Schelten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_R/0/1/0/all/0/1&quot;&gt;Ruan Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_E/0/1/0/all/0/1&quot;&gt;Eric Michael Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramanian_R/0/1/0/all/0/1&quot;&gt;Ranjan Subramanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1&quot;&gt;Xiaoqing Ellen Tan&lt;/a&gt;, et al. (15 additional authors not shown)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09312">
<title>Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media. (arXiv:2307.09312v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.09312</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the Multi-Modal Discussion Transformer (mDT), a novel multi-modal
graph-based transformer model for detecting hate speech in online social
networks. In contrast to traditional text-only methods, our approach to
labelling a comment as hate speech centers around the holistic analysis of text
and images. This is done by leveraging graph transformers to capture the
contextual relationships in the entire discussion that surrounds a comment,
with interwoven fusion layers to combine text and image embeddings instead of
processing different modalities separately. We compare the performance of our
model to baselines that only process text; we also conduct extensive ablation
studies. We conclude with future work for multimodal solutions to deliver
social value in online contexts, arguing that capturing a holistic view of a
conversation greatly advances the effort to detect anti-social behavior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hebert_L/0/1/0/all/0/1&quot;&gt;Liam Hebert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sahu_G/0/1/0/all/0/1&quot;&gt;Gaurav Sahu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sreenivas_N/0/1/0/all/0/1&quot;&gt;Nanda Kishore Sreenivas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golab_L/0/1/0/all/0/1&quot;&gt;Lukasz Golab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_R/0/1/0/all/0/1&quot;&gt;Robin Cohen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09378">
<title>Adapting an ASR Foundation Model for Spoken Language Assessment. (arXiv:2307.09378v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.09378</link>
<description rdf:parseType="Literal">&lt;p&gt;A crucial part of an accurate and reliable spoken language assessment system
is the underlying ASR model. Recently, large-scale pre-trained ASR foundation
models such as Whisper have been made available. As the output of these models
is designed to be human readable, punctuation is added, numbers are presented
in Arabic numeric form and abbreviations are included. Additionally, these
models have a tendency to skip disfluencies and hesitations in the output.
Though useful for readability, these attributes are not helpful for assessing
the ability of a candidate and providing feedback. Here a precise transcription
of what a candidate said is needed. In this paper, we give a detailed analysis
of Whisper outputs and propose two solutions: fine-tuning and soft prompt
tuning. Experiments are conducted on both public speech corpora and an English
learner dataset. Results show that we can effectively alter the decoding
behaviour of Whisper to generate the exact words spoken in the response.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1&quot;&gt;Rao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_M/0/1/0/all/0/1&quot;&gt;Mengjie Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1&quot;&gt;Mark J. F. Gales&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knill_K/0/1/0/all/0/1&quot;&gt;Kate M. Knill&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09384">
<title>Zero-shot Query Reformulation for Conversational Search. (arXiv:2307.09384v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2307.09384</link>
<description rdf:parseType="Literal">&lt;p&gt;As the popularity of voice assistants continues to surge, conversational
search has gained increased attention in Information Retrieval. However, data
sparsity issues in conversational search significantly hinder the progress of
supervised conversational search methods. Consequently, researchers are
focusing more on zero-shot conversational search approaches. Nevertheless,
existing zero-shot methods face three primary limitations: they are not
universally applicable to all retrievers, their effectiveness lacks sufficient
explainability, and they struggle to resolve common conversational ambiguities
caused by omission. To address these limitations, we introduce a novel
Zero-shot Query Reformulation (ZeQR) framework that reformulates queries based
on previous dialogue contexts without requiring supervision from conversational
search data. Specifically, our framework utilizes language models designed for
machine reading comprehension tasks to explicitly resolve two common
ambiguities: coreference and omission, in raw queries. In comparison to
existing zero-shot methods, our approach is universally applicable to any
retriever without additional adaptation or indexing. It also provides greater
explainability and effectively enhances query intent understanding because
ambiguities are explicitly and proactively resolved. Through extensive
experiments on four TREC conversational datasets, we demonstrate the
effectiveness of our method, which consistently outperforms state-of-the-art
baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Dayu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yue Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1&quot;&gt;Hui Fang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09390">
<title>How do software citation formats evolve over time? A longitudinal analysis of R programming language packages. (arXiv:2307.09390v1 [cs.DL])</title>
<link>http://arxiv.org/abs/2307.09390</link>
<description rdf:parseType="Literal">&lt;p&gt;Under the data-driven research paradigm, research software has come to play
crucial roles in nearly every stage of scientific inquiry. Scholars are
advocating for the formal citation of software in academic publications,
treating it on par with traditional research outputs. However, software is
hardly consistently cited: one software entity can be cited as different
objects, and the citations can change over time. These issues, however, are
largely overlooked in existing empirical research on software citation. To fill
the above gaps, the present study compares and analyzes a longitudinal dataset
of citation formats of all R packages collected in 2021 and 2022, in order to
understand the citation formats of R-language packages, important members in
the open-source software family, and how the citations evolve over time. In
particular, we investigate the different document types underlying the
citations and what metadata elements in the citation formats changed over time.
Furthermore, we offer an in-depth analysis of the disciplinarity of journal
articles cited as software (software papers). By undertaking this research, we
aim to contribute to a better understanding of the complexities associated with
software citation, shedding light on future software citation policies and
infrastructure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuzhuo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kai Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09416">
<title>Let&apos;s ViCE! Mimicking Human Cognitive Behavior in Image Generation Evaluation. (arXiv:2307.09416v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09416</link>
<description rdf:parseType="Literal">&lt;p&gt;Research in Image Generation has recently made significant progress,
particularly boosted by the introduction of Vision-Language models which are
able to produce high-quality visual content based on textual inputs. Despite
ongoing advancements in terms of generation quality and realism, no methodical
frameworks have been defined yet to quantitatively measure the quality of the
generated content and the adherence with the prompted requests: so far, only
human-based evaluations have been adopted for quality satisfaction and for
comparing different generative methods. We introduce a novel automated method
for Visual Concept Evaluation (ViCE), i.e. to assess consistency between a
generated/edited image and the corresponding prompt/instructions, with a
process inspired by the human cognitive behaviour. ViCE combines the strengths
of Large Language Models (LLMs) and Visual Question Answering (VQA) into a
unified pipeline, aiming to replicate the human cognitive process in quality
assessment. This method outlines visual concepts, formulates image-specific
verification questions, utilizes the Q&amp;amp;A system to investigate the image, and
scores the combined outcome. Although this brave new hypothesis of mimicking
humans in the image evaluation process is in its preliminary assessment stage,
results are promising and open the door to a new form of automatic evaluation
which could have significant impact as the image generation or the image target
editing tasks become more and more sophisticated.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Betti_F/0/1/0/all/0/1&quot;&gt;Federico Betti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Staiano_J/0/1/0/all/0/1&quot;&gt;Jacopo Staiano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1&quot;&gt;Lorenzo Baraldi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1&quot;&gt;Lorenzo Baraldi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1&quot;&gt;Rita Cucchiara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1&quot;&gt;Nicu Sebe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09455">
<title>Pseudo Outlier Exposure for Out-of-Distribution Detection using Pretrained Transformers. (arXiv:2307.09455v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.09455</link>
<description rdf:parseType="Literal">&lt;p&gt;For real-world language applications, detecting an out-of-distribution (OOD)
sample is helpful to alert users or reject such unreliable samples. However,
modern over-parameterized language models often produce overconfident
predictions for both in-distribution (ID) and OOD samples. In particular,
language models suffer from OOD samples with a similar semantic representation
to ID samples since these OOD samples lie near the ID manifold. A rejection
network can be trained with ID and diverse outlier samples to detect test OOD
samples, but explicitly collecting auxiliary OOD datasets brings an additional
burden for data collection. In this paper, we propose a simple but effective
method called Pseudo Outlier Exposure (POE) that constructs a surrogate OOD
dataset by sequentially masking tokens related to ID classes. The surrogate OOD
sample introduced by POE shows a similar representation to ID data, which is
most effective in training a rejection network. Our method does not require any
external OOD data and can be easily implemented within off-the-shelf
Transformers. A comprehensive comparison with state-of-the-art algorithms
demonstrates POE&apos;s competitiveness on several text classification benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jaeyoung Kim&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1&quot;&gt;Kyuheon Jung&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Na_D/0/1/0/all/0/1&quot;&gt;Dongbin Na&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_S/0/1/0/all/0/1&quot;&gt;Sion Jang&lt;/a&gt; (4), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_E/0/1/0/all/0/1&quot;&gt;Eunbin Park&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1&quot;&gt;Sungchul Choi&lt;/a&gt; (2) ((1) Gachon University, (2) Pukyong National University, (3) VUNO Inc, (4) Alchera Inc)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09456">
<title>A comparative analysis of SR-GAN models. (arXiv:2307.09456v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09456</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we evaluate the performance of multiple state-of-the-art SR
GAN (Super Resolution Generative Adversarial Network) models, ESRGAN,
Real-ESRGAN and EDSR, on a benchmark dataset of real-world images which undergo
degradation using a pipeline. Our results show that some models seem to
significantly increase the resolution of the input images while preserving
their visual quality, this is assessed using Tesseract OCR engine. We observe
that EDSR-BASE model from huggingface outperforms the remaining candidate
models in terms of both quantitative metrics and subjective visual quality
assessments with least compute overhead. Specifically, EDSR generates images
with higher peak signal-to-noise ratio (PSNR) and structural similarity index
(SSIM) values and are seen to return high quality OCR results with Tesseract
OCR engine. These findings suggest that EDSR is a robust and effective approach
for single-image super-resolution and may be particularly well-suited for
applications where high-quality visual fidelity is critical and optimized
compute.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikroo_F/0/1/0/all/0/1&quot;&gt;Fatemeh Rezapoor Nikroo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deshmukh_A/0/1/0/all/0/1&quot;&gt;Ajinkya Deshmukh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1&quot;&gt;Anantha Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tam_A/0/1/0/all/0/1&quot;&gt;Adrian Tam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_K/0/1/0/all/0/1&quot;&gt;Kaarthik Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noris_C/0/1/0/all/0/1&quot;&gt;Cleo Noris&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09474">
<title>ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning. (arXiv:2307.09474v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.09474</link>
<description rdf:parseType="Literal">&lt;p&gt;Human-AI interactivity is a critical aspect that reflects the usability of
multimodal large language models (MLLMs). However, existing end-to-end MLLMs
only allow users to interact with them through language instructions, leading
to the limitation of the interactive accuracy and efficiency. In this study, we
present precise referring instructions that utilize diverse reference
representations such as points and boxes as referring prompts to refer to the
special region. This enables MLLMs to focus on the region of interest and
achieve finer-grained interaction. Based on precise referring instruction, we
propose ChatSpot, a unified end-to-end multimodal large language model that
supports diverse forms of interactivity including mouse clicks, drag-and-drop,
and drawing boxes, which provides a more flexible and seamless interactive
experience. We also construct a multi-grained vision-language
instruction-following dataset based on existing datasets and GPT-4 generating.
Furthermore, we design a series of evaluation tasks to assess the effectiveness
of region recognition and interaction. Experimental results showcase ChatSpot&apos;s
promising performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Liang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_E/0/1/0/all/0/1&quot;&gt;En Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1&quot;&gt;Zheng Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jinrong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1&quot;&gt;Haoran Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hongyu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jianjian Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yuang Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1&quot;&gt;Runpei Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1&quot;&gt;Chunrui Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiangyu Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09476">
<title>Overthinking the Truth: Understanding how Language Models Process False Demonstrations. (arXiv:2307.09476v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09476</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern language models can imitate complex patterns through few-shot
learning, enabling them to complete challenging tasks without fine-tuning.
However, imitation can also lead models to reproduce inaccuracies or harmful
content if present in the context. We study harmful imitation through the lens
of a model&apos;s internal representations, and identify two related phenomena:
overthinking and false induction heads. The first phenomenon, overthinking,
appears when we decode predictions from intermediate layers, given correct vs.
incorrect few-shot demonstrations. At early layers, both demonstrations induce
similar model behavior, but the behavior diverges sharply at some &quot;critical
layer&quot;, after which the accuracy given incorrect demonstrations progressively
decreases. The second phenomenon, false induction heads, are a possible
mechanistic cause of overthinking: these are heads in late layers that attend
to and copy false information from previous demonstrations, and whose ablation
reduces overthinking. Beyond scientific understanding, our results suggest that
studying intermediate model computations could be a promising avenue for
understanding and guarding against harmful model behaviors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Halawi_D/0/1/0/all/0/1&quot;&gt;Danny Halawi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denain_J/0/1/0/all/0/1&quot;&gt;Jean-Stanislas Denain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1&quot;&gt;Jacob Steinhardt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2104.06474">
<title>On the Interpretability and Significance of Bias Metrics in Texts: a PMI-based Approach. (arXiv:2104.06474v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2104.06474</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, word embeddings have been widely used to measure biases in
texts. Even if they have proven to be effective in detecting a wide variety of
biases, metrics based on word embeddings lack transparency and
interpretability. We analyze an alternative PMI-based metric to quantify biases
in texts. It can be expressed as a function of conditional probabilities, which
provides a simple interpretation in terms of word co-occurrences. We also prove
that it can be approximated by an odds ratio, which allows estimating
confidence intervals and statistical significance of textual biases. This
approach produces similar results to metrics based on word embeddings when
capturing gender gaps of the real world embedded in large corpora.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valentini_F/0/1/0/all/0/1&quot;&gt;Francisco Valentini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosati_G/0/1/0/all/0/1&quot;&gt;Germ&amp;#xe1;n Rosati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blasi_D/0/1/0/all/0/1&quot;&gt;Dami&amp;#xe1;n Blasi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Slezak_D/0/1/0/all/0/1&quot;&gt;Diego Fernandez Slezak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Altszyler_E/0/1/0/all/0/1&quot;&gt;Edgar Altszyler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.02531">
<title>InitialGAN: A Language GAN with Completely Random Initialization. (arXiv:2208.02531v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2208.02531</link>
<description rdf:parseType="Literal">&lt;p&gt;Text generative models trained via Maximum Likelihood Estimation (MLE) suffer
from the notorious exposure bias problem, and Generative Adversarial Networks
(GANs) are shown to have potential to tackle this problem. Existing language
GANs adopt estimators like REINFORCE or continuous relaxations to model word
probabilities. The inherent limitations of such estimators lead current models
to rely on pre-training techniques (MLE pre-training or pre-trained
embeddings). Representation modeling methods which are free from those
limitations, however, are seldomly explored because of their poor performance
in previous attempts. Our analyses reveal that invalid sampling methods and
unhealthy gradients are the main contributors to such unsatisfactory
performance. In this work, we present two techniques to tackle these problems:
dropout sampling and fully normalized LSTM. Based on these two techniques, we
propose InitialGAN whose parameters are randomly initialized in full. Besides,
we introduce a new evaluation metric, Least Coverage Rate, to better evaluate
the quality of generated samples. The experimental results demonstrate that
InitialGAN outperforms both MLE and other compared models. To the best of our
knowledge, it is the first time a language GAN can outperform MLE without using
any pre-training techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_D/0/1/0/all/0/1&quot;&gt;Da Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qing Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.14348">
<title>Synthetic Text Generation with Differential Privacy: A Simple and Practical Recipe. (arXiv:2210.14348v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2210.14348</link>
<description rdf:parseType="Literal">&lt;p&gt;Privacy concerns have attracted increasing attention in data-driven products
due to the tendency of machine learning models to memorize sensitive training
data. Generating synthetic versions of such data with a formal privacy
guarantee, such as differential privacy (DP), provides a promising path to
mitigating these privacy concerns, but previous approaches in this direction
have typically failed to produce synthetic data of high quality. In this work,
we show that a simple and practical recipe in the text domain is effective:
simply fine-tuning a pretrained generative language model with DP enables the
model to generate useful synthetic text with strong privacy protection. Through
extensive empirical analyses on both benchmark and private customer data, we
demonstrate that our method produces synthetic text that is competitive in
terms of utility with its non-private counterpart, meanwhile providing strong
protection against potential privacy leakages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1&quot;&gt;Xiang Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Inan_H/0/1/0/all/0/1&quot;&gt;Huseyin A. Inan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuechen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1&quot;&gt;Girish Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McAnallen_J/0/1/0/all/0/1&quot;&gt;Julia McAnallen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shajari_H/0/1/0/all/0/1&quot;&gt;Hoda Shajari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Huan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levitan_D/0/1/0/all/0/1&quot;&gt;David Levitan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sim_R/0/1/0/all/0/1&quot;&gt;Robert Sim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.13066">
<title>A Human Word Association based model for topic detection in social networks. (arXiv:2301.13066v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2301.13066</link>
<description rdf:parseType="Literal">&lt;p&gt;With the widespread use of social networks, detecting the topics discussed in
these networks has become a significant challenge. The current works are mainly
based on frequent pattern mining or semantic relations, and the language
structure is not considered. The meaning of language structural methods is to
discover the relationship between words and how humans understand them.
Therefore, this paper uses the Concept of the Imitation of the Mental Ability
of Word Association to propose a topic detection framework in social networks.
This framework is based on the Human Word Association method. A special
extraction algorithm has also been designed for this purpose. The performance
of this method is evaluated on the FA-CUP dataset. It is a benchmark dataset in
the field of topic detection. The results show that the proposed method is a
good improvement compared to other methods, based on the Topic-recall and the
keyword F1 measure. Also, most of the previous works in the field of topic
detection are limited to the English language, and the Persian language,
especially microblogs written in this language, is considered a low-resource
language. Therefore, a data set of Telegram posts in the Farsi language has
been collected. Applying the proposed method to this dataset also shows that
this method works better than other topic detection methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khadivi_M/0/1/0/all/0/1&quot;&gt;Mehrdad Ranjbar Khadivi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akbarpour_S/0/1/0/all/0/1&quot;&gt;Shahin Akbarpour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feizi_Derakhshi_M/0/1/0/all/0/1&quot;&gt;Mohammad-Reza Feizi-Derakhshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anari_B/0/1/0/all/0/1&quot;&gt;Babak Anari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.13816">
<title>Execution-based Code Generation using Deep Reinforcement Learning. (arXiv:2301.13816v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.13816</link>
<description rdf:parseType="Literal">&lt;p&gt;The utilization of programming language (PL) models, pre-trained on
large-scale code corpora, as a means of automating software engineering
processes has demonstrated considerable potential in streamlining various code
generation tasks such as code completion, code translation, and program
synthesis. However, current approaches mainly rely on supervised fine-tuning
objectives borrowed from text generation, neglecting unique sequence-level
characteristics of code, including but not limited to compilability as well as
syntactic and functional correctness. To address this limitation, we propose
PPOCoder, a new framework for code generation that synergistically combines
pre-trained PL models with Proximal Policy Optimization (PPO) which is a widely
used deep reinforcement learning technique. By utilizing non-differentiable
feedback from code execution and structure alignment, PPOCoder seamlessly
integrates external code-specific knowledge into the model optimization
process. It&apos;s important to note that PPOCoder is a task-agnostic and
model-agnostic framework that can be used across different code generation
tasks and PLs. Extensive experiments on three code generation tasks demonstrate
the effectiveness of our proposed approach compared to SOTA methods, achieving
significant improvements in compilation success rates and functional
correctness across different PLs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shojaee_P/0/1/0/all/0/1&quot;&gt;Parshin Shojaee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1&quot;&gt;Aneesh Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tipirneni_S/0/1/0/all/0/1&quot;&gt;Sindhu Tipirneni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1&quot;&gt;Chandan K. Reddy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.09775">
<title>Persian topic detection based on Human Word association and graph embedding. (arXiv:2302.09775v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2302.09775</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a framework to detect topics in social media based
on Human Word Association. Identifying topics discussed in these media has
become a critical and significant challenge. Most of the work done in this area
is in English, but much has been done in the Persian language, especially
microblogs written in Persian. Also, the existing works focused more on
exploring frequent patterns or semantic relationships and ignored the
structural methods of language. In this paper, a topic detection framework
using HWA, a method for Human Word Association, is proposed. This method uses
the concept of imitation of mental ability for word association. This method
also calculates the Associative Gravity Force that shows how words are related.
Using this parameter, a graph can be generated. The topics can be extracted by
embedding this graph and using clustering methods. This approach has been
applied to a Persian language dataset collected from Telegram. Several
experimental studies have been performed to evaluate the proposed framework&apos;s
performance. Experimental results show that this approach works better than
other topic detection methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ranjbar_Khadivi_M/0/1/0/all/0/1&quot;&gt;Mehrdad Ranjbar-Khadivi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akbarpour_S/0/1/0/all/0/1&quot;&gt;Shahin Akbarpour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feizi_Derakhshi_M/0/1/0/all/0/1&quot;&gt;Mohammad-Reza Feizi-Derakhshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anari_B/0/1/0/all/0/1&quot;&gt;Babak Anari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.03642">
<title>Jointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs. (arXiv:2305.03642v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.03642</link>
<description rdf:parseType="Literal">&lt;p&gt;Results from Randomized Controlled Trials (RCTs) establish the comparative
effectiveness of interventions, and are in turn critical inputs for
evidence-based care. However, results from RCTs are presented in (often
unstructured) natural language articles describing the design, execution, and
outcomes of trials; clinicians must manually extract findings pertaining to
interventions and outcomes of interest from such articles. This onerous manual
process has motivated work on (semi-)automating extraction of structured
evidence from trial reports. In this work we propose and evaluate a
text-to-text model built on instruction-tuned Large Language Models (LLMs) to
jointly extract Interventions, Outcomes, and Comparators (ICO elements) from
clinical abstracts, and infer the associated results reported. Manual (expert)
and automated evaluations indicate that framing evidence extraction as a
conditional generation task and fine-tuning LLMs for this purpose realizes
considerable ($\sim$20 point absolute F1 score) gains over the previous SOTA.
We perform ablations and error analyses to assess aspects that contribute to
model performance, and to highlight potential directions for further
improvements. We apply our model to a collection of published RCTs through
mid-2022, and release a searchable database of structured findings:
&lt;a href=&quot;http://ico-relations.ebm-nlp.com&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wadhwa_S/0/1/0/all/0/1&quot;&gt;Somin Wadhwa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DeYoung_J/0/1/0/all/0/1&quot;&gt;Jay DeYoung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nye_B/0/1/0/all/0/1&quot;&gt;Benjamin Nye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amir_S/0/1/0/all/0/1&quot;&gt;Silvio Amir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1&quot;&gt;Byron C. Wallace&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.09360">
<title>GIFT: Graph-Induced Fine-Tuning for Multi-Party Conversation Understanding. (arXiv:2305.09360v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.09360</link>
<description rdf:parseType="Literal">&lt;p&gt;Addressing the issues of who saying what to whom in multi-party conversations
(MPCs) has recently attracted a lot of research attention. However, existing
methods on MPC understanding typically embed interlocutors and utterances into
sequential information flows, or utilize only the superficial of inherent graph
structures in MPCs. To this end, we present a plug-and-play and lightweight
method named graph-induced fine-tuning (GIFT) which can adapt various
Transformer-based pre-trained language models (PLMs) for universal MPC
understanding. In detail, the full and equivalent connections among utterances
in regular Transformer ignore the sparse but distinctive dependency of an
utterance on another in MPCs. To distinguish different relationships between
utterances, four types of edges are designed to integrate graph-induced signals
into attention mechanisms to refine PLMs originally designed for processing
sequential texts. We evaluate GIFT by implementing it into three PLMs, and test
the performance on three downstream tasks including addressee recognition,
speaker identification and response selection. Experimental results show that
GIFT can significantly improve the performance of three PLMs on three
downstream tasks and two benchmarks with only 4 additional parameters per
encoding layer, achieving new state-of-the-art performance on MPC
understanding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jia-Chen Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1&quot;&gt;Zhen-Hua Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Quan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Cong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1&quot;&gt;Guoping Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12421">
<title>Evaluating Open-QA Evaluation. (arXiv:2305.12421v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12421</link>
<description rdf:parseType="Literal">&lt;p&gt;This study focuses on the evaluation of the Open Question Answering (Open-QA)
task, which can directly estimate the factuality of large language models
(LLMs). Current automatic evaluation methods have shown limitations, indicating
that human evaluation still remains the most reliable approach. We introduce a
new task, Evaluating QA Evaluation (QA-Eval) and the corresponding dataset
EVOUNA, designed to assess the accuracy of AI-generated answers in relation to
standard answers within Open-QA. Our evaluation of these methods utilizes
human-annotated results to measure their performance. Specifically, the work
investigates methods that show high correlation with human evaluations, deeming
them more reliable. We also discuss the pitfalls of current methods and methods
to improve LLM-based evaluators. We believe this new QA-Eval task and
corresponding dataset EVOUNA will facilitate the development of more effective
automatic evaluation tools and prove valuable for future research in this area.
All resources are available at \url{https://github.com/wangcunxiang/QA-Eval}
and it is under the Apache-2.0 License.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Cunxiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;Sirui Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qipeng Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhikun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1&quot;&gt;Bowen Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yidong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xiangkun Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yue Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14279">
<title>Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs. (arXiv:2305.14279v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14279</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have achieved widespread success on a variety of
in-context few-shot tasks, but this success is typically evaluated via
correctness rather than consistency. We argue that self-consistency is an
important criteria for valid multi-step reasoning in tasks where the solution
is composed of the answers to multiple sub-steps. We propose two types of
self-consistency that are particularly important for multi-step reasoning --
hypothetical consistency (a model&apos;s ability to predict what its output would be
in a hypothetical other context) and compositional consistency (consistency of
a model&apos;s final outputs when intermediate sub-steps are replaced with the
model&apos;s outputs for those steps). We demonstrate that multiple variants of the
GPT-3/-4 models exhibit poor consistency rates across both types of consistency
on a variety of tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1&quot;&gt;Angelica Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phang_J/0/1/0/all/0/1&quot;&gt;Jason Phang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parrish_A/0/1/0/all/0/1&quot;&gt;Alicia Parrish&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Padmakumar_V/0/1/0/all/0/1&quot;&gt;Vishakh Padmakumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1&quot;&gt;Chen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1&quot;&gt;Samuel R. Bowman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1&quot;&gt;Kyunghyun Cho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15656">
<title>SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate via Compiler Co-design. (arXiv:2306.15656v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15656</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces SparseOptimizer, a novel deep learning optimizer that
exploits Moreau-Yosida regularization to naturally induce sparsity in large
language models such as BERT, ALBERT and GPT. Key to the design of
SparseOptimizer is an embedded shrinkage operator, which imparts sparsity
directly within the optimization process. This operator, backed by a sound
theoretical framework, includes an analytical solution, thereby reinforcing the
optimizer&apos;s robustness and efficacy. Crucially, SparseOptimizer&apos;s plug-and-play
functionality eradicates the need for code modifications, making it a
universally adaptable tool for a wide array of large language models. Empirical
evaluations on benchmark datasets such as GLUE, RACE, SQuAD1, and SQuAD2
confirm that SparseBERT and SparseALBERT, when sparsified using
SparseOptimizer, achieve performance comparable to their dense counterparts,
BERT and ALBERT, while significantly reducing their parameter count. Further,
this work proposes an innovative optimizer-compiler co-design strategy,
demonstrating the potential of inference acceleration (\textbf{3.37x},
\textbf{6.30x}, and \textbf{7.15x} in comparison with Pytorch, TensorFlow, and
LLVM generic compile, respectively) in SparseBERT when paired with an
appropriately designed compiler. This study represents a significant step
forward in the evolution of efficient, scalable, and high-performing large
language models, setting a precedent for future exploration and optimization in
this domain. The SparseOptimizer code and SparseALBERT model will be publicly
available upon paper acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1&quot;&gt;Fu-Ming Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15788">
<title>Evaluating GPT-3.5 and GPT-4 on Grammatical Error Correction for Brazilian Portuguese. (arXiv:2306.15788v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15788</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the effectiveness of GPT-3.5 and GPT-4, two large language
models, as Grammatical Error Correction (GEC) tools for Brazilian Portuguese
and compare their performance against Microsoft Word and Google Docs. We
introduce a GEC dataset for Brazilian Portuguese with four categories: Grammar,
Spelling, Internet, and Fast typing. Our results show that while GPT-4 has
higher recall than other methods, LLMs tend to have lower precision, leading to
overcorrection. This study demonstrates the potential of LLMs as practical GEC
tools for Brazilian Portuguese and encourages further exploration of LLMs for
non-English languages and other educational settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Penteado_M/0/1/0/all/0/1&quot;&gt;Maria Carolina Penteado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_F/0/1/0/all/0/1&quot;&gt;F&amp;#xe1;bio Perez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02839">
<title>Enhancing LLM with Evolutionary Fine Tuning for News Summary Generation. (arXiv:2307.02839v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02839</link>
<description rdf:parseType="Literal">&lt;p&gt;News summary generation is an important task in the field of intelligence
analysis, which can provide accurate and comprehensive information to help
people better understand and respond to complex real-world events. However,
traditional news summary generation methods face some challenges, which are
limited by the model itself and the amount of training data, as well as the
influence of text noise, making it difficult to generate reliable information
accurately. In this paper, we propose a new paradigm for news summary
generation using LLM with powerful natural language understanding and
generative capabilities. We use LLM to extract multiple structured event
patterns from the events contained in news paragraphs, evolve the event pattern
population with genetic algorithm, and select the most adaptive event pattern
to input into the LLM to generate news summaries. A News Summary Generator
(NSG) is designed to select and evolve the event pattern populations and
generate news summaries. The experimental results show that the news summary
generator is able to generate accurate and reliable news summaries with some
generalization ability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1&quot;&gt;Le Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaolin Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03109">
<title>A Survey on Evaluation of Large Language Models. (arXiv:2307.03109v5 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03109</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) are gaining increasing popularity in both
academia and industry, owing to their unprecedented performance in various
applications. As LLMs continue to play a vital role in both research and daily
use, their evaluation becomes increasingly critical, not only at the task
level, but also at the society level for better understanding of their
potential risks. Over the past years, significant efforts have been made to
examine LLMs from various perspectives. This paper presents a comprehensive
review of these evaluation methods for LLMs, focusing on three key dimensions:
what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide
an overview from the perspective of evaluation tasks, encompassing general
natural language processing tasks, reasoning, medical usage, ethics,
educations, natural and social sciences, agent applications, and other areas.
Secondly, we answer the `where&apos; and `how&apos; questions by diving into the
evaluation methods and benchmarks, which serve as crucial components in
assessing performance of LLMs. Then, we summarize the success and failure cases
of LLMs in different tasks. Finally, we shed light on several future challenges
that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to
researchers in the realm of LLMs evaluation, thereby aiding the development of
more proficient LLMs. Our key point is that evaluation should be treated as an
essential discipline to better assist the development of LLMs. We consistently
maintain the related open-source materials at:
https://github.com/MLGroupJLU/LLM-eval-survey.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1&quot;&gt;Yupeng Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jindong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1&quot;&gt;Kaijie Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Linyi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1&quot;&gt;Xiaoyuan Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Cunxiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yidong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1&quot;&gt;Wei Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yue Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1&quot;&gt;Yi Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1&quot;&gt;Philip S. Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qiang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xing Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04964">
<title>Secrets of RLHF in Large Language Models Part I: PPO. (arXiv:2307.04964v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04964</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have formulated a blueprint for the advancement
of artificial general intelligence. Its primary objective is to function as a
human-centric (helpful, honest, and harmless) assistant. Alignment with humans
assumes paramount significance, and reinforcement learning with human feedback
(RLHF) emerges as the pivotal technological paradigm underpinning this pursuit.
Current technical routes usually include \textbf{reward models} to measure
human preferences, \textbf{Proximal Policy Optimization} (PPO) to optimize
policy model outputs, and \textbf{process supervision} to improve step-by-step
reasoning capabilities. However, due to the challenges of reward design,
environment interaction, and agent training, coupled with huge trial and error
cost of large language models, there is a significant barrier for AI
researchers to motivate the development of technical alignment and safe landing
of LLMs. The stable training of RLHF has still been a puzzle. In the first
report, we dissect the framework of RLHF, re-evaluate the inner workings of
PPO, and explore how the parts comprising PPO algorithms impact policy agent
training. We identify policy constraints being the key factor for the effective
implementation of the PPO algorithm. Therefore, we explore the PPO-max, an
advanced version of PPO algorithm, to efficiently improve the training
stability of the policy model. Based on our main results, we perform a
comprehensive analysis of RLHF abilities compared with SFT models and ChatGPT.
The absence of open-source implementations has posed significant challenges to
the investigation of LLMs alignment. Therefore, we are eager to release
technical reports, reward models and PPO codes, aiming to make modest
contributions to the advancement of LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1&quot;&gt;Rui Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_S/0/1/0/all/0/1&quot;&gt;Shihan Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Songyang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1&quot;&gt;Yuan Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1&quot;&gt;Wei Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Binghai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1&quot;&gt;Senjie Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuhao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1&quot;&gt;Limao Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1&quot;&gt;Zhiheng Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1&quot;&gt;Nuo Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_W/0/1/0/all/0/1&quot;&gt;Wenbin Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1&quot;&gt;Minghao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1&quot;&gt;Cheng Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Zhangyue Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weng_R/0/1/0/all/0/1&quot;&gt;Rongxiang Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1&quot;&gt;Wensen Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Haoran Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1&quot;&gt;Tianxiang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1&quot;&gt;Hang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1&quot;&gt;Tao Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1&quot;&gt;Xipeng Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xuanjing Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07924">
<title>Communicative Agents for Software Development. (arXiv:2307.07924v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07924</link>
<description rdf:parseType="Literal">&lt;p&gt;Software engineering is a domain characterized by intricate decision-making
processes, often relying on nuanced intuition and consultation. Recent
advancements in deep learning have started to revolutionize software
engineering practices through elaborate designs implemented at various stages
of software development. In this paper, we present an innovative paradigm that
leverages large language models (LLMs) throughout the entire software
development process, streamlining and unifying key processes through natural
language communication, thereby eliminating the need for specialized models at
each phase. At the core of this paradigm lies ChatDev, a virtual chat-powered
software development company that mirrors the established waterfall model,
meticulously dividing the development process into four distinct chronological
stages: designing, coding, testing, and documenting. Each stage engages a team
of agents, such as programmers, code reviewers, and test engineers, fostering
collaborative dialogue and facilitating a seamless workflow. The chat chain
acts as a facilitator, breaking down each stage into atomic subtasks. This
enables dual roles, allowing for proposing and validating solutions through
context-aware communication, leading to efficient resolution of specific
subtasks. The instrumental analysis of ChatDev highlights its remarkable
efficacy in software generation, enabling the completion of the entire software
development process in under seven minutes at a cost of less than one dollar.
It not only identifies and alleviates potential vulnerabilities but also
rectifies potential hallucinations while maintaining commendable efficiency and
cost-effectiveness. The potential of ChatDev unveils fresh possibilities for
integrating LLMs into the realm of software development.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1&quot;&gt;Chen Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cong_X/0/1/0/all/0/1&quot;&gt;Xin Cong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Cheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weize Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yusheng Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Juyuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;Maosong Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.09550">
<title>Life of PII -- A PII Obfuscation Transformer. (arXiv:2305.09550v2 [cs.CL] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2305.09550</link>
<description rdf:parseType="Literal">&lt;p&gt;Protecting sensitive information is crucial in today&apos;s world of Large
Language Models (LLMs) and data-driven services. One common method used to
preserve privacy is by using data perturbation techniques to reduce
overreaching utility of (sensitive) Personal Identifiable Information (PII)
data while maintaining its statistical and semantic properties. Data
perturbation methods often result in significant information loss, making them
impractical for use. In this paper, we propose &apos;Life of PII&apos;, a novel
Obfuscation Transformer framework for transforming PII into faux-PII while
preserving the original information, intent, and context as much as possible.
Our approach includes an API to interface with the given document, a
configuration-based obfuscator, and a model based on the Transformer
architecture, which has shown high context preservation and performance in
natural language processing tasks and LLMs.
&lt;/p&gt;
&lt;p&gt;Our Transformer-based approach learns mapping between the original PII and
its transformed faux-PII representation, which we call &quot;obfuscated&quot; data. Our
experiments demonstrate that our method, called Life of PII, outperforms
traditional data perturbation techniques in terms of both utility preservation
and privacy protection. We show that our approach can effectively reduce
utility loss while preserving the original information, offering greater
flexibility in the trade-off between privacy protection and data utility. Our
work provides a solution for protecting PII in various real-world applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deshmukh_A/0/1/0/all/0/1&quot;&gt;Ajinkya Deshmukh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banthia_S/0/1/0/all/0/1&quot;&gt;Saumya Banthia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1&quot;&gt;Anantha Sharma&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>