<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-06T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02489" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02491" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02493" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02495" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02499" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02502" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02503" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02507" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02511" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02514" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02516" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02588" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02591" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02599" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02615" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02616" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02620" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02626" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02631" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02637" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02661" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02663" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02671" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02690" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02691" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02694" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02709" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02717" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02728" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02730" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02738" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02752" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02759" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02762" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02770" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02791" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02792" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02797" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02798" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02820" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02839" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02867" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02882" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02891" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02909" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02912" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02933" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02947" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02971" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02984" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03007" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03015" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03039" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03056" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03067" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03084" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03104" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03109" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03119" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03135" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03162" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03170" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03171" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03175" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2008.02215" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2102.11529" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2103.09118" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.13557" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.07856" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.03447" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.04779" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.05251" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.14896" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.02499" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.00443" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.09811" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.12313" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.13166" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.05449" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.07729" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.09844" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.02216" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.03789" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.09378" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18486" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.03263" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04220" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04637" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10077" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10259" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14096" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15782" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15880" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00968" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01301" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02472" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01848" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.02489">
<title>Visual Question Answering (VQA) on Images with Superimposed Text. (arXiv:2307.02489v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02489</link>
<description rdf:parseType="Literal">&lt;p&gt;Superimposed text annotations have been under-investigated, yet are
ubiquitous, useful and important, especially in medical images. Medical images
also highlight the challenges posed by low resolution, noise and superimposed
textual meta-information. Therefor we probed the impact of superimposing text
onto medical images on VQA. Our results revealed that this textual
meta-information can be added without severely degrading key measures of VQA
performance. Our findings are significant because they validate the practice of
superimposing text on images, even for medical images subjected to the VQA task
using AI techniques. The work helps advance understanding of VQA in general
and, in particular, in the domain of healthcare and medicine.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kodali_V/0/1/0/all/0/1&quot;&gt;Venkat Kodali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berleant_D/0/1/0/all/0/1&quot;&gt;Daniel Berleant&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02491">
<title>TablEye: Seeing small Tables through the Lens of Images. (arXiv:2307.02491v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.02491</link>
<description rdf:parseType="Literal">&lt;p&gt;The exploration of few-shot tabular learning becomes imperative. Tabular data
is a versatile representation that captures diverse information, yet it is not
exempt from limitations, property of data and model size. Labeling extensive
tabular data can be challenging, and it may not be feasible to capture every
important feature. Few-shot tabular learning, however, remains relatively
unexplored, primarily due to scarcity of shared information among independent
datasets and the inherent ambiguity in defining boundaries within tabular data.
To the best of our knowledge, no meaningful and unrestricted few-shot tabular
learning techniques have been developed without imposing constraints on the
dataset. In this paper, we propose an innovative framework called TablEye,
which aims to overcome the limit of forming prior knowledge for tabular data by
adopting domain transformation. It facilitates domain transformation by
generating tabular images, which effectively conserve the intrinsic semantics
of the original tabular data. This approach harnesses rigorously tested
few-shot learning algorithms and embedding functions to acquire and apply prior
knowledge. Leveraging shared data domains allows us to utilize this prior
knowledge, originally learned from the image domain. Specifically, TablEye
demonstrated a superior performance by outstripping the TabLLM in a 4-shot task
with a maximum 0.11 AUC and a STUNT in a 1- shot setting, where it led on
average by 3.17% accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seung-eon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sang-Chul Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02493">
<title>FREEDOM: Target Label &amp; Source Data &amp; Domain Information-Free Multi-Source Domain Adaptation for Unsupervised Personalization. (arXiv:2307.02493v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.02493</link>
<description rdf:parseType="Literal">&lt;p&gt;From a service perspective, Multi-Source Domain Adaptation (MSDA) is a
promising scenario to adapt a deployed model to a client&apos;s dataset. It can
provide adaptation without a target label and support the case where a source
dataset is constructed from multiple domains. However, it is impractical,
wherein its training heavily relies on prior domain information of the
multi-source dataset -- how many domains exist and the domain label of each
data sample. Moreover, MSDA requires both source and target datasets
simultaneously (physically), causing storage limitations on the client device
or data privacy issues by transferring client data to a server. For a more
practical scenario of model adaptation from a service provider&apos;s point of view,
we relax these constraints and present a novel problem scenario of Three-Free
Domain Adaptation, namely TFDA, where 1) target labels, 2) source dataset, and
mostly 3) source domain information (domain labels + the number of domains) are
unavailable. Under the problem scenario, we propose a practical adaptation
framework called FREEDOM. It leverages the power of the generative model,
disentangling data into class and style aspects, where the style is defined as
the class-independent information from the source data and designed with a
nonparametric Bayesian approach. In the adaptation stage, FREEDOM aims to match
the source class distribution with the target&apos;s under the philosophy that class
distribution is consistent even if the style is different; after then, only
part of the classification model is deployed as a personalized network. As a
result, FREEDOM achieves state-of-the-art or comparable performance even
without domain information, with reduced final model size on the target side,
independent of the number of source domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1&quot;&gt;Eunju Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_G/0/1/0/all/0/1&quot;&gt;Gyusang Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Youn_C/0/1/0/all/0/1&quot;&gt;Chan-Hyun Youn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02495">
<title>Anomaly detection in image or latent space of patch-based auto-encoders for industrial image analysis. (arXiv:2307.02495v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02495</link>
<description rdf:parseType="Literal">&lt;p&gt;We study several methods for detecting anomalies in color images, constructed
on patch-based auto-encoders. Wecompare the performance of three types of
methods based, first, on the error between the original image and its
reconstruction,second, on the support estimation of the normal image
distribution in the latent space, and third, on the error between the
originalimage and a restored version of the reconstructed image. These methods
are evaluated on the industrial image database MVTecADand compared to two
competitive state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pinon_N/0/1/0/all/0/1&quot;&gt;Nicolas Pinon&lt;/a&gt; (MYRIAD), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trombetta_R/0/1/0/all/0/1&quot;&gt;Robin Trombetta&lt;/a&gt; (MYRIAD), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lartizien_C/0/1/0/all/0/1&quot;&gt;Carole Lartizien&lt;/a&gt; (MYRIAD)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02499">
<title>mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding. (arXiv:2307.02499v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.02499</link>
<description rdf:parseType="Literal">&lt;p&gt;Document understanding refers to automatically extract, analyze and
comprehend information from various types of digital documents, such as a web
page. Existing Multi-model Large Language Models (MLLMs), including mPLUG-Owl,
have demonstrated promising zero-shot capabilities in shallow OCR-free text
recognition, indicating their potential for OCR-free document understanding.
Nevertheless, without in-domain training, these models tend to ignore
fine-grained OCR features, such as sophisticated tables or large blocks of
text, which are essential for OCR-free document understanding. In this paper,
we propose mPLUG-DocOwl based on mPLUG-Owl for OCR-free document understanding.
Specifically, we first construct a instruction tuning dataset featuring a wide
range of visual-text understanding tasks. Then, we strengthen the OCR-free
document understanding ability by jointly train the model on language-only,
general vision-and-language, and document instruction tuning dataset with our
unified instruction tuning strategy. We also build an OCR-free document
instruction understanding evaluation set LLMDoc to better compare models&apos;
capabilities on instruct compliance and document understanding. Experimental
results show that our model outperforms existing multi-modal models,
demonstrating its strong ability of document understanding. Besides, without
specific fine-tuning, mPLUG-DocOwl generalizes well on various downstream
tasks. Our code, models, training data and evaluation set are available at
https://github.com/X-PLUG/mPLUG-DocOwl.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jiabo Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1&quot;&gt;Anwen Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Haiyang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1&quot;&gt;Qinghao Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1&quot;&gt;Ming Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dan_Y/0/1/0/all/0/1&quot;&gt;Yuhao Dan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1&quot;&gt;Chenlin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1&quot;&gt;Guohai Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenliang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1&quot;&gt;Junfeng Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_Q/0/1/0/all/0/1&quot;&gt;Qian Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Ji Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Fei Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02502">
<title>Math Agents: Computational Infrastructure, Mathematical Embedding, and Genomics. (arXiv:2307.02502v1 [q-bio.OT])</title>
<link>http://arxiv.org/abs/2307.02502</link>
<description rdf:parseType="Literal">&lt;p&gt;The advancement in generative AI could be boosted with more accessible
mathematics. Beyond human-AI chat, large language models (LLMs) are emerging in
programming, algorithm discovery, and theorem proving, yet their genomics
application is limited. This project introduces Math Agents and mathematical
embedding as fresh entries to the &quot;Moore&apos;s Law of Mathematics&quot;, using a
GPT-based workflow to convert equations from literature into LaTeX and Python
formats. While many digital equation representations exist, there&apos;s a lack of
automated large-scale evaluation tools. LLMs are pivotal as linguistic user
interfaces, providing natural language access for human-AI chat and formal
languages for large-scale AI-assisted computational infrastructure. Given the
infinite formal possibility spaces, Math Agents, which interact with math,
could potentially shift us from &quot;big data&quot; to &quot;big math&quot;. Math, unlike the more
flexible natural language, has properties subject to proof, enabling its use
beyond traditional applications like high-validation math-certified icons for
AI alignment aims. This project aims to use Math Agents and mathematical
embeddings to address the ageing issue in information systems biology by
applying multiscalar physics mathematics to disease models and genomic data.
Generative AI with episodic memory could help analyse causal relations in
longitudinal health records, using SIR Precision Health models. Genomic data is
suggested for addressing the unsolved Alzheimer&apos;s disease problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Swan_M/0/1/0/all/0/1&quot;&gt;Melanie Swan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kido_T/0/1/0/all/0/1&quot;&gt;Takashi Kido&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Roland_E/0/1/0/all/0/1&quot;&gt;Eric Roland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Santos_R/0/1/0/all/0/1&quot;&gt;Renato P. dos Santos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02503">
<title>Natural Language Generation and Understanding of Big Code for AI-Assisted Programming: A Review. (arXiv:2307.02503v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2307.02503</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper provides a comprehensive review of the literature concerning the
utilization of Natural Language Processing (NLP) techniques, with a particular
focus on transformer-based large language models (LLMs) trained using Big Code,
within the domain of AI-assisted programming tasks. LLMs, augmented with
software naturalness, have played a crucial role in facilitating AI-assisted
programming applications, including code generation, code completion, code
translation, code refinement, code summarization, defect detection, and clone
detection. Notable examples of such applications include the GitHub Copilot
powered by OpenAI&apos;s Codex and DeepMind AlphaCode. This paper presents an
overview of the major LLMs and their applications in downstream tasks related
to AI-assisted programming. Furthermore, it explores the challenges and
opportunities associated with incorporating NLP techniques with software
naturalness in these applications, with a discussion on extending AI-assisted
programming capabilities to Apple&apos;s Xcode for mobile software development. This
paper also presents the challenges of and opportunities for incorporating NLP
techniques with software naturalness, empowering developers with advanced
coding assistance and streamlining the software development process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_M/0/1/0/all/0/1&quot;&gt;Man Fai Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1&quot;&gt;Shangxin Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hang_C/0/1/0/all/0/1&quot;&gt;Ching Nam Hang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_S/0/1/0/all/0/1&quot;&gt;Siu Wai Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Chee Wei Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02507">
<title>STS-CCL: Spatial-Temporal Synchronous Contextual Contrastive Learning for Urban Traffic Forecasting. (arXiv:2307.02507v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.02507</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficiently capturing the complex spatiotemporal representations from
large-scale unlabeled traffic data remains to be a challenging task. In
considering of the dilemma, this work employs the advanced contrastive learning
and proposes a novel Spatial-Temporal Synchronous Contextual Contrastive
Learning (STS-CCL) model. First, we elaborate the basic and strong augmentation
methods for spatiotemporal graph data, which not only perturb the data in terms
of graph structure and temporal characteristics, but also employ a
learning-based dynamic graph view generator for adaptive augmentation. Second,
we introduce a Spatial-Temporal Synchronous Contrastive Module (STS-CM) to
simultaneously capture the decent spatial-temporal dependencies and realize
graph-level contrasting. To further discriminate node individuals in negative
filtering, a Semantic Contextual Contrastive method is designed based on
semantic features and spatial heterogeneity, achieving node-level contrastive
learning along with negative filtering. Finally, we present a hard mutual-view
contrastive training scheme and extend the classic contrastive loss to an
integrated objective function, yielding better performance. Extensive
experiments and evaluations demonstrate that building a predictor upon STS-CCL
contrastive learning model gains superior performance than existing traffic
forecasting benchmarks. The proposed STS-CCL is highly suitable for large
datasets with only a few labeled data and other spatiotemporal tasks with data
scarcity issue.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lincan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kaixiang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1&quot;&gt;Fengji Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1&quot;&gt;Jichao Bi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02511">
<title>Diffusion Models for Computational Design at the Example of Floor Plans. (arXiv:2307.02511v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.02511</link>
<description rdf:parseType="Literal">&lt;p&gt;AI Image generators based on diffusion models are widely discussed recently
for their capability to create images from simple text prompts. But, for
practical use in civil engineering they need to be able to create specific
construction plans for given constraints. Within this paper we explore the
capabilities of those diffusion-based AI generators for computational design at
the example of floor plans and identify their current limitation. We explain
how the diffusion-models work and propose new diffusion models with improved
semantic encoding. In several experiments we show that we can improve validity
of generated floor plans from 6% to 90% and query performance for different
examples. We identify short comings and derive future research challenges of
those models and discuss the need to combine diffusion models with building
information modelling. With this we provide key insights into the current state
and future directions for diffusion models in civil engineering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ploennigs_J/0/1/0/all/0/1&quot;&gt;Joern Ploennigs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berger_M/0/1/0/all/0/1&quot;&gt;Markus Berger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02514">
<title>Exploring Multimodal Approaches for Alzheimer&apos;s Disease Detection Using Patient Speech Transcript and Audio Data. (arXiv:2307.02514v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2307.02514</link>
<description rdf:parseType="Literal">&lt;p&gt;Alzheimer&apos;s disease (AD) is a common form of dementia that severely impacts
patient health. As AD impairs the patient&apos;s language understanding and
expression ability, the speech of AD patients can serve as an indicator of this
disease. This study investigates various methods for detecting AD using
patients&apos; speech and transcripts data from the DementiaBank Pitt database. The
proposed approach involves pre-trained language models and Graph Neural Network
(GNN) that constructs a graph from the speech transcript, and extracts features
using GNN for AD detection. Data augmentation techniques, including synonym
replacement, GPT-based augmenter, and so on, were used to address the small
dataset size. Audio data was also introduced, and WavLM model was used to
extract audio features. These features were then fused with text features using
various methods. Finally, a contrastive learning approach was attempted by
converting speech transcripts back to audio and using it for contrastive
learning with the original audio. We conducted intensive experiments and
analysis on the above methods. Our findings shed light on the challenges and
potential solutions in AD detection using speech and audio data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cai_H/0/1/0/all/0/1&quot;&gt;Hongmin Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaoke Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhengliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liao_W/0/1/0/all/0/1&quot;&gt;Wenxiong Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dai_H/0/1/0/all/0/1&quot;&gt;Haixing Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zihao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhu_D/0/1/0/all/0/1&quot;&gt;Dajiang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ren_H/0/1/0/all/0/1&quot;&gt;Hui Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Quanzheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tianming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02516">
<title>Exploring new ways: Enforcing representational dissimilarity to learn new features and reduce error consistency. (arXiv:2307.02516v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.02516</link>
<description rdf:parseType="Literal">&lt;p&gt;Independently trained machine learning models tend to learn similar features.
Given an ensemble of independently trained models, this results in correlated
predictions and common failure modes. Previous attempts focusing on
decorrelation of output predictions or logits yielded mixed results,
particularly due to their reduction in model accuracy caused by conflicting
optimization objectives. In this paper, we propose the novel idea of utilizing
methods of the representational similarity field to promote dissimilarity
during training instead of measuring similarity of trained models. To this end,
we promote intermediate representations to be dissimilar at different depths
between architectures, with the goal of learning robust ensembles with disjoint
failure modes. We show that highly dissimilar intermediate representations
result in less correlated output predictions and slightly lower error
consistency, resulting in higher ensemble accuracy. With this, we shine first
light on the connection between intermediate representations and their impact
on the output predictions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wald_T/0/1/0/all/0/1&quot;&gt;Tassilo Wald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ulrich_C/0/1/0/all/0/1&quot;&gt;Constantin Ulrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isensee_F/0/1/0/all/0/1&quot;&gt;Fabian Isensee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zimmerer_D/0/1/0/all/0/1&quot;&gt;David Zimmerer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koehler_G/0/1/0/all/0/1&quot;&gt;Gregor Koehler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baumgartner_M/0/1/0/all/0/1&quot;&gt;Michael Baumgartner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maier_Hein_K/0/1/0/all/0/1&quot;&gt;Klaus H. Maier-Hein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02588">
<title>TransformerG2G: Adaptive time-stepping for learning temporal graph embeddings using transformers. (arXiv:2307.02588v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.02588</link>
<description rdf:parseType="Literal">&lt;p&gt;Dynamic graph embedding has emerged as a very effective technique for
addressing diverse temporal graph analytic tasks (i.e., link prediction, node
classification, recommender systems, anomaly detection, and graph generation)
in various applications. Such temporal graphs exhibit heterogeneous transient
dynamics, varying time intervals, and highly evolving node features throughout
their evolution. Hence, incorporating long-range dependencies from the
historical graph context plays a crucial role in accurately learning their
temporal dynamics. In this paper, we develop a graph embedding model with
uncertainty quantification, TransformerG2G, by exploiting the advanced
transformer encoder to first learn intermediate node representations from its
current state ($t$) and previous context (over timestamps [$t-1, t-l$], $l$ is
the length of context). Moreover, we employ two projection layers to generate
lower-dimensional multivariate Gaussian distributions as each node&apos;s latent
embedding at timestamp $t$. We consider diverse benchmarks with varying levels
of ``novelty&quot; as measured by the TEA plots. Our experiments demonstrate that
the proposed TransformerG2G model outperforms conventional multi-step methods
and our prior work (DynG2G) in terms of both link prediction accuracy and
computational efficiency, especially for high degree of novelty. Furthermore,
the learned time-dependent attention weights across multiple graph snapshots
reveal the development of an automatic adaptive time stepping enabled by the
transformer. Importantly, by examining the attention weights, we can uncover
temporal dependencies, identify influential elements, and gain insights into
the complex interactions within the graph structure. For example, we identified
a strong correlation between attention weights and node degree at the various
stages of the graph topology evolution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varghese_A/0/1/0/all/0/1&quot;&gt;Alan John Varghese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bora_A/0/1/0/all/0/1&quot;&gt;Aniruddha Bora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Mengjia Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karniadakis_G/0/1/0/all/0/1&quot;&gt;George Em Karniadakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02591">
<title>ODD: A Benchmark Dataset for the NLP-based Opioid Related Aberrant Behavior Detection. (arXiv:2307.02591v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.02591</link>
<description rdf:parseType="Literal">&lt;p&gt;Opioid related aberrant behaviors (ORAB) present novel risk factors for
opioid overdose. Previously, ORAB have been mainly assessed by survey results
and by monitoring drug administrations. Such methods however, cannot scale up
and do not cover the entire spectrum of aberrant behaviors. On the other hand,
ORAB are widely documented in electronic health record notes. This paper
introduces a novel biomedical natural language processing benchmark dataset
named ODD, for ORAB Detection Dataset. ODD is an expert-annotated dataset
comprising of more than 750 publicly available EHR notes. ODD has been designed
to identify ORAB from patients&apos; EHR notes and classify them into nine
categories; 1) Confirmed Aberrant Behavior, 2) Suggested Aberrant Behavior, 3)
Opioids, 4) Indication, 5) Diagnosed opioid dependency, 6) Benzodiapines, 7)
Medication Changes, 8) Central Nervous System-related, and 9) Social
Determinants of Health. We explored two state-of-the-art natural language
processing (NLP) models (finetuning pretrained language models and
prompt-tuning approaches) to identify ORAB. Experimental results show that the
prompt-tuning models outperformed the finetuning models in most cateogories and
the gains were especially higher among uncommon categories (Suggested aberrant
behavior, Diagnosed opioid dependency and Medication change). Although the best
model achieved the highest 83.92\% on area under precision recall curve,
uncommon classes (Suggested Aberrant Behavior, Diagnosed Opioid Dependence, and
Medication Change) still have a large room for performance improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_S/0/1/0/all/0/1&quot;&gt;Sunjae Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Weisong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Druhl_E/0/1/0/all/0/1&quot;&gt;Emily Druhl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sung_M/0/1/0/all/0/1&quot;&gt;Minhee L. Sung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reisman_J/0/1/0/all/0/1&quot;&gt;Joel I. Reisman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenjun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kerns_R/0/1/0/all/0/1&quot;&gt;Robert D. Kerns&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Becker_W/0/1/0/all/0/1&quot;&gt;William Becker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hong Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02599">
<title>Evade ChatGPT Detectors via A Single Space. (arXiv:2307.02599v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.02599</link>
<description rdf:parseType="Literal">&lt;p&gt;ChatGPT brings revolutionary social value but also raises concerns about the
misuse of AI-generated content. Consequently, an important question is how to
detect whether content is generated by ChatGPT or by human. Existing detectors
are built upon the assumption that there are distributional gaps between
human-generated and AI-generated content. These gaps are typically identified
using statistical information or classifiers. Our research challenges the
distributional gap assumption in detectors. We find that detectors do not
effectively discriminate the semantic and stylistic gaps between
human-generated and AI-generated content. Instead, the &quot;subtle differences&quot;,
such as an extra space, become crucial for detection. Based on this discovery,
we propose the SpaceInfi strategy to evade detection. Experiments demonstrate
the effectiveness of this strategy across multiple benchmarks and detectors. We
also provide a theoretical explanation for why SpaceInfi is successful in
evading perplexity-based detection. Our findings offer new insights and
challenges for understanding and constructing more applicable ChatGPT
detectors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1&quot;&gt;Shuyang Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1&quot;&gt;Wanyun Cui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02615">
<title>Human Inspired Progressive Alignment and Comparative Learning for Grounded Word Acquisition. (arXiv:2307.02615v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.02615</link>
<description rdf:parseType="Literal">&lt;p&gt;Human language acquisition is an efficient, supervised, and continual
process. In this work, we took inspiration from how human babies acquire their
first language, and developed a computational process for word acquisition
through comparative learning. Motivated by cognitive findings, we generated a
small dataset that enables the computation models to compare the similarities
and differences of various attributes, learn to filter out and extract the
common information for each shared linguistic label. We frame the acquisition
of words as not only the information filtration process, but also as
representation-symbol mapping. This procedure does not involve a fixed
vocabulary size, nor a discriminative objective, and allows the models to
continually learn more concepts efficiently. Our results in controlled
experiments have shown the potential of this approach for efficient continual
learning of grounded words.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1&quot;&gt;Yuwei Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lattimer_B/0/1/0/all/0/1&quot;&gt;Barrett Martin Lattimer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1&quot;&gt;Joyce Chai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02616">
<title>Federated Epidemic Surveillance. (arXiv:2307.02616v1 [stat.AP])</title>
<link>http://arxiv.org/abs/2307.02616</link>
<description rdf:parseType="Literal">&lt;p&gt;The surveillance of a pandemic is a challenging task, especially when crucial
data is distributed and stakeholders cannot or are unwilling to share. To
overcome this obstacle, federated methodologies should be developed to
incorporate less sensitive evidence that entities are willing to provide. This
study aims to explore the feasibility of pushing hypothesis tests behind each
custodian&apos;s firewall and then meta-analysis to combine the results, and to
determine the optimal approach for reconstructing the hypothesis test and
optimizing the inference. We propose a hypothesis testing framework to identify
a surge in the indicators and conduct power analyses and experiments on real
and semi-synthetic data to showcase the properties of our proposed hypothesis
test and suggest suitable methods for combining $p$-values. Our findings
highlight the potential of using $p$-value combination as a federated
methodology for pandemic surveillance and provide valuable insights into
integrating available data sources.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lyu_R/0/1/0/all/0/1&quot;&gt;Ruiqi Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wilder_B/0/1/0/all/0/1&quot;&gt;Bryan Wilder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rosenfeld_R/0/1/0/all/0/1&quot;&gt;Roni Rosenfeld&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02620">
<title>Dynamic Observation Policies in Observation Cost-Sensitive Reinforcement Learning. (arXiv:2307.02620v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.02620</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning (RL) has been shown to learn sophisticated control
policies for complex tasks including games, robotics, heating and cooling
systems and text generation. The action-perception cycle in RL, however,
generally assumes that a measurement of the state of the environment is
available at each time step without a cost. In applications such as deep-sea
and planetary robot exploration, materials design and medicine, however, there
can be a high cost associated with measuring, or even approximating, the state
of the environment. In this paper, we survey the recently growing literature
that adopts the perspective that an RL agent might not need, or even want, a
costly measurement at each time step. Within this context, we propose the Deep
Dynamic Multi-Step Observationless Agent (DMSOA), contrast it with the
literature and empirically evaluate it on OpenAI gym and Atari Pong
environments. Our results, show that DMSOA learns a better policy with fewer
decision steps and measurements than the considered alternative from the
literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellinger_C/0/1/0/all/0/1&quot;&gt;Colin Bellinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crowley_M/0/1/0/all/0/1&quot;&gt;Mark Crowley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tamblyn_I/0/1/0/all/0/1&quot;&gt;Isaac Tamblyn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02626">
<title>Real-time Workload Pattern Analysis for Large-scale Cloud Databases. (arXiv:2307.02626v1 [cs.DB])</title>
<link>http://arxiv.org/abs/2307.02626</link>
<description rdf:parseType="Literal">&lt;p&gt;Hosting database services on cloud systems has become a common practice. This
has led to the increasing volume of database workloads, which provides the
opportunity for pattern analysis. Discovering workload patterns from a business
logic perspective is conducive to better understanding the trends and
characteristics of the database system. However, existing workload pattern
discovery systems are not suitable for large-scale cloud databases which are
commonly employed by the industry. This is because the workload patterns of
large-scale cloud databases are generally far more complicated than those of
ordinary databases. In this paper, we propose Alibaba Workload Miner (AWM), a
real-time system for discovering workload patterns in complicated large-scale
workloads. AWM encodes and discovers the SQL query patterns logged from user
requests and optimizes the querying processing based on the discovered
patterns. First, Data Collection &amp;amp; Preprocessing Module collects streaming
query logs and encodes them into high-dimensional feature embeddings with rich
semantic contexts and execution features. Next, Online Workload Mining Module
separates encoded queries by business groups and discovers the workload
patterns for each group. Meanwhile, Offline Training Module collects labels and
trains the classification model using the labels. Finally, Pattern-based
Optimizing Module optimizes query processing in cloud databases by exploiting
discovered patterns. Extensive experimental results on one synthetic dataset
and two real-life datasets (extracted from Alibaba Cloud databases) show that
AWM enhances the accuracy of pattern discovery by 66% and reduce the latency of
online inference by 22%, compared with the state-of-the-arts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiaqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tianyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1&quot;&gt;Anni Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoze Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jianye Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Junyang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Feifei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yunjun Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02631">
<title>An explainable model to support the decision about the therapy protocol for AML. (arXiv:2307.02631v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.02631</link>
<description rdf:parseType="Literal">&lt;p&gt;Acute Myeloid Leukemia (AML) is one of the most aggressive types of
hematological neoplasm. To support the specialists&apos; decision about the
appropriate therapy, patients with AML receive a prognostic of outcomes
according to their cytogenetic and molecular characteristics, often divided
into three risk categories: favorable, intermediate, and adverse. However, the
current risk classification has known problems, such as the heterogeneity
between patients of the same risk group and no clear definition of the
intermediate risk category. Moreover, as most patients with AML receive an
intermediate-risk classification, specialists often demand other tests and
analyses, leading to delayed treatment and worsening of the patient&apos;s clinical
condition. This paper presents the data analysis and an explainable
machine-learning model to support the decision about the most appropriate
therapy protocol according to the patient&apos;s survival prediction. In addition to
the prediction model being explainable, the results obtained are promising and
indicate that it is possible to use it to support the specialists&apos; decisions
safely. Most importantly, the findings offered in this study have the potential
to open new avenues of research toward better treatments and prognostic
markers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Almeida_J/0/1/0/all/0/1&quot;&gt;Jade M. Almeida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castro_G/0/1/0/all/0/1&quot;&gt;Giovanna A. Castro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Machado_Neto_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o A. Machado-Neto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Almeida_T/0/1/0/all/0/1&quot;&gt;Tiago A. Almeida&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02637">
<title>Surge Routing: Event-informed Multiagent Reinforcement Learning for Autonomous Rideshare. (arXiv:2307.02637v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.02637</link>
<description rdf:parseType="Literal">&lt;p&gt;Large events such as conferences, concerts and sports games, often cause
surges in demand for ride services that are not captured in average demand
patterns, posing unique challenges for routing algorithms. We propose a
learning framework for an autonomous fleet of taxis that scrapes event data
from the internet to predict and adapt to surges in demand and generates
cooperative routing and pickup policies that service a higher number of
requests than other routing protocols. We achieve this through a combination of
(i) an event processing framework that scrapes the internet for event
information and generates dense vector representations that can be used as
input features for a neural network that predicts demand; (ii) a two neural
network system that predicts hourly demand over the entire map, using these
dense vector representations; (iii) a probabilistic approach that leverages
locale occupancy schedules to map publicly available demand data over sectors
to discretized street intersections; and finally, (iv) a scalable model-based
reinforcement learning framework that uses the predicted demand over
intersections to anticipate surges and route taxis using one-agent-at-a-time
rollout with limited sampling certainty equivalence. We learn routing and
pickup policies using real NYC ride share data for 2022 and information for
more than 2000 events across 300 unique venues in Manhattan. We test our
approach with a fleet of 100 taxis on a map with 38 different sectors (2235
street intersections). Our experimental results demonstrate that our method
obtains routing policies that service $6$ more requests on average per minute
(around $360$ more requests per hour) than other model-based RL frameworks and
other classical algorithms in operations research when dealing with surge
demand conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garces_D/0/1/0/all/0/1&quot;&gt;Daniel Garces&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gil_S/0/1/0/all/0/1&quot;&gt;Stephanie Gil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02661">
<title>Many-objective Optimization via Voting for Elites. (arXiv:2307.02661v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2307.02661</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-world problems are often comprised of many objectives and require
solutions that carefully trade-off between them. Current approaches to
many-objective optimization often require challenging assumptions, like
knowledge of the importance/difficulty of objectives in a weighted-sum
single-objective paradigm, or enormous populations to overcome the curse of
dimensionality in multi-objective Pareto optimization. Combining elements from
Many-Objective Evolutionary Algorithms and Quality Diversity algorithms like
MAP-Elites, we propose Many-objective Optimization via Voting for Elites
(MOVE). MOVE maintains a map of elites that perform well on different subsets
of the objective functions. On a 14-objective image-neuroevolution problem, we
demonstrate that MOVE is viable with a population of as few as 50 elites and
outperforms a naive single-objective baseline. We find that the algorithm&apos;s
performance relies on solutions jumping across bins (for a parent to produce a
child that is elite for a different subset of objectives). We suggest that this
type of goal-switching is an implicit method to automatic identification of
stepping stones or curriculum learning. We comment on the similarities and
differences between MOVE and MAP-Elites, hoping to provide insight to aid in
the understanding of that approach $\unicode{x2013}$ and suggest future work
that may inform this approach&apos;s use for many-objective problems in general.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dean_J/0/1/0/all/0/1&quot;&gt;Jackson Dean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheney_N/0/1/0/all/0/1&quot;&gt;Nick Cheney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02663">
<title>Convergence of Communications, Control, and Machine Learning for Secure and Autonomous Vehicle Navigation. (arXiv:2307.02663v1 [cs.IT])</title>
<link>http://arxiv.org/abs/2307.02663</link>
<description rdf:parseType="Literal">&lt;p&gt;Connected and autonomous vehicles (CAVs) can reduce human errors in traffic
accidents, increase road efficiency, and execute various tasks ranging from
delivery to smart city surveillance. Reaping these benefits requires CAVs to
autonomously navigate to target destinations. To this end, each CAV&apos;s
navigation controller must leverage the information collected by sensors and
wireless systems for decision-making on longitudinal and lateral movements.
However, enabling autonomous navigation for CAVs requires a convergent
integration of communication, control, and learning systems. The goal of this
article is to explicitly expose the challenges related to this convergence and
propose solutions to address them in two major use cases: Uncoordinated and
coordinated CAVs. In particular, challenges related to the navigation of
uncoordinated CAVs include stable path tracking, robust control against
cyber-physical attacks, and adaptive navigation controller design. Meanwhile,
when multiple CAVs coordinate their movements during navigation, fundamental
problems such as stable formation, fast collaborative learning, and distributed
intrusion detection are analyzed. For both cases, solutions using the
convergence of communication theory, control theory, and machine learning are
proposed to enable effective and secure CAV navigation. Preliminary simulation
results are provided to show the merits of proposed solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_T/0/1/0/all/0/1&quot;&gt;Tengchan Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferdowsi_A/0/1/0/all/0/1&quot;&gt;Aidin Ferdowsi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Semiari_O/0/1/0/all/0/1&quot;&gt;Omid Semiari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saad_W/0/1/0/all/0/1&quot;&gt;Walid Saad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1&quot;&gt;Choong Seon Hong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02671">
<title>AI4OPT: AI Institute for Advances in Optimization. (arXiv:2307.02671v1 [math.OC])</title>
<link>http://arxiv.org/abs/2307.02671</link>
<description rdf:parseType="Literal">&lt;p&gt;This article is a short introduction to AI4OPT, the NSF AI Institute for
Advances in Optimization. AI4OPT fuses AI and Optimization, inspired by end-use
cases in supply chains, energy systems, chip design and manufacturing, and
sustainable food systems. AI4OPT also applies its &quot;teaching the teachers&quot;
philosophy to provide longitudinal educational pathways in AI for engineering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hentenryck_P/0/1/0/all/0/1&quot;&gt;Pascal Van Hentenryck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Dalmeijer_K/0/1/0/all/0/1&quot;&gt;Kevin Dalmeijer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02690">
<title>Scaling In-Context Demonstrations with Structured Attention. (arXiv:2307.02690v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.02690</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent surge of large language models (LLMs) highlights their ability to
perform in-context learning, i.e., &quot;learning&quot; to perform a task from a few
demonstrations in the context without any parameter updates. However, their
capabilities of in-context learning are limited by the model architecture: 1)
the use of demonstrations is constrained by a maximum sentence length due to
positional embeddings; 2) the quadratic complexity of attention hinders users
from using more demonstrations efficiently; 3) LLMs are shown to be sensitive
to the order of the demonstrations. In this work, we tackle these challenges by
proposing a better architectural design for in-context learning. We propose
SAICL (Structured Attention for In-Context Learning), which replaces the
full-attention by a structured attention mechanism designed for in-context
learning, and removes unnecessary dependencies between individual
demonstrations, while making the model invariant to the permutation of
demonstrations. We evaluate SAICL in a meta-training framework and show that
SAICL achieves comparable or better performance than full attention while
obtaining up to 3.4x inference speed-up. SAICL also consistently outperforms a
strong Fusion-in-Decoder (FiD) baseline which processes each demonstration
independently. Finally, thanks to its linear nature, we demonstrate that SAICL
can easily scale to hundreds of demonstrations with continuous performance
gains with scaling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1&quot;&gt;Tianle Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kaixuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jason D. Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengdi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02691">
<title>SACHA: Soft Actor-Critic with Heuristic-Based Attention for Partially Observable Multi-Agent Path Finding. (arXiv:2307.02691v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.02691</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-Agent Path Finding (MAPF) is a crucial component for many large-scale
robotic systems, where agents must plan their collision-free paths to their
given goal positions. Recently, multi-agent reinforcement learning has been
introduced to solve the partially observable variant of MAPF by learning a
decentralized single-agent policy in a centralized fashion based on each
agent&apos;s partial observation. However, existing learning-based methods are
ineffective in achieving complex multi-agent cooperation, especially in
congested environments, due to the non-stationarity of this setting. To tackle
this challenge, we propose a multi-agent actor-critic method called Soft
Actor-Critic with Heuristic-Based Attention (SACHA), which employs novel
heuristic-based attention mechanisms for both the actors and critics to
encourage cooperation among agents. SACHA learns a neural network for each
agent to selectively pay attention to the shortest path heuristic guidance from
multiple agents within its field of view, thereby allowing for more scalable
learning of cooperation. SACHA also extends the existing multi-agent
actor-critic framework by introducing a novel critic centered on each agent to
approximate $Q$-values. Compared to existing methods that use a fully
observable critic, our agent-centered multi-agent actor-critic method results
in more impartial credit assignment and better generalizability of the learned
policy to MAPF instances with varying numbers of agents and types of
environments. We also implement SACHA(C), which embeds a communication module
in the agent&apos;s policy network to enable information exchange among agents. We
evaluate both SACHA and SACHA(C) on a variety of MAPF instances and demonstrate
decent improvements over several state-of-the-art learning-based MAPF methods
with respect to success rate and solution quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1&quot;&gt;Qiushi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Hang Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02694">
<title>Loss Functions and Metrics in Deep Learning. A Review. (arXiv:2307.02694v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.02694</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the essential components of deep learning is the choice of the loss
function and performance metrics used to train and evaluate models. This paper
reviews the most prevalent loss functions and performance measurements in deep
learning. We examine the benefits and limits of each technique and illustrate
their application to various deep-learning problems. Our review aims to give a
comprehensive picture of the different loss functions and performance
indicators used in the most common deep learning tasks and help practitioners
choose the best method for their specific task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Terven_J/0/1/0/all/0/1&quot;&gt;Juan Terven&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cordova_Esparza_D/0/1/0/all/0/1&quot;&gt;Diana M. Cordova-Esparza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramirez_Pedraza_A/0/1/0/all/0/1&quot;&gt;Alfonzo Ramirez-Pedraza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chavez_Urbiola_E/0/1/0/all/0/1&quot;&gt;Edgar A. Chavez-Urbiola&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02709">
<title>Validation of the Practicability of Logical Assessment Formula for Evaluations with Inaccurate Ground-Truth Labels. (arXiv:2307.02709v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.02709</link>
<description rdf:parseType="Literal">&lt;p&gt;Logical assessment formula (LAF) is a new theory proposed for evaluations
with inaccurate ground-truth labels (IAGTLs) to assess the predictive models
for various artificial intelligence applications. However, the practicability
of LAF for evaluations with IAGTLs has not yet been validated in real-world
practice. In this paper, to address this issue, we applied LAF to tumour
segmentation for breast cancer (TSfBC) in medical histopathology whole slide
image analysis (MHWSIA). Experimental results and analysis show the validity of
LAF for evaluations with IAGTLs in the case of TSfBC and reflect the potentials
of LAF applied to MHWSIA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yongquan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bu_H/0/1/0/all/0/1&quot;&gt;Hong Bu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02717">
<title>TL-nvSRAM-CIM: Ultra-High-Density Three-Level ReRAM-Assisted Computing-in-nvSRAM with DC-Power Free Restore and Ternary MAC Operations. (arXiv:2307.02717v1 [cs.AR])</title>
<link>http://arxiv.org/abs/2307.02717</link>
<description rdf:parseType="Literal">&lt;p&gt;Accommodating all the weights on-chip for large-scale NNs remains a great
challenge for SRAM based computing-in-memory (SRAM-CIM) with limited on-chip
capacity. Previous non-volatile SRAM-CIM (nvSRAM-CIM) addresses this issue by
integrating high-density single-level ReRAMs on the top of high-efficiency
SRAM-CIM for weight storage to eliminate the off-chip memory access. However,
previous SL-nvSRAM-CIM suffers from poor scalability for an increased number of
SL-ReRAMs and limited computing efficiency. To overcome these challenges, this
work proposes an ultra-high-density three-level ReRAMs-assisted
computing-in-nonvolatile-SRAM (TL-nvSRAM-CIM) scheme for large NN models. The
clustered n-selector-n-ReRAM (cluster-nSnRs) is employed for reliable
weight-restore with eliminated DC power. Furthermore, a ternary SRAM-CIM
mechanism with differential computing scheme is proposed for energy-efficient
ternary MAC operations while preserving high NN accuracy. The proposed
TL-nvSRAM-CIM achieves 7.8x higher storage density, compared with the
state-of-art works. Moreover, TL-nvSRAM-CIM shows up to 2.9x and 1.9x enhanced
energy-efficiency, respectively, compared to the baseline designs of SRAM-CIM
and ReRAM-CIM, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dengfeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Liukai Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Songyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_z/0/1/0/all/0/1&quot;&gt;zhi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yiming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1&quot;&gt;Weifeng He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xueqing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yanan Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02728">
<title>Hierarchical Empowerment: Towards Tractable Empowerment-Based Skill-Learning. (arXiv:2307.02728v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.02728</link>
<description rdf:parseType="Literal">&lt;p&gt;General purpose agents will require large repertoires of skills. Empowerment
-- the maximum mutual information between skills and the states -- provides a
pathway for learning large collections of distinct skills, but mutual
information is difficult to optimize. We introduce a new framework,
Hierarchical Empowerment, that makes computing empowerment more tractable by
integrating concepts from Goal-Conditioned Hierarchical Reinforcement Learning.
Our framework makes two specific contributions. First, we introduce a new
variational lower bound on mutual information that can be used to compute
empowerment over short horizons. Second, we introduce a hierarchical
architecture for computing empowerment over exponentially longer time scales.
We verify the contributions of the framework in a series of simulated robotics
tasks. In a popular ant navigation domain, our four level agents are able to
learn skills that cover a surface area over two orders of magnitude larger than
prior work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levy_A/0/1/0/all/0/1&quot;&gt;Andrew Levy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rammohan_S/0/1/0/all/0/1&quot;&gt;Sreehari Rammohan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allievi_A/0/1/0/all/0/1&quot;&gt;Alessandro Allievi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1&quot;&gt;Scott Niekum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konidaris_G/0/1/0/all/0/1&quot;&gt;George Konidaris&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02730">
<title>Fine-grained Action Analysis: A Multi-modality and Multi-task Dataset of Figure Skating. (arXiv:2307.02730v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02730</link>
<description rdf:parseType="Literal">&lt;p&gt;The fine-grained action analysis of the existing action datasets is
challenged by insufficient action categories, low fine granularities, limited
modalities, and tasks. In this paper, we propose a Multi-modality and
Multi-task dataset of Figure Skating (MMFS) which was collected from the World
Figure Skating Championships. MMFS, which possesses action recognition and
action quality assessment, captures RGB, skeleton, and is collected the score
of actions from 11671 clips with 256 categories including spatial and temporal
labels. The key contributions of our dataset fall into three aspects as
follows. (1) Independently spatial and temporal categories are first proposed
to further explore fine-grained action recognition and quality assessment. (2)
MMFS first introduces the skeleton modality for complex fine-grained action
quality assessment. (3) Our multi-modality and multi-task dataset encourage
more action analysis models. To benchmark our dataset, we adopt RGB-based and
skeleton-based baseline methods for action recognition and action quality
assessment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sheng-Lan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Yu-Ning Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Si-Fan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wen-Yue Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_N/0/1/0/all/0/1&quot;&gt;Ning Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lao_G/0/1/0/all/0/1&quot;&gt;Gui-Hong Lao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02738">
<title>RecallM: An Architecture for Temporal Context Understanding and Question Answering. (arXiv:2307.02738v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.02738</link>
<description rdf:parseType="Literal">&lt;p&gt;The ideal long-term memory mechanism for Large Language Model (LLM) based
chatbots, would lay the foundation for continual learning, complex reasoning
and allow sequential and temporal dependencies to be learnt. Creating this type
of memory mechanism is an extremely challenging problem. In this paper we
explore different methods of achieving the effect of long-term memory. We
propose a new architecture focused on creating adaptable and updatable
long-term memory for AGI systems. We demonstrate through various experiments
the benefits of the RecallM architecture, particularly the improved temporal
understanding it provides.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kynoch_B/0/1/0/all/0/1&quot;&gt;Brandon Kynoch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Latapie_H/0/1/0/all/0/1&quot;&gt;Hugo Latapie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02752">
<title>Offline Reinforcement Learning with Imbalanced Datasets. (arXiv:2307.02752v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.02752</link>
<description rdf:parseType="Literal">&lt;p&gt;The prevalent use of benchmarks in current offline reinforcement learning
(RL) research has led to a neglect of the imbalance of real-world dataset
distributions in the development of models. The real-world offline RL dataset
is often imbalanced over the state space due to the challenge of exploration or
safety considerations. In this paper, we specify properties of imbalanced
datasets in offline RL, where the state coverage follows a power law
distribution characterized by skewed policies. Theoretically and empirically,
we show that typically offline RL methods based on distributional constraints,
such as conservative Q-learning (CQL), are ineffective in extracting policies
under the imbalanced dataset. Inspired by natural intelligence, we propose a
novel offline RL method that utilizes the augmentation of CQL with a retrieval
process to recall past related experiences, effectively alleviating the
challenges posed by imbalanced datasets. We evaluate our method on several
tasks in the context of imbalanced datasets with varying levels of imbalance,
utilizing the variant of D4RL. Empirical results demonstrate the superiority of
our method over other baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1&quot;&gt;Li Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Sijie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1&quot;&gt;Jielin Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Haoran Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_W/0/1/0/all/0/1&quot;&gt;Wai Kin Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1&quot;&gt;Zhao Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02759">
<title>Knowledge Graph Self-Supervised Rationalization for Recommendation. (arXiv:2307.02759v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2307.02759</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce a new self-supervised rationalization method,
called KGRec, for knowledge-aware recommender systems. To effectively identify
informative knowledge connections, we propose an attentive knowledge
rationalization mechanism that generates rational scores for knowledge
triplets. With these scores, KGRec integrates generative and contrastive
self-supervised tasks for recommendation through rational masking. To highlight
rationales in the knowledge graph, we design a novel generative task in the
form of masking-reconstructing. By masking important knowledge with high
rational scores, KGRec is trained to rebuild and highlight useful knowledge
connections that serve as rationales. To further rationalize the effect of
collaborative interactions on knowledge graph learning, we introduce a
contrastive learning task that aligns signals from knowledge and user-item
interaction views. To ensure noise-resistant contrasting, potential noisy edges
in both graphs judged by the rational scores are masked. Extensive experiments
on three real-world datasets demonstrate that KGRec outperforms
state-of-the-art methods. We also provide the implementation codes for our
approach at https://github.com/HKUDS/KGRec.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuhao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_L/0/1/0/all/0/1&quot;&gt;Lianghao Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chunzhen Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02762">
<title>PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations. (arXiv:2307.02762v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.02762</link>
<description rdf:parseType="Literal">&lt;p&gt;Nowadays, the quality of responses generated by different modern large
language models (LLMs) are hard to evaluate and compare automatically. Recent
studies suggest and predominantly use LLMs as a reference-free metric for
open-ended question answering. More specifically, they use the recognized
&quot;strongest&quot; LLM as the evaluator, which conducts pairwise comparisons of
candidate models&apos; answers and provides a ranking score. However, this intuitive
method has multiple problems, such as bringing in self-enhancement (favoring
its own answers) and positional bias. We draw insights and lessons from the
educational domain (Cho and MacArthur, 2011; Walsh, 2014) to improve LLM-based
evaluations. Specifically, we propose the (1) peer rank (PR) algorithm that
takes into account each peer LLM&apos;s pairwise preferences of all answer pairs,
and outputs a final ranking of models; and (2) peer discussion (PD), where we
prompt two LLMs to discuss and try to reach a mutual agreement on preferences
of two answers. We conduct experiments on two benchmark datasets. We find that
our approaches achieve higher accuracy and align better with human judgments,
respectively. Interestingly, PR can induce a relatively accurate self-ranking
of models under the anonymous setting, where each model&apos;s name is unrevealed.
Our work provides space to explore evaluating models that are hard to compare
for humans.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ruosen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_T/0/1/0/all/0/1&quot;&gt;Teerth Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1&quot;&gt;Xinya Du&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02770">
<title>Censored Sampling of Diffusion Models Using 3 Minutes of Human Feedback. (arXiv:2307.02770v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02770</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have recently shown remarkable success in high-quality image
generation. Sometimes, however, a pre-trained diffusion model exhibits partial
misalignment in the sense that the model can generate good images, but it
sometimes outputs undesirable images. If so, we simply need to prevent the
generation of the bad images, and we call this task censoring. In this work, we
present censored generation with a pre-trained diffusion model using a reward
model trained on minimal human feedback. We show that censoring can be
accomplished with extreme human feedback efficiency and that labels generated
with a mere few minutes of human feedback are sufficient. Code available at:
https://github.com/tetrzim/diffusion-human-feedback.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_T/0/1/0/all/0/1&quot;&gt;TaeHo Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Myoung_K/0/1/0/all/0/1&quot;&gt;Kibeom Myoung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Keon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1&quot;&gt;Jaewoong Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+No_A/0/1/0/all/0/1&quot;&gt;Albert No&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryu_E/0/1/0/all/0/1&quot;&gt;Ernest K. Ryu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02791">
<title>The Role of Subgroup Separability in Group-Fair Medical Image Classification. (arXiv:2307.02791v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02791</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate performance disparities in deep classifiers. We find that the
ability of classifiers to separate individuals into subgroups varies
substantially across medical imaging modalities and protected characteristics;
crucially, we show that this property is predictive of algorithmic bias.
Through theoretical analysis and extensive empirical evaluation, we find a
relationship between subgroup separability, subgroup disparities, and
performance degradation when models are trained on data with systematic bias
such as underdiagnosis. Our findings shed new light on the question of how
models become biased, providing important insights for the development of fair
medical imaging AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jones_C/0/1/0/all/0/1&quot;&gt;Charles Jones&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roschewitz_M/0/1/0/all/0/1&quot;&gt;M&amp;#xe9;lanie Roschewitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1&quot;&gt;Ben Glocker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02792">
<title>What Should Data Science Education Do with Large Language Models?. (arXiv:2307.02792v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2307.02792</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid advances of large language models (LLMs), such as ChatGPT, are
revolutionizing data science and statistics. These state-of-the-art tools can
streamline complex processes. As a result, it reshapes the role of data
scientists. We argue that LLMs are transforming the responsibilities of data
scientists, shifting their focus from hands-on coding, data-wrangling and
conducting standard analyses to assessing and managing analyses performed by
these automated AIs. This evolution of roles is reminiscent of the transition
from a software engineer to a product manager. We illustrate this transition
with concrete data science case studies using LLMs in this paper. These
developments necessitate a meaningful evolution in data science education.
Pedagogy must now place greater emphasis on cultivating diverse skillsets among
students, such as LLM-informed creativity, critical thinking, AI-guided
programming. LLMs can also play a significant role in the classroom as
interactive teaching and learning tools, contributing to personalized
education. This paper discusses the opportunities, resources and open
challenges for each of these directions. As with any transformative technology,
integrating LLMs into education calls for careful consideration. While LLMs can
perform repetitive tasks efficiently, it&apos;s crucial to remember that their role
is to supplement human intelligence and creativity, not to replace it.
Therefore, the new era of data science education should balance the benefits of
LLMs while fostering complementary human expertise and innovations. In
conclusion, the rise of LLMs heralds a transformative period for data science
and its education. This paper seeks to shed light on the emerging trends,
potential opportunities, and challenges accompanying this paradigm shift,
hoping to spark further discourse and investigation into this exciting,
uncharted territory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_X/0/1/0/all/0/1&quot;&gt;Xinming Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1&quot;&gt;James Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1&quot;&gt;Weijie J. Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Linjun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02797">
<title>BHEISR: Nudging from Bias to Balance -- Promoting Belief Harmony by Eliminating Ideological Segregation in Knowledge-based Recommendations. (arXiv:2307.02797v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2307.02797</link>
<description rdf:parseType="Literal">&lt;p&gt;In the realm of personalized recommendation systems, the increasing concern
is the amplification of belief imbalance and user biases, a phenomenon
primarily attributed to the filter bubble. Addressing this critical issue, we
introduce an innovative intermediate agency (BHEISR) between users and existing
recommendation systems to attenuate the negative repercussions of the filter
bubble effect in extant recommendation systems. The main objective is to strike
a belief balance for users while minimizing the detrimental influence caused by
filter bubbles. The BHEISR model amalgamates principles from nudge theory while
upholding democratic and transparent principles. It harnesses user-specific
category information to stimulate curiosity, even in areas users might
initially deem uninteresting. By progressively stimulating interest in novel
categories, the model encourages users to broaden their belief horizons and
explore the information they typically overlook. Our model is time-sensitive
and operates on a user feedback loop. It utilizes the existing recommendation
algorithm of the model and incorporates user feedback from the prior time
frame. This approach endeavors to transcend the constraints of the filter
bubble, enrich recommendation diversity, and strike a belief balance among
users while also catering to user preferences and system-specific business
requirements. To validate the effectiveness and reliability of the BHEISR
model, we conducted a series of comprehensive experiments with real-world
datasets. These experiments compared the performance of the BHEISR model
against several baseline models using nearly 200 filter bubble-impacted users
as test subjects. Our experimental results conclusively illustrate the superior
performance of the BHEISR model in mitigating filter bubbles and balancing user
perspectives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengyan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1&quot;&gt;Zihan Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1&quot;&gt;Chenting Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Weihua Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Shiqing Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Q/0/1/0/all/0/1&quot;&gt;Quan Bai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02798">
<title>Semi-supervised Domain Adaptive Medical Image Segmentation through Consistency Regularized Disentangled Contrastive Learning. (arXiv:2307.02798v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02798</link>
<description rdf:parseType="Literal">&lt;p&gt;Although unsupervised domain adaptation (UDA) is a promising direction to
alleviate domain shift, they fall short of their supervised counterparts. In
this work, we investigate relatively less explored semi-supervised domain
adaptation (SSDA) for medical image segmentation, where access to a few labeled
target samples can improve the adaptation performance substantially.
Specifically, we propose a two-stage training process. First, an encoder is
pre-trained in a self-learning paradigm using a novel domain-content
disentangled contrastive learning (CL) along with a pixel-level feature
consistency constraint. The proposed CL enforces the encoder to learn
discriminative content-specific but domain-invariant semantics on a global
scale from the source and target images, whereas consistency regularization
enforces the mining of local pixel-level information by maintaining spatial
sensitivity. This pre-trained encoder, along with a decoder, is further
fine-tuned for the downstream task, (i.e. pixel-level segmentation) using a
semi-supervised setting. Furthermore, we experimentally validate that our
proposed method can easily be extended for UDA settings, adding to the
superiority of the proposed strategy. Upon evaluation on two domain adaptive
image segmentation tasks, our proposed method outperforms the SoTA methods,
both in SSDA and UDA settings. Code is available at
https://github.com/hritam-98/GFDA-disentangled
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basak_H/0/1/0/all/0/1&quot;&gt;Hritam Basak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Zhaozheng Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02820">
<title>Evaluating raw waveforms with deep learning frameworks for speech emotion recognition. (arXiv:2307.02820v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2307.02820</link>
<description rdf:parseType="Literal">&lt;p&gt;Speech emotion recognition is a challenging task in speech processing field.
For this reason, feature extraction process has a crucial importance to
demonstrate and process the speech signals. In this work, we represent a model,
which feeds raw audio files directly into the deep neural networks without any
feature extraction stage for the recognition of emotions utilizing six
different data sets, EMO-DB, RAVDESS, TESS, CREMA, SAVEE, and TESS+RAVDESS. To
demonstrate the contribution of proposed model, the performance of traditional
feature extraction techniques namely, mel-scale spectogram, mel-frequency
cepstral coefficients, are blended with machine learning algorithms, ensemble
learning methods, deep and hybrid deep learning techniques. Support vector
machine, decision tree, naive Bayes, random forests models are evaluated as
machine learning algorithms while majority voting and stacking methods are
assessed as ensemble learning techniques. Moreover, convolutional neural
networks, long short-term memory networks, and hybrid CNN- LSTM model are
evaluated as deep learning techniques and compared with machine learning and
ensemble learning methods. To demonstrate the effectiveness of proposed model,
the comparison with state-of-the-art studies are carried out. Based on the
experiment results, CNN model excels existent approaches with 95.86% of
accuracy for TESS+RAVDESS data set using raw audio files, thence determining
the new state-of-the-art. The proposed model performs 90.34% of accuracy for
EMO-DB with CNN model, 90.42% of accuracy for RAVDESS with CNN model, 99.48% of
accuracy for TESS with LSTM model, 69.72% of accuracy for CREMA with CNN model,
85.76% of accuracy for SAVEE with CNN model in speaker-independent audio
categorization problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kilimci_Z/0/1/0/all/0/1&quot;&gt;Zeynep Hilal Kilimci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bayraktar_U/0/1/0/all/0/1&quot;&gt;Ulku Bayraktar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kucukmanisa_A/0/1/0/all/0/1&quot;&gt;Ayhan Kucukmanisa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02839">
<title>Enhancing LLM with Evolutionary Fine Tuning for News Summary Generation. (arXiv:2307.02839v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.02839</link>
<description rdf:parseType="Literal">&lt;p&gt;News summary generation is an important task in the field of intelligence
analysis, which can provide accurate and comprehensive information to help
people better understand and respond to complex real-world events. However,
traditional news summary generation methods face some challenges, which are
limited by the model itself and the amount of training data, as well as the
influence of text noise, making it difficult to generate reliable information
accurately. In this paper, we propose a new paradigm for news summary
generation using LLM with powerful natural language understanding and
generative capabilities. We use LLM to extract multiple structured event
patterns from the events contained in news paragraphs, evolve the event pattern
population with genetic algorithm, and select the most adaptive event pattern
to input into the LLM to generate news summaries. A News Summary Generator
(NSG) is designed to select and evolve the event pattern populations and
generate news summaries. The experimental results show that the news summary
generator is able to generate accurate and reliable news summaries with some
generalization ability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1&quot;&gt;Le Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaolin Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02867">
<title>Towards a safe MLOps Process for the Continuous Development and Safety Assurance of ML-based Systems in the Railway Domain. (arXiv:2307.02867v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2307.02867</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional automation technologies alone are not sufficient to enable
driverless operation of trains (called Grade of Automation (GoA) 4) on
non-restricted infrastructure. The required perception tasks are nowadays
realized using Machine Learning (ML) and thus need to be developed and deployed
reliably and efficiently. One important aspect to achieve this is to use an
MLOps process for tackling improved reproducibility, traceability,
collaboration, and continuous adaptation of a driverless operation to changing
conditions. MLOps mixes ML application development and operation (Ops) and
enables high frequency software releases and continuous innovation based on the
feedback from operations. In this paper, we outline a safe MLOps process for
the continuous development and safety assurance of ML-based systems in the
railway domain. It integrates system engineering, safety assurance, and the ML
life-cycle in a comprehensive workflow. We present the individual stages of the
process and their interactions. Moreover, we describe relevant challenges to
automate the different stages of the safe MLOps process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeller_M/0/1/0/all/0/1&quot;&gt;Marc Zeller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waschulzik_T/0/1/0/all/0/1&quot;&gt;Thomas Waschulzik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmid_R/0/1/0/all/0/1&quot;&gt;Reiner Schmid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bahlmann_C/0/1/0/all/0/1&quot;&gt;Claus Bahlmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02882">
<title>Contrast Is All You Need. (arXiv:2307.02882v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.02882</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we analyze data-scarce classification scenarios, where
available labeled legal data is small and imbalanced, potentially hurting the
quality of the results. We focused on two finetuning objectives; SetFit
(Sentence Transformer Finetuning), a contrastive learning setup, and a vanilla
finetuning setup on a legal provision classification task. Additionally, we
compare the features that are extracted with LIME (Local Interpretable
Model-agnostic Explanations) to see which particular features contributed to
the model&apos;s classification decisions. The results show that a contrastive setup
with SetFit performed better than vanilla finetuning while using a fraction of
the training samples. LIME results show that the contrastive learning approach
helps boost both positive and negative features which are legally informative
and contribute to the classification results. Thus a model finetuned with a
contrastive objective seems to base its decisions more confidently on legally
informative features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kilic_B/0/1/0/all/0/1&quot;&gt;Burak Kilic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bex_F/0/1/0/all/0/1&quot;&gt;Florix Bex&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gatt_A/0/1/0/all/0/1&quot;&gt;Albert Gatt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02891">
<title>BaBE: Enhancing Fairness via Estimation of Latent Explaining Variables. (arXiv:2307.02891v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.02891</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of unfair discrimination between two groups and
propose a pre-processing method to achieve fairness. Corrective methods like
statistical parity usually lead to bad accuracy and do not really achieve
fairness in situations where there is a correlation between the sensitive
attribute S and the legitimate attribute E (explanatory variable) that should
determine the decision. To overcome these drawbacks, other notions of fairness
have been proposed, in particular, conditional statistical parity and equal
opportunity. However, E is often not directly observable in the data, i.e., it
is a latent variable. We may observe some other variable Z representing E, but
the problem is that Z may also be affected by S, hence Z itself can be biased.
To deal with this problem, we propose BaBE (Bayesian Bias Elimination), an
approach based on a combination of Bayes inference and the
Expectation-Maximization method, to estimate the most likely value of E for a
given Z for each group. The decision can then be based directly on the
estimated E. We show, by experiments on synthetic and real data sets, that our
approach provides a good level of fairness as well as high accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Binkyte_R/0/1/0/all/0/1&quot;&gt;Ruta Binkyte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gorla_D/0/1/0/all/0/1&quot;&gt;Daniele Gorla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palamidessi_C/0/1/0/all/0/1&quot;&gt;Catuscia Palamidessi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02909">
<title>Audio-visual End-to-end Multi-channel Speech Separation, Dereverberation and Recognition. (arXiv:2307.02909v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2307.02909</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate recognition of cocktail party speech containing overlapping
speakers, noise and reverberation remains a highly challenging task to date.
Motivated by the invariance of visual modality to acoustic signal corruption,
an audio-visual multi-channel speech separation, dereverberation and
recognition approach featuring a full incorporation of visual information into
all system components is proposed in this paper. The efficacy of the video
input is consistently demonstrated in mask-based MVDR speech separation,
DNN-WPE or spectral mapping (SpecM) based speech dereverberation front-end and
Conformer ASR back-end. Audio-visual integrated front-end architectures
performing speech separation and dereverberation in a pipelined or joint
fashion via mask-based WPD are investigated. The error cost mismatch between
the speech enhancement front-end and ASR back-end components is minimized by
end-to-end jointly fine-tuning using either the ASR cost function alone, or its
interpolation with the speech enhancement loss. Experiments were conducted on
the mixture overlapped and reverberant speech data constructed using simulation
or replay of the Oxford LRS2 dataset. The proposed audio-visual multi-channel
speech separation, dereverberation and recognition systems consistently
outperformed the comparable audio-only baseline by 9.1% and 6.2% absolute
(41.7% and 36.0% relative) word error rate (WER) reductions. Consistent speech
enhancement improvements were also obtained on PESQ, STOI and SRMR scores.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guinan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Deng_J/0/1/0/all/0/1&quot;&gt;Jiajun Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Geng_M/0/1/0/all/0/1&quot;&gt;Mengzhe Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jin_Z/0/1/0/all/0/1&quot;&gt;Zengrui Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianzi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shujie Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cui_M/0/1/0/all/0/1&quot;&gt;Mingyu Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Meng_H/0/1/0/all/0/1&quot;&gt;Helen Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xunying Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02912">
<title>LEA: Improving Sentence Similarity Robustness to Typos Using Lexical Attention Bias. (arXiv:2307.02912v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.02912</link>
<description rdf:parseType="Literal">&lt;p&gt;Textual noise, such as typos or abbreviations, is a well-known issue that
penalizes vanilla Transformers for most downstream tasks. We show that this is
also the case for sentence similarity, a fundamental task in multiple domains,
e.g. matching, retrieval or paraphrasing. Sentence similarity can be approached
using cross-encoders, where the two sentences are concatenated in the input
allowing the model to exploit the inter-relations between them. Previous works
addressing the noise issue mainly rely on data augmentation strategies, showing
improved robustness when dealing with corrupted samples that are similar to the
ones used for training. However, all these methods still suffer from the token
distribution shift induced by typos. In this work, we propose to tackle textual
noise by equipping cross-encoders with a novel LExical-aware Attention module
(LEA) that incorporates lexical similarities between words in both sentences.
By using raw text similarities, our approach avoids the tokenization shift
problem obtaining improved robustness. We demonstrate that the attention bias
introduced by LEA helps cross-encoders to tackle complex scenarios with textual
noise, specially in domains with short-text descriptions and limited context.
Experiments using three popular Transformer encoders in five e-commerce
datasets for product matching show that LEA consistently boosts performance
under the presence of noise, while remaining competitive on the original
(clean) splits. We also evaluate our approach in two datasets for textual
entailment and paraphrasing showing that LEA is robust to typos in domains with
longer sentences and more natural context. Additionally, we thoroughly analyze
several design choices in our approach, providing insights about the impact of
the decisions made and fostering future research in cross-encoders dealing with
typos.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Almagro_M/0/1/0/all/0/1&quot;&gt;Mario Almagro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Almazan_E/0/1/0/all/0/1&quot;&gt;Emilio Almaz&amp;#xe1;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortego_D/0/1/0/all/0/1&quot;&gt;Diego Ortego&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jimenez_D/0/1/0/all/0/1&quot;&gt;David Jim&amp;#xe9;nez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02933">
<title>In Time and Space: Towards Usable Adaptive Control for Assistive Robotic Arms. (arXiv:2307.02933v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2307.02933</link>
<description rdf:parseType="Literal">&lt;p&gt;Robotic solutions, in particular robotic arms, are becoming more frequently
deployed for close collaboration with humans, for example in manufacturing or
domestic care environments. These robotic arms require the user to control
several Degrees-of-Freedom (DoFs) to perform tasks, primarily involving
grasping and manipulating objects. Standard input devices predominantly have
two DoFs, requiring time-consuming and cognitively demanding mode switches to
select individual DoFs. Contemporary Adaptive DoF Mapping Controls (ADMCs) have
shown to decrease the necessary number of mode switches but were up to now not
able to significantly reduce the perceived workload. Users still bear the
mental workload of incorporating abstract mode switching into their workflow.
We address this by providing feed-forward multimodal feedback using updated
recommendations of ADMC, allowing users to visually compare the current and the
suggested mapping in real-time. We contrast the effectiveness of two new
approaches that a) continuously recommend updated DoF combinations or b) use
discrete thresholds between current robot movements and new recommendations.
Both are compared in a Virtual Reality (VR) in-person study against a classic
control method. Significant results for lowered task completion time, fewer
mode switches, and reduced perceived workload conclusively establish that in
combination with feedforward, ADMC methods can indeed outperform classic mode
switching. A lack of apparent quantitative differences between Continuous and
Threshold reveals the importance of user-centered customization options.
Including these implications in the development process will improve usability,
which is essential for successfully implementing robotic technologies with high
user acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pascher_M/0/1/0/all/0/1&quot;&gt;Max Pascher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kronhardt_K/0/1/0/all/0/1&quot;&gt;Kirill Kronhardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldau_F/0/1/0/all/0/1&quot;&gt;Felix Ferdinand Goldau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frese_U/0/1/0/all/0/1&quot;&gt;Udo Frese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gerken_J/0/1/0/all/0/1&quot;&gt;Jens Gerken&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02947">
<title>A Neuromorphic Architecture for Reinforcement Learning from Real-Valued Observations. (arXiv:2307.02947v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2307.02947</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement Learning (RL) provides a powerful framework for decision-making
in complex environments. However, implementing RL in hardware-efficient and
bio-inspired ways remains a challenge. This paper presents a novel Spiking
Neural Network (SNN) architecture for solving RL problems with real-valued
observations. The proposed model incorporates multi-layered event-based
clustering, with the addition of Temporal Difference (TD)-error modulation and
eligibility traces, building upon prior work. An ablation study confirms the
significant impact of these components on the proposed model&apos;s performance. A
tabular actor-critic algorithm with eligibility traces and a state-of-the-art
Proximal Policy Optimization (PPO) algorithm are used as benchmarks. Our
network consistently outperforms the tabular approach and successfully
discovers stable control policies on classic RL environments: mountain car,
cart-pole, and acrobot. The proposed model offers an appealing trade-off in
terms of computational and hardware implementation requirements. The model does
not require an external memory buffer nor a global error gradient computation,
and synaptic updates occur online, driven by local learning rules and a
broadcasted TD-error signal. Thus, this work contributes to the development of
more hardware-efficient RL solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chevtchenko_S/0/1/0/all/0/1&quot;&gt;Sergio F. Chevtchenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bethi_Y/0/1/0/all/0/1&quot;&gt;Yeshwanth Bethi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1&quot;&gt;Teresa B. Ludermir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Afshar_S/0/1/0/all/0/1&quot;&gt;Saeed Afshar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02971">
<title>On the Cultural Gap in Text-to-Image Generation. (arXiv:2307.02971v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02971</link>
<description rdf:parseType="Literal">&lt;p&gt;One challenge in text-to-image (T2I) generation is the inadvertent reflection
of culture gaps present in the training data, which signifies the disparity in
generated image quality when the cultural elements of the input text are rarely
collected in the training set. Although various T2I models have shown
impressive but arbitrary examples, there is no benchmark to systematically
evaluate a T2I model&apos;s ability to generate cross-cultural images. To bridge the
gap, we propose a Challenging Cross-Cultural (C3) benchmark with comprehensive
evaluation criteria, which can assess how well-suited a model is to a target
culture. By analyzing the flawed images generated by the Stable Diffusion model
on the C3 benchmark, we find that the model often fails to generate certain
cultural objects. Accordingly, we propose a novel multi-modal metric that
considers object-text alignment to filter the fine-tuning data in the target
culture, which is used to fine-tune a T2I model to improve cross-cultural
generation. Experimental results show that our multi-modal metric provides
stronger data selection performance on the C3 benchmark than existing metrics,
in which the object-text alignment is crucial. We release the benchmark, data,
code, and generated images to facilitate future research on culturally diverse
T2I generation (https://github.com/longyuewangdcu/C3-Bench).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bingshuai Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Longyue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1&quot;&gt;Chenyang Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1&quot;&gt;Jinsong Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1&quot;&gt;Shuming Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1&quot;&gt;Zhaopeng Tu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02984">
<title>A Privacy-Preserving Walk in the Latent Space of Generative Models for Medical Applications. (arXiv:2307.02984v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.02984</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Networks (GANs) have demonstrated their ability to
generate synthetic samples that match a target distribution. However, from a
privacy perspective, using GANs as a proxy for data sharing is not a safe
solution, as they tend to embed near-duplicates of real samples in the latent
space. Recent works, inspired by k-anonymity principles, address this issue
through sample aggregation in the latent space, with the drawback of reducing
the dataset by a factor of k. Our work aims to mitigate this problem by
proposing a latent space navigation strategy able to generate diverse synthetic
samples that may support effective training of deep models, while addressing
privacy concerns in a principled way. Our approach leverages an auxiliary
identity classifier as a guide to non-linearly walk between points in the
latent space, minimizing the risk of collision with near-duplicates of real
samples. We empirically demonstrate that, given any random pair of points in
the latent space, our walking strategy is safer than linear interpolation. We
then test our path-finding strategy combined to k-same methods and demonstrate,
on two benchmarks for tuberculosis and diabetic retinopathy classification,
that training a model using samples generated by our approach mitigate drops in
performance, while keeping privacy preservation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pennisi_M/0/1/0/all/0/1&quot;&gt;Matteo Pennisi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salanitri_F/0/1/0/all/0/1&quot;&gt;Federica Proietto Salanitri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellitto_G/0/1/0/all/0/1&quot;&gt;Giovanni Bellitto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palazzo_S/0/1/0/all/0/1&quot;&gt;Simone Palazzo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagci_U/0/1/0/all/0/1&quot;&gt;Ulas Bagci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spampinato_C/0/1/0/all/0/1&quot;&gt;Concetto Spampinato&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03007">
<title>Self-supervised Optimization of Hand Pose Estimation using Anatomical Features and Iterative Learning. (arXiv:2307.03007v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03007</link>
<description rdf:parseType="Literal">&lt;p&gt;Manual assembly workers face increasing complexity in their work.
Human-centered assistance systems could help, but object recognition as an
enabling technology hinders sophisticated human-centered design of these
systems. At the same time, activity recognition based on hand poses suffers
from poor pose estimation in complex usage scenarios, such as wearing gloves.
This paper presents a self-supervised pipeline for adapting hand pose
estimation to specific use cases with minimal human interaction. This enables
cheap and robust hand posebased activity recognition. The pipeline consists of
a general machine learning model for hand pose estimation trained on a
generalized dataset, spatial and temporal filtering to account for anatomical
constraints of the hand, and a retraining step to improve the model. Different
parameter combinations are evaluated on a publicly available and annotated
dataset. The best parameter and model combination is then applied to unlabelled
videos from a manual assembly scenario. The effectiveness of the pipeline is
demonstrated by training an activity recognition as a downstream task in the
manual assembly scenario.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jauch_C/0/1/0/all/0/1&quot;&gt;Christian Jauch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leitritz_T/0/1/0/all/0/1&quot;&gt;Timo Leitritz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huber_M/0/1/0/all/0/1&quot;&gt;Marco F. Huber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03015">
<title>Sequential Neural Barriers for Scalable Dynamic Obstacle Avoidance. (arXiv:2307.03015v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.03015</link>
<description rdf:parseType="Literal">&lt;p&gt;There are two major challenges for scaling up robot navigation around dynamic
obstacles: the complex interaction dynamics of the obstacles can be hard to
model analytically, and the complexity of planning and control grows
exponentially in the number of obstacles. Data-driven and learning-based
methods are thus particularly valuable in this context. However, data-driven
methods are sensitive to distribution drift, making it hard to train and
generalize learned models across different obstacle densities. We propose a
novel method for compositional learning of Sequential Neural Control Barrier
models (SNCBFs) to achieve scalability. Our approach exploits an important
observation: the spatial interaction patterns of multiple dynamic obstacles can
be decomposed and predicted through temporal sequences of states for each
obstacle. Through decomposition, we can generalize control policies trained
only with a small number of obstacles, to environments where the obstacle
density can be 100x higher. We demonstrate the benefits of the proposed methods
in improving dynamic collision avoidance in comparison with existing methods
including potential fields, end-to-end reinforcement learning, and
model-predictive control. We also perform hardware experiments and show the
practical effectiveness of the approach in the supplementary video.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hongzhan Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hirayama_C/0/1/0/all/0/1&quot;&gt;Chiaki Hirayama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Chenning Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herbert_S/0/1/0/all/0/1&quot;&gt;Sylvia Herbert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Sicun Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03039">
<title>Art Authentication with Vision Transformers. (arXiv:2307.03039v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03039</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, Transformers, initially developed for language, have been
successfully applied to visual tasks. Vision Transformers have been shown to
push the state-of-the-art in a wide range of tasks, including image
classification, object detection, and semantic segmentation. While ample
research has shown promising results in art attribution and art authentication
tasks using Convolutional Neural Networks, this paper examines if the
superiority of Vision Transformers extends to art authentication, improving,
thus, the reliability of computer-based authentication of artworks. Using a
carefully compiled dataset of authentic paintings by Vincent van Gogh and two
contrast datasets, we compare the art authentication performances of Swin
Transformers with those of EfficientNet. Using a standard contrast set
containing imitations and proxies (works by painters with styles closely
related to van Gogh), we find that EfficientNet achieves the best performance
overall. With a contrast set that only consists of imitations, we find the Swin
Transformer to be superior to EfficientNet by achieving an authentication
accuracy of over 85%. These results lead us to conclude that Vision
Transformers represent a strong and promising contender in art authentication,
particularly in enhancing the computer-based ability to detect artistic
imitations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaerf_L/0/1/0/all/0/1&quot;&gt;Ludovica Schaerf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Popovici_C/0/1/0/all/0/1&quot;&gt;Carina Popovici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Postma_E/0/1/0/all/0/1&quot;&gt;Eric Postma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03056">
<title>Generalizing Backpropagation for Gradient-Based Interpretability. (arXiv:2307.03056v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03056</link>
<description rdf:parseType="Literal">&lt;p&gt;Many popular feature-attribution methods for interpreting deep neural
networks rely on computing the gradients of a model&apos;s output with respect to
its inputs. While these methods can indicate which input features may be
important for the model&apos;s prediction, they reveal little about the inner
workings of the model itself. In this paper, we observe that the gradient
computation of a model is a special case of a more general formulation using
semirings. This observation allows us to generalize the backpropagation
algorithm to efficiently compute other interpretable statistics about the
gradient graph of a neural network, such as the highest-weighted path and
entropy. We implement this generalized algorithm, evaluate it on synthetic
datasets to better understand the statistics it computes, and apply it to study
BERT&apos;s behavior on the subject-verb number agreement task (SVA). With this
method, we (a) validate that the amount of gradient flow through a component of
a model reflects its importance to a prediction and (b) for SVA, identify which
pathways of the self-attention mechanism are most important.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_K/0/1/0/all/0/1&quot;&gt;Kevin Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hennigen_L/0/1/0/all/0/1&quot;&gt;Lucas Torroba Hennigen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoehr_N/0/1/0/all/0/1&quot;&gt;Niklas Stoehr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Warstadt_A/0/1/0/all/0/1&quot;&gt;Alexander Warstadt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1&quot;&gt;Ryan Cotterell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03067">
<title>DeepOnto: A Python Package for Ontology Engineering with Deep Learning. (arXiv:2307.03067v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.03067</link>
<description rdf:parseType="Literal">&lt;p&gt;Applying deep learning techniques, particularly language models (LMs), in
ontology engineering has raised widespread attention. However, deep learning
frameworks like PyTorch and Tensorflow are predominantly developed for Python
programming, while widely-used ontology APIs, such as the OWL API and Jena, are
primarily Java-based. To facilitate seamless integration of these frameworks
and APIs, we present Deeponto, a Python package designed for ontology
engineering. The package encompasses a core ontology processing module founded
on the widely-recognised and reliable OWL API, encapsulating its fundamental
features in a more &quot;Pythonic&quot; manner and extending its capabilities to include
other essential components including reasoning, verbalisation, normalisation,
projection, and more. Building on this module, Deeponto offers a suite of
tools, resources, and algorithms that support various ontology engineering
tasks, such as ontology alignment and completion, by harnessing deep learning
methodologies, primarily pre-trained LMs. In this paper, we also demonstrate
the practical utility of Deeponto through two use-cases: the Digital Health
Coaching in Samsung Research UK and the Bio-ML track of the Ontology Alignment
Evaluation Initiative (OAEI).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yuan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiaoyan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hang Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horrocks_I/0/1/0/all/0/1&quot;&gt;Ian Horrocks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allocca_C/0/1/0/all/0/1&quot;&gt;Carlo Allocca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Taehun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sapkota_B/0/1/0/all/0/1&quot;&gt;Brahmananda Sapkota&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03084">
<title>OpenDelta: A Plug-and-play Library for Parameter-efficient Adaptation of Pre-trained Models. (arXiv:2307.03084v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03084</link>
<description rdf:parseType="Literal">&lt;p&gt;The scale of large pre-trained models (PTMs) poses significant challenges in
adapting to downstream tasks due to the high optimization overhead and storage
costs associated with full-parameter fine-tuning. To address this, many studies
explore parameter-efficient tuning methods, also framed as &quot;delta tuning&quot;,
which updates only a small subset of parameters, known as &quot;delta modules&quot;,
while keeping the backbone model&apos;s parameters fixed. However, the practicality
and flexibility of delta tuning have been limited due to existing
implementations that directly modify the code of the backbone PTMs and
hard-code specific delta tuning methods for each PTM. In this paper, we present
OpenDelta, an open-source library that overcomes these limitations by providing
a plug-and-play implementation of various delta tuning methods. Our novel
techniques eliminate the need to modify the backbone PTMs&apos; code, making
OpenDelta compatible with different, even novel PTMs. OpenDelta is designed to
be simple, modular, and extensible, providing a comprehensive platform for
researchers and practitioners to adapt large PTMs efficiently.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shengding Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1&quot;&gt;Ning Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Weilin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1&quot;&gt;Xingtai Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;Maosong Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03104">
<title>Efficient Domain Adaptation of Sentence Embeddings using Adapters. (arXiv:2307.03104v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.03104</link>
<description rdf:parseType="Literal">&lt;p&gt;Sentence embeddings enable us to capture the semantic similarity of short
texts. Most sentence embedding models are trained for general semantic textual
similarity (STS) tasks. Therefore, to use sentence embeddings in a particular
domain, the model must be adapted to it in order to achieve good results.
Usually, this is done by fine-tuning the entire sentence embedding model for
the domain of interest. While this approach yields state-of-the-art results,
all of the model&apos;s weights are updated during fine-tuning, making this method
resource-intensive. Therefore, instead of fine-tuning entire sentence embedding
models for each target domain individually, we propose to train lightweight
adapters. These domain-specific adapters do not require fine-tuning all
underlying sentence embedding model parameters. Instead, we only train a small
number of additional parameters while keeping the weights of the underlying
sentence embedding model fixed. Training domain-specific adapters allows always
using the same base model and only exchanging the domain-specific adapters to
adapt sentence embeddings to a specific domain. We show that using adapters for
parameter-efficient domain adaptation of sentence embeddings yields competitive
performance within 1% of a domain-adapted, entirely fine-tuned sentence
embedding model while only training approximately 3.6% of the parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schopf_T/0/1/0/all/0/1&quot;&gt;Tim Schopf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_D/0/1/0/all/0/1&quot;&gt;Dennis Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matthes_F/0/1/0/all/0/1&quot;&gt;Florian Matthes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03109">
<title>A Survey on Evaluation of Large Language Models. (arXiv:2307.03109v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.03109</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) are gaining increasing popularity in both
academia and industry, owing to their unprecedented performance in various
applications. As LLMs continue to play a vital role in both research and daily
use, their evaluation becomes increasingly critical, not only at the task
level, but also at the society level for better understanding of their
potential risks. Over the past years, significant efforts have been made to
examine LLMs from various perspectives. This paper presents a comprehensive
review of these evaluation methods for LLMs, focusing on three key dimensions:
what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide
an overview from the perspective of evaluation tasks, encompassing general
natural language processing tasks, reasoning, medical usage, ethics,
educations, natural and social sciences, agent applications, and other areas.
Secondly, we answer the `where&apos; and `how&apos; questions by diving into the
evaluation methods and benchmarks, which serve as crucial components in
assessing performance of LLMs. Then, we summarize the success and failure cases
of LLMs in different tasks. Finally, we shed light on several future challenges
that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to
researchers in the realm of LLMs evaluation, thereby aiding the development of
more proficient LLMs. Our key point is that evaluation should be treated as an
essential discipline to better assist the development of LLMs. We consistently
maintain the related open-source materials at:
https://github.com/MLGroupJLU/LLM-eval-survey.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1&quot;&gt;Yupeng Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jindong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1&quot;&gt;Kaijie Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Linyi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1&quot;&gt;Xiaoyuan Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Cunxiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yidong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1&quot;&gt;Wei Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yue Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1&quot;&gt;Yi Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1&quot;&gt;Philip S. Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qiang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xing Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03119">
<title>Learning Multi-Agent Intention-Aware Communication for Optimal Multi-Order Execution in Finance. (arXiv:2307.03119v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.03119</link>
<description rdf:parseType="Literal">&lt;p&gt;Order execution is a fundamental task in quantitative finance, aiming at
finishing acquisition or liquidation for a number of trading orders of the
specific assets. Recent advance in model-free reinforcement learning (RL)
provides a data-driven solution to the order execution problem. However, the
existing works always optimize execution for an individual order, overlooking
the practice that multiple orders are specified to execute simultaneously,
resulting in suboptimality and bias. In this paper, we first present a
multi-agent RL (MARL) method for multi-order execution considering practical
constraints. Specifically, we treat every agent as an individual operator to
trade one specific order, while keeping communicating with each other and
collaborating for maximizing the overall profits. Nevertheless, the existing
MARL algorithms often incorporate communication among agents by exchanging only
the information of their partial observations, which is inefficient in
complicated financial market. To improve collaboration, we then propose a
learnable multi-round communication protocol, for the agents communicating the
intended actions with each other and refining accordingly. It is optimized
through a novel action value attribution method which is provably consistent
with the original learning objective yet more efficient. The experiments on the
data from two real-world markets have illustrated superior performance with
significantly better collaboration effectiveness achieved by our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yuchen Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1&quot;&gt;Zhenggang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1&quot;&gt;Kan Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Weiqing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Li Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1&quot;&gt;Jiang Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dongsheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weinan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tie-Yan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03135">
<title>Distilling Large Vision-Language Model with Out-of-Distribution Generalizability. (arXiv:2307.03135v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03135</link>
<description rdf:parseType="Literal">&lt;p&gt;Large vision-language models have achieved outstanding performance, but their
size and computational requirements make their deployment on
resource-constrained devices and time-sensitive tasks impractical. Model
distillation, the process of creating smaller, faster models that maintain the
performance of larger models, is a promising direction towards the solution.
This paper investigates the distillation of visual representations in large
teacher vision-language models into lightweight student models using a small-
or mid-scale dataset. Notably, this study focuses on open-vocabulary
out-of-distribution (OOD) generalization, a challenging problem that has been
overlooked in previous model distillation literature. We propose two principles
from vision and language modality perspectives to enhance student&apos;s OOD
generalization: (1) by better imitating teacher&apos;s visual representation space,
and carefully promoting better coherence in vision-language alignment with the
teacher; (2) by enriching the teacher&apos;s language representations with
informative and finegrained semantic attributes to effectively distinguish
between different labels. We propose several metrics and conduct extensive
experiments to investigate their techniques. The results demonstrate
significant improvements in zero-shot and few-shot student performance on
open-vocabulary out-of-distribution classification, highlighting the
effectiveness of our proposed approaches. Our code will be released at
https://github.com/xuanlinli17/large_vlm_distillation_ood
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuanlin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yunhao Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Minghua Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1&quot;&gt;Zhan Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1&quot;&gt;Zhuowen Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hao Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03162">
<title>BrickPal: Augmented Reality-based Assembly Instructions for Brick Models. (arXiv:2307.03162v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2307.03162</link>
<description rdf:parseType="Literal">&lt;p&gt;The assembly instruction is a mandatory component of Lego-like brick sets.The
conventional production of assembly instructions requires a considerable amount
of manual fine-tuning, which is intractable for casual users and customized
brick sets.Moreover, the traditional paper-based instructions lack
expressiveness and interactivity.To tackle the two problems above, we present
BrickPal, an augmented reality-based system, which visualizes assembly
instructions in an augmented reality head-mounted display. It utilizes Natural
Language Processing (NLP) techniques to generate plausible assembly sequences,
and provide real-time guidance in the AR headset.Our user study demonstrates
BrickPal&apos;s effectiveness at assisting users in brick assembly compared to
traditional assembly methods. Additionally, the NLP algorithm-generated
assembly sequences achieve the same usability with manually adapted sequences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yao Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+zhang_R/0/1/0/all/0/1&quot;&gt;Ran zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhou Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1&quot;&gt;Xiao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1&quot;&gt;Hongni Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yi Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03170">
<title>Focused Transformer: Contrastive Training for Context Scaling. (arXiv:2307.03170v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.03170</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models have an exceptional capability to incorporate new
information in a contextual manner. However, the full potential of such an
approach is often restrained due to a limitation in the effective context
length. One solution to this issue is to endow an attention layer with access
to an external memory, which comprises of (key, value) pairs. Yet, as the
number of documents increases, the proportion of relevant keys to irrelevant
ones decreases, leading the model to focus more on the irrelevant keys. We
identify a significant challenge, dubbed the distraction issue, where keys
linked to different semantic values might overlap, making them hard to
distinguish. To tackle this problem, we introduce the Focused Transformer
(FoT), a technique that employs a training process inspired by contrastive
learning. This novel approach enhances the structure of the (key, value) space,
enabling an extension of the context length. Our method allows for fine-tuning
pre-existing, large-scale models to lengthen their effective context. This is
demonstrated by our fine-tuning of $3B$ and $7B$ OpenLLaMA checkpoints. The
resulting models, which we name LongLLaMA, exhibit advancements in tasks
requiring a long context. We further illustrate that our LongLLaMA models
adeptly manage a $256 k$ context length for passkey retrieval.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tworkowski_S/0/1/0/all/0/1&quot;&gt;Szymon Tworkowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Staniszewski_K/0/1/0/all/0/1&quot;&gt;Konrad Staniszewski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pacek_M/0/1/0/all/0/1&quot;&gt;Miko&amp;#x142;aj Pacek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yuhuai Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michalewski_H/0/1/0/all/0/1&quot;&gt;Henryk Michalewski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milos_P/0/1/0/all/0/1&quot;&gt;Piotr Mi&amp;#x142;o&amp;#x15b;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03171">
<title>LEO: Learning Efficient Orderings for Multiobjective Binary Decision Diagrams. (arXiv:2307.03171v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.03171</link>
<description rdf:parseType="Literal">&lt;p&gt;Approaches based on Binary decision diagrams (BDDs) have recently achieved
state-of-the-art results for multiobjective integer programming problems. The
variable ordering used in constructing BDDs can have a significant impact on
their size and on the quality of bounds derived from relaxed or restricted BDDs
for single-objective optimization problems. We first showcase a similar impact
of variable ordering on the Pareto frontier (PF) enumeration time for the
multiobjective knapsack problem, suggesting the need for deriving variable
ordering methods that improve the scalability of the multiobjective BDD
approach. To that end, we derive a novel parameter configuration space based on
variable scoring functions which are linear in a small set of interpretable and
easy-to-compute variable features. We show how the configuration space can be
efficiently explored using black-box optimization, circumventing the curse of
dimensionality (in the number of variables and objectives), and finding good
orderings that reduce the PF enumeration time. However, black-box optimization
approaches incur a computational overhead that outweighs the reduction in time
due to good variable ordering. To alleviate this issue, we propose LEO, a
supervised learning approach for finding efficient variable orderings that
reduce the enumeration time. Experiments on benchmark sets from the knapsack
problem with 3-7 objectives and up to 80 variables show that LEO is ~30-300%
and ~10-200% faster at PF enumeration than common ordering strategies and
algorithm configuration. Our code and instances are available at
https://github.com/khalil-research/leo.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_R/0/1/0/all/0/1&quot;&gt;Rahul Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khalil_E/0/1/0/all/0/1&quot;&gt;Elias B. Khalil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03175">
<title>Push Past Green: Learning to Look Behind Plant Foliage by Moving It. (arXiv:2307.03175v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.03175</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous agriculture applications (e.g., inspection, phenotyping, plucking
fruits) require manipulating the plant foliage to look behind the leaves and
the branches. Partial visibility, extreme clutter, thin structures, and unknown
geometry and dynamics for plants make such manipulation challenging. We tackle
these challenges through data-driven methods. We use self-supervision to train
SRPNet, a neural network that predicts what space is revealed on execution of a
candidate action on a given plant. We use SRPNet with the cross-entropy method
to predict actions that are effective at revealing space beneath plant foliage.
Furthermore, as SRPNet does not just predict how much space is revealed but
also where it is revealed, we can execute a sequence of actions that
incrementally reveal more and more space beneath the plant foliage. We
experiment with a synthetic (vines) and a real plant (Dracaena) on a physical
test-bed across 5 settings including 2 settings that test generalization to
novel plant configurations. Our experiments reveal the effectiveness of our
overall method, PPG, over a competitive hand-crafted exploration method, and
the effectiveness of SRPNet over a hand-crafted dynamics model and relevant
ablations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1&quot;&gt;Saurabh Gupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2008.02215">
<title>A Time Leap Challenge for SAT Solving. (arXiv:2008.02215v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2008.02215</link>
<description rdf:parseType="Literal">&lt;p&gt;We compare the impact of hardware advancement and algorithm advancement for
SAT solving over the last two decades. In particular, we compare 20-year-old
SAT-solvers on new computer hardware with modern SAT-solvers on 20-year-old
hardware. Our findings show that the progress on the algorithmic side has at
least as much impact as the progress on the hardware side.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fichte_J/0/1/0/all/0/1&quot;&gt;Johannes K. Fichte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hecher_M/0/1/0/all/0/1&quot;&gt;Markus Hecher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szeider_S/0/1/0/all/0/1&quot;&gt;Stefan Szeider&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2102.11529">
<title>Differentiable Logic Machines. (arXiv:2102.11529v5 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2102.11529</link>
<description rdf:parseType="Literal">&lt;p&gt;The integration of reasoning, learning, and decision-making is key to build
more general artificial intelligence systems. As a step in this direction, we
propose a novel neural-logic architecture, called differentiable logic machine
(DLM), that can solve both inductive logic programming (ILP) and reinforcement
learning (RL) problems, where the solution can be interpreted as a first-order
logic program. Our proposition includes several innovations. Firstly, our
architecture defines a restricted but expressive continuous relaxation of the
space of first-order logic programs by assigning weights to predicates instead
of rules, in contrast to most previous neural-logic approaches. Secondly, with
this differentiable architecture, we propose several (supervised and RL)
training procedures, based on gradient descent, which can recover a
fully-interpretable solution (i.e., logic formula). Thirdly, to accelerate RL
training, we also design a novel critic architecture that enables actor-critic
algorithms. Fourthly, to solve hard problems, we propose an incremental
training procedure that can learn a logic program progressively. Compared to
state-of-the-art (SOTA) differentiable ILP methods, DLM successfully solves all
the considered ILP problems with a higher percentage of successful seeds (up to
3.5$\times$). On RL problems, without requiring an interpretable solution, DLM
outperforms other non-interpretable neural-logic RL approaches in terms of
rewards (up to 3.9%). When enforcing interpretability, DLM can solve harder RL
problems (e.g., Sorting, Path) Moreover, we show that deep logic programs can
be learned via incremental supervised training. In addition to this excellent
performance, DLM can scale well in terms of memory and computational time,
especially during the testing phase where it can deal with much more constants
($&amp;gt;$2$\times$) than SOTA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zimmer_M/0/1/0/all/0/1&quot;&gt;Matthieu Zimmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1&quot;&gt;Xuening Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glanois_C/0/1/0/all/0/1&quot;&gt;Claire Glanois&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhaohui Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weng_P/0/1/0/all/0/1&quot;&gt;Paul Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1&quot;&gt;Jianye Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wulong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2103.09118">
<title>Balancing Biases and Preserving Privacy on Balanced Faces in the Wild. (arXiv:2103.09118v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2103.09118</link>
<description rdf:parseType="Literal">&lt;p&gt;There are demographic biases present in current facial recognition (FR)
models. To measure these biases across different ethnic and gender subgroups,
we introduce our Balanced Faces in the Wild (BFW) dataset. This dataset allows
for the characterization of FR performance per subgroup. We found that relying
on a single score threshold to differentiate between genuine and imposters
sample pairs leads to suboptimal results. Additionally, performance within
subgroups often varies significantly from the global average. Therefore,
specific error rates only hold for populations that match the validation data.
To mitigate imbalanced performances, we propose a novel domain adaptation
learning scheme that uses facial features extracted from state-of-the-art
neural networks. This scheme boosts the average performance and preserves
identity information while removing demographic knowledge. Removing demographic
knowledge prevents potential biases from affecting decision-making and protects
privacy by eliminating demographic information. We explore the proposed method
and demonstrate that subgroup classifiers can no longer learn from features
projected using our domain adaptation scheme. For access to the source code and
data, please visit https://github.com/visionjo/facerec-bias-bfw.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robinson_J/0/1/0/all/0/1&quot;&gt;Joseph P Robinson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1&quot;&gt;Can Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henon_Y/0/1/0/all/0/1&quot;&gt;Yann Henon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Timoner_S/0/1/0/all/0/1&quot;&gt;Samson Timoner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yun Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.13557">
<title>AGM Belief Revision, Semantically. (arXiv:2112.13557v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2112.13557</link>
<description rdf:parseType="Literal">&lt;p&gt;We establish a generic, model-theoretic characterization of belief revision
operators implementing the paradigm of minimal change according to the seminal
work by Alchourr\&apos;{o}n, G\&quot;{a}rdenfors, and Makinson (AGM). Our
characterization applies to all Tarskian logics, that is, all logics with a
classical model-theoretic semantics, and hence a wide variety of formalisms
used in knowledge representation and beyond, including many for which a
model-theoretic characterization has hitherto been lacking. Our starting point
is the approach by Katsuno and Mendelzon (K&amp;amp;M), who provided such a
characterization for propositional logic over finite signatures. We generalize
K&amp;amp;M&apos;s approach to the setting of AGM-style revision over bases in arbitrary
Tarskian logics, where base may refer to one of the various ways of
representing an agent&apos;s beliefs (such as belief sets, arbitrary or finite sets
of sentences, or single sentences). Our first core result is a representation
theorem providing a two-way correspondence between AGM-style revision operators
and specific assignments: functions associating every base to a &quot;preference&quot;
relation over interpretations, which must be total but is - in contrast to
prior approaches - not always transitive. As our second core contribution, we
provide a characterization of all logics for which our result can be
strengthened to assignments producing transitive preference relations (as in
K&amp;amp;M&apos;s original work). Alongside these main contributions, we discuss diverse
variants of our findings as well as ramifications for other areas of belief
revision theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Falakh_F/0/1/0/all/0/1&quot;&gt;Faiq Miftakhul Falakh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rudolph_S/0/1/0/all/0/1&quot;&gt;Sebastian Rudolph&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sauerwald_K/0/1/0/all/0/1&quot;&gt;Kai Sauerwald&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.07856">
<title>Potential sources of dataset bias complicate investigation of underdiagnosis by machine learning algorithms. (arXiv:2201.07856v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2201.07856</link>
<description rdf:parseType="Literal">&lt;p&gt;An increasing number of reports raise concerns about the risk that machine
learning algorithms could amplify health disparities due to biases embedded in
the training data. Seyyed-Kalantari et al. find that models trained on three
chest X-ray datasets yield disparities in false-positive rates (FPR) across
subgroups on the &apos;no-finding&apos; label (indicating the absence of disease). The
models consistently yield higher FPR on subgroups known to be historically
underserved, and the study concludes that the models exhibit and potentially
even amplify systematic underdiagnosis. We argue that the experimental setup in
the study is insufficient to study algorithmic underdiagnosis. In the absence
of specific knowledge (or assumptions) about the extent and nature of the
dataset bias, it is difficult to investigate model bias. Importantly, their use
of test data exhibiting the same bias as the training data (due to random
splitting) severely complicates the interpretation of the reported disparities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernhardt_M/0/1/0/all/0/1&quot;&gt;M&amp;#xe9;lanie Bernhardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jones_C/0/1/0/all/0/1&quot;&gt;Charles Jones&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1&quot;&gt;Ben Glocker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.03447">
<title>Machine Learning-Friendly Biomedical Datasets for Equivalence and Subsumption Ontology Matching. (arXiv:2205.03447v7 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2205.03447</link>
<description rdf:parseType="Literal">&lt;p&gt;Ontology Matching (OM) plays an important role in many domains such as
bioinformatics and the Semantic Web, and its research is becoming increasingly
popular, especially with the application of machine learning (ML) techniques.
Although the Ontology Alignment Evaluation Initiative (OAEI) represents an
impressive effort for the systematic evaluation of OM systems, it still suffers
from several limitations including limited evaluation of subsumption mappings,
suboptimal reference mappings, and limited support for the evaluation of
ML-based systems. To tackle these limitations, we introduce five new biomedical
OM tasks involving ontologies extracted from Mondo and UMLS. Each task includes
both equivalence and subsumption matching; the quality of reference mappings is
ensured by human curation, ontology pruning, etc.; and a comprehensive
evaluation framework is proposed to measure OM performance from various
perspectives for both ML-based and non-ML-based OM systems. We report
evaluation results for OM systems of different types to demonstrate the usage
of these resources, all of which are publicly available as part of the new
BioML track at OAEI 2022.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yuan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiaoyan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hang Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jimenez_Ruiz_E/0/1/0/all/0/1&quot;&gt;Ernesto Jim&amp;#xe9;nez-Ruiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hadian_A/0/1/0/all/0/1&quot;&gt;Ali Hadian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horrocks_I/0/1/0/all/0/1&quot;&gt;Ian Horrocks&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.04779">
<title>Challenges and Opportunities in Offline Reinforcement Learning from Visual Observations. (arXiv:2206.04779v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.04779</link>
<description rdf:parseType="Literal">&lt;p&gt;Offline reinforcement learning has shown great promise in leveraging large
pre-collected datasets for policy learning, allowing agents to forgo
often-expensive online data collection. However, offline reinforcement learning
from visual observations with continuous action spaces remains under-explored,
with a limited understanding of the key challenges in this complex domain. In
this paper, we establish simple baselines for continuous control in the visual
domain and introduce a suite of benchmarking tasks for offline reinforcement
learning from visual observations designed to better represent the data
distributions present in real-world offline RL problems and guided by a set of
desiderata for offline RL from visual observations, including robustness to
visual distractions and visually identifiable changes in dynamics. Using this
suite of benchmarking tasks, we show that simple modifications to two popular
vision-based online reinforcement learning algorithms, DreamerV2 and DrQ-v2,
suffice to outperform existing offline RL methods and establish competitive
baselines for continuous control in the visual domain. We rigorously evaluate
these algorithms and perform an empirical evaluation of the differences between
state-of-the-art model-based and model-free offline RL methods for continuous
control from visual observations. All code and data used in this evaluation are
open-sourced to facilitate progress in this domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Cong Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ball_P/0/1/0/all/0/1&quot;&gt;Philip J. Ball&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rudner_T/0/1/0/all/0/1&quot;&gt;Tim G. J. Rudner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parker_Holder_J/0/1/0/all/0/1&quot;&gt;Jack Parker-Holder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osborne_M/0/1/0/all/0/1&quot;&gt;Michael A. Osborne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teh_Y/0/1/0/all/0/1&quot;&gt;Yee Whye Teh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.05251">
<title>Spotting Virus from Satellites: Modeling the Circulation of West Nile Virus Through Graph Neural Networks. (arXiv:2209.05251v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.05251</link>
<description rdf:parseType="Literal">&lt;p&gt;The occurrence of West Nile Virus (WNV) represents one of the most common
mosquito-borne zoonosis viral infections. Its circulation is usually associated
with climatic and environmental conditions suitable for vector proliferation
and virus replication. On top of that, several statistical models have been
developed to shape and forecast WNV circulation: in particular, the recent
massive availability of Earth Observation (EO) data, coupled with the
continuous advances in the field of Artificial Intelligence, offer valuable
opportunities.
&lt;/p&gt;
&lt;p&gt;In this paper, we seek to predict WNV circulation by feeding Deep Neural
Networks (DNNs) with satellite images, which have been extensively shown to
hold environmental and climatic features. Notably, while previous approaches
analyze each geographical site independently, we propose a spatial-aware
approach that considers also the characteristics of close sites. Specifically,
we build upon Graph Neural Networks (GNN) to aggregate features from
neighbouring places, and further extend these modules to consider multiple
relations, such as the difference in temperature and soil moisture between two
sites, as well as the geographical distance. Moreover, we inject time-related
information directly into the model to take into account the seasonality of
virus spread.
&lt;/p&gt;
&lt;p&gt;We design an experimental setting that combines satellite images - from
Landsat and Sentinel missions - with ground truth observations of WNV
circulation in Italy. We show that our proposed Multi-Adjacency Graph Attention
Network (MAGAT) consistently leads to higher performance when paired with an
appropriate pre-training stage. Finally, we assess the importance of each
component of MAGAT in our ablation studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonicelli_L/0/1/0/all/0/1&quot;&gt;Lorenzo Bonicelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Porrello_A/0/1/0/all/0/1&quot;&gt;Angelo Porrello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vincenzi_S/0/1/0/all/0/1&quot;&gt;Stefano Vincenzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ippoliti_C/0/1/0/all/0/1&quot;&gt;Carla Ippoliti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iapaolo_F/0/1/0/all/0/1&quot;&gt;Federica Iapaolo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Conte_A/0/1/0/all/0/1&quot;&gt;Annamaria Conte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calderara_S/0/1/0/all/0/1&quot;&gt;Simone Calderara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.14896">
<title>DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models. (arXiv:2210.14896v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.14896</link>
<description rdf:parseType="Literal">&lt;p&gt;With recent advancements in diffusion models, users can generate high-quality
images by writing text prompts in natural language. However, generating images
with desired details requires proper prompts, and it is often unclear how a
model reacts to different prompts or what the best prompts are. To help
researchers tackle these critical challenges, we introduce DiffusionDB, the
first large-scale text-to-image prompt dataset totaling 6.5TB, containing 14
million images generated by Stable Diffusion, 1.8 million unique prompts, and
hyperparameters specified by real users. We analyze the syntactic and semantic
characteristics of prompts. We pinpoint specific hyperparameter values and
prompt styles that can lead to model errors and present evidence of potentially
harmful model usage, such as the generation of misinformation. The
unprecedented scale and diversity of this human-actuated dataset provide
exciting research opportunities in understanding the interplay between prompts
and generative models, detecting deepfakes, and designing human-AI interaction
tools to help users more easily use these models. DiffusionDB is publicly
available at: https://poloclub.github.io/diffusiondb.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zijie J. Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montoya_E/0/1/0/all/0/1&quot;&gt;Evan Montoya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munechika_D/0/1/0/all/0/1&quot;&gt;David Munechika&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Haoyang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoover_B/0/1/0/all/0/1&quot;&gt;Benjamin Hoover&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1&quot;&gt;Duen Horng Chau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.02499">
<title>A Weakly-Supervised Streaming Multilingual Speech Model with Truly Zero-Shot Capability. (arXiv:2211.02499v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2211.02499</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce our work of building a Streaming Multilingual
Speech Model (SM2), which can transcribe or translate multiple spoken languages
into texts of the target language. The backbone of SM2 is Transformer
Transducer, which has high streaming capability. Instead of human labeled
speech translation (ST) data, SM2 models are trained using weakly supervised
data generated by converting the transcriptions in speech recognition corpora
with a machine translation service. With 351 thousand hours of anonymized
speech training data from 25 languages, SM2 models achieve comparable or even
better ST quality than some recent popular large-scale non-streaming speech
models. More importantly, we show that SM2 has the truly zero-shot capability
when expanding to new target languages, yielding high quality ST results for
{source-speech, target-text} pairs that are not seen during training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1&quot;&gt;Jian Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peidong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jinyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_E/0/1/0/all/0/1&quot;&gt;Eric Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.00443">
<title>Unbiased Heterogeneous Scene Graph Generation with Relation-aware Message Passing Neural Network. (arXiv:2212.00443v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2212.00443</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent scene graph generation (SGG) frameworks have focused on learning
complex relationships among multiple objects in an image. Thanks to the nature
of the message passing neural network (MPNN) that models high-order
interactions between objects and their neighboring objects, they are dominant
representation learning modules for SGG. However, existing MPNN-based
frameworks assume the scene graph as a homogeneous graph, which restricts the
context-awareness of visual relations between objects. That is, they overlook
the fact that the relations tend to be highly dependent on the objects with
which the relations are associated. In this paper, we propose an unbiased
heterogeneous scene graph generation (HetSGG) framework that captures
relation-aware context using message passing neural networks. We devise a novel
message passing layer, called relation-aware message passing neural network
(RMP), that aggregates the contextual information of an image considering the
predicate type between objects. Our extensive evaluations demonstrate that
HetSGG outperforms state-of-the-art methods, especially outperforming on tail
predicate classes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1&quot;&gt;Kanghoon Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kibum Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1&quot;&gt;Jinyoung Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1&quot;&gt;Chanyoung Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.09811">
<title>Memory-efficient NLLB-200: Language-specific Expert Pruning of a Massively Multilingual Machine Translation Model. (arXiv:2212.09811v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2212.09811</link>
<description rdf:parseType="Literal">&lt;p&gt;Compared to conventional bilingual translation systems, massively
multilingual machine translation is appealing because a single model can
translate into multiple languages and benefit from knowledge transfer for low
resource languages. On the other hand, massively multilingual models suffer
from the curse of multilinguality, unless scaling their size massively, which
increases their training and inference costs. Sparse Mixture-of-Experts models
are a way to drastically increase model capacity without the need for a
proportional amount of computing. The recently released NLLB-200 is an example
of such a model. It covers 202 languages but requires at least four 32GB GPUs
just for inference. In this work, we propose a pruning method that allows the
removal of up to 80\% of experts with a negligible loss in translation quality,
which makes it feasible to run the model on a single 32GB GPU. Further analysis
suggests that our pruning metrics allow to identify language-specific experts
and prune non-relevant experts for a given language pair.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koishekenov_Y/0/1/0/all/0/1&quot;&gt;Yeskendir Koishekenov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikoulina_V/0/1/0/all/0/1&quot;&gt;Vassilina Nikoulina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berard_A/0/1/0/all/0/1&quot;&gt;Alexandre Berard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.12313">
<title>Adapting Neural Link Predictors for Complex Query Answering. (arXiv:2301.12313v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.12313</link>
<description rdf:parseType="Literal">&lt;p&gt;Answering complex queries on incomplete knowledge graphs is a challenging
task where a model needs to answer complex logical queries in the presence of
missing knowledge. Recently, Arakelyan et al. (2021); Minervini et al. (2022)
showed that neural link predictors could also be used for answering complex
queries: their Continuous Query Decomposition (CQD) method works by decomposing
complex queries into atomic sub-queries, answers them using neural link
predictors and aggregates their scores via t-norms for ranking the answers to
each complex query. However, CQD does not handle negations and only uses the
training signal from atomic training queries: neural link prediction scores are
not calibrated to interact together via fuzzy logic t-norms during complex
query answering. In this work, we propose to address this problem by training a
parameter-efficient score adaptation model to re-calibrate neural link
prediction scores: this new component is trained on complex queries by
back-propagating through the complex query-answering process. Our method,
CQD$^{A}$, produces significantly more accurate results than current
state-of-the-art methods, improving from $34.4$ to $35.1$ Mean Reciprocal Rank
values averaged across all datasets and query types while using $\leq 35\%$ of
the available training query types. We further show that CQD$^{A}$ is
data-efficient, achieving competitive results with only $1\%$ of the training
data, and robust in out-of-domain evaluations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arakelyan_E/0/1/0/all/0/1&quot;&gt;Erik Arakelyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1&quot;&gt;Pasquale Minervini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1&quot;&gt;Isabelle Augenstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.13166">
<title>ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation. (arXiv:2301.13166v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2301.13166</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to accurately locate and navigate to a specific object is a
crucial capability for embodied agents that operate in the real world and
interact with objects to complete tasks. Such object navigation tasks usually
require large-scale training in visual environments with labeled objects, which
generalizes poorly to novel objects in unknown environments. In this work, we
present a novel zero-shot object navigation method, Exploration with Soft
Commonsense constraints (ESC), that transfers commonsense knowledge in
pre-trained models to open-world object navigation without any navigation
experience nor any other training on the visual environments. First, ESC
leverages a pre-trained vision and language model for open-world prompt-based
grounding and a pre-trained commonsense language model for room and object
reasoning. Then ESC converts commonsense knowledge into navigation actions by
modeling it as soft logic predicates for efficient exploration. Extensive
experiments on MP3D, HM3D, and RoboTHOR benchmarks show that our ESC method
improves significantly over baselines, and achieves new state-of-the-art
results for zero-shot object navigation (e.g., 288% relative Success Rate
improvement than CoW on MP3D).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1&quot;&gt;Kaiwen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1&quot;&gt;Kaizhi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pryor_C/0/1/0/all/0/1&quot;&gt;Connor Pryor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yilin Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1&quot;&gt;Hongxia Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Getoor_L/0/1/0/all/0/1&quot;&gt;Lise Getoor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Eric Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.05449">
<title>Heckerthoughts. (arXiv:2302.05449v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2302.05449</link>
<description rdf:parseType="Literal">&lt;p&gt;This manuscript is technical memoir about my work at Stanford and Microsoft
Research. Included are fundamental concepts central to machine learning and
artificial intelligence, applications of these concepts, and stories behind
their creation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heckerman_D/0/1/0/all/0/1&quot;&gt;David Heckerman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.07729">
<title>Generation of Highlights from Research Papers Using Pointer-Generator Networks and SciBERT Embeddings. (arXiv:2302.07729v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2302.07729</link>
<description rdf:parseType="Literal">&lt;p&gt;Nowadays many research articles are prefaced with research highlights to
summarize the main findings of the paper. Highlights not only help researchers
precisely and quickly identify the contributions of a paper, they also enhance
the discoverability of the article via search engines. We aim to automatically
construct research highlights given certain segments of a research paper. We
use a pointer-generator network with coverage mechanism and a contextual
embedding layer at the input that encodes the input tokens into SciBERT
embeddings. We test our model on a benchmark dataset, CSPubSum, and also
present MixSub, a new multi-disciplinary corpus of papers for automatic
research highlight generation. For both CSPubSum and MixSub, we have observed
that the proposed model achieves the best performance compared to related
variants and other models proposed in the literature. On the CSPubSum dataset,
our model achieves the best performance when the input is only the abstract of
a paper as opposed to other segments of the paper. It produces ROUGE-1, ROUGE-2
and ROUGE-L F1-scores of 38.26, 14.26 and 35.51, respectively, METEOR score of
32.62, and BERTScore F1 of 86.65 which outperform all other baselines. On the
new MixSub dataset, where only the abstract is the input, our proposed model
(when trained on the whole training corpus without distinguishing between the
subject categories) achieves ROUGE-1, ROUGE-2 and ROUGE-L F1-scores of 31.78,
9.76 and 29.3, respectively, METEOR score of 24.00, and BERTScore F1 of 85.25.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rehman_T/0/1/0/all/0/1&quot;&gt;Tohida Rehman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanyal_D/0/1/0/all/0/1&quot;&gt;Debarshi Kumar Sanyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chattopadhyay_S/0/1/0/all/0/1&quot;&gt;Samiran Chattopadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhowmick_P/0/1/0/all/0/1&quot;&gt;Plaban Kumar Bhowmick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1&quot;&gt;Partha Pratim Das&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.09844">
<title>FederatedTrust: A Solution for Trustworthy Federated Learning. (arXiv:2302.09844v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2302.09844</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid expansion of the Internet of Things (IoT) and Edge Computing has
presented challenges for centralized Machine and Deep Learning (ML/DL) methods
due to the presence of distributed data silos that hold sensitive information.
To address concerns regarding data privacy, collaborative and
privacy-preserving ML/DL techniques like Federated Learning (FL) have emerged.
However, ensuring data privacy and performance alone is insufficient since
there is a growing need to establish trust in model predictions. Existing
literature has proposed various approaches on trustworthy ML/DL (excluding data
privacy), identifying robustness, fairness, explainability, and accountability
as important pillars. Nevertheless, further research is required to identify
trustworthiness pillars and evaluation metrics specifically relevant to FL
models, as well as to develop solutions that can compute the trustworthiness
level of FL models. This work examines the existing requirements for evaluating
trustworthiness in FL and introduces a comprehensive taxonomy consisting of six
pillars (privacy, robustness, fairness, explainability, accountability, and
federation), along with over 30 metrics for computing the trustworthiness of FL
models. Subsequently, an algorithm named FederatedTrust is designed based on
the pillars and metrics identified in the taxonomy to compute the
trustworthiness score of FL models. A prototype of FederatedTrust is
implemented and integrated into the learning process of FederatedScope, a
well-established FL framework. Finally, five experiments are conducted using
different configurations of FederatedScope to demonstrate the utility of
FederatedTrust in computing the trustworthiness of FL models. Three experiments
employ the FEMNIST dataset, and two utilize the N-BaIoT dataset considering a
real-world IoT security use case.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanchez_P/0/1/0/all/0/1&quot;&gt;Pedro Miguel S&amp;#xe1;nchez S&amp;#xe1;nchez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Celdran_A/0/1/0/all/0/1&quot;&gt;Alberto Huertas Celdr&amp;#xe1;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_N/0/1/0/all/0/1&quot;&gt;Ning Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bovet_G/0/1/0/all/0/1&quot;&gt;G&amp;#xe9;r&amp;#xf4;me Bovet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_G/0/1/0/all/0/1&quot;&gt;Gregorio Mart&amp;#xed;nez P&amp;#xe9;rez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stiller_B/0/1/0/all/0/1&quot;&gt;Burkhard Stiller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.02216">
<title>Denoise Pretraining on Nonequilibrium Molecules for Accurate and Transferable Neural Potentials. (arXiv:2303.02216v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.02216</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in equivariant graph neural networks (GNNs) have made deep
learning amenable to developing fast surrogate models to expensive ab initio
quantum mechanics (QM) approaches for molecular potential predictions. However,
building accurate and transferable potential models using GNNs remains
challenging, as the data is greatly limited by the expensive computational
costs and level of theory of QM methods, especially for large and complex
molecular systems. In this work, we propose denoise pretraining on
nonequilibrium molecular conformations to achieve more accurate and
transferable GNN potential predictions. Specifically, atomic coordinates of
sampled nonequilibrium conformations are perturbed by random noises and GNNs
are pretrained to denoise the perturbed molecular conformations which recovers
the original coordinates. Rigorous experiments on multiple benchmarks reveal
that pretraining significantly improves the accuracy of neural potentials.
Furthermore, we show that the proposed pretraining approach is model-agnostic,
as it improves the performance of different invariant and equivariant GNNs.
Notably, our models pretrained on small molecules demonstrate remarkable
transferability, improving performance when fine-tuned on diverse molecular
systems, including different elements, charged molecules, biomolecules, and
larger systems. These results highlight the potential for leveraging denoise
pretraining approaches to build more generalizable neural potentials for
complex molecular systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Changwen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zijie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farimani_A/0/1/0/all/0/1&quot;&gt;Amir Barati Farimani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.03789">
<title>Fast and Multi-aspect Mining of Complex Time-stamped Event Streams. (arXiv:2303.03789v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.03789</link>
<description rdf:parseType="Literal">&lt;p&gt;Given a huge, online stream of time-evolving events with multiple attributes,
such as online shopping logs: (item, price, brand, time), and local mobility
activities: (pick-up and drop-off locations, time), how can we summarize large,
dynamic high-order tensor streams? How can we see any hidden patterns, rules,
and anomalies? Our answer is to focus on two types of patterns, i.e.,
&apos;&apos;regimes&apos;&apos; and &apos;&apos;components&apos;&apos;, for which we present CubeScope, an efficient
and effective method over high-order tensor streams. Specifically, it
identifies any sudden discontinuity and recognizes distinct dynamical patterns,
&apos;&apos;regimes&apos;&apos; (e.g., weekday/weekend/holiday patterns). In each regime, it also
performs multi-way summarization for all attributes (e.g., item, price, brand,
and time) and discovers hidden &apos;&apos;components&apos;&apos; representing latent groups (e.g.,
item/brand groups) and their relationship. Thanks to its concise but effective
summarization, CubeScope can also detect the sudden appearance of anomalies and
identify the types of anomalies that occur in practice. Our proposed method has
the following properties: (a) Effective: it captures dynamical multi-aspect
patterns, i.e., regimes and components, and statistically summarizes all the
events; (b) General: it is practical for successful application to data
compression, pattern discovery, and anomaly detection on various types of
tensor streams; (c) Scalable: our algorithm does not depend on the length of
the data stream and its dimensionality. Extensive experiments on real datasets
demonstrate that CubeScope finds meaningful patterns and anomalies correctly,
and consistently outperforms the state-of-the-art methods as regards accuracy
and execution speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakamura_K/0/1/0/all/0/1&quot;&gt;Kota Nakamura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matsubara_Y/0/1/0/all/0/1&quot;&gt;Yasuko Matsubara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawabata_K/0/1/0/all/0/1&quot;&gt;Koki Kawabata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Umeda_Y/0/1/0/all/0/1&quot;&gt;Yuhei Umeda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wada_Y/0/1/0/all/0/1&quot;&gt;Yuichiro Wada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sakurai_Y/0/1/0/all/0/1&quot;&gt;Yasushi Sakurai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.09378">
<title>Capturing Emerging Complexity in Lenia. (arXiv:2305.09378v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2305.09378</link>
<description rdf:parseType="Literal">&lt;p&gt;This research project investigates Lenia, an artificial life platform that
simulates ecosystems of digital creatures. Lenia&apos;s ecosystem consists of
simple, artificial organisms that can move, consume, grow, and reproduce. The
platform is important as a tool for studying artificial life and evolution, as
it provides a scalable and flexible environment for creating a diverse range of
organisms with varying abilities and behaviors. Measuring complexity in Lenia
is a key aspect of the study, which identifies the metrics for measuring
long-term complex emerging behavior of rules, with the aim of evolving better
Lenia behaviors which are yet not discovered. The Genetic Algorithm uses
neighborhoods or kernels as genotype while keeping the rest of the parameters
of Lenia as fixed, for example growth function, to produce different behaviors
respective to the population and then measures fitness value to decide the
complexity of the resulting behavior. First, we use Variation over Time as a
fitness function where higher variance between the frames are rewarded. Second,
we use Auto-encoder based fitness where variation of the list of reconstruction
loss for the frames is rewarded. Third, we perform combined fitness where
higher variation of the pixel density of reconstructed frames is rewarded. All
three experiments are tweaked with pixel alive threshold and frames used.
Finally, after performing nine experiments of each fitness for 500 generations,
we pick configurations from all experiments such that there is a scope of
further evolution, and run it for 2500 generations. Results show that the
kernel&apos;s center of mass increases with a specific set of pixels and together
with borders the kernel try to achieve a Gaussian distribution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1&quot;&gt;Sanyam Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shrestha_A/0/1/0/all/0/1&quot;&gt;Aarati Shrestha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nichele_S/0/1/0/all/0/1&quot;&gt;Stefano Nichele&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18486">
<title>A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets. (arXiv:2305.18486v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18486</link>
<description rdf:parseType="Literal">&lt;p&gt;The development of large language models (LLMs) such as ChatGPT has brought a
lot of attention recently. However, their evaluation in the benchmark academic
datasets remains under-explored due to the difficulty of evaluating the
generative outputs produced by this model against the ground truth. In this
paper, we aim to present a thorough evaluation of ChatGPT&apos;s performance on
diverse academic datasets, covering tasks like question-answering, text
summarization, code generation, commonsense reasoning, mathematical
problem-solving, machine translation, bias detection, and ethical
considerations. Specifically, we evaluate ChatGPT across 140 tasks and analyze
255K responses it generates in these datasets. This makes our work the largest
evaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate
the strengths and weaknesses of ChatGPT in various tasks and provide insights
for future research using LLMs. We also report a new emergent ability to follow
multi-query instructions that we mostly found in ChatGPT and other
instruction-tuned models. Our extensive evaluation shows that even though
ChatGPT is capable of performing a wide variety of tasks, and may obtain
impressive performance in several benchmark datasets, it is still far from
achieving the ability to reliably solve many challenging tasks. By providing a
thorough assessment of ChatGPT&apos;s performance across diverse NLP tasks, this
paper sets the stage for a targeted deployment of ChatGPT-like LLMs in
real-world applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laskar_M/0/1/0/all/0/1&quot;&gt;Md Tahmid Rahman Laskar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bari_M/0/1/0/all/0/1&quot;&gt;M Saiful Bari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1&quot;&gt;Mizanur Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhuiyan_M/0/1/0/all/0/1&quot;&gt;Md Amran Hossen Bhuiyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1&quot;&gt;Shafiq Joty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jimmy Xiangji Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.03263">
<title>Efficient automatic design of robots. (arXiv:2306.03263v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2306.03263</link>
<description rdf:parseType="Literal">&lt;p&gt;Robots are notoriously difficult to design because of complex
interdependencies between their physical structure, sensory and motor layouts,
and behavior. Despite this, almost every detail of every robot built to date
has been manually determined by a human designer after several months or years
of iterative ideation, prototyping, and testing. Inspired by evolutionary
design in nature, the automated design of robots using evolutionary algorithms
has been attempted for two decades, but it too remains inefficient: days of
supercomputing are required to design robots in simulation that, when
manufactured, exhibit desired behavior. Here we show for the first time de-novo
optimization of a robot&apos;s structure to exhibit a desired behavior, within
seconds on a single consumer-grade computer, and the manufactured robot&apos;s
retention of that behavior. Unlike other gradient-based robot design methods,
this algorithm does not presuppose any particular anatomical form; starting
instead from a randomly-generated apodous body plan, it consistently discovers
legged locomotion, the most efficient known form of terrestrial movement. If
combined with automated fabrication and scaled up to more challenging tasks,
this advance promises near instantaneous design, manufacture, and deployment of
unique and useful machines for medical, environmental, vehicular, and
space-based tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matthews_D/0/1/0/all/0/1&quot;&gt;David Matthews&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spielberg_A/0/1/0/all/0/1&quot;&gt;Andrew Spielberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1&quot;&gt;Daniela Rus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kriegman_S/0/1/0/all/0/1&quot;&gt;Sam Kriegman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bongard_J/0/1/0/all/0/1&quot;&gt;Josh Bongard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04220">
<title>Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL. (arXiv:2306.04220v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04220</link>
<description rdf:parseType="Literal">&lt;p&gt;Offline reinforcement learning (RL) offers an appealing approach to
real-world tasks by learning policies from pre-collected datasets without
interacting with the environment. However, the performance of existing offline
RL algorithms heavily depends on the scale and state-action space coverage of
datasets. Real-world data collection is often expensive and uncontrollable,
leading to small and narrowly covered datasets and posing significant
challenges for practical deployments of offline RL. In this paper, we provide a
new insight that leveraging the fundamental symmetry of system dynamics can
substantially enhance offline RL performance under small datasets.
Specifically, we propose a Time-reversal symmetry (T-symmetry) enforced
Dynamics Model (TDM), which establishes consistency between a pair of forward
and reverse latent dynamics. TDM provides both well-behaved representations for
small datasets and a new reliability measure for OOD samples based on
compliance with the T-symmetry. These can be readily used to construct a new
offline RL algorithm (TSRL) with less conservative policy constraints and a
reliable latent space data augmentation procedure. Based on extensive
experiments, we find TSRL achieves great performance on small benchmark
datasets with as few as 1% of the original samples, which significantly
outperforms the recent offline RL algorithms in terms of data efficiency and
generalizability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1&quot;&gt;Peng Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1&quot;&gt;Xianyuan Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhihao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenjia Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Shoucheng Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Han Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Youfang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1&quot;&gt;Li Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04637">
<title>Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection. (arXiv:2306.04637v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04637</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural sequence models based on the transformer architecture have
demonstrated remarkable \emph{in-context learning} (ICL) abilities, where they
can perform new tasks when prompted with training and test examples, without
any parameter update to the model. This work first provides a comprehensive
statistical theory for transformers to perform ICL. Concretely, we show that
transformers can implement a broad class of standard machine learning
algorithms in context, such as least squares, ridge regression, Lasso, learning
generalized linear models, and gradient descent on two-layer neural networks,
with near-optimal predictive power on various in-context data distributions.
Using an efficient implementation of in-context gradient descent as the
underlying mechanism, our transformer constructions admit mild size bounds, and
can be learned with polynomially many pretraining sequences.
&lt;/p&gt;
&lt;p&gt;Building on these ``base&apos;&apos; ICL algorithms, intriguingly, we show that
transformers can implement more complex ICL procedures involving
\emph{in-context algorithm selection}, akin to what a statistician can do in
real life -- A \emph{single} transformer can adaptively select different base
ICL algorithms -- or even perform qualitatively different tasks -- on different
input sequences, without any explicit prompting of the right algorithm or task.
We both establish this in theory by explicit constructions, and also observe
this phenomenon experimentally. In theory, we construct two general mechanisms
for algorithm selection with concrete examples: pre-ICL testing, and post-ICL
validation. As an example, we use the post-ICL validation mechanism to
construct a transformer that can perform nearly Bayes-optimal ICL on a
challenging task -- noisy linear models with mixed noise levels.
Experimentally, we demonstrate the strong in-context algorithm selection
capabilities of standard transformer architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yu Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1&quot;&gt;Fan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Huan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1&quot;&gt;Caiming Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_S/0/1/0/all/0/1&quot;&gt;Song Mei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10077">
<title>Stacking of Hyperparameter Tuned Models for Tagging Coding Problems. (arXiv:2306.10077v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10077</link>
<description rdf:parseType="Literal">&lt;p&gt;Coding problems are problems that require a solution in the form of a
computer program. Coding problems are popular among students and professionals
as it enhances their skills and career opportunities. An AI system that would
help those who practice coding problems would be highly useful and there is a
huge potential for such a system. In this work, we propose a model which uses
stacking of hyperparameter tuned boosting models to achieve impressive metric
scores of 77.8% accuracy and 0.815 PR-AUC on the dataset that was scraped from
Codeforces and Leetcode. We open source the dataset and the models developed
for this work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+TS_S/0/1/0/all/0/1&quot;&gt;Sathya Krishnan TS&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandian_S/0/1/0/all/0/1&quot;&gt;S. Lakshmana Pandian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shunmugapriya_P/0/1/0/all/0/1&quot;&gt;P. Shunmugapriya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10259">
<title>Active Policy Improvement from Multiple Black-box Oracles. (arXiv:2306.10259v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10259</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning (RL) has made significant strides in various complex
domains. However, identifying an effective policy via RL often necessitates
extensive exploration. Imitation learning aims to mitigate this issue by using
expert demonstrations to guide exploration. In real-world scenarios, one often
has access to multiple suboptimal black-box experts, rather than a single
optimal oracle. These experts do not universally outperform each other across
all states, presenting a challenge in actively deciding which oracle to use and
in which state. We introduce MAPS and MAPS-SE, a class of policy improvement
algorithms that perform imitation learning from multiple suboptimal oracles. In
particular, MAPS actively selects which of the oracles to imitate and improve
their value function estimates, and MAPS-SE additionally leverages an active
state exploration criterion to determine which states one should explore. We
provide a comprehensive theoretical analysis and demonstrate that MAPS and
MAPS-SE enjoy sample efficiency advantage over the state-of-the-art policy
improvement algorithms. Empirical results show that MAPS-SE significantly
accelerates policy optimization via state-wise imitation learning from multiple
oracles across a broad spectrum of control tasks in the DeepMind Control Suite.
Our code is publicly available at: https://github.com/ripl/maps.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xuefeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoneda_T/0/1/0/all/0/1&quot;&gt;Takuma Yoneda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chaoqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walter_M/0/1/0/all/0/1&quot;&gt;Matthew R. Walter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuxin Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14096">
<title>Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models. (arXiv:2306.14096v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14096</link>
<description rdf:parseType="Literal">&lt;p&gt;Entity-level fine-grained sentiment analysis in the financial domain is a
crucial subtask of sentiment analysis and currently faces numerous challenges.
The primary challenge stems from the lack of high-quality and large-scale
annotated corpora specifically designed for financial text sentiment analysis,
which in turn limits the availability of data necessary for developing
effective text processing techniques. Recent advancements in large language
models (LLMs) have yielded remarkable performance in natural language
processing tasks, primarily centered around language pattern matching. In this
paper, we propose a novel and extensive Chinese fine-grained financial
sentiment analysis dataset, FinChina SA, for enterprise early warning. We
thoroughly evaluate and experiment with well-known existing open-source LLMs
using our dataset. We firmly believe that our dataset will serve as a valuable
resource to advance the exploration of real-world financial sentiment analysis
tasks, which should be the focus of future research. Our dataset and all code
to replicate the experimental results will be released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1&quot;&gt;Yinyu Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yanru Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Wang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1&quot;&gt;Weiqiang Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Youhao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15782">
<title>UTRNet: High-Resolution Urdu Text Recognition In Printed Documents. (arXiv:2306.15782v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15782</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel approach to address the challenges of
printed Urdu text recognition using high-resolution, multi-scale semantic
feature extraction. Our proposed UTRNet architecture, a hybrid CNN-RNN model,
demonstrates state-of-the-art performance on benchmark datasets. To address the
limitations of previous works, which struggle to generalize to the intricacies
of the Urdu script and the lack of sufficient annotated real-world data, we
have introduced the UTRSet-Real, a large-scale annotated real-world dataset
comprising over 11,000 lines and UTRSet-Synth, a synthetic dataset with 20,000
lines closely resembling real-world and made corrections to the ground truth of
the existing IIITH dataset, making it a more reliable resource for future
research. We also provide UrduDoc, a benchmark dataset for Urdu text line
detection in scanned documents. Additionally, we have developed an online tool
for end-to-end Urdu OCR from printed documents by integrating UTRNet with a
text detection model. Our work not only addresses the current limitations of
Urdu OCR but also paves the way for future research in this area and
facilitates the continued advancement of Urdu OCR technology. The project page
with source code, datasets, annotations, trained models, and online tool is
available at abdur75648.github.io/UTRNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_A/0/1/0/all/0/1&quot;&gt;Abdur Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1&quot;&gt;Arjun Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_C/0/1/0/all/0/1&quot;&gt;Chetan Arora&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15880">
<title>Towards Open Vocabulary Learning: A Survey. (arXiv:2306.15880v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15880</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of visual scene understanding, deep neural networks have made
impressive advancements in various core tasks like segmentation, tracking, and
detection. However, most approaches operate on the close-set assumption,
meaning that the model can only identify pre-defined categories that are
present in the training set. Recently, open vocabulary settings were proposed
due to the rapid progress of vision language pre-training. These new approaches
seek to locate and recognize categories beyond the annotated label space. The
open vocabulary approach is more general, practical, and effective compared to
weakly supervised and zero-shot settings. This paper provides a thorough review
of open vocabulary learning, summarizing and analyzing recent developments in
the field. In particular, we begin by comparing it to related concepts such as
zero-shot learning, open-set recognition, and out-of-distribution detection.
Then, we review several closely related tasks in the case of segmentation and
detection, including long-tail problems, few-shot, and zero-shot settings. For
the method survey, we first present the basic knowledge of detection and
segmentation in close-set as the preliminary knowledge. Next, we examine
various scenarios in which open vocabulary learning is used, identifying common
design elements and core ideas. Then, we compare the recent detection and
segmentation approaches in commonly used datasets and benchmarks. Finally, we
conclude with insights, issues, and discussions regarding future research
directions. To our knowledge, this is the first comprehensive literature review
of open vocabulary learning. We keep tracing related works at
https://github.com/jianzongwu/Awesome-Open-Vocabulary.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jianzong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiangtai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shilin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1&quot;&gt;Haobo Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Henghui Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yibo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiangning Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1&quot;&gt;Yunhai Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xudong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1&quot;&gt;Bernard Ghanem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00209">
<title>Image Matters: A New Dataset and Empirical Study for Multimodal Hyperbole Detection. (arXiv:2307.00209v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00209</link>
<description rdf:parseType="Literal">&lt;p&gt;Hyperbole, or exaggeration, is a common linguistic phenomenon. The detection
of hyperbole is an important part of understanding human expression. There have
been several studies on hyperbole detection, but most of which focus on text
modality only. However, with the development of social media, people can create
hyperbolic expressions with various modalities, including text, images, videos,
etc. In this paper, we focus on multimodal hyperbole detection. We create a
multimodal detection dataset\footnote{The dataset will be released to the
community.} from Weibo (a Chinese social media) and carry out some studies on
it. We treat the text and image from a piece of weibo as two modalities and
explore the role of text and image for hyperbole detection. Different
pre-trained multimodal encoders are also evaluated on this downstream task to
show their performance. Besides, since this dataset is constructed from five
different topics, we also evaluate the cross-domain performance of different
models. These studies can serve as a benchmark and point out the direction of
further study on multimodal hyperbole detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huixuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1&quot;&gt;Xiaojun Wan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00968">
<title>REAL: A Representative Error-Driven Approach for Active Learning. (arXiv:2307.00968v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00968</link>
<description rdf:parseType="Literal">&lt;p&gt;Given a limited labeling budget, active learning (AL) aims to sample the most
informative instances from an unlabeled pool to acquire labels for subsequent
model training. To achieve this, AL typically measures the informativeness of
unlabeled instances based on uncertainty and diversity. However, it does not
consider erroneous instances with their neighborhood error density, which have
great potential to improve the model performance. To address this limitation,
we propose $REAL$, a novel approach to select data instances with
$\underline{R}$epresentative $\underline{E}$rrors for $\underline{A}$ctive
$\underline{L}$earning. It identifies minority predictions as \emph{pseudo
errors} within a cluster and allocates an adaptive sampling budget for the
cluster based on estimated error density. Extensive experiments on five text
classification datasets demonstrate that $REAL$ consistently outperforms all
best-performing baselines regarding accuracy and F1-macro scores across a wide
range of hyperparameter settings. Our analysis also shows that $REAL$ selects
the most representative pseudo errors that match the distribution of
ground-truth errors along the decision boundary. Our code is publicly available
at https://github.com/withchencheng/ECML_PKDD_23_Real.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Cheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1&quot;&gt;Lizi Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yueguo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1&quot;&gt;Xiaoyong Du&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01301">
<title>Reliable AI: Does the Next Generation Require Quantum Computing?. (arXiv:2307.01301v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01301</link>
<description rdf:parseType="Literal">&lt;p&gt;In this survey, we aim to explore the fundamental question of whether the
next generation of artificial intelligence requires quantum computing.
Artificial intelligence is increasingly playing a crucial role in many aspects
of our daily lives and is central to the fourth industrial revolution. It is
therefore imperative that artificial intelligence is reliable and trustworthy.
However, there are still many issues with reliability of artificial
intelligence, such as privacy, responsibility, safety, and security, in areas
such as autonomous driving, healthcare, robotics, and others. These problems
can have various causes, including insufficient data, biases, and robustness
problems, as well as fundamental issues such as computability problems on
digital hardware. The cause of these computability problems is rooted in the
fact that digital hardware is based on the computing model of the Turing
machine, which is inherently discrete. Notably, our findings demonstrate that
digital hardware is inherently constrained in solving problems about
optimization, deep learning, or differential equations. Therefore, these
limitations carry substantial implications for the field of artificial
intelligence, in particular for machine learning. Furthermore, although it is
well known that the quantum computer shows a quantum advantage for certain
classes of problems, our findings establish that some of these limitations
persist when employing quantum computing models based on the quantum circuit or
the quantum Turing machine paradigm. In contrast, analog computing models, such
as the Blum-Shub-Smale machine, exhibit the potential to surmount these
limitations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bacho_A/0/1/0/all/0/1&quot;&gt;Aras Bacho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boche_H/0/1/0/all/0/1&quot;&gt;Holger Boche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kutyniok_G/0/1/0/all/0/1&quot;&gt;Gitta Kutyniok&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02472">
<title>Deductive Additivity for Planning of Natural Language Proofs. (arXiv:2307.02472v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02472</link>
<description rdf:parseType="Literal">&lt;p&gt;Current natural language systems designed for multi-step claim validation
typically operate in two phases: retrieve a set of relevant premise statements
using heuristics (planning), then generate novel conclusions from those
statements using a large language model (deduction). The planning step often
requires expensive Transformer operations and does not scale to arbitrary
numbers of premise statements. In this paper, we investigate whether an
efficient planning heuristic is possible via embedding spaces compatible with
deductive reasoning. Specifically, we evaluate whether embedding spaces exhibit
a property we call deductive additivity: the sum of premise statement
embeddings should be close to embeddings of conclusions based on those
premises. We explore multiple sources of off-the-shelf dense embeddings in
addition to fine-tuned embeddings from GPT3 and sparse embeddings from BM25. We
study embedding models both intrinsically, evaluating whether the property of
deductive additivity holds, and extrinsically, using them to assist planning in
natural language proof generation. Lastly, we create a dataset, Single-Step
Reasoning Contrast (SSRC), to further probe performance on various reasoning
types. Our findings suggest that while standard embedding methods frequently
embed conclusions near the sums of their premises, they fall short of being
effective heuristics and lack the ability to model certain categories of
reasoning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sprague_Z/0/1/0/all/0/1&quot;&gt;Zayne Sprague&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bostrom_K/0/1/0/all/0/1&quot;&gt;Kaj Bostrom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1&quot;&gt;Swarat Chaudhuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1&quot;&gt;Greg Durrett&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01848">
<title>Embodied Task Planning with Large Language Models. (arXiv:2307.01848v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2307.01848</link>
<description rdf:parseType="Literal">&lt;p&gt;Equipping embodied agents with commonsense is important for robots to
successfully complete complex human instructions in general environments.
Recent large language models (LLM) can embed rich semantic knowledge for agents
in plan generation of complex tasks, while they lack the information about the
realistic world and usually yield infeasible action sequences. In this paper,
we propose a TAsk Planing Agent (TaPA) in embodied tasks for grounded planning
with physical scene constraint, where the agent generates executable plans
according to the existed objects in the scene by aligning LLMs with the visual
perception models. Specifically, we first construct a multimodal dataset
containing triplets of indoor scenes, instructions and action plans, where we
provide the designed prompts and the list of existing objects in the scene for
GPT-3.5 to generate a large number of instructions and corresponding planned
actions. The generated data is leveraged for grounded plan tuning of
pre-trained LLMs. During inference, we discover the objects in the scene by
extending open-vocabulary object detectors to multi-view RGB images collected
in different achievable locations. Experimental results show that the generated
plan from our TaPA framework can achieve higher success rate than LLaVA and
GPT-3.5 by a sizable margin, which indicates the practicality of embodied task
planning in general and complex environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiuwei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiwen Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1&quot;&gt;Haibin Yan&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>