<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2024-01-10T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04739" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04740" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04746" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04747" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04748" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04750" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04791" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04801" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04860" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04861" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04872" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04903" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04921" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04923" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04942" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04956" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04961" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04962" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04975" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04984" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04988" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05010" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05011" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05014" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05018" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05049" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05055" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05093" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05126" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05137" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05153" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05157" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05159" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05163" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05166" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05167" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05168" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05202" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05217" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05224" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05232" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05236" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05252" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05293" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05294" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05308" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05314" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05334" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05335" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05336" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.11018" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.14865" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.05927" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.07802" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.10193" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.14839" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.05438" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.05158" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.08455" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.11510" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.03301" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.06783" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.09171" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.16666" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.12707" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.13014" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05400" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14955" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16713" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16963" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19638" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09996" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10830" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11565" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02129" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03992" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10763" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00473" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.00727" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03851" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.09643" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.09737" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02567" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03431" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08106" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10224" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13925" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16163" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00699" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10324" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12102" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13104" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16414" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16805" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00729" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00926" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01227" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01375" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02162" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02384" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03836" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03922" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04092" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04716" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06067" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2401.04739">
<title>Content-Conditioned Generation of Stylized Free hand Sketches. (arXiv:2401.04739v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04739</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the recognition of free-hand sketches has remained a popular
task. However, in some special fields such as the military field, free-hand
sketches are difficult to sample on a large scale. Common data augmentation and
image generation techniques are difficult to produce images with various
free-hand sketching styles. Therefore, the recognition and segmentation tasks
in related fields are limited. In this paper, we propose a novel adversarial
generative network that can accurately generate realistic free-hand sketches
with various styles. We explore the performance of the model, including using
styles randomly sampled from a prior normal distribution to generate images
with various free-hand sketching styles, disentangling the painters&apos; styles
from known free-hand sketches to generate images with specific styles, and
generating images of unknown classes that are not in the training set. We
further demonstrate with qualitative and quantitative evaluations our
advantages in visual quality, content accuracy, and style imitation on
SketchIME.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiajun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Siyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1&quot;&gt;Guangming Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Liang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1&quot;&gt;Ning Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_E/0/1/0/all/0/1&quot;&gt;Eryang Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04740">
<title>Segment anything model (SAM) for brain extraction in fMRI studies. (arXiv:2401.04740v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.04740</link>
<description rdf:parseType="Literal">&lt;p&gt;Brain extraction and removal of skull artifacts from magnetic resonance
images (MRI) is an important preprocessing step in neuroimaging analysis. There
are many tools developed to handle human fMRI images, which could involve
manual steps for verifying results from brain segmentation that makes it time
consuming and inefficient. In this study, we will use the segment anything
model (SAM), a freely available neural network released by Meta[4], which has
shown promising results in many generic segmentation applications. We will
analyze the efficiency of SAM for neuroimaging brain segmentation by removing
skull artifacts. The results of the experiments showed promising results that
explore using automated segmentation algorithms for neuroimaging without the
need to train on custom medical imaging dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chenna_D/0/1/0/all/0/1&quot;&gt;Dwith Chenna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bhogawar_S/0/1/0/all/0/1&quot;&gt;Suyash Bhogawar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04746">
<title>Skin Cancer Segmentation and Classification Using Vision Transformer for Automatic Analysis in Dermatoscopy-based Non-invasive Digital System. (arXiv:2401.04746v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.04746</link>
<description rdf:parseType="Literal">&lt;p&gt;Skin cancer is a global health concern, necessitating early and accurate
diagnosis for improved patient outcomes. This study introduces a groundbreaking
approach to skin cancer classification, employing the Vision Transformer, a
state-of-the-art deep learning architecture renowned for its success in diverse
image analysis tasks. Utilizing the HAM10000 dataset of 10,015 meticulously
annotated skin lesion images, the model undergoes preprocessing for enhanced
robustness. The Vision Transformer, adapted to the skin cancer classification
task, leverages the self-attention mechanism to capture intricate spatial
dependencies, achieving superior performance over traditional deep learning
architectures. Segment Anything Model aids in precise segmentation of cancerous
areas, attaining high IOU and Dice Coefficient. Extensive experiments highlight
the model&apos;s supremacy, particularly the Google-based ViT patch-32 variant,
which achieves 96.15% accuracy and showcases potential as an effective tool for
dermatologists in skin cancer diagnosis, contributing to advancements in
dermatological practices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Himel_G/0/1/0/all/0/1&quot;&gt;Galib Muhammad Shahriar Himel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Islam_M/0/1/0/all/0/1&quot;&gt;Md. Masudul Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Al_Aff_K/0/1/0/all/0/1&quot;&gt;Kh Abdullah Al-Aff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Karim_S/0/1/0/all/0/1&quot;&gt;Shams Ibne Karim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sikder_M/0/1/0/all/0/1&quot;&gt;Md. Kabir Uddin Sikder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04747">
<title>DiffSHEG: A Diffusion-Based Approach for Real-Time Speech-driven Holistic 3D Expression and Gesture Generation. (arXiv:2401.04747v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2401.04747</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose DiffSHEG, a Diffusion-based approach for Speech-driven Holistic 3D
Expression and Gesture generation with arbitrary length. While previous works
focused on co-speech gesture or expression generation individually, the joint
generation of synchronized expressions and gestures remains barely explored. To
address this, our diffusion-based co-speech motion generation transformer
enables uni-directional information flow from expression to gesture,
facilitating improved matching of joint expression-gesture distributions.
Furthermore, we introduce an outpainting-based sampling strategy for arbitrary
long sequence generation in diffusion models, offering flexibility and
computational efficiency. Our method provides a practical solution that
produces high-quality synchronized expression and gesture generation driven by
speech. Evaluated on two public datasets, our approach achieves
state-of-the-art performance both quantitatively and qualitatively.
Additionally, a user study confirms the superiority of DiffSHEG over prior
approaches. By enabling the real-time generation of expressive and synchronized
motions, DiffSHEG showcases its potential for various applications in the
development of digital humans and embodied agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yunfei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1&quot;&gt;Ailing Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qifeng Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04748">
<title>Convolutional Neural Network Ensemble Learning for Hyperspectral Imaging-based Blackberry Fruit Ripeness Detection in Uncontrolled Farm Environment. (arXiv:2401.04748v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04748</link>
<description rdf:parseType="Literal">&lt;p&gt;Fruit ripeness estimation models have for decades depended on spectral index
features or colour-based features, such as mean, standard deviation, skewness,
colour moments, and/or histograms for learning traits of fruit ripeness.
Recently, few studies have explored the use of deep learning techniques to
extract features from images of fruits with visible ripeness cues. However, the
blackberry (Rubus fruticosus) fruit does not show obvious and reliable visible
traits of ripeness when mature and therefore poses great difficulty to fruit
pickers. The mature blackberry, to the human eye, is black before, during, and
post-ripening. To address this engineering application challenge, this paper
proposes a novel multi-input convolutional neural network (CNN) ensemble
classifier for detecting subtle traits of ripeness in blackberry fruits. The
multi-input CNN was created from a pre-trained visual geometry group 16-layer
deep convolutional network (VGG16) model trained on the ImageNet dataset. The
fully connected layers were optimized for learning traits of ripeness of mature
blackberry fruits. The resulting model served as the base for building
homogeneous ensemble learners that were ensemble using the stack generalization
ensemble (SGE) framework. The input to the network is images acquired with a
stereo sensor using visible and near-infrared (VIS-NIR) spectral filters at
wavelengths of 700 nm and 770 nm. Through experiments, the proposed model
achieved 95.1% accuracy on unseen sets and 90.2% accuracy with in-field
conditions. Further experiments reveal that machine sensory is highly and
positively correlated to human sensory over blackberry fruit skin texture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olisah_C/0/1/0/all/0/1&quot;&gt;Chollette C. Olisah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trewhella_B/0/1/0/all/0/1&quot;&gt;Ben Trewhella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_M/0/1/0/all/0/1&quot;&gt;Melvyn L. Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Winstone_B/0/1/0/all/0/1&quot;&gt;Benjamin Winstone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whitfield_E/0/1/0/all/0/1&quot;&gt;E. Charles Whitfield&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_F/0/1/0/all/0/1&quot;&gt;Felicidad Fern&amp;#xe1;ndez Fern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duncalfe_H/0/1/0/all/0/1&quot;&gt;Harriet Duncalfe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04750">
<title>DedustNet: A Frequency-dominated Swin Transformer-based Wavelet Network for Agricultural Dust Removal. (arXiv:2401.04750v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04750</link>
<description rdf:parseType="Literal">&lt;p&gt;While dust significantly affects the environmental perception of automated
agricultural machines, the existing deep learning-based methods for dust
removal require further research and improvement in this area to improve the
performance and reliability of automated agricultural machines in agriculture.
We propose an end-to-end trainable learning network (DedustNet) to solve the
real-world agricultural dust removal task. To our knowledge, DedustNet is the
first time Swin Transformer-based units have been used in wavelet networks for
agricultural image dusting. Specifically, we present the frequency-dominated
block (DWTFormer block and IDWTFormer block) by adding a spatial features
aggregation scheme (SFAS) to the Swin Transformer and combining it with the
wavelet transform, the DWTFormer block and IDWTFormer block, alleviating the
limitation of the global receptive field of Swin Transformer when dealing with
complex dusty backgrounds. Furthermore, We propose a cross-level information
fusion module to fuse different levels of features and effectively capture
global and long-range feature relationships. In addition, we present a dilated
convolution module to capture contextual information guided by wavelet
transform at multiple scales, which combines the advantages of wavelet
transform and dilated convolution. Our algorithm leverages deep learning
techniques to effectively remove dust from images while preserving the original
structural and textural features. Compared to existing state-of-the-art
methods, DedustNet achieves superior performance and more reliable results in
agricultural image dedusting, providing strong support for the application of
agricultural machinery in dusty environments. Additionally, the impressive
performance on real-world hazy datasets and application tests highlights
DedustNet superior generalization ability and computer vision-related
application performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shengli Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1&quot;&gt;Sen Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04791">
<title>SOS-SLAM: Segmentation for Open-Set SLAM in Unstructured Environments. (arXiv:2401.04791v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.04791</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel framework for open-set Simultaneous Localization and
Mapping (SLAM) in unstructured environments that uses segmentation to create a
map of objects and geometric relationships between objects for localization.
Our system consists of 1) a front-end mapping pipeline using a zero-shot
segmentation model to extract object masks from images and track them across
frames to generate an object-based map and 2) a frame alignment pipeline that
uses the geometric consistency of objects to efficiently localize within maps
taken in a variety of conditions. This approach is shown to be more robust to
changes in lighting and appearance than traditional feature-based SLAM systems
or global descriptor methods. This is established by evaluating SOS-SLAM on the
Batvik seasonal dataset which includes drone flights collected over a coastal
plot of southern Finland during different seasons and lighting conditions.
Across flights during varying environmental conditions, our approach achieves
higher recall than benchmark methods with precision of 1.0. SOS-SLAM localizes
within a reference map up to 14x faster than other feature based approaches and
has a map size less than 0.4% the size of the most compact other maps. When
considering localization performance from varying viewpoints, our approach
outperforms all benchmarks from the same viewpoint and most benchmarks from
different viewpoints. SOS-SLAM is a promising new approach for SLAM in
unstructured environments that is robust to changes in lighting and appearance
and is more computationally efficient than other approaches. We release our
code and datasets: https://acl.mit.edu/SOS-SLAM/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kinnari_J/0/1/0/all/0/1&quot;&gt;Jouko Kinnari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thomas_A/0/1/0/all/0/1&quot;&gt;Annika Thomas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lusk_P/0/1/0/all/0/1&quot;&gt;Parker Lusk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kondo_K/0/1/0/all/0/1&quot;&gt;Kota Kondo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+How_J/0/1/0/all/0/1&quot;&gt;Jonathan P. How&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04801">
<title>Refining Remote Photoplethysmography Architectures using CKA and Empirical Methods. (arXiv:2401.04801v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04801</link>
<description rdf:parseType="Literal">&lt;p&gt;Model architecture refinement is a challenging task in deep learning research
fields such as remote photoplethysmography (rPPG). One architectural
consideration, the depth of the model, can have significant consequences on the
resulting performance. In rPPG models that are overprovisioned with more layers
than necessary, redundancies exist, the removal of which can result in faster
training and reduced computational load at inference time. With too few layers
the models may exhibit sub-optimal error rates. We apply Centered Kernel
Alignment (CKA) to an array of rPPG architectures of differing depths,
demonstrating that shallower models do not learn the same representations as
deeper models, and that after a certain depth, redundant layers are added
without significantly increased functionality. An empirical study confirms
these findings and shows how this method could be used to refine rPPG
architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vance_N/0/1/0/all/0/1&quot;&gt;Nathan Vance&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flynn_P/0/1/0/all/0/1&quot;&gt;Patrick Flynn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04860">
<title>Modality-Aware Representation Learning for Zero-shot Sketch-based Image Retrieval. (arXiv:2401.04860v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04860</link>
<description rdf:parseType="Literal">&lt;p&gt;Zero-shot learning offers an efficient solution for a machine learning model
to treat unseen categories, avoiding exhaustive data collection. Zero-shot
Sketch-based Image Retrieval (ZS-SBIR) simulates real-world scenarios where it
is hard and costly to collect paired sketch-photo samples. We propose a novel
framework that indirectly aligns sketches and photos by contrasting them
through texts, removing the necessity of access to sketch-photo pairs. With an
explicit modality encoding learned from data, our approach disentangles
modality-agnostic semantics from modality-specific information, bridging the
modality gap and enabling effective cross-modal content retrieval within a
joint latent space. From comprehensive experiments, we verify the efficacy of
the proposed model on ZS-SBIR, and it can be also applied to generalized and
fine-grained settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyou_E/0/1/0/all/0/1&quot;&gt;Eunyi Lyou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Doyeon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jooeun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Joonseok Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04861">
<title>CTNeRF: Cross-Time Transformer for Dynamic Neural Radiance Field from Monocular Video. (arXiv:2401.04861v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04861</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of our work is to generate high-quality novel views from monocular
videos of complex and dynamic scenes. Prior methods, such as DynamicNeRF, have
shown impressive performance by leveraging time-varying dynamic radiation
fields. However, these methods have limitations when it comes to accurately
modeling the motion of complex objects, which can lead to inaccurate and blurry
renderings of details. To address this limitation, we propose a novel approach
that builds upon a recent generalization NeRF, which aggregates nearby views
onto new viewpoints. However, such methods are typically only effective for
static scenes. To overcome this challenge, we introduce a module that operates
in both the time and frequency domains to aggregate the features of object
motion. This allows us to learn the relationship between frames and generate
higher-quality images. Our experiments demonstrate significant improvements
over state-of-the-art methods on dynamic scene datasets. Specifically, our
approach outperforms existing methods in terms of both the accuracy and visual
quality of the synthesized views.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_X/0/1/0/all/0/1&quot;&gt;Xingyu Miao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yang Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_H/0/1/0/all/0/1&quot;&gt;Haoran Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yawen Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_F/0/1/0/all/0/1&quot;&gt;Fan Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1&quot;&gt;Yang Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yefeng Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04872">
<title>Knowledge-aware Graph Transformer for Pedestrian Trajectory Prediction. (arXiv:2401.04872v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04872</link>
<description rdf:parseType="Literal">&lt;p&gt;Predicting pedestrian motion trajectories is crucial for path planning and
motion control of autonomous vehicles. Accurately forecasting crowd
trajectories is challenging due to the uncertain nature of human motions in
different environments. For training, recent deep learning-based prediction
approaches mainly utilize information like trajectory history and interactions
between pedestrians, among others. This can limit the prediction performance
across various scenarios since the discrepancies between training datasets have
not been properly incorporated. To overcome this limitation, this paper
proposes a graph transformer structure to improve prediction performance,
capturing the differences between the various sites and scenarios contained in
the datasets. In particular, a self-attention mechanism and a domain adaption
module have been designed to improve the generalization ability of the model.
Moreover, an additional metric considering cross-dataset sequences is
introduced for training and performance evaluation purposes. The proposed
framework is validated and compared against existing methods using popular
public datasets, i.e., ETH and UCY. Experimental results demonstrate the
improved performance of our proposed scheme.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuexin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kunming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yongliang Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Worrall_S/0/1/0/all/0/1&quot;&gt;Stewart Worrall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;You-Fu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_H/0/1/0/all/0/1&quot;&gt;He Kong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04903">
<title>SnapCap: Efficient Snapshot Compressive Video Captioning. (arXiv:2401.04903v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04903</link>
<description rdf:parseType="Literal">&lt;p&gt;Video Captioning (VC) is a challenging multi-modal task since it requires
describing the scene in language by understanding various and complex videos.
For machines, the traditional VC follows the
&quot;imaging-compression-decoding-and-then-captioning&quot; pipeline, where compression
is pivot for storage and transmission. However, in such a pipeline, some
potential shortcomings are inevitable, i.e., information redundancy resulting
in low efficiency and information loss during the sampling process for
captioning. To address these problems, in this paper, we propose a novel VC
pipeline to generate captions directly from the compressed measurement, which
can be captured by a snapshot compressive sensing camera and we dub our model
SnapCap. To be more specific, benefiting from the signal simulation, we have
access to obtain abundant measurement-video-annotation data pairs for our
model. Besides, to better extract language-related visual representations from
the compressed measurement, we propose to distill the knowledge from videos via
a pre-trained CLIP with plentiful language-vision associations to guide the
learning of our SnapCap. To demonstrate the effectiveness of SnapCap, we
conduct experiments on two widely-used VC datasets. Both the qualitative and
quantitative results verify the superiority of our pipeline over conventional
VC pipelines. In particular, compared to the &quot;caption-after-reconstruction&quot;
methods, our SnapCap can run at least 3$\times$ faster, and achieve better
caption results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jianqiao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yudi Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1&quot;&gt;Ziheng Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1&quot;&gt;Zequn Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhengjue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xin Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04921">
<title>Diffusion-based Pose Refinement and Muti-hypothesis Generation for 3D Human Pose Estimaiton. (arXiv:2401.04921v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04921</link>
<description rdf:parseType="Literal">&lt;p&gt;Previous probabilistic models for 3D Human Pose Estimation (3DHPE) aimed to
enhance pose accuracy by generating multiple hypotheses. However, most of the
hypotheses generated deviate substantially from the true pose. Compared to
deterministic models, the excessive uncertainty in probabilistic models leads
to weaker performance in single-hypothesis prediction. To address these two
challenges, we propose a diffusion-based refinement framework called DRPose,
which refines the output of deterministic models by reverse diffusion and
achieves more suitable multi-hypothesis prediction for the current pose
benchmark by multi-step refinement with multiple noises. To this end, we
propose a Scalable Graph Convolution Transformer (SGCT) and a Pose Refinement
Module (PRM) for denoising and refining. Extensive experiments on Human3.6M and
MPI-INF-3DHP datasets demonstrate that our method achieves state-of-the-art
performance on both single and multi-hypothesis 3DHPE. Code is available at
https://github.com/KHB1698/DRPose.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1&quot;&gt;Hongbo Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mengyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Doudou Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1&quot;&gt;Peng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xinlin Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wenming Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04923">
<title>Inconsistency-Based Data-Centric Active Open-Set Annotation. (arXiv:2401.04923v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.04923</link>
<description rdf:parseType="Literal">&lt;p&gt;Active learning is a commonly used approach that reduces the labeling effort
required to train deep neural networks. However, the effectiveness of current
active learning methods is limited by their closed-world assumptions, which
assume that all data in the unlabeled pool comes from a set of predefined known
classes. This assumption is often not valid in practical situations, as there
may be unknown classes in the unlabeled data, leading to the active open-set
annotation problem. The presence of unknown classes in the data can
significantly impact the performance of existing active learning methods due to
the uncertainty they introduce. To address this issue, we propose a novel
data-centric active learning method called NEAT that actively annotates
open-set data. NEAT is designed to label known classes data from a pool of both
known and unknown classes unlabeled data. It utilizes the clusterability of
labels to identify the known classes from the unlabeled pool and selects
informative samples from those classes based on a consistency criterion that
measures inconsistencies between model predictions and local feature
distribution. Unlike the recently proposed learning-centric method for the same
problem, NEAT is much more computationally efficient and is a data-centric
active open-set annotation method. Our experiments demonstrate that NEAT
achieves significantly better performance than state-of-the-art active learning
methods for active open-set annotation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_R/0/1/0/all/0/1&quot;&gt;Ruiyu Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_O/0/1/0/all/0/1&quot;&gt;Ouyang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yunhui Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04942">
<title>Latency-aware Road Anomaly Segmentation in Videos: A Photorealistic Dataset and New Metrics. (arXiv:2401.04942v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04942</link>
<description rdf:parseType="Literal">&lt;p&gt;In the past several years, road anomaly segmentation is actively explored in
the academia and drawing growing attention in the industry. The rationale
behind is straightforward: if the autonomous car can brake before hitting an
anomalous object, safety is promoted. However, this rationale naturally calls
for a temporally informed setting while existing methods and benchmarks are
designed in an unrealistic frame-wise manner. To bridge this gap, we contribute
the first video anomaly segmentation dataset for autonomous driving. Since
placing various anomalous objects on busy roads and annotating them in every
frame are dangerous and expensive, we resort to synthetic data. To improve the
relevance of this synthetic dataset to real-world applications, we train a
generative adversarial network conditioned on rendering G-buffers for
photorealism enhancement. Our dataset consists of 120,000 high-resolution
frames at a 60 FPS framerate, as recorded in 7 different towns. As an initial
benchmarking, we provide baselines using latest supervised and unsupervised
road anomaly segmentation methods. Apart from conventional ones, we focus on
two new metrics: temporal consistency and latencyaware streaming accuracy. We
believe the latter is valuable as it measures whether an anomaly segmentation
algorithm can truly prevent a car from crashing in a temporally informed
setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1&quot;&gt;Beiwen Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1&quot;&gt;Huan-ang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1&quot;&gt;Leiyao Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yupeng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1&quot;&gt;Lan Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Baofeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhi_R/0/1/0/all/0/1&quot;&gt;Rong Zhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1&quot;&gt;Guyue Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hao Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04956">
<title>EmMixformer: Mix transformer for eye movement recognition. (arXiv:2401.04956v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04956</link>
<description rdf:parseType="Literal">&lt;p&gt;Eye movement (EM) is a new highly secure biometric behavioral modality that
has received increasing attention in recent years. Although deep neural
networks, such as convolutional neural network (CNN), have recently achieved
promising performance, current solutions fail to capture local and global
temporal dependencies within eye movement data. To overcome this problem, we
propose in this paper a mixed transformer termed EmMixformer to extract time
and frequency domain information for eye movement recognition. To this end, we
propose a mixed block consisting of three modules, transformer, attention Long
short-term memory (attention LSTM), and Fourier transformer. We are the first
to attempt leveraging transformer to learn long temporal dependencies within
eye movement. Second, we incorporate the attention mechanism into LSTM to
propose attention LSTM with the aim to learn short temporal dependencies.
Third, we perform self attention in the frequency domain to learn global
features. As the three modules provide complementary feature representations in
terms of local and global dependencies, the proposed EmMixformer is capable of
improving recognition accuracy. The experimental results on our eye movement
dataset and two public eye movement datasets show that the proposed EmMixformer
outperforms the state of the art by achieving the lowest verification error.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1&quot;&gt;Huafeng Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Hongyu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1&quot;&gt;Xin Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1&quot;&gt;Qun Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+El_Yacoubi_M/0/1/0/all/0/1&quot;&gt;Mounim A. El-Yacoubi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xinbo Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04961">
<title>ECC-PolypDet: Enhanced CenterNet with Contrastive Learning for Automatic Polyp Detection. (arXiv:2401.04961v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04961</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate polyp detection is critical for early colorectal cancer diagnosis.
Although remarkable progress has been achieved in recent years, the complex
colon environment and concealed polyps with unclear boundaries still pose
severe challenges in this area. Existing methods either involve computationally
expensive context aggregation or lack prior modeling of polyps, resulting in
poor performance in challenging cases. In this paper, we propose the Enhanced
CenterNet with Contrastive Learning (ECC-PolypDet), a two-stage training \&amp;amp;
end-to-end inference framework that leverages images and bounding box
annotations to train a general model and fine-tune it based on the inference
score to obtain a final robust model. Specifically, we conduct Box-assisted
Contrastive Learning (BCL) during training to minimize the intra-class
difference and maximize the inter-class difference between foreground polyps
and backgrounds, enabling our model to capture concealed polyps. Moreover, to
enhance the recognition of small polyps, we design the Semantic Flow-guided
Feature Pyramid Network (SFFPN) to aggregate multi-scale features and the
Heatmap Propagation (HP) module to boost the model&apos;s attention on polyp
targets. In the fine-tuning stage, we introduce the IoU-guided Sample
Re-weighting (ISR) mechanism to prioritize hard samples by adaptively adjusting
the loss weight for each sample during fine-tuning. Extensive experiments on
six large-scale colonoscopy datasets demonstrate the superiority of our model
compared with previous state-of-the-art detectors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yuncheng Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zixun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yiwen Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guanbin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1&quot;&gt;Xiang Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Song Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1&quot;&gt;Shuguang Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Silin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhen Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04962">
<title>Large Model based Sequential Keyframe Extraction for Video Summarization. (arXiv:2401.04962v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04962</link>
<description rdf:parseType="Literal">&lt;p&gt;Keyframe extraction aims to sum up a video&apos;s semantics with the minimum
number of its frames. This paper puts forward a Large Model based Sequential
Keyframe Extraction for video summarization, dubbed LMSKE, which contains three
stages as below. First, we use the large model &quot;TransNetV21&quot; to cut the video
into consecutive shots, and employ the large model &quot;CLIP2&quot; to generate each
frame&apos;s visual feature within each shot; Second, we develop an adaptive
clustering algorithm to yield candidate keyframes for each shot, with each
candidate keyframe locating nearest to a cluster center; Third, we further
reduce the above candidate keyframes via redundancy elimination within each
shot, and finally concatenate them in accordance with the sequence of shots as
the final sequential keyframes. To evaluate LMSKE, we curate a benchmark
dataset and conduct rich experiments, whose results exhibit that LMSKE performs
much better than quite a few SOTA competitors with average F1 of 0.5311,
average fidelity of 0.8141, and average compression ratio of 0.9922.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1&quot;&gt;Kailong Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuxiang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Q/0/1/0/all/0/1&quot;&gt;Qianchen Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Rui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04975">
<title>HaltingVT: Adaptive Token Halting Transformer for Efficient Video Recognition. (arXiv:2401.04975v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04975</link>
<description rdf:parseType="Literal">&lt;p&gt;Action recognition in videos poses a challenge due to its high computational
cost, especially for Joint Space-Time video transformers (Joint VT). Despite
their effectiveness, the excessive number of tokens in such architectures
significantly limits their efficiency. In this paper, we propose HaltingVT, an
efficient video transformer adaptively removing redundant video patch tokens,
which is primarily composed of a Joint VT and a Glimpser module. Specifically,
HaltingVT applies data-adaptive token reduction at each layer, resulting in a
significant reduction in the overall computational cost. Besides, the Glimpser
module quickly removes redundant tokens in shallow transformer layers, which
may even be misleading for video recognition tasks based on our observations.
To further encourage HaltingVT to focus on the key motion-related information
in videos, we design an effective Motion Loss during training. HaltingVT
acquires video analysis capabilities and token halting compression strategies
simultaneously in a unified training process, without requiring additional
training procedures or sub-networks. On the Mini-Kinetics dataset, we achieved
75.0% top-1 ACC with 24.2 GFLOPs, as well as 67.2% top-1 ACC with an extremely
low 9.9 GFLOPs. The code is available at
https://github.com/dun-research/HaltingVT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_R/0/1/0/all/0/1&quot;&gt;Ruoxuan Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuke Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Haoqi Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04984">
<title>MGNet: Learning Correspondences via Multiple Graphs. (arXiv:2401.04984v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04984</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning correspondences aims to find correct correspondences (inliers) from
the initial correspondence set with an uneven correspondence distribution and a
low inlier rate, which can be regarded as graph data. Recent advances usually
use graph neural networks (GNNs) to build a single type of graph or simply
stack local graphs into the global one to complete the task. But they ignore
the complementary relationship between different types of graphs, which can
effectively capture potential relationships among sparse correspondences. To
address this problem, we propose MGNet to effectively combine multiple
complementary graphs. To obtain information integrating implicit and explicit
local graphs, we construct local graphs from implicit and explicit aspects and
combine them effectively, which is used to build a global graph. Moreover, we
propose Graph~Soft~Degree~Attention (GSDA) to make full use of all sparse
correspondence information at once in the global graph, which can capture and
amplify discriminative features. Extensive experiments demonstrate that MGNet
outperforms state-of-the-art methods in different visual tasks. The code is
provided in https://github.com/DAILUANYUAN/MGNet-2024AAAI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_L/0/1/0/all/0/1&quot;&gt;Luanyuan Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hanwang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jinhui Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04988">
<title>Optimising Graph Representation for Hardware Implementation of Graph Convolutional Networks for Event-based Vision. (arXiv:2401.04988v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04988</link>
<description rdf:parseType="Literal">&lt;p&gt;Event-based vision is an emerging research field involving processing data
generated by Dynamic Vision Sensors (neuromorphic cameras). One of the latest
proposals in this area are Graph Convolutional Networks (GCNs), which allow to
process events in its original sparse form while maintaining high detection and
classification performance. In this paper, we present the hardware
implementation of a~graph generation process from an event camera data stream,
taking into account both the advantages and limitations of FPGAs. We propose
various ways to simplify the graph representation and use scaling and
quantisation of values. We consider both undirected and directed graphs that
enable the use of PointNet convolution. The results obtained show that by
appropriately modifying the graph representation, it is possible to create
a~hardware module for graph generation. Moreover, the proposed modifications
have no significant impact on object detection performance, only 0.08% mAP less
for the base model and the N-Caltech data set.Finally, we describe the proposed
hardware architecture of the graph generation module.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeziorek_K/0/1/0/all/0/1&quot;&gt;Kamil Jeziorek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wzorek_P/0/1/0/all/0/1&quot;&gt;Piotr Wzorek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blachut_K/0/1/0/all/0/1&quot;&gt;Krzysztof Blachut&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pinna_A/0/1/0/all/0/1&quot;&gt;Andrea Pinna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kryjak_T/0/1/0/all/0/1&quot;&gt;Tomasz Kryjak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05010">
<title>Less is More : A Closer Look at Multi-Modal Few-Shot Learning. (arXiv:2401.05010v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.05010</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot Learning aims to learn and distinguish new categories with a very
limited number of available images, presenting a significant challenge in the
realm of deep learning. Recent researchers have sought to leverage the
additional textual or linguistic information of these rare categories with a
pre-trained language model to facilitate learning, thus partially alleviating
the problem of insufficient supervision signals. However, the full potential of
the textual information and pre-trained language model have been underestimated
in the few-shot learning till now, resulting in limited performance
enhancements. To address this, we propose a simple but effective framework for
few-shot learning tasks, specifically designed to exploit the textual
information and language model. In more detail, we explicitly exploit the
zero-shot capability of the pre-trained language model with the learnable
prompt. And we just add the visual feature with the textual feature for
inference directly without the intricate designed fusion modules in previous
works. Additionally, we apply the self-ensemble and distillation to further
enhance these components. Our extensive experiments conducted across four
widely used few-shot datasets demonstrate that our simple framework achieves
impressive results. Particularly noteworthy is its outstanding performance in
the 1-shot learning task, surpassing state-of-the-art methods by an average of
3.0\% in classification accuracy. \footnote{We will make the source codes of
the proposed framework publicly available upon acceptance. }.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chunpeng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haishuai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xilu Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bu_J/0/1/0/all/0/1&quot;&gt;Jiajun Bu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05011">
<title>Dual-Perspective Knowledge Enrichment for Semi-Supervised 3D Object Detection. (arXiv:2401.05011v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.05011</link>
<description rdf:parseType="Literal">&lt;p&gt;Semi-supervised 3D object detection is a promising yet under-explored
direction to reduce data annotation costs, especially for cluttered indoor
scenes. A few prior works, such as SESS and 3DIoUMatch, attempt to solve this
task by utilizing a teacher model to generate pseudo-labels for unlabeled
samples. However, the availability of unlabeled samples in the 3D domain is
relatively limited compared to its 2D counterpart due to the greater effort
required to collect 3D data. Moreover, the loose consistency regularization in
SESS and restricted pseudo-label selection strategy in 3DIoUMatch lead to
either low-quality supervision or a limited amount of pseudo labels. To address
these issues, we present a novel Dual-Perspective Knowledge Enrichment approach
named DPKE for semi-supervised 3D object detection. Our DPKE enriches the
knowledge of limited training data, particularly unlabeled data, from two
perspectives: data-perspective and feature-perspective. Specifically, from the
data-perspective, we propose a class-probabilistic data augmentation method
that augments the input data with additional instances based on the varying
distribution of class probabilities. Our DPKE achieves feature-perspective
knowledge enrichment by designing a geometry-aware feature matching method that
regularizes feature-level similarity between object proposals from the student
and teacher models. Extensive experiments on the two benchmark datasets
demonstrate that our DPKE achieves superior performance over existing
state-of-the-art approaches under various label ratio conditions. The source
code will be made available to the public.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yucheng Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_N/0/1/0/all/0/1&quot;&gt;Na Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weiling Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1&quot;&gt;Keng Teck Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hanwang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05014">
<title>Source-Free Cross-Modal Knowledge Transfer by Unleashing the Potential of Task-Irrelevant Data. (arXiv:2401.05014v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.05014</link>
<description rdf:parseType="Literal">&lt;p&gt;Source-free cross-modal knowledge transfer is a crucial yet challenging task,
which aims to transfer knowledge from one source modality (e.g., RGB) to the
target modality (e.g., depth or infrared) with no access to the task-relevant
(TR) source data due to memory and privacy concerns. A recent attempt leverages
the paired task-irrelevant (TI) data and directly matches the features from
them to eliminate the modality gap. However, it ignores a pivotal clue that the
paired TI data could be utilized to effectively estimate the source data
distribution and better facilitate knowledge transfer to the target modality.
To this end, we propose a novel yet concise framework to unlock the potential
of paired TI data for enhancing source-free cross-modal knowledge transfer. Our
work is buttressed by two key technical components. Firstly, to better estimate
the source data distribution, we introduce a Task-irrelevant data-Guided
Modality Bridging (TGMB) module. It translates the target modality data (e.g.,
infrared) into the source-like RGB images based on paired TI data and the
guidance of the available source model to alleviate two key gaps: 1)
inter-modality gap between the paired TI data; 2) intra-modality gap between TI
and TR target data. We then propose a Task-irrelevant data-Guided Knowledge
Transfer (TGKT) module that transfers knowledge from the source model to the
target model by leveraging the paired TI data. Notably, due to the
unavailability of labels for the TR target data and its less reliable
prediction from the source model, our TGKT model incorporates a self-supervised
pseudo-labeling approach to enable the target model to learn from its
predictions. Extensive experiments show that our method achieves
state-of-the-art performance on three datasets (RGB-to-depth and
RGB-to-infrared).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jinjing Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yucheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05018">
<title>AdvMT: Adversarial Motion Transformer for Long-term Human Motion Prediction. (arXiv:2401.05018v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.05018</link>
<description rdf:parseType="Literal">&lt;p&gt;To achieve seamless collaboration between robots and humans in a shared
environment, accurately predicting future human movements is essential. Human
motion prediction has traditionally been approached as a sequence prediction
problem, leveraging historical human motion data to estimate future poses.
Beginning with vanilla recurrent networks, the research community has
investigated a variety of methods for learning human motion dynamics,
encompassing graph-based and generative approaches. Despite these efforts,
achieving accurate long-term predictions continues to be a significant
challenge. In this regard, we present the Adversarial Motion Transformer
(AdvMT), a novel model that integrates a transformer-based motion encoder and a
temporal continuity discriminator. This combination effectively captures
spatial and temporal dependencies simultaneously within frames. With
adversarial training, our method effectively reduces the unwanted artifacts in
predictions, thereby ensuring the learning of more realistic and fluid human
motions. The evaluation results indicate that AdvMT greatly enhances the
accuracy of long-term predictions while also delivering robust short-term
predictions
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Idrees_S/0/1/0/all/0/1&quot;&gt;Sarmad Idrees&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jongeun Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sohn_S/0/1/0/all/0/1&quot;&gt;Seokman Sohn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05049">
<title>Content-Aware Depth-Adaptive Image Restoration. (arXiv:2401.05049v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.05049</link>
<description rdf:parseType="Literal">&lt;p&gt;This work prioritizes building a modular pipeline that utilizes existing
models to systematically restore images, rather than creating new restoration
models from scratch. Restoration is carried out at an object-specific level,
with each object regenerated using its corresponding class label information.
The approach stands out by providing complete user control over the entire
restoration process. Users can select models for specialized restoration steps,
customize the sequence of steps to meet their needs, and refine the resulting
regenerated image with depth awareness. The research provides two distinct
pathways for implementing image regeneration, allowing for a comparison of
their respective strengths and limitations. The most compelling aspect of this
versatile system is its adaptability. This adaptability enables users to target
particular object categories, including medical images, by providing models
that are trained on those object classes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vargis_T/0/1/0/all/0/1&quot;&gt;Tom Richard Vargis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghiasvand_S/0/1/0/all/0/1&quot;&gt;Siavash Ghiasvand&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05055">
<title>Application of Deep Learning in Blind Motion Deblurring: Current Status and Future Prospects. (arXiv:2401.05055v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.05055</link>
<description rdf:parseType="Literal">&lt;p&gt;Motion deblurring is one of the fundamental problems of computer vision and
has received continuous attention. The variability in blur, both within and
across images, imposes limitations on non-blind deblurring techniques that rely
on estimating the blur kernel. As a response, blind motion deblurring has
emerged, aiming to restore clear and detailed images without prior knowledge of
the blur type, fueled by the advancements in deep learning methodologies.
Despite strides in this field, a comprehensive synthesis of recent progress in
deep learning-based blind motion deblurring is notably absent. This paper fills
that gap by providing an exhaustive overview of the role of deep learning in
blind motion deblurring, encompassing datasets, evaluation metrics, and methods
developed over the last six years. Specifically, we first introduce the types
of motion blur and the fundamental principles of deblurring. Next, we outline
the shortcomings of traditional non-blind deblurring algorithms, emphasizing
the advantages of employing deep learning techniques for deblurring tasks.
Following this, we categorize and summarize existing blind motion deblurring
methods based on different backbone networks, including convolutional neural
networks, generative adversarial networks, recurrent neural networks, and
Transformer networks. Subsequently, we elaborate not only on the fundamental
principles of these different categories but also provide a comprehensive
summary and comparison of their advantages and limitations. Qualitative and
quantitative experimental results conducted on four widely used datasets
further compare the performance of SOTA methods. Finally, an analysis of
present challenges and future pathways. All collected models, benchmark
datasets, source code links, and codes for evaluation have been made publicly
available at https://github.com/VisionVerse/Blind-Motion-Deblurring-Survey
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1&quot;&gt;Yawen Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Heng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chengyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1&quot;&gt;Fangwei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhongbo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yongqiang Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05093">
<title>SwiMDiff: Scene-wide Matching Contrastive Learning with Diffusion Constraint for Remote Sensing Image. (arXiv:2401.05093v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.05093</link>
<description rdf:parseType="Literal">&lt;p&gt;With recent advancements in aerospace technology, the volume of unlabeled
remote sensing image (RSI) data has increased dramatically. Effectively
leveraging this data through self-supervised learning (SSL) is vital in the
field of remote sensing. However, current methodologies, particularly
contrastive learning (CL), a leading SSL method, encounter specific challenges
in this domain. Firstly, CL often mistakenly identifies geographically adjacent
samples with similar semantic content as negative pairs, leading to confusion
during model training. Secondly, as an instance-level discriminative task, it
tends to neglect the essential fine-grained features and complex details
inherent in unstructured RSIs. To overcome these obstacles, we introduce
SwiMDiff, a novel self-supervised pre-training framework designed for RSIs.
SwiMDiff employs a scene-wide matching approach that effectively recalibrates
labels to recognize data from the same scene as false negatives. This
adjustment makes CL more applicable to the nuances of remote sensing.
Additionally, SwiMDiff seamlessly integrates CL with a diffusion model. Through
the implementation of pixel-level diffusion constraints, we enhance the
encoder&apos;s ability to capture both the global semantic information and the
fine-grained features of the images more comprehensively. Our proposed
framework significantly enriches the information available for downstream tasks
in remote sensing. Demonstrating exceptional performance in change detection
and land-cover classification tasks, SwiMDiff proves its substantial utility
and value in the field of remote sensing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1&quot;&gt;Jiayuan Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1&quot;&gt;Jie Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiaqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1&quot;&gt;Weiying Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunsong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05126">
<title>Efficient Fine-Tuning with Domain Adaptation for Privacy-Preserving Vision Transformer. (arXiv:2401.05126v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.05126</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel method for privacy-preserving deep neural networks (DNNs)
with the Vision Transformer (ViT). The method allows us not only to train
models and test with visually protected images but to also avoid the
performance degradation caused from the use of encrypted images, whereas
conventional methods cannot avoid the influence of image encryption. A domain
adaptation method is used to efficiently fine-tune ViT with encrypted images.
In experiments, the method is demonstrated to outperform conventional methods
in an image classification task on the CIFAR-10 and ImageNet datasets in terms
of classification accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagamori_T/0/1/0/all/0/1&quot;&gt;Teru Nagamori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shiota_S/0/1/0/all/0/1&quot;&gt;Sayaka Shiota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiya_H/0/1/0/all/0/1&quot;&gt;Hitoshi Kiya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05137">
<title>DISCOVER: 2-D Multiview Summarization of Optical Coherence Tomography Angiography for Automatic Diabetic Retinopathy Diagnosis. (arXiv:2401.05137v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.05137</link>
<description rdf:parseType="Literal">&lt;p&gt;Diabetic Retinopathy (DR), an ocular complication of diabetes, is a leading
cause of blindness worldwide. Traditionally, DR is monitored using Color Fundus
Photography (CFP), a widespread 2-D imaging modality. However, DR
classifications based on CFP have poor predictive power, resulting in
suboptimal DR management. Optical Coherence Tomography Angiography (OCTA) is a
recent 3-D imaging modality offering enhanced structural and functional
information (blood flow) with a wider field of view. This paper investigates
automatic DR severity assessment using 3-D OCTA. A straightforward solution to
this task is a 3-D neural network classifier. However, 3-D architectures have
numerous parameters and typically require many training samples. A lighter
solution consists in using 2-D neural network classifiers processing 2-D
en-face (or frontal) projections and/or 2-D cross-sectional slices. Such an
approach mimics the way ophthalmologists analyze OCTA acquisitions: 1) en-face
flow maps are often used to detect avascular zones and neovascularization, and
2) cross-sectional slices are commonly analyzed to detect macular edemas, for
instance. However, arbitrary data reduction or selection might result in
information loss. Two complementary strategies are thus proposed to optimally
summarize OCTA volumes with 2-D images: 1) a parametric en-face projection
optimized through deep learning and 2) a cross-sectional slice selection
process controlled through gradient-based attribution. The full summarization
and DR classification pipeline is trained from end to end. The automatic 2-D
summary can be displayed in a viewer or printed in a report to support the
decision. We show that the proposed 2-D summarization and classification
pipeline outperforms direct 3-D classification with the advantage of improved
interpretability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Daho_M/0/1/0/all/0/1&quot;&gt;Mostafa El Habib Daho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yihao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zeghlache_R/0/1/0/all/0/1&quot;&gt;Rachid Zeghlache&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Boite_H/0/1/0/all/0/1&quot;&gt;Hugo Le Boit&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Deman_P/0/1/0/all/0/1&quot;&gt;Pierre Deman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Borderie_L/0/1/0/all/0/1&quot;&gt;Laurent Borderie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ren_H/0/1/0/all/0/1&quot;&gt;Hugang Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mannivanan_N/0/1/0/all/0/1&quot;&gt;Niranchana Mannivanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lepicard_C/0/1/0/all/0/1&quot;&gt;Capucine Lepicard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cochener_B/0/1/0/all/0/1&quot;&gt;B&amp;#xe9;atrice Cochener&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Couturier_A/0/1/0/all/0/1&quot;&gt;Aude Couturier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tadayoni_R/0/1/0/all/0/1&quot;&gt;Ramin Tadayoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Conze_P/0/1/0/all/0/1&quot;&gt;Pierre-Henri Conze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lamard_M/0/1/0/all/0/1&quot;&gt;Mathieu Lamard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Quellec_G/0/1/0/all/0/1&quot;&gt;Gwenol&amp;#xe9; Quellec&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05153">
<title>CrossDiff: Exploring Self-Supervised Representation of Pansharpening via Cross-Predictive Diffusion Model. (arXiv:2401.05153v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.05153</link>
<description rdf:parseType="Literal">&lt;p&gt;Fusion of a panchromatic (PAN) image and corresponding multispectral (MS)
image is also known as pansharpening, which aims to combine abundant spatial
details of PAN and spectral information of MS. Due to the absence of
high-resolution MS images, available deep-learning-based methods usually follow
the paradigm of training at reduced resolution and testing at both reduced and
full resolution. When taking original MS and PAN images as inputs, they always
obtain sub-optimal results due to the scale variation. In this paper, we
propose to explore the self-supervised representation of pansharpening by
designing a cross-predictive diffusion model, named CrossDiff. It has two-stage
training. In the first stage, we introduce a cross-predictive pretext task to
pre-train the UNet structure based on conditional DDPM, while in the second
stage, the encoders of the UNets are frozen to directly extract spatial and
spectral features from PAN and MS, and only the fusion head is trained to adapt
for pansharpening task. Extensive experiments show the effectiveness and
superiority of the proposed model compared with state-of-the-art supervised and
unsupervised methods. Besides, the cross-sensor experiments also verify the
generalization ability of proposed self-supervised representation learners for
other satellite&apos;s datasets. We will release our code for reproducibility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1&quot;&gt;Yinghui Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1&quot;&gt;Litao Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;ShiZhou Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiuwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yanning Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05157">
<title>Toward distortion-aware change detection in realistic scenarios. (arXiv:2401.05157v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.05157</link>
<description rdf:parseType="Literal">&lt;p&gt;In the conventional change detection (CD) pipeline, two manually registered
and labeled remote sensing datasets serve as the input of the model for
training and prediction. However, in realistic scenarios, data from different
periods or sensors could fail to be aligned as a result of various coordinate
systems. Geometric distortion caused by coordinate shifting remains a thorny
issue for CD algorithms. In this paper, we propose a reusable self-supervised
framework for bitemporal geometric distortion in CD tasks. The whole framework
is composed of Pretext Representation Pre-training, Bitemporal Image Alignment,
and Down-stream Decoder Fine-Tuning. With only single-stage pre-training, the
key components of the framework can be reused for assistance in the bitemporal
image alignment, while simultaneously enhancing the performance of the CD
decoder. Experimental results in 2 large-scale realistic scenarios demonstrate
that our proposed method can alleviate the bitemporal geometric distortion in
CD tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yitao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Heng-Chao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Nanqing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05159">
<title>Derm-T2IM: Harnessing Synthetic Skin Lesion Data via Stable Diffusion Models for Enhanced Skin Disease Classification using ViT and CNN. (arXiv:2401.05159v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.05159</link>
<description rdf:parseType="Literal">&lt;p&gt;This study explores the utilization of Dermatoscopic synthetic data generated
through stable diffusion models as a strategy for enhancing the robustness of
machine learning model training. Synthetic data generation plays a pivotal role
in mitigating challenges associated with limited labeled datasets, thereby
facilitating more effective model training. In this context, we aim to
incorporate enhanced data transformation techniques by extending the recent
success of few-shot learning and a small amount of data representation in
text-to-image latent diffusion models. The optimally tuned model is further
used for rendering high-quality skin lesion synthetic data with diverse and
realistic characteristics, providing a valuable supplement and diversity to the
existing training data. We investigate the impact of incorporating newly
generated synthetic data into the training pipeline of state-of-art machine
learning models, assessing its effectiveness in enhancing model performance and
generalization to unseen real-world data. Our experimental results demonstrate
the efficacy of the synthetic data generated through stable diffusion models
helps in improving the robustness and adaptability of end-to-end CNN and vision
transformer models on two different real-world skin lesion datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farooq_M/0/1/0/all/0/1&quot;&gt;Muhammad Ali Farooq&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1&quot;&gt;Wang Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schukat_M/0/1/0/all/0/1&quot;&gt;Michael Schukat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Little_M/0/1/0/all/0/1&quot;&gt;Mark A Little&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corcoran_P/0/1/0/all/0/1&quot;&gt;Peter Corcoran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05163">
<title>MISS: A Generative Pretraining and Finetuning Approach for Med-VQA. (arXiv:2401.05163v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.05163</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical visual question answering (VQA) is a challenging multimodal task,
where Vision-Language Pre-training (VLP) models can effectively improve the
generalization performance. However, most methods in the medical field treat
VQA as an answer classification task which is difficult to transfer to
practical application scenarios. Additionally, due to the privacy of medical
images and the expensive annotation process, large-scale medical image-text
pairs datasets for pretraining are severely lacking. In this paper, we propose
a large-scale MultI-task Self-Supervised learning based framework (MISS) for
medical VQA tasks. Unlike existing methods, we treat medical VQA as a
generative task. We unify the text encoder and multimodal encoder and align
image-text features through multi-task learning. Furthermore, we propose a
Transfer-and-Caption method that extends the feature space of single-modal
image datasets using large language models (LLMs), enabling those traditional
medical vision field task data to be applied to VLP. Experiments show that our
method achieves excellent results with fewer multimodal datasets and
demonstrates the advantages of generative VQA models. The code and model
weights will be released upon the paper&apos;s acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiawei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Dingkang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yue Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lihua Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05166">
<title>REACT 2024: the Second Multiple Appropriate Facial Reaction Generation Challenge. (arXiv:2401.05166v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.05166</link>
<description rdf:parseType="Literal">&lt;p&gt;In dyadic interactions, humans communicate their intentions and state of mind
using verbal and non-verbal cues, where multiple different facial reactions
might be appropriate in response to a specific speaker behaviour. Then, how to
develop a machine learning (ML) model that can automatically generate multiple
appropriate, diverse, realistic and synchronised human facial reactions from an
previously unseen speaker behaviour is a challenging task. Following the
successful organisation of the first REACT challenge (REACT 2023), this edition
of the challenge (REACT 2024) employs a subset used by the previous challenge,
which contains segmented 30-secs dyadic interaction clips originally recorded
as part of the NOXI and RECOLA datasets, encouraging participants to develop
and benchmark Machine Learning (ML) models that can generate multiple
appropriate facial reactions (including facial image sequences and their
attributes) given an input conversational partner&apos;s stimulus under various
dyadic video conference scenarios. This paper presents: (i) the guidelines of
the REACT 2024 challenge; (ii) the dataset utilized in the challenge; and (iii)
the performance of the baseline systems on the two proposed sub-challenges:
Offline Multiple Appropriate Facial Reaction Generation and Online Multiple
Appropriate Facial Reaction Generation, respectively. The challenge baseline
code is publicly available at
https://github.com/reactmultimodalchallenge/baseline_react2024.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Siyang Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spitale_M/0/1/0/all/0/1&quot;&gt;Micol Spitale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1&quot;&gt;Cheng Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palmero_C/0/1/0/all/0/1&quot;&gt;Cristina Palmero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barquero_G/0/1/0/all/0/1&quot;&gt;German Barquero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Hengde Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1&quot;&gt;Sergio Escalera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valstar_M/0/1/0/all/0/1&quot;&gt;Michel Valstar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baur_T/0/1/0/all/0/1&quot;&gt;Tobias Baur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ringeval_F/0/1/0/all/0/1&quot;&gt;Fabien Ringeval&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andre_E/0/1/0/all/0/1&quot;&gt;Elisabeth Andre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunes_H/0/1/0/all/0/1&quot;&gt;Hatice Gunes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05167">
<title>Watermark Text Pattern Spotting in Document Images. (arXiv:2401.05167v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.05167</link>
<description rdf:parseType="Literal">&lt;p&gt;Watermark text spotting in document images can offer access to an often
unexplored source of information, providing crucial evidence about a record&apos;s
scope, audience and sometimes even authenticity. Stemming from the problem of
text spotting, detecting and understanding watermarks in documents inherits the
same hardships - in the wild, writing can come in various fonts, sizes and
forms, making generic recognition a very difficult problem. To address the lack
of resources in this field and propel further research, we propose a novel
benchmark (K-Watermark) containing 65,447 data samples generated using Wrender,
a watermark text patterns rendering procedure. A validity study using humans
raters yields an authenticity score of 0.51 against pre-generated watermarked
documents. To prove the usefulness of the dataset and rendering technique, we
developed an end-to-end solution (Wextract) for detecting the bounding box
instances of watermark text, while predicting the depicted text. To deal with
this specific task, we introduce a variance minimization loss and a
hierarchical self-attention mechanism. To the best of our knowledge, we are the
first to propose an evaluation benchmark and a complete solution for retrieving
watermarks from documents surpassing baselines by 5 AP points in detection and
4 points in character accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krubinski_M/0/1/0/all/0/1&quot;&gt;Mateusz Krubinski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matcovici_S/0/1/0/all/0/1&quot;&gt;Stefan Matcovici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grigore_D/0/1/0/all/0/1&quot;&gt;Diana Grigore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Voinea_D/0/1/0/all/0/1&quot;&gt;Daniel Voinea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Popa_A/0/1/0/all/0/1&quot;&gt;Alin-Ionut Popa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05168">
<title>CLIP-guided Source-free Object Detection in Aerial Images. (arXiv:2401.05168v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.05168</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain adaptation is crucial in aerial imagery, as the visual representation
of these images can significantly vary based on factors such as geographic
location, time, and weather conditions. Additionally, high-resolution aerial
images often require substantial storage space and may not be readily
accessible to the public. To address these challenges, we propose a novel
Source-Free Object Detection (SFOD) method. Specifically, our approach is built
upon a self-training framework; however, self-training can lead to inaccurate
learning in the absence of labeled training data. To address this issue, we
further integrate Contrastive Language-Image Pre-training (CLIP) to guide the
generation of pseudo-labels, termed CLIP-guided Aggregation. By leveraging
CLIP&apos;s zero-shot classification capability, we use it to aggregate scores with
the original predicted bounding boxes, enabling us to obtain refined scores for
the pseudo-labels. To validate the effectiveness of our method, we constructed
two new datasets from different domains based on the DIOR dataset, named DIOR-C
and DIOR-Cloudy. Experiments demonstrate that our method outperforms other
comparative algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Nanqing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yongyi Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chengxin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_P/0/1/0/all/0/1&quot;&gt;Peiliang Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Heng-Chao Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05202">
<title>Video-based Automatic Lameness Detection of Dairy Cows using Pose Estimation and Multiple Locomotion Traits. (arXiv:2401.05202v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.05202</link>
<description rdf:parseType="Literal">&lt;p&gt;This study presents an automated lameness detection system that uses
deep-learning image processing techniques to extract multiple locomotion traits
associated with lameness. Using the T-LEAP pose estimation model, the motion of
nine keypoints was extracted from videos of walking cows. The videos were
recorded outdoors, with varying illumination conditions, and T-LEAP extracted
99.6% of correct keypoints. The trajectories of the keypoints were then used to
compute six locomotion traits: back posture measurement, head bobbing, tracking
distance, stride length, stance duration, and swing duration. The three most
important traits were back posture measurement, head bobbing, and tracking
distance. For the ground truth, we showed that a thoughtful merging of the
scores of the observers could improve intra-observer reliability and agreement.
We showed that including multiple locomotion traits improves the classification
accuracy from 76.6% with only one trait to 79.9% with the three most important
traits and to 80.1% with all six locomotion traits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russello_H/0/1/0/all/0/1&quot;&gt;Helena Russello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tol_R/0/1/0/all/0/1&quot;&gt;Rik van der Tol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holzhauer_M/0/1/0/all/0/1&quot;&gt;Menno Holzhauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henten_E/0/1/0/all/0/1&quot;&gt;Eldert J. van Henten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kootstra_G/0/1/0/all/0/1&quot;&gt;Gert Kootstra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05217">
<title>Exploring Vulnerabilities of No-Reference Image Quality Assessment Models: A Query-Based Black-Box Method. (arXiv:2401.05217v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.05217</link>
<description rdf:parseType="Literal">&lt;p&gt;No-Reference Image Quality Assessment (NR-IQA) aims to predict image quality
scores consistent with human perception without relying on pristine reference
images, serving as a crucial component in various visual tasks. Ensuring the
robustness of NR-IQA methods is vital for reliable comparisons of different
image processing techniques and consistent user experiences in recommendations.
The attack methods for NR-IQA provide a powerful instrument to test the
robustness of NR-IQA. However, current attack methods of NR-IQA heavily rely on
the gradient of the NR-IQA model, leading to limitations when the gradient
information is unavailable. In this paper, we present a pioneering query-based
black box attack against NR-IQA methods. We propose the concept of \emph{score
boundary} and leverage an adaptive iterative approach with multiple score
boundaries. Meanwhile, the initial attack directions are also designed to
leverage the characteristics of the Human Visual System (HVS). Experiments show
our attack method outperforms all compared state-of-the-art methods and is far
ahead of previous black-box methods. The effective DBCNN model suffers a
Spearman rank-order correlation coefficient (SROCC) decline of $0.6972$
attacked by our method, revealing the vulnerability of NR-IQA to black-box
attacks. The proposed attack method also provides a potent tool for further
exploration into NR-IQA robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chenxi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yujia Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dingquan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+jiang_T/0/1/0/all/0/1&quot;&gt;Tingting jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05224">
<title>Do Vision and Language Encoders Represent the World Similarly?. (arXiv:2401.05224v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.05224</link>
<description rdf:parseType="Literal">&lt;p&gt;Aligned text-image encoders such as CLIP have become the de facto model for
vision-language tasks. Furthermore, modality-specific encoders achieve
impressive performances in their respective domains. This raises a central
question: does an alignment exist between uni-modal vision and language
encoders since they fundamentally represent the same physical world? Analyzing
the latent spaces structure of vision and language models on image-caption
benchmarks using the Centered Kernel Alignment (CKA), we find that the
representation spaces of unaligned and aligned encoders are semantically
similar. In the absence of statistical similarity in aligned encoders like
CLIP, we show that a possible matching of unaligned encoders exists without any
training. We frame this as a seeded graph-matching problem exploiting the
semantic similarity between graphs and propose two methods - a Fast Quadratic
Assignment Problem optimization, and a novel localized CKA metric-based
matching/retrieval. We demonstrate the effectiveness of this on several
downstream tasks including cross-lingual, cross-domain caption matching and
image classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maniparambil_M/0/1/0/all/0/1&quot;&gt;Mayug Maniparambil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akshulakov_R/0/1/0/all/0/1&quot;&gt;Raiymbek Akshulakov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Djilali_Y/0/1/0/all/0/1&quot;&gt;Yasser Abdelaziz Dahou Djilali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narayan_S/0/1/0/all/0/1&quot;&gt;Sanath Narayan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seddik_M/0/1/0/all/0/1&quot;&gt;Mohamed El Amine Seddik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mangalam_K/0/1/0/all/0/1&quot;&gt;Karttikeya Mangalam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OConnor_N/0/1/0/all/0/1&quot;&gt;Noel E. O&amp;#x27;Connor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05232">
<title>Measuring Natural Scenes SFR of Automotive Fisheye Cameras. (arXiv:2401.05232v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.05232</link>
<description rdf:parseType="Literal">&lt;p&gt;The Modulation Transfer Function (MTF) is an important image quality metric
typically used in the automotive domain. However, despite the fact that optical
quality has an impact on the performance of computer vision in vehicle
automation, for many public datasets, this metric is unknown. Additionally,
wide field-of-view (FOV) cameras have become increasingly popular, particularly
for low-speed vehicle automation applications. To investigate image quality in
datasets, this paper proposes an adaptation of the Natural Scenes Spatial
Frequency Response (NS-SFR) algorithm to suit cameras with a wide
field-of-view.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jakab_D/0/1/0/all/0/1&quot;&gt;Daniel Jakab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grua_E/0/1/0/all/0/1&quot;&gt;Eoin Martino Grua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deegan_B/0/1/0/all/0/1&quot;&gt;Brian Micheal Deegan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scanlan_A/0/1/0/all/0/1&quot;&gt;Anthony Scanlan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ven_P/0/1/0/all/0/1&quot;&gt;Pepijn Van De Ven&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eising_C/0/1/0/all/0/1&quot;&gt;Ciar&amp;#xe1;n Eising&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05236">
<title>Structure from Duplicates: Neural Inverse Graphics from a Pile of Objects. (arXiv:2401.05236v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.05236</link>
<description rdf:parseType="Literal">&lt;p&gt;Our world is full of identical objects (\emphe.g., cans of coke, cars of same
model). These duplicates, when seen together, provide additional and strong
cues for us to effectively reason about 3D. Inspired by this observation, we
introduce Structure from Duplicates (SfD), a novel inverse graphics framework
that reconstructs geometry, material, and illumination from a single image
containing multiple identical objects. SfD begins by identifying multiple
instances of an object within an image, and then jointly estimates the 6DoF
pose for all instances.An inverse graphics pipeline is subsequently employed to
jointly reason about the shape, material of the object, and the environment
light, while adhering to the shared geometry and material constraint across
instances. Our primary contributions involve utilizing object duplicates as a
robust prior for single-image inverse graphics and proposing an in-plane
rotation-robust Structure from Motion (SfM) formulation for joint 6-DoF object
pose estimation. By leveraging multi-view cues from a single image, SfD
generates more realistic and detailed 3D reconstructions, significantly
outperforming existing single image reconstruction models and multi-view
reconstruction approaches with a similar or greater number of observations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_T/0/1/0/all/0/1&quot;&gt;Tianhang Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1&quot;&gt;Wei-Chiu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_K/0/1/0/all/0/1&quot;&gt;Kaiyu Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1&quot;&gt;Antonio Torralba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shenlong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05252">
<title>PIXART-{\delta}: Fast and Controllable Image Generation with Latent Consistency Models. (arXiv:2401.05252v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.05252</link>
<description rdf:parseType="Literal">&lt;p&gt;This technical report introduces PIXART-{\delta}, a text-to-image synthesis
framework that integrates the Latent Consistency Model (LCM) and ControlNet
into the advanced PIXART-{\alpha} model. PIXART-{\alpha} is recognized for its
ability to generate high-quality images of 1024px resolution through a
remarkably efficient training process. The integration of LCM in
PIXART-{\delta} significantly accelerates the inference speed, enabling the
production of high-quality images in just 2-4 steps. Notably, PIXART-{\delta}
achieves a breakthrough 0.5 seconds for generating 1024x1024 pixel images,
marking a 7x improvement over the PIXART-{\alpha}. Additionally,
PIXART-{\delta} is designed to be efficiently trainable on 32GB V100 GPUs
within a single day. With its 8-bit inference capability (von Platen et al.,
2023), PIXART-{\delta} can synthesize 1024px images within 8GB GPU memory
constraints, greatly enhancing its usability and accessibility. Furthermore,
incorporating a ControlNet-like module enables fine-grained control over
text-to-image diffusion models. We introduce a novel ControlNet-Transformer
architecture, specifically tailored for Transformers, achieving explicit
controllability alongside high-quality image generation. As a state-of-the-art,
open-source image generation model, PIXART-{\delta} offers a promising
alternative to the Stable Diffusion family of models, contributing
significantly to text-to-image synthesis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junsong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yue Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1&quot;&gt;Simian Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1&quot;&gt;Enze Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1&quot;&gt;Sayak Paul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenguo Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05293">
<title>Score Distillation Sampling with Learned Manifold Corrective. (arXiv:2401.05293v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.05293</link>
<description rdf:parseType="Literal">&lt;p&gt;Score Distillation Sampling (SDS) is a recent but already widely popular
method that relies on an image diffusion model to control optimization problems
using text prompts. In this paper, we conduct an in-depth analysis of the SDS
loss function, identify an inherent problem with its formulation, and propose a
surprisingly easy but effective fix. Specifically, we decompose the loss into
different factors and isolate the component responsible for noisy gradients. In
the original formulation, high text guidance is used to account for the noise,
leading to unwanted side effects. Instead, we train a shallow network mimicking
the timestep-dependent denoising deficiency of the image diffusion model in
order to effectively factor it out. We demonstrate the versatility and the
effectiveness of our novel loss formulation through several qualitative and
quantitative experiments, including optimization-based image synthesis and
editing, zero-shot image translation network training, and text-to-3D
synthesis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alldieck_T/0/1/0/all/0/1&quot;&gt;Thiemo Alldieck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolotouros_N/0/1/0/all/0/1&quot;&gt;Nikos Kolotouros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sminchisescu_C/0/1/0/all/0/1&quot;&gt;Cristian Sminchisescu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05294">
<title>Enhanced Muscle and Fat Segmentation for CT-Based Body Composition Analysis: A Comparative Study. (arXiv:2401.05294v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.05294</link>
<description rdf:parseType="Literal">&lt;p&gt;Purpose: Body composition measurements from routine abdominal CT can yield
personalized risk assessments for asymptomatic and diseased patients. In
particular, attenuation and volume measures of muscle and fat are associated
with important clinical outcomes, such as cardiovascular events, fractures, and
death. This study evaluates the reliability of an Internal tool for the
segmentation of muscle and fat (subcutaneous and visceral) as compared to the
well-established public TotalSegmentator tool.
&lt;/p&gt;
&lt;p&gt;Methods: We assessed the tools across 900 CT series from the publicly
available SAROS dataset, focusing on muscle, subcutaneous fat, and visceral
fat. The Dice score was employed to assess accuracy in subcutaneous fat and
muscle segmentation. Due to the lack of ground truth segmentations for visceral
fat, Cohen&apos;s Kappa was utilized to assess segmentation agreement between the
tools.
&lt;/p&gt;
&lt;p&gt;Results: Our Internal tool achieved a 3% higher Dice (83.8 vs. 80.8) for
subcutaneous fat and a 5% improvement (87.6 vs. 83.2) for muscle segmentation
respectively. A Wilcoxon signed-rank test revealed that our results were
statistically different with p&amp;lt;0.01. For visceral fat, the Cohen&apos;s kappa score
of 0.856 indicated near-perfect agreement between the two tools. Our internal
tool also showed very strong correlations for muscle volume (R^2=0.99), muscle
attenuation (R^2=0.93), and subcutaneous fat volume (R^2=0.99) with a moderate
correlation for subcutaneous fat attenuation (R^2=0.45).
&lt;/p&gt;
&lt;p&gt;Conclusion: Our findings indicated that our Internal tool outperformed
TotalSegmentator in measuring subcutaneous fat and muscle. The high Cohen&apos;s
Kappa score for visceral fat suggests a reliable level of agreement between the
two tools. These results demonstrate the potential of our tool in advancing the
accuracy of body composition analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_B/0/1/0/all/0/1&quot;&gt;Benjamin Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mathai_T/0/1/0/all/0/1&quot;&gt;Tejas Sudharshan Mathai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jianfei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parnell_C/0/1/0/all/0/1&quot;&gt;Christopher Parnell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Summers_R/0/1/0/all/0/1&quot;&gt;Ronald M. Summers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05308">
<title>Strategic Client Selection to Address Non-IIDness in HAPS-enabled FL Networks. (arXiv:2401.05308v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2401.05308</link>
<description rdf:parseType="Literal">&lt;p&gt;The deployment of federated learning (FL) within vertical heterogeneous
networks, such as those enabled by high-altitude platform station (HAPS),
offers the opportunity to engage a wide array of clients, each endowed with
distinct communication and computational capabilities. This diversity not only
enhances the training accuracy of FL models but also hastens their convergence.
Yet, applying FL in these expansive networks presents notable challenges,
particularly the significant non-IIDness in client data distributions. Such
data heterogeneity often results in slower convergence rates and reduced
effectiveness in model training performance. Our study introduces a client
selection strategy tailored to address this issue, leveraging user network
traffic behaviour. This strategy involves the prediction and classification of
clients based on their network usage patterns while prioritizing user privacy.
By strategically selecting clients whose data exhibit similar patterns for
participation in FL training, our approach fosters a more uniform and
representative data distribution across the network. Our simulations
demonstrate that this targeted client selection methodology significantly
reduces the training loss of FL models in HAPS networks, thereby effectively
tackling a crucial challenge in implementing large-scale FL systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farajzadeh_A/0/1/0/all/0/1&quot;&gt;Amin Farajzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yadav_A/0/1/0/all/0/1&quot;&gt;Animesh Yadav&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yanikomeroglu_H/0/1/0/all/0/1&quot;&gt;Halim Yanikomeroglu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05314">
<title>ANIM-400K: A Large-Scale Dataset for Automated End-To-End Dubbing of Video. (arXiv:2401.05314v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2401.05314</link>
<description rdf:parseType="Literal">&lt;p&gt;The Internet&apos;s wealth of content, with up to 60% published in English,
starkly contrasts the global population, where only 18.8% are English speakers,
and just 5.1% consider it their native language, leading to disparities in
online information access. Unfortunately, automated processes for dubbing of
video - replacing the audio track of a video with a translated alternative -
remains a complex and challenging task due to pipelines, necessitating precise
timing, facial movement synchronization, and prosody matching. While end-to-end
dubbing offers a solution, data scarcity continues to impede the progress of
both end-to-end and pipeline-based methods. In this work, we introduce
Anim-400K, a comprehensive dataset of over 425K aligned animated video segments
in Japanese and English supporting various video-related tasks, including
automated dubbing, simultaneous translation, guided video summarization, and
genre/theme/style classification. Our dataset is made publicly available for
research purposes at https://github.com/davidmchan/Anim400K.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cai_K/0/1/0/all/0/1&quot;&gt;Kevin Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chonghua Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chan_D/0/1/0/all/0/1&quot;&gt;David M. Chan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05334">
<title>URHand: Universal Relightable Hands. (arXiv:2401.05334v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.05334</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing photorealistic relightable hand models require extensive
identity-specific observations in different views, poses, and illuminations,
and face challenges in generalizing to natural illuminations and novel
identities. To bridge this gap, we present URHand, the first universal
relightable hand model that generalizes across viewpoints, poses,
illuminations, and identities. Our model allows few-shot personalization using
images captured with a mobile phone, and is ready to be photorealistically
rendered under novel illuminations. To simplify the personalization process
while retaining photorealism, we build a powerful universal relightable prior
based on neural relighting from multi-view images of hands captured in a light
stage with hundreds of identities. The key challenge is scaling the
cross-identity training while maintaining personalized fidelity and sharp
details without compromising generalization under natural illuminations. To
this end, we propose a spatially varying linear lighting model as the neural
renderer that takes physics-inspired shading as input feature. By removing
non-linear activations and bias, our specifically designed lighting model
explicitly keeps the linearity of light transport. This enables single-stage
training from light-stage data while generalizing to real-time rendering under
arbitrary continuous illuminations across diverse identities. In addition, we
introduce the joint learning of a physically based model and our neural
relighting model, which further improves fidelity and generalization. Extensive
experiments show that our approach achieves superior performance over existing
methods in terms of both quality and generalizability. We also demonstrate
quick personalization of URHand from a short phone scan of an unseen identity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhaoxi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_G/0/1/0/all/0/1&quot;&gt;Gyeongsik Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1&quot;&gt;Kaiwen Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1&quot;&gt;Chen Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pidhorskyi_S/0/1/0/all/0/1&quot;&gt;Stanislav Pidhorskyi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simon_T/0/1/0/all/0/1&quot;&gt;Tomas Simon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1&quot;&gt;Rohan Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yuan Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yichen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pires_B/0/1/0/all/0/1&quot;&gt;Bernardo Pires&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1&quot;&gt;He Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Evans_L/0/1/0/all/0/1&quot;&gt;Lucas Evans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1&quot;&gt;Bo Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buffalini_J/0/1/0/all/0/1&quot;&gt;Julia Buffalini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trimble_A/0/1/0/all/0/1&quot;&gt;Autumn Trimble&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McPhail_K/0/1/0/all/0/1&quot;&gt;Kevyn McPhail&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schoeller_M/0/1/0/all/0/1&quot;&gt;Melissa Schoeller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Shoou-I Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romero_J/0/1/0/all/0/1&quot;&gt;Javier Romero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zollhofer_M/0/1/0/all/0/1&quot;&gt;Michael Zollh&amp;#xf6;fer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheikh_Y/0/1/0/all/0/1&quot;&gt;Yaser Sheikh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saito_S/0/1/0/all/0/1&quot;&gt;Shunsuke Saito&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05335">
<title>InseRF: Text-Driven Generative Object Insertion in Neural 3D Scenes. (arXiv:2401.05335v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.05335</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce InseRF, a novel method for generative object insertion in the
NeRF reconstructions of 3D scenes. Based on a user-provided textual description
and a 2D bounding box in a reference viewpoint, InseRF generates new objects in
3D scenes. Recently, methods for 3D scene editing have been profoundly
transformed, owing to the use of strong priors of text-to-image diffusion
models in 3D generative modeling. Existing methods are mostly effective in
editing 3D scenes via style and appearance changes or removing existing
objects. Generating new objects, however, remains a challenge for such methods,
which we address in this study. Specifically, we propose grounding the 3D
object insertion to a 2D object insertion in a reference view of the scene. The
2D edit is then lifted to 3D using a single-view object reconstruction method.
The reconstructed object is then inserted into the scene, guided by the priors
of monocular depth estimation methods. We evaluate our method on various 3D
scenes and provide an in-depth analysis of the proposed components. Our
experiments with generative insertion of objects in several 3D scenes indicate
the effectiveness of our method compared to the existing methods. InseRF is
capable of controllable and 3D-consistent object insertion without requiring
explicit 3D information as input. Please visit our project page at
https://mohamad-shahbazi.github.io/inserf.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahbazi_M/0/1/0/all/0/1&quot;&gt;Mohamad Shahbazi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Claessens_L/0/1/0/all/0/1&quot;&gt;Liesbeth Claessens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niemeyer_M/0/1/0/all/0/1&quot;&gt;Michael Niemeyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Collins_E/0/1/0/all/0/1&quot;&gt;Edo Collins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tonioni_A/0/1/0/all/0/1&quot;&gt;Alessio Tonioni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1&quot;&gt;Luc Van Gool&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1&quot;&gt;Federico Tombari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05336">
<title>Towards Online Sign Language Recognition and Translation. (arXiv:2401.05336v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.05336</link>
<description rdf:parseType="Literal">&lt;p&gt;The objective of sign language recognition is to bridge the communication gap
between the deaf and the hearing. Numerous previous works train their models
using the well-established connectionist temporal classification (CTC) loss.
During the inference stage, the CTC-based models typically take the entire sign
video as input to make predictions. This type of inference scheme is referred
to as offline recognition. In contrast, while mature speech recognition systems
can efficiently recognize spoken words on the fly, sign language recognition
still falls short due to the lack of practical online solutions. In this work,
we take the first step towards filling this gap. Our approach comprises three
phases: 1) developing a sign language dictionary encompassing all glosses
present in a target sign language dataset; 2) training an isolated sign
language recognition model on augmented signs using both conventional
classification loss and our novel saliency loss; 3) employing a sliding window
approach on the input sign sequence and feeding each sign clip to the
well-optimized model for online recognition. Furthermore, our online
recognition model can be extended to boost the performance of any offline
model, and to support online translation by appending a gloss-to-text network
onto the recognition model. By integrating our online framework with the
previously best-performing offline model, TwoStream-SLR, we achieve new
state-of-the-art performance on three benchmarks: Phoenix-2014, Phoenix-2014T,
and CSL-Daily. Code and models will be available at
https://github.com/FangyunWei/SLRT
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_R/0/1/0/all/0/1&quot;&gt;Ronglai Zuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1&quot;&gt;Fangyun Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mak_B/0/1/0/all/0/1&quot;&gt;Brian Mak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.11018">
<title>A Theoretical View of Linear Backpropagation and Its Convergence. (arXiv:2112.11018v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2112.11018</link>
<description rdf:parseType="Literal">&lt;p&gt;Backpropagation (BP) is widely used for calculating gradients in deep neural
networks (DNNs). Applied often along with stochastic gradient descent (SGD) or
its variants, BP is considered as a de-facto choice in a variety of machine
learning tasks including DNN training and adversarial attack/defense. Recently,
a linear variant of BP named LinBP was introduced for generating more
transferable adversarial examples for performing black-box attacks, by Guo et
al. Although it has been shown empirically effective in black-box attacks,
theoretical studies and convergence analyses of such a method is lacking. This
paper serves as a complement and somewhat an extension to Guo et al.&apos;s paper,
by providing theoretical analyses on LinBP in neural-network-involved learning
tasks, including adversarial attack and model training. We demonstrate that,
somewhat surprisingly, LinBP can lead to faster convergence in these tasks in
the same hyper-parameter settings, compared to BP. We confirm our theoretical
results with extensive experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Ziang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yiwen Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haodi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Changshui Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.14865">
<title>Prompt-aligned Gradient for Prompt Tuning. (arXiv:2205.14865v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2205.14865</link>
<description rdf:parseType="Literal">&lt;p&gt;Thanks to the large pre-trained vision-language models (VLMs) like CLIP, we
can craft a zero-shot classifier by &quot;prompt&quot;, e.g., the confidence score of an
image being &quot;[CLASS]&quot; can be obtained by using the VLM provided similarity
measure between the image and the prompt sentence &quot;a photo of a [CLASS]&quot;.
Therefore, prompt shows a great potential for fast adaptation of VLMs to
downstream tasks if we fine-tune the prompt-based similarity measure. However,
we find a common failure that improper fine-tuning may not only undermine the
prompt&apos;s inherent prediction for the task-related classes, but also for other
classes in the VLM vocabulary. Existing methods still address this problem by
using traditional anti-overfitting techniques such as early stopping and data
augmentation, which lack a principled solution specific to prompt. We present
Prompt-aligned Gradient, dubbed ProGrad, to prevent prompt tuning from
forgetting the the general knowledge learned from VLMs. In particular, ProGrad
only updates the prompt whose gradient is aligned (or non-conflicting) to the
&quot;general direction&quot;, which is represented as the gradient of the KL loss of the
pre-defined prompt prediction. Extensive experiments demonstrate the stronger
few-shot generalization ability of ProGrad over state-of-the-art prompt tuning
methods. Codes are available at https://github.com/BeierZhu/Prompt-align.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1&quot;&gt;Beier Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1&quot;&gt;Yulei Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yucheng Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yue Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hanwang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.05927">
<title>LinK3D: Linear Keypoints Representation for 3D LiDAR Point Cloud. (arXiv:2206.05927v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.05927</link>
<description rdf:parseType="Literal">&lt;p&gt;Feature extraction and matching are the basic parts of many robotic vision
tasks, such as 2D or 3D object detection, recognition, and registration. As is
known, 2D feature extraction and matching have already achieved great success.
Unfortunately, in the field of 3D, the current methods may fail to support the
extensive application of 3D LiDAR sensors in robotic vision tasks due to their
poor descriptiveness and inefficiency. To address this limitation, we propose a
novel 3D feature representation method: Linear Keypoints representation for 3D
LiDAR point cloud, called LinK3D. The novelty of LinK3D lies in that it fully
considers the characteristics (such as the sparsity and complexity) of LiDAR
point clouds and represents the keypoint with its robust neighbor keypoints,
which provide strong constraints in the description of the keypoint. The
proposed LinK3D has been evaluated on three public datasets, and the
experimental results show that our method achieves great matching performance.
More importantly, LinK3D also shows excellent real-time performance, faster
than the sensor frame rate at 10 Hz of a typical rotating LiDAR sensor. LinK3D
only takes an average of 30 milliseconds to extract features from the point
cloud collected by a 64-beam LiDAR and takes merely about 20 milliseconds to
match two LiDAR scans when executed on a computer with an Intel Core i7
processor. Moreover, our method can be extended to LiDAR odometry task, and
shows good scalability. We release the implementation of our method at
https://github.com/YungeCui/LinK3D.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1&quot;&gt;Yunge Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yinlong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1&quot;&gt;Jiahua Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Haibo Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xieyuanli Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1&quot;&gt;Feng Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.07802">
<title>Improving generalization by mimicking the human visual diet. (arXiv:2206.07802v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.07802</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a new perspective on bridging the generalization gap between
biological and computer vision -- mimicking the human visual diet. While
computer vision models rely on internet-scraped datasets, humans learn from
limited 3D scenes under diverse real-world transformations with objects in
natural context. Our results demonstrate that incorporating variations and
contextual cues ubiquitous in the human visual training data (visual diet)
significantly improves generalization to real-world transformations such as
lighting, viewpoint, and material changes. This improvement also extends to
generalizing from synthetic to real-world data -- all models trained with a
human-like visual diet outperform specialized architectures by large margins
when tested on natural image data. These experiments are enabled by our two key
contributions: a novel dataset capturing scene context and diverse real-world
transformations to mimic the human visual diet, and a transformer model
tailored to leverage these aspects of the human visual diet. All data and
source code can be accessed at
https://github.com/Spandan-Madan/human_visual_diet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madan_S/0/1/0/all/0/1&quot;&gt;Spandan Madan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;You Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mengmi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1&quot;&gt;Hanspeter Pfister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kreiman_G/0/1/0/all/0/1&quot;&gt;Gabriel Kreiman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.10193">
<title>Convergent autoencoder approximation of low bending and low distortion manifold embeddings. (arXiv:2208.10193v2 [math.NA] UPDATED)</title>
<link>http://arxiv.org/abs/2208.10193</link>
<description rdf:parseType="Literal">&lt;p&gt;Autoencoders, which consist of an encoder and a decoder, are widely used in
machine learning for dimension reduction of high-dimensional data. The encoder
embeds the input data manifold into a lower-dimensional latent space, while the
decoder represents the inverse map, providing a parametrization of the data
manifold by the manifold in latent space. A good regularity and structure of
the embedded manifold may substantially simplify further data processing tasks
such as cluster analysis or data interpolation. We propose and analyze a novel
regularization for learning the encoder component of an autoencoder: a loss
functional that prefers isometric, extrinsically flat embeddings and allows to
train the encoder on its own. To perform the training it is assumed that for
pairs of nearby points on the input manifold their local Riemannian distance
and their local Riemannian average can be evaluated. The loss functional is
computed via Monte Carlo integration with different sampling strategies for
pairs of points on the input manifold. Our main theorem identifies a geometric
loss functional of the embedding map as the $\Gamma$-limit of the
sampling-dependent loss functionals. Numerical tests, using image data that
encodes different explicitly given data manifolds, show that smooth manifold
embeddings into latent space are obtained. Due to the promotion of extrinsic
flatness, these embeddings are regular enough such that interpolation between
not too distant points on the manifold is well approximated by linear
interpolation in latent space as one possible postprocessing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Braunsmann_J/0/1/0/all/0/1&quot;&gt;Juliane Braunsmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Rajkovic_M/0/1/0/all/0/1&quot;&gt;Marko Rajkovi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Rumpf_M/0/1/0/all/0/1&quot;&gt;Martin Rumpf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Wirth_B/0/1/0/all/0/1&quot;&gt;Benedikt Wirth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.14839">
<title>QuantNAS for super resolution: searching for efficient quantization-friendly architectures against quantization noise. (arXiv:2208.14839v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2208.14839</link>
<description rdf:parseType="Literal">&lt;p&gt;There is a constant need for high-performing and computationally efficient
neural network models for image super-resolution: computationally efficient
models can be used via low-capacity devices and reduce carbon footprints. One
way to obtain such models is to compress models, e.g. quantization. Another way
is a neural architecture search that automatically discovers new, more
efficient solutions. We propose a novel quantization-aware procedure, the
QuantNAS that combines pros of these two approaches. To make QuantNAS work, the
procedure looks for quantization-friendly super-resolution models. The approach
utilizes entropy regularization, quantization noise, and Adaptive Deviation for
Quantization (ADQ) module to enhance the search procedure. The entropy
regularization technique prioritizes a single operation within each block of
the search space. Adding quantization noise to parameters and activations
approximates model degradation after quantization, resulting in a more
quantization-friendly architectures. ADQ helps to alleviate problems caused by
Batch Norm blocks in super-resolution models. Our experimental results show
that the proposed approximations are better for search procedure than direct
model quantization. QuantNAS discovers architectures with better PSNR/BitOps
trade-off than uniform or mixed precision quantization of fixed architectures.
We showcase the effectiveness of our method through its application to two
search spaces inspired by the state-of-the-art SR models and RFDN. Thus, anyone
can design a proper search space based on an existing architecture and apply
our method to obtain better quality and efficiency.
&lt;/p&gt;
&lt;p&gt;The proposed procedure is 30\% faster than direct weight quantization and is
more stable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shvetsov_E/0/1/0/all/0/1&quot;&gt;Egor Shvetsov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osin_D/0/1/0/all/0/1&quot;&gt;Dmitry Osin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaytsev_A/0/1/0/all/0/1&quot;&gt;Alexey Zaytsev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koryakovskiy_I/0/1/0/all/0/1&quot;&gt;Ivan Koryakovskiy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buchnev_V/0/1/0/all/0/1&quot;&gt;Valentin Buchnev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trofimov_I/0/1/0/all/0/1&quot;&gt;Ilya Trofimov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1&quot;&gt;Evgeny Burnaev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.05438">
<title>Parallel Augmentation and Dual Enhancement for Occluded Person Re-identification. (arXiv:2210.05438v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.05438</link>
<description rdf:parseType="Literal">&lt;p&gt;Occluded person re-identification (Re-ID), the task of searching for the same
person&apos;s images in occluded environments, has attracted lots of attention in
the past decades. Recent approaches concentrate on improving performance on
occluded data by data/feature augmentation or using extra models to predict
occlusions. However, they ignore the imbalance problem in this task and can not
fully utilize the information from the training data. To alleviate these two
issues, we propose a simple yet effective method with Parallel Augmentation and
Dual Enhancement (PADE), which is robust on both occluded and non-occluded data
and does not require any auxiliary clues. First, we design a parallel
augmentation mechanism (PAM) to generate more suitable occluded data to
mitigate the negative effects of unbalanced data. Second, we propose the global
and local dual enhancement strategy (DES) to promote the context information
and details. Experimental results on three widely used occluded datasets and
two non-occluded datasets validate the effectiveness of our method. The code is
available at
https://github.com/littleprince1121/PADE_Parallel_Augmentation_and_Dual_Enhancement_for_Occluded_Person_ReID
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Huaibo Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_A/0/1/0/all/0/1&quot;&gt;Aihua Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenglong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1&quot;&gt;Ran He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.05158">
<title>SemPPL: Predicting pseudo-labels for better contrastive representations. (arXiv:2301.05158v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.05158</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning from large amounts of unsupervised data and a small amount of
supervision is an important open problem in computer vision. We propose a new
semi-supervised learning method, Semantic Positives via Pseudo-Labels (SemPPL),
that combines labelled and unlabelled data to learn informative
representations. Our method extends self-supervised contrastive learning --
where representations are shaped by distinguishing whether two samples
represent the same underlying datum (positives) or not (negatives) -- with a
novel approach to selecting positives. To enrich the set of positives, we
leverage the few existing ground-truth labels to predict the missing ones
through a $k$-nearest neighbours classifier by using the learned embeddings of
the labelled data. We thus extend the set of positives with datapoints having
the same pseudo-label and call these semantic positives. We jointly learn the
representation and predict bootstrapped pseudo-labels. This creates a
reinforcing cycle. Strong initial representations enable better pseudo-label
predictions which then improve the selection of semantic positives and lead to
even better representations. SemPPL outperforms competing semi-supervised
methods setting new state-of-the-art performance of $68.5\%$ and $76\%$ top-$1$
accuracy when using a ResNet-$50$ and training on $1\%$ and $10\%$ of labels on
ImageNet, respectively. Furthermore, when using selective kernels, SemPPL
significantly outperforms previous state-of-the-art achieving $72.3\%$ and
$78.3\%$ top-$1$ accuracy on ImageNet with $1\%$ and $10\%$ labels,
respectively, which improves absolute $+7.8\%$ and $+6.2\%$ over previous work.
SemPPL also exhibits state-of-the-art performance over larger ResNet models as
well as strong robustness, out-of-distribution and transfer performance. We
release the checkpoints and the evaluation code at
https://github.com/deepmind/semppl .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bosnjak_M/0/1/0/all/0/1&quot;&gt;Matko Bo&amp;#x161;njak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richemond_P/0/1/0/all/0/1&quot;&gt;Pierre H. Richemond&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomasev_N/0/1/0/all/0/1&quot;&gt;Nenad Tomasev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strub_F/0/1/0/all/0/1&quot;&gt;Florian Strub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walker_J/0/1/0/all/0/1&quot;&gt;Jacob C. Walker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1&quot;&gt;Felix Hill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buesing_L/0/1/0/all/0/1&quot;&gt;Lars Holger Buesing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1&quot;&gt;Razvan Pascanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blundell_C/0/1/0/all/0/1&quot;&gt;Charles Blundell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitrovic_J/0/1/0/all/0/1&quot;&gt;Jovana Mitrovic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.08455">
<title>Spatial Steerability of GANs via Self-Supervision from Discriminator. (arXiv:2301.08455v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.08455</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative models make huge progress to the photorealistic image synthesis in
recent years. To enable human to steer the image generation process and
customize the output, many works explore the interpretable dimensions of the
latent space in GANs. Existing methods edit the attributes of the output image
such as orientation or color scheme by varying the latent code along certain
directions. However, these methods usually require additional human annotations
for each pretrained model, and they mostly focus on editing global attributes.
In this work, we propose a self-supervised approach to improve the spatial
steerability of GANs without searching for steerable directions in the latent
space or requiring extra annotations. Specifically, we design randomly sampled
Gaussian heatmaps to be encoded into the intermediate layers of generative
models as spatial inductive bias. Along with training the GAN model from
scratch, these heatmaps are being aligned with the emerging attention of the
GAN&apos;s discriminator in a self-supervised learning manner. During inference,
users can interact with the spatial heatmaps in an intuitive manner, enabling
them to edit the output image by adjusting the scene layout, moving, or
removing objects. Moreover, we incorporate DragGAN into our framework, which
facilitates fine-grained manipulation within a reasonable time and supports a
coarse-to-fine editing process. Extensive experiments show that the proposed
method not only enables spatial editing over human faces, animal faces, outdoor
scenes, and complicated multi-object indoor scenes but also brings improvement
in synthesis quality. Code, models, and demo video are available at
https://genforce.github.io/SpatialGAN/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhagat_L/0/1/0/all/0/1&quot;&gt;Lalit Bhagat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Ceyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yinghao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yujun Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongdong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Bolei Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.11510">
<title>Selective experience replay compression using coresets for lifelong deep reinforcement learning in medical imaging. (arXiv:2302.11510v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.11510</link>
<description rdf:parseType="Literal">&lt;p&gt;Selective experience replay is a popular strategy for integrating lifelong
learning with deep reinforcement learning. Selective experience replay aims to
recount selected experiences from previous tasks to avoid catastrophic
forgetting. Furthermore, selective experience replay based techniques are model
agnostic and allow experiences to be shared across different models. However,
storing experiences from all previous tasks make lifelong learning using
selective experience replay computationally very expensive and impractical as
the number of tasks increase. To that end, we propose a reward
distribution-preserving coreset compression technique for compressing
experience replay buffers stored for selective experience replay.
&lt;/p&gt;
&lt;p&gt;We evaluated the coreset compression technique on the brain tumor
segmentation (BRATS) dataset for the task of ventricle localization and on the
whole-body MRI for localization of left knee cap, left kidney, right
trochanter, left lung, and spleen. The coreset lifelong learning models trained
on a sequence of 10 different brain MR imaging environments demonstrated
excellent performance localizing the ventricle with a mean pixel error distance
of 12.93 for the compression ratio of 10x. In comparison, the conventional
lifelong learning model localized the ventricle with a mean pixel distance of
10.87. Similarly, the coreset lifelong learning models trained on whole-body
MRI demonstrated no significant difference (p=0.28) between the 10x compressed
coreset lifelong learning models and conventional lifelong learning models for
all the landmarks. The mean pixel distance for the 10x compressed models across
all the landmarks was 25.30, compared to 19.24 for the conventional lifelong
learning models. Our results demonstrate that the potential of the
coreset-based ERB compression method for compressing experiences without a
significant drop in performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1&quot;&gt;Guangyao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Samson Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braverman_V/0/1/0/all/0/1&quot;&gt;Vladimir Braverman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacobs_M/0/1/0/all/0/1&quot;&gt;Michael A. Jacobs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parekh_V/0/1/0/all/0/1&quot;&gt;Vishwa S. Parekh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.03301">
<title>Exploring Deep Models for Practical Gait Recognition. (arXiv:2303.03301v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.03301</link>
<description rdf:parseType="Literal">&lt;p&gt;Gait recognition is a rapidly advancing vision technique for person
identification from a distance. Prior studies predominantly employed relatively
shallow networks to extract subtle gait features, achieving impressive
successes in constrained settings. Nevertheless, experiments revealed that
existing methods mostly produce unsatisfactory results when applied to newly
released real-world gait datasets. This paper presents a unified perspective to
explore how to construct deep models for state-of-the-art outdoor gait
recognition, including the classical CNN-based and emerging Transformer-based
architectures. Specifically, we challenge the stereotype of shallow gait models
and demonstrate the superiority of explicit temporal modeling and deep
transformer structure for discriminative gait representation learning.
Consequently, the proposed CNN-based DeepGaitV2 series and Transformer-based
SwinGait series exhibit significant performance improvements on Gait3D and
GREW. As for the constrained gait datasets, the DeepGaitV2 series also reaches
a new state-of-the-art in most cases, convincingly showing its practicality and
generality. The source code is available at
https://github.com/ShiqiYu/OpenGait.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1&quot;&gt;Chao Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_S/0/1/0/all/0/1&quot;&gt;Saihui Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yongzhen Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Shiqi Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.06783">
<title>Asynchronous Decentralized Federated Lifelong Learning for Landmark Localization in Medical Imaging. (arXiv:2303.06783v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.06783</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning is a recent development in the machine learning area that
allows a system of devices to train on one or more tasks without sharing their
data to a single location or device. However, this framework still requires a
centralized global model to consolidate individual models into one, and the
devices train synchronously, which both can be potential bottlenecks for using
federated learning. In this paper, we propose a novel method of asynchronous
decentralized federated lifelong learning (ADFLL) method that inherits the
merits of federated learning and can train on multiple tasks simultaneously
without the need for a central node or synchronous training. Thus, overcoming
the potential drawbacks of conventional federated learning. We demonstrate
excellent performance on the brain tumor segmentation (BRATS) dataset for
localizing the left ventricle on multiple image sequences and image
orientation. Our framework allows agents to achieve the best performance with a
mean distance error of 7.81, better than the conventional all-knowing agent&apos;s
mean distance error of 11.78, and significantly (p=0.01) better than a
conventional lifelong learning agent with a distance error of 15.17 after eight
rounds of training. In addition, all ADFLL agents have comparable or better
performance than a conventional LL agent. In conclusion, we developed an ADFLL
framework with excellent performance and speed-up compared to conventional RL
agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1&quot;&gt;Guangyao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacobs_M/0/1/0/all/0/1&quot;&gt;Michael A. Jacobs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braverman_V/0/1/0/all/0/1&quot;&gt;Vladimir Braverman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parekh_V/0/1/0/all/0/1&quot;&gt;Vishwa S. Parekh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.09171">
<title>Empowering CAM-based Methods with Capability to Generate Fine-Grained and High-Faithfulness Explanations. (arXiv:2303.09171v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.09171</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the explanation of neural network models has garnered considerable
research attention. In computer vision, CAM (Class Activation Map)-based
methods and LRP (Layer-wise Relevance Propagation) method are two common
explanation methods. However, since most CAM-based methods can only generate
global weights, they can only generate coarse-grained explanations at a deep
layer. LRP and its variants, on the other hand, can generate fine-grained
explanations. But the faithfulness of the explanations is too low. To address
these challenges, in this paper, we propose FG-CAM (Fine-Grained CAM), which
extends CAM-based methods to enable generating fine-grained and
high-faithfulness explanations. FG-CAM uses the relationship between two
adjacent layers of feature maps with resolution differences to gradually
increase the explanation resolution, while finding the contributing pixels and
filtering out the pixels that do not contribute. Our method not only solves the
shortcoming of CAM-based methods without changing their characteristics, but
also generates fine-grained explanations that have higher faithfulness than LRP
and its variants. We also present FG-CAM with denoising, which is a variant of
FG-CAM and is able to generate less noisy explanations with almost no change in
explanation faithfulness. Experimental results show that the performance of
FG-CAM is almost unaffected by the explanation resolution. FG-CAM outperforms
existing CAM-based methods significantly in both shallow and intermediate
layers, and outperforms LRP and its variations significantly in the input
layer. Our code is available at https://github.com/dongmo-qcq/FG-CAM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_C/0/1/0/all/0/1&quot;&gt;Changqing Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_F/0/1/0/all/0/1&quot;&gt;Fusheng Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yining Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.16666">
<title>SC-VAE: Sparse Coding-based Variational Autoencoder with Learned ISTA. (arXiv:2303.16666v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.16666</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning rich data representations from unlabeled data is a key challenge
towards applying deep learning algorithms in downstream tasks. Several variants
of variational autoencoders (VAEs) have been proposed to learn compact data
representations by encoding high-dimensional data in a lower dimensional space.
Two main classes of VAEs methods may be distinguished depending on the
characteristics of the meta-priors that are enforced in the representation
learning step. The first class of methods derives a continuous encoding by
assuming a static prior distribution in the latent space. The second class of
methods learns instead a discrete latent representation using vector
quantization (VQ) along with a codebook. However, both classes of methods
suffer from certain challenges, which may lead to suboptimal image
reconstruction results. The first class suffers from posterior collapse,
whereas the second class suffers from codebook collapse. To address these
challenges, we introduce a new VAE variant, termed sparse coding-based VAE with
learned ISTA (SC-VAE), which integrates sparse coding within variational
autoencoder framework. The proposed method learns sparse data representations
that consist of a linear combination of a small number of predetermined
orthogonal atoms. The sparse coding problem is solved using a learnable version
of the iterative shrinkage thresholding algorithm (ISTA). Experiments on two
image datasets demonstrate that our model achieves improved image
reconstruction results compared to state-of-the-art methods. Moreover, we
demonstrate that the use of learned sparse code vectors allows us to perform
downstream tasks like image generation and unsupervised image segmentation
through clustering image patches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_P/0/1/0/all/0/1&quot;&gt;Pan Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_P/0/1/0/all/0/1&quot;&gt;Peijie Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ha_S/0/1/0/all/0/1&quot;&gt;Sungmin Ha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bani_A/0/1/0/all/0/1&quot;&gt;Abdalla Bani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shuang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sotiras_A/0/1/0/all/0/1&quot;&gt;Aristeidis Sotiras&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.12707">
<title>Lyapunov-Stable Deep Equilibrium Models. (arXiv:2304.12707v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.12707</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep equilibrium (DEQ) models have emerged as a promising class of implicit
layer models, which abandon traditional depth by solving for the fixed points
of a single nonlinear layer. Despite their success, the stability of the fixed
points for these models remains poorly understood. By considering DEQ models as
nonlinear dynamic systems, we propose a robust DEQ model named LyaDEQ with
guaranteed provable stability via Lyapunov theory. The crux of our method is
ensuring the Lyapunov stability of the DEQ model&apos;s fixed points, which enables
the proposed model to resist minor initial perturbations. To avoid poor
adversarial defense due to Lyapunov-stable fixed points being located near each
other, we orthogonalize the layers after the Lyapunov stability module to
separate different fixed points. We evaluate LyaDEQ models under well-known
adversarial attacks, and experimental results demonstrate significant
improvement in robustness. Furthermore, we show that the LyaDEQ model can be
combined with other defense methods, such as adversarial training, to achieve
even better adversarial robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_H/0/1/0/all/0/1&quot;&gt;Haoyu Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1&quot;&gt;Shikui Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Ting Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miyatake_Y/0/1/0/all/0/1&quot;&gt;Yuto Miyatake&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.13014">
<title>Methods and datasets for segmentation of minimally invasive surgical instruments in endoscopic images and videos: A review of the state of the art. (arXiv:2304.13014v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.13014</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of computer- and robot-assisted minimally invasive surgery,
enormous progress has been made in recent years based on the recognition of
surgical instruments in endoscopic images and videos. In particular, the
determination of the position and type of instruments is of great interest.
Current work involves both spatial and temporal information, with the idea that
predicting the movement of surgical tools over time may improve the quality of
the final segmentations. The provision of publicly available datasets has
recently encouraged the development of new methods, mainly based on deep
learning. In this review, we identify and characterize datasets used for method
development and evaluation and quantify their frequency of use in the
literature. We further present an overview of the current state of research
regarding the segmentation and tracking of minimally invasive surgical
instruments in endoscopic images and videos. The paper focuses on methods that
work purely visually, without markers of any kind attached to the instruments,
considering both single-frame semantic and instance segmentation approaches, as
well as those that incorporate temporal information. The publications analyzed
were identified through the platforms Google Scholar, Web of Science, and
PubMed. The search terms used were &quot;instrument segmentation&quot;, &quot;instrument
tracking&quot;, &quot;surgical tool segmentation&quot;, and &quot;surgical tool tracking&quot;,
resulting in a total of 741 articles published between 01/2015 and 07/2023, of
which 123 were included using systematic selection criteria. A discussion of
the reviewed literature is provided, highlighting existing shortcomings and
emphasizing the available potential for future developments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rueckert_T/0/1/0/all/0/1&quot;&gt;Tobias Rueckert&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1&quot;&gt;Daniel Rueckert&lt;/a&gt; (2 and 3), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palm_C/0/1/0/all/0/1&quot;&gt;Christoph Palm&lt;/a&gt; (1 and 4) ((1) Regensburg Medical Image Computing (ReMIC), Ostbayerische Technische Hochschule Regensburg (OTH Regensburg), Germany, (2) Artificial Intelligence in Healthcare and Medicine, Klinikum rechts der Isar, Technical University of Munich, Germany, (3) Department of Computing, Imperial College London, UK, (4) Regensburg Center of Health Sciences and Technology (RCHST), OTH Regensburg, Germany)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05400">
<title>Investigating the Corruption Robustness of Image Classifiers with Random Lp-norm Corruptions. (arXiv:2305.05400v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05400</link>
<description rdf:parseType="Literal">&lt;p&gt;Robustness is a fundamental property of machine learning classifiers required
to achieve safety and reliability. In the field of adversarial robustness of
image classifiers, robustness is commonly defined as the stability of a model
to all input changes within a p-norm distance. However, in the field of random
corruption robustness, variations observed in the real world are used, while
p-norm corruptions are rarely considered. This study investigates the use of
random p-norm corruptions to augment the training and test data of image
classifiers. We evaluate the model robustness against imperceptible random
p-norm corruptions and propose a novel robustness metric. We empirically
investigate whether robustness transfers across different p-norms and derive
conclusions on which p-norm corruptions a model should be trained and
evaluated. We find that training data augmentation with a combination of p-norm
corruptions significantly improves corruption robustness, even on top of
state-of-the-art data augmentation schemes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siedel_G/0/1/0/all/0/1&quot;&gt;Georg Siedel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1&quot;&gt;Weijia Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vock_S/0/1/0/all/0/1&quot;&gt;Silvia Vock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morozov_A/0/1/0/all/0/1&quot;&gt;Andrey Morozov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14955">
<title>DC-Net: Divide-and-Conquer for Salient Object Detection. (arXiv:2305.14955v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14955</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce Divide-and-Conquer into the salient object
detection (SOD) task to enable the model to learn prior knowledge that is for
predicting the saliency map. We design a novel network, Divide-and-Conquer
Network (DC-Net) which uses two encoders to solve different subtasks that are
conducive to predicting the final saliency map, here is to predict the edge
maps with width 4 and location maps of salient objects and then aggregate the
feature maps with different semantic information into the decoder to predict
the final saliency map. The decoder of DC-Net consists of our newly designed
two-level Residual nested-ASPP (ResASPP$^{2}$) modules, which have the ability
to capture a large number of different scale features with a small number of
convolution operations and have the advantages of maintaining high resolution
all the time and being able to obtain a large and compact effective receptive
field (ERF). Based on the advantage of Divide-and-Conquer&apos;s parallel computing,
we use Parallel Acceleration to speed up DC-Net, allowing it to achieve
competitive performance on six LR-SOD and five HR-SOD datasets under high
efficiency (60 FPS and 55 FPS). Codes and results are available:
https://github.com/PiggyJerry/DC-Net.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jiayi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1&quot;&gt;Xuebin Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elsaddik_A/0/1/0/all/0/1&quot;&gt;Abdulmotaleb Elsaddik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16713">
<title>ReConPatch : Contrastive Patch Representation Learning for Industrial Anomaly Detection. (arXiv:2305.16713v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16713</link>
<description rdf:parseType="Literal">&lt;p&gt;Anomaly detection is crucial to the advanced identification of product
defects such as incorrect parts, misaligned components, and damages in
industrial manufacturing. Due to the rare observations and unknown types of
defects, anomaly detection is considered to be challenging in machine learning.
To overcome this difficulty, recent approaches utilize the common visual
representations pre-trained from natural image datasets and distill the
relevant features. However, existing approaches still have the discrepancy
between the pre-trained feature and the target data, or require the input
augmentation which should be carefully designed, particularly for the
industrial dataset. In this paper, we introduce ReConPatch, which constructs
discriminative features for anomaly detection by training a linear modulation
of patch features extracted from the pre-trained model. ReConPatch employs
contrastive representation learning to collect and distribute features in a way
that produces a target-oriented and easily separable representation. To address
the absence of labeled pairs for the contrastive learning, we utilize two
similarity measures between data representations, pairwise and contextual
similarities, as pseudo-labels. Our method achieves the state-of-the-art
anomaly detection performance (99.72%) for the widely used and challenging
MVTec AD dataset. Additionally, we achieved a state-of-the-art anomaly
detection performance (95.8%) for the BTAD dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hyun_J/0/1/0/all/0/1&quot;&gt;Jeeho Hyun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sangyun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeon_G/0/1/0/all/0/1&quot;&gt;Giyoung Jeon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seung Hwan Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bae_K/0/1/0/all/0/1&quot;&gt;Kyunghoon Bae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1&quot;&gt;Byung Jun Kang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16963">
<title>Semantic segmentation of sparse irregular point clouds for leaf/wood discrimination. (arXiv:2305.16963v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16963</link>
<description rdf:parseType="Literal">&lt;p&gt;LiDAR (Light Detection and Ranging) has become an essential part of the
remote sensing toolbox used for biosphere monitoring. In particular, LiDAR
provides the opportunity to map forest leaf area with unprecedented accuracy,
while leaf area has remained an important source of uncertainty affecting
models of gas exchanges between the vegetation and the atmosphere. Unmanned
Aerial Vehicles (UAV) are easy to mobilize and therefore allow frequent
revisits to track the response of vegetation to climate change. However,
miniature sensors embarked on UAVs usually provide point clouds of limited
density, which are further affected by a strong decrease in density from top to
bottom of the canopy due to progressively stronger occlusion. In such a
context, discriminating leaf points from wood points presents a significant
challenge due in particular to strong class imbalance and spatially irregular
sampling intensity. Here we introduce a neural network model based on the
Pointnet ++ architecture which makes use of point geometry only (excluding any
spectral information). To cope with local data sparsity, we propose an
innovative sampling scheme which strives to preserve local important geometric
information. We also propose a loss function adapted to the severe class
imbalance. We show that our model outperforms state-of-the-art alternatives on
UAV point clouds. We discuss future possible improvements, particularly
regarding much denser point clouds acquired from below the canopy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yuchen Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durand_J/0/1/0/all/0/1&quot;&gt;Jean-Baptiste Durand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vincent_G/0/1/0/all/0/1&quot;&gt;Gr&amp;#xe9;goire Vincent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forbes_F/0/1/0/all/0/1&quot;&gt;Florence Forbes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19638">
<title>A Unified Framework for U-Net Design and Analysis. (arXiv:2305.19638v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19638</link>
<description rdf:parseType="Literal">&lt;p&gt;U-Nets are a go-to, state-of-the-art neural architecture across numerous
tasks for continuous signals on a square such as images and Partial
Differential Equations (PDE), however their design and architecture is
understudied. In this paper, we provide a framework for designing and analysing
general U-Net architectures. We present theoretical results which characterise
the role of the encoder and decoder in a U-Net, their high-resolution scaling
limits and their conjugacy to ResNets via preconditioning. We propose
Multi-ResNets, U-Nets with a simplified, wavelet-based encoder without
learnable parameters. Further, we show how to design novel U-Net architectures
which encode function constraints, natural bases, or the geometry of the data.
In diffusion models, our framework enables us to identify that high-frequency
information is dominated by noise exponentially faster, and show how U-Nets
with average pooling exploit this. In our experiments, we demonstrate how
Multi-ResNets achieve competitive and often superior performance compared to
classical U-Nets in image segmentation, PDE surrogate modelling, and generative
modelling with diffusion models. Our U-Net framework paves the way to study the
theoretical properties of U-Nets and design natural, scalable neural
architectures for a multitude of problems beyond the square.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Williams_C/0/1/0/all/0/1&quot;&gt;Christopher Williams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Falck_F/0/1/0/all/0/1&quot;&gt;Fabian Falck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Deligiannidis_G/0/1/0/all/0/1&quot;&gt;George Deligiannidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Holmes_C/0/1/0/all/0/1&quot;&gt;Chris Holmes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1&quot;&gt;Arnaud Doucet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Syed_S/0/1/0/all/0/1&quot;&gt;Saifuddin Syed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09996">
<title>Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering. (arXiv:2306.09996v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09996</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we explore effective prompting techniques to enhance zero- and
few-shot Visual Question Answering (VQA) performance in contemporary
Vision-Language Models (VLMs). Central to our investigation is the role of
question templates in guiding VLMs to generate accurate answers. We identify
that specific templates significantly influence VQA outcomes, underscoring the
need for strategic template selection. Another pivotal aspect of our study is
augmenting VLMs with image captions, providing them with additional visual cues
alongside direct image features in VQA tasks. Surprisingly, this augmentation
significantly improves the VLMs&apos; performance in many cases, even though VLMs
&quot;see&quot; the image directly! We explore chain-of-thought (CoT) reasoning and find
that while standard CoT reasoning causes drops in performance, advanced methods
like self-consistency can help recover it. Furthermore, we find that text-only
few-shot examples enhance VLMs&apos; alignment with the task format, particularly
benefiting models prone to verbose zero-shot answers. Lastly, to mitigate the
challenges associated with evaluating free-form open-ended VQA responses using
string-matching based VQA metrics, we introduce a straightforward LLM-guided
pre-processing technique to adapt the model responses to the expected
ground-truth answer distribution. In summary, our research sheds light on the
intricacies of prompting strategies in VLMs for VQA, emphasizing the
synergistic use of captions, templates, and pre-processing to enhance model
efficacy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awal_R/0/1/0/all/0/1&quot;&gt;Rabiul Awal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Le Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1&quot;&gt;Aishwarya Agrawal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10830">
<title>3D VR Sketch Guided 3D Shape Prototyping and Exploration. (arXiv:2306.10830v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10830</link>
<description rdf:parseType="Literal">&lt;p&gt;3D shape modeling is labor-intensive, time-consuming, and requires years of
expertise. To facilitate 3D shape modeling, we propose a 3D shape generation
network that takes a 3D VR sketch as a condition. We assume that sketches are
created by novices without art training and aim to reconstruct geometrically
realistic 3D shapes of a given category. To handle potential sketch ambiguity,
our method creates multiple 3D shapes that align with the original sketch&apos;s
structure. We carefully design our method, training the model step-by-step and
leveraging multi-modal 3D shape representation to support training with limited
training data. To guarantee the realism of generated 3D shapes we leverage the
normalizing flow that models the distribution of the latent space of 3D shapes.
To encourage the fidelity of the generated 3D shapes to an input sketch, we
propose a dedicated loss that we deploy at different stages of the training
process. The code is available at https://github.com/Rowl1ng/3Dsketch2shape.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1&quot;&gt;Ling Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_P/0/1/0/all/0/1&quot;&gt;Pinaki Nath Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1&quot;&gt;Tao Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yi-Zhe Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gryaditskaya_Y/0/1/0/all/0/1&quot;&gt;Yulia Gryaditskaya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11565">
<title>HomeRobot: Open-Vocabulary Mobile Manipulation. (arXiv:2306.11565v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11565</link>
<description rdf:parseType="Literal">&lt;p&gt;HomeRobot (noun): An affordable compliant robot that navigates homes and
manipulates a wide range of objects in order to complete everyday tasks.
Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object
in any unseen environment, and placing it in a commanded location. This is a
foundational challenge for robots to be useful assistants in human
environments, because it involves tackling sub-problems from across robotics:
perception, language understanding, navigation, and manipulation are all
essential to OVMM. In addition, integration of the solutions to these
sub-problems poses its own substantial challenges. To drive research in this
area, we introduce the HomeRobot OVMM benchmark, where an agent navigates
household environments to grasp novel objects and place them on target
receptacles. HomeRobot has two components: a simulation component, which uses a
large and diverse curated object set in new, high-quality multi-room home
environments; and a real-world component, providing a software stack for the
low-cost Hello Robot Stretch to encourage replication of real-world experiments
across labs. We implement both reinforcement learning and heuristic
(model-based) baselines and show evidence of sim-to-real transfer. Our
baselines achieve a 20% success rate in the real world; our experiments
identify ways future research work improve performance. See videos on our
website: https://ovmm.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yenamandra_S/0/1/0/all/0/1&quot;&gt;Sriram Yenamandra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramachandran_A/0/1/0/all/0/1&quot;&gt;Arun Ramachandran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yadav_K/0/1/0/all/0/1&quot;&gt;Karmesh Yadav&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1&quot;&gt;Austin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khanna_M/0/1/0/all/0/1&quot;&gt;Mukul Khanna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gervet_T/0/1/0/all/0/1&quot;&gt;Theophile Gervet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Tsung-Yen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1&quot;&gt;Vidhi Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clegg_A/0/1/0/all/0/1&quot;&gt;Alexander William Clegg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turner_J/0/1/0/all/0/1&quot;&gt;John Turner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1&quot;&gt;Zsolt Kira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savva_M/0/1/0/all/0/1&quot;&gt;Manolis Savva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_A/0/1/0/all/0/1&quot;&gt;Angel Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaplot_D/0/1/0/all/0/1&quot;&gt;Devendra Singh Chaplot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1&quot;&gt;Dhruv Batra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1&quot;&gt;Roozbeh Mottaghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1&quot;&gt;Yonatan Bisk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1&quot;&gt;Chris Paxton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02129">
<title>How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model. (arXiv:2307.02129v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02129</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning algorithms demonstrate a surprising ability to learn
high-dimensional tasks from limited examples. This is commonly attributed to
the depth of neural networks, enabling them to build a hierarchy of abstract,
low-dimensional data representations. However, how many training examples are
required to learn such representations remains unknown. To quantitatively study
this question, we introduce the Random Hierarchy Model: a family of synthetic
tasks inspired by the hierarchical structure of language and images. The model
is a classification task where each class corresponds to a group of high-level
features, chosen among several equivalent groups associated with the same
class. In turn, each feature corresponds to a group of sub-features chosen
among several equivalent ones and so on, following a hierarchy of composition
rules. We find that deep networks learn the task by developing internal
representations invariant to exchanging equivalent groups. Moreover, the number
of data required corresponds to the point where correlations between low-level
features and classes become detectable. Overall, our results indicate how deep
networks overcome the curse of dimensionality by building invariant
representations, and provide an estimate of the number of data required to
learn a hierarchical task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cagnetta_F/0/1/0/all/0/1&quot;&gt;Francesco Cagnetta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petrini_L/0/1/0/all/0/1&quot;&gt;Leonardo Petrini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomasini_U/0/1/0/all/0/1&quot;&gt;Umberto M. Tomasini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Favero_A/0/1/0/all/0/1&quot;&gt;Alessandro Favero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wyart_M/0/1/0/all/0/1&quot;&gt;Matthieu Wyart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03992">
<title>Stimulating the Diffusion Model for Image Denoising via Adaptive Embedding and Ensembling. (arXiv:2307.03992v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03992</link>
<description rdf:parseType="Literal">&lt;p&gt;Image denoising is a fundamental problem in computational photography, where
achieving high perception with low distortion is highly demanding. Current
methods either struggle with perceptual quality or suffer from significant
distortion. Recently, the emerging diffusion model has achieved
state-of-the-art performance in various tasks and demonstrates great potential
for image denoising. However, stimulating diffusion models for image denoising
is not straightforward and requires solving several critical problems. For one
thing, the input inconsistency hinders the connection between diffusion models
and image denoising. For another, the content inconsistency between the
generated image and the desired denoised image introduces distortion. To tackle
these problems, we present a novel strategy called the Diffusion Model for
Image Denoising (DMID) by understanding and rethinking the diffusion model from
a denoising perspective. Our DMID strategy includes an adaptive embedding
method that embeds the noisy image into a pre-trained unconditional diffusion
model and an adaptive ensembling method that reduces distortion in the denoised
image. Our DMID strategy achieves state-of-the-art performance on both
distortion-based and perception-based metrics, for both Gaussian and real-world
image denoising.The code is available at https://github.com/Li-Tong-621/DMID.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1&quot;&gt;Hansen Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lizhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Hua Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10763">
<title>Actor-agnostic Multi-label Action Recognition with Multi-modal Query. (arXiv:2307.10763v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10763</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing action recognition methods are typically actor-specific due to the
intrinsic topological and apparent differences among the actors. This requires
actor-specific pose estimation (e.g., humans vs. animals), leading to
cumbersome model design complexity and high maintenance costs. Moreover, they
often focus on learning the visual modality alone and single-label
classification whilst neglecting other available information sources (e.g.,
class name text) and the concurrent occurrence of multiple actions. To overcome
these limitations, we propose a new approach called &apos;actor-agnostic multi-modal
multi-label action recognition,&apos; which offers a unified solution for various
types of actors, including humans and animals. We further formulate a novel
Multi-modal Semantic Query Network (MSQNet) model in a transformer-based object
detection framework (e.g., DETR), characterized by leveraging visual and
textual modalities to represent the action classes better. The elimination of
actor-specific model designs is a key advantage, as it removes the need for
actor pose estimation altogether. Extensive experiments on five publicly
available benchmarks show that our MSQNet consistently outperforms the prior
arts of actor-specific alternatives on human and animal single- and multi-label
action recognition tasks by up to 50%. Code is made available at
https://github.com/mondalanindya/MSQNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mondal_A/0/1/0/all/0/1&quot;&gt;Anindya Mondal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1&quot;&gt;Sauradip Nag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prada_J/0/1/0/all/0/1&quot;&gt;Joaquin M Prada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiatian Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dutta_A/0/1/0/all/0/1&quot;&gt;Anjan Dutta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00473">
<title>Is Last Layer Re-Training Truly Sufficient for Robustness to Spurious Correlations?. (arXiv:2308.00473v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.00473</link>
<description rdf:parseType="Literal">&lt;p&gt;Models trained with empirical risk minimization (ERM) are known to learn to
rely on spurious features, i.e., their prediction is based on undesired
auxiliary features which are strongly correlated with class labels but lack
causal reasoning. This behavior particularly degrades accuracy in groups of
samples of the correlated class that are missing the spurious feature or
samples of the opposite class but with the spurious feature present. The
recently proposed Deep Feature Reweighting (DFR) method improves accuracy of
these worst groups. Based on the main argument that ERM mods can learn core
features sufficiently well, DFR only needs to retrain the last layer of the
classification model with a small group-balanced data set. In this work, we
examine the applicability of DFR to realistic data in the medical domain.
Furthermore, we investigate the reasoning behind the effectiveness of
last-layer retraining and show that even though DFR has the potential to
improve the accuracy of the worst group, it remains susceptible to spurious
correlations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_P/0/1/0/all/0/1&quot;&gt;Phuong Quynh Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlotterer_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;rg Schl&amp;#xf6;tterer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seifert_C/0/1/0/all/0/1&quot;&gt;Christin Seifert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.00727">
<title>Deep learning in medical image registration: introduction and survey. (arXiv:2309.00727v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.00727</link>
<description rdf:parseType="Literal">&lt;p&gt;Image registration (IR) is a process that deforms images to align them with
respect to a reference space, making it easier for medical practitioners to
examine various medical images in a standardized reference frame, such as
having the same rotation and scale. This document introduces image registration
using a simple numeric example. It provides a definition of image registration
along with a space-oriented symbolic representation. This review covers various
aspects of image transformations, including affine, deformable, invertible, and
bidirectional transformations, as well as medical image registration algorithms
such as Voxelmorph, Demons, SyN, Iterative Closest Point, and SynthMorph. It
also explores atlas-based registration and multistage image registration
techniques, including coarse-fine and pyramid approaches. Furthermore, this
survey paper discusses medical image registration taxonomies, datasets,
evaluation measures, such as correlation-based metrics, segmentation-based
metrics, processing time, and model size. It also explores applications in
image-guided surgery, motion tracking, and tumor diagnosis. Finally, the
document addresses future research directions, including the further
development of transformers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hammoudeh_A/0/1/0/all/0/1&quot;&gt;Ahmad Hammoudeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dupont_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Dupont&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03851">
<title>CenTime: Event-Conditional Modelling of Censoring in Survival Analysis. (arXiv:2309.03851v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.03851</link>
<description rdf:parseType="Literal">&lt;p&gt;Survival analysis is a valuable tool for estimating the time until specific
events, such as death or cancer recurrence, based on baseline observations.
This is particularly useful in healthcare to prognostically predict clinically
important events based on patient data. However, existing approaches often have
limitations; some focus only on ranking patients by survivability, neglecting
to estimate the actual event time, while others treat the problem as a
classification task, ignoring the inherent time-ordered structure of the
events. Furthermore, the effective utilization of censored samples - training
data points where the exact event time is unknown - is essential for improving
the predictive accuracy of the model. In this paper, we introduce CenTime, a
novel approach to survival analysis that directly estimates the time to event.
Our method features an innovative event-conditional censoring mechanism that
performs robustly even when uncensored data is scarce. We demonstrate that our
approach forms a consistent estimator for the event model parameters, even in
the absence of uncensored data. Furthermore, CenTime is easily integrated with
deep learning models with no restrictions on batch size or the number of
uncensored samples. We compare our approach with standard survival analysis
methods, including the Cox proportional-hazard model and DeepHit. Our results
indicate that CenTime offers state-of-the-art performance in predicting
time-to-death while maintaining comparable ranking performance. Our
implementation is publicly available at
https://github.com/ahmedhshahin/CenTime.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahin_A/0/1/0/all/0/1&quot;&gt;Ahmed H. Shahin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_A/0/1/0/all/0/1&quot;&gt;An Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whitehead_A/0/1/0/all/0/1&quot;&gt;Alexander C. Whitehead&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alexander_D/0/1/0/all/0/1&quot;&gt;Daniel C. Alexander&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacob_J/0/1/0/all/0/1&quot;&gt;Joseph Jacob&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barber_D/0/1/0/all/0/1&quot;&gt;David Barber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.09643">
<title>HiT: Building Mapping with Hierarchical Transformers. (arXiv:2309.09643v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.09643</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning-based methods have been extensively explored for automatic
building mapping from high-resolution remote sensing images over recent years.
While most building mapping models produce vector polygons of buildings for
geographic and mapping systems, dominant methods typically decompose polygonal
building extraction in some sub-problems, including segmentation,
polygonization, and regularization, leading to complex inference procedures,
low accuracy, and poor generalization. In this paper, we propose a simple and
novel building mapping method with Hierarchical Transformers, called HiT,
improving polygonal building mapping quality from high-resolution remote
sensing images. HiT builds on a two-stage detection architecture by adding a
polygon head parallel to classification and bounding box regression heads. HiT
simultaneously outputs building bounding boxes and vector polygons, which is
fully end-to-end trainable. The polygon head formulates a building polygon as
serialized vertices with the bidirectional characteristic, a simple and elegant
polygon representation avoiding the start or end vertex hypothesis. Under this
new perspective, the polygon head adopts a transformer encoder-decoder
architecture to predict serialized vertices supervised by the designed
bidirectional polygon loss. Furthermore, a hierarchical attention mechanism
combined with convolution operation is introduced in the encoder of the polygon
head, providing more geometric structures of building polygons at vertex and
edge levels. Comprehensive experiments on two benchmarks (the CrowdAI and Inria
datasets) demonstrate that our method achieves a new state-of-the-art in terms
of instance segmentation and polygonal metrics compared with state-of-the-art
methods. Moreover, qualitative results verify the superiority and effectiveness
of our model under complex scenes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mingming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qingjie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunhong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.09737">
<title>RaTrack: Moving Object Detection and Tracking with 4D Radar Point Cloud. (arXiv:2309.09737v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.09737</link>
<description rdf:parseType="Literal">&lt;p&gt;Mobile autonomy relies on the precise perception of dynamic environments.
Robustly tracking moving objects in 3D world thus plays a pivotal role for
applications like trajectory prediction, obstacle avoidance, and path planning.
While most current methods utilize LiDARs or cameras for Multiple Object
Tracking (MOT), the capabilities of 4D imaging radars remain largely
unexplored. Recognizing the challenges posed by radar noise and point sparsity
in 4D radar data, we introduce RaTrack, an innovative solution tailored for
radar-based tracking. Bypassing the typical reliance on specific object types
and 3D bounding boxes, our method focuses on motion segmentation and
clustering, enriched by a motion estimation module. Evaluated on the
View-of-Delft dataset, RaTrack showcases superior tracking precision of moving
objects, largely surpassing the performance of the state of the art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1&quot;&gt;Zhijun Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1&quot;&gt;Fangqiang Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1&quot;&gt;Hantao Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Chris Xiaoxuan Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02567">
<title>Improving Automatic VQA Evaluation Using Large Language Models. (arXiv:2310.02567v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02567</link>
<description rdf:parseType="Literal">&lt;p&gt;8 years after the visual question answering (VQA) task was proposed, accuracy
remains the primary metric for automatic evaluation. VQA Accuracy has been
effective so far in the IID evaluation setting. However, our community is
undergoing a shift towards open-ended generative models and OOD evaluation. In
this new paradigm, the existing VQA Accuracy metric is overly stringent and
underestimates the performance of VQA systems. Thus, there is a need to develop
more robust automatic VQA metrics that serve as a proxy for human judgment. In
this work, we propose to leverage the in-context learning capabilities of
instruction-tuned large language models (LLMs) to build a better VQA metric. We
formulate VQA evaluation as an answer-rating task where the LLM is instructed
to score the accuracy of a candidate answer given a set of reference answers.
We demonstrate the proposed metric better correlates with human judgment
compared to existing metrics across several VQA models and benchmarks. We hope
wide adoption of our metric will contribute to better estimating the research
progress on the VQA task. We plan to release the evaluation code and collected
human judgments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manas_O/0/1/0/all/0/1&quot;&gt;Oscar Ma&amp;#xf1;as&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krojer_B/0/1/0/all/0/1&quot;&gt;Benno Krojer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1&quot;&gt;Aishwarya Agrawal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03431">
<title>Robust Zero Level-Set Extraction from Unsigned Distance Fields Based on Double Covering. (arXiv:2310.03431v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03431</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a new method, called DoubleCoverUDF, for extracting
the zero level-set from unsigned distance fields (UDFs). DoubleCoverUDF takes a
learned UDF and a user-specified parameter $r$ (a small positive real number)
as input and extracts an iso-surface with an iso-value $r$ using the
conventional marching cubes algorithm. We show that the computed iso-surface is
the boundary of the $r$-offset volume of the target zero level-set $S$, which
is an orientable manifold, regardless of the topology of $S$. Next, the
algorithm computes a covering map to project the boundary mesh onto $S$,
preserving the mesh&apos;s topology and avoiding folding. If $S$ is an orientable
manifold surface, our algorithm separates the double-layered mesh into a single
layer using a robust minimum-cut post-processing step. Otherwise, it keeps the
double-layered mesh as the output. We validate our algorithm by reconstructing
3D surfaces of open models and demonstrate its efficacy and effectiveness on
synthetic models and benchmark datasets. Our experimental results confirm that
our method is robust and produces meshes with better quality in terms of both
visual evaluation and quantitative measures than existing UDF-based methods.
The source code is available at https://github.com/jjjkkyz/DCUDF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_F/0/1/0/all/0/1&quot;&gt;Fei Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xuhui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wencheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1&quot;&gt;Hong Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Ying He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08106">
<title>Generalized Logit Adjustment: Calibrating Fine-tuned Models by Removing Label Bias in Foundation Models. (arXiv:2310.08106v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08106</link>
<description rdf:parseType="Literal">&lt;p&gt;Foundation models like CLIP allow zero-shot transfer on various tasks without
additional training data. Yet, the zero-shot performance is less competitive
than a fully supervised one. Thus, to enhance the performance, fine-tuning and
ensembling are also commonly adopted to better fit the downstream tasks.
However, we argue that such prior work has overlooked the inherent biases in
foundation models. Due to the highly imbalanced Web-scale training set, these
foundation models are inevitably skewed toward frequent semantics, and thus the
subsequent fine-tuning or ensembling is still biased. In this study, we
systematically examine the biases in foundation models and demonstrate the
efficacy of our proposed Generalized Logit Adjustment (GLA) method. Note that
bias estimation in foundation models is challenging, as most pre-train data
cannot be explicitly accessed like in traditional long-tailed classification
tasks. To this end, GLA has an optimization-based bias estimation approach for
debiasing foundation models. As our work resolves a fundamental flaw in the
pre-training, the proposed GLA demonstrates significant improvements across a
diverse range of tasks: it achieves 1.5 pp accuracy gains on ImageNet, an large
average improvement (1.4-4.6 pp) on 11 few-shot datasets, 2.4 pp gains on
long-tailed classification. Codes are in \url{https://github.com/BeierZhu/GLA}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1&quot;&gt;Beier Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1&quot;&gt;Kaihua Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1&quot;&gt;Qianru Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hanwang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10224">
<title>Generalizing Medical Image Representations via Quaternion Wavelet Networks. (arXiv:2310.10224v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10224</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural network generalizability is becoming a broad research field due to the
increasing availability of datasets from different sources and for various
tasks. This issue is even wider when processing medical data, where a lack of
methodological standards causes large variations being provided by different
imaging centers or acquired with various devices and cofactors. To overcome
these limitations, we introduce a novel, generalizable, data- and task-agnostic
framework able to extract salient features from medical images. The proposed
quaternion wavelet network (QUAVE) can be easily integrated with any
pre-existing medical image analysis or synthesis task, and it can be involved
with real, quaternion, or hypercomplex-valued models, generalizing their
adoption to single-channel data. QUAVE first extracts different sub-bands
through the quaternion wavelet transform, resulting in both
low-frequency/approximation bands and high-frequency/fine-grained features.
Then, it weighs the most representative set of sub-bands to be involved as
input to any other neural model for image processing, replacing standard data
samples. We conduct an extensive experimental evaluation comprising different
datasets, diverse image analysis, and synthesis tasks including reconstruction,
segmentation, and modality translation. We also evaluate QUAVE in combination
with both real and quaternion-valued models. Results demonstrate the
effectiveness and the generalizability of the proposed framework that improves
network performance while being flexible to be adopted in manifold scenarios
and robust to domain shifts. The full code is available at:
https://github.com/ispamm/QWT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sigillo_L/0/1/0/all/0/1&quot;&gt;Luigi Sigillo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Grassucci_E/0/1/0/all/0/1&quot;&gt;Eleonora Grassucci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Uncini_A/0/1/0/all/0/1&quot;&gt;Aurelio Uncini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Comminiello_D/0/1/0/all/0/1&quot;&gt;Danilo Comminiello&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13925">
<title>Deep Neural Decision Forest: A Novel Approach for Predicting Recovery or Decease of COVID-19 Patients with Clinical and RT-PCR. (arXiv:2311.13925v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13925</link>
<description rdf:parseType="Literal">&lt;p&gt;COVID-19 continues to be considered an endemic disease in spite of the World
Health Organization&apos;s declaration that the pandemic is over. This pandemic has
disrupted people&apos;s lives in unprecedented ways and caused widespread morbidity
and mortality. As a result, it is important for emergency physicians to
identify patients with a higher mortality risk in order to prioritize hospital
equipment, especially in areas with limited medical services. The collected
data from patients is beneficial to predict the outcome of COVID-19 cases,
although there is a question about which data makes the most accurate
predictions. Therefore, this study aims to accomplish two main objectives.
First, we want to examine whether deep learning algorithms can predict a
patient&apos;s morality. Second, we investigated the impact of Clinical and RT-PCR
on prediction to determine which one is more reliable. We defined four stages
with different feature sets and used interpretable deep learning methods to
build appropriate model. Based on results, the deep neural decision forest
performed the best across all stages and proved its capability to predict the
recovery and death of patients. Additionally, results indicate that Clinical
alone (without the use of RT-PCR) is the most effective method of diagnosis,
with an accuracy of 80%. It is important to document and understand experiences
from the COVID-19 pandemic in order to aid future medical efforts. This study
can provide guidance for medical professionals in the event of a crisis or
outbreak similar to COVID-19.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dehghani_M/0/1/0/all/0/1&quot;&gt;Mohammad Dehghani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yazdanparast_Z/0/1/0/all/0/1&quot;&gt;Zahra Yazdanparast&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Samani_R/0/1/0/all/0/1&quot;&gt;Rasoul Samani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16163">
<title>IODeep: an IOD for the introduction of deep learning in the DICOM standard. (arXiv:2311.16163v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16163</link>
<description rdf:parseType="Literal">&lt;p&gt;Background and Objective: In recent years, Artificial Intelligence (AI) and
in particular Deep Neural Networks (DNN) became a relevant research topic in
biomedical image segmentation due to the availability of more and more data
sets along with the establishment of well known competitions. Despite the
popularity of DNN based segmentation on the research side, these techniques are
almost unused in the daily clinical practice even if they could support
effectively the physician during the diagnostic process. Apart from the issues
related to the explainability of the predictions of a neural model, such
systems are not integrated in the diagnostic workflow, and a standardization of
their use is needed to achieve this goal. Methods: This paper presents IODeep a
new DICOM Information Object Definition (IOD) aimed at storing both the weights
and the architecture of a DNN already trained on a particular image dataset
that is labeled as regards the acquisition modality, the anatomical region, and
the disease under investigation. Results: The IOD architecture is presented
along with a DNN selection algorithm from the PACS server based on the labels
outlined above, and a simple PACS viewer purposely designed for demonstrating
the effectiveness of the DICOM integration, while no modifications are required
on the PACS server side. Also a service based architecture in support of the
entire workflow has been implemented. Conclusion: IODeep ensures full
integration of a trained AI model in a DICOM infrastructure, and it is also
enables a scenario where a trained model can be either fine-tuned with hospital
data or trained in a federated learning scheme shared by different hospitals.
In this way AI models can be tailored to the real data produced by a Radiology
ward thus improving the physician decision making process. Source code is
freely available at https://github.com/CHILab1/IODeep.git
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Contino_S/0/1/0/all/0/1&quot;&gt;Salvatore Contino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cruciata_L/0/1/0/all/0/1&quot;&gt;Luca Cruciata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gambino_O/0/1/0/all/0/1&quot;&gt;Orazio Gambino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pirrone_R/0/1/0/all/0/1&quot;&gt;Roberto Pirrone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00699">
<title>Rethinking Detection Based Table Structure Recognition for Visually Rich Document Images. (arXiv:2312.00699v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.00699</link>
<description rdf:parseType="Literal">&lt;p&gt;Table Structure Recognition (TSR) is a widely discussed task aiming at
transforming unstructured table images into structured formats, such as HTML
sequences, to make text-only models, such as ChatGPT, that can further process
these tables. One type of solution is using detection models to detect table
components, such as columns and rows, then applying a rule-based
post-processing method to convert detection results into HTML sequences.
However, existing detection-based models usually cannot perform as well as
other types of solutions regarding cell-level TSR metrics, such as TEDS, and
the underlying reasons limiting the performance of these models on the TSR task
are also not well-explored. Therefore, we revisit existing detection-based
models comprehensively and explore the underlying reasons hindering these
models&apos; performance, including the improper problem definition, the mismatch
issue of detection and TSR metrics, the characteristics of detection models,
and the impact of local and long-range features extraction. Based on our
analysis and findings, we apply simple methods to tailor a typical two-stage
detection model, Cascade R-CNN, for the TSR task. The experimental results show
that the tailored Cascade R-CNN based model can improve the base Cascade R-CNN
model by 16.35\% on the FinTabNet dataset regarding the structure-only TEDS,
outperforming other types of state-of-the-art methods, demonstrating that our
findings can be a guideline for improving detection-based TSR models and that a
purely detection-based solution is competitive with other types of solutions,
such as graph-based and image-to-sequence solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1&quot;&gt;Bin Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simsek_M/0/1/0/all/0/1&quot;&gt;Murat Simsek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kantarci_B/0/1/0/all/0/1&quot;&gt;Burak Kantarci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alkheir_A/0/1/0/all/0/1&quot;&gt;Ala Abu Alkheir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10324">
<title>Federated Learning with Instance-Dependent Noisy Label. (arXiv:2312.10324v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10324</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) with noisy labels poses a significant challenge.
Existing methods designed for handling noisy labels in centralized learning
tend to lose their effectiveness in the FL setting, mainly due to the small
dataset size and the heterogeneity of client data. While some attempts have
been made to tackle FL with noisy labels, they primarily focused on scenarios
involving class-conditional noise. In this paper, we study the more challenging
and practical issue of instance-dependent noise (IDN) in FL. We introduce a
novel algorithm called FedBeat (Federated Learning with Bayesian
Ensemble-Assisted Transition Matrix Estimation). FedBeat aims to build a global
statistically consistent classifier using the IDN transition matrix (IDNTM),
which encompasses three synergistic steps: (1) A federated data extraction step
that constructs a weak global model and extracts high-confidence data using a
Bayesian model ensemble method. (2) A federated transition matrix estimation
step in which clients collaboratively train an IDNTM estimation network based
on the extracted data. (3) A federated classifier correction step that enhances
the global model&apos;s performance by training it using a loss function tailored
for noisy labels, leveraging the IDNTM. Experiments conducted on CIFAR-10 and
SVHN verify that the proposed method significantly outperforms state-of-the-art
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1&quot;&gt;Jieming Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jie Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12102">
<title>I-CEE: Tailoring Explanations of Image Classification Models to User Expertise. (arXiv:2312.12102v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.12102</link>
<description rdf:parseType="Literal">&lt;p&gt;Effectively explaining decisions of black-box machine learning models is
critical to responsible deployment of AI systems that rely on them. Recognizing
their importance, the field of explainable AI (XAI) provides several techniques
to generate these explanations. Yet, there is relatively little emphasis on the
user (the explainee) in this growing body of work and most XAI techniques
generate &quot;one-size-fits-all&quot; explanations. To bridge this gap and achieve a
step closer towards human-centered XAI, we present I-CEE, a framework that
provides Image Classification Explanations tailored to User Expertise. Informed
by existing work, I-CEE explains the decisions of image classification models
by providing the user with an informative subset of training data (i.e.,
example images), corresponding local explanations, and model decisions.
However, unlike prior work, I-CEE models the informativeness of the example
images to depend on user expertise, resulting in different examples for
different users. We posit that by tailoring the example set to user expertise,
I-CEE can better facilitate users&apos; understanding and simulatability of the
model. To evaluate our approach, we conduct detailed experiments in both
simulation and with human participants (N = 100) on multiple datasets.
Experiments with simulated users show that I-CEE improves users&apos; ability to
accurately predict the model&apos;s decisions (simulatability) compared to
baselines, providing promising preliminary results. Experiments with human
participants demonstrate that our method significantly improves user
simulatability accuracy, highlighting the importance of human-centered XAI
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1&quot;&gt;Yao Rong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_P/0/1/0/all/0/1&quot;&gt;Peizhu Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Unhelkar_V/0/1/0/all/0/1&quot;&gt;Vaibhav Unhelkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasneci_E/0/1/0/all/0/1&quot;&gt;Enkelejda Kasneci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13104">
<title>Optimizing Ego Vehicle Trajectory Prediction: The Graph Enhancement Approach. (arXiv:2312.13104v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.13104</link>
<description rdf:parseType="Literal">&lt;p&gt;Predicting the trajectory of an ego vehicle is a critical component of
autonomous driving systems. Current state-of-the-art methods typically rely on
Deep Neural Networks (DNNs) and sequential models to process front-view images
for future trajectory prediction. However, these approaches often struggle with
perspective issues affecting object features in the scene. To address this, we
advocate for the use of Bird&apos;s Eye View (BEV) perspectives, which offer unique
advantages in capturing spatial relationships and object homogeneity. In our
work, we leverage Graph Neural Networks (GNNs) and positional encoding to
represent objects in a BEV, achieving competitive performance compared to
traditional DNN-based methods. While the BEV-based approach loses some detailed
information inherent to front-view images, we balance this by enriching the BEV
data by representing it as a graph where relationships between the objects in a
scene are captured effectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1&quot;&gt;Sushil Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Aryan Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sistu_G/0/1/0/all/0/1&quot;&gt;Ganesh Sistu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Halton_M/0/1/0/all/0/1&quot;&gt;Mark Halton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eising_C/0/1/0/all/0/1&quot;&gt;Ciar&amp;#xe1;n Eising&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16414">
<title>Bellman Optimal Step-size Straightening of Flow-Matching Models. (arXiv:2312.16414v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.16414</link>
<description rdf:parseType="Literal">&lt;p&gt;Flow matching is a powerful framework for generating high-quality samples in
various applications, especially image synthesis. However, the intensive
computational demands of these models, especially during the fine-tuning
process and sampling processes, pose significant challenges for low-resource
scenarios. This paper introduces Bellman Optimal Step-size Straightening (BOSS)
technique for distilling flow-matching generative models: it aims specifically
for a few-step efficient image sampling while adhering to a computational
budget constraint. First, this technique involves a dynamic programming
algorithm that optimizes the step sizes of the pretrained network. Then, it
refines the velocity network to match the optimal step sizes, aiming to
straighten the generation paths. Extensive experimental evaluations across
image generation tasks demonstrate the efficacy of BOSS in terms of both
resource utilization and image quality. Our results reveal that BOSS achieves
substantial gains in efficiency while maintaining competitive sample quality,
effectively bridging the gap between low-resource constraints and the demanding
requirements of flow-matching generative models. Our paper also fortifies the
responsible development of artificial intelligence, offering a more sustainable
generative model that reduces computational costs and environmental footprints.
Our code can be found at https://github.com/nguyenngocbaocmt02/BOSS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1&quot;&gt;Bao Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1&quot;&gt;Binh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1&quot;&gt;Viet Anh Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16805">
<title>DarkShot: Lighting Dark Images with Low-Compute and High-Quality. (arXiv:2312.16805v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.16805</link>
<description rdf:parseType="Literal">&lt;p&gt;Nighttime photography encounters escalating challenges in extremely low-light
conditions, primarily attributable to the ultra-low signal-to-noise ratio. For
real-world deployment, a practical solution must not only produce visually
appealing results but also require minimal computation. However, most existing
methods are either focused on improving restoration performance or employ
lightweight models at the cost of quality. This paper proposes a lightweight
network that outperforms existing state-of-the-art (SOTA) methods in low-light
enhancement tasks while minimizing computation. The proposed network
incorporates Siamese Self-Attention Block (SSAB) and Skip-Channel Attention
(SCA) modules, which enhance the model&apos;s capacity to aggregate global
information and are well-suited for high-resolution images. Additionally, based
on our analysis of the low-light image restoration process, we propose a
Two-Stage Framework that achieves superior results. Our model can restore a UHD
4K resolution image with minimal computation while keeping SOTA restoration
quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Jiazhang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1&quot;&gt;Qiuping Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Cheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Li Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yangxing Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00729">
<title>NightRain: Nighttime Video Deraining via Adaptive-Rain-Removal and Adaptive-Correction. (arXiv:2401.00729v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00729</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing deep-learning-based methods for nighttime video deraining rely on
synthetic data due to the absence of real-world paired data. However, the
intricacies of the real world, particularly with the presence of light effects
and low-light regions affected by noise, create significant domain gaps,
hampering synthetic-trained models in removing rain streaks properly and
leading to over-saturation and color shifts. Motivated by this, we introduce
NightRain, a novel nighttime video deraining method with adaptive-rain-removal
and adaptive-correction. Our adaptive-rain-removal uses unlabeled rain videos
to enable our model to derain real-world rain videos, particularly in regions
affected by complex light effects. The idea is to allow our model to obtain
rain-free regions based on the confidence scores. Once rain-free regions and
the corresponding regions from our input are obtained, we can have region-based
paired real data. These paired data are used to train our model using a
teacher-student framework, allowing the model to iteratively learn from less
challenging regions to more challenging regions. Our adaptive-correction aims
to rectify errors in our model&apos;s predictions, such as over-saturation and color
shifts. The idea is to learn from clear night input training videos based on
the differences or distance between those input videos and their corresponding
predictions. Our model learns from these differences, compelling our model to
correct the errors. From extensive experiments, our method demonstrates
state-of-the-art performance. It achieves a PSNR of 26.73dB, surpassing
existing nighttime video deraining methods by a substantial margin of 13.7%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Beibei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yeying Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_W/0/1/0/all/0/1&quot;&gt;Wending Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1&quot;&gt;Wei Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yuan Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shunli Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_R/0/1/0/all/0/1&quot;&gt;Robby Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00926">
<title>Accurate Leukocyte Detection Based on Deformable-DETR and Multi-Level Feature Fusion for Aiding Diagnosis of Blood Diseases. (arXiv:2401.00926v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00926</link>
<description rdf:parseType="Literal">&lt;p&gt;In standard hospital blood tests, the traditional process requires doctors to
manually isolate leukocytes from microscopic images of patients&apos; blood using
microscopes. These isolated leukocytes are then categorized via automatic
leukocyte classifiers to determine the proportion and volume of different types
of leukocytes present in the blood samples, aiding disease diagnosis. This
methodology is not only time-consuming and labor-intensive, but it also has a
high propensity for errors due to factors such as image quality and
environmental conditions, which could potentially lead to incorrect subsequent
classifications and misdiagnosis. To address these issues, this paper proposes
an innovative method of leukocyte detection: the Multi-level Feature Fusion and
Deformable Self-attention DETR (MFDS-DETR). To tackle the issue of leukocyte
scale disparity, we designed the High-level Screening-feature Fusion Pyramid
(HS-FPN), enabling multi-level fusion. This model uses high-level features as
weights to filter low-level feature information via a channel attention module
and then merges the screened information with the high-level features, thus
enhancing the model&apos;s feature expression capability. Further, we address the
issue of leukocyte feature scarcity by incorporating a multi-scale deformable
self-attention module in the encoder and using the self-attention and
cross-deformable attention mechanisms in the decoder, which aids in the
extraction of the global features of the leukocyte feature maps. The
effectiveness, superiority, and generalizability of the proposed MFDS-DETR
method are confirmed through comparisons with other cutting-edge leukocyte
detection models using the private WBCDD, public LISC and BCCD datasets. Our
source code and private WBCCD dataset are available at
https://github.com/JustlfC03/MFDS-DETR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yifei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chenyan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Ben Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yiyu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yifei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Changmiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1&quot;&gt;Xianjun Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1&quot;&gt;Yuxing Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_F/0/1/0/all/0/1&quot;&gt;Feiwei Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yong Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yu Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01227">
<title>IdentiFace : A VGG Based Multimodal Facial Biometric System. (arXiv:2401.01227v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01227</link>
<description rdf:parseType="Literal">&lt;p&gt;The development of facial biometric systems has contributed greatly to the
development of the computer vision field. Nowadays, there&apos;s always a need to
develop a multimodal system that combines multiple biometric traits in an
efficient, meaningful way. In this paper, we introduce &quot;IdentiFace&quot; which is a
multimodal facial biometric system that combines the core of facial recognition
with some of the most important soft biometric traits such as gender, face
shape, and emotion. We also focused on developing the system using only VGG-16
inspired architecture with minor changes across different subsystems. This
unification allows for simpler integration across modalities. It makes it
easier to interpret the learned features between the tasks which gives a good
indication about the decision-making process across the facial modalities and
potential connection. For the recognition problem, we acquired a 99.2% test
accuracy for five classes with high intra-class variations using data collected
from the FERET database[1]. We achieved 99.4% on our dataset and 95.15% on the
public dataset[2] in the gender recognition problem. We were also able to
achieve a testing accuracy of 88.03% in the face-shape problem using the
celebrity face-shape dataset[3]. Finally, we achieved a decent testing accuracy
of 66.13% in the emotion task which is considered a very acceptable accuracy
compared to related work on the FER2013 dataset[4].
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rabea_M/0/1/0/all/0/1&quot;&gt;Mahmoud Rabea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_H/0/1/0/all/0/1&quot;&gt;Hanya Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahmoud_S/0/1/0/all/0/1&quot;&gt;Sohaila Mahmoud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sayed_N/0/1/0/all/0/1&quot;&gt;Nourhan Sayed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01375">
<title>Mapping Walnut Water Stress with High Resolution Multispectral UAV Imagery and Machine Learning. (arXiv:2401.01375v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01375</link>
<description rdf:parseType="Literal">&lt;p&gt;Effective monitoring of walnut water status and stress level across the whole
orchard is an essential step towards precision irrigation management of
walnuts, a significant crop in California. This study presents a machine
learning approach using Random Forest (RF) models to map stem water potential
(SWP) by integrating high-resolution multispectral remote sensing imagery from
Unmanned Aerial Vehicle (UAV) flights with weather data. From 2017 to 2018,
five flights of an UAV equipped with a seven-band multispectral camera were
conducted over a commercial walnut orchard, paired with concurrent ground
measurements of sampled walnut plants. The RF regression model, utilizing
vegetation indices derived from orthomosaiced UAV imagery and weather data,
effectively estimated ground-measured SWPs, achieving an $R^2$ of 0.63 and a
mean absolute error (MAE) of 0.80 bars. The integration of weather data was
particularly crucial for consolidating data across various flight dates.
Significant variables for SWP estimation included wind speed and vegetation
indices such as NDVI, NDRE, and PSRI.A reduced RF model excluding red-edge
indices of NDRE and PSRI, demonstrated slightly reduced accuracy ($R^2$ =
0.54). Additionally, the RF classification model predicted water stress levels
in walnut trees with 85% accuracy, surpassing the 80% accuracy of the reduced
classification model. The results affirm the efficacy of UAV-based
multispectral imaging combined with machine learning, incorporating thermal
data, NDVI, red-edge indices, and weather data, in walnut water stress
estimation and assessment. This methodology offers a scalable, cost-effective
tool for data-driven precision irrigation management at an individual plant
level in walnut orchards.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kaitlyn Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yufang Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02162">
<title>Frequency Domain Nuances Mining for Visible-Infrared Person Re-identification. (arXiv:2401.02162v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.02162</link>
<description rdf:parseType="Literal">&lt;p&gt;The key of visible-infrared person re-identification (VIReID) lies in how to
minimize the modality discrepancy between visible and infrared images. Existing
methods mainly exploit the spatial information while ignoring the
discriminative frequency information. To address this issue, this paper aims to
reduce the modality discrepancy from the frequency domain perspective.
Specifically, we propose a novel Frequency Domain Nuances Mining (FDNM) method
to explore the cross-modality frequency domain information, which mainly
includes an amplitude guided phase (AGP) module and an amplitude nuances mining
(ANM) module. These two modules are mutually beneficial to jointly explore
frequency domain visible-infrared nuances, thereby effectively reducing the
modality discrepancy in the frequency domain. Besides, we propose a
center-guided nuances mining loss to encourage the ANM module to preserve
discriminative identity information while discovering diverse cross-modality
nuances. Extensive experiments show that the proposed FDNM has significant
advantages in improving the performance of VIReID. Specifically, our method
outperforms the second-best method by 5.2\% in Rank-1 accuracy and 5.8\% in mAP
on the SYSU-MM01 dataset under the indoor search mode, respectively. Besides,
we also validate the effectiveness and generalization of our method on the
challenging visible-infrared face recognition task. \textcolor{magenta}{The
code will be available.}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yukang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yang Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yan Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hanzi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuelong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02384">
<title>ChartAssisstant: A Universal Chart Multimodal Language Model via Chart-to-Table Pre-training and Multitask Instruction Tuning. (arXiv:2401.02384v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.02384</link>
<description rdf:parseType="Literal">&lt;p&gt;Charts play a vital role in data visualization, understanding data patterns,
and informed decision-making. However, their unique combination of graphical
elements (e.g., bars, lines) and textual components (e.g., labels, legends)
poses challenges for general-purpose multimodal models. While vision-language
models trained on chart data excel in comprehension, they struggle with
generalization and require task-specific fine-tuning. To address these
challenges, we propose ChartAssistant, a chart-based vision-language model for
universal chart comprehension and reasoning. ChartAssistant leverages ChartSFT,
a comprehensive dataset covering diverse chart-related tasks with basic and
specialized chart types. It undergoes a two-stage training process, starting
with pre-training on chart-to-table parsing to align chart and text, followed
by multitask instruction-following fine-tuning. This approach enables
ChartAssistant to achieve competitive performance across various chart tasks
without task-specific fine-tuning. Experimental results demonstrate significant
performance gains over the state-of-the-art UniChart method, outperforming
OpenAI&apos;s GPT-4V(ision) on real-world chart data. The code and data are
available at https://github.com/OpenGVLab/ChartAst.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1&quot;&gt;Fanqing Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1&quot;&gt;Wenqi Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1&quot;&gt;Quanfeng Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1&quot;&gt;Peng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kaipeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03836">
<title>WidthFormer: Toward Efficient Transformer-based BEV View Transformation. (arXiv:2401.03836v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.03836</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we present WidthFormer, a novel transformer-based
Bird&apos;s-Eye-View (BEV) 3D detection method tailored for real-time
autonomous-driving applications. WidthFormer is computationally efficient,
robust and does not require any special engineering effort to deploy. In this
work, we propose a novel 3D positional encoding mechanism capable of accurately
encapsulating 3D geometric information, which enables our model to generate
high-quality BEV representations with only a single transformer decoder layer.
This mechanism is also beneficial for existing sparse 3D object detectors.
Inspired by the recently-proposed works, we further improve our model&apos;s
efficiency by vertically compressing the image features when serving as
attention keys and values. We also introduce two modules to compensate for
potential information loss due to feature compression. Experimental evaluation
on the widely-used nuScenes 3D object detection benchmark demonstrates that our
method outperforms previous approaches across different 3D detection
architectures. More importantly, our model is highly efficient. For example,
when using $256\times 704$ input images, it achieves 1.5 ms and 2.8 ms latency
on NVIDIA 3090 GPU and Horizon Journey-5 edge computing chips, respectively.
Furthermore, WidthFormer also exhibits strong robustness to different degrees
of camera perturbations. Our study offers valuable insights into the deployment
of BEV transformation methods in real-world, complex road environments. Code is
available at https://github.com/ChenhongyiYang/WidthFormer .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chenhongyi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1&quot;&gt;Tianwei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Lichao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crowley_E/0/1/0/all/0/1&quot;&gt;Elliot J. Crowley&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03922">
<title>Structure-focused Neurodegeneration Convolutional Neural Network for Modeling and Classification of Alzheimer&apos;s Disease. (arXiv:2401.03922v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.03922</link>
<description rdf:parseType="Literal">&lt;p&gt;Alzheimer&apos;s disease (AD), the predominant form of dementia, poses a growing
global challenge and underscores the urgency of accurate and early diagnosis.
The clinical technique radiologists adopt for distinguishing between mild
cognitive impairment (MCI) and AD using Machine Resonance Imaging (MRI)
encounter hurdles because they are not consistent and reliable. Machine
learning has been shown to offer promise for early AD diagnosis. However,
existing models focused on focal fine-grain features without considerations to
focal structural features that give off information on neurodegeneration of the
brain cerebral cortex. Therefore, this paper proposes a machine learning (ML)
framework that integrates Gamma correction, an image enhancement technique, and
includes a structure-focused neurodegeneration convolutional neural network
(CNN) architecture called SNeurodCNN for discriminating between AD and MCI. The
ML framework leverages the mid-sagittal and para-sagittal brain image
viewpoints of the structure-focused Alzheimer&apos;s Disease Neuroimaging Initiative
(ADNI) dataset. Through experiments, our proposed machine learning framework
shows exceptional performance. The parasagittal viewpoint set achieves 97.8%
accuracy, with 97.0% specificity and 98.5% sensitivity. The midsagittal
viewpoint is shown to present deeper insights into the structural brain changes
given the increase in accuracy, specificity, and sensitivity, which are 98.1%
97.2%, and 99.0%, respectively. Using GradCAM technique, we show that our
proposed model is capable of capturing the structural dynamics of MCI and AD
which exist about the frontal lobe, occipital lobe, cerebellum, and parietal
lobe. Therefore, our model itself as a potential brain structural change
Digi-Biomarker for early diagnosis of AD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Odimayo_S/0/1/0/all/0/1&quot;&gt;Simisola Odimayo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Olisah_C/0/1/0/all/0/1&quot;&gt;Chollette C. Olisah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mohammed_K/0/1/0/all/0/1&quot;&gt;Khadija Mohammed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04092">
<title>GPT-4V(ision) is a Human-Aligned Evaluator for Text-to-3D Generation. (arXiv:2401.04092v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.04092</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite recent advances in text-to-3D generative methods, there is a notable
absence of reliable evaluation metrics. Existing metrics usually focus on a
single criterion each, such as how well the asset aligned with the input text.
These metrics lack the flexibility to generalize to different evaluation
criteria and might not align well with human preferences. Conducting user
preference studies is an alternative that offers both adaptability and
human-aligned results. User studies, however, can be very expensive to scale.
This paper presents an automatic, versatile, and human-aligned evaluation
metric for text-to-3D generative models. To this end, we first develop a prompt
generator using GPT-4V to generate evaluating prompts, which serve as input to
compare text-to-3D models. We further design a method instructing GPT-4V to
compare two 3D assets according to user-defined criteria. Finally, we use these
pairwise comparison results to assign these models Elo ratings. Experimental
results suggest our metric strongly align with human preference across
different evaluation criteria.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guandao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhibing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1&quot;&gt;Leonidas Guibas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dahua Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wetzstein_G/0/1/0/all/0/1&quot;&gt;Gordon Wetzstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04716">
<title>Low-Resource Vision Challenges for Foundation Models. (arXiv:2401.04716v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.04716</link>
<description rdf:parseType="Literal">&lt;p&gt;Low-resource settings are well-established in natural language processing,
where many languages lack sufficient data for machine learning at scale.
However, low-resource problems are under-explored in computer vision. In this
paper, we strive to address this gap and explore the challenges of low-resource
image tasks with vision foundation models. Thus, we first collect a benchmark
of genuinely low-resource image data, covering historic maps, circuit diagrams,
and mechanical drawings. These low-resource settings all share the three
challenges of data scarcity, fine-grained differences, and the distribution
shift from natural images to the specialized domain of interest. While existing
foundation models have shown impressive generalizability, we find they cannot
transfer well to our low-resource tasks. To begin to tackle the challenges of
low-resource vision, we introduce one simple baseline per challenge.
Specifically, we propose to i) enlarge the data space by generative models, ii)
adopt the best sub-kernels to encode local regions for fine-grained difference
discovery and iii) learn attention for specialized domains. Experiments on the
three low-resource data sources in our benchmark demonstrate our proposals
already provide a better baseline than common transfer learning, data
augmentation, and fine-grained methods. This highlights the unique
characteristics and challenges of low-resource vision for foundation models
that warrant further investigation. Project website:
https://xiaobai1217.github.io/Low-Resource-Vision/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunhua Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doughty_H/0/1/0/all/0/1&quot;&gt;Hazel Doughty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1&quot;&gt;Cees G.M. Snoek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06067">
<title>Attributes Grouping and Mining Hashing for Fine-Grained Image Retrieval. (arXiv:2311.06067v1 [cs.IR] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2311.06067</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, hashing methods have been popular in the large-scale media
search for low storage and strong representation capabilities. To describe
objects with similar overall appearance but subtle differences, more and more
studies focus on hashing-based fine-grained image retrieval. Existing hashing
networks usually generate both local and global features through attention
guidance on the same deep activation tensor, which limits the diversity of
feature representations. To handle this limitation, we substitute convolutional
descriptors for attention-guided features and propose an Attributes Grouping
and Mining Hashing (AGMH), which groups and embeds the category-specific visual
attributes in multiple descriptors to generate a comprehensive feature
representation for efficient fine-grained image retrieval. Specifically, an
Attention Dispersion Loss (ADL) is designed to force the descriptors to attend
to various local regions and capture diverse subtle details. Moreover, we
propose a Stepwise Interactive External Attention (SIEA) to mine critical
attributes in each descriptor and construct correlations between fine-grained
attributes and objects. The attention mechanism is dedicated to learning
discrete attributes, which will not cost additional computations in hash codes
generation. Finally, the compact binary codes are learned by preserving
pairwise similarities. Experimental results demonstrate that AGMH consistently
yields the best performance against state-of-the-art methods on fine-grained
benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xin Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shikun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yichao Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xiaobo Lu&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>