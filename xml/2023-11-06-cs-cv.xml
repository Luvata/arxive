<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-11-02T21:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00729" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00731" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00734" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00735" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00750" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00762" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00796" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00800" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00807" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00810" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00917" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00926" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00932" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00936" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00938" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00941" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00949" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00961" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00962" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00970" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00979" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00980" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00986" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00987" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00990" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00991" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00994" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00995" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00996" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01001" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01004" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01009" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01010" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01015" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01016" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01017" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01018" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01022" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01023" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01025" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01028" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01034" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01057" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01064" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01065" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01066" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01090" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01091" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01092" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01111" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01117" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01130" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01138" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01155" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01185" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01188" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01192" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01197" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01202" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01212" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01214" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01216" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01226" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01227" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01233" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01237" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01240" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01241" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01267" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01283" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01292" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01295" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01308" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01310" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01323" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01352" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01357" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01361" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01373" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01380" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01394" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01405" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01410" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01423" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01425" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01429" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01432" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01441" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01444" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01446" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01447" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2002.07897" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2002.09625" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2104.04926" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.01209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.06950" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.04366" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.07440" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.08317" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.11201" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.10056" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.10725" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.12145" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.17835" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.10168" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.13410" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.13672" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.14030" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.01638" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.08491" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.08776" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.09253" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.11147" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14764" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18832" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04699" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05225" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07684" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07962" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08129" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10681" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15111" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15668" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16180" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05973" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07439" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10829" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10943" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03900" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.06534" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16741" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.00174" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.00733" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.01859" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.05090" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13744" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13745" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01641" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02230" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.06403" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17403" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18348" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19559" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00277" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00489" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00548" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2311.00729">
<title>ZEETAD: Adapting Pretrained Vision-Language Model for Zero-Shot End-to-End Temporal Action Detection. (arXiv:2311.00729v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.00729</link>
<description rdf:parseType="Literal">&lt;p&gt;Temporal action detection (TAD) involves the localization and classification
of action instances within untrimmed videos. While standard TAD follows fully
supervised learning with closed-set setting on large training data, recent
zero-shot TAD methods showcase the promising of open-set setting by leveraging
large-scale contrastive visual-language (ViL) pretrained models. However,
existing zero-shot TAD methods have limitations on how to properly construct
the strong relationships between two interdependent tasks of localization and
classification and adapt ViL model to video understanding. In this work, we
present ZEETAD, featuring two modules: dual-localization and zero-shot proposal
classification. The former is a Transformer-based module that detects action
events while selectively collecting crucial semantic embeddings for later
recognition. The latter one, CLIP-based module, generates semantic embeddings
from text and frame inputs for each temporal unit. Additionally, we enhance
discriminative capability on unseen classes by minimally updating the frozen
CLIP encoder with lightweight adapters. Extensive experiments on THUMOS14 and
ActivityNet-1.3 datasets demonstrate our approach&apos;s superior performance in
zero-shot TAD and effective knowledge transfer from ViL models to unseen action
categories.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phan_T/0/1/0/all/0/1&quot;&gt;Thinh Phan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vo_K/0/1/0/all/0/1&quot;&gt;Khoa Vo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1&quot;&gt;Duy Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doretto_G/0/1/0/all/0/1&quot;&gt;Gianfranco Doretto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adjeroh_D/0/1/0/all/0/1&quot;&gt;Donald Adjeroh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1&quot;&gt;Ngan Le&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00731">
<title>Enhancing Clustering Representations with Positive Proximity and Cluster Dispersion Learning. (arXiv:2311.00731v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.00731</link>
<description rdf:parseType="Literal">&lt;p&gt;Contemporary deep clustering approaches often rely on either contrastive or
non-contrastive techniques to acquire effective representations for clustering
tasks. Contrastive methods leverage negative pairs to achieve homogenous
representations but can introduce class collision issues, potentially
compromising clustering performance. On the contrary, non-contrastive
techniques prevent class collisions but may produce non-uniform representations
that lead to clustering collapse. In this work, we propose a novel end-to-end
deep clustering approach named PIPCDR, designed to harness the strengths of
both approaches while mitigating their limitations. PIPCDR incorporates a
positive instance proximity loss and a cluster dispersion regularizer. The
positive instance proximity loss ensures alignment between augmented views of
instances and their sampled neighbors, enhancing within-cluster compactness by
selecting genuinely positive pairs within the embedding space. Meanwhile, the
cluster dispersion regularizer maximizes inter-cluster distances while
minimizing within-cluster compactness, promoting uniformity in the learned
representations. PIPCDR excels in producing well-separated clusters, generating
uniform representations, avoiding class collision issues, and enhancing
within-cluster compactness. We extensively validate the effectiveness of PIPCDR
within an end-to-end Majorize-Minimization framework, demonstrating its
competitive performance on moderate-scale clustering benchmark datasets and
establishing new state-of-the-art results on large-scale datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Abhishek Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Dong-Gyu Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00734">
<title>On Manipulating Scene Text in the Wild with Diffusion Models. (arXiv:2311.00734v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.00734</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have gained attention for image editing yielding impressive
results in text-to-image tasks. On the downside, one might notice that
generated images of stable diffusion models suffer from deteriorated details.
This pitfall impacts image editing tasks that require information preservation
e.g., scene text editing. As a desired result, the model must show the
capability to replace the text on the source image to the target text while
preserving the details e.g., color, font size, and background. To leverage the
potential of diffusion models, in this work, we introduce Diffusion-BasEd Scene
Text manipulation Network so-called DBEST. Specifically, we design two
adaptation strategies, namely one-shot style adaptation and text-recognition
guidance. In experiments, we thoroughly assess and compare our proposed method
against state-of-the-arts on various scene text datasets, then provide
extensive ablation studies for each granularity to analyze our performance
gain. Also, we demonstrate the effectiveness of our proposed method to
synthesize scene text indicated by competitive Optical Character Recognition
(OCR) accuracy. Our method achieves 94.15% and 98.12% on COCO-text and
ICDAR2013 datasets for character-level evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santoso_J/0/1/0/all/0/1&quot;&gt;Joshua Santoso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simon_C/0/1/0/all/0/1&quot;&gt;Christian Simon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pao_W/0/1/0/all/0/1&quot;&gt;Williem Pao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00735">
<title>PET Tracer Conversion among Brain PET via Variable Augmented Invertible Network. (arXiv:2311.00735v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.00735</link>
<description rdf:parseType="Literal">&lt;p&gt;Positron emission tomography (PET), as an imaging technique with high
biochemical sensitivity, has been widely used in diagnosis of encephalopathy
and brain science research used in brain disease diagnosis and brain science
research. Since different tracers present different effects on the same focal
area, the choice of tracers is getting more significant for PET imaging.
Nowadays, with the wide application of PET imaging in neuropsychiatric
treatment, 6-18F-fluoro-3, 4-dihydroxy-L-phenylalanine (DOPA) has been found to
be more effective than 18F-labeled fluorine-2-deoxyglucose (FDG) in this field.
However, due to the complexity of its preparation and other limitations, DOPA
is far less widely used than FDG. To address this issue, a tracer conversion
invertible neural network (TC-INN) for image projection is developed to map FDG
images to DOPA images through deep learning. More diagnostic information is
obtained by generating PET images from FDG to DOPA. Specifically, the proposed
TC-INN consists of two separate phases, one for training the traceable data,
the other for re-building the new data. The reference DOPA PET image is used as
the learning target for the corresponding network during the training process
of tracer conversion. Mean-while, the invertible network iteratively estimates
the resultant DOPA PET data and compares it to the reference DOPA PET data.
Notably, the reversible model employed variable enhancement techniques to
achieve better power generation. Moreover, image registration needs to be
performed before training due to the angular deviation of the acquired FDG and
DOPA data information. Experimental results show generative ability in mapping
be-tween FDG images and DOPA images. It demonstrates great potential for PET
image conversion in the case of limited tracer applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1&quot;&gt;Bohui Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xubiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1&quot;&gt;Pengfei Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1&quot;&gt;Shirui Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1&quot;&gt;Xinchong Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiangsong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weirui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bingxuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qiegen Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00750">
<title>Are These the Same Apple? Comparing Images Based on Object Intrinsics. (arXiv:2311.00750v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.00750</link>
<description rdf:parseType="Literal">&lt;p&gt;The human visual system can effortlessly recognize an object under different
extrinsic factors such as lighting, object poses, and background, yet current
computer vision systems often struggle with these variations. An important step
to understanding and improving artificial vision systems is to measure image
similarity purely based on intrinsic object properties that define object
identity. This problem has been studied in the computer vision literature as
re-identification, though mostly restricted to specific object categories such
as people and cars. We propose to extend it to general object categories,
exploring an image similarity metric based on object intrinsics. To benchmark
such measurements, we collect the Common paired objects Under differenT
Extrinsics (CUTE) dataset of $18,000$ images of $180$ objects under different
extrinsic factors such as lighting, poses, and imaging conditions. While
existing methods such as LPIPS and CLIP scores do not measure object intrinsics
well, we find that combining deep features learned from contrastive
self-supervised learning with foreground filtering is a simple yet effective
approach to approximating the similarity. We conduct an extensive survey of
pre-trained features and foreground extraction methods to arrive at a strong
baseline that best measures intrinsic object-centric image similarity among
current methods. Finally, we demonstrate that our approach can aid in
downstream applications such as acting as an analog for human subjects and
improving generalizable re-identification. Please see our project website at
https://s-tian.github.io/projects/cute/ for visualizations of the data and
demos of our metric.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kotar_K/0/1/0/all/0/1&quot;&gt;Klemen Kotar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1&quot;&gt;Stephen Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hong-Xing Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamins_D/0/1/0/all/0/1&quot;&gt;Daniel L.K. Yamins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiajun Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00762">
<title>Challenges for Linguistically-Driven Computer-Based Sign Recognition from Continuous Signing for American Sign Language. (arXiv:2311.00762v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.00762</link>
<description rdf:parseType="Literal">&lt;p&gt;There have been recent advances in computer-based recognition of isolated,
citation-form signs from video. There are many challenges for such a task, not
least the naturally occurring inter- and intra- signer synchronic variation in
sign production, including sociolinguistic variation in the realization of
certain signs. However, there are several significant factors that make
recognition of signs from continuous signing an even more difficult problem.
This article presents an overview of such challenges, based in part on findings
from a large corpus of linguistically annotated video data for American Sign
Language (ASL). Some linguistic regularities in the structure of signs that can
boost handshape and sign recognition are also discussed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neidle_C/0/1/0/all/0/1&quot;&gt;Carol Neidle&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00796">
<title>Automatic counting of planting microsites via local visual detection and global count estimation. (arXiv:2311.00796v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.00796</link>
<description rdf:parseType="Literal">&lt;p&gt;In forest industry, mechanical site preparation by mounding is widely used
prior to planting operations. One of the main problems when planning planting
operations is the difficulty in estimating the number of mounds present on a
planting block, as their number may greatly vary depending on site
characteristics. This estimation is often carried out through field surveys by
several forestry workers. However, this procedure is prone to error and
slowness. Motivated by recent advances in UAV imagery and artificial
intelligence, we propose a fully automated framework to estimate the number of
mounds on a planting block. Using computer vision and machine learning, we
formulate the counting task as a supervised learning problem using two
prediction models. A local detection model is firstly used to detect visible
mounds based on deep features, while a global prediction function is
subsequently applied to provide a final estimation based on block-level
features. To evaluate the proposed method, we constructed a challenging UAV
dataset representing several plantation blocks with different characteristics.
The performed experiments demonstrated the robustness of the proposed method,
which outperforms manual methods in precision, while significantly reducing
time and cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zgaren_A/0/1/0/all/0/1&quot;&gt;Ahmed Zgaren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouachir_W/0/1/0/all/0/1&quot;&gt;Wassim Bouachir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouguila_N/0/1/0/all/0/1&quot;&gt;Nizar Bouguila&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00800">
<title>Beyond Still Images: Robust Multi-Stream Spatiotemporal Networks. (arXiv:2311.00800v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.00800</link>
<description rdf:parseType="Literal">&lt;p&gt;A defining characteristic of natural vision is its ability to withstand a
variety of input alterations, resulting in the creation of an invariant
representation of the surroundings. While convolutional neural networks exhibit
resilience to certain forms of spatial input variation, modifications in the
spatial and temporal aspects can significantly affect the representations of
video content in deep neural networks. Inspired by the resilience of natural
vision to input variations, we employ a simple multi-stream model to explore
its potential to address spatiotemporal changes by including temporal features.
Our primary goal is to introduce a video-trained model and evaluate its
robustness to diverse image and video inputs, with a particular focus on
exploring the role of temporal features in invariant recognition. Results show
that including videos and the temporal stream during training mitigates the
decline in accuracy and mAP in image and video understanding tasks by 1.36% and
3.14%, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fadaei_A/0/1/0/all/0/1&quot;&gt;AmirHosein Fadaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dehaqani_M/0/1/0/all/0/1&quot;&gt;Mohammad-Reza A. Dehaqani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00807">
<title>VQA-GEN: A Visual Question Answering Benchmark for Domain Generalization. (arXiv:2311.00807v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.00807</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual question answering (VQA) models are designed to demonstrate
visual-textual reasoning capabilities. However, their real-world applicability
is hindered by a lack of comprehensive benchmark datasets. Existing domain
generalization datasets for VQA exhibit a unilateral focus on textual shifts
while VQA being a multi-modal task contains shifts across both visual and
textual domains. We propose VQA-GEN, the first ever multi-modal benchmark
dataset for distribution shift generated through a shift induced pipeline.
Experiments demonstrate VQA-GEN dataset exposes the vulnerability of existing
methods to joint multi-modal distribution shifts. validating that comprehensive
multi-modal shifts are critical for robust VQA generalization. Models trained
on VQA-GEN exhibit improved cross-domain and in-domain performance, confirming
the value of VQA-GEN. Further, we analyze the importance of each shift
technique of our pipeline contributing to the generalization of the model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Unni_S/0/1/0/all/0/1&quot;&gt;Suraj Jyothi Unni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moraffah_R/0/1/0/all/0/1&quot;&gt;Raha Moraffah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00810">
<title>A Call to Arms: AI Should be Critical for Social Media Analysis of Conflict Zones. (arXiv:2311.00810v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2311.00810</link>
<description rdf:parseType="Literal">&lt;p&gt;The massive proliferation of social media data represents a transformative
moment in conflict studies. This data can provide unique insights into the
spread and use of weaponry, but the scale and types of data are problematic for
traditional open-source intelligence. This paper presents preliminary,
transdisciplinary work using computer vision to identify specific weapon
systems and the insignias of the armed groups using them. There is potential to
not only track how weapons are distributed through networks of armed units but
also to track which types of weapons are being used by the different types of
state and non-state military actors in Ukraine. Such a system could ultimately
be used to understand conflicts in real-time, including where humanitarian and
medical aid is most needed. We believe that using AI to help automate such
processes should be a high-priority goal for our community, with near-term
real-world payoffs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abedin_A/0/1/0/all/0/1&quot;&gt;Afia Abedin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bais_A/0/1/0/all/0/1&quot;&gt;Abdul Bais&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buntain_C/0/1/0/all/0/1&quot;&gt;Cody Buntain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Courchesne_L/0/1/0/all/0/1&quot;&gt;Laura Courchesne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McQuinn_B/0/1/0/all/0/1&quot;&gt;Brian McQuinn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_M/0/1/0/all/0/1&quot;&gt;Matthew E. Taylor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ullah_M/0/1/0/all/0/1&quot;&gt;Muhib Ullah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00917">
<title>RPCANet: Deep Unfolding RPCA Based Infrared Small Target Detection. (arXiv:2311.00917v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.00917</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning (DL) networks have achieved remarkable performance in infrared
small target detection (ISTD). However, these structures exhibit a deficiency
in interpretability and are widely regarded as black boxes, as they disregard
domain knowledge in ISTD. To alleviate this issue, this work proposes an
interpretable deep network for detecting infrared dim targets, dubbed RPCANet.
Specifically, our approach formulates the ISTD task as sparse target
extraction, low-rank background estimation, and image reconstruction in a
relaxed Robust Principle Component Analysis (RPCA) model. By unfolding the
iterative optimization updating steps into a deep-learning framework,
time-consuming and complex matrix calculations are replaced by theory-guided
neural networks. RPCANet detects targets with clear interpretability and
preserves the intrinsic image feature, instead of directly transforming the
detection task into a matrix decomposition problem. Extensive experiments
substantiate the effectiveness of our deep unfolding framework and demonstrate
its trustworthy results, surpassing baseline methods in both qualitative and
quantitative evaluations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Fengyi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianfang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yian Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1&quot;&gt;Zhenming Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00926">
<title>M2T2: Multi-Task Masked Transformer for Object-centric Pick and Place. (arXiv:2311.00926v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.00926</link>
<description rdf:parseType="Literal">&lt;p&gt;With the advent of large language models and large-scale robotic datasets,
there has been tremendous progress in high-level decision-making for object
manipulation. These generic models are able to interpret complex tasks using
language commands, but they often have difficulties generalizing to
out-of-distribution objects due to the inability of low-level action
primitives. In contrast, existing task-specific models excel in low-level
manipulation of unknown objects, but only work for a single type of action. To
bridge this gap, we present M2T2, a single model that supplies different types
of low-level actions that work robustly on arbitrary objects in cluttered
scenes. M2T2 is a transformer model which reasons about contact points and
predicts valid gripper poses for different action modes given a raw point cloud
of the scene. Trained on a large-scale synthetic dataset with 128K scenes, M2T2
achieves zero-shot sim2real transfer on the real robot, outperforming the
baseline system with state-of-the-art task-specific models by about 19% in
overall performance and 37.5% in challenging scenes where the object needs to
be re-oriented for collision-free placement. M2T2 also achieves
state-of-the-art results on a subset of language conditioned tasks in RLBench.
Videos of robot experiments on unseen objects in both real world and simulation
are available on our project website https://m2-t2.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1&quot;&gt;Wentao Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murali_A/0/1/0/all/0/1&quot;&gt;Adithyavairavan Murali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mousavian_A/0/1/0/all/0/1&quot;&gt;Arsalan Mousavian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1&quot;&gt;Dieter Fox&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00932">
<title>Towards High-quality HDR Deghosting with Conditional Diffusion Models. (arXiv:2311.00932v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.00932</link>
<description rdf:parseType="Literal">&lt;p&gt;High Dynamic Range (HDR) images can be recovered from several Low Dynamic
Range (LDR) images by existing Deep Neural Networks (DNNs) techniques. Despite
the remarkable progress, DNN-based methods still generate ghosting artifacts
when LDR images have saturation and large motion, which hinders potential
applications in real-world scenarios. To address this challenge, we formulate
the HDR deghosting problem as an image generation that leverages LDR features
as the diffusion model&apos;s condition, consisting of the feature condition
generator and the noise predictor. Feature condition generator employs
attention and Domain Feature Alignment (DFA) layer to transform the
intermediate features to avoid ghosting artifacts. With the learned features as
conditions, the noise predictor leverages a stochastic iterative denoising
process for diffusion models to generate an HDR image by steering the sampling
process. Furthermore, to mitigate semantic confusion caused by the saturation
problem of LDR images, we design a sliding window noise estimator to sample
smooth noise in a patch-based manner. In addition, an image space loss is
proposed to avoid the color distortion of the estimated HDR results. We
empirically evaluate our model on benchmark datasets for HDR imaging. The
results demonstrate that our approach achieves state-of-the-art performances
and well generalization to real-world images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1&quot;&gt;Qingsen Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1&quot;&gt;Tao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yuan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Hao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1&quot;&gt;Wei Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1&quot;&gt;Luc Van Gool&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yanning Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00936">
<title>SatBird: Bird Species Distribution Modeling with Remote Sensing and Citizen Science Data. (arXiv:2311.00936v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.00936</link>
<description rdf:parseType="Literal">&lt;p&gt;Biodiversity is declining at an unprecedented rate, impacting ecosystem
services necessary to ensure food, water, and human health and well-being.
Understanding the distribution of species and their habitats is crucial for
conservation policy planning. However, traditional methods in ecology for
species distribution models (SDMs) generally focus either on narrow sets of
species or narrow geographical areas and there remain significant knowledge
gaps about the distribution of species. A major reason for this is the limited
availability of data traditionally used, due to the prohibitive amount of
effort and expertise required for traditional field monitoring. The wide
availability of remote sensing data and the growing adoption of citizen science
tools to collect species observations data at low cost offer an opportunity for
improving biodiversity monitoring and enabling the modelling of complex
ecosystems. We introduce a novel task for mapping bird species to their
habitats by predicting species encounter rates from satellite images, and
present SatBird, a satellite dataset of locations in the USA with labels
derived from presence-absence observation data from the citizen science
database eBird, considering summer (breeding) and winter seasons. We also
provide a dataset in Kenya representing low-data regimes. We additionally
provide environmental data and species range maps for each location. We
benchmark a set of baselines on our dataset, including SOTA models for remote
sensing tasks. SatBird opens up possibilities for scalably modelling properties
of ecosystems worldwide.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teng_M/0/1/0/all/0/1&quot;&gt;M&amp;#xe9;lisande Teng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elmustafa_A/0/1/0/all/0/1&quot;&gt;Amna Elmustafa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akera_B/0/1/0/all/0/1&quot;&gt;Benjamin Akera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdelwahed_H/0/1/0/all/0/1&quot;&gt;Hager Radi Abdelwahed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Larochelle_H/0/1/0/all/0/1&quot;&gt;Hugo Larochelle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rolnick_D/0/1/0/all/0/1&quot;&gt;David Rolnick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00938">
<title>Bridging the Gap: Addressing Discrepancies in Diffusion Model Training for Classifier-Free Guidance. (arXiv:2311.00938v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.00938</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have emerged as a pivotal advancement in generative models,
setting new standards to the quality of the generated instances. In the current
paper we aim to underscore a discrepancy between conventional training methods
and the desired conditional sampling behavior of these models. While the
prevalent classifier-free guidance technique works well, it&apos;s not without
flaws. At higher values for the guidance scale parameter $w$, we often get out
of distribution samples and mode collapse, whereas at lower values for $w$ we
may not get the desired specificity. To address these challenges, we introduce
an updated loss function that better aligns training objectives with sampling
behaviors. Experimental validation with FID scores on CIFAR-10 elucidates our
method&apos;s ability to produce higher quality samples with fewer sampling
timesteps, and be more robust to the choice of guidance scale $w$. We also
experiment with fine-tuning Stable Diffusion on the proposed loss, to provide
early evidence that large diffusion models may also benefit from this refined
loss function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_N/0/1/0/all/0/1&quot;&gt;Niket Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salamanca_L/0/1/0/all/0/1&quot;&gt;Luis Salamanca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barba_L/0/1/0/all/0/1&quot;&gt;Luis Barba&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00941">
<title>Gaussian Mixture Solvers for Diffusion Models. (arXiv:2311.00941v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.00941</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, diffusion models have achieved great success in generative tasks.
Sampling from diffusion models is equivalent to solving the reverse diffusion
stochastic differential equations (SDEs) or the corresponding probability flow
ordinary differential equations (ODEs). In comparison, SDE-based solvers can
generate samples of higher quality and are suited for image translation tasks
like stroke-based synthesis. During inference, however, existing SDE-based
solvers are severely constrained by the efficiency-effectiveness dilemma. Our
investigation suggests that this is because the Gaussian assumption in the
reverse transition kernel is frequently violated (even in the case of simple
mixture data) given a limited number of discretization steps. To overcome this
limitation, we introduce a novel class of SDE-based solvers called
\emph{Gaussian Mixture Solvers (GMS)} for diffusion models. Our solver
estimates the first three-order moments and optimizes the parameters of a
Gaussian mixture transition kernel using generalized methods of moments in each
step during sampling. Empirically, our solver outperforms numerous SDE-based
solvers in terms of sample quality in image generation and stroke-based
synthesis in various diffusion models, which validates the motivation and
effectiveness of GMS. Our code is available at
https://github.com/Guohanzhong/GMS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1&quot;&gt;Hanzhong Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Cheng Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_F/0/1/0/all/0/1&quot;&gt;Fan Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1&quot;&gt;Tianyu Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1&quot;&gt;Shuicheng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1&quot;&gt;Chao Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chongxuan Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00949">
<title>Optimal Noise pursuit for Augmenting Text-to-Video Generation. (arXiv:2311.00949v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.00949</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the remarkable progress in text-to-video generation, existing
diffusion-based models often exhibit instability in terms of noise during
inference. Specifically, when different noises are fed for the given text,
these models produce videos that differ significantly in terms of both frame
quality and temporal consistency. With this observation, we posit that there
exists an optimal noise matched to each textual input; however, the widely
adopted strategies of random noise sampling often fail to capture it. In this
paper, we argue that the optimal noise can be approached through inverting the
groundtruth video using the established noise-video mapping derived from the
diffusion model. Nevertheless, the groundtruth video for the text prompt is not
available during inference. To address this challenge, we propose to
approximate the optimal noise via a search and inversion pipeline. Given a text
prompt, we initially search for a video from a predefined candidate pool that
closely relates to the text prompt. Subsequently, we invert the searched video
into the noise space, which serves as an improved noise prompt for the textual
input. In addition to addressing noise, we also observe that the text prompt
with richer details often leads to higher-quality videos. Motivated by this, we
further design a semantic-preserving rewriter to enrich the text prompt, where
a reference-guided rewriting is devised for reasonable details compensation,
and a denoising with a hybrid semantics strategy is proposed to preserve the
semantic consistency. Extensive experiments on the WebVid-10M benchmark show
that our proposed method can improve the text-to-video models with a clear
margin, while introducing no optimization burden.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Shijie Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Huayi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mengjian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_W/0/1/0/all/0/1&quot;&gt;Weidong Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Meng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaxiong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00961">
<title>Concatenated Masked Autoencoders as Spatial-Temporal Learner. (arXiv:2311.00961v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.00961</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning representations from videos requires understanding continuous motion
and visual correspondences between frames. In this paper, we introduce the
Concatenated Masked Autoencoders (CatMAE) as a spatial-temporal learner for
self-supervised video representation learning. For the input sequence of video
frames, CatMAE keeps the initial frame unchanged while applying substantial
masking (95%) to subsequent frames. The encoder in CatMAE is responsible for
encoding visible patches for each frame individually; subsequently, for each
masked frame, the decoder leverages visible patches from both previous and
current frames to reconstruct the original image. Our proposed method enables
the model to estimate the motion information between visible patches, match the
correspondences between preceding and succeeding frames, and ultimately learn
the evolution of scenes. Furthermore, we propose a new data augmentation
strategy, Video-Reverse (ViRe), which uses reversed video frames as the model&apos;s
reconstruction targets. This further encourages the model to utilize continuous
motion details and correspondences to complete the reconstruction, thereby
enhancing the model&apos;s capabilities. Compared to the most advanced pre-training
methods, CatMAE achieves a leading level in video segmentation tasks and action
recognition tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhouqiang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bowen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1&quot;&gt;Tong Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1&quot;&gt;Zhaofeng Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Hong Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guangshun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Liangzhi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00962">
<title>Detecting Generated Images by Real Images Only. (arXiv:2311.00962v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.00962</link>
<description rdf:parseType="Literal">&lt;p&gt;As deep learning technology continues to evolve, the images yielded by
generative models are becoming more and more realistic, triggering people to
question the authenticity of images. Existing generated image detection methods
detect visual artifacts in generated images or learn discriminative features
from both real and generated images by massive training. This learning paradigm
will result in efficiency and generalization issues, making detection methods
always lag behind generation methods. This paper approaches the generated image
detection problem from a new perspective: Start from real images. By finding
the commonality of real images and mapping them to a dense subspace in feature
space, the goal is that generated images, regardless of their generative model,
are then projected outside the subspace. As a result, images from different
generative models can be detected, solving some long-existing problems in the
field. Experimental results show that although our method was trained only by
real images and uses 99.9\% less training data than other deep learning-based
methods, it can compete with state-of-the-art methods and shows excellent
performance in detecting emerging generative models with high inference
efficiency. Moreover, the proposed method shows robustness against various
post-processing. These advantages allow the method to be used in real-world
scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_X/0/1/0/all/0/1&quot;&gt;Xiuli Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Fan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1&quot;&gt;Bin Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Weisheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1&quot;&gt;Gao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cosman_P/0/1/0/all/0/1&quot;&gt;Pamela C. Cosman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00970">
<title>Lightweight super resolution network for point cloud geometry compression. (arXiv:2311.00970v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.00970</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents an approach for compressing point cloud geometry by
leveraging a lightweight super-resolution network. The proposed method involves
decomposing a point cloud into a base point cloud and the interpolation
patterns for reconstructing the original point cloud. While the base point
cloud can be efficiently compressed using any lossless codec, such as
Geometry-based Point Cloud Compression, a distinct strategy is employed for
handling the interpolation patterns. Rather than directly compressing the
interpolation patterns, a lightweight super-resolution network is utilized to
learn this information through overfitting. Subsequently, the network parameter
is transmitted to assist in point cloud reconstruction at the decoder side.
Notably, our approach differentiates itself from lookup table-based methods,
allowing us to obtain more accurate interpolation patterns by accessing a
broader range of neighboring voxels at an acceptable computational cost.
Experiments on MPEG Cat1 (Solid) and Cat2 datasets demonstrate the remarkable
compression performance achieved by our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dingquan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Ge Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gao_W/0/1/0/all/0/1&quot;&gt;Wen Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00979">
<title>Overhead Line Defect Recognition Based on Unsupervised Semantic Segmentation. (arXiv:2311.00979v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.00979</link>
<description rdf:parseType="Literal">&lt;p&gt;Overhead line inspection greatly benefits from defect recognition using
visible light imagery. Addressing the limitations of existing feature
extraction techniques and the heavy data dependency of deep learning
approaches, this paper introduces a novel defect recognition framework. This is
built on the Faster RCNN network and complemented by unsupervised semantic
segmentation. The approach involves identifying the type and location of the
target equipment, utilizing semantic segmentation to differentiate between the
device and its backdrop, and finally employing similarity measures and logical
rules to categorize the type of defect. Experimental results indicate that this
methodology focuses more on the equipment rather than the defects when
identifying issues in overhead lines. This leads to a notable enhancement in
accuracy and exhibits impressive adaptability. Thus, offering a fresh
perspective for automating the inspection of distribution network equipment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weixi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1&quot;&gt;Xichen Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sizhe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xun Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00980">
<title>MAAIG: Motion Analysis And Instruction Generation. (arXiv:2311.00980v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.00980</link>
<description rdf:parseType="Literal">&lt;p&gt;Many people engage in self-directed sports training at home but lack the
real-time guidance of professional coaches, making them susceptible to injuries
or the development of incorrect habits. In this paper, we propose a novel
application framework called MAAIG(Motion Analysis And Instruction Generation).
It can generate embedding vectors for each frame based on user-provided sports
action videos. These embedding vectors are associated with the 3D skeleton of
each frame and are further input into a pretrained T5 model. Ultimately, our
model utilizes this information to generate specific sports instructions. It
has the capability to identify potential issues and provide real-time guidance
in a manner akin to professional coaches, helping users improve their sports
skills and avoid injuries.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeh_W/0/1/0/all/0/1&quot;&gt;Wei-Hsin Yeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1&quot;&gt;Pei Hsin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yu-An Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1&quot;&gt;Wen Hsiang Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ku_L/0/1/0/all/0/1&quot;&gt;Lun-Wei Ku&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00986">
<title>M&amp;M3D: Multi-Dataset Training and Efficient Network for Multi-view 3D Object Detection. (arXiv:2311.00986v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.00986</link>
<description rdf:parseType="Literal">&lt;p&gt;In this research, I proposed a network structure for multi-view 3D object
detection using camera-only data and a Bird&apos;s-Eye-View map. My work is based on
a current key challenge domain adaptation and visual data transfer. Although
many excellent camera-only 3D object detection has been continuously proposed,
many research work risk dramatic performance drop when the networks are trained
on the source domain but tested on a different target domain. Then I found it
is very surprising that predictions on bounding boxes and classes are still
replied to on 2D networks. Based on the domain gap assumption on various 3D
datasets, I found they still shared a similar data extraction on the same BEV
map size and camera data transfer. Therefore, to analyze the domain gap
influence on the current method and to make good use of 3D space information
among the dataset and the real world, I proposed a transfer learning method and
Transformer construction to study the 3D object detection on NuScenes-mini and
Lyft. Through multi-dataset training and a detection head from the Transformer,
the network demonstrated good data migration performance and efficient
detection performance by using 3D anchor query and 3D positional information.
Relying on only a small amount of source data and the existing large model
pre-training weights, the efficient network manages to achieve competitive
results on the new target domain. Moreover, my study utilizes 3D information as
available semantic information and 2D multi-view image features blending into
the visual-language transfer design. In the final 3D anchor box prediction and
object classification, my network achieved good results on standard metrics of
3D object detection, which differs from dataset-specific models on each
training domain without any fine-tuning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00987">
<title>CML-MOTS: Collaborative Multi-task Learning for Multi-Object Tracking and Segmentation. (arXiv:2311.00987v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.00987</link>
<description rdf:parseType="Literal">&lt;p&gt;The advancement of computer vision has pushed visual analysis tasks from
still images to the video domain. In recent years, video instance segmentation,
which aims to track and segment multiple objects in video frames, has drawn
much attention for its potential applications in various emerging areas such as
autonomous driving, intelligent transportation, and smart retail. In this
paper, we propose an effective framework for instance-level visual analysis on
video frames, which can simultaneously conduct object detection, instance
segmentation, and multi-object tracking. The core idea of our method is
collaborative multi-task learning which is achieved by a novel structure, named
associative connections among detection, segmentation, and tracking task heads
in an end-to-end learnable CNN. These additional connections allow information
propagation across multiple related tasks, so as to benefit these tasks
simultaneously. We evaluate the proposed method extensively on KITTI MOTS and
MOTS Challenge datasets and obtain quite encouraging results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1&quot;&gt;Yiming Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1&quot;&gt;Cheng Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dongfang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00990">
<title>VideoDreamer: Customized Multi-Subject Text-to-Video Generation with Disen-Mix Finetuning. (arXiv:2311.00990v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.00990</link>
<description rdf:parseType="Literal">&lt;p&gt;Customized text-to-video generation aims to generate text-guided videos with
customized user-given subjects, which has gained increasing attention recently.
However, existing works are primarily limited to generating videos for a single
subject, leaving the more challenging problem of customized multi-subject
text-to-video generation largely unexplored. In this paper, we fill this gap
and propose a novel VideoDreamer framework. VideoDreamer can generate
temporally consistent text-guided videos that faithfully preserve the visual
features of the given multiple subjects. Specifically, VideoDreamer leverages
the pretrained Stable Diffusion with latent-code motion dynamics and temporal
cross-frame attention as the base video generator. The video generator is
further customized for the given multiple subjects by the proposed Disen-Mix
Finetuning and Human-in-the-Loop Re-finetuning strategy, which can tackle the
attribute binding problem of multi-subject generation. We also introduce
MultiStudioBench, a benchmark for evaluating customized multi-subject
text-to-video generation models. Extensive experiments demonstrate the
remarkable ability of VideoDreamer to generate videos with new content such as
new events and backgrounds, tailored to the customized multiple subjects. Our
project page is available at https://videodreamer23.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1&quot;&gt;Guanning Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yipeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuwei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1&quot;&gt;Feilin Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wenwu Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00991">
<title>IR-UWB Radar-based Situational Awareness System for Smartphone-Distracted Pedestrians. (arXiv:2311.00991v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.00991</link>
<description rdf:parseType="Literal">&lt;p&gt;With the widespread adoption of smartphones, ensuring pedestrian safety on
roads has become a critical concern due to smartphone distraction. This paper
proposes a novel and real-time assistance system called UWB-assisted Safe Walk
(UASW) for obstacle detection and warns users about real-time situations. The
proposed method leverages Impulse Radio Ultra-Wideband (IR-UWB) radar embedded
in the smartphone, which provides excellent range resolution and high noise
resilience using short pulses. We implemented UASW specifically for Android
smartphones with IR-UWB connectivity. The framework uses complex Channel
Impulse Response (CIR) data to integrate rule-based obstacle detection with
artificial neural network (ANN) based obstacle classification. The performance
of the proposed UASW system is analyzed using real-time collected data. The
results show that the proposed system achieves an obstacle detection accuracy
of up to 97% and obstacle classification accuracy of up to 95% with an
inference delay of 26.8 ms. The results highlight the effectiveness of UASW in
assisting smartphone-distracted pedestrians and improving their situational
awareness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ppallan_J/0/1/0/all/0/1&quot;&gt;Jamsheed Manja Ppallan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandey_R/0/1/0/all/0/1&quot;&gt;Ruchi Pandey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damam_Y/0/1/0/all/0/1&quot;&gt;Yellappa Damam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tiwari_V/0/1/0/all/0/1&quot;&gt;Vijay Narayan Tiwari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arunachalam_K/0/1/0/all/0/1&quot;&gt;Karthikeyan Arunachalam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1&quot;&gt;Antariksha Ray&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00994">
<title>LaughTalk: Expressive 3D Talking Head Generation with Laughter. (arXiv:2311.00994v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.00994</link>
<description rdf:parseType="Literal">&lt;p&gt;Laughter is a unique expression, essential to affirmative social interactions
of humans. Although current 3D talking head generation methods produce
convincing verbal articulations, they often fail to capture the vitality and
subtleties of laughter and smiles despite their importance in social context.
In this paper, we introduce a novel task to generate 3D talking heads capable
of both articulate speech and authentic laughter. Our newly curated dataset
comprises 2D laughing videos paired with pseudo-annotated and human-validated
3D FLAME parameters and vertices. Given our proposed dataset, we present a
strong baseline with a two-stage training scheme: the model first learns to
talk and then acquires the ability to express laughter. Extensive experiments
demonstrate that our method performs favorably compared to existing approaches
in both talking head generation and expressing laughter signals. We further
explore potential applications on top of our proposed method for rigging
realistic avatars.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sung_Bin_K/0/1/0/all/0/1&quot;&gt;Kim Sung-Bin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hyun_L/0/1/0/all/0/1&quot;&gt;Lee Hyun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1&quot;&gt;Da Hye Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nam_S/0/1/0/all/0/1&quot;&gt;Suekyeong Nam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ju_J/0/1/0/all/0/1&quot;&gt;Janghoon Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_T/0/1/0/all/0/1&quot;&gt;Tae-Hyun Oh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00995">
<title>A Chronological Survey of Theoretical Advancements in Generative Adversarial Networks for Computer Vision. (arXiv:2311.00995v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.00995</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Networks (GANs) have been workhorse generative models
for last many years, especially in the research field of computer vision.
Accordingly, there have been many significant advancements in the theory and
application of GAN models, which are notoriously hard to train, but produce
good results if trained well. There have been many a surveys on GANs,
organizing the vast GAN literature from various focus and perspectives.
However, none of the surveys brings out the important chronological aspect: how
the multiple challenges of employing GAN models were solved one-by-one over
time, across multiple landmark research works. This survey intends to bridge
that gap and present some of the landmark research works on the theory and
application of GANs, in chronological order.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_H/0/1/0/all/0/1&quot;&gt;Hrishikesh Sharma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00996">
<title>VCISR: Blind Single Image Super-Resolution with Video Compression Synthetic Data. (arXiv:2311.00996v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.00996</link>
<description rdf:parseType="Literal">&lt;p&gt;In the blind single image super-resolution (SISR) task, existing works have
been successful in restoring image-level unknown degradations. However, when a
single video frame becomes the input, these works usually fail to address
degradations caused by video compression, such as mosquito noise, ringing,
blockiness, and staircase noise. In this work, we for the first time, present a
video compression-based degradation model to synthesize low-resolution image
data in the blind SISR task. Our proposed image synthesizing method is widely
applicable to existing image datasets, so that a single degraded image can
contain distortions caused by the lossy video compression algorithms. This
overcomes the leak of feature diversity in video data and thus retains the
training efficiency. By introducing video coding artifacts to SISR degradation
models, neural networks can super-resolve images with the ability to restore
video compression degradations, and achieve better results on restoring generic
distortions caused by image compression as well. Our proposed approach achieves
superior performance in SOTA no-reference Image Quality Assessment, and shows
better visual quality on various datasets. In addition, we evaluate the SISR
neural network trained with our degradation model on video super-resolution
(VSR) datasets. Compared to architectures specifically designed for the VSR
purpose, our method exhibits similar or better performance, evidencing that the
presented strategy on infusing video-based degradation is generalizable to
address more complicated compression artifacts even without temporal cues.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Boyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bowen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shiyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Fengyu Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01001">
<title>Fully Quantized Always-on Face Detector Considering Mobile Image Sensors. (arXiv:2311.01001v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01001</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite significant research on lightweight deep neural networks (DNNs)
designed for edge devices, the current face detectors do not fully meet the
requirements for &quot;intelligent&quot; CMOS image sensors (iCISs) integrated with
embedded DNNs. These sensors are essential in various practical applications,
such as energy-efficient mobile phones and surveillance systems with always-on
capabilities. One noteworthy limitation is the absence of suitable face
detectors for the always-on scenario, a crucial aspect of image sensor-level
applications. These detectors must operate directly with sensor RAW data before
the image signal processor (ISP) takes over. This gap poses a significant
challenge in achieving optimal performance in such scenarios. Further research
and development are necessary to bridge this gap and fully leverage the
potential of iCIS applications. In this study, we aim to bridge the gap by
exploring extremely low-bit lightweight face detectors, focusing on the
always-on face detection scenario for mobile image sensor applications. To
achieve this, our proposed model utilizes sensor-aware synthetic RAW inputs,
simulating always-on face detection processed &quot;before&quot; the ISP chain. Our
approach employs ternary (-1, 0, 1) weights for potential implementations in
image sensors, resulting in a relatively simple network architecture with
shallow layers and extremely low-bitwidth. Our method demonstrates reasonable
face detection performance and excellent efficiency in simulation studies,
offering promising possibilities for practical always-on face detectors in
real-world applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Haechang Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_W/0/1/0/all/0/1&quot;&gt;Wongi Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryu_D/0/1/0/all/0/1&quot;&gt;Dongil Ryu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Je_H/0/1/0/all/0/1&quot;&gt;Hyunwoo Je&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+No_A/0/1/0/all/0/1&quot;&gt;Albert No&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kijeong Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1&quot;&gt;Se Young Chun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01004">
<title>Sam-Guided Enhanced Fine-Grained Encoding with Mixed Semantic Learning for Medical Image Captioning. (arXiv:2311.01004v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01004</link>
<description rdf:parseType="Literal">&lt;p&gt;With the development of multimodality and large language models, the deep
learning-based technique for medical image captioning holds the potential to
offer valuable diagnostic recommendations. However, current generic text and
image pre-trained models do not yield satisfactory results when it comes to
describing intricate details within medical images. In this paper, we present a
novel medical image captioning method guided by the segment anything model
(SAM) to enable enhanced encoding with both general and detailed feature
extraction. In addition, our approach employs a distinctive pre-training
strategy with mixed semantic learning to simultaneously capture both the
overall information and finer details within medical images. We demonstrate the
effectiveness of this approach, as it outperforms the pre-trained BLIP2 model
on various evaluation metrics for generating descriptions of medical images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Gaoang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Benlu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1&quot;&gt;Weijie Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yizhi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xuechen Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guanhong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shiyan Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01009">
<title>Revamping AI Models in Dermatology: Overcoming Critical Challenges for Enhanced Skin Lesion Diagnosis. (arXiv:2311.01009v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01009</link>
<description rdf:parseType="Literal">&lt;p&gt;The surge in developing deep learning models for diagnosing skin lesions
through image analysis is notable, yet their clinical black faces challenges.
Current dermatology AI models have limitations: limited number of possible
diagnostic outputs, lack of real-world testing on uncommon skin lesions,
inability to detect out-of-distribution images, and over-reliance on
dermoscopic images. To address these, we present an All-In-One
\textbf{H}ierarchical-\textbf{O}ut of Distribution-\textbf{C}linical Triage
(HOT) model. For a clinical image, our model generates three outputs: a
hierarchical prediction, an alert for out-of-distribution images, and a
recommendation for dermoscopy if clinical image alone is insufficient for
diagnosis. When the recommendation is pursued, it integrates both clinical and
dermoscopic images to deliver final diagnosis. Extensive experiments on a
representative cutaneous lesion dataset demonstrate the effectiveness and
synergy of each component within our framework. Our versatile model provides
valuable decision support for lesion diagnosis and sets a promising precedent
for medical AI applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehta_D/0/1/0/all/0/1&quot;&gt;Deval Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Betz_Stablein_B/0/1/0/all/0/1&quot;&gt;Brigid Betz-Stablein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Toan D Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1&quot;&gt;Yaniv Gal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bowling_A/0/1/0/all/0/1&quot;&gt;Adrian Bowling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haskett_M/0/1/0/all/0/1&quot;&gt;Martin Haskett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sashindranath_M/0/1/0/all/0/1&quot;&gt;Maithili Sashindranath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonnington_P/0/1/0/all/0/1&quot;&gt;Paul Bonnington&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mar_V/0/1/0/all/0/1&quot;&gt;Victoria Mar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soyer_H/0/1/0/all/0/1&quot;&gt;H Peter Soyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1&quot;&gt;Zongyuan Ge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01010">
<title>Exploring Unified Perspective For Fast Shapley Value Estimation. (arXiv:2311.01010v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.01010</link>
<description rdf:parseType="Literal">&lt;p&gt;Shapley values have emerged as a widely accepted and trustworthy tool,
grounded in theoretical axioms, for addressing challenges posed by black-box
models like deep neural networks. However, computing Shapley values encounters
exponential complexity in the number of features. Various approaches, including
ApproSemivalue, KernelSHAP, and FastSHAP, have been explored to expedite the
computation. We analyze the consistency of existing works and conclude that
stochastic estimators can be unified as the linear transformation of importance
sampling of feature subsets. Based on this, we investigate the possibility of
designing simple amortized estimators and propose a straightforward and
efficient one, SimSHAP, by eliminating redundant techniques. Extensive
experiments conducted on tabular and image datasets validate the effectiveness
of our SimSHAP, which significantly accelerates the computation of accurate
Shapley values.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Borui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1&quot;&gt;Baotong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1&quot;&gt;Wenzhao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jie Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiwen Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01015">
<title>Act As You Wish: Fine-Grained Control of Motion Diffusion Model with Hierarchical Semantic Graphs. (arXiv:2311.01015v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01015</link>
<description rdf:parseType="Literal">&lt;p&gt;Most text-driven human motion generation methods employ sequential modeling
approaches, e.g., transformer, to extract sentence-level text representations
automatically and implicitly for human motion synthesis. However, these compact
text representations may overemphasize the action names at the expense of other
important properties and lack fine-grained details to guide the synthesis of
subtly distinct motion. In this paper, we propose hierarchical semantic graphs
for fine-grained control over motion generation. Specifically, we disentangle
motion descriptions into hierarchical semantic graphs including three levels of
motions, actions, and specifics. Such global-to-local structures facilitate a
comprehensive understanding of motion description and fine-grained control of
motion generation. Correspondingly, to leverage the coarse-to-fine topology of
hierarchical semantic graphs, we decompose the text-to-motion diffusion process
into three semantic levels, which correspond to capturing the overall motion,
local actions, and action specifics. Extensive experiments on two benchmark
human motion datasets, including HumanML3D and KIT, with superior performances,
justify the efficacy of our method. More encouragingly, by modifying the edge
weights of hierarchical semantic graphs, our method can continuously refine the
generated motion, which may have a far-reaching impact on the community. Code
and pre-training weights are available at
https://github.com/jpthu17/GraphMotion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1&quot;&gt;Peng Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yanbo Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhongqian Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yang Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1&quot;&gt;Li Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01016">
<title>Visual Analytics for Efficient Image Exploration and User-Guided Image Captioning. (arXiv:2311.01016v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01016</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in pre-trained large-scale language-image models have
ushered in a new era of visual comprehension, offering a significant leap
forward. These breakthroughs have proven particularly instrumental in
addressing long-standing challenges that were previously daunting. Leveraging
these innovative techniques, this paper tackles two well-known issues within
the realm of visual analytics: (1) the efficient exploration of large-scale
image datasets and identification of potential data biases within them; (2) the
evaluation of image captions and steering of their generation process. On the
one hand, by visually examining the captions automatically generated from
language-image models for an image dataset, we gain deeper insights into the
semantic underpinnings of the visual contents, unearthing data biases that may
be entrenched within the dataset. On the other hand, by depicting the
association between visual contents and textual captions, we expose the
weaknesses of pre-trained language-image models in their captioning capability
and propose an interactive interface to steer caption generation. The two parts
have been coalesced into a coordinated visual analytics system, fostering
mutual enrichment of visual and textual elements. We validate the effectiveness
of the system with domain practitioners through concrete case studies with
large-scale image datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yiran Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junpeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aboagye_P/0/1/0/all/0/1&quot;&gt;Prince Aboagye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeh_M/0/1/0/all/0/1&quot;&gt;Michael Yeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1&quot;&gt;Kwan-Liu Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01017">
<title>Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion. (arXiv:2311.01017v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01017</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning world models can teach an agent how the world works in an
unsupervised manner. Even though it can be viewed as a special case of sequence
modeling, progress for scaling world models on robotic applications such as
autonomous driving has been somewhat less rapid than scaling language models
with Generative Pre-trained Transformers (GPT). We identify two reasons as
major bottlenecks: dealing with complex and unstructured observation space, and
having a scalable generative model. Consequently, we propose a novel world
modeling approach that first tokenizes sensor observations with VQVAE, then
predicts the future via discrete diffusion. To efficiently decode and denoise
tokens in parallel, we recast Masked Generative Image Transformer into the
discrete diffusion framework with a few simple changes, resulting in notable
improvement. When applied to learning world models on point cloud observations,
our model reduces prior SOTA Chamfer distance by more than 65% for 1s
prediction, and more than 50% for 3s prediction, across NuScenes, KITTI
Odometry, and Argoverse2 datasets. Our results demonstrate that discrete
diffusion on tokenized agent experience can unlock the power of GPT-like
unsupervised learning for robotic agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lunjun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1&quot;&gt;Yuwen Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Ze Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casas_S/0/1/0/all/0/1&quot;&gt;Sergio Casas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1&quot;&gt;Rui Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1&quot;&gt;Raquel Urtasun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01018">
<title>Expanding Expressiveness of Diffusion Models with Limited Data via Self-Distillation based Fine-Tuning. (arXiv:2311.01018v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01018</link>
<description rdf:parseType="Literal">&lt;p&gt;Training diffusion models on limited datasets poses challenges in terms of
limited generation capacity and expressiveness, leading to unsatisfactory
results in various downstream tasks utilizing pretrained diffusion models, such
as domain translation and text-guided image manipulation. In this paper, we
propose Self-Distillation for Fine-Tuning diffusion models (SDFT), a
methodology to address these challenges by leveraging diverse features from
diffusion models pretrained on large source datasets. SDFT distills more
general features (shape, colors, etc.) and less domain-specific features
(texture, fine details, etc) from the source model, allowing successful
knowledge transfer without disturbing the training process on target datasets.
The proposed method is not constrained by the specific architecture of the
model and thus can be generally adopted to existing frameworks. Experimental
results demonstrate that SDFT enhances the expressiveness of the diffusion
model with limited datasets, resulting in improved generation capabilities
across various downstream tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hur_J/0/1/0/all/0/1&quot;&gt;Jiwan Hur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jaehyun Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_G/0/1/0/all/0/1&quot;&gt;Gyojin Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Dong-Jae Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Junmo Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01022">
<title>NeuroWrite: Predictive Handwritten Digit Classification using Deep Neural Networks. (arXiv:2311.01022v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01022</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid evolution of deep neural networks has revolutionized the field of
machine learning, enabling remarkable advancements in various domains. In this
article, we introduce NeuroWrite, a unique method for predicting the
categorization of handwritten digits using deep neural networks. Our model
exhibits outstanding accuracy in identifying and categorising handwritten
digits by utilising the strength of convolutional neural networks (CNNs) and
recurrent neural networks (RNNs).In this article, we give a thorough
examination of the data preparation methods, network design, and training
methods used in NeuroWrite. By implementing state-of-the-art techniques, we
showcase how NeuroWrite can achieve high classification accuracy and robust
generalization on handwritten digit datasets, such as MNIST. Furthermore, we
explore the model&apos;s potential for real-world applications, including digit
recognition in digitized documents, signature verification, and automated
postal code recognition. NeuroWrite is a useful tool for computer vision and
pattern recognition because of its performance and adaptability.The
architecture, training procedure, and evaluation metrics of NeuroWrite are
covered in detail in this study, illustrating how it can improve a number of
applications that call for handwritten digit classification. The outcomes show
that NeuroWrite is a promising method for raising the bar for deep neural
network-based handwritten digit recognition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asish_K/0/1/0/all/0/1&quot;&gt;Kottakota Asish&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teja_P/0/1/0/all/0/1&quot;&gt;P. Sarath Teja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chander_R/0/1/0/all/0/1&quot;&gt;R. Kishan Chander&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hema_D/0/1/0/all/0/1&quot;&gt;Dr. D. Deva Hema&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01023">
<title>Augmentation is AUtO-Net: Augmentation-Driven Contrastive Multiview Learning for Medical Image Segmentation. (arXiv:2311.01023v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01023</link>
<description rdf:parseType="Literal">&lt;p&gt;The utilisation of deep learning segmentation algorithms that learn complex
organs and tissue patterns and extract essential regions of interest from the
noisy background to improve the visual ability for medical image diagnosis has
achieved impressive results in Medical Image Computing (MIC). This thesis
focuses on retinal blood vessel segmentation tasks, providing an extensive
literature review of deep learning-based medical image segmentation approaches
while comparing the methodologies and empirical performances. The work also
examines the limitations of current state-of-the-art methods by pointing out
the two significant existing limitations: data size constraints and the
dependency on high computational resources. To address such problems, this work
proposes a novel efficient, simple multiview learning framework that
contrastively learns invariant vessel feature representation by comparing with
multiple augmented views by various transformations to overcome data shortage
and improve generalisation ability. Moreover, the hybrid network architecture
integrates the attention mechanism into a Convolutional Neural Network to
further capture complex continuous curvilinear vessel structures. The result
demonstrates the proposed method validated on the CHASE-DB1 dataset, attaining
the highest F1 score of 83.46% and the highest Intersection over Union (IOU)
score of 71.62% with UNet structure, surpassing existing benchmark UNet-based
methods by 1.95% and 2.8%, respectively. The combination of the metrics
indicates the model detects the vessel object accurately with a highly
coincidental location with the ground truth. Moreover, the proposed approach
could be trained within 30 minutes by consuming less than 3 GB GPU RAM, and
such characteristics support the efficient implementation for real-world
applications and deployments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yanming Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01025">
<title>Incorporating Language-Driven Appearance Knowledge Units with Visual Cues in Pedestrian Detection. (arXiv:2311.01025v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01025</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have shown their capability in understanding
contextual and semantic information regarding appearance knowledge of
instances. In this paper, we introduce a novel approach to utilize the strength
of an LLM in understanding contextual appearance variations and to leverage its
knowledge into a vision model (here, pedestrian detection). While pedestrian
detection is considered one of crucial tasks directly related with our safety
(e.g., intelligent driving system), it is challenging because of varying
appearances and poses in diverse scenes. Therefore, we propose to formulate
language-driven appearance knowledge units and incorporate them with visual
cues in pedestrian detection. To this end, we establish description corpus
which includes numerous narratives describing various appearances of
pedestrians and others. By feeding them through an LLM, we extract appearance
knowledge sets that contain the representations of appearance variations. After
that, we perform a task-prompting process to obtain appearance knowledge units
which are representative appearance knowledge guided to be relevant to a
downstream pedestrian detection task. Finally, we provide plentiful appearance
information by integrating the language-driven knowledge units with visual
cues. Through comprehensive experiments with various pedestrian detectors, we
verify the effectiveness of our method showing noticeable performance gains and
achieving state-of-the-art detection performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Sungjune Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyunjun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ro_Y/0/1/0/all/0/1&quot;&gt;Yong Man Ro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01028">
<title>Nonnegative/Binary Matrix Factorization for Image Classification using Quantum Annealing. (arXiv:2311.01028v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2311.01028</link>
<description rdf:parseType="Literal">&lt;p&gt;Classical computing has borne witness to the development of machine learning.
The integration of quantum technology into this mix will lead to unimaginable
benefits and be regarded as a giant leap forward in mankind&apos;s ability to
compute. Demonstrating the benefits of this integration now becomes essential.
With the advance of quantum computing, several machine-learning techniques have
been proposed that use quantum annealing. In this study, we implement a matrix
factorization method using quantum annealing for image classification and
compare the performance with traditional machine-learning methods.
Nonnegative/binary matrix factorization (NBMF) was originally introduced as a
generative model, and we propose a multiclass classification model as an
application. We extract the features of handwritten digit images using NBMF and
apply them to solve the classification problem. Our findings show that when the
amount of data, features, and epochs is small, the accuracy of models trained
by NBMF is superior to classical machine-learning methods, such as neural
networks. Moreover, we found that training models using a quantum annealing
solver significantly reduces computation time. Under certain conditions, there
is a benefit to using quantum annealing technology with machine learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Asaoka_H/0/1/0/all/0/1&quot;&gt;Hinako Asaoka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Kudo_K/0/1/0/all/0/1&quot;&gt;Kazue Kudo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01034">
<title>Learning to Adapt CLIP for Few-Shot Monocular Depth Estimation. (arXiv:2311.01034v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01034</link>
<description rdf:parseType="Literal">&lt;p&gt;Pre-trained Vision-Language Models (VLMs), such as CLIP, have shown enhanced
performance across a range of tasks that involve the integration of visual and
linguistic modalities. When CLIP is used for depth estimation tasks, the
patches, divided from the input images, can be combined with a series of
semantic descriptions of the depth information to obtain similarity results.
The coarse estimation of depth is then achieved by weighting and summing the
depth values, called depth bins, corresponding to the predefined semantic
descriptions. The zero-shot approach circumvents the computational and
time-intensive nature of traditional fully-supervised depth estimation methods.
However, this method, utilizing fixed depth bins, may not effectively
generalize as images from different scenes may exhibit distinct depth
distributions. To address this challenge, we propose a few-shot-based method
which learns to adapt the VLMs for monocular depth estimation to balance
training costs and generalization capabilities. Specifically, it assigns
different depth bins for different scenes, which can be selected by the model
during inference. Additionally, we incorporate learnable prompts to preprocess
the input text to convert the easily human-understood text into easily
model-understood vectors and further enhance the performance. With only one
image per scene for training, our extensive experiment results on the NYU V2
and KITTI dataset demonstrate that our method outperforms the previous
state-of-the-art method by up to 10.6\% in terms of MARE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xueting Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Ce Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hai_B/0/1/0/all/0/1&quot;&gt;Bowen Hai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1&quot;&gt;Ke Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zhihai He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01057">
<title>Ultra-Efficient On-Device Object Detection on AI-Integrated Smart Glasses with TinyissimoYOLO. (arXiv:2311.01057v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01057</link>
<description rdf:parseType="Literal">&lt;p&gt;Smart glasses are rapidly gaining advanced functionality thanks to
cutting-edge computing technologies, accelerated hardware architectures, and
tiny AI algorithms. Integrating AI into smart glasses featuring a small form
factor and limited battery capacity is still challenging when targeting
full-day usage for a satisfactory user experience. This paper illustrates the
design and implementation of tiny machine-learning algorithms exploiting novel
low-power processors to enable prolonged continuous operation in smart glasses.
We explore the energy- and latency-efficient of smart glasses in the case of
real-time object detection. To this goal, we designed a smart glasses prototype
as a research platform featuring two microcontrollers, including a novel
milliwatt-power RISC-V parallel processor with a hardware accelerator for
visual AI, and a Bluetooth low-power module for communication. The smart
glasses integrate power cycling mechanisms, including image and audio sensing
interfaces. Furthermore, we developed a family of novel tiny deep-learning
models based on YOLO with sub-million parameters customized for
microcontroller-based inference dubbed TinyissimoYOLO v1.3, v5, and v8, aiming
at benchmarking object detection with smart glasses for energy and latency.
Evaluations on the prototype of the smart glasses demonstrate TinyissimoYOLO&apos;s
17ms inference latency and 1.59mJ energy consumption per inference while
ensuring acceptable detection accuracy. Further evaluation reveals an
end-to-end latency from image capturing to the algorithm&apos;s prediction of 56ms
or equivalently 18 fps, with a total power consumption of 62.9mW, equivalent to
a 9.3 hours of continuous run time on a 154mAh battery. These results
outperform MCUNet (TinyNAS+TinyEngine), which runs a simpler task (image
classification) at just 7.3 fps per second.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moosmann_J/0/1/0/all/0/1&quot;&gt;Julian Moosmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonazzi_P/0/1/0/all/0/1&quot;&gt;Pietro Bonazzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yawei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_S/0/1/0/all/0/1&quot;&gt;Sizhen Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mayer_P/0/1/0/all/0/1&quot;&gt;Philipp Mayer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1&quot;&gt;Luca Benini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magno_M/0/1/0/all/0/1&quot;&gt;Michele Magno&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01064">
<title>Multimodal Foundation Models for Zero-shot Animal Species Recognition in Camera Trap Images. (arXiv:2311.01064v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01064</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to deteriorating environmental conditions and increasing human activity,
conservation efforts directed towards wildlife is crucial. Motion-activated
camera traps constitute an efficient tool for tracking and monitoring wildlife
populations across the globe. Supervised learning techniques have been
successfully deployed to analyze such imagery, however training such techniques
requires annotations from experts. Reducing the reliance on costly labelled
data therefore has immense potential in developing large-scale wildlife
tracking solutions with markedly less human labor. In this work we propose
WildMatch, a novel zero-shot species classification framework that leverages
multimodal foundation models. In particular, we instruction tune
vision-language models to generate detailed visual descriptions of camera trap
images using similar terminology to experts. Then, we match the generated
caption to an external knowledge base of descriptions in order to determine the
species in a zero-shot manner. We investigate techniques to build instruction
tuning datasets for detailed animal description generation and propose a novel
knowledge augmentation technique to enhance caption quality. We demonstrate the
performance of WildMatch on a new camera trap dataset collected in the
Magdalena Medio region of Colombia.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fabian_Z/0/1/0/all/0/1&quot;&gt;Zalan Fabian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_Z/0/1/0/all/0/1&quot;&gt;Zhongqi Miao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chunyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuanhan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9;s Hern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montes_Rojas_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9;s Montes-Rojas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Escucha_R/0/1/0/all/0/1&quot;&gt;Rafael Escucha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siabatto_L/0/1/0/all/0/1&quot;&gt;Laura Siabatto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Link_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9;s Link&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1&quot;&gt;Pablo Arbel&amp;#xe1;ez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dodhia_R/0/1/0/all/0/1&quot;&gt;Rahul Dodhia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferres_J/0/1/0/all/0/1&quot;&gt;Juan Lavista Ferres&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01065">
<title>Novel View Synthesis from a Single RGBD Image for Indoor Scenes. (arXiv:2311.01065v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01065</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose an approach for synthesizing novel view images from
a single RGBD (Red Green Blue-Depth) input. Novel view synthesis (NVS) is an
interesting computer vision task with extensive applications. Methods using
multiple images has been well-studied, exemplary ones include training
scene-specific Neural Radiance Fields (NeRF), or leveraging multi-view stereo
(MVS) and 3D rendering pipelines. However, both are either computationally
intensive or non-generalizable across different scenes, limiting their
practical value. Conversely, the depth information embedded in RGBD images
unlocks 3D potential from a singular view, simplifying NVS. The widespread
availability of compact, affordable stereo cameras, and even LiDARs in
contemporary devices like smartphones, makes capturing RGBD images more
accessible than ever. In our method, we convert an RGBD image into a point
cloud and render it from a different viewpoint, then formulate the NVS task
into an image translation problem. We leveraged generative adversarial networks
to style-transfer the rendered image, achieving a result similar to a
photograph taken from the new perspective. We explore both unsupervised
learning using CycleGAN and supervised learning with Pix2Pix, and demonstrate
the qualitative results. Our method circumvents the limitations of traditional
multi-image techniques, holding significant promise for practical, real-time
applications in NVS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hetang_C/0/1/0/all/0/1&quot;&gt;Congrui Hetang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuping Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01066">
<title>Dynamic Multimodal Information Bottleneck for Multimodality Classification. (arXiv:2311.01066v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.01066</link>
<description rdf:parseType="Literal">&lt;p&gt;Effectively leveraging multimodal data such as various images, laboratory
tests and clinical information is gaining traction in a variety of AI-based
medical diagnosis and prognosis tasks. Most existing multi-modal techniques
only focus on enhancing their performance by leveraging the differences or
shared features from various modalities and fusing feature across different
modalities. These approaches are generally not optimal for clinical settings,
which pose the additional challenges of limited training data, as well as being
rife with redundant data or noisy modality channels, leading to subpar
performance. To address this gap, we study the robustness of existing methods
to data redundancy and noise and propose a generalized dynamic multimodal
information bottleneck framework for attaining a robust fused feature
representation. Specifically, our information bottleneck module serves to
filter out the task-irrelevant information and noises in the fused feature, and
we further introduce a sufficiency loss to prevent dropping of task-relevant
information, thus explicitly preserving the sufficiency of prediction
information in the distilled feature. We validate our model on an in-house and
a public COVID19 dataset for mortality prediction as well as two public
biomedical datasets for diagnostic tasks. Extensive experiments show that our
method surpasses the state-of-the-art and is significantly more robust, being
the only method to remain performance when large-scale noisy channels exist.
Our code is publicly available at https://github.com/BII-wushuang/DMIB.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yingying Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Shuang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Sheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chaoyan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zeng_T/0/1/0/all/0/1&quot;&gt;Tieyong Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xing_X/0/1/0/all/0/1&quot;&gt;Xiaodan Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Walsh_S/0/1/0/all/0/1&quot;&gt;Simon Walsh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guang Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01090">
<title>Infusion: Internal Diffusion for Video Inpainting. (arXiv:2311.01090v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01090</link>
<description rdf:parseType="Literal">&lt;p&gt;Video inpainting is the task of filling a desired region in a video in a
visually convincing manner. It is a very challenging task due to the high
dimensionality of the signal and the temporal consistency required for
obtaining convincing results. Recently, diffusion models have shown impressive
results in modeling complex data distributions, including images and videos.
Diffusion models remain nonetheless very expensive to train and perform
inference with, which strongly restrict their application to video. We show
that in the case of video inpainting, thanks to the highly auto-similar nature
of videos, the training of a diffusion model can be restricted to the video to
inpaint and still produce very satisfying results. This leads us to adopt an
internal learning approch, which also allows for a greatly reduced network
size. We call our approach &quot;Infusion&quot;: an internal learning algorithm for video
inpainting through diffusion. Due to our frugal network, we are able to propose
the first video inpainting approach based purely on diffusion. Other methods
require supporting elements such as optical flow estimation, which limits their
performance in the case of dynamic textures for example. We introduce a new
method for efficient training and inference of diffusion models in the context
of internal learning. We split the diffusion process into different learning
intervals which greatly simplifies the learning steps. We show qualititative
and quantitative results, demonstrating that our method reaches
state-of-the-art performance, in particular in the case of dynamic backgrounds
and textures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cherel_N/0/1/0/all/0/1&quot;&gt;Nicolas Cherel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Almansa_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9;s Almansa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gousseau_Y/0/1/0/all/0/1&quot;&gt;Yann Gousseau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Newson_A/0/1/0/all/0/1&quot;&gt;Alasdair Newson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01091">
<title>Enriching Phrases with Coupled Pixel and Object Contexts for Panoptic Narrative Grounding. (arXiv:2311.01091v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01091</link>
<description rdf:parseType="Literal">&lt;p&gt;Panoptic narrative grounding (PNG) aims to segment things and stuff objects
in an image described by noun phrases of a narrative caption. As a multimodal
task, an essential aspect of PNG is the visual-linguistic interaction between
image and caption. The previous two-stage method aggregates visual contexts
from offline-generated mask proposals to phrase features, which tend to be
noisy and fragmentary. The recent one-stage method aggregates only pixel
contexts from image features to phrase features, which may incur semantic
misalignment due to lacking object priors. To realize more comprehensive
visual-linguistic interaction, we propose to enrich phrases with coupled pixel
and object contexts by designing a Phrase-Pixel-Object Transformer Decoder
(PPO-TD), where both fine-grained part details and coarse-grained entity clues
are aggregated to phrase features. In addition, we also propose a PhraseObject
Contrastive Loss (POCL) to pull closer the matched phrase-object pairs and push
away unmatched ones for aggregating more precise object contexts from more
phrase-relevant object tokens. Extensive experiments on the PNG benchmark show
our method achieves new state-of-the-art performance with large margins.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hui_T/0/1/0/all/0/1&quot;&gt;Tianrui Hui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1&quot;&gt;Zihan Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Junshi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xiaoming Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xiaolin Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1&quot;&gt;Jiao Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jizhong Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Si Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01092">
<title>Learning A Multi-Task Transformer Via Unified And Customized Instruction Tuning For Chest Radiograph Interpretation. (arXiv:2311.01092v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01092</link>
<description rdf:parseType="Literal">&lt;p&gt;The emergence of multi-modal deep learning models has made significant
impacts on clinical applications in the last decade. However, the majority of
models are limited to single-tasking, without considering disease diagnosis is
indeed a multi-task procedure. Here, we demonstrate a unified transformer model
specifically designed for multi-modal clinical tasks by incorporating
customized instruction tuning. We first compose a multi-task training dataset
comprising 13.4 million instruction and ground-truth pairs (with approximately
one million radiographs) for the customized tuning, involving both image- and
pixel-level tasks. Thus, we can unify the various vision-intensive tasks in a
single training framework with homogeneous model inputs and outputs to increase
clinical interpretability in one reading. Finally, we demonstrate the overall
superior performance of our model compared to prior arts on various chest X-ray
benchmarks across multi-tasks in both direct inference and finetuning settings.
Three radiologists further evaluate the generated reports against the recorded
ones, which also exhibit the enhanced explainability of our multi-task model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Lijian Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_Z/0/1/0/all/0/1&quot;&gt;Ziyu Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinglong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaosong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongsheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shaoting Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01111">
<title>H-NeXt: The next step towards roto-translation invariant networks. (arXiv:2311.01111v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01111</link>
<description rdf:parseType="Literal">&lt;p&gt;The widespread popularity of equivariant networks underscores the
significance of parameter efficient models and effective use of training data.
At a time when robustness to unseen deformations is becoming increasingly
important, we present H-NeXt, which bridges the gap between equivariance and
invariance. H-NeXt is a parameter-efficient roto-translation invariant network
that is trained without a single augmented image in the training set. Our
network comprises three components: an equivariant backbone for learning
roto-translation independent features, an invariant pooling layer for
discarding roto-translation information, and a classification layer. H-NeXt
outperforms the state of the art in classification on unaugmented training sets
and augmented test sets of MNIST and CIFAR-10.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karella_T/0/1/0/all/0/1&quot;&gt;Tomas Karella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sroubek_F/0/1/0/all/0/1&quot;&gt;Filip Sroubek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flusser_J/0/1/0/all/0/1&quot;&gt;Jan Flusser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blazek_J/0/1/0/all/0/1&quot;&gt;Jan Blazek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kosik_V/0/1/0/all/0/1&quot;&gt;Vasek Kosik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01117">
<title>Cheating Depth: Enhancing 3D Surface Anomaly Detection via Depth Simulation. (arXiv:2311.01117v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01117</link>
<description rdf:parseType="Literal">&lt;p&gt;RGB-based surface anomaly detection methods have advanced significantly.
However, certain surface anomalies remain practically invisible in RGB alone,
necessitating the incorporation of 3D information. Existing approaches that
employ point-cloud backbones suffer from suboptimal representations and reduced
applicability due to slow processing. Re-training RGB backbones, designed for
faster dense input processing, on industrial depth datasets is hindered by the
limited availability of sufficiently large datasets. We make several
contributions to address these challenges. (i) We propose a novel Depth-Aware
Discrete Autoencoder (DADA) architecture, that enables learning a general
discrete latent space that jointly models RGB and 3D data for 3D surface
anomaly detection. (ii) We tackle the lack of diverse industrial depth datasets
by introducing a simulation process for learning informative depth features in
the depth encoder. (iii) We propose a new surface anomaly detection method
3DSR, which outperforms all existing state-of-the-art on the challenging
MVTec3D anomaly detection benchmark, both in terms of accuracy and processing
speed. The experimental results validate the effectiveness and efficiency of
our approach, highlighting the potential of utilizing depth information for
improved surface anomaly detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zavrtanik_V/0/1/0/all/0/1&quot;&gt;Vitjan Zavrtanik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kristan_M/0/1/0/all/0/1&quot;&gt;Matej Kristan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skocaj_D/0/1/0/all/0/1&quot;&gt;Danijel Sko&amp;#x10d;aj&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01130">
<title>A deep learning experiment for semantic segmentation of overlapping characters in palimpsests. (arXiv:2311.01130v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01130</link>
<description rdf:parseType="Literal">&lt;p&gt;Palimpsests refer to historical manuscripts where erased writings have been
partially covered by the superimposition of a second writing. By employing
imaging techniques, e.g., multispectral imaging, it becomes possible to
identify features that are imperceptible to the naked eye, including faded and
erased inks. When dealing with overlapping inks, Artificial Intelligence
techniques can be utilized to disentangle complex nodes of overlapping letters.
In this work, we propose deep learning-based semantic segmentation as a method
for identifying and segmenting individual letters in overlapping characters.
The experiment was conceived as a proof of concept, focusing on the palimpsests
of the Ars Grammatica by Prisciano as a case study. Furthermore, caveats and
prospects of our approach combined with multispectral imaging are also
discussed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perino_M/0/1/0/all/0/1&quot;&gt;Michela Perino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ginolfi_M/0/1/0/all/0/1&quot;&gt;Michele Ginolfi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Felici_A/0/1/0/all/0/1&quot;&gt;Anna Candida Felici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosellini_M/0/1/0/all/0/1&quot;&gt;Michela Rosellini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01138">
<title>AeroPath: An airway segmentation benchmark dataset with challenging pathology. (arXiv:2311.01138v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01138</link>
<description rdf:parseType="Literal">&lt;p&gt;To improve the prognosis of patients suffering from pulmonary diseases, such
as lung cancer, early diagnosis and treatment are crucial. The analysis of CT
images is invaluable for diagnosis, whereas high quality segmentation of the
airway tree are required for intervention planning and live guidance during
bronchoscopy. Recently, the Multi-domain Airway Tree Modeling (ATM&apos;22)
challenge released a large dataset, both enabling training of deep-learning
based models and bringing substantial improvement of the state-of-the-art for
the airway segmentation task. However, the ATM&apos;22 dataset includes few patients
with severe pathologies affecting the airway tree anatomy. In this study, we
introduce a new public benchmark dataset (AeroPath), consisting of 27 CT images
from patients with pathologies ranging from emphysema to large tumors, with
corresponding trachea and bronchi annotations. Second, we present a multiscale
fusion design for automatic airway segmentation. Models were trained on the
ATM&apos;22 dataset, tested on the AeroPath dataset, and further evaluated against
competitive open-source methods. The same performance metrics as used in the
ATM&apos;22 challenge were used to benchmark the different considered approaches.
Lastly, an open web application is developed, to easily test the proposed model
on new data. The results demonstrated that our proposed architecture predicted
topologically correct segmentations for all the patients included in the
AeroPath dataset. The proposed method is robust and able to handle various
anomalies, down to at least the fifth airway generation. In addition, the
AeroPath dataset, featuring patients with challenging pathologies, will
contribute to development of new state-of-the-art methods. The AeroPath dataset
and the web application are made openly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoverud_K/0/1/0/all/0/1&quot;&gt;Karen-Helene St&amp;#xf8;verud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouget_D/0/1/0/all/0/1&quot;&gt;David Bouget&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedersen_A/0/1/0/all/0/1&quot;&gt;Andre Pedersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leira_H/0/1/0/all/0/1&quot;&gt;H&amp;#xe5;kon Olav Leira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lango_T/0/1/0/all/0/1&quot;&gt;Thomas Lang&amp;#xf8;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hofstad_E/0/1/0/all/0/1&quot;&gt;Erlend Fagertun Hofstad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01155">
<title>Learning Intra and Inter-Camera Invariance for Isolated Camera Supervised Person Re-identification. (arXiv:2311.01155v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01155</link>
<description rdf:parseType="Literal">&lt;p&gt;Supervised person re-identification assumes that a person has images captured
under multiple cameras. However when cameras are placed in distance, a person
rarely appears in more than one camera. This paper thus studies person re-ID
under such isolated camera supervised (ISCS) setting. Instead of trying to
generate fake cross-camera features like previous methods, we explore a novel
perspective by making efficient use of the variation in training data. Under
ISCS setting, a person only has limited images from a single camera, so the
camera bias becomes a critical issue confounding ID discrimination.
Cross-camera images are prone to being recognized as different IDs simply by
camera style. To eliminate the confounding effect of camera bias, we propose to
learn both intra- and inter-camera invariance under a unified framework. First,
we construct style-consistent environments via clustering, and perform
prototypical contrastive learning within each environment. Meanwhile, strongly
augmented images are contrasted with original prototypes to enforce
intra-camera augmentation invariance. For inter-camera invariance, we further
design a much improved variant of multi-camera negative loss that optimizes the
distance of multi-level negatives. The resulting model learns to be invariant
to both subtle and severe style variation within and cross-camera. On multiple
benchmarks, we conduct extensive experiments and validate the effectiveness and
superiority of the proposed method. Code will be available at
https://github.com/Terminator8758/IICI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Menglin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1&quot;&gt;Xiaojin Gong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01185">
<title>Revolutionizing Healthcare Image Analysis in Pandemic-Based Fog-Cloud Computing Architectures. (arXiv:2311.01185v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01185</link>
<description rdf:parseType="Literal">&lt;p&gt;The emergence of pandemics has significantly emphasized the need for
effective solutions in healthcare data analysis. One particular challenge in
this domain is the manual examination of medical images, such as X-rays and CT
scans. This process is time-consuming and involves the logistical complexities
of transferring these images to centralized cloud computing servers.
Additionally, the speed and accuracy of image analysis are vital for efficient
healthcare image management. This research paper introduces an innovative
healthcare architecture that tackles the challenges of analysis efficiency and
accuracy by harnessing the capabilities of Artificial Intelligence (AI).
Specifically, the proposed architecture utilizes fog computing and presents a
modified Convolutional Neural Network (CNN) designed specifically for image
analysis. Different architectures of CNN layers are thoroughly explored and
evaluated to optimize overall performance. To demonstrate the effectiveness of
the proposed approach, a dataset of X-ray images is utilized for analysis and
evaluation. Comparative assessments are conducted against recent models such as
VGG16, VGG19, MobileNet, and related research papers. Notably, the proposed
approach achieves an exceptional accuracy rate of 99.88% in classifying normal
cases, accompanied by a validation rate of 96.5%, precision and recall rates of
100%, and an F1 score of 100%. These results highlight the immense potential of
fog computing and modified CNNs in revolutionizing healthcare image analysis
and diagnosis, not only during pandemics but also in the future. By leveraging
these technologies, healthcare professionals can enhance the efficiency and
accuracy of medical image analysis, leading to improved patient care and
outcomes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elsayed_A/0/1/0/all/0/1&quot;&gt;Al Zahraa Elsayed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohamed_K/0/1/0/all/0/1&quot;&gt;Khalil Mohamed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harb_H/0/1/0/all/0/1&quot;&gt;Hany Harb&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01188">
<title>Terrain-Informed Self-Supervised Learning: Enhancing Building Footprint Extraction from LiDAR Data with Limited Annotations. (arXiv:2311.01188v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01188</link>
<description rdf:parseType="Literal">&lt;p&gt;Estimating building footprint maps from geospatial data is of paramount
importance in urban planning, development, disaster management, and various
other applications. Deep learning methodologies have gained prominence in
building segmentation maps, offering the promise of precise footprint
extraction without extensive post-processing. However, these methods face
challenges in generalization and label efficiency, particularly in remote
sensing, where obtaining accurate labels can be both expensive and
time-consuming. To address these challenges, we propose terrain-aware
self-supervised learning, tailored to remote sensing, using digital elevation
models from LiDAR data. We propose to learn a model to differentiate between
bare Earth and superimposed structures enabling the network to implicitly learn
domain-relevant features without the need for extensive pixel-level
annotations. We test the effectiveness of our approach by evaluating building
segmentation performance on test datasets with varying label fractions.
Remarkably, with only 1% of the labels (equivalent to 25 labeled examples), our
method improves over ImageNet pre-training, showing the advantage of leveraging
unlabeled data for feature extraction in the domain of remote sensing. The
performance improvement is more pronounced in few-shot scenarios and gradually
closes the gap with ImageNet pre-training as the label fraction increases. We
test on a dataset characterized by substantial distribution shifts and labeling
errors to demonstrate the generalizability of our approach. When compared to
other baselines, including ImageNet pretraining and more complex architectures,
our approach consistently performs better, demonstrating the efficiency and
effectiveness of self-supervised terrain-aware feature learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vats_A/0/1/0/all/0/1&quot;&gt;Anuja Vats&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Volgyes_D/0/1/0/all/0/1&quot;&gt;David V&amp;#xf6;lgyes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vermeer_M/0/1/0/all/0/1&quot;&gt;Martijn Vermeer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedersen_M/0/1/0/all/0/1&quot;&gt;Marius Pedersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raja_K/0/1/0/all/0/1&quot;&gt;Kiran Raja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fantin_D/0/1/0/all/0/1&quot;&gt;Daniele S.M.Fantin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hay_J/0/1/0/all/0/1&quot;&gt;Jacob Alexander Hay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01192">
<title>Semantic Scene Graph Generation Based on an Edge Dual Scene Graph and Message Passing Neural Network. (arXiv:2311.01192v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01192</link>
<description rdf:parseType="Literal">&lt;p&gt;Along with generative AI, interest in scene graph generation (SGG), which
comprehensively captures the relationships and interactions between objects in
an image and creates a structured graph-based representation, has significantly
increased in recent years. However, relying on object-centric and dichotomous
relationships, existing SGG methods have a limited ability to accurately
predict detailed relationships. To solve these problems, a new approach to the
modeling multiobject relationships, called edge dual scene graph generation
(EdgeSGG), is proposed herein. EdgeSGG is based on a edge dual scene graph and
Dual Message Passing Neural Network (DualMPNN), which can capture rich
contextual interactions between unconstrained objects. To facilitate the
learning of edge dual scene graphs with a symmetric graph structure, the
proposed DualMPNN learns both object- and relation-centric features for more
accurately predicting relation-aware contexts and allows fine-grained
relational updates between objects. A comparative experiment with
state-of-the-art (SoTA) methods was conducted using two public datasets for SGG
operations and six metrics for three subtasks. Compared with SoTA approaches,
the proposed model exhibited substantial performance improvements across all
SGG subtasks. Furthermore, experiment on long-tail distributions revealed that
incorporating the relationships between objects effectively mitigates existing
long-tail problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyeongjin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sangwon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jong Taek Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ko_B/0/1/0/all/0/1&quot;&gt;Byoung Chul Ko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01197">
<title>AiluRus: A Scalable ViT Framework for Dense Prediction. (arXiv:2311.01197v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01197</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision transformers (ViTs) have emerged as a prevalent architecture for
vision tasks owing to their impressive performance. However, when it comes to
handling long token sequences, especially in dense prediction tasks that
require high-resolution input, the complexity of ViTs increases significantly.
Notably, dense prediction tasks, such as semantic segmentation or object
detection, emphasize more on the contours or shapes of objects, while the
texture inside objects is less informative. Motivated by this observation, we
propose to apply adaptive resolution for different regions in the image
according to their importance. Specifically, at the intermediate layer of the
ViT, we utilize a spatial-aware density-based clustering algorithm to select
representative tokens from the token sequence. Once the representative tokens
are determined, we proceed to merge other tokens into their closest
representative token. Consequently, semantic similar tokens are merged together
to form low-resolution regions, while semantic irrelevant tokens are preserved
independently as high-resolution regions. This strategy effectively reduces the
number of tokens, allowing subsequent layers to handle a reduced token sequence
and achieve acceleration. We evaluate our proposed method on three different
datasets and observe promising performance. For example, the &quot;Segmenter ViT-L&quot;
model can be accelerated by 48% FPS without fine-tuning, while maintaining the
performance. Additionally, our method can be applied to accelerate fine-tuning
as well. Experimental results demonstrate that we can save 52% training time
while accelerating 2.46 times FPS with only a 0.09% performance drop. The code
is available at https://github.com/caddyless/ailurus/tree/main.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaoming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaopeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Bowen Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1&quot;&gt;Dongsheng Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenglin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1&quot;&gt;Wenrui Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Hongkai Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1&quot;&gt;Qi Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01202">
<title>Cross-Modal Information-Guided Network using Contrastive Learning for Point Cloud Registration. (arXiv:2311.01202v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01202</link>
<description rdf:parseType="Literal">&lt;p&gt;The majority of point cloud registration methods currently rely on extracting
features from points. However, these methods are limited by their dependence on
information obtained from a single modality of points, which can result in
deficiencies such as inadequate perception of global features and a lack of
texture information. Actually, humans can employ visual information learned
from 2D images to comprehend the 3D world. Based on this fact, we present a
novel Cross-Modal Information-Guided Network (CMIGNet), which obtains global
shape perception through cross-modal information to achieve precise and robust
point cloud registration. Specifically, we first incorporate the projected
images from the point clouds and fuse the cross-modal features using the
attention mechanism. Furthermore, we employ two contrastive learning
strategies, namely overlapping contrastive learning and cross-modal contrastive
learning. The former focuses on features in overlapping regions, while the
latter emphasizes the correspondences between 2D and 3D features. Finally, we
propose a mask prediction module to identify keypoints in the point clouds.
Extensive experiments on several benchmark datasets demonstrate that our
network achieves superior registration performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yifan Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jihua Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shiqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1&quot;&gt;Pengcheng Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01212">
<title>Multi-view Relation Learning for Cross-domain Few-shot Hyperspectral Image Classification. (arXiv:2311.01212v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01212</link>
<description rdf:parseType="Literal">&lt;p&gt;Cross-domain few-shot hyperspectral image classification focuses on learning
prior knowledge from a large number of labeled samples from source domain and
then transferring the knowledge to the tasks which contain only few labeled
samples in target domains. Following the metric-based manner, many current
methods first extract the features of the query and support samples, and then
directly predict the classes of query samples according to their distance to
the support samples or prototypes. The relations between samples have not been
fully explored and utilized. Different from current works, this paper proposes
to learn sample relations from different views and take them into the model
learning process, to improve the cross-domain few-shot hyperspectral image
classification. Building on current DCFSL method which adopts a domain
discriminator to deal with domain-level distribution difference, the proposed
method applys contrastive learning to learn the class-level sample relations to
obtain more discriminable sample features. In addition, it adopts a transformer
based cross-attention learning module to learn the set-level sample relations
and acquire the attentions from query samples to support samples. Our
experimental results have demonstrated the contribution of the multi-view
relation learning mechanism for few-shot hyperspectral image classification
when compared with the state of the art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Longwei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1&quot;&gt;Zhigang Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jianzhong Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Junyong Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01214">
<title>High-Quality Animatable Dynamic Garment Reconstruction from Monocular Videos. (arXiv:2311.01214v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01214</link>
<description rdf:parseType="Literal">&lt;p&gt;Much progress has been made in reconstructing garments from an image or a
video. However, none of existing works meet the expectations of digitizing
high-quality animatable dynamic garments that can be adjusted to various unseen
poses. In this paper, we propose the first method to recover high-quality
animatable dynamic garments from monocular videos without depending on scanned
data. To generate reasonable deformations for various unseen poses, we propose
a learnable garment deformation network that formulates the garment
reconstruction task as a pose-driven deformation problem. To alleviate the
ambiguity estimating 3D garments from monocular videos, we design a
multi-hypothesis deformation module that learns spatial representations of
multiple plausible deformations. Experimental results on several public
datasets demonstrate that our method can reconstruct high-quality dynamic
garments with coherent surface details, which can be easily animated under
unseen poses. The code will be provided for research purposes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiongzheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jinsong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1&quot;&gt;Yu-Kun Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jingyu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kun Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01216">
<title>Convergent plug-and-play with proximal denoiser and unconstrained regularization parameter. (arXiv:2311.01216v1 [math.OC])</title>
<link>http://arxiv.org/abs/2311.01216</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we present new proofs of convergence for Plug-and-Play (PnP)
algorithms. PnP methods are efficient iterative algorithms for solving image
inverse problems where regularization is performed by plugging a pre-trained
denoiser in a proximal algorithm, such as Proximal Gradient Descent (PGD) or
Douglas-Rachford Splitting (DRS). Recent research has explored convergence by
incorporating a denoiser that writes exactly as a proximal operator. However,
the corresponding PnP algorithm has then to be run with stepsize equal to $1$.
The stepsize condition for nonconvex convergence of the proximal algorithm in
use then translates to restrictive conditions on the regularization parameter
of the inverse problem. This can severely degrade the restoration capacity of
the algorithm. In this paper, we present two remedies for this limitation.
First, we provide a novel convergence proof for PnP-DRS that does not impose
any restrictions on the regularization parameter. Second, we examine a relaxed
version of the PGD algorithm that converges across a broader range of
regularization parameters. Our experimental study, conducted on deblurring and
super-resolution experiments, demonstrate that both of these solutions enhance
the accuracy of image restoration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hurault_S/0/1/0/all/0/1&quot;&gt;Samuel Hurault&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Chambolle_A/0/1/0/all/0/1&quot;&gt;Antonin Chambolle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Leclaire_A/0/1/0/all/0/1&quot;&gt;Arthur Leclaire&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Papadakis_N/0/1/0/all/0/1&quot;&gt;Nicolas Papadakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01226">
<title>Optimal Transport-Guided Conditional Score-Based Diffusion Models. (arXiv:2311.01226v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01226</link>
<description rdf:parseType="Literal">&lt;p&gt;Conditional score-based diffusion model (SBDM) is for conditional generation
of target data with paired data as condition, and has achieved great success in
image translation. However, it requires the paired data as condition, and there
would be insufficient paired data provided in real-world applications. To
tackle the applications with partially paired or even unpaired dataset, we
propose a novel Optimal Transport-guided Conditional Score-based diffusion
model (OTCS) in this paper. We build the coupling relationship for the unpaired
or partially paired dataset based on $L_2$-regularized unsupervised or
semi-supervised optimal transport, respectively. Based on the coupling
relationship, we develop the objective for training the conditional score-based
model for unpaired or partially paired settings, which is based on a
reformulation and generalization of the conditional SBDM for paired setting.
With the estimated coupling relationship, we effectively train the conditional
score-based model by designing a ``resampling-by-compatibility&apos;&apos; strategy to
choose the sampled data with high compatibility as guidance. Extensive
experiments on unpaired super-resolution and semi-paired image-to-image
translation demonstrated the effectiveness of the proposed OTCS model. From the
viewpoint of optimal transport, OTCS provides an approach to transport data
across distributions, which is a challenge for OT on large-scale datasets. We
theoretically prove that OTCS realizes the data transport in OT with a
theoretical bound. Code is available at \url{https://github.com/XJTU-XGU/OTCS}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1&quot;&gt;Xiang Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Liwei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jian Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zongben Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01227">
<title>Robust Feature Learning and Global Variance-Driven Classifier Alignment for Long-Tail Class Incremental Learning. (arXiv:2311.01227v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01227</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a two-stage framework designed to enhance long-tail
class incremental learning, enabling the model to progressively learn new
classes, while mitigating catastrophic forgetting in the context of long-tailed
data distributions. Addressing the challenge posed by the under-representation
of tail classes in long-tail class incremental learning, our approach achieves
classifier alignment by leveraging global variance as an informative measure
and class prototypes in the second stage. This process effectively captures
class properties and eliminates the need for data balancing or additional layer
tuning. Alongside traditional class incremental learning losses in the first
stage, the proposed approach incorporates mixup classes to learn robust feature
representations, ensuring smoother boundaries. The proposed framework can
seamlessly integrate as a module with any class incremental learning method to
effectively handle long-tail class incremental learning scenarios. Extensive
experimentation on the CIFAR-100 and ImageNet-Subset datasets validates the
approach&apos;s efficacy, showcasing its superiority over state-of-the-art
techniques across various long-tail CIL settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalla_J/0/1/0/all/0/1&quot;&gt;Jayateja Kalla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1&quot;&gt;Soma Biswas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01233">
<title>Long Story Short: a Summarize-then-Search Method for Long Video Question Answering. (arXiv:2311.01233v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01233</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models such as GPT-3 have demonstrated an impressive
capability to adapt to new tasks without requiring task-specific training data.
This capability has been particularly effective in settings such as narrative
question answering, where the diversity of tasks is immense, but the available
supervision data is small. In this work, we investigate if such language models
can extend their zero-shot reasoning abilities to long multimodal narratives in
multimedia content such as drama, movies, and animation, where the story plays
an essential role. We propose Long Story Short, a framework for narrative video
QA that first summarizes the narrative of the video to a short plot and then
searches parts of the video relevant to the question. We also propose to
enhance visual matching with CLIPCheck. Our model outperforms state-of-the-art
supervised models by a large margin, highlighting the potential of zero-shot QA
for long videos.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1&quot;&gt;Jiwan Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Youngjae Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01237">
<title>Log-Likelihood Score Level Fusion for Improved Cross-Sensor Smartphone Periocular Recognition. (arXiv:2311.01237v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01237</link>
<description rdf:parseType="Literal">&lt;p&gt;The proliferation of cameras and personal devices results in a wide
variability of imaging conditions, producing large intra-class variations and a
significant performance drop when images from heterogeneous environments are
compared. However, many applications require to deal with data from different
sources regularly, thus needing to overcome these interoperability problems.
Here, we employ fusion of several comparators to improve periocular performance
when images from different smartphones are compared. We use a probabilistic
fusion framework based on linear logistic regression, in which fused scores
tend to be log-likelihood ratios, obtaining a reduction in cross-sensor EER of
up to 40% due to the fusion. Our framework also provides an elegant and simple
solution to handle signals from different devices, since same-sensor and
cross-sensor score distributions are aligned and mapped to a common
probabilistic domain. This allows the use of Bayes thresholds for optimal
decision-making, eliminating the need of sensor-specific thresholds, which is
essential in operational conditions because the threshold setting critically
determines the accuracy of the authentication process in many applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alonso_Fernandez_F/0/1/0/all/0/1&quot;&gt;Fernando Alonso-Fernandez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raja_K/0/1/0/all/0/1&quot;&gt;Kiran B. Raja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1&quot;&gt;Christoph Busch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bigun_J/0/1/0/all/0/1&quot;&gt;Josef Bigun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01240">
<title>FacadeNet: Conditional Facade Synthesis via Selective Editing. (arXiv:2311.01240v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01240</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce FacadeNet, a deep learning approach for synthesizing building
facade images from diverse viewpoints. Our method employs a conditional GAN,
taking a single view of a facade along with the desired viewpoint information
and generates an image of the facade from the distinct viewpoint. To precisely
modify view-dependent elements like windows and doors while preserving the
structure of view-independent components such as walls, we introduce a
selective editing module. This module leverages image embeddings extracted from
a pre-trained vision transformer. Our experiments demonstrated state-of-the-art
performance on building facade generation, surpassing alternative methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Georgiou_Y/0/1/0/all/0/1&quot;&gt;Yiangos Georgiou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loizou_M/0/1/0/all/0/1&quot;&gt;Marios Loizou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kelly_T/0/1/0/all/0/1&quot;&gt;Tom Kelly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Averkiou_M/0/1/0/all/0/1&quot;&gt;Melinos Averkiou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01241">
<title>Exploring Deep Learning Image Super-Resolution for Iris Recognition. (arXiv:2311.01241v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.01241</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we test the ability of deep learning methods to provide an
end-to-end mapping between low and high resolution images applying it to the
iris recognition problem. Here, we propose the use of two deep learning
single-image super-resolution approaches: Stacked Auto-Encoders (SAE) and
Convolutional Neural Networks (CNN) with the most possible lightweight
structure to achieve fast speed, preserve local information and reduce
artifacts at the same time. We validate the methods with a database of 1.872
near-infrared iris images with quality assessment and recognition experiments
showing the superiority of deep learning approaches over the compared
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ribeiro_E/0/1/0/all/0/1&quot;&gt;Eduardo Ribeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Uhl_A/0/1/0/all/0/1&quot;&gt;Andreas Uhl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Alonso_Fernandez_F/0/1/0/all/0/1&quot;&gt;Fernando Alonso-Fernandez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Farrugia_R/0/1/0/all/0/1&quot;&gt;Reuben A. Farrugia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01267">
<title>UniFolding: Towards Sample-efficient, Scalable, and Generalizable Robotic Garment Folding. (arXiv:2311.01267v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.01267</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the development of UniFolding, a sample-efficient,
scalable, and generalizable robotic system for unfolding and folding various
garments. UniFolding employs the proposed UFONet neural network to integrate
unfolding and folding decisions into a single policy model that is adaptable to
different garment types and states. The design of UniFolding is based on a
garment&apos;s partial point cloud, which aids in generalization and reduces
sensitivity to variations in texture and shape. The training pipeline
prioritizes low-cost, sample-efficient data collection. Training data is
collected via a human-centric process with offline and online stages. The
offline stage involves human unfolding and folding actions via Virtual Reality,
while the online stage utilizes human-in-the-loop learning to fine-tune the
model in a real-world setting. The system is tested on two garment types:
long-sleeve and short-sleeve shirts. Performance is evaluated on 20 shirts with
significant variations in textures, shapes, and materials. More experiments and
videos can be found in the supplementary materials and on the website:
https://unifolding.robotflow.ai
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1&quot;&gt;Han Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yutong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Wenqiang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huanyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1&quot;&gt;Dongzhe Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Cewu Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01283">
<title>Distilling Knowledge from CNN-Transformer Models for Enhanced Human Action Recognition. (arXiv:2311.01283v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01283</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a study on improving human action recognition through the
utilization of knowledge distillation, and the combination of CNN and ViT
models. The research aims to enhance the performance and efficiency of smaller
student models by transferring knowledge from larger teacher models. The
proposed method employs a Transformer vision network as the student model,
while a convolutional network serves as the teacher model. The teacher model
extracts local image features, whereas the student model focuses on global
features using an attention mechanism. The Vision Transformer (ViT)
architecture is introduced as a robust framework for capturing global
dependencies in images. Additionally, advanced variants of ViT, namely PVT,
Convit, MVIT, Swin Transformer, and Twins, are discussed, highlighting their
contributions to computer vision tasks. The ConvNeXt model is introduced as a
teacher model, known for its efficiency and effectiveness in computer vision.
The paper presents performance results for human action recognition on the
Stanford 40 dataset, comparing the accuracy and mAP of student models trained
with and without knowledge distillation. The findings illustrate that the
suggested approach significantly improves the accuracy and mAP when compared to
training networks under regular settings. These findings emphasize the
potential of combining local and global features in action recognition tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmadabadi_H/0/1/0/all/0/1&quot;&gt;Hamid Ahmadabadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manzari_O/0/1/0/all/0/1&quot;&gt;Omid Nejati Manzari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayatollahi_A/0/1/0/all/0/1&quot;&gt;Ahmad Ayatollahi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01292">
<title>Joint 3D Shape and Motion Estimation from Rolling Shutter Light-Field Images. (arXiv:2311.01292v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01292</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose an approach to address the problem of 3D
reconstruction of scenes from a single image captured by a light-field camera
equipped with a rolling shutter sensor. Our method leverages the 3D information
cues present in the light-field and the motion information provided by the
rolling shutter effect. We present a generic model for the imaging process of
this sensor and a two-stage algorithm that minimizes the re-projection error
while considering the position and motion of the camera in a motion-shape
bundle adjustment estimation strategy. Thereby, we provide an instantaneous 3D
shape-and-pose-and-velocity sensing paradigm. To the best of our knowledge,
this is the first study to leverage this type of sensor for this purpose. We
also present a new benchmark dataset composed of different light-fields showing
rolling shutter effects, which can be used as a common base to improve the
evaluation and tracking the progress in the field. We demonstrate the
effectiveness and advantages of our approach through several experiments
conducted for different scenes and types of motions. The source code and
dataset are publicly available at: https://github.com/ICB-Vision-AI/RSLF
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McGriff_H/0/1/0/all/0/1&quot;&gt;Hermes McGriff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martins_R/0/1/0/all/0/1&quot;&gt;Renato Martins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andreff_N/0/1/0/all/0/1&quot;&gt;Nicolas Andreff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demonceaux_C/0/1/0/all/0/1&quot;&gt;C&amp;#xe9;dric Demonceaux&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01295">
<title>DP-Mix: Mixup-based Data Augmentation for Differentially Private Learning. (arXiv:2311.01295v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.01295</link>
<description rdf:parseType="Literal">&lt;p&gt;Data augmentation techniques, such as simple image transformations and
combinations, are highly effective at improving the generalization of computer
vision models, especially when training data is limited. However, such
techniques are fundamentally incompatible with differentially private learning
approaches, due to the latter&apos;s built-in assumption that each training image&apos;s
contribution to the learned model is bounded. In this paper, we investigate why
naive applications of multi-sample data augmentation techniques, such as mixup,
fail to achieve good performance and propose two novel data augmentation
techniques specifically designed for the constraints of differentially private
learning. Our first technique, DP-Mix_Self, achieves SoTA classification
performance across a range of datasets and settings by performing mixup on
self-augmented data. Our second technique, DP-Mix_Diff, further improves
performance by incorporating synthetic data from a pre-trained diffusion model
into the mixup process. We open-source the code at
https://github.com/wenxuan-Bao/DP-Mix.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_W/0/1/0/all/0/1&quot;&gt;Wenxuan Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pittaluga_F/0/1/0/all/0/1&quot;&gt;Francesco Pittaluga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+G_V/0/1/0/all/0/1&quot;&gt;Vijay Kumar B G&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bindschaedler_V/0/1/0/all/0/1&quot;&gt;Vincent Bindschaedler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01308">
<title>Hybrid-Fusion Transformer for Multisequence MRI. (arXiv:2311.01308v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.01308</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical segmentation has grown exponentially through the advent of a fully
convolutional network (FCN), and we have now reached a turning point through
the success of Transformer. However, the different characteristics of the
modality have not been fully integrated into Transformer for medical
segmentation. In this work, we propose the novel hybrid fusion Transformer
(HFTrans) for multisequence MRI image segmentation. We take advantage of the
differences among multimodal MRI sequences and utilize the Transformer layers
to integrate the features extracted from each modality as well as the features
of the early fused modalities. We validate the effectiveness of our
hybrid-fusion method in three-dimensional (3D) medical segmentation.
Experiments on two public datasets, BraTS2020 and MRBrainS18, show that the
proposed method outperforms previous state-of-the-art methods on the task of
brain tumor segmentation and brain structure segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cho_J/0/1/0/all/0/1&quot;&gt;Jihoon Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Jinah Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01310">
<title>Scattering Vision Transformer: Spectral Mixing Matters. (arXiv:2311.01310v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01310</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision transformers have gained significant attention and achieved
state-of-the-art performance in various computer vision tasks, including image
classification, instance segmentation, and object detection. However,
challenges remain in addressing attention complexity and effectively capturing
fine-grained information within images. Existing solutions often resort to
down-sampling operations, such as pooling, to reduce computational cost.
Unfortunately, such operations are non-invertible and can result in information
loss. In this paper, we present a novel approach called Scattering Vision
Transformer (SVT) to tackle these challenges. SVT incorporates a spectrally
scattering network that enables the capture of intricate image details. SVT
overcomes the invertibility issue associated with down-sampling operations by
separating low-frequency and high-frequency components. Furthermore, SVT
introduces a unique spectral gating network utilizing Einstein multiplication
for token and channel mixing, effectively reducing complexity. We show that SVT
achieves state-of-the-art performance on the ImageNet dataset with a
significant reduction in a number of parameters and FLOPS. SVT shows 2\%
improvement over LiTv2 and iFormer. SVT-H-S reaches 84.2\% top-1 accuracy,
while SVT-H-B reaches 85.2\% (state-of-art for base versions) and SVT-H-L
reaches 85.7\% (again state-of-art for large versions). SVT also shows
comparable results in other vision tasks such as instance segmentation. SVT
also outperforms other transformers in transfer learning on standard datasets
such as CIFAR10, CIFAR100, Oxford Flower, and Stanford Car datasets. The
project page is available on this
webpage.\url{https://badripatro.github.io/svt/}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patro_B/0/1/0/all/0/1&quot;&gt;Badri N. Patro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agneeswaran_V/0/1/0/all/0/1&quot;&gt;Vijay Srinivas Agneeswaran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01323">
<title>Towards Evaluating Transfer-based Attacks Systematically, Practically, and Fairly. (arXiv:2311.01323v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.01323</link>
<description rdf:parseType="Literal">&lt;p&gt;The adversarial vulnerability of deep neural networks (DNNs) has drawn great
attention due to the security risk of applying these models in real-world
applications. Based on transferability of adversarial examples, an increasing
number of transfer-based methods have been developed to fool black-box DNN
models whose architecture and parameters are inaccessible. Although tremendous
effort has been exerted, there still lacks a standardized benchmark that could
be taken advantage of to compare these methods systematically, fairly, and
practically. Our investigation shows that the evaluation of some methods needs
to be more reasonable and more thorough to verify their effectiveness, to
avoid, for example, unfair comparison and insufficient consideration of
possible substitute/victim models. Therefore, we establish a transfer-based
attack benchmark (TA-Bench) which implements 30+ methods. In this paper, we
evaluate and compare them comprehensively on 25 popular substitute/victim
models on ImageNet. New insights about the effectiveness of these methods are
gained and guidelines for future evaluations are provided. Code at:
https://github.com/qizhangli/TA-Bench.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qizhang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yiwen Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1&quot;&gt;Wangmeng Zuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01352">
<title>Deep learning based Image Compression for Microscopy Images: An Empirical Study. (arXiv:2311.01352v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.01352</link>
<description rdf:parseType="Literal">&lt;p&gt;With the fast development of modern microscopes and bioimaging techniques, an
unprecedentedly large amount of imaging data are being generated, stored,
analyzed, and even shared through networks. The size of the data poses great
challenges for current data infrastructure. One common way to reduce the data
size is by image compression. This present study analyzes classic and deep
learning based image compression methods, and their impact on deep learning
based image processing models. Deep learning based label-free prediction models
(i.e., predicting fluorescent images from bright field images) are used as an
example application for comparison and analysis. Effective image compression
methods could help reduce the data size significantly without losing necessary
information, and therefore reduce the burden on data management infrastructure
and permit fast transmission through the network for data sharing or cloud
computing. To compress images in such a wanted way, multiple classical lossy
image compression techniques are compared to several AI-based compression
models provided by and trained with the CompressAI toolbox using python. These
different compression techniques are compared in compression ratio, multiple
image similarity measures and, most importantly, the prediction accuracy from
label-free models on compressed images. We found that AI-based compression
techniques largely outperform the classic ones and will minimally affect the
downstream label-free task in 2D cases. In the end, we hope the present study
could shed light on the potential of deep learning based image compression and
the impact of image compression on downstream deep learning based image
analysis models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sollman_J/0/1/0/all/0/1&quot;&gt;Jan Sollman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jianxu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01357">
<title>Robust Identity Perceptual Watermark Against Deepfake Face Swapping. (arXiv:2311.01357v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01357</link>
<description rdf:parseType="Literal">&lt;p&gt;Notwithstanding offering convenience and entertainment to society, Deepfake
face swapping has caused critical privacy issues with the rapid development of
deep generative models. Due to imperceptible artifacts in high-quality
synthetic images, passive detection models against face swapping in recent
years usually suffer performance damping regarding the generalizability issue.
Therefore, several studies have been attempted to proactively protect the
original images against malicious manipulations by inserting invisible signals
in advance. However, the existing proactive defense approaches demonstrate
unsatisfactory results with respect to visual quality, detection accuracy, and
source tracing ability. In this study, we propose the first robust identity
perceptual watermarking framework that concurrently performs detection and
source tracing against Deepfake face swapping proactively. We assign identity
semantics regarding the image contents to the watermarks and devise an
unpredictable and unreversible chaotic encryption system to ensure watermark
confidentiality. The watermarks are encoded and recovered by jointly training
an encoder-decoder framework along with adversarial image manipulations.
Extensive experiments demonstrate state-of-the-art performance against Deepfake
face swapping under both cross-dataset and cross-manipulation settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1&quot;&gt;Mengxiao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1&quot;&gt;Harry Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_B/0/1/0/all/0/1&quot;&gt;Bin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yinglong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01361">
<title>GPT-4V(ision) as a Generalist Evaluator for Vision-Language Tasks. (arXiv:2311.01361v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01361</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatically evaluating vision-language tasks is challenging, especially
when it comes to reflecting human judgments due to limitations in accounting
for fine-grained details. Although GPT-4V has shown promising results in
various multi-modal tasks, leveraging GPT-4V as a generalist evaluator for
these tasks has not yet been systematically explored. We comprehensively
validate GPT-4V&apos;s capabilities for evaluation purposes, addressing tasks
ranging from foundational image-to-text and text-to-image synthesis to
high-level image-to-image translations and multi-images to text alignment. We
employ two evaluation methods, single-answer grading and pairwise comparison,
using GPT-4V. Notably, GPT-4V shows promising agreement with humans across
various tasks and evaluation methods, demonstrating immense potential for
multi-modal LLMs as evaluators. Despite limitations like restricted visual
clarity grading and real-world complex reasoning, its ability to provide
human-aligned scores enriched with detailed explanations is promising for
universal automatic evaluator.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinlu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yujie Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weizhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_A/0/1/0/all/0/1&quot;&gt;An Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Jun Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1&quot;&gt;Lianke Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Heng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1&quot;&gt;Xifeng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;William Yang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petzold_L/0/1/0/all/0/1&quot;&gt;Linda Ruth Petzold&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01373">
<title>Recognize Any Regions. (arXiv:2311.01373v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01373</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding the semantics of individual regions or patches within
unconstrained images, such as in open-world object detection, represents a
critical yet challenging task in computer vision. Building on the success of
powerful image-level vision-language (ViL) foundation models like CLIP, recent
efforts have sought to harness their capabilities by either training a
contrastive model from scratch with an extensive collection of region-label
pairs or aligning the outputs of a detection model with image-level
representations of region proposals. Despite notable progress, these approaches
are plagued by computationally intensive training requirements, susceptibility
to data noise, and deficiency in contextual information. To address these
limitations, we explore the synergistic potential of off-the-shelf foundation
models, leveraging their respective strengths in localization and semantics. We
introduce a novel, generic, and efficient region recognition architecture,
named RegionSpot, designed to integrate position-aware localization knowledge
from a localization foundation model (e.g., SAM) with semantic information
extracted from a ViL model (e.g., CLIP). To fully exploit pretrained knowledge
while minimizing training overhead, we keep both foundation models frozen,
focusing optimization efforts solely on a lightweight attention-based knowledge
integration module. Through extensive experiments in the context of open-world
object recognition, our RegionSpot demonstrates significant performance
improvements over prior alternatives, while also providing substantial
computational savings. For instance, training our model with 3 million data in
a single day using 8 V100 GPUs. Our model outperforms GLIP by 6.5 % in mean
average precision (mAP), with an even larger margin by 14.8 % for more
challenging and rare categories.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Haosen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chuofan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1&quot;&gt;Bin Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yi Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1&quot;&gt;Zehuan Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiatian Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01380">
<title>Sim2Real Bilevel Adaptation for Object Surface Classification using Vision-Based Tactile Sensors. (arXiv:2311.01380v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.01380</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we address the Sim2Real gap in the field of vision-based
tactile sensors for classifying object surfaces. We train a Diffusion Model to
bridge this gap using a relatively small dataset of real-world images randomly
collected from unlabeled everyday objects via the DIGIT sensor. Subsequently,
we employ a simulator to generate images by uniformly sampling the surface of
objects from the YCB Model Set. These simulated images are then translated into
the real domain using the Diffusion Model and automatically labeled to train a
classifier. During this training, we further align features of the two domains
using an adversarial procedure. Our evaluation is conducted on a dataset of
tactile images obtained from a set of ten 3D printed YCB objects. The results
reveal a total accuracy of 81.9%, a significant improvement compared to the
34.7% achieved by the classifier trained solely on simulated images. This
demonstrates the effectiveness of our approach. We further validate our
approach using the classifier on a 6D object pose estimation task from tactile
data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caddeo_G/0/1/0/all/0/1&quot;&gt;Gabriele M. Caddeo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maracani_A/0/1/0/all/0/1&quot;&gt;Andrea Maracani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alfano_P/0/1/0/all/0/1&quot;&gt;Paolo D. Alfano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piga_N/0/1/0/all/0/1&quot;&gt;Nicola A. Piga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosasco_L/0/1/0/all/0/1&quot;&gt;Lorenzo Rosasco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Natale_L/0/1/0/all/0/1&quot;&gt;Lorenzo Natale&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01394">
<title>Learning Realistic Traffic Agents in Closed-loop. (arXiv:2311.01394v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.01394</link>
<description rdf:parseType="Literal">&lt;p&gt;Realistic traffic simulation is crucial for developing self-driving software
in a safe and scalable manner prior to real-world deployment. Typically,
imitation learning (IL) is used to learn human-like traffic agents directly
from real-world observations collected offline, but without explicit
specification of traffic rules, agents trained from IL alone frequently display
unrealistic infractions like collisions and driving off the road. This problem
is exacerbated in out-of-distribution and long-tail scenarios. On the other
hand, reinforcement learning (RL) can train traffic agents to avoid
infractions, but using RL alone results in unhuman-like driving behaviors. We
propose Reinforcing Traffic Rules (RTR), a holistic closed-loop learning
objective to match expert demonstrations under a traffic compliance constraint,
which naturally gives rise to a joint IL + RL approach, obtaining the best of
both worlds. Our method learns in closed-loop simulations of both nominal
scenarios from real-world datasets as well as procedurally generated long-tail
scenarios. Our experiments show that RTR learns more realistic and
generalizable traffic simulation policies, achieving significantly better
tradeoffs between human-like driving and traffic compliance in both nominal and
long-tail scenarios. Moreover, when used as a data generation tool for training
prediction models, our learned traffic policy leads to considerably improved
downstream prediction metrics compared to baseline traffic agents. For more
information, visit the project website: https://waabi.ai/rtr
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chris Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_J/0/1/0/all/0/1&quot;&gt;James Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lunjun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1&quot;&gt;Kelvin Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suo_S/0/1/0/all/0/1&quot;&gt;Simon Suo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1&quot;&gt;Raquel Urtasun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01405">
<title>Learning to See Physical Properties with Active Sensing Motor Policies. (arXiv:2311.01405v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.01405</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge of terrain&apos;s physical properties inferred from color images can aid
in making efficient robotic locomotion plans. However, unlike image
classification, it is unintuitive for humans to label image patches with
physical properties. Without labeled data, building a vision system that takes
as input the observed terrain and predicts physical properties remains
challenging. We present a method that overcomes this challenge by
self-supervised labeling of images captured by robots during real-world
traversal with physical property estimators trained in simulation. To ensure
accurate labeling, we introduce Active Sensing Motor Policies (ASMP), which are
trained to explore locomotion behaviors that increase the accuracy of
estimating physical parameters. For instance, the quadruped robot learns to
swipe its foot against the ground to estimate the friction coefficient
accurately. We show that the visual system trained with a small amount of
real-world traversal data accurately predicts physical parameters. The trained
system is robust and works even with overhead images captured by a drone
despite being trained on data collected by cameras attached to a quadruped
robot walking on the ground.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Margolis_G/0/1/0/all/0/1&quot;&gt;Gabriel B. Margolis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1&quot;&gt;Xiang Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1&quot;&gt;Yandong Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1&quot;&gt;Pulkit Agrawal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01410">
<title>The Blessing of Randomness: SDE Beats ODE in General Diffusion-based Image Editing. (arXiv:2311.01410v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01410</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a unified probabilistic formulation for diffusion-based image
editing, where a latent variable is edited in a task-specific manner and
generally deviates from the corresponding marginal distribution induced by the
original stochastic or ordinary differential equation (SDE or ODE). Instead, it
defines a corresponding SDE or ODE for editing. In the formulation, we prove
that the Kullback-Leibler divergence between the marginal distributions of the
two SDEs gradually decreases while that for the ODEs remains as the time
approaches zero, which shows the promise of SDE in image editing. Inspired by
it, we provide the SDE counterparts for widely used ODE baselines in various
tasks including inpainting and image-to-image translation, where SDE shows a
consistent and substantial improvement. Moreover, we propose SDE-Drag -- a
simple yet effective method built upon the SDE formulation for point-based
content dragging. We build a challenging benchmark (termed DragBench) with
open-set natural, art, and AI-generated images for evaluation. A user study on
DragBench indicates that SDE-Drag significantly outperforms our ODE baseline,
existing diffusion-based methods, and the renowned DragGAN. Our results
demonstrate the superiority and versatility of SDE in image editing and push
the boundary of diffusion-based editing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_S/0/1/0/all/0/1&quot;&gt;Shen Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1&quot;&gt;Hanzhong Allan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Cheng Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuhao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Chenyu Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chongxuan Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01423">
<title>CenterRadarNet: Joint 3D Object Detection and Tracking Framework using 4D FMCW Radar. (arXiv:2311.01423v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01423</link>
<description rdf:parseType="Literal">&lt;p&gt;Robust perception is a vital component for ensuring safe autonomous and
assisted driving. Automotive radar (77 to 81 GHz), which offers
weather-resilient sensing, provides a complementary capability to the vision-
or LiDAR-based autonomous driving systems. Raw radio-frequency (RF) radar
tensors contain rich spatiotemporal semantics besides 3D location information.
The majority of previous methods take in 3D (Doppler-range-azimuth) RF radar
tensors, allowing prediction of an object&apos;s location, heading angle, and size
in bird&apos;s-eye-view (BEV). However, they lack the ability to at the same time
infer objects&apos; size, orientation, and identity in the 3D space. To overcome
this limitation, we propose an efficient joint architecture called
CenterRadarNet, designed to facilitate high-resolution representation learning
from 4D (Doppler-range-azimuth-elevation) radar data for 3D object detection
and re-identification (re-ID) tasks. As a single-stage 3D object detector,
CenterRadarNet directly infers the BEV object distribution confidence maps,
corresponding 3D bounding box attributes, and appearance embedding for each
pixel. Moreover, we build an online tracker utilizing the learned appearance
embedding for re-ID. CenterRadarNet achieves the state-of-the-art result on the
K-Radar 3D object detection benchmark. In addition, we present the first 3D
object-tracking result using radar on the K-Radar dataset V2. In diverse
driving scenarios, CenterRadarNet shows consistent, robust performance,
emphasizing its wide applicability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jen-Hao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuan_S/0/1/0/all/0/1&quot;&gt;Sheng-Yao Kuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Latapie_H/0/1/0/all/0/1&quot;&gt;Hugo Latapie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Gaowen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1&quot;&gt;Jenq-Neng Hwang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01425">
<title>Exploring Deep Learning Techniques for Glaucoma Detection: A Comprehensive Review. (arXiv:2311.01425v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.01425</link>
<description rdf:parseType="Literal">&lt;p&gt;Glaucoma is one of the primary causes of vision loss around the world,
necessitating accurate and efficient detection methods. Traditional manual
detection approaches have limitations in terms of cost, time, and subjectivity.
Recent developments in deep learning approaches demonstrate potential in
automating glaucoma detection by detecting relevant features from retinal
fundus images. This article provides a comprehensive overview of cutting-edge
deep learning methods used for the segmentation, classification, and detection
of glaucoma. By analyzing recent studies, the effectiveness and limitations of
these techniques are evaluated, key findings are highlighted, and potential
areas for further research are identified. The use of deep learning algorithms
may significantly improve the efficacy, usefulness, and accuracy of glaucoma
detection. The findings from this research contribute to the ongoing
advancements in automated glaucoma detection and have implications for
improving patient outcomes and reducing the global burden of glaucoma.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Soofi_A/0/1/0/all/0/1&quot;&gt;Aized Amin Soofi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fazal-e-Amin/0/1/0/all/0/1&quot;&gt;Fazal-e-Amin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01429">
<title>Efficient Vision Transformer for Accurate Traffic Sign Detection. (arXiv:2311.01429v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01429</link>
<description rdf:parseType="Literal">&lt;p&gt;This research paper addresses the challenges associated with traffic sign
detection in self-driving vehicles and driver assistance systems. The
development of reliable and highly accurate algorithms is crucial for the
widespread adoption of traffic sign recognition and detection (TSRD) in diverse
real-life scenarios. However, this task is complicated by suboptimal traffic
images affected by factors such as camera movement, adverse weather conditions,
and inadequate lighting. This study specifically focuses on traffic sign
detection methods and introduces the application of the Transformer model,
particularly the Vision Transformer variants, to tackle this task. The
Transformer&apos;s attention mechanism, originally designed for natural language
processing, offers improved parallel efficiency. Vision Transformers have
demonstrated success in various domains, including autonomous driving, object
detection, healthcare, and defense-related applications. To enhance the
efficiency of the Transformer model, the research proposes a novel strategy
that integrates a locality inductive bias and a transformer module. This
includes the introduction of the Efficient Convolution Block and the Local
Transformer Block, which effectively capture short-term and long-term
dependency information, thereby improving both detection speed and accuracy.
Experimental evaluations demonstrate the significant advancements achieved by
this approach, particularly when applied to the GTSDB dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaleybar_J/0/1/0/all/0/1&quot;&gt;Javad Mirzapour Kaleybar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khaloo_H/0/1/0/all/0/1&quot;&gt;Hooman Khaloo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naghipour_A/0/1/0/all/0/1&quot;&gt;Avaz Naghipour&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01432">
<title>Transformation Decoupling Strategy based on Screw Theory for Deterministic Point Cloud Registration with Gravity Prior. (arXiv:2311.01432v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01432</link>
<description rdf:parseType="Literal">&lt;p&gt;Point cloud registration is challenging in the presence of heavy outlier
correspondences. This paper focuses on addressing the robust
correspondence-based registration problem with gravity prior that often arises
in practice. The gravity directions are typically obtained by inertial
measurement units (IMUs) and can reduce the degree of freedom (DOF) of rotation
from 3 to 1. We propose a novel transformation decoupling strategy by
leveraging screw theory. This strategy decomposes the original 4-DOF problem
into three sub-problems with 1-DOF, 2-DOF, and 1-DOF, respectively, thereby
enhancing the computation efficiency. Specifically, the first 1-DOF represents
the translation along the rotation axis and we propose an interval
stabbing-based method to solve it. The second 2-DOF represents the pole which
is an auxiliary variable in screw theory and we utilize a branch-and-bound
method to solve it. The last 1-DOF represents the rotation angle and we propose
a global voting method for its estimation. The proposed method sequentially
solves three consensus maximization sub-problems, leading to efficient and
deterministic registration. In particular, it can even handle the
correspondence-free registration problem due to its significant robustness.
Extensive experiments on both synthetic and real-world datasets demonstrate
that our method is more efficient and robust than state-of-the-art methods,
even when dealing with outlier rates exceeding 99%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xinyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zijian Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yinlong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zimmer_W/0/1/0/all/0/1&quot;&gt;Walter Zimmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1&quot;&gt;Hu Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Feihu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1&quot;&gt;Alois Knoll&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01441">
<title>Distilling Out-of-Distribution Robustness from Vision-Language Foundation Models. (arXiv:2311.01441v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.01441</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a conceptually simple and lightweight framework for improving the
robustness of vision models through the combination of knowledge distillation
and data augmentation. We address the conjecture that larger models do not make
for better teachers by showing strong gains in out-of-distribution robustness
when distilling from pretrained foundation models. Following this finding, we
propose Discrete Adversarial Distillation (DAD), which leverages a robust
teacher to generate adversarial examples and a VQGAN to discretize them,
creating more informative samples than standard data augmentation techniques.
We provide a theoretical framework for the use of a robust teacher in the
knowledge distillation with data augmentation setting and demonstrate strong
gains in out-of-distribution robustness and clean accuracy across different
student architectures. Notably, our method adds minor computational overhead
compared to similar techniques and can be easily combined with other data
augmentations for further improvements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1&quot;&gt;Andy Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jindong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Xiong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haohan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01444">
<title>LabelFormer: Object Trajectory Refinement for Offboard Perception from LiDAR Point Clouds. (arXiv:2311.01444v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01444</link>
<description rdf:parseType="Literal">&lt;p&gt;A major bottleneck to scaling-up training of self-driving perception systems
are the human annotations required for supervision. A promising alternative is
to leverage &quot;auto-labelling&quot; offboard perception models that are trained to
automatically generate annotations from raw LiDAR point clouds at a fraction of
the cost. Auto-labels are most commonly generated via a two-stage approach --
first objects are detected and tracked over time, and then each object
trajectory is passed to a learned refinement model to improve accuracy. Since
existing refinement models are overly complex and lack advanced temporal
reasoning capabilities, in this work we propose LabelFormer, a simple,
efficient, and effective trajectory-level refinement approach. Our approach
first encodes each frame&apos;s observations separately, then exploits
self-attention to reason about the trajectory with full temporal context, and
finally decodes the refined object size and per-frame poses. Evaluation on both
urban and highway datasets demonstrates that LabelFormer outperforms existing
works by a large margin. Finally, we show that training on a dataset augmented
with auto-labels generated by our method leads to improved downstream detection
performance compared to existing methods. Please visit the project website for
details https://waabi.ai/labelformer
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1&quot;&gt;Anqi Joyce Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casas_S/0/1/0/all/0/1&quot;&gt;Sergio Casas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dvornik_N/0/1/0/all/0/1&quot;&gt;Nikita Dvornik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Segal_S/0/1/0/all/0/1&quot;&gt;Sean Segal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1&quot;&gt;Yuwen Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jordan Sir Kwang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1&quot;&gt;Carter Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1&quot;&gt;Raquel Urtasun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01446">
<title>Adv3D: Generating Safety-Critical 3D Objects through Closed-Loop Simulation. (arXiv:2311.01446v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.01446</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-driving vehicles (SDVs) must be rigorously tested on a wide range of
scenarios to ensure safe deployment. The industry typically relies on
closed-loop simulation to evaluate how the SDV interacts on a corpus of
synthetic and real scenarios and verify it performs properly. However, they
primarily only test the system&apos;s motion planning module, and only consider
behavior variations. It is key to evaluate the full autonomy system in
closed-loop, and to understand how variations in sensor data based on scene
appearance, such as the shape of actors, affect system performance. In this
paper, we propose a framework, Adv3D, that takes real world scenarios and
performs closed-loop sensor simulation to evaluate autonomy performance, and
finds vehicle shapes that make the scenario more challenging, resulting in
autonomy failures and uncomfortable SDV maneuvers. Unlike prior works that add
contrived adversarial shapes to vehicle roof-tops or roadside to harm
perception only, we optimize a low-dimensional shape representation to modify
the vehicle shape itself in a realistic manner to degrade autonomy performance
(e.g., perception, prediction, and motion planning). Moreover, we find that the
shape variations found with Adv3D optimized in closed-loop are much more
effective than those in open-loop, demonstrating the importance of finding
scene appearance variations that affect autonomy in the interactive setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarva_J/0/1/0/all/0/1&quot;&gt;Jay Sarva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingkang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_J/0/1/0/all/0/1&quot;&gt;James Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1&quot;&gt;Yuwen Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manivasagam_S/0/1/0/all/0/1&quot;&gt;Sivabalan Manivasagam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1&quot;&gt;Raquel Urtasun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01447">
<title>CADSim: Robust and Scalable in-the-wild 3D Reconstruction for Controllable Sensor Simulation. (arXiv:2311.01447v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01447</link>
<description rdf:parseType="Literal">&lt;p&gt;Realistic simulation is key to enabling safe and scalable development of %
self-driving vehicles. A core component is simulating the sensors so that the
entire autonomy system can be tested in simulation. Sensor simulation involves
modeling traffic participants, such as vehicles, with high quality appearance
and articulated geometry, and rendering them in real time. The self-driving
industry has typically employed artists to build these assets. However, this is
expensive, slow, and may not reflect reality. Instead, reconstructing assets
automatically from sensor data collected in the wild would provide a better
path to generating a diverse and large set with good real-world coverage.
Nevertheless, current reconstruction approaches struggle on in-the-wild sensor
data, due to its sparsity and noise. To tackle these issues, we present CADSim,
which combines part-aware object-class priors via a small set of CAD models
with differentiable rendering to automatically reconstruct vehicle geometry,
including articulated wheels, with high-quality appearance. Our experiments
show our method recovers more accurate shapes from sparse data compared to
existing approaches. Importantly, it also trains and renders efficiently. We
demonstrate our reconstructed vehicles in several applications, including
accurate testing of autonomy perception systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingkang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manivasagam_S/0/1/0/all/0/1&quot;&gt;Sivabalan Manivasagam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Ze Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barsan_I/0/1/0/all/0/1&quot;&gt;Ioan Andrei B&amp;#xe2;rsan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1&quot;&gt;Anqi Joyce Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1&quot;&gt;Wei-Chiu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1&quot;&gt;Raquel Urtasun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2002.07897">
<title>LocoGAN -- Locally Convolutional GAN. (arXiv:2002.07897v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2002.07897</link>
<description rdf:parseType="Literal">&lt;p&gt;In the paper we construct a fully convolutional GAN model: LocoGAN, which
latent space is given by noise-like images of possibly different resolutions.
The learning is local, i.e. we process not the whole noise-like image, but the
sub-images of a fixed size. As a consequence LocoGAN can produce images of
arbitrary dimensions e.g. LSUN bedroom data set. Another advantage of our
approach comes from the fact that we use the position channels, which allows
the generation of fully periodic (e.g. cylindrical panoramic images) or almost
periodic ,,infinitely long&quot; images (e.g. wall-papers).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Struski_L/0/1/0/all/0/1&quot;&gt;&amp;#x141;ukasz Struski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Knop_S/0/1/0/all/0/1&quot;&gt;Szymon Knop&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tabor_J/0/1/0/all/0/1&quot;&gt;Jacek Tabor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Daniec_W/0/1/0/all/0/1&quot;&gt;Wiktor Daniec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Spurek_P/0/1/0/all/0/1&quot;&gt;Przemys&amp;#x142;aw Spurek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2002.09625">
<title>Neural Architecture Search for Compressed Sensing Magnetic Resonance Image Reconstruction. (arXiv:2002.09625v7 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2002.09625</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent works have demonstrated that deep learning (DL) based compressed
sensing (CS) implementation can accelerate Magnetic Resonance (MR) Imaging by
reconstructing MR images from sub-sampled k-space data. However, network
architectures adopted in previous methods are all designed by handcraft. Neural
Architecture Search (NAS) algorithms can automatically build neural network
architectures which have outperformed human designed ones in several vision
tasks. Inspired by this, here we proposed a novel and efficient network for the
MR image reconstruction problem via NAS instead of manual attempts.
Particularly, a specific cell structure, which was integrated into the
model-driven MR reconstruction pipeline, was automatically searched from a
flexible pre-defined operation search space in a differentiable manner.
Experimental results show that our searched network can produce better
reconstruction results compared to previous state-of-the-art methods in terms
of PSNR and SSIM with 4-6 times fewer computation resources. Extensive
experiments were conducted to analyze how hyper-parameters affect
reconstruction performance and the searched structures. The generalizability of
the searched architecture was also evaluated on different organ MR datasets.
Our proposed method can reach a better trade-off between computation cost and
reconstruction performance for MR reconstruction problem with good
generalizability and offer insights to design neural networks for other medical
image applications. The evaluation code will be available at
https://github.com/yjump/NAS-for-CSMRI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Jiangpeng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongbing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2104.04926">
<title>Deep learning-based Edge-aware pre and post-processing methods for JPEG compressed images. (arXiv:2104.04926v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2104.04926</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a learning-based compression scheme that envelopes a standard
codec between pre and post-processing deep CNNs. Specifically, we demonstrate
improvements over prior approaches utilizing a compression-decompression
network by introducing: (a) an edge-aware loss function to prevent blurring
that is commonly occurred in prior works &amp;amp; (b) a super-resolution convolutional
neural network (CNN) for post-processing along with a corresponding
pre-processing network for improved rate-distortion performance in the low rate
regime. The algorithm is assessed on a variety of datasets varying from low to
high resolution namely Set 5, Set 7, Classic 5, Set 14, Live 1, Kodak, General
100, CLIC 2019. When compared to JPEG, JPEG2000, BPG, and recent CNN approach,
the proposed algorithm contributes significant improvement in PSNR with an
approximate gain of 20.75%, 8.47%, 3.22%, 3.23% and 24.59%, 14.46%, 10.14%,
8.57% at low and high bit-rates respectively. Similarly, this improvement in
MS-SSIM is approximately 71.43%, 50%, 36.36%, 23.08%, 64.70% and 64.47%,
61.29%, 47.06%, 51.52%, 16.28% at low and high bit-rates respectively. With
CLIC 2019 dataset, PSNR is found to be superior with approximately 16.67%,
10.53%, 6.78%, and 24.62%, 17.39%, 14.08% at low and high bit-rates
respectively, over JPEG2000, BPG, and recent CNN approach. Similarly, the
MS-SSIM is found to be superior with approximately 72%, 45.45%, 39.13%, 18.52%,
and 71.43%, 50%, 41.18%, 17.07% at low and high bit-rates respectively,
compared to the same approaches. A similar type of improvement is achieved with
other datasets also.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mishra_D/0/1/0/all/0/1&quot;&gt;Dipti Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Satish Kumar Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Singh_R/0/1/0/all/0/1&quot;&gt;Rajat Kumar Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.01209">
<title>EResFD: Rediscovery of the Effectiveness of Standard Convolution for Lightweight Face Detection. (arXiv:2204.01209v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2204.01209</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper analyzes the design choices of face detection architecture that
improve efficiency of computation cost and accuracy. Specifically, we
re-examine the effectiveness of the standard convolutional block as a
lightweight backbone architecture for face detection. Unlike the current
tendency of lightweight architecture design, which heavily utilizes depthwise
separable convolution layers, we show that heavily channel-pruned standard
convolution layers can achieve better accuracy and inference speed when using a
similar parameter size. This observation is supported by the analyses
concerning the characteristics of the target data domain, faces. Based on our
observation, we propose to employ ResNet with a highly reduced channel, which
surprisingly allows high efficiency compared to other mobile-friendly networks
(e.g., MobileNetV1, V2, V3). From the extensive experiments, we show that the
proposed backbone can replace that of the state-of-the-art face detector with a
faster inference speed. Also, we further propose a new feature aggregation
method to maximize the detection performance. Our proposed detector EResFD
obtained 80.4% mAP on WIDER FACE Hard subset which only takes 37.7 ms for VGA
image inference on CPU. Code is available at https://github.com/clovaai/EResFD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1&quot;&gt;Joonhyun Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1&quot;&gt;Beomyoung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Joonsang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1&quot;&gt;Youngjoon Yoo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.06950">
<title>Lossy Image Compression with Conditional Diffusion Models. (arXiv:2209.06950v6 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.06950</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper outlines an end-to-end optimized lossy image compression framework
using diffusion generative models. The approach relies on the transform coding
paradigm, where an image is mapped into a latent space for entropy coding and,
from there, mapped back to the data space for reconstruction. In contrast to
VAE-based neural compression, where the (mean) decoder is a deterministic
neural network, our decoder is a conditional diffusion model. Our approach thus
introduces an additional &quot;content&quot; latent variable on which the reverse
diffusion process is conditioned and uses this variable to store information
about the image. The remaining &quot;texture&quot; variables characterizing the diffusion
process are synthesized at decoding time. We show that the model&apos;s performance
can be tuned toward perceptual metrics of interest. Our extensive experiments
involving multiple datasets and image quality assessment metrics show that our
approach yields stronger reported FID scores than the GAN-based model, while
also yielding competitive performance with VAE-based models in several
distortion metrics. Furthermore, training the diffusion with X-parameterization
enables high-quality reconstructions in only a handful of decoding steps,
greatly affecting the model&apos;s practicality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_R/0/1/0/all/0/1&quot;&gt;Ruihan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mandt_S/0/1/0/all/0/1&quot;&gt;Stephan Mandt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.04366">
<title>KP-RNN: A Deep Learning Pipeline for Human Motion Prediction and Synthesis of Performance Art. (arXiv:2210.04366v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.04366</link>
<description rdf:parseType="Literal">&lt;p&gt;Digitally synthesizing human motion is an inherently complex process, which
can create obstacles in application areas such as virtual reality. We offer a
new approach for predicting human motion, KP-RNN, a neural network which can
integrate easily with existing image processing and generation pipelines. We
utilize a new human motion dataset of performance art, Take The Lead, as well
as the motion generation pipeline, the Everybody Dance Now system, to
demonstrate the effectiveness of KP-RNN&apos;s motion predictions. We have found
that our neural network can predict human dance movements effectively, which
serves as a baseline result for future works using the Take The Lead dataset.
Since KP-RNN can work alongside a system such as Everybody Dance Now, we argue
that our approach could inspire new methods for rendering human avatar
animation. This work also serves to benefit the visualization of performance
art in digital platforms by utilizing accessible neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perrine_P/0/1/0/all/0/1&quot;&gt;Patrick Perrine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirkby_T/0/1/0/all/0/1&quot;&gt;Trevor Kirkby&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.07440">
<title>Leveraging Automatic Personalised Nutrition: Food Image Recognition Benchmark and Dataset based on Nutrition Taxonomy. (arXiv:2211.07440v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.07440</link>
<description rdf:parseType="Literal">&lt;p&gt;Maintaining a healthy lifestyle has become increasingly challenging in
today&apos;s sedentary society marked by poor eating habits. To address this issue,
both national and international organisations have made numerous efforts to
promote healthier diets and increased physical activity. However, implementing
these recommendations in daily life can be difficult, as they are often generic
and not tailored to individuals. This study presents the AI4Food-NutritionDB
database, the first nutrition database that incorporates food images and a
nutrition taxonomy based on recommendations by national and international
health authorities. The database offers a multi-level categorisation,
comprising 6 nutritional levels, 19 main categories (e.g., &quot;Meat&quot;), 73
subcategories (e.g., &quot;White Meat&quot;), and 893 specific food products (e.g.,
&quot;Chicken&quot;). The AI4Food-NutritionDB opens the doors to new food computing
approaches in terms of food intake frequency, quality, and categorisation.
Also, we present a standardised experimental protocol and benchmark including
three tasks based on the nutrition taxonomy (i.e., category, subcategory, and
final product recognition). These resources are available to the research
community, including our deep learning models trained on AI4Food-NutritionDB,
which can serve as pre-trained models, achieving accurate recognition results
for challenging food image databases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romero_Tapiador_S/0/1/0/all/0/1&quot;&gt;Sergio Romero-Tapiador&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tolosana_R/0/1/0/all/0/1&quot;&gt;Ruben Tolosana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1&quot;&gt;Aythami Morales&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1&quot;&gt;Julian Fierrez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vera_Rodriguez_R/0/1/0/all/0/1&quot;&gt;Ruben Vera-Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Espinosa_Salinas_I/0/1/0/all/0/1&quot;&gt;Isabel Espinosa-Salinas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freixer_G/0/1/0/all/0/1&quot;&gt;Gala Freixer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pau_E/0/1/0/all/0/1&quot;&gt;Enrique Carrillo de Santa Pau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molina_A/0/1/0/all/0/1&quot;&gt;Ana Ram&amp;#xed;rez de Molina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortega_Garcia_J/0/1/0/all/0/1&quot;&gt;Javier Ortega-Garcia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.08317">
<title>Ultrasound Plane Pose Regression: Assessing Generalized Pose Coordinates in the Fetal Brain. (arXiv:2301.08317v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.08317</link>
<description rdf:parseType="Literal">&lt;p&gt;In obstetric ultrasound (US) scanning, the learner&apos;s ability to mentally
build a three-dimensional (3D) map of the fetus from a two-dimensional (2D) US
image represents a significant challenge in skill acquisition. We aim to build
a US plane localization system for 3D visualization, training, and guidance
without integrating additional sensors. This work builds on top of our previous
work, which predicts the six-dimensional (6D) pose of arbitrarily oriented US
planes slicing the fetal brain with respect to a normalized reference frame
using a convolutional neural network (CNN) regression network. Here, we analyze
in detail the assumptions of the normalized fetal brain reference frame and
quantify its accuracy with respect to the acquisition of transventricular (TV)
standard plane (SP) for fetal biometry. We investigate the impact of
registration quality in the training and testing data and its subsequent effect
on trained models. Finally, we introduce data augmentations and larger training
sets that improve the results of our previous work, achieving median errors of
2.97 mm and 6.63 degrees for translation and rotation, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vece_C/0/1/0/all/0/1&quot;&gt;Chiara Di Vece&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lous_M/0/1/0/all/0/1&quot;&gt;Maela Le Lous&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dromey_B/0/1/0/all/0/1&quot;&gt;Brian Dromey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasconcelos_F/0/1/0/all/0/1&quot;&gt;Francisco Vasconcelos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+David_A/0/1/0/all/0/1&quot;&gt;Anna L David&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peebles_D/0/1/0/all/0/1&quot;&gt;Donald Peebles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1&quot;&gt;Danail Stoyanov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.11201">
<title>Relative-Interior Solution for (Incomplete) Linear Assignment Problem with Applications to Quadratic Assignment Problem. (arXiv:2301.11201v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2301.11201</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the set of optimal solutions of the dual linear programming
formulation of the linear assignment problem (LAP) to propose a method for
computing a solution from the relative interior of this set. Assuming that an
arbitrary dual-optimal solution and an optimal assignment are available (for
which many efficient algorithms already exist), our method computes a
relative-interior solution in linear time. Since LAP occurs as a subproblem in
the linear programming relaxation of quadratic assignment problem (QAP), we
employ our method as a new component in the family of dual-ascent algorithms
that provide bounds on the optimal value of QAP. To make our results applicable
to incomplete QAP, which is of interest in practical use-cases, we also provide
a linear-time reduction from incomplete LAP to complete LAP along with a
mapping that preserves optimality and membership in the relative interior. Our
experiments on publicly available benchmarks indicate that our approach with
relative-interior solution is frequently capable of providing superior bounds
and otherwise is at least comparable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Dlask_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;&amp;#x161; Dlask&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Savchynskyy_B/0/1/0/all/0/1&quot;&gt;Bogdan Savchynskyy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.10056">
<title>GlueGen: Plug and Play Multi-modal Encoders for X-to-image Generation. (arXiv:2303.10056v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.10056</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-image (T2I) models based on diffusion processes have achieved
remarkable success in controllable image generation using user-provided
captions. However, the tight coupling between the current text encoder and
image decoder in T2I models makes it challenging to replace or upgrade. Such
changes often require massive fine-tuning or even training from scratch with
the prohibitive expense. To address this problem, we propose GlueGen, which
applies a newly proposed GlueNet model to align features from single-modal or
multi-modal encoders with the latent space of an existing T2I model. The
approach introduces a new training objective that leverages parallel corpora to
align the representation spaces of different encoders. Empirical results show
that GlueNet can be trained efficiently and enables various capabilities beyond
previous state-of-the-art models: 1) multilingual language models such as
XLM-Roberta can be aligned with existing T2I models, allowing for the
generation of high-quality images from captions beyond English; 2) GlueNet can
align multi-modal encoders such as AudioCLIP with the Stable Diffusion model,
enabling sound-to-image generation; 3) it can also upgrade the current text
encoder of the latent diffusion model for challenging case generation. By the
alignment of various feature representations, the GlueNet allows for flexible
and efficient integration of new functionality into existing T2I models and
sheds light on X-to-image (X2I) generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1&quot;&gt;Can Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1&quot;&gt;Ning Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_C/0/1/0/all/0/1&quot;&gt;Chen Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zeyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1&quot;&gt;Stefano Ermon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yun Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1&quot;&gt;Caiming Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Ran Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.10725">
<title>SIESTA: Efficient Online Continual Learning with Sleep. (arXiv:2303.10725v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.10725</link>
<description rdf:parseType="Literal">&lt;p&gt;In supervised continual learning, a deep neural network (DNN) is updated with
an ever-growing data stream. Unlike the offline setting where data is shuffled,
we cannot make any distributional assumptions about the data stream. Ideally,
only one pass through the dataset is needed for computational efficiency.
However, existing methods are inadequate and make many assumptions that cannot
be made for real-world applications, while simultaneously failing to improve
computational efficiency. In this paper, we propose a novel continual learning
method, SIESTA based on wake/sleep framework for training, which is well
aligned to the needs of on-device learning. The major goal of SIESTA is to
advance compute efficient continual learning so that DNNs can be updated
efficiently using far less time and energy. The principal innovations of SIESTA
are: 1) rapid online updates using a rehearsal-free, backpropagation-free, and
data-driven network update rule during its wake phase, and 2) expedited memory
consolidation using a compute-restricted rehearsal policy during its sleep
phase. For memory efficiency, SIESTA adapts latent rehearsal using memory
indexing from REMIND. Compared to REMIND and prior arts, SIESTA is far more
computationally efficient, enabling continual learning on ImageNet-1K in under
2 hours on a single GPU; moreover, in the augmentation-free setting it matches
the performance of the offline learner, a milestone critical to driving
adoption of continual learning in real-world applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harun_M/0/1/0/all/0/1&quot;&gt;Md Yousuf Harun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gallardo_J/0/1/0/all/0/1&quot;&gt;Jhair Gallardo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayes_T/0/1/0/all/0/1&quot;&gt;Tyler L. Hayes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kemker_R/0/1/0/all/0/1&quot;&gt;Ronald Kemker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanan_C/0/1/0/all/0/1&quot;&gt;Christopher Kanan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.12145">
<title>Efficient Feature Distillation for Zero-shot Annotation Object Detection. (arXiv:2303.12145v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.12145</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new setting for detecting unseen objects called Zero-shot
Annotation object Detection (ZAD). It expands the zero-shot object detection
setting by allowing the novel objects to exist in the training images and
restricts the additional information the detector uses to novel category names.
Recently, to detect unseen objects, large-scale vision-language models (e.g.,
CLIP) are leveraged by different methods. The distillation-based methods have
good overall performance but suffer from a long training schedule caused by two
factors. First, existing work creates distillation regions biased to the base
categories, which limits the distillation of novel category information.
Second, directly using the raw feature from CLIP for distillation neglects the
domain gap between the training data of CLIP and the detection datasets, which
makes it difficult to learn the mapping from the image region to the
vision-language feature space. To solve these problems, we propose Efficient
feature distillation for Zero-shot Annotation object Detection (EZAD). Firstly,
EZAD adapts the CLIP&apos;s feature space to the target detection domain by
re-normalizing CLIP; Secondly, EZAD uses CLIP to generate distillation
proposals with potential novel category names to avoid the distillation being
overly biased toward the base categories. Finally, EZAD takes advantage of
semantic meaning for regression to further improve the model performance. As a
result, EZAD outperforms the previous distillation-based methods in COCO by 4%
with a much shorter training schedule and achieves a 3% improvement on the LVIS
dataset. Our code is available at https://github.com/dragonlzm/EZAD
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhuoming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xuefeng Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nevatia_R/0/1/0/all/0/1&quot;&gt;Ram Nevatia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.17835">
<title>Improved Difference Images for Change Detection Classifiers in SAR Imagery Using Deep Learning. (arXiv:2303.17835v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.17835</link>
<description rdf:parseType="Literal">&lt;p&gt;Satellite-based Synthetic Aperture Radar (SAR) images can be used as a source
of remote sensed imagery regardless of cloud cover and day-night cycle.
However, the speckle noise and varying image acquisition conditions pose a
challenge for change detection classifiers. This paper proposes a new method of
improving SAR image processing to produce higher quality difference images for
the classification algorithms. The method is built on a neural network-based
mapping transformation function that produces artificial SAR images from a
location in the requested acquisition conditions. The inputs for the model are:
previous SAR images from the location, imaging angle information from the SAR
images, digital elevation model, and weather conditions. The method was tested
with data from a location in North-East Finland by using Sentinel-1 SAR images
from European Space Agency, weather data from Finnish Meteorological Institute,
and a digital elevation model from National Land Survey of Finland. In order to
verify the method, changes to the SAR images were simulated, and the
performance of the proposed method was measured using experimentation where it
gave substantial improvements to performance when compared to a more
conventional method of creating difference images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alatalo_J/0/1/0/all/0/1&quot;&gt;Janne Alatalo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sipola_T/0/1/0/all/0/1&quot;&gt;Tuomo Sipola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rantonen_M/0/1/0/all/0/1&quot;&gt;Mika Rantonen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.10168">
<title>High-Fidelity and Freely Controllable Talking Head Video Generation. (arXiv:2304.10168v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.10168</link>
<description rdf:parseType="Literal">&lt;p&gt;Talking head generation is to generate video based on a given source identity
and target motion. However, current methods face several challenges that limit
the quality and controllability of the generated videos. First, the generated
face often has unexpected deformation and severe distortions. Second, the
driving image does not explicitly disentangle movement-relevant information,
such as poses and expressions, which restricts the manipulation of different
attributes during generation. Third, the generated videos tend to have
flickering artifacts due to the inconsistency of the extracted landmarks
between adjacent frames. In this paper, we propose a novel model that produces
high-fidelity talking head videos with free control over head pose and
expression. Our method leverages both self-supervised learned landmarks and 3D
face model-based landmarks to model the motion. We also introduce a novel
motion-aware multi-scale feature alignment module to effectively transfer the
motion without face distortion. Furthermore, we enhance the smoothness of the
synthesized talking head videos with a feature context adaptation and
propagation module. We evaluate our model on challenging datasets and
demonstrate its state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yue Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinglu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ming_X/0/1/0/all/0/1&quot;&gt;Xiang Ming&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yan Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.13410">
<title>Improving Adversarial Transferability via Intermediate-level Perturbation Decay. (arXiv:2304.13410v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.13410</link>
<description rdf:parseType="Literal">&lt;p&gt;Intermediate-level attacks that attempt to perturb feature representations
following an adversarial direction drastically have shown favorable performance
in crafting transferable adversarial examples. Existing methods in this
category are normally formulated with two separate stages, where a directional
guide is required to be determined at first and the scalar projection of the
intermediate-level perturbation onto the directional guide is enlarged
thereafter. The obtained perturbation deviates from the guide inevitably in the
feature space, and it is revealed in this paper that such a deviation may lead
to sub-optimal attack. To address this issue, we develop a novel
intermediate-level method that crafts adversarial examples within a single
stage of optimization. In particular, the proposed method, named
intermediate-level perturbation decay (ILPD), encourages the intermediate-level
perturbation to be in an effective adversarial direction and to possess a great
magnitude simultaneously. In-depth discussion verifies the effectiveness of our
method. Experimental results show that it outperforms state-of-the-arts by
large margins in attacking various victim models on ImageNet (+10.07% on
average) and CIFAR-10 (+3.88% on average). Our code is at
https://github.com/qizhangli/ILPD-attack.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qizhang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yiwen Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1&quot;&gt;Wangmeng Zuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.13672">
<title>FVP: Fourier Visual Prompting for Source-Free Unsupervised Domain Adaptation of Medical Image Segmentation. (arXiv:2304.13672v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.13672</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical image segmentation methods normally perform poorly when there is a
domain shift between training and testing data. Unsupervised Domain Adaptation
(UDA) addresses the domain shift problem by training the model using both
labeled data from the source domain and unlabeled data from the target domain.
Source-Free UDA (SFUDA) was recently proposed for UDA without requiring the
source data during the adaptation, due to data privacy or data transmission
issues, which normally adapts the pre-trained deep model in the testing stage.
However, in real clinical scenarios of medical image segmentation, the trained
model is normally frozen in the testing stage. In this paper, we propose
Fourier Visual Prompting (FVP) for SFUDA of medical image segmentation.
Inspired by prompting learning in natural language processing, FVP steers the
frozen pre-trained model to perform well in the target domain by adding a
visual prompt to the input target data. In FVP, the visual prompt is
parameterized using only a small amount of low-frequency learnable parameters
in the input frequency space, and is learned by minimizing the segmentation
loss between the predicted segmentation of the prompted target image and
reliable pseudo segmentation label of the target image under the frozen model.
To our knowledge, FVP is the first work to apply visual prompts to SFUDA for
medical image segmentation. The proposed FVP is validated using three public
datasets, and experiments demonstrate that FVP yields better segmentation
results, compared with various existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jian Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yixin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_S/0/1/0/all/0/1&quot;&gt;Shuai Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Lanyun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhenzhou Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Haogang Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.14030">
<title>COSST: Multi-organ Segmentation with Partially Labeled Datasets Using Comprehensive Supervisions and Self-training. (arXiv:2304.14030v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.14030</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models have demonstrated remarkable success in multi-organ
segmentation but typically require large-scale datasets with all organs of
interest annotated. However, medical image datasets are often low in sample
size and only partially labeled, i.e., only a subset of organs are annotated.
Therefore, it is crucial to investigate how to learn a unified model on the
available partially labeled datasets to leverage their synergistic potential.
In this paper, we systematically investigate the partial-label segmentation
problem with theoretical and empirical analyses on the prior techniques. We
revisit the problem from a perspective of partial label supervision signals and
identify two signals derived from ground truth and one from pseudo labels. We
propose a novel two-stage framework termed COSST, which effectively and
efficiently integrates comprehensive supervision signals with self-training.
Concretely, we first train an initial unified model using two ground
truth-based signals and then iteratively incorporate the pseudo label signal to
the initial model using self-training. To mitigate performance degradation
caused by unreliable pseudo labels, we assess the reliability of pseudo labels
via outlier detection in latent space and exclude the most unreliable pseudo
labels from each self-training iteration. Extensive experiments are conducted
on one public and three private partial-label segmentation tasks over 12 CT
datasets. Experimental results show that our proposed COSST achieves
significant improvement over the baseline method, i.e., individual networks
trained on each partially labeled dataset. Compared to the state-of-the-art
partial-label segmentation methods, COSST demonstrates consistent superior
performance on various segmentation tasks and with different training data
sizes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Han Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhoubing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1&quot;&gt;Riqiang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chabin_G/0/1/0/all/0/1&quot;&gt;Guillaume Chabin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oguz_I/0/1/0/all/0/1&quot;&gt;Ipek Oguz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grbic_S/0/1/0/all/0/1&quot;&gt;Sasa Grbic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.01638">
<title>Sequence Modeling with Multiresolution Convolutional Memory. (arXiv:2305.01638v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.01638</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficiently capturing the long-range patterns in sequential data sources
salient to a given task -- such as classification and generative modeling --
poses a fundamental challenge. Popular approaches in the space tradeoff between
the memory burden of brute-force enumeration and comparison, as in
transformers, the computational burden of complicated sequential dependencies,
as in recurrent neural networks, or the parameter burden of convolutional
networks with many or large filters. We instead take inspiration from
wavelet-based multiresolution analysis to define a new building block for
sequence modeling, which we call a MultiresLayer. The key component of our
model is the multiresolution convolution, capturing multiscale trends in the
input sequence. Our MultiresConv can be implemented with shared filters across
a dilated causal convolution tree. Thus it garners the computational advantages
of convolutional networks and the principled theoretical motivation of wavelet
decompositions. Our MultiresLayer is straightforward to implement, requires
significantly fewer parameters, and maintains at most a $\mathcal{O}(N\log N)$
memory footprint for a length $N$ sequence. Yet, by stacking such layers, our
model yields state-of-the-art performance on a number of sequence
classification and autoregressive density estimation tasks using CIFAR-10,
ListOps, and PTB-XL datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jiaxin Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Ke Alexander Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fox_E/0/1/0/all/0/1&quot;&gt;Emily B. Fox&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.08491">
<title>Masked Collaborative Contrast for Weakly Supervised Semantic Segmentation. (arXiv:2305.08491v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.08491</link>
<description rdf:parseType="Literal">&lt;p&gt;This study introduces an efficacious approach, Masked Collaborative Contrast
(MCC), to highlight semantic regions in weakly supervised semantic
segmentation. MCC adroitly draws inspiration from masked image modeling and
contrastive learning to devise a novel framework that induces keys to contract
toward semantic regions. Unlike prevalent techniques that directly eradicate
patch regions in the input image when generating masks, we scrutinize the
neighborhood relations of patch tokens by exploring masks considering keys on
the affinity matrix. Moreover, we generate positive and negative samples in
contrastive learning by utilizing the masked local output and contrasting it
with the global output. Elaborate experiments on commonly employed datasets
evidences that the proposed MCC mechanism effectively aligns global and local
perspectives within the image, attaining impressive performance. The source
code is available at \url{https://github.com/fwu11/MCC}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Fangwen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jingxuan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1&quot;&gt;Yufei Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1&quot;&gt;Yanbin Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1&quot;&gt;Gang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1&quot;&gt;Lechao Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.08776">
<title>Bridging the Domain Gap: Self-Supervised 3D Scene Understanding with Foundation Models. (arXiv:2305.08776v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.08776</link>
<description rdf:parseType="Literal">&lt;p&gt;Foundation models have achieved remarkable results in 2D and language tasks
like image segmentation, object detection, and visual-language understanding.
However, their potential to enrich 3D scene representation learning is largely
untapped due to the existence of the domain gap. In this work, we propose an
innovative methodology called Bridge3D to address this gap by pre-training 3D
models using features, semantic masks, and captions sourced from foundation
models. Specifically, our method employs semantic masks from foundation models
to guide the masking and reconstruction process for the masked autoencoder,
enabling more focused attention on foreground representations. Moreover, we
bridge the 3D-text gap at the scene level using image captioning foundation
models, thereby facilitating scene-level knowledge distillation. We further
extend this bridging effort by introducing an innovative object-level knowledge
distillation method that harnesses highly accurate object-level masks and
semantic text data from foundation models. Our methodology significantly
surpasses the performance of existing state-of-the-art methods in 3D object
detection and semantic segmentation tasks. For instance, on the ScanNet
dataset, Bridge3D improves the baseline by a notable margin of 6.3%. Code will
be available at: https://github.com/Zhimin-C/Bridge3D
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhimin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1&quot;&gt;Longlong Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yingwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bing Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.09253">
<title>Online Continual Learning Without the Storage Constraint. (arXiv:2305.09253v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.09253</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional online continual learning (OCL) research has primarily focused on
mitigating catastrophic forgetting with fixed and limited storage allocation
throughout an agent&apos;s lifetime. However, a broad range of real-world
applications are primarily constrained by computational costs rather than
storage limitations. In this paper, we target such applications, investigating
the online continual learning problem under relaxed storage constraints and
limited computational budgets. We contribute a simple algorithm, which updates
a kNN classifier continually along with a fixed, pretrained feature extractor.
We selected this algorithm due to its exceptional suitability for online
continual learning. It can adapt to rapidly changing streams, has zero
stability gap, operates within tiny computational budgets, has low storage
requirements by only storing features, and has a consistency property: It never
forgets previously seen data. These attributes yield significant improvements,
allowing our proposed algorithm to outperform existing methods by over 20% in
accuracy on two large-scale OCL datasets: Continual LOCalization (CLOC) with
39M images and 712 classes and Continual Google Landmarks V2 (CGLM) with 580K
images and 10,788 classes, even when existing methods retain all previously
seen images. Furthermore, we achieve this superior performance with
considerably reduced computational and storage expenses. We provide code to
reproduce our results at github.com/drimpossible/ACM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabhu_A/0/1/0/all/0/1&quot;&gt;Ameya Prabhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1&quot;&gt;Zhipeng Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dokania_P/0/1/0/all/0/1&quot;&gt;Puneet Dokania&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1&quot;&gt;Philip Torr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1&quot;&gt;Vladlen Koltun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sener_O/0/1/0/all/0/1&quot;&gt;Ozan Sener&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.11147">
<title>UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild. (arXiv:2305.11147v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.11147</link>
<description rdf:parseType="Literal">&lt;p&gt;Achieving machine autonomy and human control often represent divergent
objectives in the design of interactive AI systems. Visual generative
foundation models such as Stable Diffusion show promise in navigating these
goals, especially when prompted with arbitrary languages. However, they often
fall short in generating images with spatial, structural, or geometric
controls. The integration of such controls, which can accommodate various
visual conditions in a single unified model, remains an unaddressed challenge.
In response, we introduce UniControl, a new generative foundation model that
consolidates a wide array of controllable condition-to-image (C2I) tasks within
a singular framework, while still allowing for arbitrary language prompts.
UniControl enables pixel-level-precise image generation, where visual
conditions primarily influence the generated structures and language prompts
guide the style and context. To equip UniControl with the capacity to handle
diverse visual conditions, we augment pretrained text-to-image diffusion models
and introduce a task-aware HyperNet to modulate the diffusion models, enabling
the adaptation to different C2I tasks simultaneously. Trained on nine unique
C2I tasks, UniControl demonstrates impressive zero-shot generation abilities
with unseen visual conditions. Experimental results show that UniControl often
surpasses the performance of single-task-controlled methods of comparable model
sizes. This control versatility positions UniControl as a significant
advancement in the realm of controllable visual generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1&quot;&gt;Can Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1&quot;&gt;Ning Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yihao Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xinyi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yingbo Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Huan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niebles_J/0/1/0/all/0/1&quot;&gt;Juan Carlos Niebles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1&quot;&gt;Caiming Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1&quot;&gt;Silvio Savarese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1&quot;&gt;Stefano Ermon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yun Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Ran Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14764">
<title>Detection of Non-uniformity in Parameters for Magnetic Domain Pattern Generation by Machine Learning. (arXiv:2305.14764v2 [cond-mat.mtrl-sci] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14764</link>
<description rdf:parseType="Literal">&lt;p&gt;We estimate the spatial distribution of heterogeneous physical parameters
involved in the formation of magnetic domain patterns of polycrystalline thin
films by using convolutional neural networks. We propose a method to obtain a
spatial map of physical parameters by estimating the parameters from patterns
within a small subregion window of the full magnetic domain and subsequently
shifting this window. To enhance the accuracy of parameter estimation in such
subregions, we employ large-scale models utilized for natural image
classification and exploit the benefits of pretraining. Using a model with high
estimation accuracy on these subregions, we conduct inference on simulation
data featuring spatially varying parameters and demonstrate the capability to
detect such parameter variations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Mamada_N/0/1/0/all/0/1&quot;&gt;Naoya Mamada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Mizumaki_M/0/1/0/all/0/1&quot;&gt;Masaichiro Mizumaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Akai_I/0/1/0/all/0/1&quot;&gt;Ichiro Akai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Aonishi_T/0/1/0/all/0/1&quot;&gt;Toru Aonishi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18832">
<title>ReTR: Modeling Rendering Via Transformer for Generalizable Neural Surface Reconstruction. (arXiv:2305.18832v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18832</link>
<description rdf:parseType="Literal">&lt;p&gt;Generalizable neural surface reconstruction techniques have attracted great
attention in recent years. However, they encounter limitations of low
confidence depth distribution and inaccurate surface reasoning due to the
oversimplified volume rendering process employed. In this paper, we present
Reconstruction TRansformer (ReTR), a novel framework that leverages the
transformer architecture to redesign the rendering process, enabling complex
render interaction modeling. It introduces a learnable $\textit{meta-ray
token}$ and utilizes the cross-attention mechanism to simulate the interaction
of rendering process with sampled points and render the observed color.
Meanwhile, by operating within a high-dimensional feature space rather than the
color space, ReTR mitigates sensitivity to projected colors in source views.
Such improvements result in accurate surface assessment with high confidence.
We demonstrate the effectiveness of our approach on various datasets,
showcasing how our method outperforms the current state-of-the-art approaches
in terms of reconstruction quality and generalization ability. $\textit{Our
code is available at }$ https://github.com/YixunLiang/ReTR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yixun Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;Hao He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Ying-cong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04699">
<title>DiViNeT: 3D Reconstruction from Disparate Views via Neural Template Regularization. (arXiv:2306.04699v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04699</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a volume rendering-based neural surface reconstruction method that
takes as few as three disparate RGB images as input. Our key idea is to
regularize the reconstruction, which is severely ill-posed and leaving
significant gaps between the sparse views, by learning a set of neural
templates to act as surface priors. Our method, coined DiViNet, operates in two
stages. It first learns the templates, in the form of 3D Gaussian functions,
across different scenes, without 3D supervision. In the reconstruction stage,
our predicted templates serve as anchors to help &quot;stitch&apos;&apos; the surfaces over
sparse regions. We demonstrate that our approach is not only able to complete
the surface geometry but also reconstructs surface details to a reasonable
extent from a few disparate input views. On the DTU and BlendedMVS datasets,
our approach achieves the best reconstruction quality among existing methods in
the presence of such sparse views and performs on par, if not better, with
competing methods when dense views are employed as inputs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vora_A/0/1/0/all/0/1&quot;&gt;Aditya Vora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patil_A/0/1/0/all/0/1&quot;&gt;Akshay Gadi Patil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05225">
<title>Boosting Adversarial Transferability by Achieving Flat Local Maxima. (arXiv:2306.05225v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05225</link>
<description rdf:parseType="Literal">&lt;p&gt;Transfer-based attack adopts the adversarial examples generated on the
surrogate model to attack various models, making it applicable in the physical
world and attracting increasing interest. Recently, various adversarial attacks
have emerged to boost adversarial transferability from different perspectives.
In this work, inspired by the observation that flat local minima are correlated
with good generalization, we assume and empirically validate that adversarial
examples at a flat local region tend to have good transferability by
introducing a penalized gradient norm to the original loss function. Since
directly optimizing the gradient regularization norm is computationally
expensive and intractable for generating adversarial examples, we propose an
approximation optimization method to simplify the gradient update of the
objective function. Specifically, we randomly sample an example and adopt a
first-order procedure to approximate the curvature of Hessian/vector product,
which makes computing more efficient by interpolating two neighboring
gradients. Meanwhile, in order to obtain a more stable gradient direction, we
randomly sample multiple examples and average the gradients of these examples
to reduce the variance due to random sampling during the iterative process.
Extensive experimental results on the ImageNet-compatible dataset show that the
proposed method can generate adversarial examples at flat local regions, and
significantly improve the adversarial transferability on either normally
trained models or adversarially trained models than the state-of-the-art
attacks. Our codes are available at:
https://github.com/Trustworthy-AI-Group/PGN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1&quot;&gt;Zhijin Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hongying Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaosen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1&quot;&gt;Fanhua Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuanyuan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07684">
<title>Lookaround Optimizer: $k$ steps around, 1 step average. (arXiv:2306.07684v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07684</link>
<description rdf:parseType="Literal">&lt;p&gt;Weight Average (WA) is an active research topic due to its simplicity in
ensembling deep networks and the effectiveness in promoting generalization.
Existing weight average approaches, however, are often carried out along only
one training trajectory in a post-hoc manner (i.e., the weights are averaged
after the entire training process is finished), which significantly degrades
the diversity between networks and thus impairs the effectiveness. In this
paper, inspired by weight average, we propose Lookaround, a straightforward yet
effective SGD-based optimizer leading to flatter minima with better
generalization. Specifically, Lookaround iterates two steps during the whole
training period: the around step and the average step. In each iteration, 1)
the around step starts from a common point and trains multiple networks
simultaneously, each on transformed data by a different data augmentation, and
2) the average step averages these trained networks to get the averaged
network, which serves as the starting point for the next iteration. The around
step improves the functionality diversity while the average step guarantees the
weight locality of these networks during the whole training, which is essential
for WA to work. We theoretically explain the superiority of Lookaround by
convergence analysis, and make extensive experiments to evaluate Lookaround on
popular benchmarks including CIFAR and ImageNet with both CNNs and ViTs,
demonstrating clear superiority over state-of-the-arts. Our code is available
at https://github.com/Ardcy/Lookaround.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiangtao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shunyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jie Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1&quot;&gt;Tongtian Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhengqi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1&quot;&gt;Mingli Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07962">
<title>Parting with Misconceptions about Learning-based Vehicle Motion Planning. (arXiv:2306.07962v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07962</link>
<description rdf:parseType="Literal">&lt;p&gt;The release of nuPlan marks a new era in vehicle motion planning research,
offering the first large-scale real-world dataset and evaluation schemes
requiring both precise short-term planning and long-horizon ego-forecasting.
Existing systems struggle to simultaneously meet both requirements. Indeed, we
find that these tasks are fundamentally misaligned and should be addressed
independently. We further assess the current state of closed-loop planning in
the field, revealing the limitations of learning-based methods in complex
real-world scenarios and the value of simple rule-based priors such as
centerline selection through lane graph search algorithms. More surprisingly,
for the open-loop sub-task, we observe that the best results are achieved when
using only this centerline as scene context (i.e., ignoring all information
regarding the map and other agents). Combining these insights, we propose an
extremely simple and efficient planner which outperforms an extensive set of
competitors, winning the nuPlan planning challenge 2023.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dauner_D/0/1/0/all/0/1&quot;&gt;Daniel Dauner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hallgarten_M/0/1/0/all/0/1&quot;&gt;Marcel Hallgarten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1&quot;&gt;Andreas Geiger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chitta_K/0/1/0/all/0/1&quot;&gt;Kashyap Chitta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08129">
<title>AVIS: Autonomous Visual Information Seeking with Large Language Model Agent. (arXiv:2306.08129v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08129</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose an autonomous information seeking visual question
answering framework, AVIS. Our method leverages a Large Language Model (LLM) to
dynamically strategize the utilization of external tools and to investigate
their outputs, thereby acquiring the indispensable knowledge needed to provide
answers to the posed questions. Responding to visual questions that necessitate
external knowledge, such as &quot;What event is commemorated by the building
depicted in this image?&quot;, is a complex task. This task presents a combinatorial
search space that demands a sequence of actions, including invoking APIs,
analyzing their responses, and making informed decisions. We conduct a user
study to collect a variety of instances of human decision-making when faced
with this task. This data is then used to design a system comprised of three
components: an LLM-powered planner that dynamically determines which tool to
use next, an LLM-powered reasoner that analyzes and extracts key information
from the tool outputs, and a working memory component that retains the acquired
information throughout the process. The collected user behavior serves as a
guide for our system in two key ways. First, we create a transition graph by
analyzing the sequence of decisions made by users. This graph delineates
distinct states and confines the set of actions available at each state.
Second, we use examples of user decision-making to provide our LLM-powered
planner and reasoner with relevant contextual instances, enhancing their
capacity to make informed decisions. We show that AVIS achieves
state-of-the-art results on knowledge-intensive visual question answering
benchmarks such as Infoseek and OK-VQA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Ziniu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iscen_A/0/1/0/all/0/1&quot;&gt;Ahmet Iscen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1&quot;&gt;Chen Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1&quot;&gt;Kai-Wei Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yizhou Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ross_D/0/1/0/all/0/1&quot;&gt;David A Ross&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1&quot;&gt;Cordelia Schmid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fathi_A/0/1/0/all/0/1&quot;&gt;Alireza Fathi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10681">
<title>VNVC: A Versatile Neural Video Coding Framework for Efficient Human-Machine Vision. (arXiv:2306.10681v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10681</link>
<description rdf:parseType="Literal">&lt;p&gt;Almost all digital videos are coded into compact representations before being
transmitted. Such compact representations need to be decoded back to pixels
before being displayed to humans and - as usual - before being
enhanced/analyzed by machine vision algorithms. Intuitively, it is more
efficient to enhance/analyze the coded representations directly without
decoding them into pixels. Therefore, we propose a versatile neural video
coding (VNVC) framework, which targets learning compact representations to
support both reconstruction and direct enhancement/analysis, thereby being
versatile for both human and machine vision. Our VNVC framework has a
feature-based compression loop. In the loop, one frame is encoded into compact
representations and decoded to an intermediate feature that is obtained before
performing reconstruction. The intermediate feature can be used as reference in
motion compensation and motion estimation through feature-based temporal
context mining and cross-domain motion encoder-decoder to compress the
following frames. The intermediate feature is directly fed into video
reconstruction, video enhancement, and video analysis networks to evaluate its
effectiveness. The evaluation shows that our framework with the intermediate
feature achieves high compression efficiency for video reconstruction and
satisfactory task performances with lower complexities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sheng_X/0/1/0/all/0/1&quot;&gt;Xihua Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Li Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Houqiang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15111">
<title>Self-Supervised Image Captioning with CLIP. (arXiv:2306.15111v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15111</link>
<description rdf:parseType="Literal">&lt;p&gt;Image captioning, a fundamental task in vision-language understanding, seeks
to generate accurate natural language descriptions for provided images. Current
image captioning approaches heavily rely on high-quality image-caption pairs,
which can be hard to obtain for many domains. To address this, we introduce a
self-supervised image captioning method. After learning an initial signal from
a small labeled dataset, our method transitions to self-supervised learning on
unlabeled data, leveraging the auxiliary task of enhancing the CLIP relevance
between images and generated captions. Remarkably, despite utilizing less than
2% of the labeled COCO dataset, our method delivers a performance comparable to
state-of-the-art models trained on the complete dataset. Human evaluations
further reveal that our method produces captions with greater distinctiveness
and informativeness, two attributes inherently challenging to achieve through
supervised learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1&quot;&gt;Chuanyang Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15668">
<title>Physion++: Evaluating Physical Scene Understanding that Requires Online Inference of Different Physical Properties. (arXiv:2306.15668v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15668</link>
<description rdf:parseType="Literal">&lt;p&gt;General physical scene understanding requires more than simply localizing and
recognizing objects -- it requires knowledge that objects can have different
latent properties (e.g., mass or elasticity), and that those properties affect
the outcome of physical events. While there has been great progress in physical
and video prediction models in recent years, benchmarks to test their
performance typically do not require an understanding that objects have
individual physical properties, or at best test only those properties that are
directly observable (e.g., size or color). This work proposes a novel dataset
and benchmark, termed Physion++, that rigorously evaluates visual physical
prediction in artificial systems under circumstances where those predictions
rely on accurate estimates of the latent physical properties of objects in the
scene. Specifically, we test scenarios where accurate prediction relies on
estimates of properties such as mass, friction, elasticity, and deformability,
and where the values of those properties can only be inferred by observing how
objects move and interact with other objects or fluids. We evaluate the
performance of a number of state-of-the-art prediction models that span a
variety of levels of learning vs. built-in knowledge, and compare that
performance to a set of human predictions. We find that models that have been
trained using standard regimes and datasets do not spontaneously learn to make
inferences about latent properties, but also that models that encode objectness
and physical states tend to make better predictions. However, there is still a
huge gap between all models and human performance, and all models&apos; predictions
correlate poorly with those made by humans, suggesting that no state-of-the-art
model is learning to make physical predictions in a human-like way. Project
page: https://dingmyu.github.io/physion_v2/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tung_H/0/1/0/all/0/1&quot;&gt;Hsiao-Yu Tung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1&quot;&gt;Mingyu Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhenfang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bear_D/0/1/0/all/0/1&quot;&gt;Daniel Bear&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1&quot;&gt;Chuang Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1&quot;&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamins_D/0/1/0/all/0/1&quot;&gt;Daniel LK Yamins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1&quot;&gt;Judith E Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1&quot;&gt;Kevin A. Smith&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16180">
<title>Pseudo-Bag Mixup Augmentation for Multiple Instance Learning-Based Whole Slide Image Classification. (arXiv:2306.16180v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16180</link>
<description rdf:parseType="Literal">&lt;p&gt;Given the special situation of modeling gigapixel images, multiple instance
learning (MIL) has become one of the most important frameworks for Whole Slide
Image (WSI) classification. In current practice, most MIL networks often face
two unavoidable problems in training: i) insufficient WSI data and ii) the
sample memorization inclination inherent in neural networks. These problems may
hinder MIL models from adequate and efficient training, suppressing the
continuous performance promotion of classification models on WSIs. Inspired by
the basic idea of Mixup, this paper proposes a new Pseudo-bag Mixup (PseMix)
data augmentation scheme to improve the training of MIL models. This scheme
generalizes the Mixup strategy for general images to special WSIs via
pseudo-bags so as to be applied in MIL-based WSI classification. Cooperated by
pseudo-bags, our PseMix fulfills the critical size alignment and semantic
alignment in Mixup strategy. Moreover, it is designed as an efficient and
decoupled method, neither involving time-consuming operations nor relying on
MIL model predictions. Comparative experiments and ablation studies are
specially designed to evaluate the effectiveness and advantages of our PseMix.
Experimental results show that PseMix could often assist state-of-the-art MIL
networks to refresh their classification performance on WSIs. Besides, it could
also boost the generalization performance of MIL models in special test
scenarios, and promote their robustness to patch occlusion and label noise. Our
source code is available at https://github.com/liupei101/PseMix.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1&quot;&gt;Pei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1&quot;&gt;Luping Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1&quot;&gt;Feng Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05973">
<title>VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models. (arXiv:2307.05973v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05973</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) are shown to possess a wealth of actionable
knowledge that can be extracted for robot manipulation in the form of reasoning
and planning. Despite the progress, most still rely on pre-defined motion
primitives to carry out the physical interactions with the environment, which
remains a major bottleneck. In this work, we aim to synthesize robot
trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a
large variety of manipulation tasks given an open-set of instructions and an
open-set of objects. We achieve this by first observing that LLMs excel at
inferring affordances and constraints given a free-form language instruction.
More importantly, by leveraging their code-writing capabilities, they can
interact with a vision-language model (VLM) to compose 3D value maps to ground
the knowledge into the observation space of the agent. The composed value maps
are then used in a model-based planning framework to zero-shot synthesize
closed-loop robot trajectories with robustness to dynamic perturbations. We
further demonstrate how the proposed framework can benefit from online
experiences by efficiently learning a dynamics model for scenes that involve
contact-rich interactions. We present a large-scale study of the proposed
method in both simulated and real-robot environments, showcasing the ability to
perform a large variety of everyday manipulation tasks specified in free-form
natural language. Videos and code at https://voxposer.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wenlong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruohan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunzhu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiajun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1&quot;&gt;Li Fei-Fei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07439">
<title>Atlas-Based Interpretable Age Prediction In Whole-Body MR Images. (arXiv:2307.07439v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07439</link>
<description rdf:parseType="Literal">&lt;p&gt;Age prediction is an important part of medical assessments and research. It
can aid in detecting diseases as well as abnormal ageing by highlighting the
discrepancy between chronological and biological age. To gain a comprehensive
understanding of age-related changes observed in various body parts, we
investigate them on a larger scale by using whole-body 3D images. We utilise
the Grad-CAM interpretability method to determine the body areas most
predictive of a person&apos;s age. We expand our analysis beyond individual subjects
by employing registration techniques to generate population-wide
interpretability maps. Our findings reveal three primary areas of interest: the
spine, the autochthonous back muscles, and the cardiac region, which exhibits
the highest importance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Starck_S/0/1/0/all/0/1&quot;&gt;Sophie Starck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kini_Y/0/1/0/all/0/1&quot;&gt;Yadunandan Vivekanand Kini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ritter_J/0/1/0/all/0/1&quot;&gt;Jessica Johanna Maria Ritter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Braren_R/0/1/0/all/0/1&quot;&gt;Rickmer Braren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1&quot;&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mueller_T/0/1/0/all/0/1&quot;&gt;Tamara Mueller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10829">
<title>Exact Diffusion Inversion via Bi-directional Integration Approximation. (arXiv:2307.10829v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10829</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, various methods have been proposed to address the inconsistency
issue of DDIM inversion to enable image editing, such as EDICT [36] and
Null-text inversion [22]. However, the above methods introduce considerable
computational overhead. In this paper, we propose a new technique, named
\emph{bi-directional integration approximation} (BDIA), to perform exact
diffusion inversion with neglible computational overhead. Suppose we would like
to estimate the next diffusion state $\boldsymbol{z}_{i-1}$ at timestep $t_i$
with the historical information $(i,\boldsymbol{z}_i)$ and
$(i+1,\boldsymbol{z}_{i+1})$. We first obtain the estimated Gaussian noise
$\hat{\boldsymbol{\epsilon}}(\boldsymbol{z}_i,i)$, and then apply the DDIM
update procedure twice for approximating the ODE integration over the next
time-slot $[t_i, t_{i-1}]$ in the forward manner and the previous time-slot
$[t_i, t_{t+1}]$ in the backward manner. The DDIM step for the previous
time-slot is used to refine the integration approximation made earlier when
computing $\boldsymbol{z}_i$. A nice property of BDIA-DDIM is that the update
expression for $\boldsymbol{z}_{i-1}$ is a linear combination of
$(\boldsymbol{z}_{i+1}, \boldsymbol{z}_i,
\hat{\boldsymbol{\epsilon}}(\boldsymbol{z}_i,i))$. This allows for exact
backward computation of $\boldsymbol{z}_{i+1}$ given $(\boldsymbol{z}_i,
\boldsymbol{z}_{i-1})$, thus leading to exact diffusion inversion. It is
demonstrated with experiments that (round-trip) BDIA-DDIM is particularly
effective for image editing. Our experiments further show that BDIA-DDIM
produces markedly better image sampling qualities than DDIM for text-to-image
generation.
&lt;/p&gt;
&lt;p&gt;BDIA can also be applied to improve the performance of other ODE solvers in
addition to DDIM. In our work, it is found that applying BDIA to the EDM
sampling procedure produces consistently better performance over four
pre-trained models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guoqiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lewis_J/0/1/0/all/0/1&quot;&gt;J. P. Lewis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleijn_W/0/1/0/all/0/1&quot;&gt;W. Bastiaan Kleijn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10943">
<title>Proxy Anchor-based Unsupervised Learning for Continuous Generalized Category Discovery. (arXiv:2307.10943v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10943</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in deep learning have significantly improved the performance
of various computer vision applications. However, discovering novel categories
in an incremental learning scenario remains a challenging problem due to the
lack of prior knowledge about the number and nature of new categories. Existing
methods for novel category discovery are limited by their reliance on labeled
datasets and prior knowledge about the number of novel categories and the
proportion of novel samples in the batch. To address the limitations and more
accurately reflect real-world scenarios, in this paper, we propose a novel
unsupervised class incremental learning approach for discovering novel
categories on unlabeled sets without prior knowledge. The proposed method
fine-tunes the feature extractor and proxy anchors on labeled sets, then splits
samples into old and novel categories and clusters on the unlabeled dataset.
Furthermore, the proxy anchors-based exemplar generates representative category
vectors to mitigate catastrophic forgetting. Experimental results demonstrate
that our proposed approach outperforms the state-of-the-art methods on
fine-grained datasets under real-world scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyungmin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suh_S/0/1/0/all/0/1&quot;&gt;Sungho Suh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Daehwan Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_D/0/1/0/all/0/1&quot;&gt;Daun Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1&quot;&gt;Hansang Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Junmo Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03900">
<title>Developability Approximation for Neural Implicits through Rank Minimization. (arXiv:2308.03900v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.03900</link>
<description rdf:parseType="Literal">&lt;p&gt;Developability refers to the process of creating a surface without any
tearing or shearing from a two-dimensional plane. It finds practical
applications in the fabrication industry. An essential characteristic of a
developable 3D surface is its zero Gaussian curvature, which means that either
one or both of the principal curvatures are zero. This paper introduces a
method for reconstructing an approximate developable surface from a neural
implicit surface. The central idea of our method involves incorporating a
regularization term that operates on the second-order derivatives of the neural
implicits, effectively promoting zero Gaussian curvature. Implicit surfaces
offer the advantage of smoother deformation with infinite resolution,
overcoming the high polygonal constraints of state-of-the-art methods using
discrete representations. We draw inspiration from the properties of surface
curvature and employ rank minimization techniques derived from compressed
sensing. Experimental results on both developable and non-developable surfaces,
including those affected by noise, validate the generalizability of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Selvaraju_P/0/1/0/all/0/1&quot;&gt;Pratheba Selvaraju&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.06534">
<title>Self-Supervised Pre-Training with Contrastive and Masked Autoencoder Methods for Dealing with Small Datasets in Deep Learning for Medical Imaging. (arXiv:2308.06534v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.06534</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning in medical imaging has the potential to minimize the risk of
diagnostic errors, reduce radiologist workload, and accelerate diagnosis.
Training such deep learning models requires large and accurate datasets, with
annotations for all training samples. However, in the medical imaging domain,
annotated datasets for specific tasks are often small due to the high
complexity of annotations, limited access, or the rarity of diseases. To
address this challenge, deep learning models can be pre-trained on large image
datasets without annotations using methods from the field of self-supervised
learning. After pre-training, small annotated datasets are sufficient to
fine-tune the models for a specific task. The most popular self-supervised
pre-training approaches in medical imaging are based on contrastive learning.
However, recent studies in natural image processing indicate a strong potential
for masked autoencoder approaches. Our work compares state-of-the-art
contrastive learning methods with the recently introduced masked autoencoder
approach &quot;SparK&quot; for convolutional neural networks (CNNs) on medical images.
Therefore we pre-train on a large unannotated CT image dataset and fine-tune on
several CT classification tasks. Due to the challenge of obtaining sufficient
annotated training data in medical imaging, it is of particular interest to
evaluate how the self-supervised pre-training methods perform when fine-tuning
on small datasets. By experimenting with gradually reducing the training
dataset size for fine-tuning, we find that the reduction has different effects
depending on the type of pre-training chosen. The SparK pre-training method is
more robust to the training dataset size than the contrastive methods. Based on
our results, we propose the SparK pre-training for medical imaging tasks with
only small annotated datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolf_D/0/1/0/all/0/1&quot;&gt;Daniel Wolf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Payer_T/0/1/0/all/0/1&quot;&gt;Tristan Payer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lisson_C/0/1/0/all/0/1&quot;&gt;Catharina Silvia Lisson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lisson_C/0/1/0/all/0/1&quot;&gt;Christoph Gerhard Lisson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beer_M/0/1/0/all/0/1&quot;&gt;Meinrad Beer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gotz_M/0/1/0/all/0/1&quot;&gt;Michael G&amp;#xf6;tz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ropinski_T/0/1/0/all/0/1&quot;&gt;Timo Ropinski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16741">
<title>Socratis: Are large multimodal models emotionally aware?. (arXiv:2308.16741v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.16741</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing emotion prediction benchmarks contain coarse emotion labels which do
not consider the diversity of emotions that an image and text can elicit in
humans due to various reasons. Learning diverse reactions to multimodal content
is important as intelligent machines take a central role in generating and
delivering content to society. To address this gap, we propose Socratis, a
societal reactions benchmark, where each image-caption (IC) pair is annotated
with multiple emotions and the reasons for feeling them. Socratis contains 18K
free-form reactions for 980 emotions on 2075 image-caption pairs from 5
widely-read news and image-caption (IC) datasets. We benchmark the capability
of state-of-the-art multimodal large language models to generate the reasons
for feeling an emotion given an IC pair. Based on a preliminary human study, we
observe that humans prefer human-written reasons over 2 times more often than
machine-generated ones. This shows our task is harder than standard generation
tasks because it starkly contrasts recent findings where humans cannot tell
apart machine vs human-written news articles, for instance. We further see that
current captioning metrics based on large vision-language models also fail to
correlate with human preferences. We hope that these findings and our benchmark
will inspire further research on training emotionally aware models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_K/0/1/0/all/0/1&quot;&gt;Katherine Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1&quot;&gt;Arijit Ray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_R/0/1/0/all/0/1&quot;&gt;Reuben Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gabriel_S/0/1/0/all/0/1&quot;&gt;Saadia Gabriel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plummer_B/0/1/0/all/0/1&quot;&gt;Bryan A. Plummer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1&quot;&gt;Kate Saenko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.00174">
<title>Typing on Any Surface: A Deep Learning-based Method for Real-Time Keystroke Detection in Augmented Reality. (arXiv:2309.00174v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.00174</link>
<description rdf:parseType="Literal">&lt;p&gt;Frustrating text entry interface has been a major obstacle in participating
in social activities in augmented reality (AR). Popular options, such as
mid-air keyboard interface, wireless keyboards or voice input, either suffer
from poor ergonomic design, limited accuracy, or are simply embarrassing to use
in public. This paper proposes and validates a deep-learning based approach,
that enables AR applications to accurately predict keystrokes from the user
perspective RGB video stream that can be captured by any AR headset. This
enables a user to perform typing activities on any flat surface and eliminates
the need of a physical or virtual keyboard. A two-stage model, combing an
off-the-shelf hand landmark extractor and a novel adaptive Convolutional
Recurrent Neural Network (C-RNN), was trained using our newly built dataset.
The final model was capable of adaptive processing user-perspective video
streams at ~32 FPS. This base model achieved an overall accuracy of $91.05\%$
when typing 40 Words per Minute (wpm), which is how fast an average person
types with two hands on a physical keyboard. The Normalised Levenshtein
Distance also further confirmed the real-world applicability of that our
approach. The promising results highlight the viability of our approach and the
potential for our method to be integrated into various applications. We also
discussed the limitations and future research required to bring such technique
into a production system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1&quot;&gt;Xingyu Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xi_M/0/1/0/all/0/1&quot;&gt;Mingze Xi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.00733">
<title>Learned Visual Features to Textual Explanations. (arXiv:2309.00733v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.00733</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpreting the learned features of vision models has posed a longstanding
challenge in the field of machine learning. To address this issue, we propose a
novel method that leverages the capabilities of large language models (LLMs) to
interpret the learned features of pre-trained image classifiers. Our method,
called TExplain, tackles this task by training a neural network to establish a
connection between the feature space of image classifiers and LLMs. Then,
during inference, our approach generates a vast number of sentences to explain
the features learned by the classifier for a given image. These sentences are
then used to extract the most frequent words, providing a comprehensive
understanding of the learned features and patterns within the classifier. Our
method, for the first time, utilizes these frequent words corresponding to a
visual representation to provide insights into the decision-making process of
the independently trained classifier, enabling the detection of spurious
correlations, biases, and a deeper comprehension of its behavior. To validate
the effectiveness of our approach, we conduct experiments on diverse datasets,
including ImageNet-9L and Waterbirds. The results demonstrate the potential of
our method to enhance the interpretability and robustness of image classifiers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taghanaki_S/0/1/0/all/0/1&quot;&gt;Saeid Asgari Taghanaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khani_A/0/1/0/all/0/1&quot;&gt;Aliasghar Khani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khasahmadi_A/0/1/0/all/0/1&quot;&gt;Amir Khasahmadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanghi_A/0/1/0/all/0/1&quot;&gt;Aditya Sanghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Willis_K/0/1/0/all/0/1&quot;&gt;Karl D.D. Willis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahdavi_Amiri_A/0/1/0/all/0/1&quot;&gt;Ali Mahdavi-Amiri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.01859">
<title>NLLB-CLIP -- train performant multilingual image retrieval model on a budget. (arXiv:2309.01859v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.01859</link>
<description rdf:parseType="Literal">&lt;p&gt;Today, the exponential rise of large models developed by academic and
industrial institutions with the help of massive computing resources raises the
question of whether someone without access to such resources can make a
valuable scientific contribution. To explore this, we tried to solve the
challenging task of multilingual image retrieval having a limited budget of
$1,000. As a result, we present NLLB-CLIP - CLIP model with a text encoder from
the NLLB model. To train the model, we used an automatically created dataset of
106,246 good-quality images with captions in 201 languages derived from the
LAION COCO dataset. We trained multiple models using image and text encoders of
various sizes and kept different parts of the model frozen during the training.
We thoroughly analyzed the trained models using existing evaluation datasets
and newly created XTD200 and Flickr30k-200 datasets. We show that NLLB-CLIP is
comparable in quality to state-of-the-art models and significantly outperforms
them on low-resource languages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Visheratin_A/0/1/0/all/0/1&quot;&gt;Alexander Visheratin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.05090">
<title>Sculpting Efficiency: Pruning Medical Imaging Models for On-Device Inference. (arXiv:2309.05090v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.05090</link>
<description rdf:parseType="Literal">&lt;p&gt;Leveraging ML advancements to augment healthcare systems can improve patient
outcomes. Yet, uninformed engineering decisions in early-stage research
inadvertently hinder the feasibility of such solutions for high-throughput,
on-device inference, particularly in settings involving legacy hardware and
multi-modal gigapixel images. Through a preliminary case study concerning
segmentation in cardiology, we highlight the excess operational complexity in a
suboptimally configured ML model from prior work and demonstrate that it can be
sculpted away using pruning to meet deployment criteria. Our results show a
compression rate of 1148x with minimal loss in quality (~4%) and, at higher
rates, achieve faster inference on a CPU than the GPU baseline, stressing the
need to consider task complexity and architectural details when using
off-the-shelf models. With this, we consider avenues for future research in
streamlining workflows for clinical researchers to develop models quicker and
better suited for real-world use.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sreeram_S/0/1/0/all/0/1&quot;&gt;Sudarshan Sreeram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kainz_B/0/1/0/all/0/1&quot;&gt;Bernhard Kainz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13744">
<title>A Systematic Literature Review of Computer Vision Applications in Robotized Wire Harness Assembly. (arXiv:2309.13744v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.13744</link>
<description rdf:parseType="Literal">&lt;p&gt;This article presents a systematic literature review on computer vision
applications that have been proposed for robotized wire harness assembly,
derives challenges from existing studies, and identifies opportunities for
future research to promote a more practical robotized assembly of wire
harnesses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salunkhe_O/0/1/0/all/0/1&quot;&gt;Omkar Salunkhe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quadrini_W/0/1/0/all/0/1&quot;&gt;Walter Quadrini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johansson_B/0/1/0/all/0/1&quot;&gt;Bj&amp;#xf6;rn Johansson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lamkull_D/0/1/0/all/0/1&quot;&gt;Dan L&amp;#xe4;mkull&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ore_F/0/1/0/all/0/1&quot;&gt;Fredrik Ore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Despeisse_M/0/1/0/all/0/1&quot;&gt;M&amp;#xe9;lanie Despeisse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fumagalli_L/0/1/0/all/0/1&quot;&gt;Luca Fumagalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stahre_J/0/1/0/all/0/1&quot;&gt;Johan Stahre&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13745">
<title>Computer Vision Technology for Robotized Wire Harness Assembly. (arXiv:2309.13745v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2309.13745</link>
<description rdf:parseType="Literal">&lt;p&gt;Wire harnesses are essential hardware for electronic systems in modern
automotive vehicles. With a shift in the automotive industry towards
electrification and autonomous driving, more and more automotive electronics
are responsible for energy transmission and safety-critical functions such as
maneuvering, driver assistance, and safety system. This paradigm shift places
more demand on automotive wiring harnesses from the safety perspective and
stresses the greater importance of high-quality wire harness assembly in
vehicles. However, most of the current operations of wire harness assembly are
still performed manually by skilled workers, and some of the manual processes
are problematic from different perspectives, such as quality control and
ergonomics. There is also a persistent demand in the industry to increase
competitiveness and gain market share. Hence, assuring assembly quality while
improving ergonomics and optimizing labor costs is desired. Robotized assembly,
accomplished by robots or in human-robot collaboration, is a key enabler for
fulfilling the increasingly demanding quality and safety as it enables more
replicable, transparent, and comprehensible processes than completely manual
operations. However, robotized assembly of wire harnesses is challenging in
real environments due to the flexibility of the deformable objects, though many
preliminary automation solutions have been proposed under simplified industrial
configurations. Previous research efforts have proposed the use of computer
vision technology to facilitate robotized automation of wire harness assembly,
enabling the robots to better perceive and manipulate the flexible wire
harness. This article presents an overview on computer vision technology
proposed for robotized wire harness assembly and derives research gaps that
require further study to facilitate a more practical robotized assembly of wire
harness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salunkhe_O/0/1/0/all/0/1&quot;&gt;Omkar Salunkhe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quadrini_W/0/1/0/all/0/1&quot;&gt;Walter Quadrini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lamkull_D/0/1/0/all/0/1&quot;&gt;Dan L&amp;#xe4;mkull&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ore_F/0/1/0/all/0/1&quot;&gt;Fredrik Ore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johansson_B/0/1/0/all/0/1&quot;&gt;Bj&amp;#xf6;rn Johansson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stahre_J/0/1/0/all/0/1&quot;&gt;Johan Stahre&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01641">
<title>You Only Look at Once for Real-time and Generic Multi-Task. (arXiv:2310.01641v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01641</link>
<description rdf:parseType="Literal">&lt;p&gt;High precision, lightweight, and real-time responsiveness are three essential
requirements for implementing autonomous driving. In this study, we present an
adaptive, real-time, and lightweight multi-task model designed to concurrently
address object detection, drivable area segmentation, and lane line
segmentation tasks. Specifically, we developed an end-to-end multi-task model
with a unified and streamlined segmentation structure. We introduced a
learnable parameter that adaptively concatenate features in segmentation necks,
using the same loss function for all segmentation tasks. This eliminates the
need for customizations and enhances the model&apos;s generalization capabilities.
We also introduced a segmentation head composed only of a series of
convolutional layers, which reduces the inference time. We achieved competitive
results on the BDD100k dataset, particularly in visualization outcomes. The
performance results show a mAP50 of 81.1% for object detection, a mIoU of 91.0%
for drivable area segmentation, and an IoU of 28.8% for lane line segmentation.
Additionally, we introduced real-world scenarios to evaluate our model&apos;s
performance in a real scene, which significantly outperforms competitors. This
demonstrates that our model not only exhibits competitive performance but is
also more flexible and faster than existing multi-task models. The source codes
and pre-trained models are released at
https://github.com/JiayuanWang-JW/YOLOv8-multi-task
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiayuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Q. M. Jonathan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Ning Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02230">
<title>Leveraging Diffusion Disentangled Representations to Mitigate Shortcuts in Underspecified Visual Tasks. (arXiv:2310.02230v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02230</link>
<description rdf:parseType="Literal">&lt;p&gt;Spurious correlations in the data, where multiple cues are predictive of the
target labels, often lead to shortcut learning phenomena, where a model may
rely on erroneous, easy-to-learn, cues while ignoring reliable ones. In this
work, we propose an ensemble diversification framework exploiting the
generation of synthetic counterfactuals using Diffusion Probabilistic Models
(DPMs). We discover that DPMs have the inherent capability to represent
multiple visual cues independently, even when they are largely correlated in
the training data. We leverage this characteristic to encourage model diversity
and empirically show the efficacy of the approach with respect to several
diversification objectives. We show that diffusion-guided diversification can
lead models to avert attention from shortcut cues, achieving ensemble diversity
performance comparable to previous methods requiring additional data
collection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scimeca_L/0/1/0/all/0/1&quot;&gt;Luca Scimeca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rubinstein_A/0/1/0/all/0/1&quot;&gt;Alexander Rubinstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nicolicioiu_A/0/1/0/all/0/1&quot;&gt;Armand Mihai Nicolicioiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teney_D/0/1/0/all/0/1&quot;&gt;Damien Teney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.06403">
<title>Boundary Discretization and Reliable Classification Network for Temporal Action Detection. (arXiv:2310.06403v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.06403</link>
<description rdf:parseType="Literal">&lt;p&gt;Temporal action detection aims to recognize the action category and determine
the starting and ending time of each action instance in untrimmed videos. The
mixed methods have achieved remarkable performance by simply merging
anchor-based and anchor-free approaches. However, there are still two crucial
issues in the mixed framework: (1) Brute-force merging and handcrafted anchors
design affect the performance and practical application of the mixed methods.
(2) A large number of false positives in action category predictions further
impact the detection performance. In this paper, we propose a novel Boundary
Discretization and Reliable Classification Network (BDRC-Net) that addresses
the above issues by introducing boundary discretization and reliable
classification modules. Specifically, the boundary discretization module (BDM)
elegantly merges anchor-based and anchor-free approaches in the form of
boundary discretization, avoiding the handcrafted anchors design required by
traditional mixed methods. Furthermore, the reliable classification module
(RCM) predicts reliable action categories to reduce false positives in action
category predictions. Extensive experiments conducted on different benchmarks
demonstrate that our proposed method achieves favorable performance compared
with the state-of-the-art. For example, BDRC-Net hits an average mAP of 68.6%
on THUMOS&apos;14, outperforming the previous best by 1.5%. The code will be
released at https://github.com/zhenyingfang/BDRC-Net.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1&quot;&gt;Zhenying Fang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17403">
<title>Detection Defenses: An Empty Promise against Adversarial Patch Attacks on Optical Flow. (arXiv:2310.17403v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.17403</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial patches undermine the reliability of optical flow predictions
when placed in arbitrary scene locations. Therefore, they pose a realistic
threat to real-world motion detection and its downstream applications.
Potential remedies are defense strategies that detect and remove adversarial
patches, but their influence on the underlying motion prediction has not been
investigated. In this paper, we thoroughly examine the currently available
detect-and-remove defenses ILP and LGS for a wide selection of state-of-the-art
optical flow methods, and illuminate their side effects on the quality and
robustness of the final flow predictions. In particular, we implement
defense-aware attacks to investigate whether current defenses are able to
withstand attacks that take the defense mechanism into account. Our experiments
yield two surprising results: Detect-and-remove defenses do not only lower the
optical flow quality on benign scenes, in doing so, they also harm the
robustness under patch attacks for all tested optical flow methods except
FlowNetC. As currently employed detect-and-remove defenses fail to deliver the
promised adversarial robustness for optical flow, they evoke a false sense of
security. The code is available at
https://github.com/cv-stuttgart/DetectionDefenses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scheurer_E/0/1/0/all/0/1&quot;&gt;Erik Scheurer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmalfuss_J/0/1/0/all/0/1&quot;&gt;Jenny Schmalfuss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lis_A/0/1/0/all/0/1&quot;&gt;Alexander Lis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bruhn_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9;s Bruhn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18348">
<title>Meaning Representations from Trajectories in Autoregressive Models. (arXiv:2310.18348v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18348</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose to extract meaning representations from autoregressive language
models by considering the distribution of all possible trajectories extending
an input text. This strategy is prompt-free, does not require fine-tuning, and
is applicable to any pre-trained autoregressive model. Moreover, unlike
vector-based representations, distribution-based representations can also model
asymmetric relations (e.g., direction of logical entailment, hypernym/hyponym
relations) by using algebraic operations between likelihood functions. These
ideas are grounded in distributional perspectives on semantics and are
connected to standard constructions in automata theory, but to our knowledge
they have not been applied to modern language models. We empirically show that
the representations obtained from large models align well with human
annotations, outperform other zero-shot and prompt-free methods on semantic
similarity tasks, and can be used to solve more complex entailment and
containment tasks that standard embeddings cannot handle. Finally, we extend
our method to represent data from different modalities (e.g., image and text)
using multimodal autoregressive models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tian Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trager_M/0/1/0/all/0/1&quot;&gt;Matthew Trager&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1&quot;&gt;Alessandro Achille&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perera_P/0/1/0/all/0/1&quot;&gt;Pramuditha Perera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zancato_L/0/1/0/all/0/1&quot;&gt;Luca Zancato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1&quot;&gt;Stefano Soatto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19559">
<title>Disentangled Counterfactual Learning for Physical Audiovisual Commonsense Reasoning. (arXiv:2310.19559v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19559</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a Disentangled Counterfactual Learning~(DCL)
approach for physical audiovisual commonsense reasoning. The task aims to infer
objects&apos; physics commonsense based on both video and audio input, with the main
challenge is how to imitate the reasoning ability of humans. Most of the
current methods fail to take full advantage of different characteristics in
multi-modal data, and lacking causal reasoning ability in models impedes the
progress of implicit physical knowledge inferring. To address these issues, our
proposed DCL method decouples videos into static (time-invariant) and dynamic
(time-varying) factors in the latent space by the disentangled sequential
encoder, which adopts a variational autoencoder (VAE) to maximize the mutual
information with a contrastive loss function. Furthermore, we introduce a
counterfactual learning module to augment the model&apos;s reasoning ability by
modeling physical knowledge relationships among different objects under
counterfactual intervention. Our proposed method is a plug-and-play module that
can be incorporated into any baseline. In experiments, we show that our
proposed method improves baseline methods and achieves state-of-the-art
performance. Our source code is available at https://github.com/Andy20178/DCL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1&quot;&gt;Changsheng Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shuai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yapeng Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_M/0/1/0/all/0/1&quot;&gt;Mengshi Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Huadong Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00277">
<title>OpenForest: A data catalogue for machine learning in forest monitoring. (arXiv:2311.00277v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00277</link>
<description rdf:parseType="Literal">&lt;p&gt;Forests play a crucial role in Earth&apos;s system processes and provide a suite
of social and economic ecosystem services, but are significantly impacted by
human activities, leading to a pronounced disruption of the equilibrium within
ecosystems. Advancing forest monitoring worldwide offers advantages in
mitigating human impacts and enhancing our comprehension of forest composition,
alongside the effects of climate change. While statistical modeling has
traditionally found applications in forest biology, recent strides in machine
learning and computer vision have reached important milestones using remote
sensing data, such as tree species identification, tree crown segmentation and
forest biomass assessments. For this, the significance of open access data
remains essential in enhancing such data-driven algorithms and methodologies.
Here, we provide a comprehensive and extensive overview of 86 open access
forest datasets across spatial scales, encompassing inventories, ground-based,
aerial-based, satellite-based recordings, and country or world maps. These
datasets are grouped in OpenForest, a dynamic catalogue open to contributions
that strives to reference all available open access forest datasets. Moreover,
in the context of these datasets, we aim to inspire research in machine
learning applied to forest biology by establishing connections between
contemporary topics, perspectives and challenges inherent in both domains. We
hope to encourage collaborations among scientists, fostering the sharing and
exploration of diverse datasets through the application of machine learning
methods for large-scale forest monitoring. OpenForest is available at
https://github.com/RolnickLab/OpenForest .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouaknine_A/0/1/0/all/0/1&quot;&gt;Arthur Ouaknine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kattenborn_T/0/1/0/all/0/1&quot;&gt;Teja Kattenborn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laliberte_E/0/1/0/all/0/1&quot;&gt;Etienne Lalibert&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rolnick_D/0/1/0/all/0/1&quot;&gt;David Rolnick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00489">
<title>Deep Neural Networks for Automatic Speaker Recognition Do Not Learn Supra-Segmental Temporal Features. (arXiv:2311.00489v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00489</link>
<description rdf:parseType="Literal">&lt;p&gt;While deep neural networks have shown impressive results in automatic speaker
recognition and related tasks, it is dissatisfactory how little is understood
about what exactly is responsible for these results. Part of the success has
been attributed in prior work to their capability to model supra-segmental
temporal information (SST), i.e., learn rhythmic-prosodic characteristics of
speech in addition to spectral features. In this paper, we (i) present and
apply a novel test to quantify to what extent the performance of
state-of-the-art neural networks for speaker recognition can be explained by
modeling SST; and (ii) present several means to force respective nets to focus
more on SST and evaluate their merits. We find that a variety of CNN- and
RNN-based neural network architectures for speaker recognition do not model SST
to any sufficient degree, even when forced. The results provide a highly
relevant basis for impactful future research into better exploitation of the
full speech signal and give insights into the inner workings of such networks,
enhancing explainability of deep learning for speech technologies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neururer_D/0/1/0/all/0/1&quot;&gt;Daniel Neururer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dellwo_V/0/1/0/all/0/1&quot;&gt;Volker Dellwo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stadelmann_T/0/1/0/all/0/1&quot;&gt;Thilo Stadelmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00548">
<title>Continual atlas-based segmentation of prostate MRI. (arXiv:2311.00548v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00548</link>
<description rdf:parseType="Literal">&lt;p&gt;Continual learning (CL) methods designed for natural image classification
often fail to reach basic quality standards for medical image segmentation.
Atlas-based segmentation, a well-established approach in medical imaging,
incorporates domain knowledge on the region of interest, leading to
semantically coherent predictions. This is especially promising for CL, as it
allows us to leverage structural information and strike an optimal balance
between model rigidity and plasticity over time. When combined with
privacy-preserving prototypes, this process offers the advantages of
rehearsal-based CL without compromising patient privacy. We propose Atlas
Replay, an atlas-based segmentation approach that uses prototypes to generate
high-quality segmentation masks through image registration that maintain
consistency even as the training distribution changes. We explore how our
proposed method performs compared to state-of-the-art CL methods in terms of
knowledge transferability across seven publicly available prostate segmentation
datasets. Prostate segmentation plays a vital role in diagnosing prostate
cancer, however, it poses challenges due to substantial anatomical variations,
benign structural differences in older age groups, and fluctuating acquisition
parameters. Our results show that Atlas Replay is both robust and generalizes
well to yet-unseen domains while being able to maintain knowledge, unlike
end-to-end segmentation methods. Our code base is available under
https://github.com/MECLabTUDA/Atlas-Replay.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ranem_A/0/1/0/all/0/1&quot;&gt;Amin Ranem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_C/0/1/0/all/0/1&quot;&gt;Camila Gonz&amp;#xe1;lez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_D/0/1/0/all/0/1&quot;&gt;Daniel Pinto dos Santos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bucher_A/0/1/0/all/0/1&quot;&gt;Andreas Michael Bucher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Othman_A/0/1/0/all/0/1&quot;&gt;Ahmed Ezzat Othman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukhopadhyay_A/0/1/0/all/0/1&quot;&gt;Anirban Mukhopadhyay&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>