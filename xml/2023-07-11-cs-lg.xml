<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.LG updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Machine Learning (cs.LG) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-10T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03756" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03758" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03759" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03761" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03764" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03783" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03798" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03803" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03810" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03811" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03813" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03816" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03827" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03838" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03848" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03854" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03860" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03863" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03864" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03872" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03875" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03884" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03886" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03887" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03899" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03902" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03906" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03908" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03920" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03927" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03929" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03930" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03945" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03980" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03997" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04001" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04010" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04011" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04024" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04028" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04033" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04036" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04042" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04049" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04050" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04052" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04055" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04056" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04057" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04065" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04075" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04077" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04081" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04090" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04093" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04094" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04097" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04099" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04102" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04105" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04110" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04114" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04121" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04131" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04138" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04147" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04149" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04191" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04204" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04205" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04208" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04210" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04215" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04216" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04228" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04251" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04274" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04287" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04298" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04305" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1908.08016" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2007.07302" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2011.12177" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2012.03774" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2102.04363" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2102.07060" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2103.03943" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2104.12031" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2105.04738" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2107.13136" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2109.10476" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.03360" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.05854" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.12468" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2111.09266" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2111.14778" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.03626" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.07344" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.05527" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.07286" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.11931" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.11936" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.05767" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.02372" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.08620" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.10268" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.02364" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.07932" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.13679" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.14568" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.04530" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.04857" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.08516" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.10477" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.12252" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.06154" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.11719" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.01996" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.06536" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.10264" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.10967" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.02370" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.02905" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.04547" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.04562" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.13513" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.14233" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.14249" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.01081" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.04653" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.09809" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.13573" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.16189" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.16575" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.00172" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.02222" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.04284" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.10805" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.12316" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.13291" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.14960" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.15657" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.06951" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.14106" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.01828" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.07733" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.12068" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.00160" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.01018" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.02766" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.07419" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.08332" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.10325" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.13485" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.14686" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.00501" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.01483" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.02719" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.04865" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.06010" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.07308" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.10834" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.11582" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.14773" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.16897" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.17607" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.00215" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.03452" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.04000" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.10207" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.14251" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.01520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.06569" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.09028" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.09235" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13399" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16044" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17033" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18404" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18451" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.01505" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.02312" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.02913" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.03718" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04139" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04919" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05987" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09417" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10006" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11197" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11342" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14435" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16906" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16926" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17066" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17181" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00534" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01689" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01750" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02198" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02245" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02329" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02903" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03393" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03493" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03577" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00457" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01226" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02889" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03056" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.03756">
<title>FITS: Modeling Time Series with $10k$ Parameters. (arXiv:2307.03756v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03756</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce FITS, a lightweight yet powerful model for time
series analysis. Unlike existing models that directly process raw time-domain
data, FITS operates on the principle that time series can be manipulated
through interpolation in the complex frequency domain. By discarding
high-frequency components with negligible impact on time series data, FITS
achieves performance comparable to state-of-the-art models for time series
forecasting and anomaly detection tasks, while having a remarkably compact size
of only approximately $10k$ parameters. Such a lightweight model can be easily
trained and deployed in edge devices, creating opportunities for various
applications. The anonymous code repo is available in:
\url{https://anonymous.4open.science/r/FITS}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhijian Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1&quot;&gt;Ailing Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qiang Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03758">
<title>Federated Learning over a Wireless Network: Distributed User Selection through Random Access. (arXiv:2307.03758v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03758</link>
<description rdf:parseType="Literal">&lt;p&gt;User selection has become crucial for decreasing the communication costs of
federated learning (FL) over wireless networks. However, centralized user
selection causes additional system complexity. This study proposes a network
intrinsic approach of distributed user selection that leverages the radio
resource competition mechanism in random access. Taking the carrier sensing
multiple access (CSMA) mechanism as an example of random access, we manipulate
the contention window (CW) size to prioritize certain users for obtaining radio
resources in each round of training. Training data bias is used as a target
scenario for FL with user selection. Prioritization is based on the distance
between the newly trained local model and the global model of the previous
round. To avoid excessive contribution by certain users, a counting mechanism
is used to ensure fairness. Simulations with various datasets demonstrate that
this method can rapidly achieve convergence similar to that of the centralized
user selection approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1&quot;&gt;Chen Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Shiyao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Ce Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Songtao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_T/0/1/0/all/0/1&quot;&gt;Tao Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1&quot;&gt;Lingjuan Lyu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03759">
<title>A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection. (arXiv:2307.03759v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03759</link>
<description rdf:parseType="Literal">&lt;p&gt;Time series are the primary data type used to record dynamic system
measurements and generated in great volume by both physical sensors and online
processes (virtual sensors). Time series analytics is therefore crucial to
unlocking the wealth of information implicit in available data. With the recent
advancements in graph neural networks (GNNs), there has been a surge in
GNN-based approaches for time series analysis. Approaches can explicitly model
inter-temporal and inter-variable relationships, which traditional and other
deep neural network-based methods struggle to do. In this survey, we provide a
comprehensive review of graph neural networks for time series analysis
(GNN4TS), encompassing four fundamental dimensions: Forecasting,
classification, anomaly detection, and imputation. Our aim is to guide
designers and practitioners to understand, build applications, and advance
research of GNN4TS. At first, we provide a comprehensive task-oriented taxonomy
of GNN4TS. Then, we present and discuss representative research works and,
finally, discuss mainstream applications of GNN4TS. A comprehensive discussion
of potential future research directions completes the survey. This survey, for
the first time, brings together a vast array of knowledge on GNN-based time
series research, highlighting both the foundations, practical applications, and
opportunities of graph neural networks for time series analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1&quot;&gt;Ming Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koh_H/0/1/0/all/0/1&quot;&gt;Huan Yee Koh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Q/0/1/0/all/0/1&quot;&gt;Qingsong Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zambon_D/0/1/0/all/0/1&quot;&gt;Daniele Zambon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alippi_C/0/1/0/all/0/1&quot;&gt;Cesare Alippi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Webb_G/0/1/0/all/0/1&quot;&gt;Geoffrey I. Webb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1&quot;&gt;Irwin King&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1&quot;&gt;Shirui Pan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03761">
<title>Dynamic Graph Attention for Anomaly Detection in Heterogeneous Sensor Networks. (arXiv:2307.03761v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03761</link>
<description rdf:parseType="Literal">&lt;p&gt;In the era of digital transformation, systems monitored by the Industrial
Internet of Things (IIoTs) generate large amounts of Multivariate Time Series
(MTS) data through heterogeneous sensor networks. While this data facilitates
condition monitoring and anomaly detection, the increasing complexity and
interdependencies within the sensor network pose significant challenges for
anomaly detection. Despite progress in this field, much of the focus has been
on point anomalies and contextual anomalies, with lesser attention paid to
collective anomalies. A less addressed but common variant of collective
anomalies is when the abnormal collective behavior is caused by shifts in
interrelationships within the system. This can be due to abnormal environmental
conditions like overheating, improper operational settings resulting from
cyber-physical attacks, or system-level faults. To address these challenges,
this paper proposes DyGATAD (Dynamic Graph Attention for Anomaly Detection), a
graph-based anomaly detection framework that leverages the attention mechanism
to construct a continuous graph representation of multivariate time series by
inferring dynamic edges between time series. DyGATAD incorporates an operating
condition-aware reconstruction combined with a topology-based anomaly score,
thereby enhancing the detection ability of relationship shifts. We evaluate the
performance of DyGATAD using both a synthetic dataset with controlled varying
fault severity levels and an industrial-scale multiphase flow facility
benchmark featuring various fault types with different detection difficulties.
Our proposed approach demonstrated superior performance in collective anomaly
detection for sensor networks, showing particular strength in early-stage fault
detection, even in the case of faults with minimal severity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1&quot;&gt;Mengjie Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fink_O/0/1/0/all/0/1&quot;&gt;Olga Fink&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03764">
<title>For Women, Life, Freedom: A Participatory AI-Based Social Web Analysis of a Watershed Moment in Iran&apos;s Gender Struggles. (arXiv:2307.03764v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2307.03764</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a computational analysis of the Persian language
Twitter discourse with the aim to estimate the shift in stance toward gender
equality following the death of Mahsa Amini in police custody. We present an
ensemble active learning pipeline to train a stance classifier. Our novelty
lies in the involvement of Iranian women in an active role as annotators in
building this AI system. Our annotators not only provide labels, but they also
suggest valuable keywords for more meaningful corpus creation as well as
provide short example documents for a guided sampling step. Our analyses
indicate that Mahsa Amini&apos;s death triggered polarized Persian language
discourse where both fractions of negative and positive tweets toward gender
equality increased. The increase in positive tweets was slightly greater than
the increase in negative tweets. We also observe that with respect to account
creation time, between the state-aligned Twitter accounts and pro-protest
Twitter accounts, pro-protest accounts are more similar to baseline Persian
Twitter activity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khorramrouz_A/0/1/0/all/0/1&quot;&gt;Adel Khorramrouz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1&quot;&gt;Sujan Dutta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+KhudaBukhsh_A/0/1/0/all/0/1&quot;&gt;Ashiqur R. KhudaBukhsh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03783">
<title>Neural Abstraction-Based Controller Synthesis and Deployment. (arXiv:2307.03783v1 [eess.SY])</title>
<link>http://arxiv.org/abs/2307.03783</link>
<description rdf:parseType="Literal">&lt;p&gt;Abstraction-based techniques are an attractive approach for synthesizing
correct-by-construction controllers to satisfy high-level temporal
requirements. A main bottleneck for successful application of these techniques
is the memory requirement, both during controller synthesis and in controller
deployment.
&lt;/p&gt;
&lt;p&gt;We propose memory-efficient methods for mitigating the high memory demands of
the abstraction-based techniques using neural network representations. To
perform synthesis for reach-avoid specifications, we propose an on-the-fly
algorithm that relies on compressed neural network representations of the
forward and backward dynamics of the system. In contrast to usual applications
of neural representations, our technique maintains soundness of the end-to-end
process. To ensure this, we correct the output of the trained neural network
such that the corrected output representations are sound with respect to the
finite abstraction. For deployment, we provide a novel training algorithm to
find a neural network representation of the synthesized controller and
experimentally show that the controller can be correctly represented as a
combination of a neural network and a look-up table that requires a
substantially smaller memory.
&lt;/p&gt;
&lt;p&gt;We demonstrate experimentally that our approach significantly reduces the
memory requirements of abstraction-based methods. For the selected benchmarks,
our approach reduces the memory requirements respectively for the synthesis and
deployment by a factor of $1.31\times 10^5$ and $7.13\times 10^3$ on average,
and up to $7.54\times 10^5$ and $3.18\times 10^4$. Although this reduction is
at the cost of increased off-line computations to train the neural networks,
all the steps of our approach are parallelizable and can be implemented on
machines with higher number of processing units to reduce the required
computational time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Majumdar_R/0/1/0/all/0/1&quot;&gt;Rupak Majumdar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Salamati_M/0/1/0/all/0/1&quot;&gt;Mahmoud Salamati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Soudjani_S/0/1/0/all/0/1&quot;&gt;Sadegh Soudjani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03798">
<title>CLIPMasterPrints: Fooling Contrastive Language-Image Pre-training Using Latent Variable Evolution. (arXiv:2307.03798v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03798</link>
<description rdf:parseType="Literal">&lt;p&gt;Models leveraging both visual and textual data such as Contrastive
Language-Image Pre-training (CLIP), are increasingly gaining importance. In
this work, we show that despite their versatility, such models are vulnerable
to what we refer to as fooling master images. Fooling master images are capable
of maximizing the confidence score of a CLIP model for a significant number of
widely varying prompts, while being unrecognizable for humans. We demonstrate
how fooling master images can be mined by searching the latent space of
generative models by means of an evolution strategy or stochastic gradient
descent. We investigate the properties of the mined fooling master images, and
find that images trained on a small number of image captions potentially
generalize to a much larger number of semantically related captions. Further,
we evaluate two possible mitigation strategies and find that vulnerability to
fooling master examples is closely related to a modality gap in contrastive
pre-trained multi-modal networks. From the perspective of vulnerability to
off-manifold attacks, we therefore argue for the mitigation of modality gaps in
CLIP and related multi-modal approaches. Source code and mined CLIPMasterPrints
are available at https://github.com/matfrei/CLIPMasterPrints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freiberger_M/0/1/0/all/0/1&quot;&gt;Matthias Freiberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kun_P/0/1/0/all/0/1&quot;&gt;Peter Kun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lovlie_A/0/1/0/all/0/1&quot;&gt;Anders Sundnes L&amp;#xf8;vlie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Risi_S/0/1/0/all/0/1&quot;&gt;Sebastian Risi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03803">
<title>A Theoretical Perspective on Subnetwork Contributions to Adversarial Robustness. (arXiv:2307.03803v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03803</link>
<description rdf:parseType="Literal">&lt;p&gt;The robustness of deep neural networks (DNNs) against adversarial attacks has
been studied extensively in hopes of both better understanding how deep
learning models converge and in order to ensure the security of these models in
safety-critical applications. Adversarial training is one approach to
strengthening DNNs against adversarial attacks, and has been shown to offer a
means for doing so at the cost of applying computationally expensive training
methods to the entire model. To better understand these attacks and facilitate
more efficient adversarial training, in this paper we develop a novel
theoretical framework that investigates how the adversarial robustness of a
subnetwork contributes to the robustness of the entire network. To do so we
first introduce the concept of semirobustness, which is a measure of the
adversarial robustness of a subnetwork. Building on this concept, we then
provide a theoretical analysis to show that if a subnetwork is semirobust and
there is a sufficient dependency between it and each subsequent layer in the
network, then the remaining layers are also guaranteed to be robust. We
validate these findings empirically across multiple DNN architectures,
datasets, and adversarial attacks. Experiments show the ability of a robust
subnetwork to promote full-network robustness, and investigate the layer-wise
dependencies required for this full-network robustness to be achieved.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Craig_J/0/1/0/all/0/1&quot;&gt;Jovon Craig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andle_J/0/1/0/all/0/1&quot;&gt;Josh Andle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nowak_T/0/1/0/all/0/1&quot;&gt;Theodore S. Nowak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sekeh_S/0/1/0/all/0/1&quot;&gt;Salimeh Yasaei Sekeh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03810">
<title>URL: A Representation Learning Benchmark for Transferable Uncertainty Estimates. (arXiv:2307.03810v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03810</link>
<description rdf:parseType="Literal">&lt;p&gt;Representation learning has significantly driven the field to develop
pretrained models that can act as a valuable starting point when transferring
to new datasets. With the rising demand for reliable machine learning and
uncertainty quantification, there is a need for pretrained models that not only
provide embeddings but also transferable uncertainty estimates. To guide the
development of such models, we propose the Uncertainty-aware Representation
Learning (URL) benchmark. Besides the transferability of the representations,
it also measures the zero-shot transferability of the uncertainty estimate
using a novel metric. We apply URL to evaluate eleven uncertainty quantifiers
that are pretrained on ImageNet and transferred to eight downstream datasets.
We find that approaches that focus on the uncertainty of the representation
itself or estimate the prediction risk directly outperform those that are based
on the probabilities of upstream classes. Yet, achieving transferable
uncertainty quantification remains an open challenge. Our findings indicate
that it is not necessarily in conflict with traditional representation learning
goals. Code is provided under https://github.com/mkirchhof/url .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirchhof_M/0/1/0/all/0/1&quot;&gt;Michael Kirchhof&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mucsanyi_B/0/1/0/all/0/1&quot;&gt;B&amp;#xe1;lint Mucs&amp;#xe1;nyi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1&quot;&gt;Seong Joon Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasneci_E/0/1/0/all/0/1&quot;&gt;Enkelejda Kasneci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03811">
<title>Formulation Graphs for Mapping Structure-Composition of Battery Electrolytes to Device Performance. (arXiv:2307.03811v1 [cond-mat.mtrl-sci])</title>
<link>http://arxiv.org/abs/2307.03811</link>
<description rdf:parseType="Literal">&lt;p&gt;Advanced computational methods are being actively sought for addressing the
challenges associated with discovery and development of new combinatorial
material such as formulations. A widely adopted approach involves domain
informed high-throughput screening of individual components that can be
combined into a formulation. This manages to accelerate the discovery of new
compounds for a target application but still leave the process of identifying
the right &apos;formulation&apos; from the shortlisted chemical space largely a
laboratory experiment-driven process. We report a deep learning model,
Formulation Graph Convolution Network (F-GCN), that can map
structure-composition relationship of the individual components to the property
of liquid formulation as whole. Multiple GCNs are assembled in parallel that
featurize formulation constituents domain-intuitively on the fly. The resulting
molecular descriptors are scaled based on respective constituent&apos;s molar
percentage in the formulation, followed by formalizing into a combined
descriptor that represents a complete formulation to an external learning
architecture. The use case of proposed formulation learning model is
demonstrated for battery electrolytes by training and testing it on two
exemplary datasets representing electrolyte formulations vs battery performance
-- one dataset is sourced from literature about Li/Cu half-cells, while the
other is obtained by lab-experiments related to lithium-iodide full-cell
chemistry. The model is shown to predict the performance metrics like Coulombic
Efficiency (CE) and specific capacity of new electrolyte formulations with
lowest reported errors. The best performing F-GCN model uses molecular
descriptors derived from molecular graphs that are informed with HOMO-LUMO and
electric moment properties of the molecules using a knowledge transfer
technique.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Sharma_V/0/1/0/all/0/1&quot;&gt;Vidushi Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Giammona_M/0/1/0/all/0/1&quot;&gt;Maxwell Giammona&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Zubarev_D/0/1/0/all/0/1&quot;&gt;Dmitry Zubarev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Tek_A/0/1/0/all/0/1&quot;&gt;Andy Tek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Nugyuen_K/0/1/0/all/0/1&quot;&gt;Khanh Nugyuen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Sundberg_L/0/1/0/all/0/1&quot;&gt;Linda Sundberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Congiu_D/0/1/0/all/0/1&quot;&gt;Daniele Congiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+La_Y/0/1/0/all/0/1&quot;&gt;Young-Hye La&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03813">
<title>Controlling Chaotic Maps using Next-Generation Reservoir Computing. (arXiv:2307.03813v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03813</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we combine nonlinear system control techniques with
next-generation reservoir computing, a best-in-class machine learning approach
for predicting the behavior of dynamical systems. We demonstrate the
performance of the controller in a series of control tasks for the chaotic
H\&apos;enon map, including controlling the system between unstable fixed-points,
stabilizing the system to higher order periodic orbits, and to an arbitrary
desired state. We show that our controller succeeds in these tasks, requires
only 10 data points for training, can control the system to a desired
trajectory in a single iteration, and is robust to noise and modeling error.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kent_R/0/1/0/all/0/1&quot;&gt;Robert M. Kent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barbosa_W/0/1/0/all/0/1&quot;&gt;Wendson A. S. Barbosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gauthier_D/0/1/0/all/0/1&quot;&gt;Daniel J. Gauthier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03816">
<title>A Combinatorial Characterization of Online Learning Games with Bounded Losses. (arXiv:2307.03816v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03816</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the online learnability of hypothesis classes with respect to
arbitrary, but bounded, loss functions. We give a new scale-sensitive
combinatorial dimension, named the sequential Minimax dimension, and show that
it gives a tight quantitative characterization of online learnability. As
applications, we give the first quantitative characterization of online
learnability for two natural learning settings: vector-valued regression and
multilabel classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raman_V/0/1/0/all/0/1&quot;&gt;Vinod Raman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subedi_U/0/1/0/all/0/1&quot;&gt;Unique Subedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tewari_A/0/1/0/all/0/1&quot;&gt;Ambuj Tewari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03827">
<title>Effect of Intensity Standardization on Deep Learning for WML Segmentation in Multi-Centre FLAIR MRI. (arXiv:2307.03827v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.03827</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning (DL) methods for white matter lesion (WML) segmentation in MRI
suffer a reduction in performance when applied on data from a scanner or centre
that is out-of-distribution (OOD) from the training data. This is critical for
translation and widescale adoption, since current models cannot be readily
applied to data from new institutions. In this work, we evaluate several
intensity standardization methods for MRI as a preprocessing step for WML
segmentation in multi-centre Fluid-Attenuated Inversion Recovery (FLAIR) MRI.
We evaluate a method specifically developed for FLAIR MRI called IAMLAB along
with other popular normalization techniques such as White-strip, Nyul and
Z-score. We proposed an Ensemble model that combines predictions from each of
these models. A skip-connection UNet (SC UNet) was trained on the standardized
images, as well as the original data and segmentation performance was evaluated
over several dimensions. The training (in-distribution) data consists of a
single study, of 60 volumes, and the test (OOD) data is 128 unseen volumes from
three clinical cohorts. Results show IAMLAB and Ensemble provide higher WML
segmentation performance compared to models from original data or other
normalization methods. IAMLAB &amp;amp; Ensemble have the highest dice similarity
coefficient (DSC) on the in-distribution data (0.78 &amp;amp; 0.80) and on clinical OOD
data. DSC was significantly higher for IAMLAB compared to the original data
(p&amp;lt;0.05) for all lesion categories (LL&amp;gt;25mL: 0.77 vs. 0.71; 10mL&amp;lt;= LL&amp;lt;25mL:
0.66 vs. 0.61; LL&amp;lt;10mL: 0.53 vs. 0.52). The IAMLAB and Ensemble normalization
methods are mitigating MRI domain shift and are optimal for DL-based WML
segmentation in unseen FLAIR data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ghazvanchahi_A/0/1/0/all/0/1&quot;&gt;Abdollah Ghazvanchahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Maralani_P/0/1/0/all/0/1&quot;&gt;Pejman Jahbedar Maralani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Moody_A/0/1/0/all/0/1&quot;&gt;Alan R. Moody&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khademi_A/0/1/0/all/0/1&quot;&gt;April Khademi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03838">
<title>RADAR: Robust AI-Text Detection via Adversarial Learning. (arXiv:2307.03838v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.03838</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in large language models (LLMs) and the intensifying
popularity of ChatGPT-like applications have blurred the boundary of
high-quality text generation between humans and machines. However, in addition
to the anticipated revolutionary changes to our technology and society, the
difficulty of distinguishing LLM-generated texts (AI-text) from human-generated
texts poses new challenges of misuse and fairness, such as fake content
generation, plagiarism, and false accusation of innocent writers. While
existing works show that current AI-text detectors are not robust to LLM-based
paraphrasing, this paper aims to bridge this gap by proposing a new framework
called RADAR, which jointly trains a Robust AI-text Detector via Adversarial
leaRning. RADAR is based on adversarial training of a paraphraser and a
detector. The paraphraser&apos;s goal is to generate realistic contents to evade
AI-text detection. RADAR uses the feedback from the detector to update the
paraphraser, and vice versa. Evaluated with 8 different LLMs (Pythia, Dolly
2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) across 4 datasets,
experimental results show that RADAR significantly outperforms existing AI-text
detection methods, especially when paraphrasing is in place. We also identify
the strong transferability of RADAR from instruction-tuned LLMs to other LLMs,
and evaluate the improved capability of RADAR via GPT-3.5.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xiaomeng Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pin-Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_T/0/1/0/all/0/1&quot;&gt;Tsung-Yi Ho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03848">
<title>Optimal Learners for Realizable Regression: PAC Learning and Online Learning. (arXiv:2307.03848v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03848</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we aim to characterize the statistical complexity of realizable
regression both in the PAC learning setting and the online learning setting.
&lt;/p&gt;
&lt;p&gt;Previous work had established the sufficiency of finiteness of the fat
shattering dimension for PAC learnability and the necessity of finiteness of
the scaled Natarajan dimension, but little progress had been made towards a
more complete characterization since the work of Simon 1997 (SICOMP &apos;97). To
this end, we first introduce a minimax instance optimal learner for realizable
regression and propose a novel dimension that both qualitatively and
quantitatively characterizes which classes of real-valued predictors are
learnable. We then identify a combinatorial dimension related to the Graph
dimension that characterizes ERM learnability in the realizable setting.
Finally, we establish a necessary condition for learnability based on a
combinatorial dimension related to the DS dimension, and conjecture that it may
also be sufficient in this context.
&lt;/p&gt;
&lt;p&gt;Additionally, in the context of online learning we provide a dimension that
characterizes the minimax instance optimal cumulative loss up to a constant
factor and design an optimal online learner for realizable regression, thus
resolving an open question raised by Daskalakis and Golowich in STOC &apos;22.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Attias_I/0/1/0/all/0/1&quot;&gt;Idan Attias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanneke_S/0/1/0/all/0/1&quot;&gt;Steve Hanneke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalavasis_A/0/1/0/all/0/1&quot;&gt;Alkis Kalavasis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karbasi_A/0/1/0/all/0/1&quot;&gt;Amin Karbasi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Velegkas_G/0/1/0/all/0/1&quot;&gt;Grigoris Velegkas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03854">
<title>inTformer: A Time-Embedded Attention-Based Transformer for Crash Likelihood Prediction at Intersections Using Connected Vehicle Data. (arXiv:2307.03854v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03854</link>
<description rdf:parseType="Literal">&lt;p&gt;The real-time crash likelihood prediction model is an essential component of
the proactive traffic safety management system. Over the years, numerous
studies have attempted to construct a crash likelihood prediction model in
order to enhance traffic safety, but mostly on freeways. In the majority of the
existing studies, researchers have primarily employed a deep learning-based
framework to identify crash potential. Lately, Transformer has emerged as a
potential deep neural network that fundamentally operates through
attention-based mechanisms. Transformer has several functional benefits over
extant deep learning models such as Long Short-Term Memory (LSTM), Convolution
Neural Network (CNN), etc. Firstly, Transformer can readily handle long-term
dependencies in a data sequence. Secondly, Transformer can parallelly process
all elements in a data sequence during training. Finally, Transformer does not
have the vanishing gradient issue. Realizing the immense possibility of
Transformer, this paper proposes inTersection-Transformer (inTformer), a
time-embedded attention-based Transformer model that can effectively predict
intersection crash likelihood in real-time. The proposed model was evaluated
using connected vehicle data extracted from INRIX&apos;s Signal Analytics Platform.
The data was parallelly formatted and stacked at different timesteps to develop
nine inTformer models. The best inTformer model achieved a sensitivity of 73%.
This model was also compared to earlier studies on crash likelihood prediction
at intersections and with several established deep learning models trained on
the same connected vehicle dataset. In every scenario, this inTformer
outperformed the benchmark models confirming the viability of the proposed
inTformer architecture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anik_B/0/1/0/all/0/1&quot;&gt;B.M. Tazbiul Hassan Anik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_Z/0/1/0/all/0/1&quot;&gt;Zubayer Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdel_Aty_M/0/1/0/all/0/1&quot;&gt;Mohamed Abdel-Aty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03860">
<title>Reinforcement and Deep Reinforcement Learning-based Solutions for Machine Maintenance Planning, Scheduling Policies, and Optimization. (arXiv:2307.03860v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03860</link>
<description rdf:parseType="Literal">&lt;p&gt;Systems and machines undergo various failure modes that result in machine
health degradation, so maintenance actions are required to restore them back to
a state where they can perform their expected functions. Since maintenance
tasks are inevitable, maintenance planning is essential to ensure the smooth
operations of the production system and other industries at large. Maintenance
planning is a decision-making problem that aims at developing optimum
maintenance policies and plans that help reduces maintenance costs, extend
asset life, maximize their availability, and ultimately ensure workplace
safety. Reinforcement learning is a data-driven decision-making algorithm that
has been increasingly applied to develop dynamic maintenance plans while
leveraging the continuous information from condition monitoring of the system
and machine states. By leveraging the condition monitoring data of systems and
machines with reinforcement learning, smart maintenance planners can be
developed, which is a precursor to achieving a smart factory. This paper
presents a literature review on the applications of reinforcement and deep
reinforcement learning for maintenance planning and optimization problems. To
capture the common ideas without losing touch with the uniqueness of each
publication, taxonomies used to categorize the systems were developed, and
reviewed publications were highlighted, classified, and summarized based on
these taxonomies. Adopted methodologies, findings, and well-defined
interpretations of the reviewed studies were summarized in graphical and
tabular representations to maximize the utility of the work for both
researchers and practitioners. This work also highlights the research gaps, key
insights from the literature, and areas for future work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ogunfowora_O/0/1/0/all/0/1&quot;&gt;Oluwaseyi Ogunfowora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Najjaran_H/0/1/0/all/0/1&quot;&gt;Homayoun Najjaran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03863">
<title>Memory-Immersed Collaborative Digitization for Area-Efficient Compute-in-Memory Deep Learning. (arXiv:2307.03863v1 [cs.AR])</title>
<link>http://arxiv.org/abs/2307.03863</link>
<description rdf:parseType="Literal">&lt;p&gt;This work discusses memory-immersed collaborative digitization among
compute-in-memory (CiM) arrays to minimize the area overheads of a conventional
analog-to-digital converter (ADC) for deep learning inference. Thereby, using
the proposed scheme, significantly more CiM arrays can be accommodated within
limited footprint designs to improve parallelism and minimize external memory
accesses. Under the digitization scheme, CiM arrays exploit their parasitic bit
lines to form a within-memory capacitive digital-to-analog converter (DAC) that
facilitates area-efficient successive approximation (SA) digitization. CiM
arrays collaborate where a proximal array digitizes the analog-domain
product-sums when an array computes the scalar product of input and weights. We
discuss various networking configurations among CiM arrays where Flash, SA, and
their hybrid digitization steps can be efficiently implemented using the
proposed memory-immersed scheme. The results are demonstrated using a 65 nm
CMOS test chip. Compared to a 40 nm-node 5-bit SAR ADC, our 65 nm design
requires $\sim$25$\times$ less area and $\sim$1.4$\times$ less energy by
leveraging in-memory computing structures. Compared to a 40 nm-node 5-bit Flash
ADC, our design requires $\sim$51$\times$ less area and $\sim$13$\times$ less
energy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nasrin_S/0/1/0/all/0/1&quot;&gt;Shamma Nasrin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hashem_M/0/1/0/all/0/1&quot;&gt;Maeesha Binte Hashem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darabi_N/0/1/0/all/0/1&quot;&gt;Nastaran Darabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parpillon_B/0/1/0/all/0/1&quot;&gt;Benjamin Parpillon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fahim_F/0/1/0/all/0/1&quot;&gt;Farah Fahim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomes_W/0/1/0/all/0/1&quot;&gt;Wilfred Gomes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1&quot;&gt;Amit Ranjan Trivedi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03864">
<title>When Do Transformers Shine in RL? Decoupling Memory from Credit Assignment. (arXiv:2307.03864v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03864</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning (RL) algorithms face two distinct challenges: learning
effective representations of past and present observations, and determining how
actions influence future returns. Both challenges involve modeling long-term
dependencies. The transformer architecture has been very successful to solve
problems that involve long-term dependencies, including in the RL domain.
However, the underlying reason for the strong performance of Transformer-based
RL methods remains unclear: is it because they learn effective memory, or
because they perform effective credit assignment? After introducing formal
definitions of memory length and credit assignment length, we design simple
configurable tasks to measure these distinct quantities. Our empirical results
reveal that Transformers can enhance the memory capacity of RL algorithms,
scaling up to tasks that require memorizing observations $1500$ steps ago.
However, Transformers do not improve long-term credit assignment. In summary,
our results provide an explanation for the success of Transformers in RL, while
also highlighting an important area for future research and benchmark design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_T/0/1/0/all/0/1&quot;&gt;Tianwei Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1&quot;&gt;Michel Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eysenbach_B/0/1/0/all/0/1&quot;&gt;Benjamin Eysenbach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bacon_P/0/1/0/all/0/1&quot;&gt;Pierre-Luc Bacon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03872">
<title>Domain Adaptation using Silver Standard Labels for Ki-67 Scoring in Digital Pathology: A Step Closer to Widescale Deployment. (arXiv:2307.03872v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.03872</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning systems have been proposed to improve the objectivity and
efficiency of Ki- 67 PI scoring. The challenge is that while very accurate,
deep learning techniques suffer from reduced performance when applied to
out-of-domain data. This is a critical challenge for clinical translation, as
models are typically trained using data available to the vendor, which is not
from the target domain. To address this challenge, this study proposes a domain
adaptation pipeline that employs an unsupervised framework to generate silver
standard (pseudo) labels in the target domain, which is used to augment the
gold standard (GS) source domain data. Five training regimes were tested on two
validated Ki-67 scoring architectures (UV-Net and piNET), (1) SS Only: trained
on target silver standard (SS) labels, (2) GS Only: trained on source GS
labels, (3) Mixed: trained on target SS and source GS labels, (4) GS+SS:
trained on source GS labels and fine-tuned on target SS labels, and our
proposed method (5) SS+GS: trained on source SS labels and fine-tuned on source
GS labels. The SS+GS method yielded significantly (p &amp;lt; 0.05) higher PI accuracy
(95.9%) and more consistent results compared to the GS Only model on target
data. Analysis of t-SNE plots showed features learned by the SS+GS models are
more aligned for source and target data, resulting in improved generalization.
The proposed pipeline provides an efficient method for learning the target
distribution without manual annotations, which are time-consuming and costly to
generate for medical images. This framework can be applied to any target site
as a per-laboratory calibration method, for widescale deployment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dy_A/0/1/0/all/0/1&quot;&gt;Amanda Dy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nguyen_N/0/1/0/all/0/1&quot;&gt;Ngoc-Nhu Jennifer Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mirjahanmardi_S/0/1/0/all/0/1&quot;&gt;Seyed Hossein Mirjahanmardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dawe_M/0/1/0/all/0/1&quot;&gt;Melanie Dawe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fyles_A/0/1/0/all/0/1&quot;&gt;Anthony Fyles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shi_W/0/1/0/all/0/1&quot;&gt;Wei Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fei-Fei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Androutsos_D/0/1/0/all/0/1&quot;&gt;Dimitrios Androutsos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Done_S/0/1/0/all/0/1&quot;&gt;Susan Done&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khademi_A/0/1/0/all/0/1&quot;&gt;April Khademi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03875">
<title>Large Language Models for Supply Chain Optimization. (arXiv:2307.03875v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.03875</link>
<description rdf:parseType="Literal">&lt;p&gt;Supply chain operations traditionally involve a variety of complex decision
making problems. Over the last few decades, supply chains greatly benefited
from advances in computation, which allowed the transition from manual
processing to automation and cost-effective optimization. Nonetheless, business
operators still need to spend substantial efforts in \emph{explaining} and
interpreting the optimization outcomes to stakeholders. Motivated by the recent
advances in Large Language Models (LLMs), we study how this disruptive
technology can help bridge the gap between supply chain automation and human
comprehension and trust thereof. We design \name{} -- a framework that accepts
as input queries in plain text, and outputs insights about the underlying
optimization outcomes. Our framework does not forgo the state-of-the-art
combinatorial optimization technology, but rather leverages it to
quantitatively answer what-if scenarios (e.g., how would the cost change if we
used supplier B instead of supplier A for a given demand?). Importantly, our
design does not require sending proprietary data over to LLMs, which can be a
privacy concern in some circumstances. We demonstrate the effectiveness of our
framework on a real server placement scenario within Microsoft&apos;s cloud supply
chain. Along the way, we develop a general evaluation benchmark, which can be
used to evaluate the accuracy of the LLM output in other scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Beibin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mellou_K/0/1/0/all/0/1&quot;&gt;Konstantina Mellou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pathuri_J/0/1/0/all/0/1&quot;&gt;Jeevan Pathuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menache_I/0/1/0/all/0/1&quot;&gt;Ishai Menache&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03884">
<title>Noisy Tensor Ring approximation for computing gradients of Variational Quantum Eigensolver for Combinatorial Optimization. (arXiv:2307.03884v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2307.03884</link>
<description rdf:parseType="Literal">&lt;p&gt;Variational Quantum algorithms, especially Quantum Approximate Optimization
and Variational Quantum Eigensolver (VQE) have established their potential to
provide computational advantage in the realm of combinatorial optimization.
However, these algorithms suffer from classically intractable gradients
limiting the scalability. This work addresses the scalability challenge for VQE
by proposing a classical gradient computation method which utilizes the
parameter shift rule but computes the expected values from the circuits using a
tensor ring approximation. The parametrized gates from the circuit transform
the tensor ring by contracting the matrix along the free edges of the tensor
ring. While the single qubit gates do not alter the ring structure, the state
transformations from the two qubit rotations are evaluated by truncating the
singular values thereby preserving the structure of the tensor ring and
reducing the computational complexity. This variation of the Matrix product
state approximation grows linearly in number of qubits and the number of two
qubit gates as opposed to the exponential growth in the classical simulations,
allowing for a faster evaluation of the gradients on classical simulators.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Peddireddy_D/0/1/0/all/0/1&quot;&gt;Dheeraj Peddireddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Priyam_U/0/1/0/all/0/1&quot;&gt;Utkarsh Priyam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Aggarwal_V/0/1/0/all/0/1&quot;&gt;Vaneet Aggarwal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03886">
<title>On Regularization and Inference with Label Constraints. (arXiv:2307.03886v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03886</link>
<description rdf:parseType="Literal">&lt;p&gt;Prior knowledge and symbolic rules in machine learning are often expressed in
the form of label constraints, especially in structured prediction problems. In
this work, we compare two common strategies for encoding label constraints in a
machine learning pipeline, regularization with constraints and constrained
inference, by quantifying their impact on model performance. For
regularization, we show that it narrows the generalization gap by precluding
models that are inconsistent with the constraints. However, its preference for
small violations introduces a bias toward a suboptimal model. For constrained
inference, we show that it reduces the population risk by correcting a model&apos;s
violation, and hence turns the violation into an advantage. Given these
differences, we further explore the use of two approaches together and propose
conditions for constrained inference to compensate for the bias introduced by
regularization, aiming to improve both the model complexity and optimal risk.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kaifu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;Hangfeng He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tin D. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1&quot;&gt;Piyush Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1&quot;&gt;Dan Roth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03887">
<title>Improving Prototypical Part Networks with Reward Reweighing, Reselection, and Retraining. (arXiv:2307.03887v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03887</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, work has gone into developing deep interpretable methods for
image classification that clearly attributes a model&apos;s output to specific
features of the data. One such of these methods is the prototypical part
network (ProtoPNet), which attempts to classify images based on meaningful
parts of the input. While this method results in interpretable classifications,
this method often learns to classify from spurious or inconsistent parts of the
image. Hoping to remedy this, we take inspiration from the recent developments
in Reinforcement Learning with Human Feedback (RLHF) to fine-tune these
prototypes. By collecting human annotations of prototypes quality via a 1-5
scale on the CUB-200-2011 dataset, we construct a reward model that learns to
identify non-spurious prototypes. In place of a full RL update, we propose the
reweighted, reselected, and retrained prototypical part network (R3-ProtoPNet),
which adds an additional three steps to the ProtoPNet training loop. The first
two steps are reward-based reweighting and reselection, which align prototypes
with human feedback. The final step is retraining to realign the model&apos;s
features with the updated prototypes. We find that R3-ProtoPNet improves the
overall consistency and meaningfulness of the prototypes, but lower the test
predictive accuracy when used independently. When multiple R3-ProtoPNets are
incorporated into an ensemble, we find an increase in test predictive
performance while maintaining interpretability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Netzorg_R/0/1/0/all/0/1&quot;&gt;Robin Netzorg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiaxun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Bin Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03899">
<title>Active Learning in Physics: From 101, to Progress, and Perspective. (arXiv:2307.03899v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2307.03899</link>
<description rdf:parseType="Literal">&lt;p&gt;Active Learning (AL) is a family of machine learning (ML) algorithms that
predates the current era of artificial intelligence. Unlike traditional
approaches that require labeled samples for training, AL iteratively selects
unlabeled samples to be annotated by an expert. This protocol aims to
prioritize the most informative samples, leading to improved model performance
compared to training with all labeled samples. In recent years, AL has gained
increasing attention, particularly in the field of physics. This paper presents
a comprehensive and accessible introduction to the theory of AL reviewing the
latest advancements across various domains. Additionally, we explore the
potential integration of AL with quantum ML, envisioning a synergistic fusion
of these two fields rather than viewing AL as a mere extension of classical ML
into the quantum realm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Yongcheng Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Martin_Guerrero_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; D. Mart&amp;#xed;n-Guerrero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Vives_Gilabert_Y/0/1/0/all/0/1&quot;&gt;Yolanda Vives-Gilabert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xi Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03902">
<title>Feature selection simultaneously preserving both class and cluster structures. (arXiv:2307.03902v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03902</link>
<description rdf:parseType="Literal">&lt;p&gt;When a data set has significant differences in its class and cluster
structure, selecting features aiming only at the discrimination of classes
would lead to poor clustering performance, and similarly, feature selection
aiming only at preserving cluster structures would lead to poor classification
performance. To the best of our knowledge, a feature selection method that
simultaneously considers class discrimination and cluster structure
preservation is not available in the literature. In this paper, we have tried
to bridge this gap by proposing a neural network-based feature selection method
that focuses both on class discrimination and structure preservation in an
integrated manner. In addition to assessing typical classification problems, we
have investigated its effectiveness on band selection in hyperspectral images.
Based on the results of the experiments, we may claim that the proposed
feature/band selection can select a subset of features that is good for both
classification and clustering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1&quot;&gt;Suchismita Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pal_N/0/1/0/all/0/1&quot;&gt;Nikhil R. Pal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03906">
<title>ScriptWorld: Text Based Environment For Learning Procedural Knowledge. (arXiv:2307.03906v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.03906</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-based games provide a framework for developing natural language
understanding and commonsense knowledge about the world in reinforcement
learning based agents. Existing text-based environments often rely on fictional
situations and characters to create a gaming framework and are far from
real-world scenarios. In this paper, we introduce ScriptWorld: a text-based
environment for teaching agents about real-world daily chores and hence
imparting commonsense knowledge. To the best of our knowledge, it is the first
interactive text-based gaming framework that consists of daily real-world human
activities designed using scripts dataset. We provide gaming environments for
10 daily activities and perform a detailed analysis of the proposed
environment. We develop RL-based baseline models/agents to play the games in
Scriptworld. To understand the role of language models in such environments, we
leverage features obtained from pre-trained language models in the RL agents.
Our experiments show that prior knowledge obtained from a pre-trained language
model helps to solve real-world text-based gaming environments. We release the
environment via Github: https://github.com/Exploration-Lab/ScriptWorld
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1&quot;&gt;Abhinav Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmad_A/0/1/0/all/0/1&quot;&gt;Areeb Ahmad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandey_U/0/1/0/all/0/1&quot;&gt;Umang Pandey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Modi_A/0/1/0/all/0/1&quot;&gt;Ashutosh Modi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03908">
<title>Incorporating Deep Q -- Network with Multiclass Classification Algorithms. (arXiv:2307.03908v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03908</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we explore how Deep Q-Network (DQN) might improve the
functionality of multiclass classification algorithms. We will use a benchmark
dataset from Kaggle to create a framework incorporating DQN with existing
supervised multiclass classification algorithms. The findings of this study
will bring insight into how deep reinforcement learning strategies may be used
to increase multiclass classification accuracy. They have been used in a number
of fields, including image recognition, natural language processing, and
bioinformatics. This study is focused on the prediction of financial distress
in companies in addition to the wider application of Deep Q-Network in
multiclass classification. Identifying businesses that are likely to experience
financial distress is a crucial task in the fields of finance and risk
management. Whenever a business experiences serious challenges keeping its
operations going and meeting its financial responsibilities, it is said to be
in financial distress. It commonly happens when a company has a sharp and
sustained recession in profitability, cash flow issues, or an unsustainable
level of debt.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zambare_N/0/1/0/all/0/1&quot;&gt;Noopur Zambare&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sawane_R/0/1/0/all/0/1&quot;&gt;Ravindranath Sawane&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03920">
<title>Training Physics-Informed Neural Networks via Multi-Task Optimization for Traffic Density Prediction. (arXiv:2307.03920v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2307.03920</link>
<description rdf:parseType="Literal">&lt;p&gt;Physics-informed neural networks (PINNs) are a newly emerging research
frontier in machine learning, which incorporate certain physical laws that
govern a given data set, e.g., those described by partial differential
equations (PDEs), into the training of the neural network (NN) based on such a
data set. In PINNs, the NN acts as the solution approximator for the PDE while
the PDE acts as the prior knowledge to guide the NN training, leading to the
desired generalization performance of the NN when facing the limited
availability of training data. However, training PINNs is a non-trivial task
largely due to the complexity of the loss composed of both NN and physical law
parts. In this work, we propose a new PINN training framework based on the
multi-task optimization (MTO) paradigm. Under this framework, multiple
auxiliary tasks are created and solved together with the given (main) task,
where the useful knowledge from solving one task is transferred in an adaptive
mode to assist in solving some other tasks, aiming to uplift the performance of
solving the main task. We implement the proposed framework and apply it to
train the PINN for addressing the traffic density prediction problem.
Experimental results demonstrate that our proposed training framework leads to
significant performance improvement in comparison to the traditional way of
training the PINN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_A/0/1/0/all/0/1&quot;&gt;A. K. Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shafiei_S/0/1/0/all/0/1&quot;&gt;Sajjad Shafiei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dia_H/0/1/0/all/0/1&quot;&gt;Hussein Dia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mihaita_A/0/1/0/all/0/1&quot;&gt;Adriana-Simona Mihaita&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grzybowska_H/0/1/0/all/0/1&quot;&gt;Hanna Grzybowska&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03927">
<title>Fast Empirical Scenarios. (arXiv:2307.03927v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2307.03927</link>
<description rdf:parseType="Literal">&lt;p&gt;We seek to extract a small number of representative scenarios from large and
high-dimensional panel data that are consistent with sample moments. Among two
novel algorithms, the first identifies scenarios that have not been observed
before, and comes with a scenario-based representation of covariance matrices.
The second proposal picks important data points from states of the world that
have already realized, and are consistent with higher-order sample moment
information. Both algorithms are efficient to compute, and lend themselves to
consistent scenario-based modeling and high-dimensional numerical integration.
Extensive numerical benchmarking studies and an application in portfolio
optimization favor the proposed algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Multerer_M/0/1/0/all/0/1&quot;&gt;Michael Multerer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schneider_P/0/1/0/all/0/1&quot;&gt;Paul Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sen_R/0/1/0/all/0/1&quot;&gt;Rohan Sen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03929">
<title>Fairness-Aware Graph Neural Networks: A Survey. (arXiv:2307.03929v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03929</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) have become increasingly important due to their
representational power and state-of-the-art predictive performance on many
fundamental learning tasks. Despite this success, GNNs suffer from fairness
issues that arise as a result of the underlying graph data and the fundamental
aggregation mechanism that lies at the heart of the large class of GNN models.
In this article, we examine and categorize fairness techniques for improving
the fairness of GNNs. Previous work on fair GNN models and techniques are
discussed in terms of whether they focus on improving fairness during a
preprocessing step, during training, or in a post-processing phase.
Furthermore, we discuss how such techniques can be used together whenever
appropriate, and highlight the advantages and intuition as well. We also
introduce an intuitive taxonomy for fairness evaluation metrics including
graph-level fairness, neighborhood-level fairness, embedding-level fairness,
and prediction-level fairness metrics. In addition, graph datasets that are
useful for benchmarking the fairness of GNN models are summarized succinctly.
Finally, we highlight key open problems and challenges that remain to be
addressed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1&quot;&gt;April Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rossi_R/0/1/0/all/0/1&quot;&gt;Ryan A. Rossi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1&quot;&gt;Namyong Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trivedi_P/0/1/0/all/0/1&quot;&gt;Puja Trivedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1&quot;&gt;Tong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sungchul Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1&quot;&gt;Franck Dernoncourt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_N/0/1/0/all/0/1&quot;&gt;Nesreen K. Ahmed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03930">
<title>Rosko: Row Skipping Outer Products for Sparse Matrix Multiplication Kernels. (arXiv:2307.03930v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03930</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose Rosko -- row skipping outer products -- for deriving sparse matrix
multiplication (SpMM) kernels in reducing computation and memory access
requirements of deep neural networks (DNNs). Rosko allows skipping of entire
row computations during program execution with low sparsity-management
overheads. We analytically derive sparse CPU kernels that adapt to given
hardware characteristics to effectively utilize processor cores and minimize
data movement without the need for auto-tuning or search space exploration.
Rosko can be integrated with other outer product scheduling methods, allowing
them to leverage row skipping by using Rosko&apos;s packing format to skip
unnecessary computation.
&lt;/p&gt;
&lt;p&gt;Rosko kernels outperform existing auto-tuning and search-based solutions as
well as state-of-the-art vendor-optimized libraries on real hardware across a
variety of neural network workloads. For matrices with sparsities ranging from
65% to 99.8% typically found in machine learning, Rosko kernels achieve up to a
6.5x runtime reduction on Intel and ARM CPUs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Natesh_V/0/1/0/all/0/1&quot;&gt;Vikas Natesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabot_A/0/1/0/all/0/1&quot;&gt;Andrew Sabot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kung_H/0/1/0/all/0/1&quot;&gt;H.T. Kung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ting_M/0/1/0/all/0/1&quot;&gt;Mark Ting&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03945">
<title>Fault Monitoring in Passive Optical Networks using Machine Learning Techniques. (arXiv:2307.03945v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03945</link>
<description rdf:parseType="Literal">&lt;p&gt;Passive optical network (PON) systems are vulnerable to a variety of
failures, including fiber cuts and optical network unit (ONU)
transmitter/receiver failures. Any service interruption caused by a fiber cut
can result in huge financial losses for service providers or operators.
Identifying the faulty ONU becomes difficult in the case of nearly equidistant
branch terminations because the reflections from the branches overlap, making
it difficult to distinguish the faulty branch given the global backscattering
signal. With increasing network size, the complexity of fault monitoring in PON
systems increases, resulting in less reliable monitoring. To address these
challenges, we propose in this paper various machine learning (ML) approaches
for fault monitoring in PON systems, and we validate them using experimental
optical time domain reflectometry (OTDR) data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdelli_K/0/1/0/all/0/1&quot;&gt;Khouloud Abdelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tropschug_C/0/1/0/all/0/1&quot;&gt;Carsten Tropschug&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Griesser_H/0/1/0/all/0/1&quot;&gt;Helmut Griesser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pachnicke_S/0/1/0/all/0/1&quot;&gt;Stephan Pachnicke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03980">
<title>Building and Road Segmentation Using EffUNet and Transfer Learning Approach. (arXiv:2307.03980v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03980</link>
<description rdf:parseType="Literal">&lt;p&gt;In city, information about urban objects such as water supply, railway lines,
power lines, buildings, roads, etc., is necessary for city planning. In
particular, information about the spread of these objects, locations and
capacity is needed for the policymakers to make impactful decisions. This
thesis aims to segment the building and roads from the aerial image captured by
the satellites and UAVs. Many different architectures have been proposed for
the semantic segmentation task and UNet being one of them. In this thesis, we
propose a novel architecture based on Google&apos;s newly proposed EfficientNetV2 as
an encoder for feature extraction with UNet decoder for constructing the
segmentation map. Using this approach we achieved a benchmark score for the
Massachusetts Building and Road dataset with an mIOU of 0.8365 and 0.9153
respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gangurde_S/0/1/0/all/0/1&quot;&gt;Sahil Gangurde&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03997">
<title>Efficient Model-Free Exploration in Low-Rank MDPs. (arXiv:2307.03997v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03997</link>
<description rdf:parseType="Literal">&lt;p&gt;A major challenge in reinforcement learning is to develop practical,
sample-efficient algorithms for exploration in high-dimensional domains where
generalization and function approximation is required. Low-Rank Markov Decision
Processes -- where transition probabilities admit a low-rank factorization
based on an unknown feature embedding -- offer a simple, yet expressive
framework for RL with function approximation, but existing algorithms are
either (1) computationally intractable, or (2) reliant upon restrictive
statistical assumptions such as latent variable structure, access to
model-based function approximation, or reachability. In this work, we propose
the first provably sample-efficient algorithm for exploration in Low-Rank MDPs
that is both computationally efficient and model-free, allowing for general
function approximation and requiring no additional structural assumptions. Our
algorithm, VoX, uses the notion of a generalized optimal design for the feature
embedding as an efficiently computable basis for exploration, performing
efficient optimal design computation by interleaving representation learning
and policy optimization. Our analysis -- which is appealingly simple and
modular -- carefully combines several techniques, including a new reduction
from optimal design computation to policy optimization based on the Frank-Wolfe
method, and an improved analysis of a certain minimax representation learning
objective found in prior work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mhammedi_Z/0/1/0/all/0/1&quot;&gt;Zakaria Mhammedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Block_A/0/1/0/all/0/1&quot;&gt;Adam Block&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foster_D/0/1/0/all/0/1&quot;&gt;Dylan J. Foster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rakhlin_A/0/1/0/all/0/1&quot;&gt;Alexander Rakhlin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04001">
<title>Polynomial Width is Sufficient for Set Representation with High-dimensional Features. (arXiv:2307.04001v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.04001</link>
<description rdf:parseType="Literal">&lt;p&gt;Set representation has become ubiquitous in deep learning for modeling the
inductive bias of neural networks that are insensitive to the input order.
DeepSets is the most widely used neural network architecture for set
representation. It involves embedding each set element into a latent space with
dimension $L$, followed by a sum pooling to obtain a whole-set embedding, and
finally mapping the whole-set embedding to the output. In this work, we
investigate the impact of the dimension $L$ on the expressive power of
DeepSets. Previous analyses either oversimplified high-dimensional features to
be one-dimensional features or were limited to analytic activations, thereby
diverging from practical use or resulting in $L$ that grows exponentially with
the set size $N$ and feature dimension $D$. To investigate the minimal value of
$L$ that achieves sufficient expressive power, we present two set-element
embedding layers: (a) linear + power activation (LP) and (b) linear +
exponential activations (LE). We demonstrate that $L$ being poly$(N, D)$ is
sufficient for set representation using both embedding layers. We also provide
a lower bound of $L$ for the LP embedding layer. Furthermore, we extend our
results to permutation-equivariant set functions and the complex field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peihao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shenghao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Pan Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04010">
<title>Understanding the Efficacy of U-Net &amp; Vision Transformer for Groundwater Numerical Modelling. (arXiv:2307.04010v1 [physics.flu-dyn])</title>
<link>http://arxiv.org/abs/2307.04010</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a comprehensive comparison of various machine learning
models, namely U-Net, U-Net integrated with Vision Transformers (ViT), and
Fourier Neural Operator (FNO), for time-dependent forward modelling in
groundwater systems. Through testing on synthetic datasets, it is demonstrated
that U-Net and U-Net + ViT models outperform FNO in accuracy and efficiency,
especially in sparse data scenarios. These findings underscore the potential of
U-Net-based models for groundwater modelling in real-world applications where
data scarcity is prevalent.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Taccari_M/0/1/0/all/0/1&quot;&gt;Maria Luisa Taccari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ovadia_O/0/1/0/all/0/1&quot;&gt;Oded Ovadia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;He Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kahana_A/0/1/0/all/0/1&quot;&gt;Adar Kahana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaohui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Jimack_P/0/1/0/all/0/1&quot;&gt;Peter K. Jimack&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04011">
<title>Robust Learning-Based Incipient Slip Detection using the PapillArray Optical Tactile Sensor for Improved Robotic Gripping. (arXiv:2307.04011v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.04011</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to detect slip, particularly incipient slip, enables robotic
systems to take corrective measures to prevent a grasped object from being
dropped. Therefore, slip detection can enhance the overall security of robotic
gripping. However, accurately detecting incipient slip remains a significant
challenge. In this paper, we propose a novel learning-based approach to detect
incipient slip using the PapillArray (Contactile, Australia) tactile sensor.
The resulting model is highly effective in identifying patterns associated with
incipient slip, achieving a detection success rate of 95.6% when tested with an
offline dataset. Furthermore, we introduce several data augmentation methods to
enhance the robustness of our model. When transferring the trained model to a
robotic gripping environment distinct from where the training data was
collected, our model maintained robust performance, with a success rate of
96.8%, providing timely feedback for stabilizing several practical gripping
tasks. Our project website:
https://sites.google.com/view/incipient-slip-detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ulloa_P/0/1/0/all/0/1&quot;&gt;Pablo Martinez Ulloa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burke_R/0/1/0/all/0/1&quot;&gt;Robert Burke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bulens_D/0/1/0/all/0/1&quot;&gt;David Cordova Bulens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Redmond_S/0/1/0/all/0/1&quot;&gt;Stephen J. Redmond&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04024">
<title>Robust Ranking Explanations. (arXiv:2307.04024v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.04024</link>
<description rdf:parseType="Literal">&lt;p&gt;Robust explanations of machine learning models are critical to establish
human trust in the models. Due to limited cognition capability, most humans can
only interpret the top few salient features. It is critical to make top salient
features robust to adversarial attacks, especially those against the more
vulnerable gradient-based explanations. Existing defense measures robustness
using $\ell_p$-norms, which have weaker protection power. We define explanation
thickness for measuring salient features ranking stability, and derive
tractable surrogate bounds of the thickness to design the \textit{R2ET}
algorithm to efficiently maximize the thickness and anchor top salient
features. Theoretically, we prove a connection between R2ET and adversarial
training. Experiments with a wide spectrum of network architectures and data
modalities, including brain networks, demonstrate that R2ET attains higher
explanation robustness under stealthy attacks while retaining accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1&quot;&gt;Chenghua Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_G/0/1/0/all/0/1&quot;&gt;Guixiang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1&quot;&gt;Ming Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1&quot;&gt;Sihong Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04028">
<title>Measuring the Success of Diffusion Models at Imitating Human Artists. (arXiv:2307.04028v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04028</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern diffusion models have set the state-of-the-art in AI image generation.
Their success is due, in part, to training on Internet-scale data which often
includes copyrighted work. This prompts questions about the extent to which
these models learn from, imitate, or copy the work of human artists. This work
suggests that tying copyright liability to the capabilities of the model may be
useful given the evolving ecosystem of generative models. Specifically, much of
the legal analysis of copyright and generative systems focuses on the use of
protected data for training. As a result, the connections between data,
training, and the system are often obscured. In our approach, we consider
simple image classification techniques to measure a model&apos;s ability to imitate
specific artists. Specifically, we use Contrastive Language-Image Pretrained
(CLIP) encoders to classify images in a zero-shot fashion. Our process first
prompts a model to imitate a specific artist. Then, we test whether CLIP can be
used to reclassify the artist (or the artist&apos;s work) from the imitation. If
these tests match the imitation back to the original artist, this suggests the
model can imitate that artist&apos;s expression. Our approach is simple and
quantitative. Furthermore, it uses standard techniques and does not require
additional training. We demonstrate our approach with an audit of Stable
Diffusion&apos;s capacity to imitate 70 professional digital artists with
copyrighted work online. When Stable Diffusion is prompted to imitate an artist
from this set, we find that the artist can be identified from the imitation
with an average accuracy of 81.0%. Finally, we also show that a sample of the
artist&apos;s work can be matched to these imitation images with a high degree of
statistical reliability. Overall, these results suggest that Stable Diffusion
is broadly successful at imitating individual human artists.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casper_S/0/1/0/all/0/1&quot;&gt;Stephen Casper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zifan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mogulothu_S/0/1/0/all/0/1&quot;&gt;Shreya Mogulothu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marinov_Z/0/1/0/all/0/1&quot;&gt;Zachary Marinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deshpande_C/0/1/0/all/0/1&quot;&gt;Chinmay Deshpande&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yew_R/0/1/0/all/0/1&quot;&gt;Rui-Jie Yew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1&quot;&gt;Zheng Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hadfield_Menell_D/0/1/0/all/0/1&quot;&gt;Dylan Hadfield-Menell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04033">
<title>Learning Variational Neighbor Labels for Test-Time Domain Generalization. (arXiv:2307.04033v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.04033</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper strives for domain generalization, where models are trained
exclusively on source domains before being deployed at unseen target domains.
We follow the strict separation of source training and target testing but
exploit the value of the unlabeled target data itself during inference. We make
three contributions. First, we propose probabilistic pseudo-labeling of target
samples to generalize the source-trained model to the target domain at test
time. We formulate the generalization at test time as a variational inference
problem by modeling pseudo labels as distributions to consider the uncertainty
during generalization and alleviate the misleading signal of inaccurate pseudo
labels. Second, we learn variational neighbor labels that incorporate the
information of neighboring target samples to generate more robust pseudo
labels. Third, to learn the ability to incorporate more representative target
information and generate more precise and robust variational neighbor labels,
we introduce a meta-generalization stage during training to simulate the
generalization procedure. Experiments on six widely-used datasets demonstrate
the benefits, abilities, and effectiveness of our proposal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ambekar_S/0/1/0/all/0/1&quot;&gt;Sameer Ambekar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1&quot;&gt;Zehao Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1&quot;&gt;Jiayi Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1&quot;&gt;Xiantong Zhen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1&quot;&gt;Cees G. M. Snoek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04036">
<title>Designing a Direct Feedback Loop between Humans and Convolutional Neural Networks through Local Explanations. (arXiv:2307.04036v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2307.04036</link>
<description rdf:parseType="Literal">&lt;p&gt;The local explanation provides heatmaps on images to explain how
Convolutional Neural Networks (CNNs) derive their output. Due to its visual
straightforwardness, the method has been one of the most popular explainable AI
(XAI) methods for diagnosing CNNs. Through our formative study (S1), however,
we captured ML engineers&apos; ambivalent perspective about the local explanation as
a valuable and indispensable envision in building CNNs versus the process that
exhausts them due to the heuristic nature of detecting vulnerability. Moreover,
steering the CNNs based on the vulnerability learned from the diagnosis seemed
highly challenging. To mitigate the gap, we designed DeepFuse, the first
interactive design that realizes the direct feedback loop between a user and
CNNs in diagnosing and revising CNN&apos;s vulnerability using local explanations.
DeepFuse helps CNN engineers to systemically search &quot;unreasonable&quot; local
explanations and annotate the new boundaries for those identified as
unreasonable in a labor-efficient manner. Next, it steers the model based on
the given annotation such that the model doesn&apos;t introduce similar mistakes. We
conducted a two-day study (S2) with 12 experienced CNN engineers. Using
DeepFuse, participants made a more accurate and &quot;reasonable&quot; model than the
current state-of-the-art. Also, participants found the way DeepFuse guides
case-based reasoning can practically improve their current practice. We provide
implications for design that explain how future HCI-driven design can move our
practice forward to make XAI-driven insights more actionable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1&quot;&gt;Tong Steven Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yuyang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khaladkar_S/0/1/0/all/0/1&quot;&gt;Shubham Khaladkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sijia Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Liang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Young-Ho Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1&quot;&gt;Sungsoo Ray Hong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04042">
<title>Sup-Norm Convergence of Deep Neural Network Estimator for Nonparametric Regression by Adversarial Training. (arXiv:2307.04042v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2307.04042</link>
<description rdf:parseType="Literal">&lt;p&gt;We show the sup-norm convergence of deep neural network estimators with a
novel adversarial training scheme. For the nonparametric regression problem, it
has been shown that an estimator using deep neural networks can achieve better
performances in the sense of the $L2$-norm. In contrast, it is difficult for
the neural estimator with least-squares to achieve the sup-norm convergence,
due to the deep structure of neural network models. In this study, we develop
an adversarial training scheme and investigate the sup-norm convergence of deep
neural network estimators. First, we find that ordinary adversarial training
makes neural estimators inconsistent. Second, we show that a deep neural
network estimator achieves the optimal rate in the sup-norm sense by the
proposed adversarial training with correction. We extend our adversarial
training to general setups of a loss function and a data-generating function.
Our experiments support the theoretical findings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Imaizumi_M/0/1/0/all/0/1&quot;&gt;Masaaki Imaizumi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04049">
<title>Parallel Algorithms Align with Neural Execution. (arXiv:2307.04049v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.04049</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural algorithmic reasoners are parallel processors. Teaching them
sequential algorithms contradicts this nature, rendering a significant share of
their computations redundant. Parallel algorithms however may exploit their
full computational power, therefore requiring fewer layers to be executed. This
drastically reduces training times, as we observe when comparing parallel
implementations of searching, sorting and finding strongly connected components
to their sequential counterparts on the CLRS framework. Additionally, parallel
versions achieve strongly superior predictive performance in most cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Engelmayer_V/0/1/0/all/0/1&quot;&gt;Valerie Engelmayer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Georgiev_D/0/1/0/all/0/1&quot;&gt;Dobrik Georgiev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Velickovic_P/0/1/0/all/0/1&quot;&gt;Petar Veli&amp;#x10d;kovi&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04050">
<title>Optimization-based Learning for Dynamic Load Planning in Trucking Service Networks. (arXiv:2307.04050v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.04050</link>
<description rdf:parseType="Literal">&lt;p&gt;The load planning problem is a critical challenge in service network design
for parcel carriers: it decides how many trailers (or loads) to assign for
dispatch over time between pairs of terminals. Another key challenge is to
determine a flow plan, which specifies how parcel volumes are assigned to
planned loads. This paper considers the Dynamic Load Planning Problem (DLPP)
that considers both flow and load planning challenges jointly to adjust loads
and flows as the demand forecast changes over time before the day of
operations. The paper aims at developing a decision-support tool to inform
planners making these decisions at terminals across the network. The paper
formulates the DLPP as a MIP and shows that it admits a large number of
symmetries in a network where each commodity can be routed through primary and
alternate paths. As a result, an optimization solver may return fundamentally
different solutions to closely related problems, confusing planners and
reducing trust in optimization. To remedy this limitation, the paper proposes a
Goal-Directed Optimization that eliminates those symmetries by generating
optimal solutions staying close to a reference plan. The paper also proposes an
optimization proxy to address the computational challenges of the optimization
models. The proxy combines a machine learning model and a feasibility
restoration model and finds solutions that satisfy real-time constraints
imposed by planners-in-the-loop. An extensive computational study on industrial
instances shows that the optimization proxy is around 10 times faster than the
commercial solver in obtaining the same quality solutions and orders of
magnitude faster for generating solutions that are consistent with each other.
The proposed approach also demonstrates the benefits of the DLPP for load
consolidation, and the significant savings obtained from combining machine
learning and optimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ojha_R/0/1/0/all/0/1&quot;&gt;Ritesh Ojha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenbo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hanyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khir_R/0/1/0/all/0/1&quot;&gt;Reem Khir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erera_A/0/1/0/all/0/1&quot;&gt;Alan Erera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hentenryck_P/0/1/0/all/0/1&quot;&gt;Pascal Van Hentenryck&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04052">
<title>Learning to Group Auxiliary Datasets for Molecule. (arXiv:2307.04052v1 [q-bio.BM])</title>
<link>http://arxiv.org/abs/2307.04052</link>
<description rdf:parseType="Literal">&lt;p&gt;The limited availability of annotations in small molecule datasets presents a
challenge to machine learning models. To address this, one common strategy is
to collaborate with additional auxiliary datasets. However, having more data
does not always guarantee improvements. Negative transfer can occur when the
knowledge in the target dataset differs or contradicts that of the auxiliary
molecule datasets. In light of this, identifying the auxiliary molecule
datasets that can benefit the target dataset when jointly trained remains a
critical and unresolved problem. Through an empirical analysis, we observe that
combining graph structure similarity and task similarity can serve as a more
reliable indicator for identifying high-affinity auxiliary datasets. Motivated
by this insight, we propose MolGroup, which separates the dataset affinity into
task and structure affinity to predict the potential benefits of each auxiliary
molecule dataset. MolGroup achieves this by utilizing a routing mechanism
optimized through a bi-level optimization framework. Empowered by the meta
gradient, the routing mechanism is optimized toward maximizing the target
dataset&apos;s performance and quantifies the affinity as the gating score. As a
result, MolGroup is capable of predicting the optimal combination of auxiliary
datasets for each target dataset. Our extensive experiments demonstrate the
efficiency and effectiveness of MolGroup, showing an average improvement of
4.41%/3.47% for GIN/Graphormer trained with the group of molecule datasets
selected by MolGroup on 11 target molecule datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tinglin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Ziniu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ying_R/0/1/0/all/0/1&quot;&gt;Rex Ying&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04055">
<title>Contextual Dynamic Pricing with Strategic Buyers. (arXiv:2307.04055v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2307.04055</link>
<description rdf:parseType="Literal">&lt;p&gt;Personalized pricing, which involves tailoring prices based on individual
characteristics, is commonly used by firms to implement a consumer-specific
pricing policy. In this process, buyers can also strategically manipulate their
feature data to obtain a lower price, incurring certain manipulation costs.
Such strategic behavior can hinder firms from maximizing their profits. In this
paper, we study the contextual dynamic pricing problem with strategic buyers.
The seller does not observe the buyer&apos;s true feature, but a manipulated feature
according to buyers&apos; strategic behavior. In addition, the seller does not
observe the buyers&apos; valuation of the product, but only a binary response
indicating whether a sale happens or not. Recognizing these challenges, we
propose a strategic dynamic pricing policy that incorporates the buyers&apos;
strategic behavior into the online learning to maximize the seller&apos;s cumulative
revenue. We first prove that existing non-strategic pricing policies that
neglect the buyers&apos; strategic behavior result in a linear $\Omega(T)$ regret
with $T$ the total time horizon, indicating that these policies are not better
than a random pricing policy. We then establish that our proposed policy
achieves a sublinear regret upper bound of $O(\sqrt{T})$. Importantly, our
policy is not a mere amalgamation of existing dynamic pricing policies and
strategic behavior handling algorithms. Our policy can also accommodate the
scenario when the marginal cost of manipulation is unknown in advance. To
account for it, we simultaneously estimate the valuation parameter and the cost
parameter in the online pricing policy, which is shown to also achieve an
$O(\sqrt{T})$ regret bound. Extensive experiments support our theoretical
developments and demonstrate the superior performance of our policy compared to
other pricing policies that are unaware of the strategic behaviors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_P/0/1/0/all/0/1&quot;&gt;Pangpang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhuoran Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhaoran Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Will Wei Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04056">
<title>Manifold Filter-Combine Networks. (arXiv:2307.04056v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2307.04056</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a large class of manifold neural networks (MNNs) which we call
Manifold Filter-Combine Networks. This class includes as special cases, the
MNNs considered in previous work by Wang, Ruiz, and Ribeiro, the manifold
scattering transform (a wavelet-based model of neural networks), and other
interesting examples not previously considered in the literature such as the
manifold equivalent of Kipf and Welling&apos;s graph convolutional network. We then
consider a method, based on building a data-driven graph, for implementing such
networks when one does not have global knowledge of the manifold, but merely
has access to finitely many sample points. We provide sufficient conditions for
the network to provably converge to its continuum limit as the number of sample
points tends to infinity. Unlike previous work (which focused on specific MNN
architectures and graph constructions), our rate of convergence does not
explicitly depend on the number of filters used. Moreover, it exhibits linear
dependence on the depth of the network rather than the exponential dependence
obtained previously.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chew_J/0/1/0/all/0/1&quot;&gt;Joyce Chew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Brouwer_E/0/1/0/all/0/1&quot;&gt;Edward De Brouwer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Krishnaswamy_S/0/1/0/all/0/1&quot;&gt;Smita Krishnaswamy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Needell_D/0/1/0/all/0/1&quot;&gt;Deanna Needell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Perlmutter_M/0/1/0/all/0/1&quot;&gt;Michael Perlmutter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04057">
<title>Bidirectional Attention as a Mixture of Continuous Word Experts. (arXiv:2307.04057v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.04057</link>
<description rdf:parseType="Literal">&lt;p&gt;Bidirectional attention $\unicode{x2013}$ composed of self-attention with
positional encodings and the masked language model (MLM) objective
$\unicode{x2013}$ has emerged as a key component of modern large language
models (LLMs). Despite its empirical success, few studies have examined its
statistical underpinnings: What statistical model is bidirectional attention
implicitly fitting? What sets it apart from its non-attention predecessors? We
explore these questions in this paper. The key observation is that fitting a
single-layer single-head bidirectional attention, upon reparameterization, is
equivalent to fitting a continuous bag of words (CBOW) model with
mixture-of-experts (MoE) weights. Further, bidirectional attention with
multiple heads and multiple layers is equivalent to stacked MoEs and a mixture
of MoEs, respectively. This statistical viewpoint reveals the distinct use of
MoE in bidirectional attention, which aligns with its practical effectiveness
in handling heterogeneous data. It also suggests an immediate extension to
categorical tabular data, if we view each word location in a sentence as a
tabular feature. Across empirical studies, we find that this extension
outperforms existing tabular extensions of transformers in out-of-distribution
(OOD) generalization. Finally, this statistical perspective of bidirectional
attention enables us to theoretically characterize when linear word analogies
are present in its word embeddings. These analyses show that bidirectional
attention can require much stronger assumptions to exhibit linear word
analogies than its non-attention predecessors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wibisono_K/0/1/0/all/0/1&quot;&gt;Kevin Christian Wibisono&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yixin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04065">
<title>Large-scale global optimization of ultra-high dimensional non-convex landscapes based on generative neural networks. (arXiv:2307.04065v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.04065</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a non-convex optimization algorithm metaheuristic, based on the
training of a deep generative network, which enables effective searching within
continuous, ultra-high dimensional landscapes. During network training,
populations of sampled local gradients are utilized within a customized loss
function to evolve the network output distribution function towards one peak at
high-performing optima. The deep network architecture is tailored to support
progressive growth over the course of training, which allows the algorithm to
manage the curse of dimensionality characteristic of high-dimensional
landscapes. We apply our concept to a range of standard optimization problems
with dimensions as high as one thousand and show that our method performs
better with fewer function evaluations compared to state-of-the-art algorithm
benchmarks. We also discuss the role of deep network over-parameterization,
loss function engineering, and proper network architecture selection in
optimization, and why the required batch size of sampled local gradients is
independent of problem dimension. These concepts form the foundation for a new
class of algorithms that utilize customizable and expressive deep generative
networks to solve non-convex optimization problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jiaqi Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1&quot;&gt;Jonathan A. Fan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04075">
<title>Multi-Head Attention Mechanism Learning for Cancer New Subtypes and Treatment Based on Cancer Multi-Omics Data. (arXiv:2307.04075v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.04075</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the high heterogeneity and clinical characteristics of cancer, there
are significant differences in multi-omics data and clinical features among
subtypes of different cancers. Therefore, the identification and discovery of
cancer subtypes are crucial for the diagnosis, treatment, and prognosis of
cancer. In this study, we proposed a generalization framework based on
attention mechanisms for unsupervised contrastive learning (AMUCL) to analyze
cancer multi-omics data for the identification and characterization of cancer
subtypes. AMUCL framework includes a unsupervised multi-head attention
mechanism, which deeply extracts multi-omics data features. Importantly, a
decoupled contrastive learning model (DMACL) based on a multi-head attention
mechanism is proposed to learn multi-omics data features and clusters and
identify new cancer subtypes. This unsupervised contrastive learning method
clusters subtypes by calculating the similarity between samples in the feature
space and sample space of multi-omics data. Compared to 11 other deep learning
models, the DMACL model achieved a C-index of 0.002, a Silhouette score of
0.801, and a Davies Bouldin Score of 0.38 on a single-cell multi-omics dataset.
On a cancer multi-omics dataset, the DMACL model obtained a C-index of 0.016, a
Silhouette score of 0.688, and a Davies Bouldin Score of 0.46, and obtained the
most reliable cancer subtype clustering results for each type of cancer.
Finally, we used the DMACL model in the AMUCL framework to reveal six cancer
subtypes of AML. By analyzing the GO functional enrichment, subtype-specific
biological functions, and GSEA of AML, we further enhanced the interpretability
of cancer subtype analysis based on the generalizable AMUCL framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1&quot;&gt;Liangrui Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dazhen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1&quot;&gt;Yutao Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1&quot;&gt;Zhichao Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rong_P/0/1/0/all/0/1&quot;&gt;Pengfei Rong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Liwen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1&quot;&gt;Shaoliang Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04077">
<title>Towards Fast and Scalable Private Inference. (arXiv:2307.04077v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2307.04077</link>
<description rdf:parseType="Literal">&lt;p&gt;Privacy and security have rapidly emerged as first order design constraints.
Users now demand more protection over who can see their data (confidentiality)
as well as how it is used (control). Here, existing cryptographic techniques
for security fall short: they secure data when stored or communicated but must
decrypt it for computation. Fortunately, a new paradigm of computing exists,
which we refer to as privacy-preserving computation (PPC). Emerging PPC
technologies can be leveraged for secure outsourced computation or to enable
two parties to compute without revealing either users&apos; secret data. Despite
their phenomenal potential to revolutionize user protection in the digital age,
the realization has been limited due to exorbitant computational,
communication, and storage overheads.
&lt;/p&gt;
&lt;p&gt;This paper reviews recent efforts on addressing various PPC overheads using
private inference (PI) in neural network as a motivating application. First,
the problem and various technologies, including homomorphic encryption (HE),
secret sharing (SS), garbled circuits (GCs), and oblivious transfer (OT), are
introduced. Next, a characterization of their overheads when used to implement
PI is covered. The characterization motivates the need for both GCs and HE
accelerators. Then two solutions are presented: HAAC for accelerating GCs and
RPU for accelerating HE. To conclude, results and effects are shown with a
discussion on what future work is needed to overcome the remaining overheads of
PI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mo_J/0/1/0/all/0/1&quot;&gt;Jianqiao Mo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garimella_K/0/1/0/all/0/1&quot;&gt;Karthik Garimella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neda_N/0/1/0/all/0/1&quot;&gt;Negar Neda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ebel_A/0/1/0/all/0/1&quot;&gt;Austin Ebel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reagen_B/0/1/0/all/0/1&quot;&gt;Brandon Reagen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04081">
<title>Score-based Conditional Generation with Fewer Labeled Data by Self-calibrating Classifier Guidance. (arXiv:2307.04081v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04081</link>
<description rdf:parseType="Literal">&lt;p&gt;Score-based Generative Models (SGMs) are a popular family of deep generative
models that achieves leading image generation quality. Earlier studies have
extended SGMs to tackle class-conditional generation by coupling an
unconditional SGM with the guidance of a trained classifier. Nevertheless, such
classifier-guided SGMs do not always achieve accurate conditional generation,
especially when trained with fewer labeled data. We argue that the issue is
rooted in unreliable gradients of the classifier and the inability to fully
utilize unlabeled data during training. We then propose to improve
classifier-guided SGMs by letting the classifier calibrate itself. Our key idea
is to use principles from energy-based models to convert the classifier as
another view of the unconditional SGM. Then, existing loss for the
unconditional SGM can be adopted to calibrate the classifier using both labeled
and unlabeled data. Empirical results validate that the proposed approach
significantly improves the conditional generation quality across different
percentages of labeled data. The improved performance makes the proposed
approach consistently superior to other conditional SGMs when using fewer
labeled data. The results confirm the potential of the proposed approach for
generative modeling with limited labeled data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1&quot;&gt;Paul Kuo-Ming Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Si-An Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1&quot;&gt;Hsuan-Tien Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04090">
<title>DebateKG: Automatic Policy Debate Case Creation with Semantic Knowledge Graphs. (arXiv:2307.04090v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.04090</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work within the Argument Mining community has shown the applicability
of Natural Language Processing systems for solving problems found within
competitive debate. One of the most important tasks within competitive debate
is for debaters to create high quality debate cases. We show that effective
debate cases can be constructed using constrained shortest path traversals on
Argumentative Semantic Knowledge Graphs. We study this potential in the context
of a type of American Competitive Debate, called Policy Debate, which already
has a large scale dataset targeting it called DebateSum. We significantly
improve upon DebateSum by introducing 53180 new examples, as well as further
useful metadata for every example, to the dataset. We leverage the txtai
semantic search and knowledge graph toolchain to produce and contribute 9
semantic knowledge graphs built on this dataset. We create a unique method for
evaluating which knowledge graphs are better in the context of producing policy
debate cases. A demo which automatically generates debate cases, along with all
other code and the Knowledge Graphs, are open-sourced and made available to the
public here: https://github.com/Hellisotherpeople/DebateKG
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roush_A/0/1/0/all/0/1&quot;&gt;Allen Roush&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04093">
<title>Properly Learning Decision Trees with Queries Is NP-Hard. (arXiv:2307.04093v1 [cs.CC])</title>
<link>http://arxiv.org/abs/2307.04093</link>
<description rdf:parseType="Literal">&lt;p&gt;We prove that it is NP-hard to properly PAC learn decision trees with
queries, resolving a longstanding open problem in learning theory (Bshouty
1993; Guijarro-Lavin-Raghavan 1999; Mehta-Raghavan 2002; Feldman 2016). While
there has been a long line of work, dating back to (Pitt-Valiant 1988),
establishing the hardness of properly learning decision trees from random
examples, the more challenging setting of query learners necessitates different
techniques and there were no previous lower bounds. En route to our main
result, we simplify and strengthen the best known lower bounds for a different
problem of Decision Tree Minimization (Zantema-Bodlaender 2000; Sieling 2003).
&lt;/p&gt;
&lt;p&gt;On a technical level, we introduce the notion of hardness distillation, which
we study for decision tree complexity but can be considered for any complexity
measure: for a function that requires large decision trees, we give a general
method for identifying a small set of inputs that is responsible for its
complexity. Our technique even rules out query learners that are allowed
constant error. This contrasts with existing lower bounds for the setting of
random examples which only hold for inverse-polynomial error.
&lt;/p&gt;
&lt;p&gt;Our result, taken together with a recent almost-polynomial time query
algorithm for properly learning decision trees under the uniform distribution
(Blanc-Lange-Qiao-Tan 2022), demonstrates the dramatic impact of distributional
assumptions on the problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koch_C/0/1/0/all/0/1&quot;&gt;Caleb Koch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strassle_C/0/1/0/all/0/1&quot;&gt;Carmen Strassle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1&quot;&gt;Li-Yang Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04094">
<title>Class-Incremental Mixture of Gaussians for Deep Continual Learning. (arXiv:2307.04094v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.04094</link>
<description rdf:parseType="Literal">&lt;p&gt;Continual learning models for stationary data focus on learning and retaining
concepts coming to them in a sequential manner. In the most generic
class-incremental environment, we have to be ready to deal with classes coming
one by one, without any higher-level grouping. This requirement invalidates
many previously proposed methods and forces researchers to look for more
flexible alternative approaches. In this work, we follow the idea of
centroid-driven methods and propose end-to-end incorporation of the mixture of
Gaussians model into the continual learning framework. By employing the
gradient-based approach and designing losses capable of learning discriminative
features while avoiding degenerate solutions, we successfully combine the
mixture model with a deep feature extractor allowing for joint optimization and
adjustments in the latent space. Additionally, we show that our model can
effectively learn in memory-free scenarios with fixed extractors. In the
conducted experiments, we empirically demonstrate the effectiveness of the
proposed solutions and exhibit the competitiveness of our model when compared
with state-of-the-art continual learning baselines evaluated in the context of
image classification problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korycki_L/0/1/0/all/0/1&quot;&gt;Lukasz Korycki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krawczyk_B/0/1/0/all/0/1&quot;&gt;Bartosz Krawczyk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04097">
<title>Restricted Generative Projection for One-Class Classification and Anomaly Detection. (arXiv:2307.04097v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.04097</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a simple framework for one-class classification and anomaly
detection. The core idea is to learn a mapping to transform the unknown
distribution of training (normal) data to a known target distribution.
Crucially, the target distribution should be sufficiently simple, compact, and
informative. The simplicity is to ensure that we can sample from the
distribution easily, the compactness is to ensure that the decision boundary
between normal data and abnormal data is clear and reliable, and the
informativeness is to ensure that the transformed data preserve the important
information of the original data. Therefore, we propose to use truncated
Gaussian, uniform in hypersphere, uniform on hypersphere, or uniform between
hyperspheres, as the target distribution. We then minimize the distance between
the transformed data distribution and the target distribution while keeping the
reconstruction error for the original data small enough. Comparative studies on
multiple benchmark datasets verify the effectiveness of our methods in
comparison to baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_F/0/1/0/all/0/1&quot;&gt;Feng Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1&quot;&gt;Ruoyu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1&quot;&gt;Jicong Fan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04099">
<title>GNP Attack: Transferable Adversarial Examples via Gradient Norm Penalty. (arXiv:2307.04099v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.04099</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial examples (AE) with good transferability enable practical
black-box attacks on diverse target models, where insider knowledge about the
target models is not required. Previous methods often generate AE with no or
very limited transferability; that is, they easily overfit to the particular
architecture and feature representation of the source, white-box model and the
generated AE barely work for target, black-box models. In this paper, we
propose a novel approach to enhance AE transferability using Gradient Norm
Penalty (GNP). It drives the loss function optimization procedure to converge
to a flat region of local optima in the loss landscape. By attacking 11
state-of-the-art (SOTA) deep learning models and 6 advanced defense methods, we
empirically show that GNP is very effective in generating AE with high
transferability. We also demonstrate that it is very flexible in that it can be
easily integrated with other gradient based methods for stronger transfer-based
attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1&quot;&gt;Tie Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wunsch_D/0/1/0/all/0/1&quot;&gt;Donald C. Wunsch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04102">
<title>A generative flow for conditional sampling via optimal transport. (arXiv:2307.04102v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2307.04102</link>
<description rdf:parseType="Literal">&lt;p&gt;Sampling conditional distributions is a fundamental task for Bayesian
inference and density estimation. Generative models, such as normalizing flows
and generative adversarial networks, characterize conditional distributions by
learning a transport map that pushes forward a simple reference (e.g., a
standard Gaussian) to a target distribution. While these approaches
successfully describe many non-Gaussian problems, their performance is often
limited by parametric bias and the reliability of gradient-based (adversarial)
optimizers to learn these transformations. This work proposes a non-parametric
generative model that iteratively maps reference samples to the target. The
model uses block-triangular transport maps, whose components are shown to
characterize conditionals of the target distribution. These maps arise from
solving an optimal transport problem with a weighted $L^2$ cost function,
thereby extending the data-driven approach in [Trigila and Tabak, 2016] for
conditional sampling. The proposed approach is demonstrated on a two
dimensional example and on a parameter inference problem involving nonlinear
ODEs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Alfonso_J/0/1/0/all/0/1&quot;&gt;Jason Alfonso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Baptista_R/0/1/0/all/0/1&quot;&gt;Ricardo Baptista&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bhakta_A/0/1/0/all/0/1&quot;&gt;Anupam Bhakta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gal_N/0/1/0/all/0/1&quot;&gt;Noam Gal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hou_A/0/1/0/all/0/1&quot;&gt;Alfin Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lyubimova_I/0/1/0/all/0/1&quot;&gt;Isa Lyubimova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pocklington_D/0/1/0/all/0/1&quot;&gt;Daniel Pocklington&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sajonz_J/0/1/0/all/0/1&quot;&gt;Josef Sajonz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Trigila_G/0/1/0/all/0/1&quot;&gt;Giulio Trigila&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tsai_R/0/1/0/all/0/1&quot;&gt;Ryan Tsai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04105">
<title>Towards Assumption-free Bias Mitigation. (arXiv:2307.04105v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.04105</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the impressive prediction ability, machine learning models show
discrimination towards certain demographics and suffer from unfair prediction
behaviors. To alleviate the discrimination, extensive studies focus on
eliminating the unequal distribution of sensitive attributes via multiple
approaches. However, due to privacy concerns, sensitive attributes are often
either unavailable or missing in real-world scenarios. Therefore, several
existing works alleviate the bias without sensitive attributes. Those studies
face challenges, either in inaccurate predictions of sensitive attributes or
the need to mitigate unequal distribution of manually defined non-sensitive
attributes related to bias. The latter requires strong assumptions about the
correlation between sensitive and non-sensitive attributes. As data
distribution and task goals vary, the strong assumption on non-sensitive
attributes may not be valid and require domain expertise. In this work, we
propose an assumption-free framework to detect the related attributes
automatically by modeling feature interaction for bias mitigation. The proposed
framework aims to mitigate the unfair impact of identified biased feature
interactions. Experimental results on four real-world datasets demonstrate that
our proposed framework can significantly alleviate unfair prediction behaviors
by considering biased feature interactions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1&quot;&gt;Chia-Yuan Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1&quot;&gt;Yu-Neng Chuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_K/0/1/0/all/0/1&quot;&gt;Kwei-Herng Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xiaotian Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xia Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_N/0/1/0/all/0/1&quot;&gt;Na Zou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04110">
<title>Learning Space-Time Continuous Neural PDEs from Partially Observed States. (arXiv:2307.04110v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.04110</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a novel grid-independent model for learning partial differential
equations (PDEs) from noisy and partial observations on irregular
spatiotemporal grids. We propose a space-time continuous latent neural PDE
model with an efficient probabilistic framework and a novel encoder design for
improved data efficiency and grid independence. The latent state dynamics are
governed by a PDE model that combines the collocation method and the method of
lines. We employ amortized variational inference for approximate posterior
estimation and utilize a multiple shooting technique for enhanced training
speed and stability. Our model demonstrates state-of-the-art performance on
complex synthetic and real-world datasets, overcoming limitations of previous
approaches and effectively handling partially-observed data. The proposed model
outperforms recent methods, showing its potential to advance data-driven PDE
modeling and enabling robust, grid-independent modeling of complex
partially-observed dynamic processes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iakovlev_V/0/1/0/all/0/1&quot;&gt;Valerii Iakovlev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heinonen_M/0/1/0/all/0/1&quot;&gt;Markus Heinonen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lahdesmaki_H/0/1/0/all/0/1&quot;&gt;Harri L&amp;#xe4;hdesm&amp;#xe4;ki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04114">
<title>FILM: How can Few-Shot Image Classification Benefit from Pre-Trained Language Models?. (arXiv:2307.04114v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.04114</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot learning aims to train models that can be generalized to novel
classes with only a few samples. Recently, a line of works are proposed to
enhance few-shot learning with accessible semantic information from class
names. However, these works focus on improving existing modules such as visual
prototypes and feature extractors of the standard few-shot learning framework.
This limits the full potential use of semantic information. In this paper, we
propose a novel few-shot learning framework that uses pre-trained language
models based on contrastive learning. To address the challenge of alignment
between visual features and textual embeddings obtained from text-based
pre-trained language model, we carefully design the textual branch of our
framework and introduce a metric module to generalize the cosine similarity.
For better transferability, we let the metric module adapt to different
few-shot tasks and adopt MAML to train the model via bi-level optimization.
Moreover, we conduct extensive experiments on multiple benchmarks to
demonstrate the effectiveness of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zihao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dang_Y/0/1/0/all/0/1&quot;&gt;Yunkai Dang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_D/0/1/0/all/0/1&quot;&gt;Dong Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huishuai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Weiran Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04121">
<title>A Deep Learning Framework for Solving Hyperbolic Partial Differential Equations: Part I. (arXiv:2307.04121v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.04121</link>
<description rdf:parseType="Literal">&lt;p&gt;Physics informed neural networks (PINNs) have emerged as a powerful tool to
provide robust and accurate approximations of solutions to partial differential
equations (PDEs). However, PINNs face serious difficulties and challenges when
trying to approximate PDEs with dominant hyperbolic character. This research
focuses on the development of a physics informed deep learning framework to
approximate solutions to nonlinear PDEs that can develop shocks or
discontinuities without any a-priori knowledge of the solution or the location
of the discontinuities. The work takes motivation from finite element method
that solves for solution values at nodes in the discretized domain and use
these nodal values to obtain a globally defined solution field. Built on the
rigorous mathematical foundations of the discontinuous Galerkin method, the
framework naturally handles imposition of boundary conditions
(Neumann/Dirichlet), entropy conditions, and regularity requirements. Several
numerical experiments and validation with analytical solutions demonstrate the
accuracy, robustness, and effectiveness of the proposed framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_R/0/1/0/all/0/1&quot;&gt;Rajat Arora&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04131">
<title>Carbon-Efficient Neural Architecture Search. (arXiv:2307.04131v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.04131</link>
<description rdf:parseType="Literal">&lt;p&gt;This work presents a novel approach to neural architecture search (NAS) that
aims to reduce energy costs and increase carbon efficiency during the model
design process. The proposed framework, called carbon-efficient NAS (CE-NAS),
consists of NAS evaluation algorithms with different energy requirements, a
multi-objective optimizer, and a heuristic GPU allocation strategy. CE-NAS
dynamically balances energy-efficient sampling and energy-consuming evaluation
tasks based on current carbon emissions. Using a recent NAS benchmark dataset
and two carbon traces, our trace-driven simulations demonstrate that CE-NAS
achieves better carbon and search efficiency than the three baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yiyang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1&quot;&gt;Tian Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04138">
<title>On The Impact of Machine Learning Randomness on Group Fairness. (arXiv:2307.04138v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.04138</link>
<description rdf:parseType="Literal">&lt;p&gt;Statistical measures for group fairness in machine learning reflect the gap
in performance of algorithms across different groups. These measures, however,
exhibit a high variance between different training instances, which makes them
unreliable for empirical evaluation of fairness. What causes this high
variance? We investigate the impact on group fairness of different sources of
randomness in training neural networks. We show that the variance in group
fairness measures is rooted in the high volatility of the learning process on
under-represented groups. Further, we recognize the dominant source of
randomness as the stochasticity of data order during training. Based on these
findings, we show how one can control group-level accuracy (i.e., model
fairness), with high efficiency and negligible impact on the model&apos;s overall
performance, by simply changing the data order for a single epoch.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganesh_P/0/1/0/all/0/1&quot;&gt;Prakhar Ganesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1&quot;&gt;Hongyan Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strobel_M/0/1/0/all/0/1&quot;&gt;Martin Strobel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shokri_R/0/1/0/all/0/1&quot;&gt;Reza Shokri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04147">
<title>A Survey and Approach to Chart Classification. (arXiv:2307.04147v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04147</link>
<description rdf:parseType="Literal">&lt;p&gt;Charts represent an essential source of visual information in documents and
facilitate a deep understanding and interpretation of information typically
conveyed numerically. In the scientific literature, there are many charts, each
with its stylistic differences. Recently the document understanding community
has begun to address the problem of automatic chart understanding, which begins
with chart classification. In this paper, we present a survey of the current
state-of-the-art techniques for chart classification and discuss the available
datasets and their supported chart types. We broadly classify these
contributions as traditional approaches based on ML, CNN, and Transformers.
Furthermore, we carry out an extensive comparative performance analysis of
CNN-based and transformer-based approaches on the recently published CHARTINFO
UB-UNITECH PMC dataset for the CHART-Infographics competition at ICPR 2022. The
data set includes 15 different chart categories, including 22,923 training
images and 13,260 test images. We have implemented a vision-based transformer
model that produces state-of-the-art results in chart classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhote_A/0/1/0/all/0/1&quot;&gt;Anurag Dhote&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Javed_M/0/1/0/all/0/1&quot;&gt;Mohammed Javed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doermann_D/0/1/0/all/0/1&quot;&gt;David S Doermann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04149">
<title>Latent Graph Attention for Enhanced Spatial Context. (arXiv:2307.04149v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04149</link>
<description rdf:parseType="Literal">&lt;p&gt;Global contexts in images are quite valuable in image-to-image translation
problems. Conventional attention-based and graph-based models capture the
global context to a large extent, however, these are computationally expensive.
Moreover, the existing approaches are limited to only learning the pairwise
semantic relation between any two points on the image. In this paper, we
present Latent Graph Attention (LGA) a computationally inexpensive (linear to
the number of nodes) and stable, modular framework for incorporating the global
context in the existing architectures, especially empowering small-scale
architectures to give performance closer to large size architectures, thus
making the light-weight architectures more useful for edge devices with lower
compute power and lower energy needs. LGA propagates information spatially
using a network of locally connected graphs, thereby facilitating to construct
a semantically coherent relation between any two spatially distant points that
also takes into account the influence of the intermediate pixels. Moreover, the
depth of the graph network can be used to adapt the extent of contextual spread
to the target dataset, thereby being able to explicitly control the added
computational cost. To enhance the learning mechanism of LGA, we also introduce
a novel contrastive loss term that helps our LGA module to couple well with the
original architecture at the expense of minimal additional computational load.
We show that incorporating LGA improves the performance on three challenging
applications, namely transparent object segmentation, image restoration for
dehazing and optical flow estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Ayush Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhambhu_Y/0/1/0/all/0/1&quot;&gt;Yash Bhambhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buckchash_H/0/1/0/all/0/1&quot;&gt;Himanshu Buckchash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1&quot;&gt;Deepak K. Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prasad_D/0/1/0/all/0/1&quot;&gt;Dilip K. Prasad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04191">
<title>On the sample complexity of estimation in logistic regression. (arXiv:2307.04191v1 [math.ST])</title>
<link>http://arxiv.org/abs/2307.04191</link>
<description rdf:parseType="Literal">&lt;p&gt;The logistic regression model is one of the most popular data generation
model in noisy binary classification problems. In this work, we study the
sample complexity of estimating the parameters of the logistic regression model
up to a given $\ell_2$ error, in terms of the dimension and the inverse
temperature, with standard normal covariates. The inverse temperature controls
the signal-to-noise ratio of the data generation process. While both
generalization bounds and asymptotic performance of the maximum-likelihood
estimator for logistic regression are well-studied, the non-asymptotic sample
complexity that shows the dependence on error and the inverse temperature for
parameter estimation is absent from previous analyses. We show that the sample
complexity curve has two change-points (or critical points) in terms of the
inverse temperature, clearly separating the low, moderate, and high temperature
regimes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hsu_D/0/1/0/all/0/1&quot;&gt;Daniel Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Mazumdar_A/0/1/0/all/0/1&quot;&gt;Arya Mazumdar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04204">
<title>Trajectory Alignment: Understanding the Edge of Stability Phenomenon via Bifurcation Theory. (arXiv:2307.04204v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.04204</link>
<description rdf:parseType="Literal">&lt;p&gt;Cohen et al. (2021) empirically study the evolution of the largest eigenvalue
of the loss Hessian, also known as sharpness, along the gradient descent (GD)
trajectory and observe a phenomenon called the Edge of Stability (EoS). The
sharpness increases at the early phase of training (referred to as progressive
sharpening), and eventually saturates close to the threshold of $2 /
\text{(step size)}$. In this paper, we start by demonstrating through empirical
studies that when the EoS phenomenon occurs, different GD trajectories (after a
proper reparameterization) align on a specific bifurcation diagram independent
of initialization. We then rigorously prove this trajectory alignment
phenomenon for a two-layer fully-connected linear network and a single-neuron
nonlinear network trained with a single data point. Our trajectory alignment
analysis establishes both progressive sharpening and EoS phenomena,
encompassing and extending recent findings in the literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1&quot;&gt;Minhak Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_C/0/1/0/all/0/1&quot;&gt;Chulhee Yun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04205">
<title>Extending the Forward Forward Algorithm. (arXiv:2307.04205v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.04205</link>
<description rdf:parseType="Literal">&lt;p&gt;The Forward Forward algorithm, proposed by Geoffrey Hinton in November 2022,
is a novel method for training neural networks as an alternative to
backpropagation. In this project, we replicate Hinton&apos;s experiments on the
MNIST dataset, and subsequently extend the scope of the method with two
significant contributions. First, we establish a baseline performance for the
Forward Forward network on the IMDb movie reviews dataset. As far as we know,
our results on this sentiment analysis task marks the first instance of the
algorithm&apos;s extension beyond computer vision. Second, we introduce a novel
pyramidal optimization strategy for the loss threshold - a hyperparameter
specific to the Forward Forward method. Our pyramidal approach shows that a
good thresholding strategy causes a difference of upto 8% in test error. 1
Lastly, we perform visualizations of the trained parameters and derived several
significant insights, such as a notably larger (10-20x) mean and variance in
the weights acquired by the Forward Forward network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gandhi_S/0/1/0/all/0/1&quot;&gt;Saumya Gandhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gala_R/0/1/0/all/0/1&quot;&gt;Ritu Gala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kornberg_J/0/1/0/all/0/1&quot;&gt;Jonah Kornberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sridhar_A/0/1/0/all/0/1&quot;&gt;Advaith Sridhar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04208">
<title>On the Challenges of Deploying Privacy-Preserving Synthetic Data in the Enterprise. (arXiv:2307.04208v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.04208</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative AI technologies are gaining unprecedented popularity, causing a
mix of excitement and apprehension through their remarkable capabilities. In
this paper, we study the challenges associated with deploying synthetic data, a
subfield of Generative AI. Our focus centers on enterprise deployment, with an
emphasis on privacy concerns caused by the vast amount of personal and highly
sensitive data. We identify 40+ challenges and systematize them into five main
groups -- i) generation, ii) infrastructure &amp;amp; architecture, iii) governance,
iv) compliance &amp;amp; regulation, and v) adoption. Additionally, we discuss a
strategic and systematic approach that enterprises can employ to effectively
address the challenges and achieve their goals by establishing trust in the
implemented solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arthur_L/0/1/0/all/0/1&quot;&gt;Lauren Arthur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Costello_J/0/1/0/all/0/1&quot;&gt;Jason Costello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hardy_J/0/1/0/all/0/1&quot;&gt;Jonathan Hardy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OBrien_W/0/1/0/all/0/1&quot;&gt;Will O&amp;#x27;Brien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rea_J/0/1/0/all/0/1&quot;&gt;James Rea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rees_G/0/1/0/all/0/1&quot;&gt;Gareth Rees&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganev_G/0/1/0/all/0/1&quot;&gt;Georgi Ganev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04210">
<title>Investigating the Edge of Stability Phenomenon in Reinforcement Learning. (arXiv:2307.04210v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.04210</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent progress has been made in understanding optimisation dynamics in
neural networks trained with full-batch gradient descent with momentum with the
uncovering of the edge of stability phenomenon in supervised learning. The edge
of stability phenomenon occurs as the leading eigenvalue of the Hessian reaches
the divergence threshold of the underlying optimisation algorithm for a
quadratic loss, after which it starts oscillating around the threshold, and the
loss starts to exhibit local instability but decreases over long time frames.
In this work, we explore the edge of stability phenomenon in reinforcement
learning (RL), specifically off-policy Q-learning algorithms across a variety
of data regimes, from offline to online RL. Our experiments reveal that,
despite significant differences to supervised learning, such as
non-stationarity of the data distribution and the use of bootstrapping, the
edge of stability phenomenon can be present in off-policy deep RL. Unlike
supervised learning, however, we observe strong differences depending on the
underlying loss, with DQN -- using a Huber loss -- showing a strong edge of
stability effect that we do not observe with C51 -- using a cross entropy loss.
Our results suggest that, while neural network structure can lead to
optimisation dynamics that transfer between problem domains, certain aspects of
deep RL optimisation can differentiate it from domains such as supervised
learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iordan_R/0/1/0/all/0/1&quot;&gt;Rares Iordan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deisenroth_M/0/1/0/all/0/1&quot;&gt;Marc Peter Deisenroth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosca_M/0/1/0/all/0/1&quot;&gt;Mihaela Rosca&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04215">
<title>Generalized Action-based Ball Recovery Model using 360$^\circ$ data. (arXiv:2307.04215v1 [stat.AP])</title>
<link>http://arxiv.org/abs/2307.04215</link>
<description rdf:parseType="Literal">&lt;p&gt;Even though having more possession does not necessarily lead to winning,
teams like Manchester City, Liverpool, and Leeds United notably have tried to
recover the ball quickly after they lost it over the past few years. Nowadays,
some of the top managers in the world apply high-pressing styles, and concepts
such as the five-second rule, usually credited to Guardiola, have been
spreading out [9][10], becoming a fundamental part of how lots of teams have
played over the recent years. Expressions like &quot;don&apos;t let them breathe&quot; and
&quot;get the ball back as soon as possible&quot; are often heard in the media [4][5][6],
but what are the actions that most lead to a change in possession? What is the
influence of a team&apos;s positioning on the ball recovery? Which are the players
that more often collapse when under pressure? Can we evaluate the defensive
dynamics of teams that do not necessarily press the player in possession as
intensely as those mentioned above? We try to answer those and other questions
in this paper by creating a Generalized Action based Ball Recovery model (GABR)
using Statsbomb 360$^\circ$ data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nascimento_R/0/1/0/all/0/1&quot;&gt;Ricardo Furbino Marques do Nascimento&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rios_Neto_H/0/1/0/all/0/1&quot;&gt;Hugo M. R. Rios-Neto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04216">
<title>Hierarchical Autoencoder-based Lossy Compression for Large-scale High-resolution Scientific Data. (arXiv:2307.04216v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.04216</link>
<description rdf:parseType="Literal">&lt;p&gt;Lossy compression has become an important technique to reduce data size in
many domains. This type of compression is especially valuable for large-scale
scientific data, whose size ranges up to several petabytes. Although
Autoencoder-based models have been successfully leveraged to compress images
and videos, such neural networks have not widely gained attention in the
scientific data domain. Our work presents a neural network that not only
significantly compresses large-scale scientific data but also maintains high
reconstruction quality. The proposed model is tested with scientific benchmark
data available publicly and applied to a large-scale high-resolution climate
modeling data set. Our model achieves a compression ratio of 140 on several
benchmark data sets without compromising the reconstruction quality. Simulation
data from the High-Resolution Community Earth System Model (CESM) Version 1.3
over 500 years are also being compressed with a compression ratio of 200 while
the reconstruction error is negligible for scientific analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1&quot;&gt;Hieu Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_H/0/1/0/all/0/1&quot;&gt;Hernan Santos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1&quot;&gt;Jian Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04228">
<title>Efficient Bayesian travel-time tomography with geologically-complex priors using sensitivity-informed polynomial chaos expansion and deep generative networks. (arXiv:2307.04228v1 [physics.geo-ph])</title>
<link>http://arxiv.org/abs/2307.04228</link>
<description rdf:parseType="Literal">&lt;p&gt;Monte Carlo Markov Chain (MCMC) methods commonly confront two fundamental
challenges: the accurate characterization of the prior distribution and the
efficient evaluation of the likelihood. In the context of Bayesian studies on
tomography, principal component analysis (PCA) can in some cases facilitate the
straightforward definition of the prior distribution, while simultaneously
enabling the implementation of accurate surrogate models based on polynomial
chaos expansion (PCE) to replace computationally intensive full-physics forward
solvers. When faced with scenarios where PCA does not offer a direct means of
easily defining the prior distribution alternative methods like deep generative
models (e.g., variational autoencoders (VAEs)), can be employed as viable
options. However, accurately producing a surrogate capable of capturing the
intricate non-linear relationship between the latent parameters of a VAE and
the outputs of forward modeling presents a notable challenge. Indeed, while PCE
models provide high accuracy when the input-output relationship can be
effectively approximated by relatively low-degree multivariate polynomials,
this condition is typically unmet when utilizing latent variables derived from
deep generative models. In this contribution, we present a strategy that
combines the excellent reconstruction performances of VAE in terms of prio
representation with the accuracy of PCA-PCE surrogate modeling in the context
of Bayesian ground penetrating radar (GPR) travel-time tomography. Within the
MCMC process, the parametrization of the VAE is leveraged for prior exploration
and sample proposal. Concurrently, modeling is conducted using PCE, which
operates on either globally or locally defined principal components of the VAE
samples under examination.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Meles_G/0/1/0/all/0/1&quot;&gt;Giovanni Angelo Meles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Amaya_M/0/1/0/all/0/1&quot;&gt;Macarena Amaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Levy_S/0/1/0/all/0/1&quot;&gt;Shiran Levy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Marelli_S/0/1/0/all/0/1&quot;&gt;Stefano Marelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Linde_N/0/1/0/all/0/1&quot;&gt;Niklas Linde&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04251">
<title>ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey. (arXiv:2307.04251v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.04251</link>
<description rdf:parseType="Literal">&lt;p&gt;ChatGPT is a large language model (LLM) created by OpenAI that has been
carefully trained on a large amount of data. It has revolutionized the field of
natural language processing (NLP) and has pushed the boundaries of LLM
capabilities. ChatGPT has played a pivotal role in enabling widespread public
interaction with generative artificial intelligence (GAI) on a large scale. It
has also sparked research interest in developing similar technologies and
investigating their applications and implications. In this paper, our primary
goal is to provide a concise survey on the current lines of research on ChatGPT
and its evolution. We considered both the glass box and black box views of
ChatGPT, encompassing the components and foundational elements of the
technology, as well as its applications, impacts, and implications. The glass
box approach focuses on understanding the inner workings of the technology, and
the black box approach embraces it as a complex system, and thus examines its
inputs, outputs, and effects. This paves the way for a comprehensive
exploration of the technology and provides a road map for further research and
experimentation. We also lay out essential foundational literature on LLMs and
GAI in general and their connection with ChatGPT. This overview sheds light on
existing and missing research lines in the emerging field of LLMs, benefiting
both public users and developers. Furthermore, the paper delves into the broad
spectrum of applications and significant concerns in fields such as education,
research, healthcare, finance, etc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohamadi_S/0/1/0/all/0/1&quot;&gt;Salman Mohamadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mujtaba_G/0/1/0/all/0/1&quot;&gt;Ghulam Mujtaba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1&quot;&gt;Ngan Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doretto_G/0/1/0/all/0/1&quot;&gt;Gianfranco Doretto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adjeroh_D/0/1/0/all/0/1&quot;&gt;Donald A. Adjeroh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04274">
<title>Assessing the efficacy of large language models in generating accurate teacher responses. (arXiv:2307.04274v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.04274</link>
<description rdf:parseType="Literal">&lt;p&gt;(Tack et al., 2023) organized the shared task hosted by the 18th Workshop on
Innovative Use of NLP for Building Educational Applications on generation of
teacher language in educational dialogues. Following the structure of the
shared task, in this study, we attempt to assess the generative abilities of
large language models in providing informative and helpful insights to
students, thereby simulating the role of a knowledgeable teacher. To this end,
we present an extensive evaluation of several benchmarking generative models,
including GPT-4 (few-shot, in-context learning), fine-tuned GPT-2, and
fine-tuned DialoGPT. Additionally, to optimize for pedagogical quality, we
fine-tuned the Flan-T5 model using reinforcement learning. Our experimental
findings on the Teacher-Student Chatroom Corpus subset indicate the efficacy of
GPT-4 over other fine-tuned models, measured using BERTScore and DialogRPT.
&lt;/p&gt;
&lt;p&gt;We hypothesize that several dataset characteristics, including sampling,
representativeness, and dialog completeness, pose significant challenges to
fine-tuning, thus contributing to the poor generalizability of the fine-tuned
models. Finally, we note the need for these generative models to be evaluated
with a metric that relies not only on dialog coherence and matched language
modeling distribution but also on the model&apos;s ability to showcase pedagogical
skills.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hicke_Y/0/1/0/all/0/1&quot;&gt;Yann Hicke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Masand_A/0/1/0/all/0/1&quot;&gt;Abhishek Masand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1&quot;&gt;Wentao Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gangavarapu_T/0/1/0/all/0/1&quot;&gt;Tushaar Gangavarapu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04287">
<title>Generalizing Graph ODE for Learning Complex System Dynamics across Environments. (arXiv:2307.04287v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.04287</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning multi-agent system dynamics has been extensively studied for various
real-world applications, such as molecular dynamics in biology. Most of the
existing models are built to learn single system dynamics from observed
historical data and predict the future trajectory. In practice, however, we
might observe multiple systems that are generated across different
environments, which differ in latent exogenous factors such as temperature and
gravity. One simple solution is to learn multiple environment-specific models,
but it fails to exploit the potential commonalities among the dynamics across
environments and offers poor prediction results where per-environment data is
sparse or limited. Here, we present GG-ODE (Generalized Graph Ordinary
Differential Equations), a machine learning framework for learning continuous
multi-agent system dynamics across environments. Our model learns system
dynamics using neural ordinary differential equations (ODE) parameterized by
Graph Neural Networks (GNNs) to capture the continuous interaction among
agents. We achieve the model generalization by assuming the dynamics across
different environments are governed by common physics laws that can be captured
via learning a shared ODE function. The distinct latent exogenous factors
learned for each environment are incorporated into the ODE function to account
for their differences. To improve model performance, we additionally design two
regularization losses to (1) enforce the orthogonality between the learned
initial states and exogenous factors via mutual information minimization; and
(2) reduce the temporal variance of learned exogenous factors within the same
system via contrastive learning. Experiments over various physical simulations
show that our model can accurately predict system dynamics, especially in the
long range, and can generalize well to new systems with few observations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zijie Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yizhou Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wei Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04298">
<title>Edge Storage Management Recipe with Zero-Shot Data Compression for Road Anomaly Detection. (arXiv:2307.04298v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2307.04298</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies show edge computing-based road anomaly detection systems which
may also conduct data collection simultaneously. However, the edge computers
will have small data storage but we need to store the collected audio samples
for a long time in order to update existing models or develop a novel method.
Therefore, we should consider an approach for efficient storage management
methods while preserving high-fidelity audio. A hardware-perspective approach,
such as using a low-resolution microphone, is an intuitive way to reduce file
size but is not recommended because it fundamentally cuts off high-frequency
components. On the other hand, a computational file compression approach that
encodes collected high-resolution audio into a compact code should be
recommended because it also provides a corresponding decoding method. Motivated
by this, we propose a way of simple yet effective pre-trained autoencoder-based
data compression method. The pre-trained autoencoder is trained for the purpose
of audio super-resolution so it can be utilized to encode or decode any
arbitrary sampling rate. Moreover, it will reduce the communication cost for
data transmission from the edge to the central server. Via the comparative
experiments, we confirm that the zero-shot audio compression and decompression
highly preserve anomaly detection performance while enhancing storage and
transmission efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1&quot;&gt;YeongHyeon Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gim_U/0/1/0/all/0/1&quot;&gt;Uju Gim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Myung Jin Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04305">
<title>Automatic Piano Transcription with Hierarchical Frequency-Time Transformer. (arXiv:2307.04305v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2307.04305</link>
<description rdf:parseType="Literal">&lt;p&gt;Taking long-term spectral and temporal dependencies into account is essential
for automatic piano transcription. This is especially helpful when determining
the precise onset and offset for each note in the polyphonic piano content. In
this case, we may rely on the capability of self-attention mechanism in
Transformers to capture these long-term dependencies in the frequency and time
axes. In this work, we propose hFT-Transformer, which is an automatic music
transcription method that uses a two-level hierarchical frequency-time
Transformer architecture. The first hierarchy includes a convolutional block in
the time axis, a Transformer encoder in the frequency axis, and a Transformer
decoder that converts the dimension in the frequency axis. The output is then
fed into the second hierarchy which consists of another Transformer encoder in
the time axis. We evaluated our method with the widely used MAPS and MAESTRO
v3.0.0 datasets, and it demonstrated state-of-the-art performance on all the
F1-scores of the metrics among Frame, Note, Note with Offset, and Note with
Offset and Velocity estimations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toyama_K/0/1/0/all/0/1&quot;&gt;Keisuke Toyama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akama_T/0/1/0/all/0/1&quot;&gt;Taketo Akama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ikemiya_Y/0/1/0/all/0/1&quot;&gt;Yukara Ikemiya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takida_Y/0/1/0/all/0/1&quot;&gt;Yuhta Takida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1&quot;&gt;Wei-Hsiang Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitsufuji_Y/0/1/0/all/0/1&quot;&gt;Yuki Mitsufuji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1908.08016">
<title>Testing Robustness Against Unforeseen Adversaries. (arXiv:1908.08016v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1908.08016</link>
<description rdf:parseType="Literal">&lt;p&gt;When considering real-world adversarial settings, defenders are unlikely to
have access to the full range of deployment-time adversaries during training,
and adversaries are likely to use realistic adversarial distortions that will
not be limited to small L_p-constrained perturbations. To narrow in on this
discrepancy between research and reality we introduce eighteen novel
adversarial attacks, which we use to create ImageNet-UA, a new benchmark for
evaluating model robustness against a wide range of unforeseen adversaries. We
make use of our benchmark to identify a range of defense strategies which can
help overcome this generalization gap, finding a rich space of techniques which
can improve unforeseen robustness. We hope the greater variety and realism of
ImageNet-UA will make it a useful tool for those working on real-world
worst-case robustness, enabling development of more robust defenses which can
generalize beyond attacks seen during training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaufmann_M/0/1/0/all/0/1&quot;&gt;Max Kaufmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1&quot;&gt;Daniel Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yi Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basart_S/0/1/0/all/0/1&quot;&gt;Steven Basart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1&quot;&gt;Xuwang Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazeika_M/0/1/0/all/0/1&quot;&gt;Mantas Mazeika&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1&quot;&gt;Akul Arora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dziedzic_A/0/1/0/all/0/1&quot;&gt;Adam Dziedzic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boenisch_F/0/1/0/all/0/1&quot;&gt;Franziska Boenisch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_T/0/1/0/all/0/1&quot;&gt;Tom Brown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1&quot;&gt;Jacob Steinhardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1&quot;&gt;Dan Hendrycks&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2007.07302">
<title>Optimal Learning for Structured Bandits. (arXiv:2007.07302v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2007.07302</link>
<description rdf:parseType="Literal">&lt;p&gt;We study structured multi-armed bandits, which is the problem of online
decision-making under uncertainty in the presence of structural information. In
this problem, the decision-maker needs to discover the best course of action
despite observing only uncertain rewards over time. The decision-maker is aware
of certain convex structural information regarding the reward distributions;
that is, the decision-maker knows the reward distributions of the arms belong
to a convex compact set. In the presence such structural information, they then
would like to minimize their regret by exploiting this information, where the
regret is its performance difference against a benchmark policy that knows the
best action ahead of time. In the absence of structural information, the
classical upper confidence bound (UCB) and Thomson sampling algorithms are well
known to suffer minimal regret. As recently pointed out, neither algorithms
are, however, capable of exploiting structural information that is commonly
available in practice. We propose a novel learning algorithm that we call
&quot;DUSA&quot; whose regret matches the information-theoretic regret lower bound up to
a constant factor and can handle a wide range of structural information. Our
algorithm DUSA solves a dual counterpart of the regret lower bound at the
empirical reward distribution and follows its suggested play. We show that this
idea leads to the first computationally viable learning policy with asymptotic
minimal regret for various structural information, including well-known
structured bandits such as linear, Lipschitz, and convex bandits, and novel
structured bandits that have not been studied in the literature due to the lack
of a unified and flexible framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parys_B/0/1/0/all/0/1&quot;&gt;Bart P.G. Van Parys&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golrezaei_N/0/1/0/all/0/1&quot;&gt;Negin Golrezaei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2011.12177">
<title>Automated Detection of Double Nuclei Galaxies using GOTHIC and the Discovery of a Large Sample of Dual AGN. (arXiv:2011.12177v3 [astro-ph.GA] UPDATED)</title>
<link>http://arxiv.org/abs/2011.12177</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel algorithm to detect double nuclei galaxies (DNG) called
GOTHIC (Graph BOosted iterated HIll Climbing) - that detects whether a given
image of a galaxy has two or more closely separated nuclei. Our aim is to
detect samples of dual or multiple active galactic nuclei (AGN) in galaxies.
Although galaxy mergers are common, the detection of dual AGN is rare. Their
detection is very important as they help us understand the formation of
supermassive black hole (SMBH) binaries, SMBH growth and AGN feedback effects
in multiple nuclei systems. There is thus a need for an algorithm to do a
systematic survey of existing imaging data for the discovery of DNGs and dual
AGN. We have tested GOTHIC on a known sample of DNGs and subsequently applied
it to a sample of a million SDSS DR16 galaxies lying in the redshift range of 0
to 0.75 approximately, and have available spectroscopic data. We have detected
159 dual AGN in this sample, of which 2 are triple AGN systems. Our results
show that dual AGN are not common, and triple AGN even rarer. The color (u-r)
magnitude plots of the DNGs indicate that star formation is quenched as the
nuclei come closer and as the AGN fraction increases. The quenching is
especially prominent for dual/triple AGN galaxies that lie in the extreme end
of the red sequence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Bhattacharya_A/0/1/0/all/0/1&quot;&gt;Anwesh Bhattacharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+P%2E_N/0/1/0/all/0/1&quot;&gt;Nehal C. P.&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Das_M/0/1/0/all/0/1&quot;&gt;Mousumi Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Paswan_A/0/1/0/all/0/1&quot;&gt;Abhishek Paswan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Saha_S/0/1/0/all/0/1&quot;&gt;Snehanshu Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Combes_F/0/1/0/all/0/1&quot;&gt;Francoise Combes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2012.03774">
<title>Learning to extrapolate using continued fractions: Predicting the critical temperature of superconductor materials. (arXiv:2012.03774v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2012.03774</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of Artificial Intelligence (AI) and Machine Learning (ML), the
approximation of unknown target functions $y=f(\mathbf{x})$ using limited
instances $S={(\mathbf{x^{(i)}},y^{(i)})}$, where $\mathbf{x^{(i)}} \in D$ and
$D$ represents the domain of interest, is a common objective. We refer to $S$
as the training set and aim to identify a low-complexity mathematical model
that can effectively approximate this target function for new instances
$\mathbf{x}$. Consequently, the model&apos;s generalization ability is evaluated on
a separate set $T=\{\mathbf{x^{(j)}}\} \subset D$, where $T \neq S$, frequently
with $T \cap S = \emptyset$, to assess its performance beyond the training set.
However, certain applications require accurate approximation not only within
the original domain $D$ but also in an extended domain $D&apos;$ that encompasses
$D$. This becomes particularly relevant in scenarios involving the design of
new structures, where minimizing errors in approximations is crucial. For
example, when developing new materials through data-driven approaches, the
AI/ML system can provide valuable insights to guide the design process by
serving as a surrogate function. Consequently, the learned model can be
employed to facilitate the design of new laboratory experiments. In this paper,
we propose a method for multivariate regression based on iterative fitting of a
continued fraction, incorporating additive spline models. We compare the
performance of our method with established techniques, including AdaBoost,
Kernel Ridge, Linear Regression, Lasso Lars, Linear Support Vector Regression,
Multi-Layer Perceptrons, Random Forests, Stochastic Gradient Descent, and
XGBoost. To evaluate these methods, we focus on an important problem in the
field: predicting the critical temperature of superconductors based on
physical-chemical characteristics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moscato_P/0/1/0/all/0/1&quot;&gt;Pablo Moscato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haque_M/0/1/0/all/0/1&quot;&gt;Mohammad Nazmul Haque&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kevin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sloan_J/0/1/0/all/0/1&quot;&gt;Julia Sloan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliveira_J/0/1/0/all/0/1&quot;&gt;Jon C. de Oliveira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2102.04363">
<title>Efficient Data-Driven Optimization with Noisy Data. (arXiv:2102.04363v3 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2102.04363</link>
<description rdf:parseType="Literal">&lt;p&gt;Classical Kullback-Leibler or entropic distances are known to enjoy certain
desirable statistical properties in the context of decision-making with
noiseless data. However, in most practical situations the data available to a
decision maker is subject to a certain amount of measurement noise. We hence
study here data-driven prescription problems in which the data is corrupted by
a known noise source. We derive efficient data-driven formulations in this
noisy regime and indicate that they enjoy an entropic optimal transport
interpretation. Finally, we show that these efficient robust formulations are
tractable in several interesting settings by exploiting a classical
representation result by Strassen.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Parys_B/0/1/0/all/0/1&quot;&gt;Bart P.G. Van Parys&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2102.07060">
<title>Achieving Efficiency in Black Box Simulation of Distribution Tails with Self-structuring Importance Samplers. (arXiv:2102.07060v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2102.07060</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel Importance Sampling (IS) scheme for estimating
distribution tails of performance measures modeled with a rich set of tools
such as linear programs, integer linear programs, piecewise linear/quadratic
objectives, feature maps specified with deep neural networks, etc. The
conventional approach of explicitly identifying efficient changes of measure
suffers from feasibility and scalability concerns beyond highly stylized
models, due to their need to be tailored intricately to the objective and the
underlying probability distribution. This bottleneck is overcome in the
proposed scheme with an elementary transformation which is capable of
implicitly inducing an effective IS distribution in a variety of models by
replicating the concentration properties observed in less rare samples. This
novel approach is guided by developing a large deviations principle that brings
out the phenomenon of self-similarity of optimal IS distributions. The proposed
sampler is the first to attain asymptotically optimal variance reduction across
a spectrum of multivariate distributions despite being oblivious to the
specifics of the underlying model. Its applicability is illustrated with
contextual shortest path and portfolio credit risk models informed by neural
networks
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Deo_A/0/1/0/all/0/1&quot;&gt;Anand Deo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Murthy_K/0/1/0/all/0/1&quot;&gt;Karthyek Murthy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2103.03943">
<title>Novelty Detection in Sequential Data by Informed Clustering and Modeling. (arXiv:2103.03943v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2103.03943</link>
<description rdf:parseType="Literal">&lt;p&gt;Novelty detection in discrete sequences is a challenging task, since
deviations from the process generating the normal data are often small or
intentionally hidden. Novelties can be detected by modeling normal sequences
and measuring the deviations of a new sequence from the model predictions.
However, in many applications data is generated by several distinct processes
so that models trained on all the data tend to over-generalize and novelties
remain undetected. We propose to approach this challenge through decomposition:
by clustering the data we break down the problem, obtaining simpler modeling
task in each cluster which can be modeled more accurately. However, this comes
at a trade-off, since the amount of training data per cluster is reduced. This
is a particular problem for discrete sequences where state-of-the-art models
are data-hungry. The success of this approach thus depends on the quality of
the clustering, i.e., whether the individual learning problems are sufficiently
simpler than the joint problem. While clustering discrete sequences
automatically is a challenging and domain-specific task, it is often easy for
human domain experts, given the right tools. In this paper, we adapt a
state-of-the-art visual analytics tool for discrete sequence clustering to
obtain informed clusters from domain experts and use LSTMs to model each
cluster individually. Our extensive empirical evaluation indicates that this
informed clustering outperforms automatic ones and that our approach
outperforms state-of-the-art novelty detection methods for discrete sequences
in three real-world application scenarios. In particular, decomposition
outperforms a global model despite less training data on each individual
cluster.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adilova_L/0/1/0/all/0/1&quot;&gt;Linara Adilova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Siming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamp_M/0/1/0/all/0/1&quot;&gt;Michael Kamp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2104.12031">
<title>Low-rank Tensor Estimation via Riemannian Gauss-Newton: Statistical Optimality and Second-Order Convergence. (arXiv:2104.12031v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2104.12031</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we consider the estimation of a low Tucker rank tensor from a
number of noisy linear measurements. The general problem covers many specific
examples arising from applications, including tensor regression, tensor
completion, and tensor PCA/SVD. We consider an efficient Riemannian
Gauss-Newton (RGN) method for low Tucker rank tensor estimation. Different from
the generic (super)linear convergence guarantee of RGN in the literature, we
prove the first local quadratic convergence guarantee of RGN for low-rank
tensor estimation in the noisy setting under some regularity conditions and
provide the corresponding estimation error upper bounds. A deterministic
estimation error lower bound, which matches the upper bound, is provided that
demonstrates the statistical optimality of RGN. The merit of RGN is illustrated
through two machine learning applications: tensor regression and tensor SVD.
Finally, we provide the simulation results to corroborate our theoretical
findings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yuetian Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Anru R. Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2105.04738">
<title>Lightweight Distributed Gaussian Process Regression for Online Machine Learning. (arXiv:2105.04738v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2105.04738</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the problem where a group of agents aim to
collaboratively learn a common static latent function through streaming data.
We propose a lightweight distributed Gaussian process regression (GPR)
algorithm that is cognizant of agents&apos; limited capabilities in communication,
computation and memory. Each agent independently runs agent-based GPR using
local streaming data to predict test points of interest; then the agents
collaboratively execute distributed GPR to obtain global predictions over a
common sparse set of test points; finally, each agent fuses results from
distributed GPR with agent-based GPR to refine its predictions. By quantifying
the transient and steady-state performances in predictive variance and error,
we show that limited inter-agent communication improves learning performances
in the sense of Pareto. Monte Carlo simulation is conducted to evaluate the
developed algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1&quot;&gt;Zhenyuan Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1&quot;&gt;Minghui Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2107.13136">
<title>Insights from Generative Modeling for Neural Video Compression. (arXiv:2107.13136v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2107.13136</link>
<description rdf:parseType="Literal">&lt;p&gt;While recent machine learning research has revealed connections between deep
generative models such as VAEs and rate-distortion losses used in learned
compression, most of this work has focused on images. In a similar spirit, we
view recently proposed neural video coding algorithms through the lens of deep
autoregressive and latent variable modeling. We present these codecs as
instances of a generalized stochastic temporal autoregressive transform, and
propose new avenues for further improvements inspired by normalizing flows and
structured priors. We propose several architectures that yield state-of-the-art
video compression performance on high-resolution video and discuss their
tradeoffs and ablations. In particular, we propose (i) improved temporal
autoregressive transforms, (ii) improved entropy models with structured and
temporal dependencies, and (iii) variable bitrate versions of our algorithms.
Since our improvements are compatible with a large class of existing models, we
provide further evidence that the generative modeling viewpoint can advance the
neural video coding field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_R/0/1/0/all/0/1&quot;&gt;Ruihan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yibo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Marino_J/0/1/0/all/0/1&quot;&gt;Joseph Marino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mandt_S/0/1/0/all/0/1&quot;&gt;Stephan Mandt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2109.10476">
<title>Self-Supervised Learning to Prove Equivalence Between Straight-Line Programs via Rewrite Rules. (arXiv:2109.10476v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2109.10476</link>
<description rdf:parseType="Literal">&lt;p&gt;We target the problem of automatically synthesizing proofs of semantic
equivalence between two programs made of sequences of statements. We represent
programs using abstract syntax trees (AST), where a given set of
semantics-preserving rewrite rules can be applied on a specific AST pattern to
generate a transformed and semantically equivalent program. In our system, two
programs are equivalent if there exists a sequence of application of these
rewrite rules that leads to rewriting one program into the other. We propose a
neural network architecture based on a transformer model to generate proofs of
equivalence between program pairs. The system outputs a sequence of rewrites,
and the validity of the sequence is simply checked by verifying it can be
applied. If no valid sequence is produced by the neural network, the system
reports the programs as non-equivalent, ensuring by design no programs may be
incorrectly reported as equivalent. Our system is fully implemented for one
single grammar which can represent straight-line programs with function calls
and multiple types. To efficiently train the system to generate such sequences,
we develop an original incremental training technique, named self-supervised
sample selection. We extensively study the effectiveness of this novel training
approach on proofs of increasing complexity and length. Our system, S4Eq,
achieves 97% proof success on a curated dataset of 10,000 pairs of equivalent
programs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kommrusch_S/0/1/0/all/0/1&quot;&gt;Steve Kommrusch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monperrus_M/0/1/0/all/0/1&quot;&gt;Martin Monperrus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pouchet_L/0/1/0/all/0/1&quot;&gt;Louis-No&amp;#xeb;l Pouchet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.03360">
<title>Sparse MoEs meet Efficient Ensembles. (arXiv:2110.03360v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2110.03360</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning models based on the aggregated outputs of submodels, either
at the activation or prediction levels, often exhibit strong performance
compared to individual models. We study the interplay of two popular classes of
such models: ensembles of neural networks and sparse mixture of experts (sparse
MoEs). First, we show that the two approaches have complementary features whose
combination is beneficial. This includes a comprehensive evaluation of sparse
MoEs in uncertainty related benchmarks. Then, we present Efficient Ensemble of
Experts (E$^3$), a scalable and simple ensemble of sparse MoEs that takes the
best of both classes of models, while using up to 45% fewer FLOPs than a deep
ensemble. Extensive experiments demonstrate the accuracy, log-likelihood,
few-shot learning, robustness, and uncertainty improvements of E$^3$ over
several challenging vision Transformer-based baselines. E$^3$ not only
preserves its efficiency while scaling to models with up to 2.7B parameters,
but also provides better predictive performance and uncertainty estimates for
larger models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allingham_J/0/1/0/all/0/1&quot;&gt;James Urquhart Allingham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wenzel_F/0/1/0/all/0/1&quot;&gt;Florian Wenzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mariet_Z/0/1/0/all/0/1&quot;&gt;Zelda E Mariet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mustafa_B/0/1/0/all/0/1&quot;&gt;Basil Mustafa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puigcerver_J/0/1/0/all/0/1&quot;&gt;Joan Puigcerver&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1&quot;&gt;Neil Houlsby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jerfel_G/0/1/0/all/0/1&quot;&gt;Ghassen Jerfel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fortuin_V/0/1/0/all/0/1&quot;&gt;Vincent Fortuin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1&quot;&gt;Balaji Lakshminarayanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Snoek_J/0/1/0/all/0/1&quot;&gt;Jasper Snoek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1&quot;&gt;Dustin Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruiz_C/0/1/0/all/0/1&quot;&gt;Carlos Riquelme Ruiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jenatton_R/0/1/0/all/0/1&quot;&gt;Rodolphe Jenatton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.05854">
<title>A scalable and fast artificial neural network syndrome decoder for surface codes. (arXiv:2110.05854v5 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2110.05854</link>
<description rdf:parseType="Literal">&lt;p&gt;Surface code error correction offers a highly promising pathway to achieve
scalable fault-tolerant quantum computing. When operated as stabilizer codes,
surface code computations consist of a syndrome decoding step where measured
stabilizer operators are used to determine appropriate corrections for errors
in physical qubits. Decoding algorithms have undergone substantial development,
with recent work incorporating machine learning (ML) techniques. Despite
promising initial results, the ML-based syndrome decoders are still limited to
small scale demonstrations with low latency and are incapable of handling
surface codes with boundary conditions and various shapes needed for lattice
surgery and braiding. Here, we report the development of an artificial neural
network (ANN) based scalable and fast syndrome decoder capable of decoding
surface codes of arbitrary shape and size with data qubits suffering from the
depolarizing error model. Based on rigorous training over 50 million random
quantum error instances, our ANN decoder is shown to work with code distances
exceeding 1000 (more than 4 million physical qubits), which is the largest
ML-based decoder demonstration to-date. The established ANN decoder
demonstrates an execution time in principle independent of code distance,
implying that its implementation on dedicated hardware could potentially offer
surface code decoding times of O($\mu$sec), commensurate with the
experimentally realisable qubit coherence times. With the anticipated scale-up
of quantum processors within the next decade, their augmentation with a fast
and scalable syndrome decoder such as developed in our work is expected to play
a decisive role towards experimental implementation of fault-tolerant quantum
information processing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Gicev_S/0/1/0/all/0/1&quot;&gt;Spiro Gicev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Hollenberg_L/0/1/0/all/0/1&quot;&gt;Lloyd C. L. Hollenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Usman_M/0/1/0/all/0/1&quot;&gt;Muhammad Usman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.12468">
<title>SCORE: Spurious COrrelation REduction for Offline Reinforcement Learning. (arXiv:2110.12468v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2110.12468</link>
<description rdf:parseType="Literal">&lt;p&gt;Offline reinforcement learning (RL) harnesses the power of massive datasets
for resolving sequential decision problems. Most existing papers only discuss
defending against out-of-distribution (OOD) actions while we investigate a
broader issue, the spurious correlations between epistemic uncertainty and
decision-making, an essential factor that causes suboptimality. In this paper,
we propose Spurious COrrelation REduction (SCORE) for offline RL, a practically
effective and theoretically provable algorithm. We empirically show that SCORE
achieves the SoTA performance with 3.1x acceleration on various tasks in a
standard benchmark (D4RL). The proposed algorithm introduces an annealing
behavior cloning regularizer to help produce a high-quality estimation of
uncertainty which is critical for eliminating spurious correlations from
suboptimality. Theoretically, we justify the rationality of the proposed method
and prove its convergence to the optimal policy with a sublinear rate under
mild assumptions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1&quot;&gt;Zhihong Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1&quot;&gt;Zuyue Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lingxiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhuoran Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_C/0/1/0/all/0/1&quot;&gt;Chenjia Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1&quot;&gt;Tianyi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhaoran Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jing Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2111.09266">
<title>GFlowNet Foundations. (arXiv:2111.09266v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2111.09266</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Flow Networks (GFlowNets) have been introduced as a method to
sample a diverse set of candidates in an active learning context, with a
training objective that makes them approximately sample in proportion to a
given reward function. In this paper, we show a number of additional
theoretical properties of GFlowNets. They can be used to estimate joint
probability distributions and the corresponding marginal distributions where
some variables are unspecified and, of particular interest, can represent
distributions over composite objects like sets and graphs. GFlowNets amortize
the work typically done by computationally expensive MCMC methods in a single
but trained generative pass. They could also be used to estimate partition
functions and free energies, conditional probabilities of supersets
(supergraphs) given a subset (subgraph), as well as marginal distributions over
all supersets (supergraphs) of a given set (graph). We introduce variations
enabling the estimation of entropy and mutual information, sampling from a
Pareto frontier, connections to reward-maximizing policies, and extensions to
stochastic environments, continuous actions and modular energy functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lahlou_S/0/1/0/all/0/1&quot;&gt;Salem Lahlou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deleu_T/0/1/0/all/0/1&quot;&gt;Tristan Deleu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_E/0/1/0/all/0/1&quot;&gt;Edward J. Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tiwari_M/0/1/0/all/0/1&quot;&gt;Mo Tiwari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_E/0/1/0/all/0/1&quot;&gt;Emmanuel Bengio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2111.14778">
<title>Contextual Combinatorial Multi-output GP Bandits with Group Constraints. (arXiv:2111.14778v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2111.14778</link>
<description rdf:parseType="Literal">&lt;p&gt;In federated multi-armed bandit problems, maximizing global reward while
satisfying minimum privacy requirements to protect clients is the main goal. To
formulate such problems, we consider a combinatorial contextual bandit setting
with groups and changing action sets, where similar base arms arrive in groups
and a set of base arms, called a super arm, must be chosen in each round to
maximize super arm reward while satisfying the constraints of the rewards of
groups from which base arms were chosen. To allow for greater flexibility, we
let each base arm have two outcomes, modeled as the output of a two-output
Gaussian process (GP), where one outcome is used to compute super arm reward
and the other for group reward. We then propose a novel double-UCB GP-bandit
algorithm, called Thresholded Combinatorial Gaussian Process Upper Confidence
Bounds (TCGP-UCB), which balances between maximizing cumulative super arm
reward and satisfying group reward constraints and can be tuned to prefer one
over the other. We also define a new notion of regret that combines super arm
regret with group reward constraint satisfaction and prove that TCGP-UCB incurs
$\tilde{O}(\sqrt{\lambda^*(K)KT\overline{\gamma}_{T}} )$ regret with high
probability, where $\overline{\gamma}_{T}$ is the maximum information gain
associated with the set of base arm contexts that appeared in the first $T$
rounds and $K$ is the maximum super arm cardinality over all rounds. We lastly
show in experiments using synthetic and real-world data and based on a
federated learning setup as well as a content-recommendation one that our
algorithm performs better then the current non-GP state-of-the-art
combinatorial bandit algorithm, while satisfying group constraints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elahi_S/0/1/0/all/0/1&quot;&gt;Sepehr Elahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atalar_B/0/1/0/all/0/1&quot;&gt;Baran Atalar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ogut_S/0/1/0/all/0/1&quot;&gt;Sevda &amp;#xd6;&amp;#x11f;&amp;#xfc;t&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tekin_C/0/1/0/all/0/1&quot;&gt;Cem Tekin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.03626">
<title>Phase transitions in nonparametric regressions. (arXiv:2112.03626v6 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/2112.03626</link>
<description rdf:parseType="Literal">&lt;p&gt;When the unknown regression function of a single variable is known to have
derivatives up to the $(\gamma+1)$th order bounded in absolute values by a
common constant everywhere or a.e. (i.e., $(\gamma+1)$th degree of smoothness),
the minimax optimal rate of the mean integrated squared error (MISE) is stated
as $\left(\frac{1}{n}\right)^{\frac{2\gamma+2}{2\gamma+3}}$ in the literature.
This paper shows that: (i) if $n\leq\left(\gamma+1\right)^{2\gamma+3}$, the
minimax optimal MISE rate is $\frac{\log n}{n\log(\log n)}$ and the optimal
degree of smoothness to exploit is roughly $\max\left\{ \left\lfloor \frac{\log
n}{2\log\left(\log n\right)}\right\rfloor ,\,1\right\} $; (ii) if
$n&amp;gt;\left(\gamma+1\right)^{2\gamma+3}$, the minimax optimal MISE rate is
$\left(\frac{1}{n}\right)^{\frac{2\gamma+2}{2\gamma+3}}$ and the optimal degree
of smoothness to exploit is $\gamma+1$. The fundamental contribution of this
paper is a set of metric entropy bounds we develop for smooth function classes.
Some of our bounds are original, and some of them improve and/or generalize the
ones in the literature (e.g., Kolmogorov and Tikhomirov, 1959). Our metric
entropy bounds allow us to show phase transitions in the minimax optimal MISE
rates associated with some commonly seen smoothness classes as well as
non-standard smoothness classes, and can also be of independent interest
outside the nonparametric regression problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Ying Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.07344">
<title>SCORE: Approximating Curvature Information under Self-Concordant Regularization. (arXiv:2112.07344v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2112.07344</link>
<description rdf:parseType="Literal">&lt;p&gt;Optimization problems that include regularization functions in their
objectives are regularly solved in many applications. When one seeks
second-order methods for such problems, it may be desirable to exploit specific
properties of some of these regularization functions when accounting for
curvature information in the solution steps to speed up convergence. In this
paper, we propose the SCORE (self-concordant regularization) framework for
unconstrained minimization problems which incorporates second-order information
in the Newton-decrement framework for convex optimization. We propose the
generalized Gauss-Newton with Self-Concordant Regularization (GGN-SCORE)
algorithm that updates the minimization variables each time it receives a new
input batch. The proposed algorithm exploits the structure of the second-order
information in the Hessian matrix, thereby reducing computational overhead.
GGN-SCORE demonstrates how to speed up convergence while also improving model
generalization for problems that involve regularized minimization under the
proposed SCORE framework. Numerical experiments show the efficiency of our
method and its fast convergence, which compare favorably against baseline
first-order and quasi-Newton methods. Additional experiments involving
non-convex (overparameterized) neural network training problems show that the
proposed method is promising for non-convex optimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adeoye_A/0/1/0/all/0/1&quot;&gt;Adeyemi D. Adeoye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bemporad_A/0/1/0/all/0/1&quot;&gt;Alberto Bemporad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.05527">
<title>Federated Continual Learning for Socially Aware Robotics. (arXiv:2201.05527v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2201.05527</link>
<description rdf:parseType="Literal">&lt;p&gt;From learning assistance to companionship, social robots promise to enhance
many aspects of daily life. However, social robots have not seen widespread
adoption, in part because (1) they do not adapt their behavior to new users,
and (2) they do not provide sufficient privacy protections. Centralized
learning, whereby robots develop skills by gathering data on a server,
contributes to these limitations by preventing online learning of new
experiences and requiring storage of privacy-sensitive data. In this work, we
propose a decentralized learning alternative that improves the privacy and
personalization of social robots. We combine two machine learning approaches,
Federated Learning and Continual Learning, to capture interaction dynamics
distributed physically across robots and temporally across repeated robot
encounters. We define a set of criteria that should be balanced in
decentralized robot learning scenarios. We also develop a new algorithm --
Elastic Transfer -- that leverages importance-based regularization to preserve
relevant parameters across robots and interactions with multiple humans. We
show that decentralized learning is a viable alternative to centralized
learning in a proof-of-concept Socially-Aware Navigation domain, and
demonstrate how Elastic Transfer improves several of the proposed criteria.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guerdan_L/0/1/0/all/0/1&quot;&gt;Luke Guerdan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunes_H/0/1/0/all/0/1&quot;&gt;Hatice Gunes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.07286">
<title>Conservative Distributional Reinforcement Learning with Safety Constraints. (arXiv:2201.07286v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2201.07286</link>
<description rdf:parseType="Literal">&lt;p&gt;Safety exploration can be regarded as a constrained Markov decision problem
where the expected long-term cost is constrained. Previous off-policy
algorithms convert the constrained optimization problem into the corresponding
unconstrained dual problem by introducing the Lagrangian relaxation technique.
However, the cost function of the above algorithms provides inaccurate
estimations and causes the instability of the Lagrange multiplier learning. In
this paper, we present a novel off-policy reinforcement learning algorithm
called Conservative Distributional Maximum a Posteriori Policy Optimization
(CDMPO). At first, to accurately judge whether the current situation satisfies
the constraints, CDMPO adapts distributional reinforcement learning method to
estimate the Q-function and C-function. Then, CDMPO uses a conservative value
function loss to reduce the number of violations of constraints during the
exploration process. In addition, we utilize Weighted Average Proportional
Integral Derivative (WAPID) to update the Lagrange multiplier stably. Empirical
results show that the proposed method has fewer violations of constraints in
the early exploration process. The final test results also illustrate that our
method has better risk control.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hengrui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Youfang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1&quot;&gt;Sheng Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_K/0/1/0/all/0/1&quot;&gt;Kai Lv&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.11931">
<title>Fast Interpretable Greedy-Tree Sums. (arXiv:2201.11931v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2201.11931</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern machine learning has achieved impressive prediction performance, but
often sacrifices interpretability, a critical consideration in high-stakes
domains such as medicine. In such settings, practitioners often use highly
interpretable decision tree models, but these suffer from inductive bias
against additive structure. To overcome this bias, we propose Fast
Interpretable Greedy-Tree Sums (FIGS), which generalizes the CART algorithm to
simultaneously grow a flexible number of trees in summation. By combining
logical rules with addition, FIGS is able to adapt to additive structure while
remaining highly interpretable. Extensive experiments on real-world datasets
show that FIGS achieves state-of-the-art prediction performance. To demonstrate
the usefulness of FIGS in high-stakes domains, we adapt FIGS to learn clinical
decision instruments (CDIs), which are tools for guiding clinical
decision-making. Specifically, we introduce a variant of FIGS known as G-FIGS
that accounts for the heterogeneity in medical data. G-FIGS derives CDIs that
reflect domain knowledge and enjoy improved specificity (by up to 20% over
CART) without sacrificing sensitivity or interpretability. To provide further
insight into FIGS, we prove that FIGS learns components of additive models, a
property we refer to as disentanglement. Further, we show (under oracle
conditions) that unconstrained tree-sum models leverage disentanglement to
generalize more efficiently than single decision tree models when fitted to
additive regression functions. Finally, to avoid overfitting with an
unconstrained number of splits, we develop Bagging-FIGS, an ensemble version of
FIGS that borrows the variance reduction techniques of random forests.
Bagging-FIGS enjoys competitive performance with random forests and XGBoost on
real-world datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1&quot;&gt;Yan Shuo Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_C/0/1/0/all/0/1&quot;&gt;Chandan Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nasseri_K/0/1/0/all/0/1&quot;&gt;Keyan Nasseri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1&quot;&gt;Abhineet Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duncan_J/0/1/0/all/0/1&quot;&gt;James Duncan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ronen_O/0/1/0/all/0/1&quot;&gt;Omer Ronen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Epland_M/0/1/0/all/0/1&quot;&gt;Matthew Epland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kornblith_A/0/1/0/all/0/1&quot;&gt;Aaron Kornblith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Bin Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.11936">
<title>Consistent Collaborative Filtering via Tensor Decomposition. (arXiv:2201.11936v3 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2201.11936</link>
<description rdf:parseType="Literal">&lt;p&gt;Collaborative filtering is the de facto standard for analyzing users&apos;
activities and building recommendation systems for items. In this work we
develop Sliced Anti-symmetric Decomposition (SAD), a new model for
collaborative filtering based on implicit feedback. In contrast to traditional
techniques where a latent representation of users (user vectors) and items
(item vectors) are estimated, SAD introduces one additional latent vector to
each item, using a novel three-way tensor view of user-item interactions. This
new vector extends user-item preferences calculated by standard dot products to
general inner products, producing interactions between items when evaluating
their relative preferences. SAD reduces to state-of-the-art (SOTA)
collaborative filtering models when the vector collapses to 1, while in this
paper we allow its value to be estimated from data. Allowing the values of the
new item vector to be different from 1 has profound implications. It suggests
users may have nonlinear mental models when evaluating items, allowing the
existence of cycles in pairwise comparisons. We demonstrate the efficiency of
SAD in both simulated and real world datasets containing over 1M user-item
interactions. By comparing with seven SOTA collaborative filtering models with
implicit feedbacks, SAD produces the most consistent personalized preferences,
in the meanwhile maintaining top-level of accuracy in personalized
recommendations. We release the model and inference algorithms in a Python
library https://github.com/apple/ml-sad.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shiwen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crissman_C/0/1/0/all/0/1&quot;&gt;Charles Crissman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sapiro_G/0/1/0/all/0/1&quot;&gt;Guillermo R Sapiro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.05767">
<title>A PDE-Based Analysis of the Symmetric Two-Armed Bernoulli Bandit. (arXiv:2202.05767v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2202.05767</link>
<description rdf:parseType="Literal">&lt;p&gt;This work addresses a version of the two-armed Bernoulli bandit problem where
the sum of the means of the arms is one (the symmetric two-armed Bernoulli
bandit). In a regime where the gap between these means goes to zero and the
number of prediction periods approaches infinity, we obtain the leading order
terms of the minmax optimal regret and pseudoregret for this problem by
associating each of them with a solution of a linear heat equation. Our results
improve upon the previously known results; specifically, we explicitly compute
these leading order terms in three different scaling regimes for the gap.
Additionally, we obtain new non-asymptotic bounds for any given time horizon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kobzar_V/0/1/0/all/0/1&quot;&gt;Vladimir A. Kobzar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kohn_R/0/1/0/all/0/1&quot;&gt;Robert V. Kohn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.02372">
<title>Jump-Start Reinforcement Learning. (arXiv:2204.02372v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2204.02372</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning (RL) provides a theoretical framework for continuously
improving an agent&apos;s behavior via trial and error. However, efficiently
learning policies from scratch can be very difficult, particularly for tasks
with exploration challenges. In such settings, it might be desirable to
initialize RL with an existing policy, offline data, or demonstrations.
However, naively performing such initialization in RL often works poorly,
especially for value-based methods. In this paper, we present a meta algorithm
that can use offline data, demonstrations, or a pre-existing policy to
initialize an RL policy, and is compatible with any RL approach. In particular,
we propose Jump-Start Reinforcement Learning (JSRL), an algorithm that employs
two policies to solve tasks: a guide-policy, and an exploration-policy. By
using the guide-policy to form a curriculum of starting states for the
exploration-policy, we are able to efficiently improve performance on a set of
simulated robotic tasks. We show via experiments that JSRL is able to
significantly outperform existing imitation and reinforcement learning
algorithms, particularly in the small-data regime. In addition, we provide an
upper bound on the sample complexity of JSRL and show that with the help of a
guide-policy, one can improve the sample complexity for non-optimism
exploration methods from exponential in horizon to polynomial.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uchendu_I/0/1/0/all/0/1&quot;&gt;Ikechukwu Uchendu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1&quot;&gt;Ted Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1&quot;&gt;Banghua Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1&quot;&gt;Mengyuan Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simon_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9;phine Simon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bennice_M/0/1/0/all/0/1&quot;&gt;Matthew Bennice&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1&quot;&gt;Chuyuan Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Cong Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1&quot;&gt;Jiantao Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hausman_K/0/1/0/all/0/1&quot;&gt;Karol Hausman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.08620">
<title>Quantifying Spatial Under-reporting Disparities in Resident Crowdsourcing. (arXiv:2204.08620v3 [stat.AP] UPDATED)</title>
<link>http://arxiv.org/abs/2204.08620</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern city governance relies heavily on crowdsourcing (``co-production&apos;&apos;) to
identify problems such as downed trees and power-lines. A major concern is that
residents do not report problems at the same rates, with heterogeneous
reporting delays directly translating to downstream disparities in how quickly
incidents can be addressed. Measuring such under-reporting is a difficult
statistical task, as, by definition, we do not observe incidents that are not
reported or when reported incidents first occurred. Thus, low reporting rates
and low ground-truth incident rates cannot be naively distinguished, and
reporting delays are unobserved. We develop a method to identify
(heterogeneous) reporting delays, without using external ground truth data. Our
insight is that rates on \textit{duplicate} reports about the same incident can
be leveraged to disambiguate whether an incident has occurred with its
reporting rate once it has occurred. Using this idea, we reduce the question to
a standard Poisson rate estimation task -- even though the full incident
reporting interval is also unobserved.
&lt;/p&gt;
&lt;p&gt;We apply our method to over 100,000 resident reports made in New York City
and to over 900,000 reports made in Chicago, finding that there are substantial
spatial disparities in how quickly incidents are reported, even after
controlling for incident characteristics -- some neighborhoods report three
times as quickly as do others. These spatial disparities correspond to
socio-economic characteristics: in NYC, higher population density, fraction of
people with college degrees, income, and fraction of population that is White
all positively correlate with reporting rates.
&lt;/p&gt;
&lt;p&gt;Finally, leveraging a collaboration with the NYC Department of Parks and
Recreation, we demonstrate how estimating reporting delays leads to
\textit{practical} insights and interventions for more equitable, efficient
government service.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bhandaram_U/0/1/0/all/0/1&quot;&gt;Uma Bhandaram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Garg_N/0/1/0/all/0/1&quot;&gt;Nikhil Garg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.10268">
<title>Out-of-distribution generalization for learning quantum dynamics. (arXiv:2204.10268v3 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2204.10268</link>
<description rdf:parseType="Literal">&lt;p&gt;Generalization bounds are a critical tool to assess the training data
requirements of Quantum Machine Learning (QML). Recent work has established
guarantees for in-distribution generalization of quantum neural networks
(QNNs), where training and testing data are drawn from the same data
distribution. However, there are currently no results on out-of-distribution
generalization in QML, where we require a trained model to perform well even on
data drawn from a different distribution to the training distribution. Here, we
prove out-of-distribution generalization for the task of learning an unknown
unitary. In particular, we show that one can learn the action of a unitary on
entangled states having trained only product states. Since product states can
be prepared using only single-qubit gates, this advances the prospects of
learning quantum dynamics on near term quantum hardware, and further opens up
new methods for both the classical and quantum compilation of quantum circuits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Caro_M/0/1/0/all/0/1&quot;&gt;Matthias C. Caro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Hsin-Yuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Ezzell_N/0/1/0/all/0/1&quot;&gt;Nicholas Ezzell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Gibbs_J/0/1/0/all/0/1&quot;&gt;Joe Gibbs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Sornborger_A/0/1/0/all/0/1&quot;&gt;Andrew T. Sornborger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Cincio_L/0/1/0/all/0/1&quot;&gt;Lukasz Cincio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Coles_P/0/1/0/all/0/1&quot;&gt;Patrick J. Coles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Holmes_Z/0/1/0/all/0/1&quot;&gt;Zo&amp;#xeb; Holmes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.02364">
<title>KenSwQuAD -- A Question Answering Dataset for Swahili Low Resource Language. (arXiv:2205.02364v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2205.02364</link>
<description rdf:parseType="Literal">&lt;p&gt;The need for Question Answering datasets in low resource languages is the
motivation of this research, leading to the development of Kencorpus Swahili
Question Answering Dataset, KenSwQuAD. This dataset is annotated from raw story
texts of Swahili low resource language, which is a predominantly spoken in
Eastern African and in other parts of the world. Question Answering (QA)
datasets are important for machine comprehension of natural language for tasks
such as internet search and dialog systems. Machine learning systems need
training data such as the gold standard Question Answering set developed in
this research. The research engaged annotators to formulate QA pairs from
Swahili texts collected by the Kencorpus project, a Kenyan languages corpus.
The project annotated 1,445 texts from the total 2,585 texts with at least 5 QA
pairs each, resulting into a final dataset of 7,526 QA pairs. A quality
assurance set of 12.5% of the annotated texts confirmed that the QA pairs were
all correctly annotated. A proof of concept on applying the set to the QA task
confirmed that the dataset can be usable for such tasks. KenSwQuAD has also
contributed to resourcing of the Swahili language.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wanjawa_B/0/1/0/all/0/1&quot;&gt;Barack W. Wanjawa&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wanzare_L/0/1/0/all/0/1&quot;&gt;Lilian D.A. Wanzare&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Indede_F/0/1/0/all/0/1&quot;&gt;Florence Indede&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McOnyango_O/0/1/0/all/0/1&quot;&gt;Owen McOnyango&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muchemi_L/0/1/0/all/0/1&quot;&gt;Lawrence Muchemi&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ombui_E/0/1/0/all/0/1&quot;&gt;Edward Ombui&lt;/a&gt; (3) ((1) University of Nairobi Kenya, (2) Maseno University Kenya (3) Africa Nazarene University Kenya)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.07932">
<title>DDAC-SpAM: A Distributed Algorithm for Fitting High-dimensional Sparse Additive Models with Feature Division and Decorrelation. (arXiv:2205.07932v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.07932</link>
<description rdf:parseType="Literal">&lt;p&gt;Distributed statistical learning has become a popular technique for
large-scale data analysis. Most existing work in this area focuses on dividing
the observations, but we propose a new algorithm, DDAC-SpAM, which divides the
features under a high-dimensional sparse additive model. Our approach involves
three steps: divide, decorrelate, and conquer. The decorrelation operation
enables each local estimator to recover the sparsity pattern for each additive
component without imposing strict constraints on the correlation structure
among variables. The effectiveness and efficiency of the proposed algorithm are
demonstrated through theoretical analysis and empirical results on both
synthetic and real data. The theoretical results include both the consistent
sparsity pattern recovery as well as statistical inference for each additive
functional component. Our approach provides a practical solution for fitting
sparse additive models, with promising applications in a wide range of domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yifan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1&quot;&gt;Ruiyang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yong Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yang Feng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.13679">
<title>SeedGNN: Graph Neural Networks for Supervised Seeded Graph Matching. (arXiv:2205.13679v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.13679</link>
<description rdf:parseType="Literal">&lt;p&gt;There is a growing interest in designing Graph Neural Networks (GNNs) for
seeded graph matching, which aims to match two unlabeled graphs using only
topological information and a small set of seed nodes. However, most previous
GNNs for this task use a semi-supervised approach, which requires a large
number of seeds and cannot learn knowledge that is transferable to unseen
graphs. In contrast, this paper proposes a new supervised approach that can
learn from a training set how to match unseen graphs with only a few seeds. Our
SeedGNN architecture incorporates several novel designs, inspired by
theoretical studies of seeded graph matching: 1) it can learn to compute and
use witness-like information from different hops, in a way that can be
generalized to graphs of different sizes; 2) it can use easily-matched
node-pairs as new seeds to improve the matching in subsequent layers. We
evaluate SeedGNN on synthetic and real-world graphs and demonstrate significant
performance improvements over both non-learning and learning algorithms in the
existing literature. Furthermore, our experiments confirm that the knowledge
learned by SeedGNN from training graphs can be generalized to test graphs of
different sizes and categories.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Liren Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiaming Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xiaojun Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.14568">
<title>Conditionally Calibrated Predictive Distributions by Probability-Probability Map: Application to Galaxy Redshift Estimation and Probabilistic Forecasting. (arXiv:2205.14568v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2205.14568</link>
<description rdf:parseType="Literal">&lt;p&gt;Uncertainty quantification is crucial for assessing the predictive ability of
AI algorithms. Much research has been devoted to describing the predictive
distribution (PD) $F(y|\mathbf{x})$ of a target variable $y \in \mathbb{R}$
given complex input features $\mathbf{x} \in \mathcal{X}$. However,
off-the-shelf PDs (from, e.g., normalizing flows and Bayesian neural networks)
often lack conditional calibration with the probability of occurrence of an
event given input $\mathbf{x}$ being significantly different from the predicted
probability. Current calibration methods do not fully assess and enforce
conditionally calibrated PDs. Here we propose \texttt{Cal-PIT}, a method that
addresses both PD diagnostics and recalibration by learning a single
probability-probability map from calibration data. The key idea is to regress
probability integral transform scores against $\mathbf{x}$. The estimated
regression provides interpretable diagnostics of conditional coverage across
the feature space. The same regression function morphs the misspecified PD to a
re-calibrated PD for all $\mathbf{x}$. We benchmark our corrected prediction
bands (a by-product of corrected PDs) against oracle bands and state-of-the-art
predictive inference algorithms for synthetic data. We also provide results for
two applications: (i) probabilistic nowcasting given sequences of satellite
images, and (ii) conditional density estimation of galaxy distances given
imaging data (so-called photometric redshift estimation). Our code is available
as a Python package https://github.com/lee-group-cmu/Cal-PIT .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dey_B/0/1/0/all/0/1&quot;&gt;Biprateep Dey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;David Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Newman_J/0/1/0/all/0/1&quot;&gt;Jeffrey A. Newman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Andrews_B/0/1/0/all/0/1&quot;&gt;Brett H. Andrews&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Izbicki_R/0/1/0/all/0/1&quot;&gt;Rafael Izbicki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lee_A/0/1/0/all/0/1&quot;&gt;Ann B. Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.04530">
<title>DORA: Exploring Outlier Representations in Deep Neural Networks. (arXiv:2206.04530v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.04530</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Neural Networks (DNNs) excel at learning complex abstractions within
their internal representations. However, the concepts they learn remain opaque,
a problem that becomes particularly acute when models unintentionally learn
spurious correlations. In this work, we present DORA (Data-agnOstic
Representation Analysis), the first data-agnostic framework for analyzing the
representational space of DNNs. Central to our framework is the proposed
Extreme-Activation (EA) distance measure, which assesses similarities between
representations by analyzing their activation patterns on data points that
cause the highest level of activation. As spurious correlations often manifest
in features of data that are anomalous to the desired task, such as watermarks
or artifacts, we demonstrate that internal representations capable of detecting
such artifactual concepts can be found by analyzing relationships within neural
representations. We validate the EA metric quantitatively, demonstrating its
effectiveness both in controlled scenarios and real-world applications.
Finally, we provide practical examples from popular Computer Vision models to
illustrate that representations identified as outliers using the EA metric
often correspond to undesired and spurious concepts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bykov_K/0/1/0/all/0/1&quot;&gt;Kirill Bykov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deb_M/0/1/0/all/0/1&quot;&gt;Mayukh Deb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grinwald_D/0/1/0/all/0/1&quot;&gt;Dennis Grinwald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1&quot;&gt;Klaus-Robert M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hohne_M/0/1/0/all/0/1&quot;&gt;Marina M.-C. H&amp;#xf6;hne&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.04857">
<title>Mixed integer linear optimization formulations for learning optimal binary classification trees. (arXiv:2206.04857v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.04857</link>
<description rdf:parseType="Literal">&lt;p&gt;Decision trees are powerful tools for classification and regression that
attract many researchers working in the burgeoning area of machine learning.
One advantage of decision trees over other methods is their interpretability,
which is often preferred over other higher accuracy methods that are relatively
uninterpretable. A binary classification tree has two types of vertices: (i)
branching vertices which have exactly two children and where datapoints are
assessed on a set of discrete features; and (ii) leaf vertices at which
datapoints are given a discrete prediction. An optimal binary classification
tree can be obtained by solving a biobjective optimization problem that seeks
to (i) maximize the number of correctly classified datapoints and (ii) minimize
the number of branching vertices. In this paper, we propose four mixed integer
linear optimization (MILO) formulations for designing optimal binary
classification trees: two flow-based formulations and two-cut based
formulations. We provide theoretical comparisons between our proposed
formulations and the strongest flow-based MILO formulation of Aghaei et al.
(2021). We conduct experiments on 13 publicly available datasets to show the
models&apos; ability to scale and the strength of a biobjective approach using
Pareto frontiers. Our code and data are available on GitHub.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alston_B/0/1/0/all/0/1&quot;&gt;Brandon Alston&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Validi_H/0/1/0/all/0/1&quot;&gt;Hamidreza Validi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hicks_I/0/1/0/all/0/1&quot;&gt;Illya V. Hicks&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.08516">
<title>MetaFed: Federated Learning among Federations with Cyclic Knowledge Distillation for Personalized Healthcare. (arXiv:2206.08516v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.08516</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning has attracted increasing attention to building models
without accessing the raw user data, especially in healthcare. In real
applications, different federations can seldom work together due to possible
reasons such as data heterogeneity and distrust/inexistence of the central
server. In this paper, we propose a novel framework called MetaFed to
facilitate trustworthy FL between different federations. MetaFed obtains a
personalized model for each federation without a central server via the
proposed Cyclic Knowledge Distillation. Specifically, MetaFed treats each
federation as a meta distribution and aggregates knowledge of each federation
in a cyclic manner. The training is split into two parts: common knowledge
accumulation and personalization. Comprehensive experiments on three benchmarks
demonstrate that MetaFed without a server achieves better accuracy compared to
state-of-the-art methods (e.g., 10%+ accuracy improvement compared to the
baseline for PAMAP2) with fewer communication costs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yiqiang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1&quot;&gt;Wang Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1&quot;&gt;Xin Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jindong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xing Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.10477">
<title>Survival Kernets: Scalable and Interpretable Deep Kernel Survival Analysis with an Accuracy Guarantee. (arXiv:2206.10477v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.10477</link>
<description rdf:parseType="Literal">&lt;p&gt;Kernel survival analysis models estimate individual survival distributions
with the help of a kernel function, which measures the similarity between any
two data points. Such a kernel function can be learned using deep kernel
survival models. In this paper, we present a new deep kernel survival model
called a survival kernet, which scales to large datasets in a manner that is
amenable to model interpretation and also theoretical analysis. Specifically,
the training data are partitioned into clusters based on a recently developed
training set compression scheme for classification and regression called kernel
netting that we extend to the survival analysis setting. At test time, each
data point is represented as a weighted combination of these clusters, and each
such cluster can be visualized. For a special case of survival kernets, we
establish a finite-sample error bound on predicted survival distributions that
is, up to a log factor, optimal. Whereas scalability at test time is achieved
using the aforementioned kernel netting compression strategy, scalability
during training is achieved by a warm-start procedure based on tree ensembles
such as XGBoost and a heuristic approach to accelerating neural architecture
search. On four standard survival analysis datasets of varying sizes (up to
roughly 3 million data points), we show that survival kernets are highly
competitive compared to various baselines tested in terms of time-dependent
concordance index. Our code is available at:
https://github.com/georgehc/survival-kernets
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;George H. Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.12252">
<title>Indecision Trees: Learning Argument-Based Reasoning under Quantified Uncertainty. (arXiv:2206.12252v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.12252</link>
<description rdf:parseType="Literal">&lt;p&gt;Using Machine Learning systems in the real world can often be problematic,
with inexplicable black-box models, the assumed certainty of imperfect
measurements, or providing a single classification instead of a probability
distribution.
&lt;/p&gt;
&lt;p&gt;This paper introduces Indecision Trees, a modification to Decision Trees
which learn under uncertainty, can perform inference under uncertainty, provide
a robust distribution over the possible labels, and can be disassembled into a
set of logical arguments for use in other reasoning systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kent_J/0/1/0/all/0/1&quot;&gt;Jonathan S. Kent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menager_D/0/1/0/all/0/1&quot;&gt;David H. Menager&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.06154">
<title>On the Robustness of Bayesian Neural Networks to Adversarial Attacks. (arXiv:2207.06154v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2207.06154</link>
<description rdf:parseType="Literal">&lt;p&gt;Vulnerability to adversarial attacks is one of the principal hurdles to the
adoption of deep learning in safety-critical applications. Despite significant
efforts, both practical and theoretical, training deep learning models robust
to adversarial attacks is still an open problem. In this paper, we analyse the
geometry of adversarial attacks in the large-data, overparameterized limit for
Bayesian Neural Networks (BNNs). We show that, in the limit, vulnerability to
gradient-based attacks arises as a result of degeneracy in the data
distribution, i.e., when the data lies on a lower-dimensional submanifold of
the ambient space. As a direct consequence, we demonstrate that in this limit
BNN posteriors are robust to gradient-based adversarial attacks. Crucially, we
prove that the expected gradient of the loss with respect to the BNN posterior
distribution is vanishing, even when each neural network sampled from the
posterior is vulnerable to gradient-based attacks. Experimental results on the
MNIST, Fashion MNIST, and half moons datasets, representing the finite data
regime, with BNNs trained with Hamiltonian Monte Carlo and Variational
Inference, support this line of arguments, showing that BNNs can display both
high accuracy on clean data and robustness to both gradient-based and
gradient-free based adversarial attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bortolussi_L/0/1/0/all/0/1&quot;&gt;Luca Bortolussi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carbone_G/0/1/0/all/0/1&quot;&gt;Ginevra Carbone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laurenti_L/0/1/0/all/0/1&quot;&gt;Luca Laurenti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patane_A/0/1/0/all/0/1&quot;&gt;Andrea Patane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanguinetti_G/0/1/0/all/0/1&quot;&gt;Guido Sanguinetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wicker_M/0/1/0/all/0/1&quot;&gt;Matthew Wicker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.11719">
<title>Gradient-based Bi-level Optimization for Deep Learning: A Survey. (arXiv:2207.11719v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2207.11719</link>
<description rdf:parseType="Literal">&lt;p&gt;Bi-level optimization, especially the gradient-based category, has been
widely used in the deep learning community including hyperparameter
optimization and meta-knowledge extraction. Bi-level optimization embeds one
problem within another and the gradient-based category solves the outer-level
task by computing the hypergradient, which is much more efficient than
classical methods such as the evolutionary algorithm. In this survey, we first
give a formal definition of the gradient-based bi-level optimization. Next, we
delineate criteria to determine if a research problem is apt for bi-level
optimization and provide a practical guide on structuring such problems into a
bi-level optimization framework, a feature particularly beneficial for those
new to this domain. More specifically, there are two formulations: the
single-task formulation to optimize hyperparameters such as regularization
parameters and the distilled data, and the multi-task formulation to extract
meta-knowledge such as the model initialization. With a bi-level formulation,
we then discuss four bi-level optimization solvers to update the outer variable
including explicit gradient update, proxy update, implicit function update, and
closed-form update. Finally, we wrap up the survey by highlighting two
prospective future directions: (1) Effective Data Optimization for Science
examined through the lens of task formulation. (2) Accurate Explicit Proxy
Update analyzed from an optimization standpoint.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Can Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chen Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zixuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xue Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.01996">
<title>Adaptive Domain Generalization via Online Disagreement Minimization. (arXiv:2208.01996v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2208.01996</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks suffer from significant performance deterioration when
there exists distribution shift between deployment and training. Domain
Generalization (DG) aims to safely transfer a model to unseen target domains by
only relying on a set of source domains. Although various DG approaches have
been proposed, a recent study named DomainBed, reveals that most of them do not
beat the simple Empirical Risk Minimization (ERM). To this end, we propose a
general framework that is orthogonal to existing DG algorithms and could
improve their performance consistently. Unlike previous DG works that stake on
a static source model to be hopefully a universal one, our proposed AdaODM
adaptively modifies the source model at test time for different target domains.
Specifically, we create multiple domain-specific classifiers upon a shared
domain-generic feature extractor. The feature extractor and classifiers are
trained in an adversarial way, where the feature extractor embeds the input
samples into a domain-invariant space, and the multiple classifiers capture the
distinct decision boundaries that each of them relates to a specific source
domain. During testing, distribution differences between target and source
domains could be effectively measured by leveraging prediction disagreement
among source classifiers. By fine-tuning source models to minimize the
disagreement at test time, target domain features are well aligned to the
invariant feature space. We verify AdaODM on two popular DG methods, namely ERM
and CORAL, and four DG benchmarks, namely VLCS, PACS, OfficeHome, and
TerraIncognita. The results show AdaODM stably improves the generalization
capacity on unseen domains and achieves state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Ying-Cong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.06536">
<title>Double Auctions with Two-sided Bandit Feedback. (arXiv:2208.06536v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2208.06536</link>
<description rdf:parseType="Literal">&lt;p&gt;Double Auction enables decentralized transfer of goods between multiple
buyers and sellers, thus underpinning functioning of many online marketplaces.
Buyers and sellers compete in these markets through bidding, but do not often
know their own valuation a-priori. As the allocation and pricing happens
through bids, the profitability of participants, hence sustainability of such
markets, depends crucially on learning respective valuations through repeated
interactions. We initiate the study of Double Auction markets under bandit
feedback on both buyers&apos; and sellers&apos; side. We show with confidence bound based
bidding, and `Average Pricing&apos; there is an efficient price discovery among the
participants. In particular, the regret on combined valuation of the buyers and
the sellers -- a.k.a. the social regret -- is $O(\log(T)/\Delta)$ in $T$
rounds, where $\Delta$ is the minimum price gap. Moreover, the buyers and
sellers exchanging goods attain $O(\sqrt{T})$ regret, individually. The buyers
and sellers who do not benefit from exchange in turn only experience
$O(\log{T}/ \Delta)$ regret individually in $T$ rounds. We augment our upper
bound by showing that $\omega(\sqrt{T})$ individual regret, and
$\omega(\log{T})$ social regret is unattainable in certain Double Auction
markets. Our paper is the first to provide decentralized learning algorithms in
a two-sided market where \emph{both sides have uncertain preference} that need
to be learned.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1&quot;&gt;Soumya Basu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sankararaman_A/0/1/0/all/0/1&quot;&gt;Abishek Sankararaman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.10264">
<title>Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies. (arXiv:2208.10264v5 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2208.10264</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a new type of test, called a Turing Experiment (TE), for
evaluating to what extent a given language model, such as GPT models, can
simulate different aspects of human behavior. A TE can also reveal consistent
distortions in a language model&apos;s simulation of a specific human behavior.
Unlike the Turing Test, which involves simulating a single arbitrary
individual, a TE requires simulating a representative sample of participants in
human subject research. We carry out TEs that attempt to replicate
well-established findings from prior studies. We design a methodology for
simulating TEs and illustrate its use to compare how well different language
models are able to reproduce classic economic, psycholinguistic, and social
psychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock
Experiment, and Wisdom of Crowds. In the first three TEs, the existing findings
were replicated using recent models, while the last TE reveals a
&quot;hyper-accuracy distortion&quot; present in some language models (including ChatGPT
and GPT-4), which could affect downstream applications in education and the
arts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aher_G/0/1/0/all/0/1&quot;&gt;Gati Aher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arriaga_R/0/1/0/all/0/1&quot;&gt;Rosa I. Arriaga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalai_A/0/1/0/all/0/1&quot;&gt;Adam Tauman Kalai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.10967">
<title>The Value of Out-of-Distribution Data. (arXiv:2208.10967v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2208.10967</link>
<description rdf:parseType="Literal">&lt;p&gt;We expect the generalization error to improve with more samples from a
similar task, and to deteriorate with more samples from an out-of-distribution
(OOD) task. In this work, we show a counter-intuitive phenomenon: the
generalization error of a task can be a non-monotonic function of the number of
OOD samples. As the number of OOD samples increases, the generalization error
on the target task improves before deteriorating beyond a threshold. In other
words, there is value in training on small amounts of OOD data. We use Fisher&apos;s
Linear Discriminant on synthetic datasets and deep networks on computer vision
benchmarks such as MNIST, CIFAR-10, CINIC-10, PACS and DomainNet to demonstrate
and analyze this phenomenon. In the idealistic setting where we know which
samples are OOD, we show that these non-monotonic trends can be exploited using
an appropriately weighted objective of the target and OOD empirical risk. While
its practical utility is limited, this does suggest that if we can detect OOD
samples, then there may be ways to benefit from them. When we do not know which
samples are OOD, we show how a number of go-to strategies such as
data-augmentation, hyper-parameter optimization, and pre-training are not
enough to ensure that the target generalization error does not deteriorate with
the number of OOD samples in the dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_A/0/1/0/all/0/1&quot;&gt;Ashwin De Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramesh_R/0/1/0/all/0/1&quot;&gt;Rahul Ramesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Priebe_C/0/1/0/all/0/1&quot;&gt;Carey E. Priebe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhari_P/0/1/0/all/0/1&quot;&gt;Pratik Chaudhari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vogelstein_J/0/1/0/all/0/1&quot;&gt;Joshua T. Vogelstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.02370">
<title>Continual Learning, Fast and Slow. (arXiv:2209.02370v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2209.02370</link>
<description rdf:parseType="Literal">&lt;p&gt;According to the Complementary Learning Systems (CLS)
theory~\cite{mcclelland1995there} in neuroscience, humans do effective
\emph{continual learning} through two complementary systems: a fast learning
system centered on the hippocampus for rapid learning of the specifics,
individual experiences; and a slow learning system located in the neocortex for
the gradual acquisition of structured knowledge about the environment.
Motivated by this theory, we propose \emph{DualNets} (for Dual Networks), a
general continual learning framework comprising a fast learning system for
supervised learning of pattern-separated representation from specific tasks and
a slow learning system for representation learning of task-agnostic general
representation via Self-Supervised Learning (SSL). DualNets can seamlessly
incorporate both representation types into a holistic framework to facilitate
better continual learning in deep neural networks. Via extensive experiments,
we demonstrate the promising results of DualNets on a wide range of continual
learning protocols, ranging from the standard offline, task-aware setting to
the challenging online, task-free scenario. Notably, on the
CTrL~\cite{veniat2020efficient} benchmark that has unrelated tasks with vastly
different visual images, DualNets can achieve competitive performance with
existing state-of-the-art dynamic architecture
strategies~\cite{ostapenko2021continual}. Furthermore, we conduct comprehensive
ablation studies to validate DualNets efficacy, robustness, and scalability.
Code will be made available at \url{https://github.com/phquang/DualNet}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_Q/0/1/0/all/0/1&quot;&gt;Quang Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chenghao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1&quot;&gt;Steven C. H. Hoi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.02905">
<title>A Data-dependent Approach for High Dimensional (Robust) Wasserstein Alignment. (arXiv:2209.02905v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.02905</link>
<description rdf:parseType="Literal">&lt;p&gt;Many real-world problems can be formulated as the alignment between two
geometric patterns. Previously, a great amount of research focus on the
alignment of 2D or 3D patterns in the field of computer vision. Recently, the
alignment problem in high dimensions finds several novel applications in
practice. However, the research is still rather limited in the algorithmic
aspect. To the best of our knowledge, most existing approaches are just simple
extensions of their counterparts for 2D and 3D cases, and often suffer from the
issues such as high computational complexities. In this paper, we propose an
effective framework to compress the high dimensional geometric patterns. Any
existing alignment method can be applied to the compressed geometric patterns
and the time complexity can be significantly reduced. Our idea is inspired by
the observation that high dimensional data often has a low intrinsic dimension.
Our framework is a ``data-dependent&apos;&apos; approach that has the complexity
depending on the intrinsic dimension of the input data. Our experimental
results reveal that running the alignment algorithm on compressed patterns can
achieve similar qualities, comparing with the results on the original patterns,
but the runtimes (including the times cost for compression) are substantially
lower.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Hu Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wenjie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1&quot;&gt;Mingquan Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.04547">
<title>Defend Data Poisoning Attacks on Voice Authentication. (arXiv:2209.04547v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2209.04547</link>
<description rdf:parseType="Literal">&lt;p&gt;With the advances in deep learning, speaker verification has achieved very
high accuracy and is gaining popularity as a type of biometric authentication
option in many scenes of our daily life, especially the growing market of web
services. Compared to traditional passwords, &quot;vocal passwords&quot; are much more
convenient as they relieve people from memorizing different passwords. However,
new machine learning attacks are putting these voice authentication systems at
risk. Without a strong security guarantee, attackers could access legitimate
users&apos; web accounts by fooling the deep neural network (DNN) based voice
recognition models. In this paper, we demonstrate an easy-to-implement data
poisoning attack to the voice authentication system, which can hardly be
captured by existing defense mechanisms. Thus, we propose a more robust defense
method, called Guardian, which is a convolutional neural network-based
discriminator. The Guardian discriminator integrates a series of novel
techniques including bias reduction, input augmentation, and ensemble learning.
Our approach is able to distinguish about 95% of attacked accounts from normal
accounts, which is much more effective than existing approaches with only 60%
accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Ke Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baird_C/0/1/0/all/0/1&quot;&gt;Cameron Baird&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dan Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.04562">
<title>The Bayan Algorithm: Detecting Communities in Networks Through Exact and Approximate Optimization of Modularity. (arXiv:2209.04562v3 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/2209.04562</link>
<description rdf:parseType="Literal">&lt;p&gt;Community detection is a classic problem in network science with extensive
applications in various fields. Among numerous approaches, the most common
method is modularity maximization. Despite their design philosophy and wide
adoption, heuristic modularity maximization algorithms rarely return an optimal
partition or anything similar. We propose a specialized algorithm, Bayan, which
returns partitions with a guarantee of either optimality or proximity to an
optimal partition. At the core of the Bayan algorithm is a branch-and-cut
scheme that solves an integer programming formulation of the modularity
maximization problem to optimality or approximate it within a factor. We
compare Bayan against 30 alternative community detection methods using
structurally diverse synthetic and real networks. Our results demonstrate
Bayan&apos;s distinctive accuracy and stability in retrieving ground-truth
communities of standard benchmark graphs. Bayan is several times faster than
open-source and commercial solvers for modularity maximization making it
capable of finding optimal partitions for instances that cannot be optimized by
any other existing method. Overall, our assessments point to Bayan as a
suitable choice for exact maximization of modularity in real networks with up
to 3000 edges (in their largest connected component) and approximating maximum
modularity in larger instances on ordinary computers. A Python implementation
of the Bayan algorithm (the bayanpy library) is publicly available through the
package installer for Python (pip).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aref_S/0/1/0/all/0/1&quot;&gt;Samin Aref&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chheda_H/0/1/0/all/0/1&quot;&gt;Hriday Chheda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mostajabdaveh_M/0/1/0/all/0/1&quot;&gt;Mahdi Mostajabdaveh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.13513">
<title>DynDepNet: Learning Time-Varying Dependency Structures from fMRI Data via Dynamic Graph Structure Learning. (arXiv:2209.13513v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.13513</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph neural networks (GNNs) have demonstrated success in learning
representations of brain graphs derived from functional magnetic resonance
imaging (fMRI) data. However, existing GNN methods assume brain graphs are
static over time and the graph adjacency matrix is known prior to model
training. These assumptions contradict evidence that brain graphs are
time-varying with a connectivity structure that depends on the choice of
functional connectivity measure. Incorrectly representing fMRI data with noisy
brain graphs can adversely affect GNN performance. To address this, we propose
DynDepNet, a novel method for learning the optimal time-varying dependency
structure of fMRI data induced by downstream prediction tasks. Experiments on
real-world fMRI datasets, for the task of sex classification, demonstrate that
DynDepNet achieves state-of-the-art results, outperforming the best baseline in
terms of accuracy by approximately 8 and 6 percentage points, respectively.
Furthermore, analysis of the learned dynamic graphs reveals prediction-related
brain regions consistent with existing neuroscience literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Campbell_A/0/1/0/all/0/1&quot;&gt;Alexander Campbell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zippo_A/0/1/0/all/0/1&quot;&gt;Antonio Giuliano Zippo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Passamonti_L/0/1/0/all/0/1&quot;&gt;Luca Passamonti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toschi_N/0/1/0/all/0/1&quot;&gt;Nicola Toschi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1&quot;&gt;Pietro Lio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.14233">
<title>Obstacle Identification and Ellipsoidal Decomposition for Fast Motion Planning in Unknown Dynamic Environments. (arXiv:2209.14233v4 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2209.14233</link>
<description rdf:parseType="Literal">&lt;p&gt;Collision avoidance in the presence of dynamic obstacles in unknown
environments is one of the most critical challenges for unmanned systems. In
this paper, we present a method that identifies obstacles in terms of
ellipsoids to estimate linear and angular obstacle velocities. Our proposed
method is based on the idea of any object can be approximately expressed by
ellipsoids. To achieve this, we propose a method based on variational Bayesian
estimation of Gaussian mixture model, the Kyachiyan algorithm, and a refinement
algorithm. Our proposed method does not require knowledge of the number of
clusters and can operate in real-time, unlike existing optimization-based
methods. In addition, we define an ellipsoid-based feature vector to match
obstacles given two timely close point frames. Our method can be applied to any
environment with static and dynamic obstacles, including the ones with rotating
obstacles. We compare our algorithm with other clustering methods and show that
when coupled with a trajectory planner, the overall system can efficiently
traverse unknown environments in the presence of dynamic obstacles.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaymaz_M/0/1/0/all/0/1&quot;&gt;Mehmetcan Kaymaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ure_N/0/1/0/all/0/1&quot;&gt;Nazim Kemal Ure&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.14249">
<title>Compositional Score Modeling for Simulation-based Inference. (arXiv:2209.14249v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.14249</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Posterior Estimation methods for simulation-based inference can be
ill-suited for dealing with posterior distributions obtained by conditioning on
multiple observations, as they tend to require a large number of simulator
calls to learn accurate approximations. In contrast, Neural Likelihood
Estimation methods can handle multiple observations at inference time after
learning from individual observations, but they rely on standard inference
methods, such as MCMC or variational inference, which come with certain
performance drawbacks. We introduce a new method based on conditional score
modeling that enjoys the benefits of both approaches. We model the scores of
the (diffused) posterior distributions induced by individual observations, and
introduce a way of combining the learned scores to approximately sample from
the target posterior distribution. Our approach is sample-efficient, can
naturally aggregate multiple observations at inference time, and avoids the
drawbacks of standard inference methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geffner_T/0/1/0/all/0/1&quot;&gt;Tomas Geffner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papamakarios_G/0/1/0/all/0/1&quot;&gt;George Papamakarios&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mnih_A/0/1/0/all/0/1&quot;&gt;Andriy Mnih&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.01081">
<title>On The Effects Of Data Normalisation For Domain Adaptation On EEG Data. (arXiv:2210.01081v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.01081</link>
<description rdf:parseType="Literal">&lt;p&gt;In the Machine Learning (ML) literature, a well-known problem is the Dataset
Shift problem where, differently from the ML standard hypothesis, the data in
the training and test sets can follow different probability distributions,
leading ML systems toward poor generalisation performances. This problem is
intensely felt in the Brain-Computer Interface (BCI) context, where bio-signals
as Electroencephalographic (EEG) are often used. In fact, EEG signals are
highly non-stationary both over time and between different subjects. To
overcome this problem, several proposed solutions are based on recent transfer
learning approaches such as Domain Adaption (DA). In several cases, however,
the actual causes of the improvements remain ambiguous. This paper focuses on
the impact of data normalisation, or standardisation strategies applied
together with DA methods. In particular, using \textit{SEED}, \textit{DEAP},
and \textit{BCI Competition IV 2a} EEG datasets, we experimentally evaluated
the impact of different normalization strategies applied with and without
several well-known DA methods, comparing the obtained performances. It results
that the choice of the normalisation strategy plays a key role on the
classifier performances in DA scenarios, and interestingly, in several cases,
the use of only an appropriate normalisation schema outperforms the DA
technique.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Apicella_A/0/1/0/all/0/1&quot;&gt;Andrea Apicella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isgro_F/0/1/0/all/0/1&quot;&gt;Francesco Isgr&amp;#xf2;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pollastro_A/0/1/0/all/0/1&quot;&gt;Andrea Pollastro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prevete_R/0/1/0/all/0/1&quot;&gt;Roberto Prevete&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.04653">
<title>Rejecting noise in Baikal-GVD data with neural networks. (arXiv:2210.04653v2 [astro-ph.IM] UPDATED)</title>
<link>http://arxiv.org/abs/2210.04653</link>
<description rdf:parseType="Literal">&lt;p&gt;Baikal-GVD is a large ($\sim$1 km$^3$) underwater neutrino telescope
installed in the fresh waters of Lake Baikal. The deep lake water environment
is pervaded by background light, which is detectable by Baikal-GVD&apos;s
photosensors. We introduce a neural network for an efficient separation of
these noise hits from the signal ones, stemming from the propagation of
relativistic particles through the detector. The model has a U-net-like
architecture and employs temporal (causal) structure of events. The neural
network&apos;s metrics reach up to 99\% signal purity (precision) and 96\% survival
efficiency (recall) on Monte-Carlo simulated dataset. We compare the developed
method with the algorithmic approach to rejecting the noise and discuss other
possible architectures of neural networks, including graph-based ones.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Kharuk_I/0/1/0/all/0/1&quot;&gt;I. Kharuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Rubtsov_G/0/1/0/all/0/1&quot;&gt;G. Rubtsov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Safronov_G/0/1/0/all/0/1&quot;&gt;G. Safronov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.09809">
<title>Analysis of Convolutions, Non-linearity and Depth in Graph Neural Networks using Neural Tangent Kernel. (arXiv:2210.09809v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.09809</link>
<description rdf:parseType="Literal">&lt;p&gt;The fundamental principle of Graph Neural Networks (GNNs) is to exploit the
structural information of the data by aggregating the neighboring nodes using a
`graph convolution&apos; in conjunction with a suitable choice for the network
architecture, such as depth and activation functions. Therefore, understanding
the influence of each of the design choice on the network performance is
crucial. Convolutions based on graph Laplacian have emerged as the dominant
choice with the symmetric normalization of the adjacency matrix as the most
widely adopted one. However, some empirical studies show that row normalization
of the adjacency matrix outperforms it in node classification. Despite the
widespread use of GNNs, there is no rigorous theoretical study on the
representation power of these convolutions, that could explain this behavior.
Similarly, the empirical observation of the linear GNNs performance being on
par with non-linear ReLU GNNs lacks rigorous theory.
&lt;/p&gt;
&lt;p&gt;In this work, we theoretically analyze the influence of different aspects of
the GNN architecture using the Graph Neural Tangent Kernel in a semi-supervised
node classification setting. Under the population Degree Corrected Stochastic
Block Model, we prove that: (i) linear networks capture the class information
as good as ReLU networks; (ii) row normalization preserves the underlying class
structure better than other convolutions; (iii) performance degrades with
network depth due to over-smoothing, but the loss in class information is the
slowest in row normalization; (iv) skip connections retain the class
information even at infinite depth, thereby eliminating over-smoothing. We
finally validate our theoretical findings numerically and on real datasets such
as Cora and Citeseer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabanayagam_M/0/1/0/all/0/1&quot;&gt;Mahalakshmi Sabanayagam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Esser_P/0/1/0/all/0/1&quot;&gt;Pascal Esser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghoshdastidar_D/0/1/0/all/0/1&quot;&gt;Debarghya Ghoshdastidar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.13573">
<title>Conditionally Risk-Averse Contextual Bandits. (arXiv:2210.13573v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2210.13573</link>
<description rdf:parseType="Literal">&lt;p&gt;Contextual bandits with average-case statistical guarantees are inadequate in
risk-averse situations because they might trade off degraded worst-case
behaviour for better average performance. Designing a risk-averse contextual
bandit is challenging because exploration is necessary but risk-aversion is
sensitive to the entire distribution of rewards; nonetheless we exhibit the
first risk-averse contextual bandit algorithm with an online regret guarantee.
We conduct experiments from diverse scenarios where worst-case outcomes should
be avoided, from dynamic pricing, inventory management, and self-tuning
software; including a production exascale data processing system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Farsang_M/0/1/0/all/0/1&quot;&gt;M&amp;#xf3;nika Farsang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mineiro_P/0/1/0/all/0/1&quot;&gt;Paul Mineiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wangda Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.16189">
<title>Preferential Subsampling for Stochastic Gradient Langevin Dynamics. (arXiv:2210.16189v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2210.16189</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic gradient MCMC (SGMCMC) offers a scalable alternative to
traditional MCMC, by constructing an unbiased estimate of the gradient of the
log-posterior with a small, uniformly-weighted subsample of the data. While
efficient to compute, the resulting gradient estimator may exhibit a high
variance and impact sampler performance. The problem of variance control has
been traditionally addressed by constructing a better stochastic gradient
estimator, often using control variates. We propose to use a discrete,
non-uniform probability distribution to preferentially subsample data points
that have a greater impact on the stochastic gradient. In addition, we present
a method of adaptively adjusting the subsample size at each iteration of the
algorithm, so that we increase the subsample size in areas of the sample space
where the gradient is harder to estimate. We demonstrate that such an approach
can maintain the same level of accuracy while substantially reducing the
average subsample size that is used.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Putcha_S/0/1/0/all/0/1&quot;&gt;Srshti Putcha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nemeth_C/0/1/0/all/0/1&quot;&gt;Christopher Nemeth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fearnhead_P/0/1/0/all/0/1&quot;&gt;Paul Fearnhead&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.16575">
<title>Self-Improving Safety Performance of Reinforcement Learning Based Driving with Black-Box Verification Algorithms. (arXiv:2210.16575v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2210.16575</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we propose a self-improving artificial intelligence system to
enhance the safety performance of reinforcement learning (RL)-based autonomous
driving (AD) agents using black-box verification methods. RL algorithms have
become popular in AD applications in recent years. However, the performance of
existing RL algorithms heavily depends on the diversity of training scenarios.
A lack of safety-critical scenarios during the training phase could result in
poor generalization performance in real-world driving applications. We propose
a novel framework in which the weaknesses of the training set are explored
through black-box verification methods. After discovering AD failure scenarios,
the RL agent&apos;s training is re-initiated via transfer learning to improve the
performance of previously unsafe scenarios. Simulation results demonstrate that
our approach efficiently discovers safety failures of action decisions in
RL-based adaptive cruise control (ACC) applications and significantly reduces
the number of vehicle collisions through iterative applications of our method.
The source code is publicly available at
https://github.com/data-and-decision-lab/self-improving-RL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dagdanov_R/0/1/0/all/0/1&quot;&gt;Resul Dagdanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durmus_H/0/1/0/all/0/1&quot;&gt;Halil Durmus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ure_N/0/1/0/all/0/1&quot;&gt;Nazim Kemal Ure&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.00172">
<title>Infusing known operators in convolutional neural networks for lateral strain imaging in ultrasound elastography. (arXiv:2211.00172v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.00172</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional Neural Networks (CNN) have been employed for displacement
estimation in ultrasound elastography (USE). High-quality axial strains
(derivative of the axial displacement in the axial direction) can be estimated
by the proposed networks. In contrast to axial strain, lateral strain, which is
highly required in Poisson&apos;s ratio imaging and elasticity reconstruction, has a
poor quality. The main causes include low sampling frequency, limited motion,
and lack of phase information in the lateral direction. Recently, physically
inspired constraint in unsupervised regularized elastography (PICTURE) has been
proposed. This method took into account the range of the feasible lateral
strain defined by the rules of physics of motion and employed a regularization
strategy to improve the lateral strains. Despite the substantial improvement,
the regularization was only applied during the training; hence it did not
guarantee during the test that the lateral strain is within the feasible range.
Furthermore, only the feasible range was employed, other constraints such as
incompressibility were not investigated. In this paper, we address these two
issues and propose kPICTURE in which two iterative algorithms were infused into
the network architecture in the form of known operators to ensure the lateral
strain is within the feasible range and impose incompressibility during the
test phase.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tehrani_A/0/1/0/all/0/1&quot;&gt;Ali K. Z. Tehrani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rivaz_H/0/1/0/all/0/1&quot;&gt;Hassan Rivaz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.02222">
<title>The Benefits of Model-Based Generalization in Reinforcement Learning. (arXiv:2211.02222v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.02222</link>
<description rdf:parseType="Literal">&lt;p&gt;Model-Based Reinforcement Learning (RL) is widely believed to have the
potential to improve sample efficiency by allowing an agent to synthesize large
amounts of imagined experience. Experience Replay (ER) can be considered a
simple kind of model, which has proved effective at improving the stability and
efficiency of deep RL. In principle, a learned parametric model could improve
on ER by generalizing from real experience to augment the dataset with
additional plausible experience. However, given that learned value functions
can also generalize, it is not immediately obvious why model generalization
should be better. Here, we provide theoretical and empirical insight into when,
and how, we can expect data generated by a learned model to be useful. First,
we provide a simple theorem motivating how learning a model as an intermediate
step can narrow down the set of possible value functions more than learning a
value function directly from data using the Bellman equation. Second, we
provide an illustrative example showing empirically how a similar effect occurs
in a more concrete setting with neural network function approximation. Finally,
we provide extensive experiments showing the benefit of model-based learning
for online RL in environments with combinatorial complexity, but factored
structure that allows a learned model to generalize. In these experiments, we
take care to control for other factors in order to isolate, insofar as
possible, the benefit of using experience generated by a learned model relative
to ER alone.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Young_K/0/1/0/all/0/1&quot;&gt;Kenny Young&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramesh_A/0/1/0/all/0/1&quot;&gt;Aditya Ramesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirsch_L/0/1/0/all/0/1&quot;&gt;Louis Kirsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1&quot;&gt;J&amp;#xfc;rgen Schmidhuber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.04284">
<title>Efficient Compressed Ratio Estimation Using Online Sequential Learning for Edge Computing. (arXiv:2211.04284v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.04284</link>
<description rdf:parseType="Literal">&lt;p&gt;Owing to the widespread adoption of the Internet of Things, a vast amount of
sensor information is being acquired in real time. Accordingly, the
communication cost of data from edge devices is increasing. Compressed sensing
(CS), a data compression method that can be used on edge devices, has been
attracting attention as a method to reduce communication costs. In CS,
estimating the appropriate compression ratio is important. There is a method to
adaptively estimate the compression ratio for the acquired data using
reinforcement learning (RL). However, the computational costs associated with
existing RL methods that can be utilized on edges are often high. In this
study, we developed an efficient RL method for edge devices, referred to as the
actor--critic online sequential extreme learning machine (AC-OSELM), and a
system to compress data by estimating an appropriate compression ratio on the
edge using AC-OSELM. The performance of the proposed method in estimating the
compression ratio is evaluated by comparing it with other RL methods for edge
devices. The experimental results indicate that AC-OSELM demonstrated the same
or better compression performance and faster compression ratio estimation than
the existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oikawa_H/0/1/0/all/0/1&quot;&gt;Hiroki Oikawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_H/0/1/0/all/0/1&quot;&gt;Hangli Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koshizuka_N/0/1/0/all/0/1&quot;&gt;Noboru Koshizuka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.10805">
<title>On the Pointwise Behavior of Recursive Partitioning and Its Implications for Heterogeneous Causal Effect Estimation. (arXiv:2211.10805v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2211.10805</link>
<description rdf:parseType="Literal">&lt;p&gt;Decision tree learning is increasingly being used for pointwise inference.
Important applications include causal heterogenous treatment effects and
dynamic policy decisions, as well as conditional quantile regression and design
of experiments, where tree estimation and inference is conducted at specific
values of the covariates. In this paper, we call into question the use of
decision trees (trained by adaptive recursive partitioning) for such purposes
by demonstrating that they can fail to achieve polynomial rates of convergence
in uniform norm, even with pruning. Instead, the convergence may be
poly-logarithmic or, in some important special cases, such as honest regression
trees, fail completely. We show that random forests can remedy the situation,
turning poor performing trees into nearly optimal procedures, at the cost of
losing interpretability and introducing two additional tuning parameters. The
two hallmarks of random forests, subsampling and the random feature selection
mechanism, are seen to each distinctively contribute to achieving nearly
optimal performance for the model class considered.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cattaneo_M/0/1/0/all/0/1&quot;&gt;Matias D. Cattaneo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Klusowski_J/0/1/0/all/0/1&quot;&gt;Jason M. Klusowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tian_P/0/1/0/all/0/1&quot;&gt;Peter M. Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.12316">
<title>Simplicity Bias in Transformers and their Ability to Learn Sparse Boolean Functions. (arXiv:2211.12316v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.12316</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the widespread success of Transformers on NLP tasks, recent works
have found that they struggle to model several formal languages when compared
to recurrent models. This raises the question of why Transformers perform well
in practice and whether they have any properties that enable them to generalize
better than recurrent models. In this work, we conduct an extensive empirical
study on Boolean functions to demonstrate the following: (i) Random
Transformers are relatively more biased towards functions of low sensitivity.
(ii) When trained on Boolean functions, both Transformers and LSTMs prioritize
learning functions of low sensitivity, with Transformers ultimately converging
to functions of lower sensitivity. (iii) On sparse Boolean functions which have
low sensitivity, we find that Transformers generalize near perfectly even in
the presence of noisy labels whereas LSTMs overfit and achieve poor
generalization accuracy. Overall, our results provide strong quantifiable
evidence that suggests differences in the inductive biases of Transformers and
recurrent models which may help explain Transformer&apos;s effective generalization
performance despite relatively limited expressiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattamishra_S/0/1/0/all/0/1&quot;&gt;Satwik Bhattamishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1&quot;&gt;Arkil Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanade_V/0/1/0/all/0/1&quot;&gt;Varun Kanade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blunsom_P/0/1/0/all/0/1&quot;&gt;Phil Blunsom&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.13291">
<title>Learning and Testing Latent-Tree Ising Models Efficiently. (arXiv:2211.13291v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.13291</link>
<description rdf:parseType="Literal">&lt;p&gt;We provide time- and sample-efficient algorithms for learning and testing
latent-tree Ising models, i.e. Ising models that may only be observed at their
leaf nodes. On the learning side, we obtain efficient algorithms for learning a
tree-structured Ising model whose leaf node distribution is close in Total
Variation Distance, improving on the results of prior work. On the testing
side, we provide an efficient algorithm with fewer samples for testing whether
two latent-tree Ising models have leaf-node distributions that are close or far
in Total Variation distance. We obtain our algorithms by showing novel
localization results for the total variation distance between the leaf-node
distributions of tree-structured Ising models, in terms of their marginals on
pairs of leaves.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choo_D/0/1/0/all/0/1&quot;&gt;Davin Choo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dagan_Y/0/1/0/all/0/1&quot;&gt;Yuval Dagan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daskalakis_C/0/1/0/all/0/1&quot;&gt;Constantinos Daskalakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kandiros_A/0/1/0/all/0/1&quot;&gt;Anthimos Vardis Kandiros&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.14960">
<title>Label Alignment Regularization for Distribution Shift. (arXiv:2211.14960v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.14960</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work has highlighted the label alignment property (LAP) in supervised
learning, where the vector of all labels in the dataset is mostly in the span
of the top few singular vectors of the data matrix. Drawing inspiration from
this observation, we propose a regularization method for unsupervised domain
adaptation that encourages alignment between the predictions in the target
domain and its top singular vectors. Unlike conventional domain adaptation
approaches that focus on regularizing representations, we instead regularize
the classifier to align with the unsupervised target data, guided by the LAP in
both the source and target domains. Theoretical analysis demonstrates that,
under certain assumptions, our solution resides within the span of the top
right singular vectors of the target domain data and aligns with the optimal
solution. By removing the reliance on the commonly used optimal joint risk
assumption found in classic domain adaptation theory, we showcase the
effectiveness of our method on addressing problems where traditional domain
adaptation methods often fall short due to high joint error. Additionally, we
report improved performance over domain adaptation baselines in well-known
tasks such as MNIST-USPS domain adaptation and cross-lingual sentiment
analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Imani_E/0/1/0/all/0/1&quot;&gt;Ehsan Imani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guojun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Jun Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poupart_P/0/1/0/all/0/1&quot;&gt;Pascal Poupart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1&quot;&gt;Philip H.S. Torr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yangchen Pan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.15657">
<title>Is Conditional Generative Modeling all you need for Decision-Making?. (arXiv:2211.15657v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.15657</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent improvements in conditional generative modeling have made it possible
to generate high-quality images from language descriptions alone. We
investigate whether these methods can directly address the problem of
sequential decision-making. We view decision-making not through the lens of
reinforcement learning (RL), but rather through conditional generative
modeling. To our surprise, we find that our formulation leads to policies that
can outperform existing offline RL approaches across standard benchmarks. By
modeling a policy as a return-conditional diffusion model, we illustrate how we
may circumvent the need for dynamic programming and subsequently eliminate many
of the complexities that come with traditional offline RL. We further
demonstrate the advantages of modeling policies as conditional diffusion models
by considering two other conditioning variables: constraints and skills.
Conditioning on a single constraint or skill during training leads to behaviors
at test-time that can satisfy several constraints together or demonstrate a
composition of skills. Our results illustrate that conditional generative
modeling is a powerful tool for decision-making.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ajay_A/0/1/0/all/0/1&quot;&gt;Anurag Ajay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yilun Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Abhi Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1&quot;&gt;Joshua Tenenbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1&quot;&gt;Tommi Jaakkola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1&quot;&gt;Pulkit Agrawal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.06951">
<title>AI Ethics on Blockchain: Topic Analysis on Twitter Data for Blockchain Security. (arXiv:2212.06951v5 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2212.06951</link>
<description rdf:parseType="Literal">&lt;p&gt;Blockchain has empowered computer systems to be more secure using a
distributed network. However, the current blockchain design suffers from
fairness issues in transaction ordering. Miners are able to reorder
transactions to generate profits, the so-called miner extractable value (MEV).
Existing research recognizes MEV as a severe security issue and proposes
potential solutions, including prominent Flashbots. However, previous studies
have mostly analyzed blockchain data, which might not capture the impacts of
MEV in a much broader AI society. Thus, in this research, we applied natural
language processing (NLP) methods to comprehensively analyze topics in tweets
on MEV. We collected more than 20000 tweets with #MEV and #Flashbots hashtags
and analyzed their topics. Our results show that the tweets discussed profound
topics of ethical concern, including security, equity, emotional sentiments,
and the desire for solutions to MEV. We also identify the co-movements of MEV
activities on blockchain and social media platforms. Our study contributes to
the literature at the interface of blockchain security, MEV solutions, and AI
ethics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yihang Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1&quot;&gt;Zesen Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Luyao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.14106">
<title>Provable Robust Saliency-based Explanations. (arXiv:2212.14106v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.14106</link>
<description rdf:parseType="Literal">&lt;p&gt;Robust explanations of machine learning models are critical to establishing
human trust in the models. The top-$k$ intersection is widely used to evaluate
the robustness of explanations. However, most existing attacking and defense
strategies are based on $\ell_p$ norms, thus creating a mismatch between the
evaluation and optimization objectives. To this end, we define explanation
thickness for measuring top-$k$ salient features ranking stability, and design
the \textit{R2ET} algorithm based on a novel tractable surrogate to maximize
the thickness and stabilize the top salient features efficiently.
Theoretically, we prove a connection between R2ET and adversarial training;
using a novel multi-objective optimization formulation and a generalization
error bound, we further prove that the surrogate objective can improve both the
numerical and statistical stability of the explanations. Experiments with a
wide spectrum of network architectures and data modalities demonstrate that
R2ET attains higher explanation robustness under stealthy attacks while
retaining model accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1&quot;&gt;Chenghua Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_G/0/1/0/all/0/1&quot;&gt;Guixiang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1&quot;&gt;Ming Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1&quot;&gt;Sihong Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.01828">
<title>On Sequential Bayesian Inference for Continual Learning. (arXiv:2301.01828v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.01828</link>
<description rdf:parseType="Literal">&lt;p&gt;Sequential Bayesian inference can be used for continual learning to prevent
catastrophic forgetting of past tasks and provide an informative prior when
learning new tasks. We revisit sequential Bayesian inference and test whether
having access to the true posterior is guaranteed to prevent catastrophic
forgetting in Bayesian neural networks. To do this we perform sequential
Bayesian inference using Hamiltonian Monte Carlo. We propagate the posterior as
a prior for new tasks by fitting a density estimator on Hamiltonian Monte Carlo
samples. We find that this approach fails to prevent catastrophic forgetting
demonstrating the difficulty in performing sequential Bayesian inference in
neural networks. From there we study simple analytical examples of sequential
Bayesian inference and CL and highlight the issue of model misspecification
which can lead to sub-optimal continual learning performance despite exact
inference. Furthermore, we discuss how task data imbalances can cause
forgetting. From these limitations, we argue that we need probabilistic models
of the continual learning generative process rather than relying on sequential
Bayesian inference over Bayesian neural network weights. In this vein, we also
propose a simple baseline called Prototypical Bayesian Continual Learning,
which is competitive with state-of-the-art Bayesian continual learning methods
on class incremental continual learning vision benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kessler_S/0/1/0/all/0/1&quot;&gt;Samuel Kessler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cobb_A/0/1/0/all/0/1&quot;&gt;Adam Cobb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rudner_T/0/1/0/all/0/1&quot;&gt;Tim G. J. Rudner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zohren_S/0/1/0/all/0/1&quot;&gt;Stefan Zohren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roberts_S/0/1/0/all/0/1&quot;&gt;Stephen J. Roberts&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.07733">
<title>Learning-Rate-Free Learning by D-Adaptation. (arXiv:2301.07733v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.07733</link>
<description rdf:parseType="Literal">&lt;p&gt;D-Adaptation is an approach to automatically setting the learning rate which
asymptotically achieves the optimal rate of convergence for minimizing convex
Lipschitz functions, with no back-tracking or line searches, and no additional
function value or gradient evaluations per step. Our approach is the first
hyper-parameter free method for this class without additional multiplicative
log factors in the convergence rate. We present extensive experiments for SGD
and Adam variants of our method, where the method automatically matches
hand-tuned learning rates across more than a dozen diverse machine learning
problems, including large-scale vision and language problems.
&lt;/p&gt;
&lt;p&gt;An open-source implementation is available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Defazio_A/0/1/0/all/0/1&quot;&gt;Aaron Defazio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishchenko_K/0/1/0/all/0/1&quot;&gt;Konstantin Mishchenko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.12068">
<title>Pre-Training Protein Encoder via Siamese Sequence-Structure Diffusion Trajectory Prediction. (arXiv:2301.12068v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.12068</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised pre-training methods on proteins have recently gained
attention, with most approaches focusing on either protein sequences or
structures, neglecting the exploration of their joint distribution, which is
crucial for a comprehensive understanding of protein functions by integrating
co-evolutionary information and structural characteristics. In this work,
inspired by the success of denoising diffusion models in generative tasks, we
propose the DiffPreT approach to pre-train a protein encoder by
sequence-structure joint diffusion modeling. DiffPreT guides the encoder to
recover the native protein sequences and structures from the perturbed ones
along the joint diffusion trajectory, which acquires the joint distribution of
sequences and structures. Considering the essential protein conformational
variations, we enhance DiffPreT by a method called Siamese Diffusion Trajectory
Prediction (SiamDiff) to capture the correlation between different conformers
of a protein. SiamDiff attains this goal by maximizing the mutual information
between representations of diffusion trajectories of structurally-correlated
conformers. We study the effectiveness of DiffPreT and SiamDiff on both atom-
and residue-level structure-based protein understanding tasks. Experimental
results show that the performance of DiffPreT is consistently competitive on
all tasks, and SiamDiff achieves new state-of-the-art performance, considering
the mean ranks on all tasks. Our implementation is available at
https://github.com/DeepGraphLearning/SiamDiff.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zuobai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Minghao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lozano_A/0/1/0/all/0/1&quot;&gt;Aur&amp;#xe9;lie Lozano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chenthamarakshan_V/0/1/0/all/0/1&quot;&gt;Vijil Chenthamarakshan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1&quot;&gt;Payel Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jian Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.00160">
<title>Local transfer learning from one data space to another. (arXiv:2302.00160v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.00160</link>
<description rdf:parseType="Literal">&lt;p&gt;A fundamental problem in manifold learning is to approximate a functional
relationship in a data chosen randomly from a probability distribution
supported on a low dimensional sub-manifold of a high dimensional ambient
Euclidean space. The manifold is essentially defined by the data set itself
and, typically, designed so that the data is dense on the manifold in some
sense. The notion of a data space is an abstraction of a manifold encapsulating
the essential properties that allow for function approximation. The problem of
transfer learning (meta-learning) is to use the learning of a function on one
data set to learn a similar function on a new data set. In terms of function
approximation, this means lifting a function on one data space (the base data
space) to another (the target data space). This viewpoint enables us to connect
some inverse problems in applied mathematics (such as inverse Radon transform)
with transfer learning. In this paper we examine the question of such lifting
when the data is assumed to be known only on a part of the base data space. We
are interested in determining subsets of the target data space on which the
lifting can be defined, and how the local smoothness of the function and its
lifting are related.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mhaskar_H/0/1/0/all/0/1&quot;&gt;H. N. Mhaskar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+ODowd_R/0/1/0/all/0/1&quot;&gt;Ryan O&amp;#x27;Dowd&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.01018">
<title>Graph Neural Networks for temporal graphs: State of the art, open challenges, and opportunities. (arXiv:2302.01018v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.01018</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) have become the leading paradigm for learning on
(static) graph-structured data. However, many real-world systems are dynamic in
nature, since the graph and node/edge attributes change over time. In recent
years, GNN-based models for temporal graphs have emerged as a promising area of
research to extend the capabilities of GNNs. In this work, we provide the first
comprehensive overview of the current state-of-the-art of temporal GNN,
introducing a rigorous formalization of learning settings and tasks and a novel
taxonomy categorizing existing approaches in terms of how the temporal aspect
is represented and processed. We conclude the survey with a discussion of the
most relevant open challenges for the field, from both research and application
perspectives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Longa_A/0/1/0/all/0/1&quot;&gt;Antonio Longa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lachi_V/0/1/0/all/0/1&quot;&gt;Veronica Lachi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santin_G/0/1/0/all/0/1&quot;&gt;Gabriele Santin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bianchini_M/0/1/0/all/0/1&quot;&gt;Monica Bianchini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lepri_B/0/1/0/all/0/1&quot;&gt;Bruno Lepri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1&quot;&gt;Pietro Lio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scarselli_F/0/1/0/all/0/1&quot;&gt;Franco Scarselli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Passerini_A/0/1/0/all/0/1&quot;&gt;Andrea Passerini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.02766">
<title>Generalization Bounds with Data-dependent Fractal Dimensions. (arXiv:2302.02766v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2302.02766</link>
<description rdf:parseType="Literal">&lt;p&gt;Providing generalization guarantees for modern neural networks has been a
crucial task in statistical learning. Recently, several studies have attempted
to analyze the generalization error in such settings by using tools from
fractal geometry. While these works have successfully introduced new
mathematical tools to apprehend generalization, they heavily rely on a
Lipschitz continuity assumption, which in general does not hold for neural
networks and might make the bounds vacuous. In this work, we address this issue
and prove fractal geometry-based generalization bounds without requiring any
Lipschitz assumption. To achieve this goal, we build up on a classical covering
argument in learning theory and introduce a data-dependent fractal dimension.
Despite introducing a significant amount of technical complications, this new
notion lets us control the generalization error (over either fixed or random
hypothesis spaces) along with certain mutual information (MI) terms. To provide
a clearer interpretation to the newly introduced MI terms, as a next step, we
introduce a notion of &quot;geometric stability&quot; and link our bounds to the prior
art. Finally, we make a rigorous connection between the proposed data-dependent
dimension and topological data analysis tools, which then enables us to compute
the dimension in a numerically efficient way. We support our theory with
experiments conducted on various settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dupuis_B/0/1/0/all/0/1&quot;&gt;Benjamin Dupuis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Deligiannidis_G/0/1/0/all/0/1&quot;&gt;George Deligiannidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Simsekli_U/0/1/0/all/0/1&quot;&gt;Umut &amp;#x15e;im&amp;#x15f;ekli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.07419">
<title>Spatially heterogeneous learning by a deep student machine. (arXiv:2302.07419v4 [cond-mat.dis-nn] UPDATED)</title>
<link>http://arxiv.org/abs/2302.07419</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNN) with a huge number of adjustable parameters remain
largely black boxes. To shed light on the hidden layers of DNN, we study
supervised learning by a DNN of width $N$ and depth $L$ consisting of $NL$
perceptrons with $c$ inputs by a statistical mechanics approach called the
teacher-student setting. We consider an ensemble of student machines that
exactly reproduce $M$ sets of $N$ dimensional input/output relations provided
by a teacher machine. We show that the problem becomes exactly solvable in what
we call as &apos;dense limit&apos;: $N \gg c \gg 1$ and $M \gg 1$ with fixed $\alpha=M/c$
using the replica method developed in (H. Yoshino, (2020)). We also study the
model numerically performing simple greedy MC simulations. Simulations reveal
that learning by the DNN is quite heterogeneous in the network space:
configurations of the teacher and the student machines are more correlated
within the layers closer to the input/output boundaries while the central
region remains much less correlated due to the over-parametrization in
qualitative agreement with the theoretical prediction. We evaluate the
generalization-error of the DNN with various depth $L$ both theoretically and
numerically. Remarkably both the theory and simulation suggest
generalization-ability of the student machines, which are only weakly
correlated with the teacher in the center, does not vanish even in the deep
limit $L \gg 1$ where the system becomes heavily over-parametrized. We also
consider the impact of effective dimension $D(\leq N)$ of data by incorporating
the hidden manifold model (S. Goldt et. al., (2020)) into our model. The theory
implies that the loop corrections to the dense limit become enhanced by either
decreasing the width $N$ or decreasing the effective dimension $D$ of the data.
Simulation suggests both lead to significant improvements in
generalization-ability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Yoshino_H/0/1/0/all/0/1&quot;&gt;Hajime Yoshino&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.08332">
<title>Magnetohydrodynamics with Physics Informed Neural Operators. (arXiv:2302.08332v2 [physics.comp-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2302.08332</link>
<description rdf:parseType="Literal">&lt;p&gt;The modeling of multi-scale and multi-physics complex systems typically
involves the use of scientific software that can optimally leverage extreme
scale computing. Despite major developments in recent years, these simulations
continue to be computationally intensive and time consuming. Here we explore
the use of AI to accelerate the modeling of complex systems at a fraction of
the computational cost of classical methods, and present the first application
of physics informed neural operators to model 2D incompressible
magnetohydrodynamics simulations. Our AI models incorporate tensor Fourier
neural operators as their backbone, which we implemented with the TensorLY
package. Our results indicate that physics informed neural operators can
accurately capture the physics of magnetohydrodynamics simulations that
describe laminar flows with Reynolds numbers $Re\leq250$. We also explore the
applicability of our AI surrogates for turbulent flows, and discuss a variety
of methodologies that may be incorporated in future work to create AI models
that provide a computationally efficient and high fidelity description of
magnetohydrodynamics simulations for a broad range of Reynolds numbers. The
scientific software developed in this project is released with this manuscript.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Rosofsky_S/0/1/0/all/0/1&quot;&gt;Shawn G. Rosofsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Huerta_E/0/1/0/all/0/1&quot;&gt;E. A. Huerta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.10325">
<title>Adaptive Sparse Gaussian Process. (arXiv:2302.10325v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.10325</link>
<description rdf:parseType="Literal">&lt;p&gt;Adaptive learning is necessary for non-stationary environments where the
learning machine needs to forget past data distribution. Efficient algorithms
require a compact model update to not grow in computational burden with the
incoming data and with the lowest possible computational cost for online
parameter updating. Existing solutions only partially cover these needs. Here,
we propose the first adaptive sparse Gaussian Process (GP) able to address all
these issues. We first reformulate a variational sparse GP algorithm to make it
adaptive through a forgetting factor. Next, to make the model inference as
simple as possible, we propose updating a single inducing point of the sparse
GP model together with the remaining model parameters every time a new sample
arrives. As a result, the algorithm presents a fast convergence of the
inference process, which allows an efficient model update (with a single
inference iteration) even in highly non-stationary environments. Experimental
results demonstrate the capabilities of the proposed algorithm and its good
performance in modeling the predictive posterior in mean and confidence
interval estimation compared to state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomez_Verdejo_V/0/1/0/all/0/1&quot;&gt;Vanessa G&amp;#xf3;mez-Verdejo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parrado_Hernandez_E/0/1/0/all/0/1&quot;&gt;Emilio Parrado-Hern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_Ramon_M/0/1/0/all/0/1&quot;&gt;Manel Mart&amp;#xed;nez-Ram&amp;#xf3;n&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.13485">
<title>FedCLIP: Fast Generalization and Personalization for CLIP in Federated Learning. (arXiv:2302.13485v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.13485</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) has emerged as a new paradigm for privacy-preserving
computation in recent years. Unfortunately, FL faces two critical challenges
that hinder its actual performance: data distribution heterogeneity and high
resource costs brought by large foundation models. Specifically, the non-IID
data in different clients make existing FL algorithms hard to converge while
the high resource costs, including computational and communication costs that
increase the deployment difficulty in real-world scenarios. In this paper, we
propose an effective yet simple method, named FedCLIP, to achieve fast
generalization and personalization for CLIP in federated learning. Concretely,
we design an attention-based adapter for the large model, CLIP, and the rest
operations merely depend on adapters. Lightweight adapters can make the most
use of pretrained model information and ensure models be adaptive for clients
in specific tasks. Simultaneously, small-scale operations can mitigate the
computational burden and communication burden caused by large models. Extensive
experiments are conducted on three datasets with distribution shifts.
Qualitative and quantitative results demonstrate that FedCLIP significantly
outperforms other baselines (9% overall improvements on PACS) and effectively
reduces computational and communication costs (283x faster than FedAVG). Our
code will be available at: https://github.com/microsoft/PersonalizedFL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1&quot;&gt;Wang Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xixu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jindong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xing Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.14686">
<title>Approximately Stationary Bandits with Knapsacks. (arXiv:2302.14686v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.14686</link>
<description rdf:parseType="Literal">&lt;p&gt;Bandits with Knapsacks (BwK), the generalization of the Bandits problem under
global budget constraints, has received a lot of attention in recent years.
Previous work has focused on one of the two extremes: Stochastic BwK where the
rewards and consumptions of the resources of each round are sampled from an
i.i.d. distribution, and Adversarial BwK where these parameters are picked by
an adversary. Achievable guarantees in the two cases exhibit a massive gap:
No-regret learning is achievable in the stochastic case, but in the adversarial
case only competitive ratio style guarantees are achievable, where the
competitive ratio depends either on the budget or on both the time and the
number of resources. What makes this gap so vast is that in Adversarial BwK the
guarantees get worse in the typical case when the budget is more binding. While
``best-of-both-worlds&apos;&apos; type algorithms are known (single algorithms that
provide the best achievable guarantee in each extreme case), their bounds
degrade to the adversarial case as soon as the environment is not fully
stochastic.
&lt;/p&gt;
&lt;p&gt;Our work aims to bridge this gap, offering guarantees for a workload that is
not exactly stochastic but is also not worst-case. We define a condition,
Approximately Stationary BwK, that parameterizes how close to stochastic or
adversarial an instance is. Based on these parameters, we explore what is the
best competitive ratio attainable in BwK. We explore two algorithms that are
oblivious to the values of the parameters but guarantee competitive ratios that
smoothly transition between the best possible guarantees in the two extreme
cases, depending on the values of the parameters. Our guarantees offer great
improvement over the adversarial guarantee, especially when the available
budget is small. We also prove bounds on the achievable guarantee, showing that
our results are approximately tight when the budget is small.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fikioris_G/0/1/0/all/0/1&quot;&gt;Giannis Fikioris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tardos_E/0/1/0/all/0/1&quot;&gt;&amp;#xc9;va Tardos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.00501">
<title>OmniForce: On Human-Centered, Large Model Empowered and Cloud-Edge Collaborative AutoML System. (arXiv:2303.00501v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.00501</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated machine learning (AutoML) seeks to build ML models with minimal
human effort. While considerable research has been conducted in the area of
AutoML in general, aiming to take humans out of the loop when building
artificial intelligence (AI) applications, scant literature has focused on how
AutoML works well in open-environment scenarios such as the process of training
and updating large models, industrial supply chains or the industrial
metaverse, where people often face open-loop problems during the search
process: they must continuously collect data, update data and models, satisfy
the requirements of the development and deployment environment, support massive
devices, modify evaluation metrics, etc. Addressing the open-environment issue
with pure data-driven approaches requires considerable data, computing
resources, and effort from dedicated data engineers, making current AutoML
systems and platforms inefficient and computationally intractable.
Human-computer interaction is a practical and feasible way to tackle the
problem of open-environment AI. In this paper, we introduce OmniForce, a
human-centered AutoML (HAML) system that yields both human-assisted ML and
ML-assisted human techniques, to put an AutoML system into practice and build
adaptive AI in open-environment scenarios. Specifically, we present OmniForce
in terms of ML version management; pipeline-driven development and deployment
collaborations; a flexible search strategy framework; and widely provisioned
and crowdsourced application algorithms, including large models. Furthermore,
the (large) models constructed by OmniForce can be automatically turned into
remote services in a few minutes; this process is dubbed model as a service
(MaaS). Experimental results obtained in multiple search spaces and real-world
use cases demonstrate the efficacy and efficiency of OmniForce.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_C/0/1/0/all/0/1&quot;&gt;Chao Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1&quot;&gt;Shuai Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhenfang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiaxing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1&quot;&gt;Xuyang Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1&quot;&gt;Liang Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shanshan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1&quot;&gt;Qiong Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yibo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1&quot;&gt;Fengxiang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_B/0/1/0/all/0/1&quot;&gt;Bohua Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_R/0/1/0/all/0/1&quot;&gt;Rongcheng Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yiyan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Heliang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiangyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dongkai Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Daqing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Li Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shijin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yukang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guanpu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shixiang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1&quot;&gt;Yibing Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chaoyue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.01483">
<title>Auxiliary Functions as Koopman Observables: Data-Driven Analysis of Dynamical Systems via Polynomial Optimization. (arXiv:2303.01483v3 [math.DS] UPDATED)</title>
<link>http://arxiv.org/abs/2303.01483</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a flexible data-driven method for dynamical system analysis that
does not require explicit model discovery. The method is rooted in
well-established techniques for approximating the Koopman operator from data
and is implemented as a semidefinite program that can be solved numerically.
Furthermore, the method is agnostic of whether data is generated through a
deterministic or stochastic process, so its implementation requires no prior
adjustments by the user to accommodate these different scenarios. Rigorous
convergence results justify the applicability of the method, while also
extending and uniting similar results from across the literature. Examples on
discovering Lyapunov functions, performing ergodic optimization, and bounding
extrema over attractors for both deterministic and stochastic dynamics
exemplify these convergence results and demonstrate the performance of the
method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Bramburger_J/0/1/0/all/0/1&quot;&gt;Jason J. Bramburger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Fantuzzi_G/0/1/0/all/0/1&quot;&gt;Giovanni Fantuzzi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.02719">
<title>A Comparative Study of Self-Supervised Speech Representations in Read and Spontaneous TTS. (arXiv:2303.02719v2 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2303.02719</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work has explored using self-supervised learning (SSL) speech
representations such as wav2vec2.0 as the representation medium in standard
two-stage TTS, in place of conventionally used mel-spectrograms. It is however
unclear which speech SSL is the better fit for TTS, and whether or not the
performance differs between read and spontaneous TTS, the later of which is
arguably more challenging. This study aims at addressing these questions by
testing several speech SSLs, including different layers of the same SSL, in
two-stage TTS on both read and spontaneous corpora, while maintaining constant
TTS model architecture and training settings. Results from listening tests show
that the 9th layer of 12-layer wav2vec2.0 (ASR finetuned) outperforms other
tested SSLs and mel-spectrogram, in both read and spontaneous TTS. Our work
sheds light on both how speech SSL can readily improve current TTS systems, and
how SSLs compare in the challenging generative task of TTS. Audio examples can
be found at https://www.speech.kth.se/tts-demos/ssr_tts
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Siyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Henter_G/0/1/0/all/0/1&quot;&gt;Gustav Eje Henter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gustafson_J/0/1/0/all/0/1&quot;&gt;Joakim Gustafson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Szekely_E/0/1/0/all/0/1&quot;&gt;&amp;#xc9;va Sz&amp;#xe9;kely&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.04865">
<title>Convergence Rates for Localized Actor-Critic in Networked Markov Potential Games. (arXiv:2303.04865v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.04865</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a class of networked Markov potential games in which agents are
associated with nodes in a network. Each agent has its own local potential
function, and the reward of each agent depends only on the states and actions
of the agents within a neighborhood. In this context, we propose a localized
actor-critic algorithm. The algorithm is scalable since each agent uses only
local information and does not need access to the global state. Further, the
algorithm overcomes the curse of dimensionality through the use of function
approximation. Our main results provide finite-sample guarantees up to a
localization error and a function approximation error. Specifically, we achieve
an $\tilde{\mathcal{O}}(\tilde{\epsilon}^{-4})$ sample complexity measured by
the averaged Nash regret. This is the first finite-sample bound for multi-agent
competitive games that does not depend on the number of agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhaoyi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zaiwei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yiheng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wierman_A/0/1/0/all/0/1&quot;&gt;Adam Wierman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.06010">
<title>Local-Global Methods for Generalised Solar Irradiance Forecasting. (arXiv:2303.06010v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.06010</link>
<description rdf:parseType="Literal">&lt;p&gt;As the use of solar power increases, having accurate and timely forecasts
will be essential for smooth grid operators. There are many proposed methods
for forecasting solar irradiance / solar power production. However, many of
these methods formulate the problem as a time-series, relying on near real-time
access to observations at the location of interest to generate forecasts. This
requires both access to a real-time stream of data and enough historical
observations for these methods to be deployed. In this paper, we propose the
use of Global methods to train our models in a generalised way, enabling them
to generate forecasts for unseen locations. We apply this approach to both
classical ML and state of the art methods. Using data from 20 locations
distributed throughout the UK and widely available weather data, we show that
it is possible to build systems that do not require access to this data. We
utilise and compare both satellite and ground observations (e.g. temperature,
pressure) of weather data. Leveraging weather observations and measurements
from other locations we show it is possible to create models capable of
accurately forecasting solar irradiance at new locations. This could facilitate
use planning and optimisation for both newly deployed solar farms and domestic
installations from the moment they come online. Additionally, we show that
training a single global model for multiple locations can produce a more robust
model with more consistent and accurate results across locations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cargan_T/0/1/0/all/0/1&quot;&gt;Timothy Cargan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Landa_Silva_D/0/1/0/all/0/1&quot;&gt;Dario Landa-Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Triguero_I/0/1/0/all/0/1&quot;&gt;Isaac Triguero&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.07308">
<title>NeuSE: Neural SE(3)-Equivariant Embedding for Consistent Spatial Understanding with Objects. (arXiv:2303.07308v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2303.07308</link>
<description rdf:parseType="Literal">&lt;p&gt;We present NeuSE, a novel Neural SE(3)-Equivariant Embedding for objects, and
illustrate how it supports object SLAM for consistent spatial understanding
with long-term scene changes. NeuSE is a set of latent object embeddings
created from partial object observations. It serves as a compact point cloud
surrogate for complete object models, encoding full shape information while
transforming SE(3)-equivariantly in tandem with the object in the physical
world. With NeuSE, relative frame transforms can be directly derived from
inferred latent codes. Our proposed SLAM paradigm, using NeuSE for object shape
and pose characterization, can operate independently or in conjunction with
typical SLAM systems. It directly infers SE(3) camera pose constraints that are
compatible with general SLAM pose graph optimization, while also maintaining a
lightweight object-centric map that adapts to real-world changes. Our approach
is evaluated on synthetic and real-world sequences featuring changed objects
and shows improved localization accuracy and change-aware mapping capability,
when working either standalone or jointly with a common SLAM pipeline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jiahui Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yilun Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1&quot;&gt;Kurran Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1&quot;&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leonard_J/0/1/0/all/0/1&quot;&gt;John J. Leonard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.10834">
<title>Object-Centric Slot Diffusion. (arXiv:2303.10834v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.10834</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent success of transformer-based image generative models in
object-centric learning highlights the importance of powerful image generators
for handling complex scenes. However, despite the high expressiveness of
diffusion models in image generation, their integration into object-centric
learning remains largely unexplored in this domain. In this paper, we explore
the feasibility and potential of integrating diffusion models into
object-centric learning and investigate the pros and cons of this approach. We
introduce Latent Slot Diffusion (LSD), a novel model that serves dual purposes:
it is the first object-centric learning model to replace conventional slot
decoders with a latent diffusion model conditioned on object slots, and it is
also the first unsupervised compositional conditional diffusion model that
operates without the need for supervised annotations like text. Through
experiments on various object-centric tasks, including the first application of
the FFHQ dataset in this field, we demonstrate that LSD significantly
outperforms state-of-the-art transformer-based decoders, particularly in more
complex scenes, and exhibits superior unsupervised compositional generation
quality. Project page is available at
$\href{https://latentslotdiffusion.github.io}{here}$
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jindong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_F/0/1/0/all/0/1&quot;&gt;Fei Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1&quot;&gt;Gautam Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1&quot;&gt;Sungjin Ahn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.11582">
<title>Adaptive Experimentation at Scale: A Computational Framework for Flexible Batches. (arXiv:2303.11582v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.11582</link>
<description rdf:parseType="Literal">&lt;p&gt;Standard bandit algorithms that assume continual reallocation of measurement
effort are challenging to implement due to delayed feedback and
infrastructural/organizational difficulties. Motivated by practical instances
involving a handful of reallocation epochs in which outcomes are measured in
batches, we develop a computation-driven adaptive experimentation framework
that can flexibly handle batching. Our main observation is that normal
approximations, which are universal in statistical inference, can also guide
the design of adaptive algorithms. By deriving a Gaussian sequential
experiment, we formulate a dynamic program that can leverage prior information
on average rewards. Instead of the typical theory-driven paradigm, we leverage
computational tools and empirical benchmarking for algorithm development. In
particular, our empirical analysis highlights a simple yet effective algorithm,
Residual Horizon Optimization, which iteratively solves a planning problem
using stochastic gradient descent. Our approach significantly improves
statistical power over standard methods, even when compared to Bayesian bandit
algorithms (e.g., Thompson sampling) that require full distributional knowledge
of individual rewards. Overall, we expand the scope of adaptive experimentation
to settings that are difficult for standard methods, involving a small number
of reallocation epochs, low signal-to-noise ratio, and unknown reward
distributions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Che_E/0/1/0/all/0/1&quot;&gt;Ethan Che&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Namkoong_H/0/1/0/all/0/1&quot;&gt;Hongseok Namkoong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.14773">
<title>BlackVIP: Black-Box Visual Prompting for Robust Transfer Learning. (arXiv:2303.14773v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.14773</link>
<description rdf:parseType="Literal">&lt;p&gt;With the surge of large-scale pre-trained models (PTMs), fine-tuning these
models to numerous downstream tasks becomes a crucial problem. Consequently,
parameter efficient transfer learning (PETL) of large models has grasped huge
attention. While recent PETL methods showcase impressive performance, they rely
on optimistic assumptions: 1) the entire parameter set of a PTM is available,
and 2) a sufficiently large memory capacity for the fine-tuning is equipped.
However, in most real-world applications, PTMs are served as a black-box API or
proprietary software without explicit parameter accessibility. Besides, it is
hard to meet a large memory requirement for modern PTMs. In this work, we
propose black-box visual prompting (BlackVIP), which efficiently adapts the
PTMs without knowledge about model architectures and parameters. BlackVIP has
two components; 1) Coordinator and 2) simultaneous perturbation stochastic
approximation with gradient correction (SPSA-GC). The Coordinator designs
input-dependent image-shaped visual prompts, which improves few-shot adaptation
and robustness on distribution/location shift. SPSA-GC efficiently estimates
the gradient of a target model to update Coordinator. Extensive experiments on
16 datasets demonstrate that BlackVIP enables robust adaptation to diverse
domains without accessing PTMs&apos; parameters, with minimal memory requirements.
Code: \url{https://github.com/changdaeoh/BlackVIP}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_C/0/1/0/all/0/1&quot;&gt;Changdae Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_H/0/1/0/all/0/1&quot;&gt;Hyeji Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hee-young Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_Y/0/1/0/all/0/1&quot;&gt;YongTaek Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_G/0/1/0/all/0/1&quot;&gt;Geunyoung Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1&quot;&gt;Jiyoung Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1&quot;&gt;Hosik Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1&quot;&gt;Kyungwoo Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.16897">
<title>Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos. (arXiv:2303.16897v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.16897</link>
<description rdf:parseType="Literal">&lt;p&gt;Modeling sounds emitted from physical object interactions is critical for
immersive perceptual experiences in real and virtual worlds. Traditional
methods of impact sound synthesis use physics simulation to obtain a set of
physics parameters that could represent and synthesize the sound. However, they
require fine details of both the object geometries and impact locations, which
are rarely available in the real world and can not be applied to synthesize
impact sounds from common videos. On the other hand, existing video-driven deep
learning-based approaches could only capture the weak correspondence between
visual content and impact sounds since they lack of physics knowledge. In this
work, we propose a physics-driven diffusion model that can synthesize
high-fidelity impact sound for a silent video clip. In addition to the video
content, we propose to use additional physics priors to guide the impact sound
synthesis procedure. The physics priors include both physics parameters that
are directly estimated from noisy real-world impact sound examples without
sophisticated setup and learned residual parameters that interpret the sound
environment via neural networks. We further implement a novel diffusion model
with specific training and inference strategies to combine physics priors and
visual information for impact sound synthesis. Experimental results show that
our model outperforms several existing systems in generating realistic impact
sounds. More importantly, the physics-based representations are fully
interpretable and transparent, thus enabling us to perform sound editing
flexibly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_K/0/1/0/all/0/1&quot;&gt;Kun Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_K/0/1/0/all/0/1&quot;&gt;Kaizhi Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shlizerman_E/0/1/0/all/0/1&quot;&gt;Eli Shlizerman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1&quot;&gt;Antonio Torralba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1&quot;&gt;Chuang Gan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.17607">
<title>Machine learning for discovering laws of nature. (arXiv:2303.17607v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.17607</link>
<description rdf:parseType="Literal">&lt;p&gt;Based on Darwin&apos;s natural selection, we developed &quot;machine scientists&quot; to
discover the laws of nature by learning from raw data. &quot;Machine scientists&quot;
construct physical theories by applying a logic tree (state Decision Tree) and
a value tree (observation Function Tree); the logical tree determines the state
of the entity, and the value tree determines the absolute value between the two
observations of the entity. A logic Tree and a value tree together can
reconstruct an entity&apos;s trajectory and make predictions about its future
outcomes. Our proposed algorithmic model has an emphasis on machine learning -
where &quot;machine scientists&quot; builds up its experience by being rewarded or
punished for each decision they make - eventually leading to rediscovering
Newton&apos;s equation (classical physics) and the Born&apos;s rule (quantum mechanics).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xin_L/0/1/0/all/0/1&quot;&gt;Lizhi Xin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xin_K/0/1/0/all/0/1&quot;&gt;Kevin Xin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xin_H/0/1/0/all/0/1&quot;&gt;Houwen Xin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.00215">
<title>Inductive Relation Prediction from Relational Paths and Context with Hierarchical Transformers. (arXiv:2304.00215v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2304.00215</link>
<description rdf:parseType="Literal">&lt;p&gt;Relation prediction on knowledge graphs (KGs) is a key research topic.
Dominant embedding-based methods mainly focus on the transductive setting and
lack the inductive ability to generalize to new entities for inference.
Existing methods for inductive reasoning mostly mine the connections between
entities, i.e., relational paths, without considering the nature of head and
tail entities contained in the relational context. This paper proposes a novel
method that captures both connections between entities and the intrinsic nature
of entities, by simultaneously aggregating RElational Paths and cOntext with a
unified hieRarchical Transformer framework, namely REPORT. REPORT relies solely
on relation semantics and can naturally generalize to the fully-inductive
setting, where KGs for training and inference have no common entities. In the
experiments, REPORT performs consistently better than all baselines on almost
all the eight version subsets of two fully-inductive datasets. Moreover. REPORT
is interpretable by providing each element&apos;s contribution to the prediction
results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiaang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Quan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1&quot;&gt;Zhendong Mao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.03452">
<title>Graph Enabled Cross-Domain Knowledge Transfer. (arXiv:2304.03452v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.03452</link>
<description rdf:parseType="Literal">&lt;p&gt;To leverage machine learning in any decision-making process, one must convert
the given knowledge (for example, natural language, unstructured text) into
representation vectors that can be understood and processed by machine learning
model in their compatible language and data format. The frequently encountered
difficulty is, however, the given knowledge is not rich or reliable enough in
the first place. In such cases, one seeks to fuse side information from a
separate domain to mitigate the gap between good representation learning and
the scarce knowledge in the domain of interest. This approach is named
Cross-Domain Knowledge Transfer. It is crucial to study the problem because of
the commonality of scarce knowledge in many scenarios, from online healthcare
platform analyses to financial market risk quantification, leaving an obstacle
in front of us benefiting from automated decision making. From the machine
learning perspective, the paradigm of semi-supervised learning takes advantage
of large amount of data without ground truth and achieves impressive learning
performance improvement. It is adopted in this dissertation for cross-domain
knowledge transfer. (to be continued)
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1&quot;&gt;Shibo Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.04000">
<title>SimbaML: Connecting Mechanistic Models and Machine Learning with Augmented Data. (arXiv:2304.04000v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.04000</link>
<description rdf:parseType="Literal">&lt;p&gt;Training sophisticated machine learning (ML) models requires large datasets
that are difficult or expensive to collect for many applications. If prior
knowledge about system dynamics is available, mechanistic representations can
be used to supplement real-world data. We present SimbaML (Simulation-Based
ML), an open-source tool that unifies realistic synthetic dataset generation
from ordinary differential equation-based models and the direct analysis and
inclusion in ML pipelines. SimbaML conveniently enables investigating transfer
learning from synthetic to real-world data, data augmentation, identifying
needs for data collection, and benchmarking physics-informed ML approaches.
SimbaML is available from https://pypi.org/project/simba-ml/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleissl_M/0/1/0/all/0/1&quot;&gt;Maximilian Kleissl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Drews_L/0/1/0/all/0/1&quot;&gt;Lukas Drews&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heyder_B/0/1/0/all/0/1&quot;&gt;Benedict B. Heyder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zabbarov_J/0/1/0/all/0/1&quot;&gt;Julian Zabbarov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iversen_P/0/1/0/all/0/1&quot;&gt;Pascal Iversen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Witzke_S/0/1/0/all/0/1&quot;&gt;Simon Witzke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Renard_B/0/1/0/all/0/1&quot;&gt;Bernhard Y. Renard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baum_K/0/1/0/all/0/1&quot;&gt;Katharina Baum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.10207">
<title>Learning Graph Patterns of Reflection Coefficient for Non-destructive Diagnosis of Cu Interconnects. (arXiv:2304.10207v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.10207</link>
<description rdf:parseType="Literal">&lt;p&gt;With the increasing operating frequencies and clock speeds in processors,
interconnects affect both the reliability and performance of entire electronic
systems. Fault detection and diagnosis of the interconnects are crucial for
prognostics and health management (PHM) of electronics. However, traditional
approaches using electrical signals as prognostic factors often face challenges
in distinguishing defect root causes, necessitating additional destructive
evaluations, and are prone to noise interference, leading to potential false
alarms. To address these limitations, this paper introduces a novel approach
for non-destructive detection and diagnosis of defects in Cu interconnects,
offering early detection, enhanced diagnostic accuracy, and noise resilience.
Our approach uniquely analyzes both the root cause and severity of interconnect
defects by leveraging graph patterns of reflection coefficient, a technique
distinct from traditional time series signal analysis. We experimentally
demonstrate that the graph patterns possess the capability for fault diagnosis
and serve as effective input data for learning algorithms. Additionally, we
introduce a novel severity rating ensemble learning (SREL) approach, which
significantly enhances diagnostic accuracy and noise robustness. Experimental
results demonstrate that the proposed method outperforms conventional machine
learning methods and multi-class convolutional neural networks (CNN), achieving
a maximum accuracy of 99.3%, especially under elevated noise levels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_T/0/1/0/all/0/1&quot;&gt;Tae Yeob Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Haebom Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suh_S/0/1/0/all/0/1&quot;&gt;Sungho Suh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.14251">
<title>Variational Bayes Made Easy. (arXiv:2304.14251v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.14251</link>
<description rdf:parseType="Literal">&lt;p&gt;Variational Bayes is a popular method for approximate inference but its
derivation can be cumbersome. To simplify the process, we give a 3-step recipe
to identify the posterior form by explicitly looking for linearity with respect
to expectations of well-known distributions. We can then directly write the
update by simply ``reading-off&apos;&apos; the terms in front of those expectations. The
recipe makes the derivation easier, faster, shorter, and more general.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Mohammad Emtiyaz Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.01520">
<title>Conditional Graph Information Bottleneck for Molecular Relational Learning. (arXiv:2305.01520v2 [q-bio.MN] UPDATED)</title>
<link>http://arxiv.org/abs/2305.01520</link>
<description rdf:parseType="Literal">&lt;p&gt;Molecular relational learning, whose goal is to learn the interaction
behavior between molecular pairs, got a surge of interest in molecular sciences
due to its wide range of applications. Recently, graph neural networks have
recently shown great success in molecular relational learning by modeling a
molecule as a graph structure, and considering atom-level interactions between
two molecules. Despite their success, existing molecular relational learning
methods tend to overlook the nature of chemistry, i.e., a chemical compound is
composed of multiple substructures such as functional groups that cause
distinctive chemical reactions. In this work, we propose a novel relational
learning framework, called CGIB, that predicts the interaction behavior between
a pair of graphs by detecting core subgraphs therein. The main idea is, given a
pair of graphs, to find a subgraph from a graph that contains the minimal
sufficient information regarding the task at hand conditioned on the paired
graph based on the principle of conditional graph information bottleneck. We
argue that our proposed method mimics the nature of chemical reactions, i.e.,
the core substructure of a molecule varies depending on which other molecule it
interacts with. Extensive experiments on various tasks with real-world datasets
demonstrate the superiority of CGIB over state-of-the-art baselines. Our code
is available at https://github.com/Namkyeong/CGIB.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lee_N/0/1/0/all/0/1&quot;&gt;Namkyeong Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hyun_D/0/1/0/all/0/1&quot;&gt;Dongmin Hyun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Na_G/0/1/0/all/0/1&quot;&gt;Gyoung S. Na&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sungwon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Junseok Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Park_C/0/1/0/all/0/1&quot;&gt;Chanyoung Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.06569">
<title>How to Index Item IDs for Recommendation Foundation Models. (arXiv:2305.06569v4 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2305.06569</link>
<description rdf:parseType="Literal">&lt;p&gt;Recommendation foundation model utilizes large language models (LLM) for
recommendation by converting recommendation tasks into natural language tasks.
It enables generative recommendation which directly generates the item(s) to
recommend rather than calculating a ranking score for each and every candidate
item in traditional recommendation models, simplifying the recommendation
pipeline from multi-stage filtering to single-stage filtering. To avoid
generating excessively long text when deciding which item(s) to recommend,
creating LLM-compatible item IDs is essential for recommendation foundation
models. In this study, we systematically examine the item indexing problem for
recommendation foundation models, using P5 as the representative backbone model
and replicating its results with various indexing methods. To emphasize the
importance of item indexing, we first discuss the issues of several trivial
item indexing methods, such as independent indexing, title indexing, and random
indexing. We then propose four simple yet effective solutions, including
sequential indexing, collaborative indexing, semantic (content-based) indexing,
and hybrid indexing. Our reproducibility study of P5 highlights the significant
influence of item indexing methods on the model performance, and our results on
real-world datasets validate the effectiveness of our proposed solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1&quot;&gt;Wenyue Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shuyuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yingqiang Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongfeng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.09028">
<title>SKI to go Faster: Accelerating Toeplitz Neural Networks via Asymmetric Kernels. (arXiv:2305.09028v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2305.09028</link>
<description rdf:parseType="Literal">&lt;p&gt;Toeplitz Neural Networks (TNNs) (Qin et. al. 2023) are a recent sequence
model with impressive results. They require O(n log n) computational complexity
and O(n) relative positional encoder (RPE) multi-layer perceptron (MLP) and
decay bias calls. We aim to reduce both. We first note that the RPE is a
non-SPD (symmetric positive definite) kernel and the Toeplitz matrices are
pseudo-Gram matrices. Further 1) the learned kernels display spiky behavior
near the main diagonals with otherwise smooth behavior; 2) the RPE MLP is slow.
For bidirectional models, this motivates a sparse plus low-rank Toeplitz matrix
decomposition. For the sparse component&apos;s action, we do a small 1D convolution.
For the low rank component, we replace the RPE MLP with linear interpolation
and use asymmetric Structured Kernel Interpolation (SKI) (Wilson et. al. 2015)
for O(n) complexity: we provide rigorous error analysis. For causal models,
&quot;fast&quot; causal masking (Katharopoulos et. al. 2020) negates SKI&apos;s benefits.
Working in the frequency domain, we avoid an explicit decay bias. To enforce
causality, we represent the kernel via the real part of its frequency response
using the RPE and compute the imaginary part via a Hilbert transform. This
maintains O(n log n) complexity but achieves an absolute speedup. Modeling the
frequency response directly is also competitive for bidirectional training,
using one fewer FFT. We set a speed state of the art on Long Range Arena (Tay
et. al. 2020) with minimal score degradation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Moreno_A/0/1/0/all/0/1&quot;&gt;Alexander Moreno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mei_J/0/1/0/all/0/1&quot;&gt;Jonathan Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Walters_L/0/1/0/all/0/1&quot;&gt;Luke Walters&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.09235">
<title>Synthetic data, real errors: how (not) to publish and use synthetic data. (arXiv:2305.09235v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.09235</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating synthetic data through generative models is gaining interest in
the ML community and beyond, promising a future where datasets can be tailored
to individual needs. Unfortunately, synthetic data is usually not perfect,
resulting in potential errors in downstream tasks. In this work we explore how
the generative process affects the downstream ML task. We show that the naive
synthetic data approach -- using synthetic data as if it is real -- leads to
downstream models and analyses that do not generalize well to real data. As a
first step towards better ML in the synthetic data regime, we introduce Deep
Generative Ensemble (DGE) -- a framework inspired by Deep Ensembles that aims
to implicitly approximate the posterior distribution over the generative
process model parameters. DGE improves downstream model training, evaluation,
and uncertainty quantification, vastly outperforming the naive approach on
average. The largest improvements are achieved for minority classes and
low-density regions of the original data, for which the generative uncertainty
is largest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Breugel_B/0/1/0/all/0/1&quot;&gt;Boris van Breugel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1&quot;&gt;Zhaozhi Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1&quot;&gt;Mihaela van der Schaar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13399">
<title>Efficient Large-Scale Visual Representation Learning. (arXiv:2305.13399v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13399</link>
<description rdf:parseType="Literal">&lt;p&gt;In this article, we present our approach to single-modality visual
representation learning. Understanding visual representations of product
content is vital for recommendations, search, and advertising applications in
e-commerce. We detail and contrast techniques used to fine-tune large-scale
visual representation learning models in an efficient manner under low-resource
settings, including several pretrained backbone architectures, both in the
convolutional neural network as well as the vision transformer family. We
highlight the challenges for e-commerce applications at-scale and highlight the
efforts to more efficiently train, evaluate, and serve visual representations.
We present ablation studies evaluating the representation offline performance
for several downstream tasks, including our visually similar ad
recommendations. To this end, we present a novel text-to-image generative
offline evaluation method for visually similar recommendation systems. Finally,
we include online results from deployed machine learning systems in production
at Etsy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dolev_E/0/1/0/all/0/1&quot;&gt;Eden Dolev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awad_A/0/1/0/all/0/1&quot;&gt;Alaa Awad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roberts_D/0/1/0/all/0/1&quot;&gt;Denisa Roberts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ebrahimzadeh_Z/0/1/0/all/0/1&quot;&gt;Zahra Ebrahimzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mejran_M/0/1/0/all/0/1&quot;&gt;Marcin Mejran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malpani_V/0/1/0/all/0/1&quot;&gt;Vaibhav Malpani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yavuz_M/0/1/0/all/0/1&quot;&gt;Mahir Yavuz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16044">
<title>Exploiting Noise as a Resource for Computation and Learning in Spiking Neural Networks. (arXiv:2305.16044v4 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16044</link>
<description rdf:parseType="Literal">&lt;p&gt;-- A theoretical framework that subsumes conventional deterministic spiking
neural networks and surrogate gradients, facilitating more efficient and
effective employment of various neuromorphic hardware developments in
real-world applications.
&lt;/p&gt;
&lt;p&gt;-- Scalable spiking neural models that incorporate noisy neuronal dynamics
for implicit regularization, improved robustness, and computational accounts of
biological neural computation, revealing that unreliable neural substrates
yield reliable computation and learning.
&lt;/p&gt;
&lt;p&gt;Networks of spiking neurons underpin the extraordinary information-processing
capabilities of the brain and have emerged as pillar models in neuromorphic
intelligence. Despite extensive research on spiking neural networks (SNNs),
most are established on deterministic models. Integrating noise into SNNs leads
to biophysically more realistic neural dynamics and may benefit model
performance. This work presents the noisy spiking neural network (NSNN) and the
noise-driven learning rule (NDL) by introducing a spiking neuron model
incorporating noisy neuronal dynamics. Our approach shows how noise may serve
as a resource for computation and learning and theoretically provides a
framework for general SNNs. We show that our method exhibits competitive
performance and improved robustness against challenging perturbations than
deterministic SNNs and better reproduces probabilistic neural computation in
neural coding. This study offers a powerful and easy-to-use tool for machine
learning, neuromorphic intelligence practitioners, and computational
neuroscience researchers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_G/0/1/0/all/0/1&quot;&gt;Gehua Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1&quot;&gt;Rui Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Huajin Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17033">
<title>The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs). (arXiv:2305.17033v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17033</link>
<description rdf:parseType="Literal">&lt;p&gt;Pediatric tumors of the central nervous system are the most common cause of
cancer-related death in children. The five-year survival rate for high-grade
gliomas in children is less than 20\%. Due to their rarity, the diagnosis of
these entities is often delayed, their treatment is mainly based on historic
treatment concepts, and clinical trials require multi-institutional
collaborations. The MICCAI Brain Tumor Segmentation (BraTS) Challenge is a
landmark community benchmark event with a successful history of 12 years of
resource creation for the segmentation and analysis of adult glioma. Here we
present the CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023 challenge, which
represents the first BraTS challenge focused on pediatric brain tumors with
data acquired across multiple international consortia dedicated to pediatric
neuro-oncology and clinical trials. The BraTS-PEDs 2023 challenge focuses on
benchmarking the development of volumentric segmentation algorithms for
pediatric brain glioma through standardized quantitative performance evaluation
metrics utilized across the BraTS 2023 cluster of challenges. Models gaining
knowledge from the BraTS-PEDs multi-parametric structural MRI (mpMRI) training
data will be evaluated on separate validation and unseen test mpMRI dataof
high-grade pediatric glioma. The CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023
challenge brings together clinicians and AI/imaging scientists to lead to
faster development of automated segmentation techniques that could benefit
clinical trials, and ultimately the care of children with brain tumors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kazerooni_A/0/1/0/all/0/1&quot;&gt;Anahita Fathi Kazerooni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khalili_N/0/1/0/all/0/1&quot;&gt;Nastaran Khalili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Haldar_D/0/1/0/all/0/1&quot;&gt;Debanjan Haldar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhifan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Anwar_S/0/1/0/all/0/1&quot;&gt;Syed Muhammed Anwar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Albrecht_J/0/1/0/all/0/1&quot;&gt;Jake Albrecht&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Adewole_M/0/1/0/all/0/1&quot;&gt;Maruf Adewole&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Anazodo_U/0/1/0/all/0/1&quot;&gt;Udunna Anazodo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Anderson_H/0/1/0/all/0/1&quot;&gt;Hannah Anderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bagheri_S/0/1/0/all/0/1&quot;&gt;Sina Bagheri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Baid_U/0/1/0/all/0/1&quot;&gt;Ujjwal Baid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bergquist_T/0/1/0/all/0/1&quot;&gt;Timothy Bergquist&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Borja_A/0/1/0/all/0/1&quot;&gt;Austin J. Borja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Calabrese_E/0/1/0/all/0/1&quot;&gt;Evan Calabrese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chung_V/0/1/0/all/0/1&quot;&gt;Verena Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Conte_G/0/1/0/all/0/1&quot;&gt;Gian-Marco Conte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dako_F/0/1/0/all/0/1&quot;&gt;Farouk Dako&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Eddy_J/0/1/0/all/0/1&quot;&gt;James Eddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ezhov_I/0/1/0/all/0/1&quot;&gt;Ivan Ezhov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Familiar_A/0/1/0/all/0/1&quot;&gt;Ariana Familiar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Farahani_K/0/1/0/all/0/1&quot;&gt;Keyvan Farahani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Haldar_S/0/1/0/all/0/1&quot;&gt;Shuvanjan Haldar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Iglesias_J/0/1/0/all/0/1&quot;&gt;Juan Eugenio Iglesias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Janas_A/0/1/0/all/0/1&quot;&gt;Anastasia Janas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Johansen_E/0/1/0/all/0/1&quot;&gt;Elaine Johansen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jones_B/0/1/0/all/0/1&quot;&gt;Blaise V Jones&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kofler_F/0/1/0/all/0/1&quot;&gt;Florian Kofler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+LaBella_D/0/1/0/all/0/1&quot;&gt;Dominic LaBella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lai_H/0/1/0/all/0/1&quot;&gt;Hollie Anne Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Leemput_K/0/1/0/all/0/1&quot;&gt;Koen Van Leemput&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongwei Bran Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Maleki_N/0/1/0/all/0/1&quot;&gt;Nazanin Maleki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+McAllister_A/0/1/0/all/0/1&quot;&gt;Aaron S McAllister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Meier_Z/0/1/0/all/0/1&quot;&gt;Zeke Meier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Menze_B/0/1/0/all/0/1&quot;&gt;Bjoern Menze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Moawad_A/0/1/0/all/0/1&quot;&gt;Ahmed W Moawad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nandolia_K/0/1/0/all/0/1&quot;&gt;Khanak K Nandolia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pavaine_J/0/1/0/all/0/1&quot;&gt;Julija Pavaine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Piraud_M/0/1/0/all/0/1&quot;&gt;Marie Piraud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Poussaint_T/0/1/0/all/0/1&quot;&gt;Tina Poussaint&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Prabhu_S/0/1/0/all/0/1&quot;&gt;Sanjay P Prabhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Reitman_Z/0/1/0/all/0/1&quot;&gt;Zachary Reitman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rodriguez_A/0/1/0/all/0/1&quot;&gt;Andres Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rudie_J/0/1/0/all/0/1&quot;&gt;Jeffrey D Rudie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shaikh_I/0/1/0/all/0/1&quot;&gt;Ibraheem Salman Shaikh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shah_L/0/1/0/all/0/1&quot;&gt;Lubdha M. Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sheth_N/0/1/0/all/0/1&quot;&gt;Nakul Sheth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shinohara_R/0/1/0/all/0/1&quot;&gt;Russel Taki Shinohara&lt;/a&gt;, et al. (23 additional authors not shown)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18404">
<title>Conformal Prediction with Large Language Models for Multi-Choice Question Answering. (arXiv:2305.18404v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18404</link>
<description rdf:parseType="Literal">&lt;p&gt;As large language models continue to be widely developed, robust uncertainty
quantification techniques will become crucial for their safe deployment in
high-stakes scenarios. In this work, we explore how conformal prediction can be
used to provide uncertainty quantification in language models for the specific
task of multiple-choice question-answering. We find that the uncertainty
estimates from conformal prediction are tightly correlated with prediction
accuracy. This observation can be useful for downstream applications such as
selective classification and filtering out low-quality predictions. We also
investigate the exchangeability assumption required by conformal prediction to
out-of-subject questions, which may be a more realistic scenario for many
practical applications. Our work contributes towards more trustworthy and
reliable usage of large language models in safety-critical situations, where
robust guarantees of error rate are required.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_B/0/1/0/all/0/1&quot;&gt;Bhawesh Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Charlie Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_G/0/1/0/all/0/1&quot;&gt;Gauri Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palepu_A/0/1/0/all/0/1&quot;&gt;Anil Palepu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellamy_D/0/1/0/all/0/1&quot;&gt;David Bellamy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raskar_R/0/1/0/all/0/1&quot;&gt;Ramesh Raskar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beam_A/0/1/0/all/0/1&quot;&gt;Andrew Beam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18451">
<title>Shift-Robust Molecular Relational Learning with Causal Substructure. (arXiv:2305.18451v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18451</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, molecular relational learning, whose goal is to predict the
interaction behavior between molecular pairs, got a surge of interest in
molecular sciences due to its wide range of applications. In this work, we
propose CMRL that is robust to the distributional shift in molecular relational
learning by detecting the core substructure that is causally related to
chemical reactions. To do so, we first assume a causal relationship based on
the domain knowledge of molecular sciences and construct a structural causal
model (SCM) that reveals the relationship between variables. Based on the SCM,
we introduce a novel conditional intervention framework whose intervention is
conditioned on the paired molecule. With the conditional intervention
framework, our model successfully learns from the causal substructure and
alleviates the confounding effect of shortcut substructures that are spuriously
correlated to chemical reactions. Extensive experiments on various tasks with
real-world and synthetic datasets demonstrate the superiority of CMRL over
state-of-the-art baseline models. Our code is available at
https://github.com/Namkyeong/CMRL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1&quot;&gt;Namkyeong Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1&quot;&gt;Kanghoon Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Na_G/0/1/0/all/0/1&quot;&gt;Gyoung S. Na&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sein Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1&quot;&gt;Chanyoung Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.01505">
<title>Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations. (arXiv:2306.01505v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.01505</link>
<description rdf:parseType="Literal">&lt;p&gt;Extracting generalized and robust representations is a major challenge in
emotion recognition in conversations (ERC). To address this, we propose a
supervised adversarial contrastive learning (SACL) framework for learning
class-spread structured representations in a supervised manner. SACL applies
contrast-aware adversarial training to generate worst-case samples and uses
joint class-spread contrastive learning to extract structured representations.
It can effectively utilize label-level feature consistency and retain
fine-grained intra-class features. To avoid the negative impact of adversarial
perturbations on context-dependent data, we design a contextual adversarial
training (CAT) strategy to learn more diverse features from context and enhance
the model&apos;s context robustness. Under the framework with CAT, we develop a
sequence-based SACL-LSTM to learn label-consistent and context-robust features
for ERC. Experiments on three datasets show that SACL-LSTM achieves
state-of-the-art performance on ERC. Extended experiments prove the
effectiveness of SACL and CAT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1&quot;&gt;Dou Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1&quot;&gt;Yinan Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1&quot;&gt;Lingwei Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Songlin Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.02312">
<title>(Un)reasonable Allure of Ante-hoc Interpretability for High-stakes Domains: Transparency Is Necessary but Insufficient for Comprehensibility. (arXiv:2306.02312v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.02312</link>
<description rdf:parseType="Literal">&lt;p&gt;Ante-hoc interpretability has become the holy grail of explainable artificial
intelligence for high-stakes domains such as healthcare; however, this notion
is elusive, lacks a widely-accepted definition and depends on the operational
context. It can refer to predictive models whose structure adheres to
domain-specific constraints, or ones that are inherently transparent. The
latter conceptualisation assumes observers who judge this quality, whereas the
former presupposes them to have technical and domain expertise (thus alienating
other groups of explainees). Additionally, the distinction between ante-hoc
interpretability and the less desirable post-hoc explainability, which refers
to methods that construct a separate explanatory model, is vague given that
transparent predictive models may still require (post-)processing to yield
suitable explanatory insights. Ante-hoc interpretability is thus an overloaded
concept that comprises a range of implicit properties, which we unpack in this
paper to better understand what is needed for its safe adoption across
high-stakes domains. To this end, we outline modelling and explaining
desiderata that allow us to navigate its distinct realisations in view of the
envisaged application and audience.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sokol_K/0/1/0/all/0/1&quot;&gt;Kacper Sokol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vogt_J/0/1/0/all/0/1&quot;&gt;Julia E. Vogt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.02913">
<title>Decentralized SGD and Average-direction SAM are Asymptotically Equivalent. (arXiv:2306.02913v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.02913</link>
<description rdf:parseType="Literal">&lt;p&gt;Decentralized stochastic gradient descent (D-SGD) allows collaborative
learning on massive devices simultaneously without the control of a central
server. However, existing theories claim that decentralization invariably
undermines generalization. In this paper, we challenge the conventional belief
and present a completely new perspective for understanding decentralized
learning. We prove that D-SGD implicitly minimizes the loss function of an
average-direction Sharpness-aware minimization (SAM) algorithm under general
non-convex non-$\beta$-smooth settings. This surprising asymptotic equivalence
reveals an intrinsic regularization-optimization trade-off and three advantages
of decentralization: (1) there exists a free uncertainty evaluation mechanism
in D-SGD to improve posterior estimation; (2) D-SGD exhibits a gradient
smoothing effect; and (3) the sharpness regularization effect of D-SGD does not
decrease as total batch size increases, which justifies the potential
generalization benefit of D-SGD over centralized SGD (C-SGD) in large-batch
scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1&quot;&gt;Tongtian Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1&quot;&gt;Fengxiang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kaixuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1&quot;&gt;Mingli Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.03718">
<title>Emotion-Conditioned Melody Harmonization with Hierarchical Variational Autoencoder. (arXiv:2306.03718v3 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2306.03718</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing melody harmonization models have made great progress in improving
the quality of generated harmonies, but most of them ignored the emotions
beneath the music. Meanwhile, the variability of harmonies generated by
previous methods is insufficient. To solve these problems, we propose a novel
LSTM-based Hierarchical Variational Auto-Encoder (LHVAE) to investigate the
influence of emotional conditions on melody harmonization, while improving the
quality of generated harmonies and capturing the abundant variability of chord
progressions. Specifically, LHVAE incorporates latent variables and emotional
conditions at different levels (piece- and bar-level) to model the global and
local music properties. Additionally, we introduce an attention-based melody
context vector at each step to better learn the correspondence between melodies
and harmonies. Objective experimental results show that our proposed model
outperforms other LSTM-based models. Through subjective evaluation, we conclude
that only altering the type of chord hardly changes the overall emotion of the
music. The qualitative analysis demonstrates the ability of our model to
generate variable harmonies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1&quot;&gt;Shulei Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xinyu Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04139">
<title>A Comprehensive Survey on Generative Diffusion Models for Structured Data. (arXiv:2306.04139v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04139</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, generative diffusion models have achieved a rapid paradigm
shift in deep generative models by showing groundbreaking performance across
various applications. Meanwhile, structured data, encompassing tabular and time
series data, has been received comparatively limited attention from the deep
learning research community, despite its omnipresence and extensive
applications. Thus, there is still a lack of literature and its reviews on
structured data modelling via diffusion models, compared to other data
modalities such as visual and textual data. To address this gap, we present a
comprehensive review of recently proposed diffusion models in the field of
structured data. First, this survey provides a concise overview of the
score-based diffusion model theory, subsequently proceeding to the technical
descriptions of the majority of pioneering works that used structured data in
both data-driven general tasks and domain-specific applications. Thereafter, we
analyse and discuss the limitations and challenges shown in existing works and
suggest potential research directions. We hope this review serves as a catalyst
for the research community, promoting developments in generative diffusion
models for structured data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koo_H/0/1/0/all/0/1&quot;&gt;Heejoon Koo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;To Eun Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04919">
<title>Unsupervised Cross-Domain Soft Sensor Modelling via Deep Physics-Inspired Particle Flow Bayes. (arXiv:2306.04919v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04919</link>
<description rdf:parseType="Literal">&lt;p&gt;Data-driven soft sensors are essential for achieving accurate perception
through reliable state inference. However, developing representative soft
sensor models is challenged by issues such as missing labels, domain
adaptability, and temporal coherence in data. To address these challenges, we
propose a deep Particle Flow Bayes (DPFB) framework for cross-domain soft
sensor modeling in the absence of target state labels. In particular, a
sequential Bayes objective is first formulated to perform the maximum
likelihood estimation underlying the cross-domain soft sensing problem. At the
core of the framework, we incorporate a physics-inspired particle flow that
optimizes the sequential Bayes objective to perform an exact Bayes update of
the model extracted latent and hidden features. As a result, these
contributions enable the proposed framework to learn a rich approximate
posterior feature representation capable of characterizing complex cross-domain
system dynamics and performing effective time series unsupervised domain
adaptation (UDA). Finally, we validate the framework on a complex industrial
multiphase flow process system with complex dynamics and multiple operating
conditions. The results demonstrate that the DPFB framework achieves superior
cross-domain soft sensing performance, outperforming state-of-the-art deep UDA
and normalizing flow approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loo_J/0/1/0/all/0/1&quot;&gt;Junn Yong Loo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1&quot;&gt;Ze Yang Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nurzaman_S/0/1/0/all/0/1&quot;&gt;Surya G. Nurzaman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ting_C/0/1/0/all/0/1&quot;&gt;Chee-Ming Ting&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baskaran_V/0/1/0/all/0/1&quot;&gt;Vishnu Monn Baskaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Chee Pin Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05987">
<title>Liquidity takers behavior representation through a contrastive learning approach. (arXiv:2306.05987v2 [q-fin.ST] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05987</link>
<description rdf:parseType="Literal">&lt;p&gt;Thanks to the access to the labeled orders on the CAC40 data from Euronext,
we are able to analyze agents&apos; behaviors in the market based on their placed
orders. In this study, we construct a self-supervised learning model using
triplet loss to effectively learn the representation of agent market orders. By
acquiring this learned representation, various downstream tasks become
feasible. In this work, we utilize the K-means clustering algorithm on the
learned representation vectors of agent orders to identify distinct behavior
types within each cluster.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Ruan_R/0/1/0/all/0/1&quot;&gt;Ruihua Ruan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Bacry_E/0/1/0/all/0/1&quot;&gt;Emmanuel Bacry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Muzy_J/0/1/0/all/0/1&quot;&gt;Jean-Fran&amp;#xe7;ois Muzy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09417">
<title>Diff-TTSG: Denoising probabilistic integrated speech and gesture synthesis. (arXiv:2306.09417v2 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09417</link>
<description rdf:parseType="Literal">&lt;p&gt;With read-aloud speech synthesis achieving high naturalness scores, there is
a growing research interest in synthesising spontaneous speech. However, human
spontaneous face-to-face conversation has both spoken and non-verbal aspects
(here, co-speech gestures). Only recently has research begun to explore the
benefits of jointly synthesising these two modalities in a single system. The
previous state of the art used non-probabilistic methods, which fail to capture
the variability of human speech and motion, and risk producing oversmoothing
artefacts and sub-optimal synthesis quality. We present the first
diffusion-based probabilistic model, called Diff-TTSG, that jointly learns to
synthesise speech and gestures together. Our method can be trained on small
datasets from scratch. Furthermore, we describe a set of careful uni- and
multi-modal subjective tests for evaluating integrated speech and gesture
synthesis systems, and use them to validate our proposed approach. For
synthesised examples please see https://shivammehta25.github.io/Diff-TTSG
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mehta_S/0/1/0/all/0/1&quot;&gt;Shivam Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Siyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Alexanderson_S/0/1/0/all/0/1&quot;&gt;Simon Alexanderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Beskow_J/0/1/0/all/0/1&quot;&gt;Jonas Beskow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Szekely_E/0/1/0/all/0/1&quot;&gt;&amp;#xc9;va Sz&amp;#xe9;kely&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Henter_G/0/1/0/all/0/1&quot;&gt;Gustav Eje Henter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10006">
<title>Unsupervised Learning of Style-Aware Facial Animation from Real Acting Performances. (arXiv:2306.10006v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10006</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel approach for text/speech-driven animation of a
photo-realistic head model based on blend-shape geometry, dynamic textures, and
neural rendering. Training a VAE for geometry and texture yields a parametric
model for accurate capturing and realistic synthesis of facial expressions from
a latent feature vector. Our animation method is based on a conditional CNN
that transforms text or speech into a sequence of animation parameters. In
contrast to previous approaches, our animation model learns
disentangling/synthesizing different acting-styles in an unsupervised manner,
requiring only phonetic labels that describe the content of training sequences.
For realistic real-time rendering, we train a U-Net that refines
rasterization-based renderings by computing improved pixel colors and a
foreground matte. We compare our framework qualitatively/quantitatively against
recent methods for head modeling as well as facial animation and evaluate the
perceived rendering/animation quality in a user-study, which indicates large
improvements compared to state-of-the-art approaches
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paier_W/0/1/0/all/0/1&quot;&gt;Wolfgang Paier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hilsmann_A/0/1/0/all/0/1&quot;&gt;Anna Hilsmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eisert_P/0/1/0/all/0/1&quot;&gt;Peter Eisert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11197">
<title>Sparse Modular Activation for Efficient Sequence Modeling. (arXiv:2306.11197v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11197</link>
<description rdf:parseType="Literal">&lt;p&gt;Linear State Space Models (SSMs) have demonstrated strong performance in a
variety of sequence modeling tasks due to their efficient encoding of the
recurrent structure. However, in more comprehensive tasks like language
modeling and machine translation, self-attention-based models still outperform
SSMs. Hybrid models employing both SSM and self-attention generally show
promising performance, but current approaches apply attention modules
statically and uniformly to all elements in the input sequences, leading to
sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse
Modular Activation (SMA), a general mechanism enabling neural networks to
sparsely and dynamically activate sub-modules for sequence elements in a
differentiable manner. Through allowing each element to skip non-activated
sub-modules, SMA reduces computation and memory consumption at both training
and inference stages of sequence modeling. As a specific instantiation of SMA,
we design a novel neural architecture, SeqBoat, which employs SMA to sparsely
activate a Gated Attention Unit (GAU) based on the state representations
learned from an SSM. By constraining the GAU to only conduct local attention on
the activated inputs, SeqBoat can achieve linear inference complexity with
theoretically infinite attention span, and provide substantially better
quality-efficiency trade-off than the chunking-based models. With experiments
on a wide range of tasks, including language modeling, speech classification
and long-range arena, SeqBoat brings new state-of-the-art results among hybrid
models with linear complexity and reveals the amount of attention needed for
each task through the learned sparse activation patterns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1&quot;&gt;Liliang Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuohang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yichong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Chenguang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1&quot;&gt;ChengXiang Zhai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11342">
<title>Exploring Antitrust and Platform Power in Generative AI. (arXiv:2306.11342v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11342</link>
<description rdf:parseType="Literal">&lt;p&gt;The concentration of power in a few digital technology companies has become a
subject of increasing interest in both academic and non-academic discussions.
One of the most noteworthy contributions to the debate is Lina Khan&apos;s Amazon&apos;s
Antitrust Paradox. In this work, Khan contends that Amazon has systematically
exerted its dominance in online retail to eliminate competitors and
subsequently charge above-market prices. This work contributed to Khan&apos;s
appointment as the chair of the US Federal Trade Commission (FTC), one of the
most influential antitrust organisations. Today, several ongoing antitrust
lawsuits in the US and Europe involve major technology companies like Apple,
Google/Alphabet, and Facebook/Meta. In the realm of generative AI, we are once
again witnessing the same companies taking the lead in technological
advancements, leaving little room for others to compete. This article examines
the market dominance of these corporations in the technology stack behind
generative AI from an antitrust law perspective.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kollnig_K/0/1/0/all/0/1&quot;&gt;Konrad Kollnig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qian Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14435">
<title>DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing. (arXiv:2306.14435v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14435</link>
<description rdf:parseType="Literal">&lt;p&gt;Precise and controllable image editing is a challenging task that has
attracted significant attention. Recently, DragGAN enables an interactive
point-based image editing framework and achieves impressive editing results
with pixel-level precision. However, since this method is based on generative
adversarial networks (GAN), its generality is upper-bounded by the capacity of
the pre-trained GAN models. In this work, we extend such an editing framework
to diffusion models and propose DragDiffusion. By leveraging large-scale
pretrained diffusion models, we greatly improve the applicability of
interactive point-based editing in real world scenarios. While most existing
diffusion-based image editing methods work on text embeddings, DragDiffusion
optimizes the diffusion latent to achieve precise spatial control. Although
diffusion models generate images in an iterative manner, we empirically show
that optimizing diffusion latent at one single step suffices to generate
coherent results, enabling DragDiffusion to complete high-quality editing
efficiently. Extensive experiments across a wide range of challenging cases
(e.g., multi-objects, diverse object categories, various styles, etc.)
demonstrate the versatility and generality of DragDiffusion. Code:
https://github.com/Yujun-Shi/DragDiffusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yujun Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_C/0/1/0/all/0/1&quot;&gt;Chuhui Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Jiachun Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1&quot;&gt;Vincent Y. F. Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1&quot;&gt;Song Bai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16906">
<title>Numerical Data Imputation for Multimodal Data Sets: A Probabilistic Nearest-Neighbor Kernel Density Approach. (arXiv:2306.16906v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16906</link>
<description rdf:parseType="Literal">&lt;p&gt;Numerical data imputation algorithms replace missing values by estimates to
leverage incomplete data sets. Current imputation methods seek to minimize the
error between the unobserved ground truth and the imputed values. But this
strategy can create artifacts leading to poor imputation in the presence of
multimodal or complex distributions. To tackle this problem, we introduce the
$k$NN$\times$KDE algorithm: a data imputation method combining nearest neighbor
estimation ($k$NN) and density estimation with Gaussian kernels (KDE). We
compare our method with previous data imputation methods using artificial and
real-world data with different data missing scenarios and various data missing
rates, and show that our method can cope with complex original data structure,
yields lower data imputation errors, and provides probabilistic estimates with
higher likelihood than current methods. We release the code in open-source for
the community: https://github.com/DeltaFloflo/knnxkde
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lalande_F/0/1/0/all/0/1&quot;&gt;Florian Lalande&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Doya_K/0/1/0/all/0/1&quot;&gt;Kenji Doya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16926">
<title>OSP: Boosting Distributed Model Training with 2-stage Synchronization. (arXiv:2306.16926v2 [cs.DC] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16926</link>
<description rdf:parseType="Literal">&lt;p&gt;Distributed deep learning (DDL) is a promising research area, which aims to
increase the efficiency of training deep learning tasks with large size of
datasets and models. As the computation capability of DDL nodes continues to
increase, the network connection between nodes is becoming a major bottleneck.
Various methods of gradient compression and improved model synchronization have
been proposed to address this bottleneck in Parameter-Server-based DDL.
However, these two types of methods can result in accuracy loss due to
discarded gradients and have limited enhancement on the throughput of model
synchronization, respectively. To address these challenges, we propose a new
model synchronization method named Overlapped Synchronization Parallel (OSP),
which achieves efficient communication with a 2-stage synchronization approach
and uses Local-Gradient-based Parameter correction (LGP) to avoid accuracy loss
caused by stale parameters. The prototype of OSP has been implemented using
PyTorch and evaluated on commonly used deep learning models and datasets with a
9-node testbed. Evaluation results show that OSP can achieve up to 50\%
improvement in throughput without accuracy loss compared to popular
synchronization models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zixuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1&quot;&gt;Lei Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xuandong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiahui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yang Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17066">
<title>On the Predictive Accuracy of Neural Temporal Point Process Models for Continuous-time Event Data. (arXiv:2306.17066v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17066</link>
<description rdf:parseType="Literal">&lt;p&gt;Temporal Point Processes (TPPs) serve as the standard mathematical framework
for modeling asynchronous event sequences in continuous time. However,
classical TPP models are often constrained by strong assumptions, limiting
their ability to capture complex real-world event dynamics. To overcome this
limitation, researchers have proposed Neural TPPs, which leverage neural
network parametrizations to offer more flexible and efficient modeling. While
recent studies demonstrate the effectiveness of Neural TPPs, they often lack a
unified setup, relying on different baselines, datasets, and experimental
configurations. This makes it challenging to identify the key factors driving
improvements in predictive accuracy, hindering research progress. To bridge
this gap, we present a comprehensive large-scale experimental study that
systematically evaluates the predictive accuracy of state-of-the-art neural TPP
models. Our study encompasses multiple real-world and synthetic event sequence
datasets, following a carefully designed unified setup. We thoroughly
investigate the influence of major architectural components such as event
encoding, history encoder, and decoder parametrization on both time and mark
prediction tasks. Additionally, we delve into the less explored area of
probabilistic calibration for neural TPP models. By analyzing our results, we
draw insightful conclusions regarding the significance of history size and the
impact of architectural components on predictive accuracy. Furthermore, we shed
light on the miscalibration of mark distributions in neural TPP models. Our
study aims to provide valuable insights into the performance and
characteristics of neural TPP models, contributing to a better understanding of
their strengths and limitations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bosser_T/0/1/0/all/0/1&quot;&gt;Tanguy Bosser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taieb_S/0/1/0/all/0/1&quot;&gt;Souhaib Ben Taieb&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17181">
<title>Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis. (arXiv:2306.17181v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17181</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Networks (GAN) is a model for data synthesis, which
creates plausible data through the competition of generator and discriminator.
Although GAN application to image synthesis is extensively studied, it has
inherent limitations to natural language generation. Because natural language
is composed of discrete tokens, a generator has difficulty updating its
gradient through backpropagation; therefore, most text-GAN studies generate
sentences starting with a random token based on a reward system. Thus, the
generators of previous studies are pre-trained in an autoregressive way before
adversarial training, causing data memorization that synthesized sentences
reproduce the training data. In this paper, we synthesize sentences using a
framework similar to the original GAN. More specifically, we propose Text
Embedding Space Generative Adversarial Networks (TESGAN) which generate
continuous text embedding spaces instead of discrete tokens to solve the
gradient backpropagation problem. Furthermore, TESGAN conducts unsupervised
learning which does not directly refer to the text of the training data to
overcome the data memorization issue. By adopting this novel method, TESGAN can
synthesize new sentences, showing the potential of unsupervised learning for
text synthesis. We expect to see extended research combining Large Language
Models with a new perspective of viewing text as an continuous space.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jun-Min Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ha_T/0/1/0/all/0/1&quot;&gt;Tae-Bin Ha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00534">
<title>Shared Growth of Graph Neural Networks via Free-direction Knowledge Distillation. (arXiv:2307.00534v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00534</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge distillation (KD) has shown to be effective to boost the
performance of graph neural networks (GNNs), where the typical objective is to
distill knowledge from a deeper teacher GNN into a shallower student GNN.
However, it is often quite challenging to train a satisfactory deeper GNN due
to the well-known over-parametrized and over-smoothing issues, leading to
invalid knowledge transfer in practical applications. In this paper, we propose
the first Free-direction Knowledge Distillation framework via reinforcement
learning for GNNs, called FreeKD, which is no longer required to provide a
deeper well-optimized teacher GNN. Our core idea is to collaboratively learn
two shallower GNNs in an effort to exchange knowledge between them via
reinforcement learning in a hierarchical way. As we observe that one typical
GNN model often exhibits better and worse performances at different nodes
during training, we devise a dynamic and free-direction knowledge transfer
strategy that involves two levels of actions: 1) node-level action determines
the directions of knowledge transfer between the corresponding nodes of two
networks; and then 2) structure-level action determines which of the local
structures generated by the node-level actions to be propagated. Furthermore,
considering the diverse knowledge present in different GNNs when dealing with
multi-view inputs, we introduce FreeKD++ as a solution to enable free-direction
knowledge transfer among multiple shallow GNNs operating on multi-view inputs.
Extensive experiments on five benchmark datasets demonstrate our approaches
outperform the base GNNs in a large margin, and shows their efficacy to various
GNNs. More surprisingly, our FreeKD has comparable or even better performance
than traditional KD algorithms that distill knowledge from a deeper and
stronger teacher GNN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_K/0/1/0/all/0/1&quot;&gt;Kaituo Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1&quot;&gt;Yikun Miao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Changsheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Ye Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guoren Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01689">
<title>Online Learning and Solving Infinite Games with an ERM Oracle. (arXiv:2307.01689v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01689</link>
<description rdf:parseType="Literal">&lt;p&gt;While ERM suffices to attain near-optimal generalization error in the
stochastic learning setting, this is not known to be the case in the online
learning setting, where algorithms for general concept classes rely on
computationally inefficient oracles such as the Standard Optimal Algorithm
(SOA). In this work, we propose an algorithm for online binary classification
setting that relies solely on ERM oracle calls, and show that it has finite
regret in the realizable setting and sublinearly growing regret in the agnostic
setting. We bound the regret in terms of the Littlestone and threshold
dimensions of the underlying concept class.
&lt;/p&gt;
&lt;p&gt;We obtain similar results for nonparametric games, where the ERM oracle can
be interpreted as a best response oracle, finding the best response of a player
to a given history of play of the other players. In this setting, we provide
learning algorithms that only rely on best response oracles and converge to
approximate-minimax equilibria in two-player zero-sum games and approximate
coarse correlated equilibria in multi-player general-sum games, as long as the
game has a bounded fat-threshold dimension. Our algorithms apply to both
binary-valued and real-valued games and can be viewed as providing
justification for the wide use of double oracle and multiple oracle algorithms
in the practice of solving large games.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Assos_A/0/1/0/all/0/1&quot;&gt;Angelos Assos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Attias_I/0/1/0/all/0/1&quot;&gt;Idan Attias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dagan_Y/0/1/0/all/0/1&quot;&gt;Yuval Dagan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daskalakis_C/0/1/0/all/0/1&quot;&gt;Constantinos Daskalakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fishelson_M/0/1/0/all/0/1&quot;&gt;Maxwell Fishelson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01750">
<title>SRCD: Semantic Reasoning with Compound Domains for Single-Domain Generalized Object Detection. (arXiv:2307.01750v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01750</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper provides a novel framework for single-domain generalized object
detection (i.e., Single-DGOD), where we are interested in learning and
maintaining the semantic structures of self-augmented compound cross-domain
samples to enhance the model&apos;s generalization ability. Different from DGOD
trained on multiple source domains, Single-DGOD is far more challenging to
generalize well to multiple target domains with only one single source domain.
Existing methods mostly adopt a similar treatment from DGOD to learn
domain-invariant features by decoupling or compressing the semantic space.
However, there may have two potential limitations: 1) pseudo attribute-label
correlation, due to extremely scarce single-domain data; and 2) the semantic
structural information is usually ignored, i.e., we found the affinities of
instance-level semantic relations in samples are crucial to model
generalization. In this paper, we introduce Semantic Reasoning with Compound
Domains (SRCD) for Single-DGOD. Specifically, our SRCD contains two main
components, namely, the texture-based self-augmentation (TBSA) module, and the
local-global semantic reasoning (LGSR) module. TBSA aims to eliminate the
effects of irrelevant attributes associated with labels, such as light, shadow,
color, etc., at the image level by a light-yet-efficient self-augmentation.
Moreover, LGSR is used to further model the semantic relationships on instance
features to uncover and maintain the intrinsic semantic structures. Extensive
experiments on multiple benchmarks demonstrate the effectiveness of the
proposed SRCD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_Z/0/1/0/all/0/1&quot;&gt;Zhijie Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jingcai Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1&quot;&gt;Luyao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yue Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1&quot;&gt;Xinghao Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1&quot;&gt;Song Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02198">
<title>ChiENN: Embracing Molecular Chirality with Graph Neural Networks. (arXiv:2307.02198v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02198</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) play a fundamental role in many deep learning
problems, in particular in cheminformatics. However, typical GNNs cannot
capture the concept of chirality, which means they do not distinguish between
the 3D graph of a chemical compound and its mirror image (enantiomer). The
ability to distinguish between enantiomers is important especially in drug
discovery because enantiomers can have very distinct biochemical properties. In
this paper, we propose a theoretically justified message-passing scheme, which
makes GNNs sensitive to the order of node neighbors. We apply that general
concept in the context of molecular chirality to construct Chiral Edge Neural
Network (ChiENN) layer which can be appended to any GNN model to enable
chirality-awareness. Our experiments show that adding ChiENN layers to a GNN
outperforms current state-of-the-art methods in chiral-sensitive molecular
property prediction tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gainski_P/0/1/0/all/0/1&quot;&gt;Piotr Gai&amp;#x144;ski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koziarski_M/0/1/0/all/0/1&quot;&gt;Micha&amp;#x142; Koziarski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tabor_J/0/1/0/all/0/1&quot;&gt;Jacek Tabor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smieja_M/0/1/0/all/0/1&quot;&gt;Marek &amp;#x15a;mieja&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02245">
<title>Set Learning for Accurate and Calibrated Models. (arXiv:2307.02245v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02245</link>
<description rdf:parseType="Literal">&lt;p&gt;Model overconfidence and poor calibration are common in machine learning and
difficult to account for when applying standard empirical risk minimization. In
this work, we propose a novel method to alleviate these problems that we call
odd-$k$-out learning (OKO), which minimizes the cross-entropy error for sets
rather than for single examples. This naturally allows the model to capture
correlations across data examples and achieves both better accuracy and
calibration, especially in limited training data and class-imbalanced regimes.
Perhaps surprisingly, OKO often yields better calibration even when training
with hard labels and dropping any additional calibration parameter tuning, such
as temperature scaling. We provide theoretical justification, establishing that
OKO naturally yields better calibration, and provide extensive experimental
analyses that corroborate our theoretical findings. We emphasize that OKO is a
general framework that can be easily adapted to many settings and the trained
model can be applied to single examples at inference time, without introducing
significant run-time overhead or architecture changes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muttenthaler_L/0/1/0/all/0/1&quot;&gt;Lukas Muttenthaler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vandermeulen_R/0/1/0/all/0/1&quot;&gt;Robert A. Vandermeulen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qiuyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Unterthiner_T/0/1/0/all/0/1&quot;&gt;Thomas Unterthiner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1&quot;&gt;Klaus-Robert M&amp;#xfc;ller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02329">
<title>Data-driven Predictive Latency for 5G: A Theoretical and Experimental Analysis Using Network Measurements. (arXiv:2307.02329v2 [cs.NI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02329</link>
<description rdf:parseType="Literal">&lt;p&gt;The advent of novel 5G services and applications with binding latency
requirements and guaranteed Quality of Service (QoS) hastened the need to
incorporate autonomous and proactive decision-making in network management
procedures. The objective of our study is to provide a thorough analysis of
predictive latency within 5G networks by utilizing real-world network data that
is accessible to mobile network operators (MNOs). In particular, (i) we present
an analytical formulation of the user-plane latency as a Hypoexponential
distribution, which is validated by means of a comparative analysis with
empirical measurements, and (ii) we conduct experimental results of
probabilistic regression, anomaly detection, and predictive forecasting
leveraging on emerging domains in Machine Learning (ML), such as Bayesian
Learning (BL) and Machine Learning on Graphs (GML). We test our predictive
framework using data gathered from scenarios of vehicular mobility, dense-urban
traffic, and social gathering events. Our results provide valuable insights
into the efficacy of predictive algorithms in practical applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skocaj_M/0/1/0/all/0/1&quot;&gt;Marco Skocaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Conserva_F/0/1/0/all/0/1&quot;&gt;Francesca Conserva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grande_N/0/1/0/all/0/1&quot;&gt;Nicol Sarcone Grande&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orsi_A/0/1/0/all/0/1&quot;&gt;Andrea Orsi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Micheli_D/0/1/0/all/0/1&quot;&gt;Davide Micheli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghinamo_G/0/1/0/all/0/1&quot;&gt;Giorgio Ghinamo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bizzarri_S/0/1/0/all/0/1&quot;&gt;Simone Bizzarri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verdone_R/0/1/0/all/0/1&quot;&gt;Roberto Verdone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02903">
<title>PUFFIN: A Path-Unifying Feed-Forward Interfaced Network for Vapor Pressure Prediction. (arXiv:2307.02903v2 [physics.chem-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02903</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurately predicting vapor pressure is vital for various industrial and
environmental applications. However, obtaining accurate measurements for all
compounds of interest is not possible due to the resource and labor intensity
of experiments. The demand for resources and labor further multiplies when a
temperature-dependent relationship for predicting vapor pressure is desired. In
this paper, we propose PUFFIN (Path-Unifying Feed-Forward Interfaced Network),
a machine learning framework that combines transfer learning with a new
inductive bias node inspired by domain knowledge (the Antoine equation) to
improve vapor pressure prediction. By leveraging inductive bias and transfer
learning using graph embeddings, PUFFIN outperforms alternative strategies that
do not use inductive bias or that use generic descriptors of compounds. The
framework&apos;s incorporation of domain-specific knowledge to overcome the
limitation of poor data availability shows its potential for broader
applications in chemical compound analysis, including the prediction of other
physicochemical properties. Importantly, our proposed machine learning
framework is partially interpretable, because the inductive Antoine node yields
network-derived Antoine equation coefficients. It would then be possible to
directly incorporate the obtained analytical expression in process design
software for better prediction and control of processes occurring in industry
and the environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Santana_V/0/1/0/all/0/1&quot;&gt;Vinicius Viena Santana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Rebello_C/0/1/0/all/0/1&quot;&gt;Carine Menezes Rebello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Queiroz_L/0/1/0/all/0/1&quot;&gt;Luana P. Queiroz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ribeiro_A/0/1/0/all/0/1&quot;&gt;Ana Mafalda Ribeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Shardt_N/0/1/0/all/0/1&quot;&gt;Nadia Shardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Nogueira_I/0/1/0/all/0/1&quot;&gt;Idelfonso B. R. Nogueira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03393">
<title>Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs. (arXiv:2307.03393v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03393</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning on Graphs has attracted immense attention due to its wide real-world
applications. The most popular pipeline for learning on graphs with textual
node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes
shallow text embedding as initial node representations, which has limitations
in general knowledge and profound semantic understanding. In recent years,
Large Language Models (LLMs) have been proven to possess extensive common
knowledge and powerful semantic comprehension abilities that have
revolutionized existing workflows to handle text data. In this paper, we aim to
explore the potential of LLMs in graph machine learning, especially the node
classification task, and investigate two possible pipelines: LLMs-as-Enhancers
and LLMs-as-Predictors. The former leverages LLMs to enhance nodes&apos; text
attributes with their massive knowledge and then generate predictions through
GNNs. The latter attempts to directly employ LLMs as standalone predictors. We
conduct comprehensive and systematical studies on these two pipelines under
various settings. From comprehensive empirical results, we make original
observations and find new insights that open new possibilities and suggest
promising directions to leverage LLMs for learning on graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhikai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1&quot;&gt;Haitao Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1&quot;&gt;Wei Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1&quot;&gt;Hongzhi Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xiaochi Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuaiqiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1&quot;&gt;Dawei Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1&quot;&gt;Wenqi Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiliang Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03493">
<title>ITA: An Energy-Efficient Attention and Softmax Accelerator for Quantized Transformers. (arXiv:2307.03493v2 [cs.AR] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03493</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer networks have emerged as the state-of-the-art approach for
natural language processing tasks and are gaining popularity in other domains
such as computer vision and audio processing. However, the efficient hardware
acceleration of transformer models poses new challenges due to their high
arithmetic intensities, large memory requirements, and complex dataflow
dependencies. In this work, we propose ITA, a novel accelerator architecture
for transformers and related models that targets efficient inference on
embedded systems by exploiting 8-bit quantization and an innovative softmax
implementation that operates exclusively on integer values. By computing
on-the-fly in streaming mode, our softmax implementation minimizes data
movement and energy consumption. ITA achieves competitive energy efficiency
with respect to state-of-the-art transformer accelerators with 16.9 TOPS/W,
while outperforming them in area efficiency with 5.93 TOPS/mm$^2$ in 22 nm
fully-depleted silicon-on-insulator technology at 0.8 V.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islamoglu_G/0/1/0/all/0/1&quot;&gt;Gamze &amp;#x130;slamo&amp;#x11f;lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scherer_M/0/1/0/all/0/1&quot;&gt;Moritz Scherer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paulin_G/0/1/0/all/0/1&quot;&gt;Gianna Paulin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fischer_T/0/1/0/all/0/1&quot;&gt;Tim Fischer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_V/0/1/0/all/0/1&quot;&gt;Victor J.B. Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garofalo_A/0/1/0/all/0/1&quot;&gt;Angelo Garofalo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1&quot;&gt;Luca Benini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03577">
<title>Programmable Synthetic Tabular Data Generation. (arXiv:2307.03577v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03577</link>
<description rdf:parseType="Literal">&lt;p&gt;Large amounts of tabular data remain underutilized due to privacy, data
quality, and data sharing limitations. While training a generative model
producing synthetic data resembling the original distribution addresses some of
these issues, most applications require additional constraints from the
generated data. Existing synthetic data approaches are limited as they
typically only handle specific constraints, e.g., differential privacy (DP) or
increased fairness, and lack an accessible interface for declaring general
specifications. In this work, we introduce ProgSyn, the first programmable
synthetic tabular data generation algorithm that allows for comprehensive
customization over the generated data. To ensure high data quality while
adhering to custom specifications, ProgSyn pre-trains a generative model on the
original dataset and fine-tunes it on a differentiable loss automatically
derived from the provided specifications. These can be programmatically
declared using statistical and logical expressions, supporting a wide range of
requirements (e.g., DP or fairness, among others). We conduct an extensive
experimental evaluation of ProgSyn on a number of constraints, achieving a new
state-of-the-art on some, while remaining general. For instance, at the same
fairness level we achieve 2.3% higher downstream accuracy than the
state-of-the-art in fair synthetic data generation on the Adult dataset.
Overall, ProgSyn provides a versatile and accessible framework for generating
constrained synthetic tabular data, allowing for specifications that generalize
beyond the capabilities of prior work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vero_M/0/1/0/all/0/1&quot;&gt;Mark Vero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balunovic_M/0/1/0/all/0/1&quot;&gt;Mislav Balunovi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vechev_M/0/1/0/all/0/1&quot;&gt;Martin Vechev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00457">
<title>GenRec: Large Language Model for Generative Recommendation. (arXiv:2307.00457v2 [cs.IR] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2307.00457</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, large language models (LLM) have emerged as powerful tools
for diverse natural language processing tasks. However, their potential for
recommender systems under the generative recommendation paradigm remains
relatively unexplored. This paper presents an innovative approach to
recommendation systems using large language models (LLMs) based on text data.
In this paper, we present a novel LLM for generative recommendation (GenRec)
that utilized the expressive power of LLM to directly generate the target item
to recommend, rather than calculating ranking score for each candidate item one
by one as in traditional discriminative recommendation. GenRec uses LLM&apos;s
understanding ability to interpret context, learn user preferences, and
generate relevant recommendation. Our proposed approach leverages the vast
knowledge encoded in large language models to accomplish recommendation tasks.
We first we formulate specialized prompts to enhance the ability of LLM to
comprehend recommendation tasks. Subsequently, we use these prompts to
fine-tune the LLaMA backbone LLM on a dataset of user-item interactions,
represented by textual data, to capture user preferences and item
characteristics. Our research underscores the potential of LLM-based generative
recommendation in revolutionizing the domain of recommendation systems and
offers a foundational framework for future explorations in this field. We
conduct extensive experiments on benchmark datasets, and the experiments shows
that our GenRec has significant better results on large dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1&quot;&gt;Jianchao Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zelong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shuyuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1&quot;&gt;Wenyue Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yingqiang Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1&quot;&gt;Juntao Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongfeng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01226">
<title>vONTSS: vMF based semi-supervised neural topic modeling with optimal transport. (arXiv:2307.01226v1 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2307.01226</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, Neural Topic Models (NTM), inspired by variational autoencoders,
have attracted a lot of research interest; however, these methods have limited
applications in the real world due to the challenge of incorporating human
knowledge. This work presents a semi-supervised neural topic modeling method,
vONTSS, which uses von Mises-Fisher (vMF) based variational autoencoders and
optimal transport. When a few keywords per topic are provided, vONTSS in the
semi-supervised setting generates potential topics and optimizes topic-keyword
quality and topic classification. Experiments show that vONTSS outperforms
existing semi-supervised topic modeling methods in classification accuracy and
diversity. vONTSS also supports unsupervised topic modeling. Quantitative and
qualitative experiments show that vONTSS in the unsupervised setting
outperforms recent NTMs on multiple aspects: vONTSS discovers highly clustered
and coherent topics on benchmark datasets. It is also much faster than the
state-of-the-art weakly supervised text classification method while achieving
similar classification performance. We further prove the equivalence of optimal
transport loss and cross-entropy loss at the global minimum.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Weijie Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sengamedu_S/0/1/0/all/0/1&quot;&gt;Srinivasan H. Sengamedu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iannacci_F/0/1/0/all/0/1&quot;&gt;Francis Iannacci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jinjin Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02889">
<title>Learning to Solve Tasks with Exploring Prior Behaviours. (arXiv:2307.02889v1 [cs.RO] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2307.02889</link>
<description rdf:parseType="Literal">&lt;p&gt;Demonstrations are widely used in Deep Reinforcement Learning (DRL) for
facilitating solving tasks with sparse rewards. However, the tasks in
real-world scenarios can often have varied initial conditions from the
demonstration, which would require additional prior behaviours. For example,
consider we are given the demonstration for the task of \emph{picking up an
object from an open drawer}, but the drawer is closed in the training. Without
acquiring the prior behaviours of opening the drawer, the robot is unlikely to
solve the task. To address this, in this paper we propose an Intrinsic Rewards
Driven Example-based Control \textbf{(IRDEC)}. Our method can endow agents with
the ability to explore and acquire the required prior behaviours and then
connect to the task-specific behaviours in the demonstration to solve
sparse-reward tasks without requiring additional demonstration of the prior
behaviours. The performance of our method outperforms other baselines on three
navigation tasks and one robotic manipulation task with sparse rewards. Codes
are available at https://github.com/Ricky-Zhu/IRDEC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1&quot;&gt;Ruiqi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Siyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1&quot;&gt;Tianhong Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chongjie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Celiktutan_O/0/1/0/all/0/1&quot;&gt;Oya Celiktutan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03056">
<title>Generalizing Backpropagation for Gradient-Based Interpretability. (arXiv:2307.03056v1 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2307.03056</link>
<description rdf:parseType="Literal">&lt;p&gt;Many popular feature-attribution methods for interpreting deep neural
networks rely on computing the gradients of a model&apos;s output with respect to
its inputs. While these methods can indicate which input features may be
important for the model&apos;s prediction, they reveal little about the inner
workings of the model itself. In this paper, we observe that the gradient
computation of a model is a special case of a more general formulation using
semirings. This observation allows us to generalize the backpropagation
algorithm to efficiently compute other interpretable statistics about the
gradient graph of a neural network, such as the highest-weighted path and
entropy. We implement this generalized algorithm, evaluate it on synthetic
datasets to better understand the statistics it computes, and apply it to study
BERT&apos;s behavior on the subject-verb number agreement task (SVA). With this
method, we (a) validate that the amount of gradient flow through a component of
a model reflects its importance to a prediction and (b) for SVA, identify which
pathways of the self-attention mechanism are most important.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_K/0/1/0/all/0/1&quot;&gt;Kevin Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hennigen_L/0/1/0/all/0/1&quot;&gt;Lucas Torroba Hennigen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoehr_N/0/1/0/all/0/1&quot;&gt;Niklas Stoehr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Warstadt_A/0/1/0/all/0/1&quot;&gt;Alexander Warstadt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1&quot;&gt;Ryan Cotterell&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>