<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-12-17T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09269" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09300" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09310" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09313" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09316" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09323" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09334" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09337" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09366" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09369" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09387" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09397" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09401" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09402" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09410" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09411" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09420" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09423" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09424" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09426" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09434" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09436" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09442" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09446" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09456" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09460" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09462" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09466" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09467" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09468" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09469" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09478" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09498" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09501" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09507" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09510" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09511" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09513" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09532" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09539" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09545" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09546" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09548" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09561" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09579" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09584" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09585" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09588" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09613" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09630" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09639" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09645" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09646" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09658" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09665" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09672" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09676" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09691" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09693" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09695" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09699" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09703" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09708" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09723" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09738" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09758" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09773" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09781" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09788" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09792" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09799" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09802" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09806" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09812" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09818" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09844" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09857" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09877" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09881" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09885" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09894" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09897" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09906" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09907" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09917" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09922" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09926" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09928" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09932" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09938" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09950" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09953" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09958" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09963" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09966" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09971" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09982" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09983" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09995" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09997" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10008" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10029" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1803.03114" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.10547" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.16230" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.17505" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.07514" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.05754" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.03724" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.05836" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.08754" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05421" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.11476" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13030" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15835" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04308" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14448" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13332" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.16387" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04237" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.10822" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12367" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.02730" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.04339" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07473" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07597" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11236" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03708" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04687" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15539" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15950" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16218" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18333" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02544" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05741" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06330" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10329" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15786" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16973" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17104" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17401" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04103" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06371" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06718" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07559" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08078" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08084" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08274" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08282" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08374" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08672" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08688" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08702" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08782" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08931" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08948" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09162" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2312.09269">
<title>Efficient speech detection in environmental audio using acoustic recognition and knowledge distillation. (arXiv:2312.09269v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2312.09269</link>
<description rdf:parseType="Literal">&lt;p&gt;The ongoing biodiversity crisis, driven by factors such as land-use change
and global warming, emphasizes the need for effective ecological monitoring
methods. Acoustic monitoring of biodiversity has emerged as an important
monitoring tool. Detecting human voices in soundscape monitoring projects is
useful both for analysing human disturbance and for privacy filtering. Despite
significant strides in deep learning in recent years, the deployment of large
neural networks on compact devices poses challenges due to memory and latency
constraints. Our approach focuses on leveraging knowledge distillation
techniques to design efficient, lightweight student models for speech detection
in bioacoustics. In particular, we employed the MobileNetV3-Small-Pi model to
create compact yet effective student architectures to compare against the
larger EcoVAD teacher model, a well-regarded voice detection architecture in
eco-acoustic monitoring. The comparative analysis included examining various
configurations of the MobileNetV3-Small-Pi derived student models to identify
optimal performance. Additionally, a thorough evaluation of different
distillation techniques was conducted to ascertain the most effective method
for model selection. Our findings revealed that the distilled models exhibited
comparable performance to the EcoVAD teacher model, indicating a promising
approach to overcoming computational barriers for real-time ecological
monitoring.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Priebe_D/0/1/0/all/0/1&quot;&gt;Drew Priebe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghani_B/0/1/0/all/0/1&quot;&gt;Burooj Ghani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stowell_D/0/1/0/all/0/1&quot;&gt;Dan Stowell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09300">
<title>Self-Evaluation Improves Selective Generation in Large Language Models. (arXiv:2312.09300v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.09300</link>
<description rdf:parseType="Literal">&lt;p&gt;Safe deployment of large language models (LLMs) may benefit from a reliable
method for assessing their generated content to determine when to abstain or to
selectively generate. While likelihood-based metrics such as perplexity are
widely employed, recent research has demonstrated the limitations of using
sequence-level probability estimates given by LLMs as reliable indicators of
generation quality. Conversely, LLMs have demonstrated strong calibration at
the token level, particularly when it comes to choosing correct answers in
multiple-choice questions or evaluating true/false statements. In this work, we
reformulate open-ended generation tasks into token-level prediction tasks, and
leverage LLMs&apos; superior calibration at the token level. We instruct an LLM to
self-evaluate its answers, employing either a multi-way comparison or a
point-wise evaluation approach, with the option to include a ``None of the
above&apos;&apos; option to express the model&apos;s uncertainty explicitly. We benchmark a
range of scoring methods based on self-evaluation and evaluate their
performance in selective generation using TruthfulQA and TL;DR. Through
experiments with PaLM-2 and GPT-3, we demonstrate that self-evaluation based
scores not only improve accuracy, but also correlate better with the overall
quality of generated content.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1&quot;&gt;Jie Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1&quot;&gt;Tu Vu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1&quot;&gt;Peter J. Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1&quot;&gt;Balaji Lakshminarayanan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09310">
<title>Neural Time-Reversed Generalized Riccati Equation. (arXiv:2312.09310v1 [math.OC])</title>
<link>http://arxiv.org/abs/2312.09310</link>
<description rdf:parseType="Literal">&lt;p&gt;Optimal control deals with optimization problems in which variables steer a
dynamical system, and its outcome contributes to the objective function. Two
classical approaches to solving these problems are Dynamic Programming and the
Pontryagin Maximum Principle. In both approaches, Hamiltonian equations offer
an interpretation of optimality through auxiliary variables known as costates.
However, Hamiltonian equations are rarely used due to their reliance on
forward-backward algorithms across the entire temporal domain. This paper
introduces a novel neural-based approach to optimal control, with the aim of
working forward-in-time. Neural networks are employed not only for implementing
state dynamics but also for estimating costate variables. The parameters of the
latter network are determined at each time step using a newly introduced local
policy referred to as the time-reversed generalized Riccati equation. This
policy is inspired by a result discussed in the Linear Quadratic (LQ) problem,
which we conjecture stabilizes state dynamics. We support this conjecture by
discussing experimental results from a range of optimal control case studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Betti_A/0/1/0/all/0/1&quot;&gt;Alessandro Betti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Casoni_M/0/1/0/all/0/1&quot;&gt;Michele Casoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Gori_M/0/1/0/all/0/1&quot;&gt;Marco Gori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Marullo_S/0/1/0/all/0/1&quot;&gt;Simone Marullo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Melacci_S/0/1/0/all/0/1&quot;&gt;Stefano Melacci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Tiezzi_M/0/1/0/all/0/1&quot;&gt;Matteo Tiezzi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09313">
<title>LatentEditor: Text Driven Local Editing of 3D Scenes. (arXiv:2312.09313v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09313</link>
<description rdf:parseType="Literal">&lt;p&gt;While neural fields have made significant strides in view synthesis and scene
reconstruction, editing them poses a formidable challenge due to their implicit
encoding of geometry and texture information from multi-view inputs. In this
paper, we introduce \textsc{LatentEditor}, an innovative framework designed to
empower users with the ability to perform precise and locally controlled
editing of neural fields using text prompts. Leveraging denoising diffusion
models, we successfully embed real-world scenes into the latent space,
resulting in a faster and more adaptable NeRF backbone for editing compared to
traditional methods. To enhance editing precision, we introduce a delta score
to calculate the 2D mask in the latent space that serves as a guide for local
modifications while preserving irrelevant regions. Our novel pixel-level
scoring approach harnesses the power of InstructPix2Pix (IP2P) to discern the
disparity between IP2P conditional and unconditional noise predictions in the
latent space. The edited latents conditioned on the 2D masks are then
iteratively updated in the training set to achieve 3D local editing. Our
approach achieves faster editing speeds and superior output quality compared to
existing 3D editing models, bridging the gap between textual instructions and
high-quality 3D scene editing in latent space. We show the superiority of our
approach on four benchmark 3D datasets, LLFF, IN2N, NeRFStudio and NeRF-Art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khalid_U/0/1/0/all/0/1&quot;&gt;Umar Khalid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iqbal_H/0/1/0/all/0/1&quot;&gt;Hasan Iqbal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karim_N/0/1/0/all/0/1&quot;&gt;Nazmul Karim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_J/0/1/0/all/0/1&quot;&gt;Jing Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09316">
<title>Distributional Latent Variable Models with an Application in Active Cognitive Testing. (arXiv:2312.09316v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.09316</link>
<description rdf:parseType="Literal">&lt;p&gt;Cognitive modeling commonly relies on asking participants to complete a
battery of varied tests in order to estimate attention, working memory, and
other latent variables. In many cases, these tests result in highly variable
observation models. A near-ubiquitous approach is to repeat many observations
for each test, resulting in a distribution over the outcomes from each test
given to each subject. In this paper, we explore the usage of latent variable
modeling to enable learning across many correlated variables simultaneously. We
extend latent variable models (LVMs) to the setting where observed data for
each subject are a series of observations from many different distributions,
rather than simple vectors to be reconstructed. By embedding test battery
results for individuals in a latent space that is trained jointly across a
population, we are able to leverage correlations both between tests for a
single participant and between multiple participants. We then propose an active
learning framework that leverages this model to conduct more efficient
cognitive test batteries. We validate our approach by demonstrating with
real-time data acquisition that it performs comparably to conventional methods
in making item-level predictions with fewer test items.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasumba_R/0/1/0/all/0/1&quot;&gt;Robert Kasumba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marticorena_D/0/1/0/all/0/1&quot;&gt;Dom CP Marticorena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pahor_A/0/1/0/all/0/1&quot;&gt;Anja Pahor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramani_G/0/1/0/all/0/1&quot;&gt;Geetha Ramani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goffney_I/0/1/0/all/0/1&quot;&gt;Imani Goffney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaeggi_S/0/1/0/all/0/1&quot;&gt;Susanne M Jaeggi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seitz_A/0/1/0/all/0/1&quot;&gt;Aaron Seitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gardner_J/0/1/0/all/0/1&quot;&gt;Jacob R Gardner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barbour_D/0/1/0/all/0/1&quot;&gt;Dennis L Barbour&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09323">
<title>Perspectives on the State and Future of Deep Learning -- 2023. (arXiv:2312.09323v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.09323</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of this series is to chronicle opinions and issues in the field of
machine learning as they stand today and as they change over time. The plan is
to host this survey periodically until the AI singularity
paperclip-frenzy-driven doomsday, keeping an updated list of topical questions
and interviewing new community members for each edition. In this issue, we
probed people&apos;s opinions on interpretable AI, the value of benchmarking in
modern NLP, the state of progress towards understanding deep learning, and the
future of academia.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1&quot;&gt;Micah Goldblum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1&quot;&gt;Anima Anandkumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1&quot;&gt;Richard Baraniuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1&quot;&gt;Tom Goldstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1&quot;&gt;Kyunghyun Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1&quot;&gt;Zachary C Lipton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitchell_M/0/1/0/all/0/1&quot;&gt;Melanie Mitchell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakkiran_P/0/1/0/all/0/1&quot;&gt;Preetum Nakkiran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1&quot;&gt;Max Welling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1&quot;&gt;Andrew Gordon Wilson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09334">
<title>ArchiGuesser -- AI Art Architecture Educational Game. (arXiv:2312.09334v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.09334</link>
<description rdf:parseType="Literal">&lt;p&gt;The use of generative AI in education is a controversial topic. Current
technology offers the potential to create educational content from text,
speech, to images based on simple input prompts. This can enhance productivity
by summarizing knowledge and improving communication, quickly adjusting to
different types of learners. Moreover, generative AI holds the promise of
making the learning itself more fun, by responding to user inputs and
dynamically generating high-quality creative material. In this paper we present
the multisensory educational game ArchiGuesser that combines various AI
technologies from large language models, image generation, to computer vision
to serve a single purpose: Teaching students in a playful way the diversity of
our architectural history and how generative AI works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ploennigs_J/0/1/0/all/0/1&quot;&gt;Joern Ploennigs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berger_M/0/1/0/all/0/1&quot;&gt;Markus Berger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carnein_E/0/1/0/all/0/1&quot;&gt;Eva Carnein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09337">
<title>Promptable Behaviors: Personalizing Multi-Objective Rewards from Human Preferences. (arXiv:2312.09337v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09337</link>
<description rdf:parseType="Literal">&lt;p&gt;Customizing robotic behaviors to be aligned with diverse human preferences is
an underexplored challenge in the field of embodied AI. In this paper, we
present Promptable Behaviors, a novel framework that facilitates efficient
personalization of robotic agents to diverse human preferences in complex
environments. We use multi-objective reinforcement learning to train a single
policy adaptable to a broad spectrum of preferences. We introduce three
distinct methods to infer human preferences by leveraging different types of
interactions: (1) human demonstrations, (2) preference feedback on trajectory
comparisons, and (3) language instructions. We evaluate the proposed method in
personalized object-goal navigation and flee navigation tasks in ProcTHOR and
RoboTHOR, demonstrating the ability to prompt agent behaviors to satisfy human
preferences in various scenarios. Project page:
https://promptable-behaviors.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_M/0/1/0/all/0/1&quot;&gt;Minyoung Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weihs_L/0/1/0/all/0/1&quot;&gt;Luca Weihs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1&quot;&gt;Chanwoo Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kimin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kembhavi_A/0/1/0/all/0/1&quot;&gt;Aniruddha Kembhavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ehsani_K/0/1/0/all/0/1&quot;&gt;Kiana Ehsani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09366">
<title>Arabic Mini-ClimateGPT : A Climate Change and Sustainability Tailored Arabic LLM. (arXiv:2312.09366v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.09366</link>
<description rdf:parseType="Literal">&lt;p&gt;Climate change is one of the most significant challenges we face together as
a society. Creating awareness and educating policy makers the wide-ranging
impact of climate change is an essential step towards a sustainable future.
Recently, Large Language Models (LLMs) like ChatGPT and Bard have shown
impressive conversational abilities and excel in a wide variety of NLP tasks.
While these models are close-source, recently alternative open-source LLMs such
as Stanford Alpaca and Vicuna have shown promising results. However, these
open-source models are not specifically tailored for climate related domain
specific information and also struggle to generate meaningful responses in
other languages such as, Arabic. To this end, we propose a light-weight Arabic
Mini-ClimateGPT that is built on an open-source LLM and is specifically
fine-tuned on a conversational-style instruction tuning curated Arabic dataset
Clima500-Instruct with over 500k instructions about climate change and
sustainability. Further, our model also utilizes a vector embedding based
retrieval mechanism during inference. We validate our proposed model through
quantitative and qualitative evaluations on climate-related queries. Our model
surpasses the baseline LLM in 88.3% of cases during ChatGPT-based evaluation.
Furthermore, our human expert evaluation reveals an 81.6% preference for our
model&apos;s responses over multiple popular open-source models. Our open-source
demos, code-base and models are available here
https://github.com/mbzuai-oryx/ClimateGPT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mullappilly_S/0/1/0/all/0/1&quot;&gt;Sahal Shaji Mullappilly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaker_A/0/1/0/all/0/1&quot;&gt;Abdelrahman Shaker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thawakar_O/0/1/0/all/0/1&quot;&gt;Omkar Thawakar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cholakkal_H/0/1/0/all/0/1&quot;&gt;Hisham Cholakkal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anwer_R/0/1/0/all/0/1&quot;&gt;Rao Muhammad Anwer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Salman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1&quot;&gt;Fahad Shahbaz Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09369">
<title>Audio-visual fine-tuning of audio-only ASR models. (arXiv:2312.09369v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2312.09369</link>
<description rdf:parseType="Literal">&lt;p&gt;Audio-visual automatic speech recognition (AV-ASR) models are very effective
at reducing word error rates on noisy speech, but require large amounts of
transcribed AV training data. Recently, audio-visual self-supervised learning
(SSL) approaches have been developed to reduce this dependence on transcribed
AV data, but these methods are quite complex and computationally expensive. In
this work, we propose replacing these expensive AV-SSL methods with a simple
and fast \textit{audio-only} SSL method, and then performing AV supervised
fine-tuning. We show that this approach is competitive with state-of-the-art
(SOTA) AV-SSL methods on the LRS3-TED benchmark task (within 0.5% absolute
WER), while being dramatically simpler and more efficient (12-30x faster to
pre-train). Furthermore, we show we can extend this approach to convert a SOTA
audio-only ASR model into an AV model. By doing so, we match SOTA AV-SSL
results, even though no AV data was used during pre-training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+May_A/0/1/0/all/0/1&quot;&gt;Avner May&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serdyuk_D/0/1/0/all/0/1&quot;&gt;Dmitriy Serdyuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1&quot;&gt;Ankit Parag Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braga_O/0/1/0/all/0/1&quot;&gt;Otavio Braga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siohan_O/0/1/0/all/0/1&quot;&gt;Olivier Siohan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09387">
<title>High-Resolution Maps of Left Atrial Displacements and Strains Estimated with 3D CINE MRI and Unsupervised Neural Networks. (arXiv:2312.09387v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09387</link>
<description rdf:parseType="Literal">&lt;p&gt;The functional analysis of the left atrium (LA) is important for evaluating
cardiac health and understanding diseases like atrial fibrillation. Cine MRI is
ideally placed for the detailed 3D characterisation of LA motion and
deformation, but it is lacking appropriate acquisition and analysis tools. In
this paper, we present Analysis for Left Atrial Displacements and Deformations
using unsupervIsed neural Networks, \textit{Aladdin}, to automatically and
reliably characterise regional LA deformations from high-resolution 3D Cine
MRI. The tool includes: an online few-shot segmentation network (Aladdin-S), an
online unsupervised image registration network (Aladdin-R), and a strain
calculations pipeline tailored to the LA. We create maps of LA Displacement
Vector Field (DVF) magnitude and LA principal strain values from images of 10
healthy volunteers and 8 patients with cardiovascular disease (CVD). We
additionally create an atlas of these biomarkers using the data from the
healthy volunteers. Aladdin is able to accurately track the LA wall across the
cardiac cycle and characterize its motion and deformation. The overall DVF
magnitude and principal strain values are significantly higher in the healthy
group vs CVD patients: $2.85 \pm 1.59~mm$ and $0.09 \pm 0.05$ vs $1.96 \pm
0.74~mm$ and $0.03 \pm 0.04$, respectively. The time course of these metrics is
also different in the two groups, with a more marked active contraction phase
observed in the healthy cohort. Finally, utilizing the LA atlas allows us to
identify regional deviations from the population distribution that may indicate
focal tissue abnormalities. The proposed tool for the quantification of novel
regional LA deformation biomarkers should have important clinical applications.
The source code, anonymized images, generated maps and atlas are publicly
available: https://github.com/cgalaz01/aladdin_cmr_la.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galazis_C/0/1/0/all/0/1&quot;&gt;Christoforos Galazis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shepperd_S/0/1/0/all/0/1&quot;&gt;Samuel Shepperd&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brouwer_E/0/1/0/all/0/1&quot;&gt;Emma Brouwer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Queiros_S/0/1/0/all/0/1&quot;&gt;Sandro Queir&amp;#xf3;s&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alskaf_E/0/1/0/all/0/1&quot;&gt;Ebraham Alskaf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anjari_M/0/1/0/all/0/1&quot;&gt;Mustafa Anjari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiribiri_A/0/1/0/all/0/1&quot;&gt;Amedeo Chiribiri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jack Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bharath_A/0/1/0/all/0/1&quot;&gt;Anil A. Bharath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varela_M/0/1/0/all/0/1&quot;&gt;Marta Varela&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09397">
<title>Large Language Models for Autonomous Driving: Real-World Experiments. (arXiv:2312.09397v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.09397</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous driving systems are increasingly popular in today&apos;s technological
landscape, where vehicles with partial automation have already been widely
available on the market, and the full automation era with ``driverless&apos;&apos;
capabilities is near the horizon. However, accurately understanding humans&apos;
commands, particularly for autonomous vehicles that have only passengers
instead of drivers, and achieving a high level of personalization remain
challenging tasks in the development of autonomous driving systems. In this
paper, we introduce a Large Language Model (LLM)-based framework Talk-to-Drive
(Talk2Drive) to process verbal commands from humans and make autonomous driving
decisions with contextual information, satisfying their personalized
preferences for safety, efficiency, and comfort. First, a speech recognition
module is developed for Talk2Drive to interpret verbal inputs from humans to
textual instructions, which are then sent to LLMs for reasoning. Then,
appropriate commands for the Electrical Control Unit (ECU) are generated,
achieving a 100\% success rate in executing codes. Real-world experiments show
that our framework can substantially reduce the takeover rate for a diverse
range of drivers by up to 90.1\%. To the best of our knowledge, Talk2Drive
marks the first instance of employing an LLM-based system in a real-world
autonomous driving environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1&quot;&gt;Can Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zichong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yupeng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yunsheng Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Juanwu Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziran Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09401">
<title>Inter-Layer Scheduling Space Exploration for Multi-model Inference on Heterogeneous Chiplets. (arXiv:2312.09401v1 [cs.AR])</title>
<link>http://arxiv.org/abs/2312.09401</link>
<description rdf:parseType="Literal">&lt;p&gt;To address increasing compute demand from recent multi-model workloads with
heavy models like large language models, we propose to deploy heterogeneous
chiplet-based multi-chip module (MCM)-based accelerators. We develop an
advanced scheduling framework for heterogeneous MCM accelerators that
comprehensively consider complex heterogeneity and inter-chiplet pipelining.
Our experiments using our framework on GPT-2 and ResNet-50 models on a
4-chiplet system have shown upto 2.2x and 1.9x increase in throughput and
energy efficiency, compared to a monolithic accelerator with an optimized
output-stationary dataflow.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Odema_M/0/1/0/all/0/1&quot;&gt;Mohanad Odema&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1&quot;&gt;Hyoukjun Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faruque_M/0/1/0/all/0/1&quot;&gt;Mohammad Abdullah Al Faruque&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09402">
<title>CERN for AGI: A Theoretical Framework for Autonomous Simulation-Based Artificial Intelligence Testing and Alignment. (arXiv:2312.09402v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2312.09402</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the potential of a multidisciplinary approach to testing
and aligning artificial general intelligence (AGI) and LLMs. Due to the rapid
development and wide application of LLMs, challenges such as ethical alignment,
controllability, and predictability of these models have become important
research topics. This study investigates an innovative simulation-based
multi-agent system within a virtual reality framework that replicates the
real-world environment. The framework is populated by automated &apos;digital
citizens,&apos; simulating complex social structures and interactions to examine and
optimize AGI. Application of various theories from the fields of sociology,
social psychology, computer science, physics, biology, and economics
demonstrates the possibility of a more human-aligned and socially responsible
AGI. The purpose of such a digital environment is to provide a dynamic platform
where advanced AI agents can interact and make independent decisions, thereby
mimicking realistic scenarios. The actors in this digital city, operated by the
LLMs, serve as the primary agents, exhibiting high degrees of autonomy. While
this approach shows immense potential, there are notable challenges and
limitations, most significantly the unpredictable nature of real-world social
dynamics. This research endeavors to contribute to the development and
refinement of AGI, emphasizing the integration of social, ethical, and
theoretical dimensions for future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bojic_L/0/1/0/all/0/1&quot;&gt;Ljubisa Bojic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cinelli_M/0/1/0/all/0/1&quot;&gt;Matteo Cinelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Culibrk_D/0/1/0/all/0/1&quot;&gt;Dubravko Culibrk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delibasic_B/0/1/0/all/0/1&quot;&gt;Boris Delibasic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09410">
<title>Prediction of rare events in the operation of household equipment using co-evolving time series. (arXiv:2312.09410v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.09410</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we propose an approach for predicting rare events by
exploiting time series in coevolution. Our approach involves a weighted
autologistic regression model, where we leverage the temporal behavior of the
data to enhance predictive capabilities. By addressing the issue of imbalanced
datasets, we establish constraints leading to weight estimation and to improved
performance. Evaluation on synthetic and real-world datasets confirms that our
approach outperform state-of-the-art of predicting home equipment failure
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mecheri_H/0/1/0/all/0/1&quot;&gt;Hadia Mecheri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benamirouche_I/0/1/0/all/0/1&quot;&gt;Islam Benamirouche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fass_F/0/1/0/all/0/1&quot;&gt;Feriel Fass&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziou_D/0/1/0/all/0/1&quot;&gt;Djemel Ziou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kadri_N/0/1/0/all/0/1&quot;&gt;Nassima Kadri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09411">
<title>OTOv3: Automatic Architecture-Agnostic Neural Network Training and Compression from Structured Pruning to Erasing Operators. (arXiv:2312.09411v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.09411</link>
<description rdf:parseType="Literal">&lt;p&gt;Compressing a predefined deep neural network (DNN) into a compact sub-network
with competitive performance is crucial in the efficient machine learning
realm. This topic spans various techniques, from structured pruning to neural
architecture search, encompassing both pruning and erasing operators
perspectives. Despite advancements, existing methods suffers from complex,
multi-stage processes that demand substantial engineering and domain knowledge,
limiting their broader applications. We introduce the third-generation
Only-Train-Once (OTOv3), which first automatically trains and compresses a
general DNN through pruning and erasing operations, creating a compact and
competitive sub-network without the need of fine-tuning. OTOv3 simplifies and
automates the training and compression process, minimizes the engineering
efforts required from users. It offers key technological advancements: (i)
automatic search space construction for general DNNs based on dependency graph
analysis; (ii) Dual Half-Space Projected Gradient (DHSPG) and its enhanced
version with hierarchical search (H2SPG) to reliably solve (hierarchical)
structured sparsity problems and ensure sub-network validity; and (iii)
automated sub-network construction using solutions from DHSPG/H2SPG and
dependency graphs. Our empirical results demonstrate the efficacy of OTOv3
across various benchmarks in structured pruning and neural architecture search.
OTOv3 produces sub-networks that match or exceed the state-of-the-arts. The
source code will be available at https://github.com/tianyic/only_train_once.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_T/0/1/0/all/0/1&quot;&gt;Tianyu Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhihui Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zeyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;HsiangTao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zharkov_I/0/1/0/all/0/1&quot;&gt;Ilya Zharkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1&quot;&gt;Luming Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09420">
<title>Fairness-Driven Optimization of RIS-Augmented 5G Networks for Seamless 3D UAV Connectivity Using DRL Algorithms. (arXiv:2312.09420v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2312.09420</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the problem of joint active and passive beamforming
for reconfigurable intelligent surface (RIS)-assisted massive multiple-input
multiple-output systems towards the extension of the wireless cellular coverage
in 3D, where multiple RISs, each equipped with an array of passive elements,
are deployed to assist a base station (BS) to simultaneously serve multiple
unmanned aerial vehicles (UAVs) in the same time-frequency resource of 5G
wireless communications. With a focus on ensuring fairness among UAVs, our
objective is to maximize the minimum signal-to-interference-plus-noise ratio
(SINR) at UAVs by jointly optimizing the transmit beamforming parameters at the
BS and phase shift parameters at RISs. We propose two novel algorithms to
address this problem. The first algorithm aims to mitigate interference by
calculating the BS beamforming matrix through matrix inverse operations once
the phase shift parameters are determined. The second one is based on the
principle that one RIS element only serves one UAV and the phase shift
parameter of this RIS element is optimally designed to compensate the phase
offset caused by the propagation and fading. To obtain the optimal parameters,
we utilize one state-of-the-art reinforcement learning algorithm, deep
deterministic policy gradient, to solve these two optimization problems.
Simulation results are provided to illustrate the effectiveness of our proposed
solution and some insightful remarks are observed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yu Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Alhammadi_A/0/1/0/all/0/1&quot;&gt;Ahmed Alhammadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jiguang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fakhreddine_A/0/1/0/all/0/1&quot;&gt;Aymen Fakhreddine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bader_F/0/1/0/all/0/1&quot;&gt;Faouzi Bader&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09423">
<title>Decoding EEG-based Workload Levels Using Spatio-temporal Features Under Flight Environment. (arXiv:2312.09423v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2312.09423</link>
<description rdf:parseType="Literal">&lt;p&gt;The detection of pilots&apos; mental states is important due to the potential for
their abnormal mental states to result in catastrophic accidents. This study
introduces the feasibility of employing deep learning techniques to classify
different workload levels, specifically normal state, low workload, and high
workload. To the best of our knowledge, this study is the first attempt to
classify workload levels of pilots. Our approach involves the hybrid deep
neural network that consists of five convolutional blocks and one long
short-term memory block to extract the significant features from
electroencephalography signals. Ten pilots participated in the experiment,
which was conducted within the simulated flight environment. In contrast to
four conventional models, our proposed model achieved a superior grand--average
accuracy of 0.8613, surpassing other conventional models by at least 0.0597 in
classifying workload levels across all participants. Our model not only
successfully classified workload levels but also provided valuable feedback to
the participants. Hence, we anticipate that our study will make the significant
contributions to the advancement of autonomous flight and driving leveraging
artificial intelligence technology in the future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Dae-Hyeok Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sung-Jin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Si-Hyun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seong-Whan Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09424">
<title>Open Domain Knowledge Extraction for Knowledge Graphs. (arXiv:2312.09424v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.09424</link>
<description rdf:parseType="Literal">&lt;p&gt;The quality of a knowledge graph directly impacts the quality of downstream
applications (e.g. the number of answerable questions using the graph). One
ongoing challenge when building a knowledge graph is to ensure completeness and
freshness of the graph&apos;s entities and facts. In this paper, we introduce ODKE,
a scalable and extensible framework that sources high-quality entities and
facts from open web at scale. ODKE utilizes a wide range of extraction models
and supports both streaming and batch processing at different latency. We
reflect on the challenges and design decisions made and share lessons learned
when building and deploying ODKE to grow an industry-scale open domain
knowledge graph.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_K/0/1/0/all/0/1&quot;&gt;Kun Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belyi_A/0/1/0/all/0/1&quot;&gt;Anton Belyi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Fei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khorshidi_S/0/1/0/all/0/1&quot;&gt;Samira Khorshidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikfarjam_A/0/1/0/all/0/1&quot;&gt;Azadeh Nikfarjam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khot_R/0/1/0/all/0/1&quot;&gt;Rahul Khot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sang_Y/0/1/0/all/0/1&quot;&gt;Yisi Sang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luna_K/0/1/0/all/0/1&quot;&gt;Katherine Luna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1&quot;&gt;Xianqi Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1&quot;&gt;Eric Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Govind_Y/0/1/0/all/0/1&quot;&gt;Yash Govind&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seivwright_C/0/1/0/all/0/1&quot;&gt;Chloe Seivwright&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yiwen Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fakhry_A/0/1/0/all/0/1&quot;&gt;Ahmed Fakhry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rekatsinas_T/0/1/0/all/0/1&quot;&gt;Theo Rekatsinas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilyas_I/0/1/0/all/0/1&quot;&gt;Ihab Ilyas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1&quot;&gt;Xiaoguang Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunyao Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09426">
<title>Deep Learning Models for Arrhythmia Classification Using Stacked Time-frequency Scalogram Images from ECG Signals. (arXiv:2312.09426v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2312.09426</link>
<description rdf:parseType="Literal">&lt;p&gt;Electrocardiograms (ECGs), a medical monitoring technology recording cardiac
activity, are widely used for diagnosing cardiac arrhythmia. The diagnosis is
based on the analysis of the deformation of the signal shapes due to irregular
heart rates associated with heart diseases. Due to the infeasibility of manual
examination of large volumes of ECG data, this paper aims to propose an
automated AI based system for ECG-based arrhythmia classification. To this
front, a deep learning based solution has been proposed for ECG-based
arrhythmia classification. Twelve lead electrocardiograms (ECG) of length 10
sec from 45, 152 individuals from Shaoxing People&apos;s Hospital (SPH) dataset from
PhysioNet with four different types of arrhythmias were used. The sampling
frequency utilized was 500 Hz. Median filtering was used to preprocess the ECG
signals. For every 1 sec of ECG signal, the time-frequency (TF) scalogram was
estimated and stacked row wise to obtain a single image from 12 channels,
resulting in 10 stacked TF scalograms for each ECG signal. These stacked TF
scalograms are fed to the pretrained convolutional neural network (CNN), 1D
CNN, and 1D CNN-LSTM (Long short-term memory) models, for arrhythmia
classification. The fine-tuned CNN models obtained the best test accuracy of
about 98% followed by 95% test accuracy by basic CNN-LSTM in arrhythmia
classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Aarotale_P/0/1/0/all/0/1&quot;&gt;Parshuram N. Aarotale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rattani_A/0/1/0/all/0/1&quot;&gt;Ajita Rattani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09434">
<title>Task Tree Retrieval For Robotic Cooking. (arXiv:2312.09434v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.09434</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper is based on developing different algorithms, which generate the
task tree planning for the given goal node(recipe). The knowledge
representation of the dishes is called FOON. It contains the different objects
and their between them with respective to the motion node The graphical
representation of FOON is made by noticing the change in the state of an object
with respect to the human manipulators. We will explore how the FOON is created
for different recipes by the robots. Task planning contains difficulties in
exploring unknown problems, as its knowledge is limited to the FOON. To get the
task tree planning for a given recipe, the robot will retrieve the information
of different functional units from the knowledge retrieval process called FOON.
Thus the generated subgraphs will allow the robot to cook the required dish.
Thus the robot can able to cook the given recipe by following the sequence of
instructions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nallu_C/0/1/0/all/0/1&quot;&gt;Chakradhar Reddy Nallu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09436">
<title>Temporal Transfer Learning for Traffic Optimization with Coarse-grained Advisory Autonomy. (arXiv:2312.09436v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.09436</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent development of connected and automated vehicle (CAV) technologies
has spurred investigations to optimize dense urban traffic. This paper
considers advisory autonomy, in which real-time driving advisories are issued
to drivers, thus blending the CAV and the human driver. Due to the complexity
of traffic systems, recent studies of coordinating CAVs have resorted to
leveraging deep reinforcement learning (RL). Advisory autonomy is formalized as
zero-order holds, and we consider a range of hold duration from 0.1 to 40
seconds. However, despite the similarity of the higher frequency tasks on CAVs,
a direct application of deep RL fails to be generalized to advisory autonomy
tasks. We introduce Temporal Transfer Learning (TTL) algorithms to select
source tasks, systematically leveraging the temporal structure to solve the
full range of tasks. TTL selects the most suitable source tasks to maximize the
performance of the range of tasks. We validate our algorithms on diverse
mixed-traffic scenarios, demonstrating that TTL more reliably solves the tasks
than baselines. This paper underscores the potential of coarse-grained advisory
autonomy with TTL in traffic flow optimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1&quot;&gt;Jung-Hoon Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sirui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jeongyun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Cathy Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09442">
<title>A Compact LSTM-SVM Fusion Model for Long-Duration Cardiovascular Diseases Detection. (arXiv:2312.09442v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2312.09442</link>
<description rdf:parseType="Literal">&lt;p&gt;Globally, cardiovascular diseases (CVDs) are the leading cause of mortality,
accounting for an estimated 17.9 million deaths annually. One critical clinical
objective is the early detection of CVDs using electrocardiogram (ECG) data, an
area that has received significant attention from the research community.
Recent advancements based on machine learning and deep learning have achieved
great progress in this domain. However, existing methodologies exhibit inherent
limitations, including inappropriate model evaluations and instances of data
leakage. In this study, we present a streamlined workflow paradigm for
preprocessing ECG signals into consistent 10-second durations, eliminating the
need for manual feature extraction/beat detection. We also propose a hybrid
model of Long Short-Term Memory (LSTM) with Support Vector Machine (SVM) for
fraud detection. This architecture consists of two LSTM layers and an SVM
classifier, which achieves a SOTA results with an Average precision score of
0.9402 on the MIT-BIH arrhythmia dataset and 0.9563 on the MIT-BIH atrial
fibrillation dataset. Based on the results, we believe our method can
significantly benefit the early detection and management of CVDs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Siyang Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09446">
<title>A Distributed Inference System for Detecting Task-wise Single Trial Event-Related Potential in Stream of Satellite Images. (arXiv:2312.09446v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2312.09446</link>
<description rdf:parseType="Literal">&lt;p&gt;Brain-computer interface (BCI) has garnered the significant attention for
their potential in various applications, with event-related potential (ERP)
performing a considerable role in BCI systems. This paper introduces a novel
Distributed Inference System tailored for detecting task-wise single-trial ERPs
in a stream of satellite images. Unlike traditional methodologies that employ a
single model for target detection, our system utilizes multiple models, each
optimized for specific tasks, ensuring enhanced performance across varying
image transition times and target onset times. Our experiments, conducted on
four participants, employed two paradigms: the Normal paradigm and an AI
paradigm with bounding boxes. Results indicate that our proposed system
outperforms the conventional methods in both paradigms, achieving the highest
$F_{\beta}$ scores. Furthermore, including bounding boxes in the AI paradigm
significantly improved target recognition. This study underscores the potential
of our Distributed Inference System in advancing the field of ERP detection in
satellite image streams.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sung-Jin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kwak_H/0/1/0/all/0/1&quot;&gt;Heon-Gyu Kwak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Han_H/0/1/0/all/0/1&quot;&gt;Hyeon-Taek Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Dae-Hyeok Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jeong_J/0/1/0/all/0/1&quot;&gt;Ji-Hoon Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seong-Whan Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09456">
<title>Pioneering EEG Motor Imagery Classification Through Counterfactual Analysis. (arXiv:2312.09456v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2312.09456</link>
<description rdf:parseType="Literal">&lt;p&gt;The application of counterfactual explanation (CE) techniques in the realm of
electroencephalography (EEG) classification has been relatively infrequent in
contemporary research. In this study, we attempt to introduce and explore a
novel non-generative approach to CE, specifically tailored for the analysis of
EEG signals. This innovative approach assesses the model&apos;s decision-making
process by strategically swapping patches derived from time-frequency analyses.
By meticulously examining the variations and nuances introduced in the
classification outcomes through this method, we aim to derive insights that can
enhance interpretability. The empirical results obtained from our experimental
investigations serve not only to validate the efficacy of our proposed approach
but also to reinforce human confidence in the model&apos;s predictive capabilities.
Consequently, these findings underscore the significance and potential value of
conducting further, more extensive research in this promising direction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yin_K/0/1/0/all/0/1&quot;&gt;Kang Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shin_H/0/1/0/all/0/1&quot;&gt;Hye-Bin Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hee-Dong Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seong-Whan Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09460">
<title>Taming Waves: A Physically-Interpretable Machine Learning Framework for Realizable Control of Wave Dynamics. (arXiv:2312.09460v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2312.09460</link>
<description rdf:parseType="Literal">&lt;p&gt;Controlling systems governed by partial differential equations is an
inherently hard problem. Specifically, control of wave dynamics is challenging
due to additional physical constraints and intrinsic properties of wave
phenomena such as dissipation, attenuation, reflection, and scattering. In this
work, we introduce an environment designed for the study of the control of
acoustic waves by actuated metamaterial designs. We utilize this environment
for the development of a novel machine-learning method, based on deep neural
networks, for efficiently learning the dynamics of an acoustic PDE from
samples. Our model is fully interpretable and maps physical constraints and
intrinsic properties of the real acoustic environment into its latent
representation of information. Within our model we use a trainable perfectly
matched layer to explicitly learn the property of acoustic energy dissipation.
Our model can be used to predict and control scattered wave energy. The
capabilities of our model are demonstrated on an important problem in
acoustics, which is the minimization of total scattered energy. Furthermore, we
show that the prediction of scattered energy by our model generalizes in time
and can be extended to long time horizons. We make our code repository publicly
available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shah_T/0/1/0/all/0/1&quot;&gt;Tristan Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Amirkulova_F/0/1/0/all/0/1&quot;&gt;Feruza Amirkulova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tiomkin_S/0/1/0/all/0/1&quot;&gt;Stas Tiomkin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09462">
<title>Applying Machine Learning Models on Metrology Data for Predicting Device Electrical Performance. (arXiv:2312.09462v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2312.09462</link>
<description rdf:parseType="Literal">&lt;p&gt;Moore Law states that transistor density will double every two years, which
is sustained until today due to continuous multi-directional innovations, such
as extreme ultraviolet lithography, novel patterning techniques etc., leading
the semiconductor industry towards 3nm node and beyond. For any patterning
scheme, the most important metric to evaluate the quality of printed patterns
is EPE, with overlay being its largest contribution. Overlay errors can lead to
fatal failures of IC devices such as short circuits or broken connections in
terms of P2P electrical contacts. Therefore, it is essential to develop
effective overlay analysis and control techniques to ensure good functionality
of fabricated semiconductor devices. In this work we have used an imec N14 BEOL
process flow using LELE patterning technique to print metal layers with minimum
pitch of 48nm with 193i lithography. FF structures are decomposed into two mask
layers (M1A and M1B) and then the LELE flow is carried out to make the final
patterns. Since a single M1 layer is decomposed into two masks, control of
overlay between the two masks is critical. The goal of this work is of two-fold
as, (a) to quantify the impact of overlay on capacitance and (b) to see if we
can predict the final capacitance measurements with selected machine learning
models at an early stage. To do so, scatterometry spectra are collected on
these electrical test structures at (a)post litho, (b)post TiN hardmask etch,
and (c)post Cu plating and CMP. Critical Dimension and overlay measurements for
line-space pattern are done with SEM post litho, post etch and post Cu CMP.
Various machine learning models are applied to do the capacitance prediction
with multiple metrology inputs at different steps of wafer processing. Finally,
we demonstrate that by using appropriate machine learning models we are able to
do better prediction of electrical results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dey_B/0/1/0/all/0/1&quot;&gt;Bappaditya Dey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ngo_A/0/1/0/all/0/1&quot;&gt;Anh Tuan Ngo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sacchi_S/0/1/0/all/0/1&quot;&gt;Sara Sacchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Blanco_V/0/1/0/all/0/1&quot;&gt;Victor Blanco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Leray_P/0/1/0/all/0/1&quot;&gt;Philippe Leray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Halder_S/0/1/0/all/0/1&quot;&gt;Sandip Halder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09466">
<title>Enhancing Trajectory Prediction through Self-Supervised Waypoint Noise Prediction. (arXiv:2312.09466v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.09466</link>
<description rdf:parseType="Literal">&lt;p&gt;Trajectory prediction is an important task that involves modeling the
indeterminate nature of traffic actors to forecast future trajectories given
the observed trajectory sequences. However, current methods confine themselves
to presumed data manifolds, assuming that trajectories strictly adhere to these
manifolds, resulting in overly simplified predictions. To this end, we propose
a novel approach called SSWNP (Self-Supervised Waypoint Noise Prediction). In
our approach, we first create clean and noise-augmented views of past observed
trajectories across the spatial domain of waypoints. We then compel the
trajectory prediction model to maintain spatial consistency between predictions
from these two views, in addition to the trajectory prediction task.
Introducing the noise-augmented view mitigates the model&apos;s reliance on a narrow
interpretation of the data manifold, enabling it to learn more plausible and
diverse representations. We also predict the noise present in the two views of
past observed trajectories as an auxiliary self-supervised task, enhancing the
model&apos;s understanding of the underlying representation and future predictions.
Empirical evidence demonstrates that the incorporation of SSWNP into the model
learning process significantly improves performance, even in noisy
environments, when compared to baseline methods. Our approach can complement
existing trajectory prediction methods. To showcase the effectiveness of our
approach, we conducted extensive experiments on three datasets: NBA Sports VU,
ETH-UCY, and TrajNet++, with experimental results highlighting the substantial
improvement achieved in trajectory prediction tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chib_P/0/1/0/all/0/1&quot;&gt;Pranav Singh Chib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1&quot;&gt;Pravendra Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09467">
<title>Performance Analysis of Fixed Broadband Wireless Access in mmWave Band in 5G. (arXiv:2312.09467v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2312.09467</link>
<description rdf:parseType="Literal">&lt;p&gt;An end-to-end fiber-based network holds the potential to provide
multi-gigabit fixed access to end-users. However, deploying fiber access,
especially in areas where fiber is non-existent, can be time-consuming and
costly, resulting in delayed returns for Operators. This work investigates
transmission data from fixed broadband wireless access in the mmWave band in
5G. Given the growing interest in this domain, understanding the transmission
characteristics of the data becomes crucial. While existing datasets for the
mmWave band are available, they are often generated from simulated
environments. In this study, we introduce a dataset compiled from real-world
transmission data collected from the Fixed Broadband Wireless Access in mmWave
Band device (RWM6050). The aim is to facilitate self-configuration based on
transmission characteristics. To achieve this, we propose an online machine
learning-based approach for real-time training and classification of
transmission characteristics. Additionally, we present two advanced temporal
models for more accurate classifications. Our results demonstrate the ability
to detect transmission angle and distance directly from the analysis of
transmission data with very high accuracy, reaching up to 99% accuracy on the
combined classification task. Finally, we outline promising future research
directions based on the collected data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Banerjee_S/0/1/0/all/0/1&quot;&gt;Soumya Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gochhayat_S/0/1/0/all/0/1&quot;&gt;Sarada Prasad Gochhayat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shetty_S/0/1/0/all/0/1&quot;&gt;Sachin Shetty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09468">
<title>Safe Reinforcement Learning in a Simulated Robotic Arm. (arXiv:2312.09468v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.09468</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning (RL) agents need to explore their environments in
order to learn optimal policies. In many environments and tasks, safety is of
critical importance. The widespread use of simulators offers a number of
advantages, including safe exploration which will be inevitable in cases when
RL systems need to be trained directly in the physical environment (e.g. in
human-robot interaction). The popular Safety Gym library offers three mobile
agent types that can learn goal-directed tasks while considering various safety
constraints. In this paper, we extend the applicability of safe RL algorithms
by creating a customized environment with Panda robotic arm where Safety Gym
algorithms can be tested. We performed pilot experiments with the popular PPO
algorithm comparing the baseline with the constrained version and show that the
constrained version is able to learn the equally good policy while better
complying with safety constraints and taking longer training time as expected.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kovac_L/0/1/0/all/0/1&quot;&gt;Luka Kova&amp;#x10d;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farkas_I/0/1/0/all/0/1&quot;&gt;Igor Farka&amp;#x161;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09469">
<title>Clinical Text Deduplication Practices for Efficient Pretraining and Improved Clinical Tasks. (arXiv:2312.09469v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.09469</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite being a unique source of information on patients&apos; status and disease
progression, clinical notes are characterized by high levels of duplication and
information redundancy. In general domain text, it has been shown that
deduplication does not harm language model (LM) pretraining, thus helping
reduce the training cost. Although large LMs have proven to learn medical
knowledge, they still require specialized domain adaptation for improved
downstream clinical tasks. By leveraging large real-world clinical corpora, we
first provided a fine-grained characterization of duplicates stemming from
common writing practices and clinical relevancy. Second, we demonstrated that
deduplicating clinical text can help clinical LMs encode less redundant
information in a more efficient manner and do not harm classification tasks via
prompt-based learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Landi_I/0/1/0/all/0/1&quot;&gt;Isotta Landi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alleva_E/0/1/0/all/0/1&quot;&gt;Eugenia Alleva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valentine_A/0/1/0/all/0/1&quot;&gt;Alissa A. Valentine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lepow_L/0/1/0/all/0/1&quot;&gt;Lauren A. Lepow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Charney_A/0/1/0/all/0/1&quot;&gt;Alexander W. Charney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09478">
<title>Entropy Causal Graphs for Multivariate Time Series Anomaly Detection. (arXiv:2312.09478v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.09478</link>
<description rdf:parseType="Literal">&lt;p&gt;Many multivariate time series anomaly detection frameworks have been proposed
and widely applied. However, most of these frameworks do not consider intrinsic
relationships between variables in multivariate time series data, thus ignoring
the causal relationship among variables and degrading anomaly detection
performance. This work proposes a novel framework called CGAD, an entropy
Causal Graph for multivariate time series Anomaly Detection. CGAD utilizes
transfer entropy to construct graph structures that unveil the underlying
causal relationships among time series data. Weighted graph convolutional
networks combined with causal convolutions are employed to model both the
causal graph structures and the temporal patterns within multivariate time
series data. Furthermore, CGAD applies anomaly scoring, leveraging median
absolute deviation-based normalization to improve the robustness of the anomaly
identification process. Extensive experiments demonstrate that CGAD outperforms
state-of-the-art methods on real-world datasets with a 15% average improvement
based on three different multivariate time series anomaly detection metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Febrinanto_F/0/1/0/all/0/1&quot;&gt;Falih Gozi Febrinanto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moore_K/0/1/0/all/0/1&quot;&gt;Kristen Moore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thapa_C/0/1/0/all/0/1&quot;&gt;Chandra Thapa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mujie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saikrishna_V/0/1/0/all/0/1&quot;&gt;Vidya Saikrishna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jiangang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1&quot;&gt;Feng Xia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09498">
<title>Neural Gaussian Similarity Modeling for Differential Graph Structure Learning. (arXiv:2312.09498v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.09498</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Structure Learning (GSL) has demonstrated considerable potential in the
analysis of graph-unknown non-Euclidean data across a wide range of domains.
However, constructing an end-to-end graph structure learning model poses a
challenge due to the impediment of gradient flow caused by the nearest neighbor
sampling strategy. In this paper, we construct a differential graph structure
learning model by replacing the non-differentiable nearest neighbor sampling
with a differentiable sampling using the reparameterization trick. Under this
framework, we argue that the act of sampling \mbox{nearest} neighbors may not
invariably be essential, particularly in instances where node features exhibit
a significant degree of similarity. To alleviate this issue, the bell-shaped
Gaussian Similarity (GauSim) modeling is proposed to sample non-nearest
neighbors. To adaptively model the similarity, we further propose Neural
Gaussian Similarity (NeuralGauSim) with learnable parameters featuring flexible
sampling behaviors. In addition, we develop a scalable method by transferring
the large-scale graph to the transition graph to significantly reduce the
complexity. Experimental results demonstrate the effectiveness of the proposed
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1&quot;&gt;Xiaolong Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1&quot;&gt;Maoguo Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yue Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1&quot;&gt;Zedong Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jieyi Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09501">
<title>EDA: Evolving and Distinct Anchors for Multimodal Motion Prediction. (arXiv:2312.09501v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09501</link>
<description rdf:parseType="Literal">&lt;p&gt;Motion prediction is a crucial task in autonomous driving, and one of its
major challenges lands in the multimodality of future behaviors. Many
successful works have utilized mixture models which require identification of
positive mixture components, and correspondingly fall into two main lines:
prediction-based and anchor-based matching. The prediction clustering
phenomenon in prediction-based matching makes it difficult to pick
representative trajectories for downstream tasks, while the anchor-based
matching suffers from a limited regression capability. In this paper, we
introduce a novel paradigm, named Evolving and Distinct Anchors (EDA), to
define the positive and negative components for multimodal motion prediction
based on mixture models. We enable anchors to evolve and redistribute
themselves under specific scenes for an enlarged regression capacity.
Furthermore, we select distinct anchors before matching them with the ground
truth, which results in impressive scoring performance. Our approach enhances
all metrics compared to the baseline MTR, particularly with a notable relative
reduction of 13.5% in Miss Rate, resulting in state-of-the-art performance on
the Waymo Open Motion Dataset. Code is available at
https://github.com/Longzhong-Lin/EDA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Longzhong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xuewu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1&quot;&gt;Tianwei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Lichao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1&quot;&gt;Rong Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yue Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09507">
<title>WAVER: Writing-style Agnostic Video Retrieval via Distilling Vision-Language Models Through Open-Vocabulary Knowledge. (arXiv:2312.09507v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09507</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-video retrieval, a prominent sub-field within the broader domain of
multimedia content management, has witnessed remarkable growth and innovation
over the past decade. However, existing methods assume the video scenes are
consistent and the description annotators are unbiased. These limitations fail
to align with fluid real-world scenarios, and descriptions can be influenced by
annotator biases, diverse writing styles, and varying textual perspectives. To
overcome the aforementioned problems, we introduce WAVER, a cross-domain
knowledge distillation mechanism designed to tackle the challenge of handling
writing-style agnostics. WAVER capitalizes on the open-vocabulary properties
inherent in pre-trained vision-language models and employs an implicit
knowledge distillation approach to transfer text-based knowledge from a teacher
model to a vision-based student. Empirical studies conducted across four
standard benchmark datasets, encompassing various settings, provide compelling
evidence that \WAVER can achieve state-of-the-art performance in text-video
retrieval tasks while handling writing-style variations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1&quot;&gt;Huy Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kieu_T/0/1/0/all/0/1&quot;&gt;Tung Kieu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1&quot;&gt;Anh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1&quot;&gt;Ngan Le&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09510">
<title>Fast Sampling generative model for Ultrasound image reconstruction. (arXiv:2312.09510v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09510</link>
<description rdf:parseType="Literal">&lt;p&gt;Image reconstruction from radio-frequency data is pivotal in ultrafast plane
wave ultrasound imaging. Unlike the conventional delay-and-sum (DAS) technique,
which relies on somewhat imprecise assumptions, deep learning-based methods
perform image reconstruction by training on paired data, leading to a notable
enhancement in image quality. Nevertheless, these strategies often exhibit
limited generalization capabilities. Recently, denoising diffusion models have
become the preferred paradigm for image reconstruction tasks. However, their
reliance on an iterative sampling procedure results in prolonged generation
time. In this paper, we propose a novel sampling framework that concurrently
enforces data consistency of ultrasound signals and data-driven priors. By
leveraging the advanced diffusion model, the generation of high-quality images
is substantially expedited. Experimental evaluations on an in-vivo dataset
indicate that our approach with a single plane wave surpasses DAS with spatial
coherent compounding of 75 plane waves.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_H/0/1/0/all/0/1&quot;&gt;Hengrong Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1&quot;&gt;Qiong He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Jianwen Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09511">
<title>MONET: Modality-Embracing Graph Convolutional Network and Target-Aware Attention for Multimedia Recommendation. (arXiv:2312.09511v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2312.09511</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we focus on multimedia recommender systems using graph
convolutional networks (GCNs) where the multimodal features as well as
user-item interactions are employed together. Our study aims to exploit
multimodal features more effectively in order to accurately capture users&apos;
preferences for items. To this end, we point out following two limitations of
existing GCN-based multimedia recommender systems: (L1) although multimodal
features of interacted items by a user can reveal her preferences on items,
existing methods utilize GCN designed to focus only on capturing collaborative
signals, resulting in insufficient reflection of the multimodal features in the
final user/item embeddings; (L2) although a user decides whether to prefer the
target item by considering its multimodal features, existing methods represent
her as only a single embedding regardless of the target item&apos;s multimodal
features and then utilize her embedding to predict her preference for the
target item. To address the above issues, we propose a novel multimedia
recommender system, named MONET, composed of following two core ideas:
modality-embracing GCN (MeGCN) and target-aware attention. Through extensive
experiments using four real-world datasets, we demonstrate i) the significant
superiority of MONET over seven state-of-the-art competitors (up to 30.32%
higher accuracy in terms of recall@20, compared to the best competitor) and ii)
the effectiveness of the two core ideas in MONET. All MONET codes are available
at https://github.com/Kimyungi/MONET.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yungi Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Taeri Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_W/0/1/0/all/0/1&quot;&gt;Won-Yong Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sang-Wook Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09513">
<title>CGS-Mask: Making Time Series Predictions Intuitive for Al. (arXiv:2312.09513v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.09513</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence (AI) has immense potential in time series prediction,
but most explainable tools have limited capabilities in providing a systematic
understanding of important features over time. These tools typically rely on
evaluating a single time point, overlook the time ordering of inputs, and
neglect the time-sensitive nature of time series applications. These factors
make it difficult for users, particularly those without domain knowledge, to
comprehend AI model decisions and obtain meaningful explanations. We propose
CGS-Mask, a post-hoc and model-agnostic cellular genetic strip mask-based
saliency approach to address these challenges. CGS-Mask uses consecutive time
steps as a cohesive entity to evaluate the impact of features on the final
prediction, providing binary and sustained feature importance scores over time.
Our algorithm optimizes the mask population iteratively to obtain the optimal
mask in a reasonable time. We evaluated CGS-Mask on synthetic and real-world
datasets, and it outperformed state-of-the-art methods in elucidating the
importance of features over time. According to our pilot user study via a
questionnaire survey, CGS-Mask is the most effective approach in presenting
easily understandable time series prediction results, enabling users to
comprehend the decision-making process of AI models with ease.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_F/0/1/0/all/0/1&quot;&gt;Feng Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yifei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1&quot;&gt;Cheng Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1&quot;&gt;Yufei Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zomaya_A/0/1/0/all/0/1&quot;&gt;Albert Y. Zomaya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09532">
<title>Grounding for Artificial Intelligence. (arXiv:2312.09532v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.09532</link>
<description rdf:parseType="Literal">&lt;p&gt;A core function of intelligence is grounding, which is the process of
connecting the natural language and abstract knowledge to the internal
representation of the real world in an intelligent being, e.g., a human. Human
cognition is grounded in our sensorimotor experiences in the external world and
subjective feelings in our internal world. We use languages to communicate with
each other and the languages are grounded on our shared sensorimotor
experiences and feelings. Without this shard grounding, it is impossible for us
to understand each other because all natural languages are highly abstract and
are only able to describe a tiny portion of what has happened or is happening
in the real world. Although grounding at high or abstract levels has been
studied in different fields and applications, to our knowledge, limited
systematic work at fine-grained levels has been done. With the rapid progress
of large language models (LLMs), it is imperative that we have a sound
understanding of grounding in order to move to the next level of intelligence.
It is also believed that grounding is necessary for Artificial General
Intelligence (AGI). This paper makes an attempt to systematically study this
problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bing Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09539">
<title>Situation-Dependent Causal Influence-Based Cooperative Multi-agent Reinforcement Learning. (arXiv:2312.09539v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.09539</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning to collaborate has witnessed significant progress in multi-agent
reinforcement learning (MARL). However, promoting coordination among agents and
enhancing exploration capabilities remain challenges. In multi-agent
environments, interactions between agents are limited in specific situations.
Effective collaboration between agents thus requires a nuanced understanding of
when and how agents&apos; actions influence others. To this end, in this paper, we
propose a novel MARL algorithm named Situation-Dependent Causal Influence-Based
Cooperative Multi-agent Reinforcement Learning (SCIC), which incorporates a
novel Intrinsic reward mechanism based on a new cooperation criterion measured
by situation-dependent causal influence among agents. Our approach aims to
detect inter-agent causal influences in specific situations based on the
criterion using causal intervention and conditional mutual information. This
effectively assists agents in exploring states that can positively impact other
agents, thus promoting cooperation between agents. The resulting update links
coordinated exploration and intrinsic reward distribution, which enhance
overall collaboration and performance. Experimental results on various MARL
benchmarks demonstrate the superiority of our method compared to
state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1&quot;&gt;Xiao Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1&quot;&gt;Yutong Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Pengyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yaning Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mingsong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Ting Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09545">
<title>GPT-4 Surpassing Human Performance in Linguistic Pragmatics. (arXiv:2312.09545v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.09545</link>
<description rdf:parseType="Literal">&lt;p&gt;As Large Language Models (LLMs) become increasingly integrated into everyday
life, their capabilities to understand and emulate human cognition are under
steady examination. This study investigates the ability of LLMs to comprehend
and interpret linguistic pragmatics, an aspect of communication that considers
context and implied meanings. Using Grice&apos;s communication principles, LLMs and
human subjects (N=76) were evaluated based on their responses to various
dialogue-based tasks. The findings revealed the superior performance and speed
of LLMs, particularly GPT4, over human subjects in interpreting pragmatics.
GPT4 also demonstrated accuracy in the pre-testing of human-written samples,
indicating its potential in text analysis. In a comparative analysis of LLMs
using human individual and average scores, the models exhibited significant
chronological improvement. The models were ranked from lowest to highest score,
with GPT2 positioned at 78th place, GPT3 ranking at 23rd, Bard at 10th, GPT3.5
placing 5th, Best Human scoring 2nd, and GPT4 achieving the top spot. The
findings highlight the remarkable progress made in the development and
performance of these LLMs. Future studies should consider diverse subjects,
multiple languages, and other cognitive aspects to fully comprehend the
capabilities of LLMs. This research holds significant implications for the
development and application of AI-based models in communication-centered
sectors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bojic_L/0/1/0/all/0/1&quot;&gt;Ljubisa Bojic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kovacevic_P/0/1/0/all/0/1&quot;&gt;Predrag Kovacevic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cabarkapa_M/0/1/0/all/0/1&quot;&gt;Milan Cabarkapa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09546">
<title>On a Functional Definition of Intelligence. (arXiv:2312.09546v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.09546</link>
<description rdf:parseType="Literal">&lt;p&gt;Without an agreed-upon definition of intelligence, asking &quot;is this system
intelligent?&quot;&quot; is an untestable question. This lack of consensus hinders
research, and public perception, on Artificial Intelligence (AI), particularly
since the rise of generative- and large-language models. Most work on precisely
capturing what we mean by &quot;intelligence&quot; has come from the fields of
philosophy, psychology, and cognitive science. Because these perspectives are
intrinsically linked to intelligence as it is demonstrated by natural
creatures, we argue such fields cannot, and will not, provide a sufficiently
rigorous definition that can be applied to artificial means. Thus, we present
an argument for a purely functional, black-box definition of intelligence,
distinct from how that intelligence is actually achieved; focusing on the
&quot;what&quot;, rather than the &quot;how&quot;. To achieve this, we first distinguish other
related concepts (sentience, sensation, agency, etc.) from the notion of
intelligence, particularly identifying how these concepts pertain to artificial
intelligent systems. As a result, we achieve a formal definition of
intelligence that is conceptually testable from only external observation, that
suggests intelligence is a continuous variable. We conclude by identifying
challenges that still remain towards quantifiable measurement. This work
provides a useful perspective for both the development of AI, and for public
perception of the capabilities and risks of AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sritriratanarak_W/0/1/0/all/0/1&quot;&gt;Warisa Sritriratanarak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_P/0/1/0/all/0/1&quot;&gt;Paulo Garcia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09548">
<title>Integrating AI and Learning Analytics for Data-Driven Pedagogical Decisions and Personalized Interventions in Education. (arXiv:2312.09548v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2312.09548</link>
<description rdf:parseType="Literal">&lt;p&gt;This research study delves into the conceptualization, development, and
deployment of an innovative learning analytics tool, leveraging the
capabilities of OpenAI&apos;s GPT-4 model. This tool is designed to quantify student
engagement, map learning progression, and evaluate the efficacy of diverse
instructional strategies within an educational context. Through the analysis of
various critical data points such as students&apos; stress levels, curiosity,
confusion, agitation, topic preferences, and study methods, the tool offers a
rich, multi-dimensional view of the learning environment. Furthermore, it
employs Bloom&apos;s taxonomy as a framework to gauge the cognitive levels addressed
by students&apos; questions, thereby elucidating their learning progression. The
information gathered from these measurements can empower educators by providing
valuable insights to enhance teaching methodologies, pinpoint potential areas
for improvement, and craft personalized interventions for individual students.
The study articulates the design intricacies, implementation strategy, and
thorough evaluation of the learning analytics tool, underscoring its
prospective contributions to enhancing educational outcomes and bolstering
student success. Moreover, the practicalities of integrating the tool within
existing educational platforms and the requisite robust, secure, and scalable
technical infrastructure are addressed. This research opens avenues for
harnessing AI&apos;s potential in shaping the future of education, facilitating
data-driven pedagogical decisions, and ultimately fostering a more conducive,
personalized learning environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sajja_R/0/1/0/all/0/1&quot;&gt;Ramteja Sajja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sermet_Y/0/1/0/all/0/1&quot;&gt;Yusuf Sermet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cwiertny_D/0/1/0/all/0/1&quot;&gt;David Cwiertny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demir_I/0/1/0/all/0/1&quot;&gt;Ibrahim Demir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09561">
<title>Investigating Responsible AI for Scientific Research: An Empirical Study. (arXiv:2312.09561v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.09561</link>
<description rdf:parseType="Literal">&lt;p&gt;Scientific research organizations that are developing and deploying
Artificial Intelligence (AI) systems are at the intersection of technological
progress and ethical considerations. The push for Responsible AI (RAI) in such
institutions underscores the increasing emphasis on integrating ethical
considerations within AI design and development, championing core values like
fairness, accountability, and transparency. For scientific research
organizations, prioritizing these practices is paramount not just for
mitigating biases and ensuring inclusivity, but also for fostering trust in AI
systems among both users and broader stakeholders. In this paper, we explore
the practices at a research organization concerning RAI practices, aiming to
assess the awareness and preparedness regarding the ethical risks inherent in
AI design and development. We have adopted a mixed-method research approach,
utilising a comprehensive survey combined with follow-up in-depth interviews
with selected participants from AI-related projects. Our results have revealed
certain knowledge gaps concerning ethical, responsible, and inclusive AI, with
limitations in awareness of the available AI ethics frameworks. This revealed
an overarching underestimation of the ethical risks that AI technologies can
present, especially when implemented without proper guidelines and governance.
Our findings reveal the need for a holistic and multi-tiered strategy to uplift
capabilities and better support science research teams for responsible,
ethical, and inclusive AI development and deployment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bano_M/0/1/0/all/0/1&quot;&gt;Muneera Bano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zowghi_D/0/1/0/all/0/1&quot;&gt;Didar Zowghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shea_P/0/1/0/all/0/1&quot;&gt;Pip Shea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ibarra_G/0/1/0/all/0/1&quot;&gt;Georgina Ibarra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09579">
<title>MobileSAMv2: Faster Segment Anything to Everything. (arXiv:2312.09579v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09579</link>
<description rdf:parseType="Literal">&lt;p&gt;Segment anything model (SAM) addresses two practical yet challenging
segmentation tasks: \textbf{segment anything (SegAny)}, which utilizes a
certain point to predict the mask for a single object of interest, and
\textbf{segment everything (SegEvery)}, which predicts the masks for all
objects on the image. What makes SegAny slow for SAM is its heavyweight image
encoder, which has been addressed by MobileSAM via decoupled knowledge
distillation. The efficiency bottleneck of SegEvery with SAM, however, lies in
its mask decoder because it needs to first generate numerous masks with
redundant grid-search prompts and then perform filtering to obtain the final
valid masks. We propose to improve its efficiency by directly generating the
final masks with only valid prompts, which can be obtained through object
discovery. Our proposed approach not only helps reduce the total time on the
mask decoder by at least 16 times but also achieves superior performance.
Specifically, our approach yields an average performance boost of 3.6\% (42.5\%
\textit{v.s.} 38.9\%) for zero-shot object proposal on the LVIS dataset with
the mask AR@$K$ metric. Qualitative results show that our approach generates
fine-grained masks while avoiding over-segmenting things. This project
targeting faster SegEvery than the original SAM is termed MobileSAMv2 to
differentiate from MobileSAM which targets faster SegAny. Moreover, we
demonstrate that our new prompt sampling is also compatible with the distilled
image encoders in MobileSAM, contributing to a unified framework for efficient
SegAny and SegEvery. The code is available at the same link as MobileSAM
Project
\href{https://github.com/ChaoningZhang/MobileSAM}{\textcolor{red}{https://github.com/ChaoningZhang/MobileSAM}}.
\end{abstract}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chaoning Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1&quot;&gt;Dongshen Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Sheng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jinwoo Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Tae-Ho Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1&quot;&gt;Choong Seon Hong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09584">
<title>Multiscale Vision Transformer With Deep Clustering-Guided Refinement for Weakly Supervised Object Localization. (arXiv:2312.09584v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09584</link>
<description rdf:parseType="Literal">&lt;p&gt;This work addresses the task of weakly-supervised object localization. The
goal is to learn object localization using only image-level class labels, which
are much easier to obtain compared to bounding box annotations. This task is
important because it reduces the need for labor-intensive ground-truth
annotations. However, methods for object localization trained using weak
supervision often suffer from limited accuracy in localization. To address this
challenge and enhance localization accuracy, we propose a multiscale object
localization transformer (MOLT). It comprises multiple object localization
transformers that extract patch embeddings across various scales. Moreover, we
introduce a deep clustering-guided refinement method that further enhances
localization accuracy by utilizing separately extracted image segments. These
segments are obtained by clustering pixels using convolutional neural networks.
Finally, we demonstrate the effectiveness of our proposed method by conducting
experiments on the publicly available ILSVRC-2012 dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;David Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1&quot;&gt;Sinhae Cha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1&quot;&gt;Byeongkeun Kang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09585">
<title>Joint State Estimation and Noise Identification Based on Variational Optimization. (arXiv:2312.09585v1 [eess.SY])</title>
<link>http://arxiv.org/abs/2312.09585</link>
<description rdf:parseType="Literal">&lt;p&gt;In this article, the state estimation problems with unknown process noise and
measurement noise covariances for both linear and nonlinear systems are
considered. By formulating the joint estimation of system state and noise
parameters into an optimization problem, a novel adaptive Kalman filter method
based on conjugate-computation variational inference, referred to as CVIAKF, is
proposed to approximate the joint posterior probability density function of the
latent variables. Unlike the existing adaptive Kalman filter methods utilizing
variational inference in natural-parameter space, CVIAKF performs optimization
in expectation-parameter space, resulting in a faster and simpler solution.
Meanwhile, CVIAKF divides optimization objectives into conjugate and
non-conjugate parts of nonlinear dynamical models, whereas conjugate
computations and stochastic mirror-descent are applied, respectively.
Remarkably, the reparameterization trick is used to reduce the variance of
stochastic gradients of the non-conjugate parts. The effectiveness of CVIAKF is
validated through synthetic and real-world datasets of maneuvering target
tracking.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lan_H/0/1/0/all/0/1&quot;&gt;Hua Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shijie Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jinjie Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zengfu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jing Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09588">
<title>NeuroFlow: Development of lightweight and efficient model integration scheduling strategy for autonomous driving system. (arXiv:2312.09588v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.09588</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a specialized autonomous driving system that takes into
account the unique constraints and characteristics of automotive systems,
aiming for innovative advancements in autonomous driving technology. The
proposed system systematically analyzes the intricate data flow in autonomous
driving and provides functionality to dynamically adjust various factors that
influence deep learning models. Additionally, for algorithms that do not rely
on deep learning models, the system analyzes the flow to determine resource
allocation priorities. In essence, the system optimizes data flow and schedules
efficiently to ensure real-time performance and safety. The proposed system was
implemented in actual autonomous vehicles and experimentally validated across
various driving scenarios. The experimental results provide evidence of the
system&apos;s stable inference and effective control of autonomous vehicles, marking
a significant turning point in the development of autonomous driving systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_E/0/1/0/all/0/1&quot;&gt;Eunbin Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_G/0/1/0/all/0/1&quot;&gt;Gwanjun Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1&quot;&gt;Eunho Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09613">
<title>Rethinking Causal Relationships Learning in Graph Neural Networks. (arXiv:2312.09613v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.09613</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) demonstrate their significance by effectively
modeling complex interrelationships within graph-structured data. To enhance
the credibility and robustness of GNNs, it becomes exceptionally crucial to
bolster their ability to capture causal relationships. However, despite recent
advancements that have indeed strengthened GNNs from a causal learning
perspective, conducting an in-depth analysis specifically targeting the causal
modeling prowess of GNNs remains an unresolved issue. In order to
comprehensively analyze various GNN models from a causal learning perspective,
we constructed an artificially synthesized dataset with known and controllable
causal relationships between data and labels. The rationality of the generated
data is further ensured through theoretical foundations. Drawing insights from
analyses conducted using our dataset, we introduce a lightweight and highly
adaptable GNN module designed to strengthen GNNs&apos; causal learning capabilities
across a diverse range of tasks. Through a series of experiments conducted on
both synthetic datasets and other real-world datasets, we empirically validate
the effectiveness of the proposed module.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1&quot;&gt;Hang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_C/0/1/0/all/0/1&quot;&gt;Chengyu Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiangmeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1&quot;&gt;Lingyu Si&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yifan Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Fengge Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Changwen Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huaping Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09630">
<title>Pixel-Superpixel Contrastive Learning and Pseudo-Label Correction for Hyperspectral Image Clustering. (arXiv:2312.09630v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09630</link>
<description rdf:parseType="Literal">&lt;p&gt;Hyperspectral image (HSI) clustering is gaining considerable attention owing
to recent methods that overcome the inefficiency and misleading results from
the absence of supervised information. Contrastive learning methods excel at
existing pixel level and super pixel level HSI clustering tasks. The
pixel-level contrastive learning method can effectively improve the ability of
the model to capture fine features of HSI but requires a large time overhead.
The super pixel-level contrastive learning method utilizes the homogeneity of
HSI and reduces computing resources; however, it yields rough classification
results. To exploit the strengths of both methods, we present a pixel super
pixel contrastive learning and pseudo-label correction (PSCPC) method for the
HSI clustering. PSCPC can reasonably capture domain-specific and fine-grained
features through super pixels and the comparative learning of a small number of
pixels within the super pixels. To improve the clustering performance of super
pixels, this paper proposes a pseudo-label correction module that aligns the
clustering pseudo-labels of pixels and super-pixels. In addition, pixel-level
clustering results are used to supervise super pixel-level clustering,
improving the generalization ability of the model. Extensive experiments
demonstrate the effectiveness and efficiency of PSCPC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_R/0/1/0/all/0/1&quot;&gt;Renxiang Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zihao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xianju Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1&quot;&gt;Chang Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09639">
<title>Multiple Instance Learning for Uplift Modeling. (arXiv:2312.09639v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.09639</link>
<description rdf:parseType="Literal">&lt;p&gt;Uplift modeling is widely used in performance marketing to estimate effects
of promotion campaigns (e.g., increase of customer retention rate). Since it is
impossible to observe outcomes of a recipient in treatment (e.g., receiving a
certain promotion) and control (e.g., without promotion) groups simultaneously
(i.e., counter-factual), uplift models are mainly trained on instances of
treatment and control groups separately to form two models respectively, and
uplifts are predicted by the difference of predictions from these two models
(i.e., two-model method). When responses are noisy and the treatment effect is
fractional, induced individual uplift predictions will be inaccurate, resulting
in targeting undesirable customers. Though it is impossible to obtain the ideal
ground-truth individual uplifts, known as Individual Treatment Effects (ITEs),
alternatively, an average uplift of a group of users, called Average Treatment
Effect (ATE), can be observed from experimental deliveries. Upon this, similar
to Multiple Instance Learning (MIL) in which each training sample is a bag of
instances, our framework sums up individual user uplift predictions for each
bag of users as its bag-wise ATE prediction, and regularizes it to its ATE
label, thus learning more accurate individual uplifts. Additionally, to amplify
the fractional treatment effect, bags are composed of instances with adjacent
individual uplift predictions, instead of random instances. Experiments
conducted on two datasets show the effectiveness and universality of the
proposed framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haipeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1&quot;&gt;Shiwei Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1&quot;&gt;Ruiying Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jinjie Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guannan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09645">
<title>Fine-Tuned Self-Supervised Speech Representations for Language Diarization in Multilingual Code-Switched Speech. (arXiv:2312.09645v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2312.09645</link>
<description rdf:parseType="Literal">&lt;p&gt;Annotating a multilingual code-switched corpus is a painstaking process
requiring specialist linguistic expertise. This is partly due to the large
number of language combinations that may appear within and across utterances,
which might require several annotators with different linguistic expertise to
consider an utterance sequentially. This is time-consuming and costly. It would
be useful if the spoken languages in an utterance and the boundaries thereof
were known before annotation commences, to allow segments to be assigned to the
relevant language experts in parallel. To address this, we investigate the
development of a continuous multilingual language diarizer using fine-tuned
speech representations extracted from a large pre-trained self-supervised
architecture (WavLM). We experiment with a code-switched corpus consisting of
five South African languages (isiZulu, isiXhosa, Setswana, Sesotho and English)
and show substantial diarization error rate improvements for language families,
language groups, and individual languages over baseline systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Frost_G/0/1/0/all/0/1&quot;&gt;Geoffrey Frost&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Morris_E/0/1/0/all/0/1&quot;&gt;Emily Morris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vuren_J/0/1/0/all/0/1&quot;&gt;Joshua Jansen van V&amp;#xfc;ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Niesler_T/0/1/0/all/0/1&quot;&gt;Thomas Niesler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09646">
<title>Exact Algorithms and Lowerbounds for Multiagent Pathfinding: Power of Treelike Topology. (arXiv:2312.09646v1 [cs.CC])</title>
<link>http://arxiv.org/abs/2312.09646</link>
<description rdf:parseType="Literal">&lt;p&gt;In the Multiagent Path Finding problem (MAPF for short), we focus on
efficiently finding non-colliding paths for a set of $k$ agents on a given
graph $G$, where each agent seeks a path from its source vertex to a target. An
important measure of the quality of the solution is the length of the proposed
schedule $\ell$, that is, the length of a longest path (including the waiting
time). In this work, we propose a systematic study under the parameterized
complexity framework. The hardness results we provide align with many
heuristics used for this problem, whose running time could potentially be
improved based on our fixed-parameter tractability results.
&lt;/p&gt;
&lt;p&gt;We show that MAPF is W[1]-hard with respect to $k$ (even if $k$ is combined
with the maximum degree of the input graph). The problem remains NP-hard in
planar graphs even if the maximum degree and the makespan$\ell$ are fixed
constants. On the positive side, we show an FPT algorithm for $k+\ell$.
&lt;/p&gt;
&lt;p&gt;As we delve further, the structure of~$G$ comes into play. We give an FPT
algorithm for parameter $k$ plus the diameter of the graph~$G$. The MAPF
problem is W[1]-hard for cliquewidth of $G$ plus $\ell$ while it is FPT for
treewidth of $G$ plus $\ell$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fioravantes_F/0/1/0/all/0/1&quot;&gt;Foivos Fioravantes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knop_D/0/1/0/all/0/1&quot;&gt;Du&amp;#x161;an Knop&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kristan_J/0/1/0/all/0/1&quot;&gt;Jan Maty&amp;#xe1;&amp;#x161; K&amp;#x159;i&amp;#x161;&amp;#x165;an&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melissinos_N/0/1/0/all/0/1&quot;&gt;Nikolaos Melissinos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Opler_M/0/1/0/all/0/1&quot;&gt;Michal Opler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09658">
<title>Algorithms for automatic intents extraction and utterances classification for goal-oriented dialogue systems. (arXiv:2312.09658v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.09658</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern machine learning techniques in the natural language processing domain
can be used to automatically generate scripts for goal-oriented dialogue
systems. The current article presents a general framework for studying the
automatic generation of scripts for goal-oriented dialogue systems. A method
for preprocessing dialog data sets in JSON format is described. A comparison is
made of two methods for extracting user intent based on BERTopic and latent
Dirichlet allocation. A comparison has been made of two implemented algorithms
for classifying statements of users of a goal-oriented dialogue system based on
logistic regression and BERT transformer models. The BERT transformer approach
using the bert-base-uncased model showed better results for the three metrics
Precision (0.80), F1-score (0.78) and Matthews correlation coefficient (0.74)
in comparison with other methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Legashev_L/0/1/0/all/0/1&quot;&gt;Leonid Legashev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shukhman_A/0/1/0/all/0/1&quot;&gt;Alexander Shukhman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhigalov_A/0/1/0/all/0/1&quot;&gt;Arthur Zhigalov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09665">
<title>FlowMur: A Stealthy and Practical Audio Backdoor Attack with Limited Knowledge. (arXiv:2312.09665v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.09665</link>
<description rdf:parseType="Literal">&lt;p&gt;Speech recognition systems driven by DNNs have revolutionized human-computer
interaction through voice interfaces, which significantly facilitate our daily
lives. However, the growing popularity of these systems also raises special
concerns on their security, particularly regarding backdoor attacks. A backdoor
attack inserts one or more hidden backdoors into a DNN model during its
training process, such that it does not affect the model&apos;s performance on
benign inputs, but forces the model to produce an adversary-desired output if a
specific trigger is present in the model input. Despite the initial success of
current audio backdoor attacks, they suffer from the following limitations: (i)
Most of them require sufficient knowledge, which limits their widespread
adoption. (ii) They are not stealthy enough, thus easy to be detected by
humans. (iii) Most of them cannot attack live speech, reducing their
practicality. To address these problems, in this paper, we propose FlowMur, a
stealthy and practical audio backdoor attack that can be launched with limited
knowledge. FlowMur constructs an auxiliary dataset and a surrogate model to
augment adversary knowledge. To achieve dynamicity, it formulates trigger
generation as an optimization problem and optimizes the trigger over different
attachment positions. To enhance stealthiness, we propose an adaptive data
poisoning method according to Signal-to-Noise Ratio (SNR). Furthermore, ambient
noise is incorporated into the process of trigger generation and data poisoning
to make FlowMur robust to ambient noise and improve its practicality. Extensive
experiments conducted on two datasets demonstrate that FlowMur achieves high
attack performance in both digital and physical settings while remaining
resilient to state-of-the-art defenses. In particular, a human study confirms
that triggers generated by FlowMur are not easily detected by participants.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_J/0/1/0/all/0/1&quot;&gt;Jiahe Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1&quot;&gt;Baochen Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zheng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertino_E/0/1/0/all/0/1&quot;&gt;Elisa Bertino&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09672">
<title>InstructPipe: Building Visual Programming Pipelines with Human Instructions. (arXiv:2312.09672v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2312.09672</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual programming provides beginner-level programmers with a coding-free
experience to build their customized pipelines. Existing systems require users
to build a pipeline entirely from scratch, implying that novice users need to
set up and link appropriate nodes all by themselves, starting from a blank
workspace. We present InstructPipe, an AI assistant that enables users to start
prototyping machine learning (ML) pipelines with text instructions. We designed
two LLM modules and a code interpreter to execute our solution. LLM modules
generate pseudocode of a target pipeline, and the interpreter renders a
pipeline in the node-graph editor for further human-AI collaboration. Technical
evaluations reveal that InstructPipe reduces user interactions by 81.1%
compared to traditional methods. Our user study (N=16) showed that InstructPipe
empowers novice users to streamline their workflow in creating desired ML
pipelines, reduce their learning curve, and spark innovative ideas with
open-ended commands.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhongyi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1&quot;&gt;Jing Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phadnis_V/0/1/0/all/0/1&quot;&gt;Vrushank Phadnis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xiuxiu Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jun Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1&quot;&gt;Xun Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingtao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yiyi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yinda Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wright_K/0/1/0/all/0/1&quot;&gt;Kristen Wright&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mayes_J/0/1/0/all/0/1&quot;&gt;Jason Mayes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sherwood_M/0/1/0/all/0/1&quot;&gt;Mark Sherwood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Johnny Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olwal_A/0/1/0/all/0/1&quot;&gt;Alex Olwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;David Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iyengar_R/0/1/0/all/0/1&quot;&gt;Ram Iyengar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1&quot;&gt;Na Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_R/0/1/0/all/0/1&quot;&gt;Ruofei Du&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09676">
<title>nuScenes Knowledge Graph -- A comprehensive semantic representation of traffic scenes for trajectory prediction. (arXiv:2312.09676v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09676</link>
<description rdf:parseType="Literal">&lt;p&gt;Trajectory prediction in traffic scenes involves accurately forecasting the
behaviour of surrounding vehicles. To achieve this objective it is crucial to
consider contextual information, including the driving path of vehicles, road
topology, lane dividers, and traffic rules. Although studies demonstrated the
potential of leveraging heterogeneous context for improving trajectory
prediction, state-of-the-art deep learning approaches still rely on a limited
subset of this information. This is mainly due to the limited availability of
comprehensive representations. This paper presents an approach that utilizes
knowledge graphs to model the diverse entities and their semantic connections
within traffic scenes. Further, we present nuScenes Knowledge Graph (nSKG), a
knowledge graph for the nuScenes dataset, that models explicitly all scene
participants and road elements, as well as their semantic and spatial
relationships. To facilitate the usage of the nSKG via graph neural networks
for trajectory prediction, we provide the data in a format, ready-to-use by the
PyG library. All artefacts can be found here:
https://github.com/boschresearch/nuScenes_Knowledge_Graph
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mlodzian_L/0/1/0/all/0/1&quot;&gt;Leon Mlodzian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhigang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berkemeyer_H/0/1/0/all/0/1&quot;&gt;Hendrik Berkemeyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monka_S/0/1/0/all/0/1&quot;&gt;Sebastian Monka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zixu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dietze_S/0/1/0/all/0/1&quot;&gt;Stefan Dietze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Halilaj_L/0/1/0/all/0/1&quot;&gt;Lavdim Halilaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luettin_J/0/1/0/all/0/1&quot;&gt;Juergen Luettin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09691">
<title>Quilt: Robust Data Segment Selection against Concept Drifts. (arXiv:2312.09691v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.09691</link>
<description rdf:parseType="Literal">&lt;p&gt;Continuous machine learning pipelines are common in industrial settings where
models are periodically trained on data streams. Unfortunately, concept drifts
may occur in data streams where the joint distribution of the data X and label
y, P(X, y), changes over time and possibly degrade model accuracy. Existing
concept drift adaptation approaches mostly focus on updating the model to the
new data possibly using ensemble techniques of previous models and tend to
discard the drifted historical data. However, we contend that explicitly
utilizing the drifted data together leads to much better model accuracy and
propose Quilt, a data-centric framework for identifying and selecting data
segments that maximize model accuracy. To address the potential downside of
efficiency, Quilt extends existing data subset selection techniques, which can
be used to reduce the training data without compromising model accuracy. These
techniques cannot be used as is because they only assume virtual drifts where
the posterior probabilities P(y|X) are assumed not to change. In contrast, a
key challenge in our setup is to also discard undesirable data segments with
concept drifts. Quilt thus discards drifted data segments and selects data
segment subsets holistically for accurate and efficient model training. The two
operations use gradient-based scores, which have little computation overhead.
In our experiments, we show that Quilt outperforms state-of-the-art drift
adaptation and data selection baselines on synthetic and real datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Minsu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1&quot;&gt;Seong-Hyeon Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whang_S/0/1/0/all/0/1&quot;&gt;Steven Euijong Whang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09693">
<title>Prompting Large Language Models for Topic Modeling. (arXiv:2312.09693v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.09693</link>
<description rdf:parseType="Literal">&lt;p&gt;Topic modeling is a widely used technique for revealing underlying thematic
structures within textual data. However, existing models have certain
limitations, particularly when dealing with short text datasets that lack
co-occurring words. Moreover, these models often neglect sentence-level
semantics, focusing primarily on token-level semantics. In this paper, we
propose PromptTopic, a novel topic modeling approach that harnesses the
advanced language understanding of large language models (LLMs) to address
these challenges. It involves extracting topics at the sentence level from
individual documents, then aggregating and condensing these topics into a
predefined quantity, ultimately providing coherent topics for texts of varying
lengths. This approach eliminates the need for manual parameter tuning and
improves the quality of extracted topics. We benchmark PromptTopic against the
state-of-the-art baselines on three vastly diverse datasets, establishing its
proficiency in discovering meaningful topics. Furthermore, qualitative analysis
showcases PromptTopic&apos;s ability to uncover relevant topics in multiple
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Han Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prakash_N/0/1/0/all/0/1&quot;&gt;Nirmalendu Prakash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoang_N/0/1/0/all/0/1&quot;&gt;Nguyen Khoi Hoang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hee_M/0/1/0/all/0/1&quot;&gt;Ming Shan Hee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naseem_U/0/1/0/all/0/1&quot;&gt;Usman Naseem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1&quot;&gt;Roy Ka-Wei Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09695">
<title>Robustness Verification of Deep Reinforcement Learning Based Control Systems using Reward Martingales. (arXiv:2312.09695v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.09695</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Reinforcement Learning (DRL) has gained prominence as an effective
approach for control systems. However, its practical deployment is impeded by
state perturbations that can severely impact system performance. Addressing
this critical challenge requires robustness verification about system
performance, which involves tackling two quantitative questions: (i) how to
establish guaranteed bounds for expected cumulative rewards, and (ii) how to
determine tail bounds for cumulative rewards. In this work, we present the
first approach for robustness verification of DRL-based control systems by
introducing reward martingales, which offer a rigorous mathematical foundation
to characterize the impact of state perturbations on system performance in
terms of cumulative rewards. Our verified results provide provably quantitative
certificates for the two questions. We then show that reward martingales can be
implemented and trained via neural networks, against different types of control
policies. Experimental results demonstrate that our certified bounds tightly
enclose simulation outcomes on various DRL-based control systems, indicating
the effectiveness and generality of the proposed approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhi_D/0/1/0/all/0/1&quot;&gt;Dapeng Zhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peixin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Cheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Min Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09699">
<title>Social, Legal, Ethical, Empathetic, and Cultural Rules: Compilation and Reasoning (Extended Version). (arXiv:2312.09699v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.09699</link>
<description rdf:parseType="Literal">&lt;p&gt;The rise of AI-based and autonomous systems is raising concerns and
apprehension due to potential negative repercussions stemming from their
behavior or decisions. These systems must be designed to comply with the human
contexts in which they will operate. To this extent, Townsend et al. (2022)
introduce the concept of SLEEC (social, legal, ethical, empathetic, or
cultural) rules that aim to facilitate the formulation, verification, and
enforcement of the rules AI-based and autonomous systems should obey. They lay
out a methodology to elicit them and to let philosophers, lawyers, domain
experts, and others to formulate them in natural language. To enable their
effective use in AI systems, it is necessary to translate these rules
systematically into a formal language that supports automated reasoning. In
this study, we first conduct a linguistic analysis of the SLEEC rules pattern,
which justifies the translation of SLEEC rules into classical logic. Then we
investigate the computational complexity of reasoning about SLEEC rules and
show how logical programming frameworks can be employed to implement SLEEC
rules in practical scenarios. The result is a readily applicable strategy for
implementing AI systems that conform to norms expressed as SLEEC rules.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Troquard_N/0/1/0/all/0/1&quot;&gt;Nicolas Troquard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanctis_M/0/1/0/all/0/1&quot;&gt;Martina De Sanctis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Inverardi_P/0/1/0/all/0/1&quot;&gt;Paola Inverardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pelliccione_P/0/1/0/all/0/1&quot;&gt;Patrizio Pelliccione&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scoccia_G/0/1/0/all/0/1&quot;&gt;Gian Luca Scoccia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09703">
<title>Gradient Based Hybridization of PSO. (arXiv:2312.09703v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2312.09703</link>
<description rdf:parseType="Literal">&lt;p&gt;Particle Swarm Optimization (PSO) has emerged as a powerful metaheuristic
global optimization approach over the past three decades. Its appeal lies in
its ability to tackle complex multidimensional problems that defy conventional
algorithms. However, PSO faces challenges, such as premature stagnation in
single-objective scenarios and the need to strike a balance between exploration
and exploitation. Hybridizing PSO by integrating its cooperative nature with
established optimization techniques from diverse paradigms offers a promising
solution. In this paper, we investigate various strategies for synergizing
gradient-based optimizers with PSO. We introduce different hybridization
principles and explore several approaches, including sequential decoupled
hybridization, coupled hybridization, and adaptive hybridization. These
strategies aim to enhance the efficiency and effectiveness of PSO, ultimately
improving its ability to navigate intricate optimization landscapes. By
combining the strengths of gradient-based methods with the inherent social
dynamics of PSO, we seek to address the critical objectives of intelligent
exploration and exploitation in complex optimization tasks. Our study delves
into the comparative merits of these hybridization techniques and offers
insights into their application across different problem domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pujari_A/0/1/0/all/0/1&quot;&gt;Arun K Pujari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veeramachaneni_S/0/1/0/all/0/1&quot;&gt;Sowmini Devi Veeramachaneni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09708">
<title>GraphRARE: Reinforcement Learning Enhanced Graph Neural Network with Relative Entropy. (arXiv:2312.09708v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.09708</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph neural networks (GNNs) have shown advantages in graph-based analysis
tasks. However, most existing methods have the homogeneity assumption and show
poor performance on heterophilic graphs, where the linked nodes have dissimilar
features and different class labels, and the semantically related nodes might
be multi-hop away. To address this limitation, this paper presents GraphRARE, a
general framework built upon node relative entropy and deep reinforcement
learning, to strengthen the expressive capability of GNNs. An innovative node
relative entropy, which considers node features and structural similarity, is
used to measure mutual information between node pairs. In addition, to avoid
the sub-optimal solutions caused by mixing useful information and noises of
remote nodes, a deep reinforcement learning-based algorithm is developed to
optimize the graph topology. This algorithm selects informative nodes and
discards noisy nodes based on the defined node relative entropy. Extensive
experiments are conducted on seven real-world datasets. The experimental
results demonstrate the superiority of GraphRARE in node classification and its
capability to optimize the original graph topology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_T/0/1/0/all/0/1&quot;&gt;Tianhao Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wenjun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1&quot;&gt;Haitao Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_Z/0/1/0/all/0/1&quot;&gt;Zhifeng Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pengrui_Z/0/1/0/all/0/1&quot;&gt;Zhao Pengrui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xin Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xuetao Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yu Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pu_Y/0/1/0/all/0/1&quot;&gt;Yanjun Pu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09723">
<title>Tracking Skiers from the Top to the Bottom. (arXiv:2312.09723v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09723</link>
<description rdf:parseType="Literal">&lt;p&gt;Skiing is a popular winter sport discipline with a long history of
competitive events. In this domain, computer vision has the potential to
enhance the understanding of athletes&apos; performance, but its application lags
behind other sports due to limited studies and datasets. This paper makes a
step forward in filling such gaps. A thorough investigation is performed on the
task of skier tracking in a video capturing his/her complete performance.
Obtaining continuous and accurate skier localization is preemptive for further
higher-level performance analyses. To enable the study, the largest and most
annotated dataset for computer vision in skiing, SkiTB, is introduced. Several
visual object tracking algorithms, including both established methodologies and
a newly introduced skier-optimized baseline algorithm, are tested using the
dataset. The results provide valuable insights into the applicability of
different tracking methods for vision-based skiing analysis. SkiTB, code, and
results are available at https://machinelearning.uniud.it/datasets/skitb.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dunnhofer_M/0/1/0/all/0/1&quot;&gt;Matteo Dunnhofer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sordi_L/0/1/0/all/0/1&quot;&gt;Luca Sordi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinel_N/0/1/0/all/0/1&quot;&gt;Niki Martinel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Micheloni_C/0/1/0/all/0/1&quot;&gt;Christian Micheloni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09738">
<title>3DAxiesPrompts: Unleashing the 3D Spatial Task Capabilities of GPT-4V. (arXiv:2312.09738v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.09738</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we present a new visual prompting method called 3DAxiesPrompts
(3DAP) to unleash the capabilities of GPT-4V in performing 3D spatial tasks.
Our investigation reveals that while GPT-4V exhibits proficiency in discerning
the position and interrelations of 2D entities through current visual prompting
techniques, its abilities in handling 3D spatial tasks have yet to be explored.
In our approach, we create a 3D coordinate system tailored to 3D imagery,
complete with annotated scale information. By presenting images infused with
the 3DAP visual prompt as inputs, we empower GPT-4V to ascertain the spatial
positioning information of the given 3D target image with a high degree of
precision. Through experiments, We identified three tasks that could be stably
completed using the 3DAP method, namely, 2D to 3D Point Reconstruction, 2D to
3D point matching, and 3D Object Detection. We perform experiments on our
proposed dataset 3DAP-Data, the results from these experiments validate the
efficacy of 3DAP-enhanced GPT-4V inputs, marking a significant stride in 3D
spatial task execution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dingning Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1&quot;&gt;Xiaomeng Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Renrui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xu Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1&quot;&gt;Peng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaoshui Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yongshun Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhihui Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09758">
<title>Diagnosing and Rectifying Fake OOD Invariance: A Restructured Causal Approach. (arXiv:2312.09758v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.09758</link>
<description rdf:parseType="Literal">&lt;p&gt;Invariant representation learning (IRL) encourages the prediction from
invariant causal features to labels de-confounded from the environments,
advancing the technical roadmap of out-of-distribution (OOD) generalization.
Despite spotlights around, recent theoretical results verified that some causal
features recovered by IRLs merely pretend domain-invariantly in the training
environments but fail in unseen domains. The \emph{fake invariance} severely
endangers OOD generalization since the trustful objective can not be diagnosed
and existing causal surgeries are invalid to rectify. In this paper, we review
a IRL family (InvRat) under the Partially and Fully Informative Invariant
Feature Structural Causal Models (PIIF SCM /FIIF SCM) respectively, to certify
their weaknesses in representing fake invariant features, then, unify their
causal diagrams to propose ReStructured SCM (RS-SCM). RS-SCM can ideally
rebuild the spurious and the fake invariant features simultaneously. Given
this, we further develop an approach based on conditional mutual information
with respect to RS-SCM, then rigorously rectify the spurious and fake invariant
effects. It can be easily implemented by a small feature selection subnet
introduced in the IRL family, which is alternatively optimized to achieve our
goal. Experiments verified the superiority of our approach to fight against the
fake invariant issue across a variety of OOD generalization benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Ziliang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yongsen Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_Z/0/1/0/all/0/1&quot;&gt;Zhao-Rong Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_Q/0/1/0/all/0/1&quot;&gt;Quanlong Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Liang Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09773">
<title>In vivo learning-based control of microbial populations density in bioreactors. (arXiv:2312.09773v1 [eess.SY])</title>
<link>http://arxiv.org/abs/2312.09773</link>
<description rdf:parseType="Literal">&lt;p&gt;A key problem toward the use of microorganisms as bio-factories is reaching
and maintaining cellular communities at a desired density and composition so
that they can efficiently convert their biomass into useful compounds.
Promising technological platforms for the real time, scalable control of
cellular density are bioreactors. In this work, we developed a learning-based
strategy to expand the toolbox of available control algorithms capable of
regulating the density of a \textit{single} bacterial population in
bioreactors. Specifically, we used a sim-to-real paradigm, where a simple
mathematical model, calibrated using a few data, was adopted to generate
synthetic data for the training of the controller. The resulting policy was
then exhaustively tested in vivo using a low-cost bioreactor known as Chi.Bio,
assessing performance and robustness. In addition, we compared the performance
with more traditional controllers (namely, a PI and an MPC), confirming that
the learning-based controller exhibits similar performance in vivo. Our work
showcases the viability of learning-based strategies for the control of
cellular density in bioreactors, making a step forward toward their use for the
control of the composition of microbial consortia.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Brancato_S/0/1/0/all/0/1&quot;&gt;Sara Maria Brancato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Salzano_D/0/1/0/all/0/1&quot;&gt;Davide Salzano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lellis_F/0/1/0/all/0/1&quot;&gt;Francesco De Lellis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fiore_D/0/1/0/all/0/1&quot;&gt;Davide Fiore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Russo_G/0/1/0/all/0/1&quot;&gt;Giovanni Russo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bernardo_M/0/1/0/all/0/1&quot;&gt;Mario di Bernardo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09781">
<title>GSQA: An End-to-End Model for Generative Spoken Question Answering. (arXiv:2312.09781v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.09781</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent advancements in spoken question answering (QA), end-to-end models
have made significant strides. However, previous research has primarily focused
on extractive span selection. While this extractive-based approach is effective
when answers are present directly within the input, it falls short in
addressing abstractive questions, where answers are not directly extracted but
inferred from the given information. To bridge this gap, we introduce the first
end-to-end Generative Spoken Question Answering (GSQA) model that empowers the
system to engage in abstractive reasoning. The challenge in training our GSQA
model lies in the absence of a spoken abstractive QA dataset. We propose using
text models for initialization and leveraging the extractive QA dataset to
transfer knowledge from the text generative model to the spoken generative
model. Experimental results indicate that our model surpasses the previous
extractive model by 3% on extractive QA datasets. Furthermore, the GSQA model
has only been fine-tuned on the spoken extractive QA dataset. Despite not
having seen any spoken abstractive QA data, it can still closely match the
performance of the cascade model. In conclusion, our GSQA model shows the
potential to generalize to a broad spectrum of questions, thus further
expanding spoken question answering capabilities of abstractive QA. Our code is
available at
\href{https://voidful.github.io/GSQA}{https://voidful.github.io/GSQA}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shih_M/0/1/0/all/0/1&quot;&gt;Min-Han Shih&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1&quot;&gt;Ho-Lam Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pai_Y/0/1/0/all/0/1&quot;&gt;Yu-Chi Pai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_M/0/1/0/all/0/1&quot;&gt;Ming-Hao Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1&quot;&gt;Guan-Ting Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shang-Wen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hung-yi Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09788">
<title>Collaborating Foundation models for Domain Generalized Semantic Segmentation. (arXiv:2312.09788v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09788</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain Generalized Semantic Segmentation (DGSS) deals with training a model
on a labeled source domain with the aim of generalizing to unseen domains
during inference. Existing DGSS methods typically effectuate robust features by
means of Domain Randomization (DR). Such an approach is often limited as it can
only account for style diversification and not content. In this work, we take
an orthogonal approach to DGSS and propose to use an assembly of CoLlaborative
FOUndation models for Domain Generalized Semantic Segmentation (CLOUDS). In
detail, CLOUDS is a framework that integrates FMs of various kinds: (i) CLIP
backbone for its robust feature representation, (ii) generative models to
diversify the content, thereby covering various modes of the possible target
distribution, and (iii) Segment Anything Model (SAM) for iteratively refining
the predictions of the segmentation model. Extensive experiments show that our
CLOUDS excels in adapting from synthetic to real DGSS benchmarks and under
varying weather conditions, notably outperforming prior methods by 5.6% and
6.7% on averaged miou, respectively. The code is available at :
https://github.com/yasserben/CLOUDS
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benigmim_Y/0/1/0/all/0/1&quot;&gt;Yasser Benigmim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1&quot;&gt;Subhankar Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Essid_S/0/1/0/all/0/1&quot;&gt;Slim Essid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalogeiton_V/0/1/0/all/0/1&quot;&gt;Vicky Kalogeiton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lathuiliere_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Lathuili&amp;#xe8;re&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09792">
<title>Latent Diffusion Models with Image-Derived Annotations for Enhanced AI-Assisted Cancer Diagnosis in Histopathology. (arXiv:2312.09792v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09792</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial Intelligence (AI) based image analysis has an immense potential to
support diagnostic histopathology, including cancer diagnostics. However,
developing supervised AI methods requires large-scale annotated datasets. A
potentially powerful solution is to augment training data with synthetic data.
Latent diffusion models, which can generate high-quality, diverse synthetic
images, are promising. However, the most common implementations rely on
detailed textual descriptions, which are not generally available in this
domain. This work proposes a method that constructs structured textual prompts
from automatically extracted image features. We experiment with the PCam
dataset, composed of tissue patches only loosely annotated as healthy or
cancerous. We show that including image-derived features in the prompt, as
opposed to only healthy and cancerous labels, improves the Fr\&apos;echet Inception
Distance (FID) from 178.8 to 90.2. We also show that pathologists find it
challenging to detect synthetic images, with a median sensitivity/specificity
of 0.55/0.55. Finally, we show that synthetic data effectively trains AI
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osorio_P/0/1/0/all/0/1&quot;&gt;Pedro Osorio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jimenez_Perez_G/0/1/0/all/0/1&quot;&gt;Guillermo Jimenez-Perez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montalt_Tordera_J/0/1/0/all/0/1&quot;&gt;Javier Montalt-Tordera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hooge_J/0/1/0/all/0/1&quot;&gt;Jens Hooge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duran_Ballester_G/0/1/0/all/0/1&quot;&gt;Guillem Duran-Ballester&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Shivam Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radbruch_M/0/1/0/all/0/1&quot;&gt;Moritz Radbruch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bach_U/0/1/0/all/0/1&quot;&gt;Ute Bach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schroeder_S/0/1/0/all/0/1&quot;&gt;Sabrina Schroeder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siudak_K/0/1/0/all/0/1&quot;&gt;Krystyna Siudak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vienenkoetter_J/0/1/0/all/0/1&quot;&gt;Julia Vienenkoetter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lawrenz_B/0/1/0/all/0/1&quot;&gt;Bettina Lawrenz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohammadi_S/0/1/0/all/0/1&quot;&gt;Sadegh Mohammadi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09799">
<title>IQNet: Image Quality Assessment Guided Just Noticeable Difference Prefiltering For Versatile Video Coding. (arXiv:2312.09799v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.09799</link>
<description rdf:parseType="Literal">&lt;p&gt;Image prefiltering with just noticeable distortion (JND) improves coding
efficiency in a visual lossless way by filtering the perceptually redundant
information prior to compression. However, real JND cannot be well modeled with
inaccurate masking equations in traditional approaches or image-level subject
tests in deep learning approaches. Thus, this paper proposes a fine-grained JND
prefiltering dataset guided by image quality assessment for accurate
block-level JND modeling. The dataset is constructed from decoded images to
include coding effects and is also perceptually enhanced with block overlap and
edge preservation. Furthermore, based on this dataset, we propose a lightweight
JND prefiltering network, IQNet, which can be applied directly to different
quantization cases with the same model and only needs 3K parameters. The
experimental results show that the proposed approach to Versatile Video Coding
could yield maximum/average bitrate savings of 41\%/15\% and 53\%/19\% for
all-intra and low-delay P configurations, respectively, with negligible
subjective quality loss. Our method demonstrates higher perceptual quality and
a model size that is an order of magnitude smaller than previous deep learning
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yu-Han Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_C/0/1/0/all/0/1&quot;&gt;Chiang Lo-Hsuan Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chang_T/0/1/0/all/0/1&quot;&gt;Tian-Sheuan Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09802">
<title>Concept Prerequisite Relation Prediction by Using Permutation-Equivariant Directed Graph Neural Networks. (arXiv:2312.09802v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.09802</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies the problem of CPRP, concept prerequisite relation
prediction, which is a fundamental task in using AI for education. CPRP is
usually formulated into a link-prediction task on a relationship graph of
concepts and solved by training the graph neural network (GNN) model. However,
current directed GNNs fail to manage graph isomorphism which refers to the
invariance of non-isomorphic graphs, reducing the expressivity of resulting
representations. We present a permutation-equivariant directed GNN model by
introducing the Weisfeiler-Lehman test into directed GNN learning. Our method
is then used for CPRP and evaluated on three public datasets. The experimental
results show that our model delivers better prediction performance than the
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1&quot;&gt;Xiran Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_X/0/1/0/all/0/1&quot;&gt;Xuequn Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yupei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09806">
<title>Improving Biomedical Entity Linking with Retrieval-enhanced Learning. (arXiv:2312.09806v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.09806</link>
<description rdf:parseType="Literal">&lt;p&gt;Biomedical entity linking (BioEL) has achieved remarkable progress with the
help of pre-trained language models. However, existing BioEL methods usually
struggle to handle rare and difficult entities due to long-tailed distribution.
To address this limitation, we introduce a new scheme $k$NN-BioEL, which
provides a BioEL model with the ability to reference similar instances from the
entire training corpus as clues for prediction, thus improving the
generalization capabilities. Moreover, we design a contrastive learning
objective with dynamic hard negative sampling (DHNS) that improves the quality
of the retrieved neighbors during inference. Extensive experimental results
show that $k$NN-BioEL outperforms state-of-the-art baselines on several
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhenxi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Ziheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yefeng Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09812">
<title>Structural Information Guided Multimodal Pre-training for Vehicle-centric Perception. (arXiv:2312.09812v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09812</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding vehicles in images is important for various applications such
as intelligent transportation and self-driving system. Existing vehicle-centric
works typically pre-train models on large-scale classification datasets and
then fine-tune them for specific downstream tasks. However, they neglect the
specific characteristics of vehicle perception in different tasks and might
thus lead to sub-optimal performance. To address this issue, we propose a novel
vehicle-centric pre-training framework called VehicleMAE, which incorporates
the structural information including the spatial structure from vehicle profile
information and the semantic structure from informative high-level natural
language descriptions for effective masked vehicle appearance reconstruction.
To be specific, we explicitly extract the sketch lines of vehicles as a form of
the spatial structure to guide vehicle reconstruction. The more comprehensive
knowledge distilled from the CLIP big model based on the similarity between the
paired/unpaired vehicle image-text sample is further taken into consideration
to help achieve a better understanding of vehicles. A large-scale dataset is
built to pre-train our model, termed Autobot1M, which contains about 1M vehicle
images and 12693 text information. Extensive experiments on four vehicle-based
downstream tasks fully validated the effectiveness of our VehicleMAE. The
source code and pre-trained models will be released at
https://github.com/Event-AHU/VehicleMAE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wentao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenglong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhicheng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhe Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yukai Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jin Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09818">
<title>SMILE: Multimodal Dataset for Understanding Laughter in Video with Language Models. (arXiv:2312.09818v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.09818</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the recent advances of the artificial intelligence, building social
intelligence remains a challenge. Among social signals, laughter is one of the
distinctive expressions that occurs during social interactions between humans.
In this work, we tackle a new challenge for machines to understand the
rationale behind laughter in video, Video Laugh Reasoning. We introduce this
new task to explain why people laugh in a particular video and a dataset for
this task. Our proposed dataset, SMILE, comprises video clips and language
descriptions of why people laugh. We propose a baseline by leveraging the
reasoning capacity of large language models (LLMs) with textual video
representation. Experiments show that our baseline can generate plausible
explanations for laughter. We further investigate the scalability of our
baseline by probing other video understanding tasks and in-the-wild videos. We
release our dataset, code, and model checkpoints on
https://github.com/SMILE-data/SMILE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hyun_L/0/1/0/all/0/1&quot;&gt;Lee Hyun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sung_Bin_K/0/1/0/all/0/1&quot;&gt;Kim Sung-Bin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1&quot;&gt;Seungju Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Youngjae Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_T/0/1/0/all/0/1&quot;&gt;Tae-Hyun Oh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09844">
<title>Small Dataset, Big Gains: Enhancing Reinforcement Learning by Offline Pre-Training with Model Based Augmentation. (arXiv:2312.09844v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.09844</link>
<description rdf:parseType="Literal">&lt;p&gt;Offline reinforcement learning leverages pre-collected datasets of
transitions to train policies. It can serve as effective initialization for
online algorithms, enhancing sample efficiency and speeding up convergence.
However, when such datasets are limited in size and quality, offline
pre-training can produce sub-optimal policies and lead to degraded online
reinforcement learning performance. In this paper we propose a model-based data
augmentation strategy to maximize the benefits of offline reinforcement
learning pre-training and reduce the scale of data needed to be effective. Our
approach leverages a world model of the environment trained on the offline
dataset to augment states during offline pre-training. We evaluate our approach
on a variety of MuJoCo robotic tasks and our results show it can jump-start
online fine-tuning and substantially reduce - in some cases by an order of
magnitude - the required number of environment interactions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Macaluso_G/0/1/0/all/0/1&quot;&gt;Girolamo Macaluso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sestini_A/0/1/0/all/0/1&quot;&gt;Alessandro Sestini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagdanov_A/0/1/0/all/0/1&quot;&gt;Andrew D. Bagdanov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09857">
<title>Deep Unsupervised Domain Adaptation for Time Series Classification: a Benchmark. (arXiv:2312.09857v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.09857</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised Domain Adaptation (UDA) aims to harness labeled source data to
train models for unlabeled target data. Despite extensive research in domains
like computer vision and natural language processing, UDA remains underexplored
for time series data, which has widespread real-world applications ranging from
medicine and manufacturing to earth observation and human activity recognition.
Our paper addresses this gap by introducing a comprehensive benchmark for
evaluating UDA techniques for time series classification, with a focus on deep
learning methods. We provide seven new benchmark datasets covering various
domain shifts and temporal dynamics, facilitating fair and standardized UDA
method assessments with state of the art neural network backbones (e.g.
Inception) for time series data. This benchmark offers insights into the
strengths and limitations of the evaluated approaches while preserving the
unsupervised nature of domain adaptation, making it directly applicable to
practical problems. Our paper serves as a vital resource for researchers and
practitioners, advancing domain adaptation solutions for time series data and
fostering innovation in this critical field. The implementation code of this
benchmark is available at https://github.com/EricssonResearch/UDA-4-TSC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fawaz_H/0/1/0/all/0/1&quot;&gt;Hassan Ismail Fawaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grosso_G/0/1/0/all/0/1&quot;&gt;Ganesh Del Grosso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kerdoncuff_T/0/1/0/all/0/1&quot;&gt;Tanguy Kerdoncuff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boisbunon_A/0/1/0/all/0/1&quot;&gt;Aurelie Boisbunon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saffar_I/0/1/0/all/0/1&quot;&gt;Illyyne Saffar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09877">
<title>Distributed Learning of Mixtures of Experts. (arXiv:2312.09877v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.09877</link>
<description rdf:parseType="Literal">&lt;p&gt;In modern machine learning problems we deal with datasets that are either
distributed by nature or potentially large for which distributing the
computations is usually a standard way to proceed, since centralized algorithms
are in general ineffective. We propose a distributed learning approach for
mixtures of experts (MoE) models with an aggregation strategy to construct a
reduction estimator from local estimators fitted parallelly to distributed
subsets of the data. The aggregation is based on an optimal minimization of an
expected transportation divergence between the large MoE composed of local
estimators and the unknown desired MoE model. We show that the provided
reduction estimator is consistent as soon as the local estimators to be
aggregated are consistent, and its construction is performed by a proposed
majorization-minimization (MM) algorithm that is computationally effective. We
study the statistical and numerical properties for the proposed reduction
estimator on experiments that demonstrate its performance compared to namely
the global estimator constructed in a centralized way from the full dataset.
For some situations, the computation time is more than ten times faster, for a
comparable performance. Our source codes are publicly available on Github.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chamroukhi_F/0/1/0/all/0/1&quot;&gt;Fa&amp;#xef;cel Chamroukhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_N/0/1/0/all/0/1&quot;&gt;Nhat Thien Pham&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09881">
<title>Dynamic Heterogeneous Federated Learning with Multi-Level Prototypes. (arXiv:2312.09881v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.09881</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning shows promise as a privacy-preserving collaborative
learning technique. Existing heterogeneous federated learning mainly focuses on
skewing the label distribution across clients. However, most approaches suffer
from catastrophic forgetting and concept drift, mainly when the global
distribution of all classes is extremely unbalanced and the data distribution
of the client dynamically evolves over time. In this paper, we study the new
task, i.e., Dynamic Heterogeneous Federated Learning (DHFL), which addresses
the practical scenario where heterogeneous data distributions exist among
different clients and dynamic tasks within the client. Accordingly, we propose
a novel federated learning framework named Federated Multi-Level Prototypes
(FedMLP) and design federated multi-level regularizations. To mitigate concept
drift, we construct prototypes and semantic prototypes to provide fruitful
generalization knowledge and ensure the continuity of prototype spaces. To
maintain the model stability and consistency of convergence, three
regularizations are introduced as training losses, i.e., prototype-based
regularization, semantic prototype-based regularization, and federated
inter-task regularization. Extensive experiments show that the proposed method
achieves state-of-the-art performance in various settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1&quot;&gt;Shunxin Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongsong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1&quot;&gt;Xin Geng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09885">
<title>Simple Weak Coresets for Non-Decomposable Classification Measures. (arXiv:2312.09885v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.09885</link>
<description rdf:parseType="Literal">&lt;p&gt;While coresets have been growing in terms of their application, barring few
exceptions, they have mostly been limited to unsupervised settings. We consider
supervised classification problems, and non-decomposable evaluation measures in
such settings. We show that stratified uniform sampling based coresets have
excellent empirical performance that are backed by theoretical guarantees too.
We focus on the F1 score and Matthews Correlation Coefficient, two widely used
non-decomposable objective functions that are nontrivial to optimize for and
show that uniform coresets attain a lower bound for coreset size, and have good
empirical performance, comparable with ``smarter&apos;&apos; coreset construction
strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malaviya_J/0/1/0/all/0/1&quot;&gt;Jayesh Malaviya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dasgupta_A/0/1/0/all/0/1&quot;&gt;Anirban Dasgupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chhaya_R/0/1/0/all/0/1&quot;&gt;Rachit Chhaya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09894">
<title>PathoDuet: Foundation Models for Pathological Slide Analysis of H&amp;E and IHC Stains. (arXiv:2312.09894v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09894</link>
<description rdf:parseType="Literal">&lt;p&gt;Large amounts of digitized histopathological data display a promising future
for developing pathological foundation models via self-supervised learning
methods. Foundation models pretrained with these methods serve as a good basis
for downstream tasks. However, the gap between natural and histopathological
images hinders the direct application of existing methods. In this work, we
present PathoDuet, a series of pretrained models on histopathological images,
and a new self-supervised learning framework in histopathology. The framework
is featured by a newly-introduced pretext token and later task raisers to
explicitly utilize certain relations between images, like multiple
magnifications and multiple stains. Based on this, two pretext tasks,
cross-scale positioning and cross-stain transferring, are designed to pretrain
the model on Hematoxylin and Eosin (H\&amp;amp;E) images and transfer the model to
immunohistochemistry (IHC) images, respectively. To validate the efficacy of
our models, we evaluate the performance over a wide variety of downstream
tasks, including patch-level colorectal cancer subtyping and whole slide image
(WSI)-level classification in H\&amp;amp;E field, together with expression level
prediction of IHC marker and tumor identification in IHC field. The
experimental results show the superiority of our models over most tasks and the
efficacy of proposed pretext tasks. The codes and models are available at
https://github.com/openmedlab/PathoDuet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_S/0/1/0/all/0/1&quot;&gt;Shengyi Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_F/0/1/0/all/0/1&quot;&gt;Fang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1&quot;&gt;Tianle Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaofan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09897">
<title>A Novel Dataset for Financial Education Text Simplification in Spanish. (arXiv:2312.09897v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.09897</link>
<description rdf:parseType="Literal">&lt;p&gt;Text simplification, crucial in natural language processing, aims to make
texts more comprehensible, particularly for specific groups like visually
impaired Spanish speakers, a less-represented language in this field. In
Spanish, there are few datasets that can be used to create text simplification
systems. Our research has the primary objective to develop a Spanish financial
text simplification dataset. We created a dataset with 5,314 complex and
simplified sentence pairs using established simplification rules. We also
compared our dataset with the simplifications generated from GPT-3, Tuner, and
MT5, in order to evaluate the feasibility of data augmentation using these
systems. In this manuscript we present the characteristics of our dataset and
the findings of the comparisons with other systems. The dataset is available at
Hugging face, saul1917/FEINA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_Rojas_N/0/1/0/all/0/1&quot;&gt;Nelson Perez-Rojas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calderon_Ramirez_S/0/1/0/all/0/1&quot;&gt;Saul Calderon-Ramirez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solis_Salazar_M/0/1/0/all/0/1&quot;&gt;Martin Solis-Salazar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romero_Sandoval_M/0/1/0/all/0/1&quot;&gt;Mario Romero-Sandoval&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arias_Monge_M/0/1/0/all/0/1&quot;&gt;Monica Arias-Monge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saggion_H/0/1/0/all/0/1&quot;&gt;Horacio Saggion&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09906">
<title>Sample-Efficient Learning to Solve a Real-World Labyrinth Game Using Data-Augmented Model-Based Reinforcement Learning. (arXiv:2312.09906v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.09906</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivated by the challenge of achieving rapid learning in physical
environments, this paper presents the development and training of a robotic
system designed to navigate and solve a labyrinth game using model-based
reinforcement learning techniques. The method involves extracting
low-dimensional observations from camera images, along with a cropped and
rectified image patch centered on the current position within the labyrinth,
providing valuable information about the labyrinth layout. The learning of a
control policy is performed purely on the physical system using model-based
reinforcement learning, where the progress along the labyrinth&apos;s path serves as
a reward signal. Additionally, we exploit the system&apos;s inherent symmetries to
augment the training data. Consequently, our approach learns to successfully
solve a popular real-world labyrinth game in record time, with only 5 hours of
real-world training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_T/0/1/0/all/0/1&quot;&gt;Thomas Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DAndrea_R/0/1/0/all/0/1&quot;&gt;Raffaello D&amp;#x27;Andrea&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09907">
<title>Exploring Automatic Text Simplification of German Narrative Documents. (arXiv:2312.09907v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.09907</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we apply transformer-based Natural Language Generation (NLG)
techniques to the problem of text simplification. Currently, there are only a
few German datasets available for text simplification, even fewer with larger
and aligned documents, and not a single one with narrative texts. In this
paper, we explore to which degree modern NLG techniques can be applied to
German narrative text simplifications. We use Longformer attention and a
pre-trained mBART model. Our findings indicate that the existing approaches for
German are not able to solve the task properly. We conclude on a few directions
for future research to address this problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schomacker_T/0/1/0/all/0/1&quot;&gt;Thorben Schomacker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Donicke_T/0/1/0/all/0/1&quot;&gt;Tillmann D&amp;#xf6;nicke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tropmann_Frick_M/0/1/0/all/0/1&quot;&gt;Marina Tropmann-Frick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09917">
<title>Red AI? Inconsistent Responses from GPT3.5 Models on Political Issues in the US and China. (arXiv:2312.09917v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.09917</link>
<description rdf:parseType="Literal">&lt;p&gt;The rising popularity of ChatGPT and other AI-powered large language models
(LLMs) has led to increasing studies highlighting their susceptibility to
mistakes and biases. However, most of these studies focus on models trained on
English texts. Taking an innovative approach, this study investigates political
biases in GPT&apos;s multilingual models. We posed the same question about
high-profile political issues in the United States and China to GPT in both
English and simplified Chinese, and our analysis of the bilingual responses
revealed that GPT&apos;s bilingual models&apos; political &quot;knowledge&quot; (content) and the
political &quot;attitude&quot; (sentiment) are significantly more inconsistent on
political issues in China. The simplified Chinese GPT models not only tended to
provide pro-China information but also presented the least negative sentiment
towards China&apos;s problems, whereas the English GPT was significantly more
negative towards China. This disparity may stem from Chinese state censorship
and US-China geopolitical tensions, which influence the training corpora of GPT
bilingual models. Moreover, both Chinese and English models tended to be less
critical towards the issues of &quot;their own&quot; represented by the language used,
than the issues of &quot;the other.&quot; This suggests that GPT multilingual models
could potentially develop a &quot;political identity&quot; and an associated sentiment
bias based on their training language. We discussed the implications of our
findings for information transmission and communication in an increasingly
divided world.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1&quot;&gt;Di Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yinxian Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09922">
<title>A Unifying Tensor View for Lightweight CNNs. (arXiv:2312.09922v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.09922</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the decomposition of convolutional kernels for lightweight CNNs being
well studied, existing works that rely on tensor network diagrams or
hyperdimensional abstraction lack geometry intuition. This work devises a new
perspective by linking a 3D-reshaped kernel tensor to its various slice-wise
and rank-1 decompositions, permitting a straightforward connection between
various tensor approximations and efficient CNN modules. Specifically, it is
discovered that a pointwise-depthwise-pointwise (PDP) configuration constitutes
a viable construct for lightweight CNNs. Moreover, a novel link to the latest
ShiftNet is established, inspiring a first-ever shift layer pruning that
achieves nearly 50% compression with &amp;lt; 1% drop in accuracy for ShiftResNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jason Chun Lok Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1&quot;&gt;Rui Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jiajun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_E/0/1/0/all/0/1&quot;&gt;Edmund Yin Mun Lam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_N/0/1/0/all/0/1&quot;&gt;Ngai Wong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09926">
<title>FuXi-S2S: An accurate machine learning model for global subseasonal forecasts. (arXiv:2312.09926v1 [physics.ao-ph])</title>
<link>http://arxiv.org/abs/2312.09926</link>
<description rdf:parseType="Literal">&lt;p&gt;Skillful subseasonal forecasts beyond 2 weeks are crucial for a wide range of
applications across various sectors of society. Recently, state-of-the-art
machine learning based weather forecasting models have made significant
advancements, outperforming the high-resolution forecast (HRES) from the
European Centre for Medium-Range Weather Forecasts (ECMWF). However, the full
potential of machine learning models in subseasonal forecasts has yet to be
fully explored. In this study, we introduce FuXi Subseasonal-to-Seasonal
(FuXi-S2S), a machine learning based subseasonal forecasting model that
provides global daily mean forecasts up to 42 days, covering 5 upper-air
atmospheric variables at 13 pressure levels and 11 surface variables. FuXi-S2S
integrates an enhanced FuXi base model with a perturbation module for
flow-dependent perturbations in hidden features, and incorporates Perlin noise
to perturb initial conditions. The model is developed using 72 years of daily
statistics from ECMWF ERA5 reanalysis data. When compared to the ECMWF
Subseasonal-to-Seasonal (S2S) reforecasts, the FuXi-S2S forecasts demonstrate
superior deterministic and ensemble forecasts for total precipitation (TP),
outgoing longwave radiation (OLR), and geopotential at 500 hPa (Z500). Although
it shows slightly inferior performance in predicting 2-meter temperature (T2M),
it has clear advantages over land area. Regarding the extreme forecasts,
FuXi-S2S outperforms ECMWF S2S globally for TP. Furthermore, FuXi-S2S forecasts
surpass the ECMWF S2S reforecasts in predicting the Madden Julian Oscillation
(MJO), a key source of subseasonal predictability. They extend the skillful
prediction of MJO from 30 days to 36 days.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zhong_X/0/1/0/all/0/1&quot;&gt;Xiaohui Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jie Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Deliang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Xie_S/0/1/0/all/0/1&quot;&gt;Shangping Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chao_Q/0/1/0/all/0/1&quot;&gt;Qingchen Chao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chensen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zixin Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Lu_B/0/1/0/all/0/1&quot;&gt;Bo Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Qi_Y/0/1/0/all/0/1&quot;&gt;Yuan Qi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09928">
<title>Neurosymbolic Value-Inspired AI (Why, What, and How). (arXiv:2312.09928v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.09928</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid progression of Artificial Intelligence (AI) systems, facilitated by
the advent of Large Language Models (LLMs), has resulted in their widespread
application to provide human assistance across diverse industries. This trend
has sparked significant discourse centered around the ever-increasing need for
LLM-based AI systems to function among humans as part of human society, sharing
human values, especially as these systems are deployed in high-stakes settings
(e.g., healthcare, autonomous driving, etc.). Towards this end, neurosymbolic
AI systems are attractive due to their potential to enable easy-to-understand
and interpretable interfaces for facilitating value-based decision-making, by
leveraging explicit representations of shared values. In this paper, we
introduce substantial extensions to Khaneman&apos;s System one/two framework and
propose a neurosymbolic computational framework called Value-Inspired AI (VAI).
It outlines the crucial components essential for the robust and practical
implementation of VAI systems, aiming to represent and integrate various
dimensions of human values. Finally, we further offer insights into the current
progress made in this direction and outline potential future directions for the
field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1&quot;&gt;Amit Sheth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1&quot;&gt;Kaushik Roy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09932">
<title>RDR: the Recap, Deliberate, and Respond Method for Enhanced Language Understanding. (arXiv:2312.09932v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.09932</link>
<description rdf:parseType="Literal">&lt;p&gt;Natural language understanding (NLU) using neural network pipelines often
requires additional context that is not solely present in the input data.
Through Prior research, it has been evident that NLU benchmarks are susceptible
to manipulation by neural models, wherein these models exploit statistical
artifacts within the encoded external knowledge to artificially inflate
performance metrics for downstream tasks. Our proposed approach, known as the
Recap, Deliberate, and Respond (RDR) paradigm, addresses this issue by
incorporating three distinct objectives within the neural network pipeline.
Firstly, the Recap objective involves paraphrasing the input text using a
paraphrasing model in order to summarize and encapsulate its essence. Secondly,
the Deliberation objective entails encoding external graph information related
to entities mentioned in the input text, utilizing a graph embedding model.
Finally, the Respond objective employs a classification head model that
utilizes representations from the Recap and Deliberation modules to generate
the final prediction. By cascading these three models and minimizing a combined
loss, we mitigate the potential for gaming the benchmark and establish a robust
method for capturing the underlying semantic patterns, thus enabling accurate
predictions. To evaluate the effectiveness of the RDR method, we conduct tests
on multiple GLUE benchmark tasks. Our results demonstrate improved performance
compared to competitive baselines, with an enhancement of up to 2\% on standard
metrics. Furthermore, we analyze the observed evidence for semantic
understanding exhibited by RDR models, emphasizing their ability to avoid
gaming the benchmark and instead accurately capture the true underlying
semantic patterns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zi_Y/0/1/0/all/0/1&quot;&gt;Yuxin Zi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veeramani_H/0/1/0/all/0/1&quot;&gt;Hariram Veeramani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1&quot;&gt;Kaushik Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1&quot;&gt;Amit Sheth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09938">
<title>Assume-Guarantee Reinforcement Learning. (arXiv:2312.09938v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.09938</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a modular approach to \emph{reinforcement learning} (RL) in
environments consisting of simpler components evolving in parallel. A
monolithic view of such modular environments may be prohibitively large to
learn, or may require unrealizable communication between the components in the
form of a centralized controller. Our proposed approach is based on the
assume-guarantee paradigm where the optimal control for the individual
components is synthesized in isolation by making \emph{assumptions} about the
behaviors of neighboring components, and providing \emph{guarantees} about
their own behavior. We express these \emph{assume-guarantee contracts} as
regular languages and provide automatic translations to scalar rewards to be
used in RL. By combining local probabilities of satisfaction for each
component, we provide a lower bound on the probability of satisfaction of the
complete system. By solving a Markov game for each component, RL can produce a
controller for each component that maximizes this lower bound. The controller
utilizes the information it receives through communication, observations, and
any knowledge of a coarse model of other agents. We experimentally demonstrate
the efficiency of the proposed approach on a variety of case studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kazemi_M/0/1/0/all/0/1&quot;&gt;Milad Kazemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_M/0/1/0/all/0/1&quot;&gt;Mateo Perez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Somenzi_F/0/1/0/all/0/1&quot;&gt;Fabio Somenzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soudjani_S/0/1/0/all/0/1&quot;&gt;Sadegh Soudjani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1&quot;&gt;Ashutosh Trivedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Velasquez_A/0/1/0/all/0/1&quot;&gt;Alvaro Velasquez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09950">
<title>Peer Learning: Learning Complex Policies in Groups from Scratch via Action Recommendations. (arXiv:2312.09950v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.09950</link>
<description rdf:parseType="Literal">&lt;p&gt;Peer learning is a novel high-level reinforcement learning framework for
agents learning in groups. While standard reinforcement learning trains an
individual agent in trial-and-error fashion, all on its own, peer learning
addresses a related setting in which a group of agents, i.e., peers, learns to
master a task simultaneously together from scratch. Peers are allowed to
communicate only about their own states and actions recommended by others:
&quot;What would you do in my situation?&quot;. Our motivation is to study the learning
behavior of these agents. We formalize the teacher selection process in the
action advice setting as a multi-armed bandit problem and therefore highlight
the need for exploration. Eventually, we analyze the learning behavior of the
peers and observe their ability to rank the agents&apos; performance within the
study group and understand which agents give reliable advice. Further, we
compare peer learning with single agent learning and a state-of-the-art action
advice baseline. We show that peer learning is able to outperform single-agent
learning and the baseline in several challenging discrete and continuous OpenAI
Gym domains. Doing so, we also show that within such a framework complex
policies from action recommendations beyond discrete action spaces can evolve.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Derstroff_C/0/1/0/all/0/1&quot;&gt;Cedric Derstroff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cerrato_M/0/1/0/all/0/1&quot;&gt;Mattia Cerrato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brugger_J/0/1/0/all/0/1&quot;&gt;Jannis Brugger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1&quot;&gt;Jan Peters&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kramer_S/0/1/0/all/0/1&quot;&gt;Stefan Kramer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09953">
<title>Deep Reinforcement Learning for Joint Cruise Control and Intelligent Data Acquisition in UAVs-Assisted Sensor Networks. (arXiv:2312.09953v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2312.09953</link>
<description rdf:parseType="Literal">&lt;p&gt;Unmanned aerial vehicle (UAV)-assisted sensor networks (UASNets), which play
a crucial role in creating new opportunities, are experiencing significant
growth in civil applications worldwide. UASNets improve disaster management
through timely surveillance and advance precision agriculture with detailed
crop monitoring, thereby significantly transforming the commercial economy.
UASNets revolutionize the commercial sector by offering greater efficiency,
safety, and cost-effectiveness, highlighting their transformative impact. A
fundamental aspect of these new capabilities and changes is the collection of
data from rugged and remote areas. Due to their excellent mobility and
maneuverability, UAVs are employed to collect data from ground sensors in harsh
environments, such as natural disaster monitoring, border surveillance, and
emergency response monitoring. One major challenge in these scenarios is that
the movements of UAVs affect channel conditions and result in packet loss. Fast
movements of UAVs lead to poor channel conditions and rapid signal degradation,
resulting in packet loss. On the other hand, slow mobility of a UAV can cause
buffer overflows of the ground sensors, as newly arrived data is not promptly
collected by the UAV.
&lt;/p&gt;
&lt;p&gt;Our proposal to address this challenge is to minimize packet loss by jointly
optimizing the velocity controls and data collection schedules of multiple
UAVs.Furthermore, in UASNets, swift movements of UAVs result in poor channel
conditions and fast signal attenuation, leading to an extended age of
information (AoI). In contrast, slow movements of UAVs prolong flight time,
thereby extending the AoI of ground sensors.To address this challenge, we
propose a new mean-field flight resource allocation optimization to minimize
the AoI of sensory data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Emami_Y/0/1/0/all/0/1&quot;&gt;Yousef Emami&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09958">
<title>Distilling Large Language Models for Matching Patients to Clinical Trials. (arXiv:2312.09958v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.09958</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent success of large language models (LLMs) has paved the way for
their adoption in the high-stakes domain of healthcare. Specifically, the
application of LLMs in patient-trial matching, which involves assessing patient
eligibility against clinical trial&apos;s nuanced inclusion and exclusion criteria,
has shown promise. Recent research has shown that GPT-3.5, a widely recognized
LLM developed by OpenAI, can outperform existing methods with minimal &apos;variable
engineering&apos; by simply comparing clinical trial information against patient
summaries. However, there are significant challenges associated with using
closed-source proprietary LLMs like GPT-3.5 in practical healthcare
applications, such as cost, privacy and reproducibility concerns. To address
these issues, this study presents the first systematic examination of the
efficacy of both proprietary (GPT-3.5, and GPT-4) and open-source LLMs (LLAMA
7B,13B, and 70B) for the task of patient-trial matching. Employing a
multifaceted evaluation framework, we conducted extensive automated and
human-centric assessments coupled with a detailed error analysis for each
model. To enhance the adaptability of open-source LLMs, we have created a
specialized synthetic dataset utilizing GPT-4, enabling effective fine-tuning
under constrained data conditions. Our findings reveal that open-source LLMs,
when fine-tuned on this limited and synthetic dataset, demonstrate performance
parity with their proprietary counterparts. This presents a massive opportunity
for their deployment in real-world healthcare applications. To foster further
research and applications in this field, we release both the annotated
evaluation dataset along with the fine-tuned LLM -- Trial-LLAMA -- for public
use.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nievas_M/0/1/0/all/0/1&quot;&gt;Mauro Nievas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basu_A/0/1/0/all/0/1&quot;&gt;Aditya Basu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanshan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_H/0/1/0/all/0/1&quot;&gt;Hrituraj Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09963">
<title>Symbolic Numeric Planning with Patterns. (arXiv:2312.09963v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.09963</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel approach for solving linear numeric
planning problems, called Symbolic Pattern Planning. Given a planning problem
$\Pi$, a bound $n$ and a pattern -- defined as an arbitrary sequence of actions
-- we encode the problem of finding a plan for $\Pi$ with bound $n$ as a
formula with fewer variables and/or clauses than the state-of-the-art rolled-up
and relaxed-relaxed-$\exists$ encodings. More importantly, we prove that for
any given bound, it is never the case that the latter two encodings allow
finding a valid plan while ours does not. On the experimental side, we consider
6 other planning systems -- including the ones which participated in this
year&apos;s International Planning Competition (IPC) -- and we show that our planner
Patty has remarkably good comparative performances on this year&apos;s IPC problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cardellini_M/0/1/0/all/0/1&quot;&gt;Matteo Cardellini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giunchiglia_E/0/1/0/all/0/1&quot;&gt;Enrico Giunchiglia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maratea_M/0/1/0/all/0/1&quot;&gt;Marco Maratea&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09966">
<title>Data and Approaches for German Text simplification -- towards an Accessibility-enhanced Communication. (arXiv:2312.09966v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.09966</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper examines the current state-of-the-art of German text
simplification, focusing on parallel and monolingual German corpora. It reviews
neural language models for simplifying German texts and assesses their
suitability for legal texts and accessibility requirements. Our findings
highlight the need for additional training data and more appropriate approaches
that consider the specific linguistic characteristics of German, as well as the
importance of the needs and preferences of target groups with cognitive or
language impairments. The authors launched the interdisciplinary OPEN-LS
project in April 2023 to address these research gaps. The project aims to
develop a framework for text formats tailored to individuals with low literacy
levels, integrate legal texts, and enhance comprehensibility for those with
linguistic or cognitive impairments. It will also explore cost-effective ways
to enhance the data with audience-specific illustrations using image-generating
AI.
&lt;/p&gt;
&lt;p&gt;For more and up-to-date information, please visit our project homepage
https://open-ls.entavis.com
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schomacker_T/0/1/0/all/0/1&quot;&gt;Thorben Schomacker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gille_M/0/1/0/all/0/1&quot;&gt;Michael Gille&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hulls_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;rg von der H&amp;#xfc;lls&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tropmann_Frick_M/0/1/0/all/0/1&quot;&gt;Marina Tropmann-Frick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09971">
<title>GreenLightningAI: An Efficient AI System with Decoupled Structural and Quantitative Knowledge. (arXiv:2312.09971v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.09971</link>
<description rdf:parseType="Literal">&lt;p&gt;The number and complexity of artificial intelligence (AI) applications is
growing relentlessly. As a result, even with the many algorithmic and
mathematical advances experienced over past decades as well as the impressive
energy efficiency and computational capacity of current hardware accelerators,
training the most powerful and popular deep neural networks comes at very high
economic and environmental costs. Recognising that additional optimisations of
conventional neural network training is very difficult, this work takes a
radically different approach by proposing GreenLightningAI, a new AI system
design consisting of a linear model that is capable of emulating the behaviour
of deep neural networks by subsetting the model for each particular sample. The
new AI system stores the information required to select the system subset for a
given sample (referred to as structural information) separately from the linear
model parameters (referred to as quantitative knowledge). In this paper we
present a proof of concept, showing that the structural information stabilises
far earlier than the quantitative knowledge. Additionally, we show
experimentally that the structural information can be kept unmodified when
re-training the AI system with new samples while still achieving a validation
accuracy similar to that obtained when re-training a neural network with
similar size. Since the proposed AI system is based on a linear model, multiple
copies of the model, trained with different datasets, can be easily combined.
This enables faster and greener (re)-training algorithms, including incremental
re-training and federated incremental re-training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duato_J/0/1/0/all/0/1&quot;&gt;Jose Duato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mestre_J/0/1/0/all/0/1&quot;&gt;Jose I. Mestre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dolz_M/0/1/0/all/0/1&quot;&gt;Manuel F. Dolz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quintana_Orti_E/0/1/0/all/0/1&quot;&gt;Enrique S. Quintana-Ort&amp;#xed;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09982">
<title>ACPO: AI-Enabled Compiler-Driven Program Optimization. (arXiv:2312.09982v1 [cs.PL])</title>
<link>http://arxiv.org/abs/2312.09982</link>
<description rdf:parseType="Literal">&lt;p&gt;The key to performance optimization of a program is to decide correctly when
a certain transformation should be applied by a compiler. Traditionally, such
profitability decisions are made by hand-coded algorithms tuned for a very
small number of benchmarks, usually requiring a great deal of effort to be
retuned when the benchmark suite changes. This is an ideal opportunity to apply
machine-learning models to speed up the tuning process; while this realization
has been around since the late 90s, only recent advancements in ML enabled a
practical application of ML to compilers as an end-to-end framework. Even so,
seamless integration of ML into the compiler would require constant rebuilding
of the compiler when models are updated.
&lt;/p&gt;
&lt;p&gt;This paper presents ACPO: \textbf{\underline{A}}I-Enabled
\textbf{\underline{C}}ompiler-driven \textbf{\underline{P}}rogram
\textbf{\underline{O}}ptimization; a novel framework to provide LLVM with
simple and comprehensive tools to benefit from employing ML models for
different optimization passes. We first showcase the high-level view, class
hierarchy, and functionalities of ACPO and subsequently, demonstrate \taco{a
couple of use cases of ACPO by ML-enabling the Loop Unroll and Function
Inlining passes and describe how ACPO can be leveraged to optimize other
passes. Experimental results reveal that ACPO model for Loop Unroll is able to
gain on average 4\% and 3\%, 5.4\%, 0.2\% compared to LLVM&apos;s O3 optimization
when deployed on Polybench, Coral-2, CoreMark, and Graph-500, respectively.
Furthermore, by adding the Inliner model as well, ACPO is able to provide up to
4.5\% and 2.4\% on Polybench and Cbench compared with LLVM&apos;s O3 optimization,
respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ashouri_A/0/1/0/all/0/1&quot;&gt;Amir H. Ashouri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manzoor_M/0/1/0/all/0/1&quot;&gt;Muhammad Asif Manzoor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vu_D/0/1/0/all/0/1&quot;&gt;Duc Minh Vu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Raymond Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziwen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Angel Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_B/0/1/0/all/0/1&quot;&gt;Bryan Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Czajkowski_T/0/1/0/all/0/1&quot;&gt;Tomasz S. Czajkowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yaoqing Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09983">
<title>Toward Computationally Efficient Inverse Reinforcement Learning via Reward Shaping. (arXiv:2312.09983v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.09983</link>
<description rdf:parseType="Literal">&lt;p&gt;Inverse reinforcement learning (IRL) is computationally challenging, with
common approaches requiring the solution of multiple reinforcement learning
(RL) sub-problems. This work motivates the use of potential-based reward
shaping to reduce the computational burden of each RL sub-problem. This work
serves as a proof-of-concept and we hope will inspire future developments
towards computationally efficient IRL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cooke_L/0/1/0/all/0/1&quot;&gt;Lauren H. Cooke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klyne_H/0/1/0/all/0/1&quot;&gt;Harvey Klyne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1&quot;&gt;Edwin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laidlaw_C/0/1/0/all/0/1&quot;&gt;Cassidy Laidlaw&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tambe_M/0/1/0/all/0/1&quot;&gt;Milind Tambe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doshi_Velez_F/0/1/0/all/0/1&quot;&gt;Finale Doshi-Velez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09995">
<title>SAT-Based Algorithms for Regular Graph Pattern Matching. (arXiv:2312.09995v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.09995</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph matching is a fundamental problem in pattern recognition, with many
applications such as software analysis and computational biology. One
well-known type of graph matching problem is graph isomorphism, which consists
of deciding if two graphs are identical. Despite its usefulness, the properties
that one may check using graph isomorphism are rather limited, since it only
allows strict equality checks between two graphs. For example, it does not
allow one to check complex structural properties such as if the target graph is
an arbitrary length sequence followed by an arbitrary size loop.
&lt;/p&gt;
&lt;p&gt;We propose a generalization of graph isomorphism that allows one to check
such properties through a declarative specification. This specification is
given in the form of a Regular Graph Pattern (ReGaP), a special type of graph,
inspired by regular expressions, that may contain wildcard nodes that represent
arbitrary structures such as variable-sized sequences or subgraphs. We propose
a SAT-based algorithm for checking if a target graph matches a given ReGaP. We
also propose a preprocessing technique for improving the performance of the
algorithm and evaluate it through an extensive experimental evaluation on
benchmarks from the CodeSearchNet dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Terra_Neves_M/0/1/0/all/0/1&quot;&gt;Miguel Terra-Neves&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amaral_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Amaral&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lemos_A/0/1/0/all/0/1&quot;&gt;Alexandre Lemos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quintino_R/0/1/0/all/0/1&quot;&gt;Rui Quintino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Resende_P/0/1/0/all/0/1&quot;&gt;Pedro Resende&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alegria_A/0/1/0/all/0/1&quot;&gt;Antonio Alegria&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09997">
<title>One Self-Configurable Model to Solve Many Abstract Visual Reasoning Problems. (arXiv:2312.09997v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.09997</link>
<description rdf:parseType="Literal">&lt;p&gt;Abstract Visual Reasoning (AVR) comprises a wide selection of various
problems similar to those used in human IQ tests. Recent years have brought
dynamic progress in solving particular AVR tasks, however, in the contemporary
literature AVR problems are largely dealt with in isolation, leading to highly
specialized task-specific methods. With the aim of developing universal
learning systems in the AVR domain, we propose the unified model for solving
Single-Choice Abstract visual Reasoning tasks (SCAR), capable of solving
various single-choice AVR tasks, without making any a priori assumptions about
the task structure, in particular the number and location of panels. The
proposed model relies on a novel Structure-Aware dynamic Layer (SAL), which
adapts its weights to the structure of the considered AVR problem. Experiments
conducted on Raven&apos;s Progressive Matrices, Visual Analogy Problems, and Odd One
Out problems show that SCAR (SAL-based models, in general) effectively solves
diverse AVR tasks, and its performance is on par with the state-of-the-art
task-specific baselines. What is more, SCAR demonstrates effective knowledge
reuse in multi-task and transfer learning settings. To our knowledge, this work
is the first successful attempt to construct a general single-choice AVR solver
relying on self-configurable architecture and unified solving method. With this
work we aim to stimulate and foster progress on task-independent research paths
in the AVR domain, with the long-term goal of development of a general AVR
solver.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malkinski_M/0/1/0/all/0/1&quot;&gt;Miko&amp;#x142;aj Ma&amp;#x142;ki&amp;#x144;ski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mandziuk_J/0/1/0/all/0/1&quot;&gt;Jacek Ma&amp;#x144;dziuk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10008">
<title>Movement Primitive Diffusion: Learning Gentle Robotic Manipulation of Deformable Objects. (arXiv:2312.10008v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.10008</link>
<description rdf:parseType="Literal">&lt;p&gt;Policy learning in robot-assisted surgery (RAS) lacks data efficient and
versatile methods that exhibit the desired motion quality for delicate surgical
interventions. To this end, we introduce Movement Primitive Diffusion (MPD), a
novel method for imitation learning (IL) in RAS that focuses on gentle
manipulation of deformable objects. The approach combines the versatility of
diffusion-based imitation learning (DIL) with the high-quality motion
generation capabilities of Probabilistic Dynamic Movement Primitives (ProDMPs).
This combination enables MPD to achieve gentle manipulation of deformable
objects, while maintaining data efficiency critical for RAS applications where
demonstration data is scarce. We evaluate MPD across various simulated tasks
and a real world robotic setup on both state and image observations. MPD
outperforms state-of-the-art DIL methods in success rate, motion quality, and
data efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scheikl_P/0/1/0/all/0/1&quot;&gt;Paul Maria Scheikl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schreiber_N/0/1/0/all/0/1&quot;&gt;Nicolas Schreiber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haas_C/0/1/0/all/0/1&quot;&gt;Christoph Haas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freymuth_N/0/1/0/all/0/1&quot;&gt;Niklas Freymuth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1&quot;&gt;Gerhard Neumann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lioutikov_R/0/1/0/all/0/1&quot;&gt;Rudolf Lioutikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mathis_Ullrich_F/0/1/0/all/0/1&quot;&gt;Franziska Mathis-Ullrich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10029">
<title>Challenges with unsupervised LLM knowledge discovery. (arXiv:2312.10029v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.10029</link>
<description rdf:parseType="Literal">&lt;p&gt;We show that existing unsupervised methods on large language model (LLM)
activations do not discover knowledge -- instead they seem to discover whatever
feature of the activations is most prominent. The idea behind unsupervised
knowledge elicitation is that knowledge satisfies a consistency structure,
which can be used to discover knowledge. We first prove theoretically that
arbitrary features (not just knowledge) satisfy the consistency structure of a
particular leading unsupervised knowledge-elicitation method,
contrast-consistent search (Burns et al. - &lt;a href=&quot;/abs/2212.03827&quot;&gt;arXiv:2212.03827&lt;/a&gt;). We then present a
series of experiments showing settings in which unsupervised methods result in
classifiers that do not predict knowledge, but instead predict a different
prominent feature. We conclude that existing unsupervised methods for
discovering latent knowledge are insufficient, and we contribute sanity checks
to apply to evaluating future knowledge elicitation methods. Conceptually, we
hypothesise that the identification issues explored here, e.g. distinguishing a
model&apos;s knowledge from that of a simulated character&apos;s, will persist for future
unsupervised methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farquhar_S/0/1/0/all/0/1&quot;&gt;Sebastian Farquhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varma_V/0/1/0/all/0/1&quot;&gt;Vikrant Varma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kenton_Z/0/1/0/all/0/1&quot;&gt;Zachary Kenton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gasteiger_J/0/1/0/all/0/1&quot;&gt;Johannes Gasteiger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mikulik_V/0/1/0/all/0/1&quot;&gt;Vladimir Mikulik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1&quot;&gt;Rohin Shah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03114">
<title>Concise Fuzzy Planar Embedding of Graphs: a Dimensionality Reduction Approach. (arXiv:1803.03114v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1803.03114</link>
<description rdf:parseType="Literal">&lt;p&gt;The enormous amount of data to be represented using large graphs exceeds in
some cases the resources of a conventional computer. Edges in particular can
take up a considerable amount of memory as compared to the number of nodes.
However, rigorous edge storage might not always be essential to be able to draw
the needed conclusions. A similar problem takes records with many variables and
attempts to extract the most discernible features. It is said that the
``dimension&apos;&apos; of this data is reduced. Following an approach with the same
objective in mind, we can map a graph representation to a $k$-dimensional space
and answer queries of neighboring nodes mainly by measuring Euclidean
distances. The accuracy of our answers would decrease but would be compensated
for by fuzzy logic which gives an idea about the likelihood of error. This
method allows for reasonable representation in memory while maintaining a fair
amount of useful information, and allows for concise embedding in
$k$-dimensional Euclidean space as well as solving some problems without having
to decompress the graph. Of particular interest is the case where $k=2$.
Promising highly accurate experimental results are obtained and reported.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abu_Khzam_F/0/1/0/all/0/1&quot;&gt;Faisal N. Abu-Khzam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mouawi_R/0/1/0/all/0/1&quot;&gt;Rana H. Mouawi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmad_A/0/1/0/all/0/1&quot;&gt;Amer Hajj Ahmad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thoumi_S/0/1/0/all/0/1&quot;&gt;Sergio Thoumi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.10547">
<title>Optimal Data Selection: An Online Distributed View. (arXiv:2201.10547v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2201.10547</link>
<description rdf:parseType="Literal">&lt;p&gt;The blessing of ubiquitous data also comes with a curse: the communication,
storage, and labeling of massive, mostly redundant datasets. We seek to solve
this problem at its core, collecting only valuable data and throwing out the
rest via submodular maximization. Specifically, we develop algorithms for the
online and distributed version of the problem, where data selection occurs in
an uncoordinated fashion across multiple data streams. We design a general and
flexible core selection routine for our algorithms which, given any stream of
data, any assessment of its value, and any formulation of its selection cost,
extracts the most valuable subset of the stream up to a constant factor while
using minimal memory. Notably, our methods have the same theoretical guarantees
as their offline counterparts, and, as far as we know, provide the first
guarantees for online distributed submodular optimization in the literature.
Finally, in learning tasks on ImageNet and MNIST, we show that our selection
methods outperform random selection by $5-20\%$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Werner_M/0/1/0/all/0/1&quot;&gt;Mariel Werner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Angelopoulos_A/0/1/0/all/0/1&quot;&gt;Anastasios Angelopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bates_S/0/1/0/all/0/1&quot;&gt;Stephen Bates&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1&quot;&gt;Michael I. Jordan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.16230">
<title>Evaluation of semantic relations impact in query expansion-based retrieval systems. (arXiv:2203.16230v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2203.16230</link>
<description rdf:parseType="Literal">&lt;p&gt;With the increasing demand of intelligent systems capable of operating in
different contexts (e.g. users on the move) the correct interpretation of the
user-need by such systems has become crucial to give consistent answers to the
user questions. The most effective applications addressing such task are in the
fields of natural language processing and semantic expansion of terms. These
techniques are aimed at estimating the goal of an input query reformulating it
as an intent, commonly relying on textual resources built exploiting different
semantic relations like \emph{synonymy}, \emph{antonymy} and many others. The
aim of this paper is to generate such resources using the labels of a given
taxonomy as source of information. The obtained resources are integrated into a
plain classifier for reformulating a set of input queries as intents and
tracking the effect of each relation, in order to quantify the impact of each
semantic relation on the classification. As an extension to this, the best
tradeoff between improvement and noise introduction when combining such
relations is evaluated. The assessment is made generating the resources and
their combinations and using them for tuning the classifier which is used to
reformulate the user questions as labels. The evaluation employs a wide and
varied taxonomy as a use-case, exploiting its labels as basis for the semantic
expansion and producing several corpora with the purpose of enhancing the
pseudo-queries estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Massai_L/0/1/0/all/0/1&quot;&gt;Lorenzo Massai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.17505">
<title>Space-Fluid Adaptive Sampling by Self-Organisation. (arXiv:2210.17505v5 [cs.DC] UPDATED)</title>
<link>http://arxiv.org/abs/2210.17505</link>
<description rdf:parseType="Literal">&lt;p&gt;A recurrent task in coordinated systems is managing (estimating, predicting,
or controlling) signals that vary in space, such as distributed sensed data or
computation outcomes. Especially in large-scale settings, the problem can be
addressed through decentralised and situated computing systems: nodes can
locally sense, process, and act upon signals, and coordinate with neighbours to
implement collective strategies. Accordingly, in this work we devise
distributed coordination strategies for the estimation of a spatial phenomenon
through collaborative adaptive sampling. Our design is based on the idea of
dynamically partitioning space into regions that compete and grow/shrink to
provide accurate aggregate sampling. Such regions hence define a sort of
virtualised space that is &quot;fluid&quot;, since its structure adapts in response to
pressure forces exerted by the underlying phenomenon. We provide an adaptive
sampling algorithm in the field-based coordination framework, and prove it is
self-stabilising and locally optimal. Finally, we verify by simulation that the
proposed algorithm effectively carries out a spatially adaptive sampling while
maintaining a tuneable trade-off between accuracy and efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casadei_R/0/1/0/all/0/1&quot;&gt;Roberto Casadei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mariani_S/0/1/0/all/0/1&quot;&gt;Stefano Mariani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pianini_D/0/1/0/all/0/1&quot;&gt;Danilo Pianini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Viroli_M/0/1/0/all/0/1&quot;&gt;Mirko Viroli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zambonelli_F/0/1/0/all/0/1&quot;&gt;Franco Zambonelli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.07514">
<title>PulseImpute: A Novel Benchmark Task for Pulsative Physiological Signal Imputation. (arXiv:2212.07514v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.07514</link>
<description rdf:parseType="Literal">&lt;p&gt;The promise of Mobile Health (mHealth) is the ability to use wearable sensors
to monitor participant physiology at high frequencies during daily life to
enable temporally-precise health interventions. However, a major challenge is
frequent missing data. Despite a rich imputation literature, existing
techniques are ineffective for the pulsative signals which comprise many
mHealth applications, and a lack of available datasets has stymied progress. We
address this gap with PulseImpute, the first large-scale pulsative signal
imputation challenge which includes realistic mHealth missingness models, an
extensive set of baselines, and clinically-relevant downstream tasks. Our
baseline models include a novel transformer-based architecture designed to
exploit the structure of pulsative signals. We hope that PulseImpute will
enable the ML community to tackle this significant and challenging task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Maxwell A. Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moreno_A/0/1/0/all/0/1&quot;&gt;Alexander Moreno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagesh_S/0/1/0/all/0/1&quot;&gt;Supriya Nagesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aydemir_V/0/1/0/all/0/1&quot;&gt;V. Burak Aydemir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wetter_D/0/1/0/all/0/1&quot;&gt;David W. Wetter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Santosh Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rehg_J/0/1/0/all/0/1&quot;&gt;James M. Rehg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.05754">
<title>Decomposed Diffusion Sampler for Accelerating Large-Scale Inverse Problems. (arXiv:2303.05754v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.05754</link>
<description rdf:parseType="Literal">&lt;p&gt;Krylov subspace, which is generated by multiplying a given vector by the
matrix of a linear transformation and its successive powers, has been
extensively studied in classical optimization literature to design algorithms
that converge quickly for large linear inverse problems. For example, the
conjugate gradient method (CG), one of the most popular Krylov subspace
methods, is based on the idea of minimizing the residual error in the Krylov
subspace. However, with the recent advancement of high-performance diffusion
solvers for inverse problems, it is not clear how classical wisdom can be
synergistically combined with modern diffusion models. In this study, we
propose a novel and efficient diffusion sampling strategy that synergistically
combine the diffusion sampling and Krylov subspace methods. Specifically, we
prove that if the tangent space at a denoised sample by Tweedie&apos;s formula forms
a Krylov subspace, then the CG initialized with the denoised data ensures the
data consistency update to remain in the tangent space. This negates the need
to compute the manifold-constrained gradient (MCG), leading to a more efficient
diffusion sampling method. Our method is applicable regardless of the
parametrization and setting (i.e., VE, VP). Notably, we achieve
state-of-the-art reconstruction quality on challenging real-world medical
inverse imaging problems, including multi-coil MRI reconstruction and 3D CT
reconstruction. Moreover, our proposed method achieves more than 80 times
faster inference time than the previous state-of-the-art method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1&quot;&gt;Hyungjin Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Suhyeon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jong Chul Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.03724">
<title>GeoTMI:Predicting quantum chemical property with easy-to-obtain geometry via positional denoising. (arXiv:2304.03724v3 [physics.chem-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2304.03724</link>
<description rdf:parseType="Literal">&lt;p&gt;As quantum chemical properties have a dependence on their geometries, graph
neural networks (GNNs) using 3D geometric information have achieved high
prediction accuracy in many tasks. However, they often require 3D geometries
obtained from high-level quantum mechanical calculations, which are practically
infeasible, limiting their applicability to real-world problems. To tackle
this, we propose a new training framework, GeoTMI, that employs denoising
process to predict properties accurately using easy-to-obtain geometries
(corrupted versions of correct geometries, such as those obtained from
low-level calculations). Our starting point was the idea that the correct
geometry is the best description of the target property. Hence, to incorporate
information of the correct, GeoTMI aims to maximize mutual information between
three variables: the correct and the corrupted geometries and the property.
GeoTMI also explicitly updates the corrupted input to approach the correct
geometry as it passes through the GNN layers, contributing to more effective
denoising. We investigated the performance of the proposed method using 3D GNNs
for three prediction tasks: molecular properties, a chemical reaction property,
and relaxed energy in a heterogeneous catalytic system. Our results showed
consistent improvements in accuracy across various tasks, demonstrating the
effectiveness and robustness of GeoTMI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyeonsu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Woo_J/0/1/0/all/0/1&quot;&gt;Jeheon Woo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seonghwan Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Moon_S/0/1/0/all/0/1&quot;&gt;Seokhyun Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jun Hyeong Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kim_W/0/1/0/all/0/1&quot;&gt;Woo Youn Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.05836">
<title>A Game-theoretic Framework for Privacy-preserving Federated Learning. (arXiv:2304.05836v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.05836</link>
<description rdf:parseType="Literal">&lt;p&gt;In federated learning, benign participants aim to optimize a global model
collaboratively. However, the risk of \textit{privacy leakage} cannot be
ignored in the presence of \textit{semi-honest} adversaries. Existing research
has focused either on designing protection mechanisms or on inventing attacking
mechanisms. While the battle between defenders and attackers seems
never-ending, we are concerned with one critical question: is it possible to
prevent potential attacks in advance? To address this, we propose the first
game-theoretic framework that considers both FL defenders and attackers in
terms of their respective payoffs, which include computational costs, FL model
utilities, and privacy leakage risks. We name this game the federated learning
privacy game (FLPG), in which neither defenders nor attackers are aware of all
participants&apos; payoffs.
&lt;/p&gt;
&lt;p&gt;To handle the \textit{incomplete information} inherent in this situation, we
propose associating the FLPG with an \textit{oracle} that has two primary
responsibilities. First, the oracle provides lower and upper bounds of the
payoffs for the players. Second, the oracle acts as a correlation device,
privately providing suggested actions to each player. With this novel
framework, we analyze the optimal strategies of defenders and attackers.
Furthermore, we derive and demonstrate conditions under which the attacker, as
a rational decision-maker, should always follow the oracle&apos;s suggestion
\textit{not to attack}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaojin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1&quot;&gt;Lixin Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Siwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenjie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qiang Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.08754">
<title>W-MAE: Pre-trained weather model with masked autoencoder for multi-variable weather forecasting. (arXiv:2304.08754v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.08754</link>
<description rdf:parseType="Literal">&lt;p&gt;Weather forecasting is a long-standing computational challenge with direct
societal and economic impacts. This task involves a large amount of continuous
data collection and exhibits rich spatiotemporal dependencies over long
periods, making it highly suitable for deep learning models. In this paper, we
apply pre-training techniques to weather forecasting and propose W-MAE, a
Weather model with Masked AutoEncoder pre-training for weather forecasting.
W-MAE is pre-trained in a self-supervised manner to reconstruct spatial
correlations within meteorological variables. On the temporal scale, we
fine-tune the pre-trained W-MAE to predict the future states of meteorological
variables, thereby modeling the temporal dependencies present in weather data.
We conduct our experiments using the fifth-generation ECMWF Reanalysis (ERA5)
data, with samples selected every six hours. Experimental results show that our
W-MAE framework offers three key benefits: 1) when predicting the future state
of meteorological variables, the utilization of our pre-trained W-MAE can
effectively alleviate the problem of cumulative errors in prediction,
maintaining stable performance in the short-to-medium term; 2) when predicting
diagnostic variables (e.g., total precipitation), our model exhibits
significant performance advantages over FourCastNet; 3) Our task-agnostic
pre-training schema can be easily integrated with various task-specific models.
When our pre-training framework is applied to FourCastNet, it yields an average
20% performance improvement in Anomaly Correlation Coefficient (ACC).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Man_X/0/1/0/all/0/1&quot;&gt;Xin Man&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chenghong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jin Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Changyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1&quot;&gt;Jie Shao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05421">
<title>DC3DCD: unsupervised learning for multiclass 3D point cloud change detection. (arXiv:2305.05421v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05421</link>
<description rdf:parseType="Literal">&lt;p&gt;In a constant evolving world, change detection is of prime importance to keep
updated maps. To better sense areas with complex geometry (urban areas in
particular), considering 3D data appears to be an interesting alternative to
classical 2D images. In this context, 3D point clouds (PCs), whether obtained
through LiDAR or photogrammetric techniques, provide valuable information.
While recent studies showed the considerable benefit of using deep
learning-based methods to detect and characterize changes into raw 3D PCs,
these studies rely on large annotated training data to obtain accurate results.
The collection of these annotations are tricky and time-consuming. The
availability of unsupervised or weakly supervised approaches is then of prime
interest. In this paper, we propose an unsupervised method, called DeepCluster
3D Change Detection (DC3DCD), to detect and categorize multiclass changes at
point level. We classify our approach in the unsupervised family given the fact
that we extract in a completely unsupervised way a number of clusters
associated with potential changes. Let us precise that in the end of the
process, the user has only to assign a label to each of these clusters to
derive the final change map. Our method builds upon the DeepCluster approach,
originally designed for image classification, to handle complex raw 3D PCs and
perform change segmentation task. An assessment of the method on both simulated
and real public dataset is provided. The proposed method allows to outperform
fully-supervised traditional machine learning algorithm and to be competitive
with fully-supervised deep learning networks applied on rasterization of 3D PCs
with a mean of IoU over classes of change of 57.06\% and 66.69\% for the
simulated and the real datasets, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gelis_I/0/1/0/all/0/1&quot;&gt;Iris de G&amp;#xe9;lis&lt;/a&gt; (1 and 2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lefevre_S/0/1/0/all/0/1&quot;&gt;S&amp;#xe9;bastien Lef&amp;#xe8;vre&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corpetti_T/0/1/0/all/0/1&quot;&gt;Thomas Corpetti&lt;/a&gt; (3) ((1) Magellium, (2) Institut de Recherche en Informatique et Syst&amp;#xe8;mes Al&amp;#xe9;atoires IRISA - UMR 6074 - Universit&amp;#xe9; Bretagne Sud, (3) Littoral - Environnement - T&amp;#xe9;l&amp;#xe9;d&amp;#xe9;tection - G&amp;#xe9;omatique LETG - UMR 6554 - Universit&amp;#xe9; Rennes 2)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.11476">
<title>Learning Diverse Risk Preferences in Population-based Self-play. (arXiv:2305.11476v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.11476</link>
<description rdf:parseType="Literal">&lt;p&gt;Among the great successes of Reinforcement Learning (RL), self-play
algorithms play an essential role in solving competitive games. Current
self-play algorithms optimize the agent to maximize expected win-rates against
its current or historical copies, making it often stuck in the local optimum
and its strategy style simple and homogeneous. A possible solution is to
improve the diversity of policies, which helps the agent break the stalemate
and enhances its robustness when facing different opponents. However, enhancing
diversity in the self-play algorithms is not trivial. In this paper, we aim to
introduce diversity from the perspective that agents could have diverse risk
preferences in the face of uncertainty. Specifically, we design a novel
reinforcement learning algorithm called Risk-sensitive Proximal Policy
Optimization (RPPO), which smoothly interpolates between worst-case and
best-case policy learning and allows for policy learning with desired risk
preferences. Seamlessly integrating RPPO with population-based self-play,
agents in the population optimize dynamic risk-sensitive objectives with
experiences from playing against diverse opponents. Empirical results show that
our method achieves comparable or superior performance in competitive games and
that diverse modes of behaviors emerge. Our code is public online at
\url{https://github.com/Jackory/RPBT}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yuhua Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qihan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiaoteng Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenghao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yiqin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_B/0/1/0/all/0/1&quot;&gt;Bin Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qianchuan Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13030">
<title>Adaptive action supervision in reinforcement learning from real-world multi-agent demonstrations. (arXiv:2305.13030v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13030</link>
<description rdf:parseType="Literal">&lt;p&gt;Modeling of real-world biological multi-agents is a fundamental problem in
various scientific and engineering fields. Reinforcement learning (RL) is a
powerful framework to generate flexible and diverse behaviors in cyberspace;
however, when modeling real-world biological multi-agents, there is a domain
gap between behaviors in the source (i.e., real-world data) and the target
(i.e., cyberspace for RL), and the source environment parameters are usually
unknown. In this paper, we propose a method for adaptive action supervision in
RL from real-world demonstrations in multi-agent scenarios. We adopt an
approach that combines RL and supervised learning by selecting actions of
demonstrations in RL based on the minimum distance of dynamic time warping for
utilizing the information of the unknown source dynamics. This approach can be
easily applied to many existing neural network architectures and provide us
with an RL model balanced between reproducibility as imitation and
generalization ability to obtain rewards in cyberspace. In the experiments,
using chase-and-escape and football tasks with the different dynamics between
the unknown source and target environments, we show that our approach achieved
a balance between the reproducibility and the generalization ability compared
with the baselines. In particular, we used the tracking data of professional
football players as expert demonstrations in football and show successful
performances despite the larger gap between behaviors in the source and target
environments than the chase-and-escape task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fujii_K/0/1/0/all/0/1&quot;&gt;Keisuke Fujii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsutsui_K/0/1/0/all/0/1&quot;&gt;Kazushi Tsutsui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scott_A/0/1/0/all/0/1&quot;&gt;Atom Scott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakahara_H/0/1/0/all/0/1&quot;&gt;Hiroshi Nakahara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takeishi_N/0/1/0/all/0/1&quot;&gt;Naoya Takeishi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawahara_Y/0/1/0/all/0/1&quot;&gt;Yoshinobu Kawahara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15835">
<title>PDE+: Enhancing Generalization via PDE with Adaptive Distributional Diffusion. (arXiv:2305.15835v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15835</link>
<description rdf:parseType="Literal">&lt;p&gt;The generalization of neural networks is a central challenge in machine
learning, especially concerning the performance under distributions that differ
from training ones. Current methods, mainly based on the data-driven paradigm
such as data augmentation, adversarial training, and noise injection, may
encounter limited generalization due to model non-smoothness. In this paper, we
propose to investigate generalization from a Partial Differential Equation
(PDE) perspective, aiming to enhance it directly through the underlying
function of neural networks, rather than focusing on adjusting input data.
Specifically, we first establish the connection between neural network
generalization and the smoothness of the solution to a specific PDE, namely
&quot;transport equation&quot;. Building upon this, we propose a general framework that
introduces adaptive distributional diffusion into transport equation to enhance
the smoothness of its solution, thereby improving generalization. In the
context of neural networks, we put this theoretical framework into practice as
$\textbf{PDE+}$ ($\textbf{PDE}$ with $\textbf{A}$daptive
$\textbf{D}$istributional $\textbf{D}$iffusion) which diffuses each sample into
a distribution covering semantically similar inputs. This enables better
coverage of potentially unobserved distributions in training, thus improving
generalization beyond merely data-driven methods. The effectiveness of PDE+ is
validated through extensive experimental settings, demonstrating its superior
performance compared to SOTA methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yige Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1&quot;&gt;Bingbing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Bo Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1&quot;&gt;Liang Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1&quot;&gt;Fei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Huawei Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xueqi Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04308">
<title>Personality testing of GPT-3: Limited temporal reliability, but highlighted social desirability of GPT-3&apos;s personality instruments results. (arXiv:2306.04308v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04308</link>
<description rdf:parseType="Literal">&lt;p&gt;As AI-bots continue to gain popularity due to their human-like traits and the
intimacy they offer to users, their societal impact inevitably expands. This
leads to the rising necessity for comprehensive studies to fully understand
AI-bots and reveal their potential opportunities, drawbacks, and overall
societal impact. With that in mind, this research conducted an extensive
investigation into ChatGPT3, a renowned AI bot, aiming to assess the temporal
reliability of its personality profile. Psychological questionnaires were
administered to the chatbot on two separate occasions, followed by a comparison
of the responses to human normative data. The findings revealed varying levels
of agreement in chatbot&apos;s responses over time, with some scales displaying
excellent agreement while others demonstrated poor agreement. Overall,
Davinci-003 displayed a socially desirable and pro-social personality profile,
particularly in the domain of communion. However, the underlying basis of the
chatbot&apos;s responses-whether driven by conscious self reflection or
predetermined algorithms-remains uncertain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bodroza_B/0/1/0/all/0/1&quot;&gt;Bojana Bodroza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dinic_B/0/1/0/all/0/1&quot;&gt;Bojana M. Dinic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bojic_L/0/1/0/all/0/1&quot;&gt;Ljubisa Bojic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14448">
<title>Progressive Energy-Based Cooperative Learning for Multi-Domain Image-to-Image Translation. (arXiv:2306.14448v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14448</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies a novel energy-based cooperative learning framework for
multi-domain image-to-image translation. The framework consists of four
components: descriptor, translator, style encoder, and style generator. The
descriptor is a multi-head energy-based model that represents a multi-domain
image distribution. The components of translator, style encoder, and style
generator constitute a diversified image generator. Specifically, given an
input image from a source domain, the translator turns it into a stylised
output image of the target domain according to a style code, which can be
inferred by the style encoder from a reference image or produced by the style
generator from a random noise. Since the style generator is represented as an
domain-specific distribution of style codes, the translator can provide a
one-to-many transformation (i.e., diversified generation) between source domain
and target domain. To train our framework, we propose a likelihood-based
multi-domain cooperative learning algorithm to jointly train the multi-domain
descriptor and the diversified image generator (including translator, style
encoder, and style generator modules) via multi-domain MCMC teaching, in which
the descriptor guides the diversified image generator to shift its probability
density toward the data distribution, while the diversified image generator
uses its randomly translated images to initialize the descriptor&apos;s Langevin
dynamics process for efficient sampling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1&quot;&gt;Weinan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yaxuan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1&quot;&gt;Lei He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yingnian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1&quot;&gt;Jianwen Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13332">
<title>The Optimal Approximation Factors in Misspecified Off-Policy Value Function Estimation. (arXiv:2307.13332v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13332</link>
<description rdf:parseType="Literal">&lt;p&gt;Theoretical guarantees in reinforcement learning (RL) are known to suffer
multiplicative blow-up factors with respect to the misspecification error of
function approximation. Yet, the nature of such \emph{approximation factors} --
especially their optimal form in a given learning problem -- is poorly
understood. In this paper we study this question in linear off-policy value
function estimation, where many open questions remain. We study the
approximation factor in a broad spectrum of settings, such as with the weighted
$L_2$-norm (where the weighting is the offline state distribution), the
$L_\infty$ norm, the presence vs. absence of state aliasing, and full vs.
partial coverage of the state space. We establish the optimal asymptotic
approximation factors (up to constants) for all of these settings. In
particular, our bounds identify two instance-dependent factors for the
$L_2(\mu)$ norm and only one for the $L_\infty$ norm, which are shown to
dictate the hardness of off-policy evaluation under misspecification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amortila_P/0/1/0/all/0/1&quot;&gt;Philip Amortila&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1&quot;&gt;Nan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1&quot;&gt;Csaba Szepesv&amp;#xe1;ri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.16387">
<title>Relation-Oriented: Toward Causal Knowledge-Aligned AGI. (arXiv:2307.16387v13 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.16387</link>
<description rdf:parseType="Literal">&lt;p&gt;The potential surge of causal reasoning in AI models toward AGI is imminent,
given the impending saturation of observation-based applications in fields like
image and language processing. It is both critical and underrecognized that the
essence of causality lies in the temporal nonlinearity (i.e., dynamics) of
causal effects. Capturing such featured causal representations is key to
realizing AGI. This paper advocates for a thorough reevaluation and potential
overhaul of existing causal inference theories and the traditional learning
paradigm, which predominantly relies on the observational i.i.d assumption. Our
aim is to align these theories and methodologies with the intrinsic demands of
AGI development. We introduce a novel Relation-Oriented paradigm for
relationship modeling, and the Relation-Indexed Representation Learning (RIRL)
method as its foundational implementation. Extensive experiments confirm RIRL&apos;s
efficacy in autonomously capturing dynamical effects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04237">
<title>Federated Inference with Reliable Uncertainty Quantification over Wireless Channels via Conformal Prediction. (arXiv:2308.04237v2 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/2308.04237</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we consider a wireless federated inference scenario in which
devices and a server share a pre-trained machine learning model. The devices
communicate statistical information about their local data to the server over a
common wireless channel, aiming to enhance the quality of the inference
decision at the server. Recent work has introduced federated conformal
prediction (CP), which leverages devices-to-server communication to improve the
reliability of the server&apos;s decision. With federated CP, devices communicate to
the server information about the loss accrued by the shared pre-trained model
on the local data, and the server leverages this information to calibrate a
decision interval, or set, so that it is guaranteed to contain the correct
answer with a pre-defined target reliability level. Previous work assumed
noise-free communication, whereby devices can communicate a single real number
to the server. In this paper, we study for the first time federated CP in a
wireless setting. We introduce a novel protocol, termed wireless federated
conformal prediction (WFCP), which builds on type-based multiple access (TBMA)
and on a novel quantile correction strategy. WFCP is proved to provide formal
reliability guarantees in terms of coverage of the predicted set produced by
the server. Using numerical results, we demonstrate the significant advantages
of WFCP against digital implementations of existing federated CP schemes,
especially in regimes with limited communication resources and/or large number
of devices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1&quot;&gt;Meiyi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zecchin_M/0/1/0/all/0/1&quot;&gt;Matteo Zecchin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Sangwoo Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1&quot;&gt;Caili Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1&quot;&gt;Chunyan Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simeone_O/0/1/0/all/0/1&quot;&gt;Osvaldo Simeone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.10822">
<title>A Novel Ehanced Move Recognition Algorithm Based on Pre-trained Models with Positional Embeddings. (arXiv:2308.10822v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.10822</link>
<description rdf:parseType="Literal">&lt;p&gt;The recognition of abstracts is crucial for effectively locating the content
and clarifying the article. Existing move recognition algorithms lack the
ability to learn word position information to obtain contextual semantics. This
paper proposes a novel enhanced move recognition algorithm with an improved
pre-trained model and a gated network with attention mechanism for unstructured
abstracts of Chinese scientific and technological papers. The proposed
algorithm first performs summary data segmentation and vocabulary training. The
EP-ERNIE$\_$AT-GRU framework is leveraged to incorporate word positional
information, facilitating deep semantic learning and targeted feature
extraction. Experimental results demonstrate that the proposed algorithm
achieves 13.37$\%$ higher accuracy on the split dataset than on the original
dataset and a 7.55$\%$ improvement in accuracy over the basic comparison model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1&quot;&gt;Hao Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_X/0/1/0/all/0/1&quot;&gt;Xiaodong Qiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12367">
<title>SafeAR: Towards Safer Algorithmic Recourse by Risk-Aware Policies. (arXiv:2308.12367v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12367</link>
<description rdf:parseType="Literal">&lt;p&gt;With the growing use of machine learning (ML) models in critical domains such
as finance and healthcare, the need to offer recourse for those adversely
affected by the decisions of ML models has become more important; individuals
ought to be provided with recommendations on actions to take for improving
their situation and thus receiving a favorable decision. Prior work on
sequential algorithmic recourse -- which recommends a series of changes --
focuses on action feasibility and uses the proximity of feature changes to
determine action costs. However, the uncertainties of feature changes and the
risk of higher than average costs in recourse have not been considered. It is
undesirable if a recourse could (with some probability) result in a worse
situation from which recovery requires an extremely high cost. It is essential
to incorporate risks when computing and evaluating recourse. We call the
recourse computed with such risk considerations as Safer Algorithmic Recourse
(SafeAR). The objective is to empower people to choose a recourse based on
their risk tolerance. In this work, we discuss and show how existing recourse
desiderata can fail to capture the risk of higher costs. We present a method to
compute recourse policies that consider variability in cost and connect
algorithmic recourse literature with risk-sensitive reinforcement learning. We
also adopt measures &quot;Value at Risk&quot; and &quot;Conditional Value at Risk&quot; from the
financial literature to summarize risk concisely. We apply our method to two
real-world datasets and compare policies with different risk-aversion levels
using risk measures and recourse desiderata (sparsity and proximity).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Haochen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1&quot;&gt;Shubham Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patra_S/0/1/0/all/0/1&quot;&gt;Sunandita Patra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gopalakrishnan_S/0/1/0/all/0/1&quot;&gt;Sriram Gopalakrishnan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.02730">
<title>Stylebook: Content-Dependent Speaking Style Modeling for Any-to-Any Voice Conversion using Only Speech Data. (arXiv:2309.02730v3 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2309.02730</link>
<description rdf:parseType="Literal">&lt;p&gt;While many recent any-to-any voice conversion models succeed in transferring
some target speech&apos;s style information to the converted speech, they still lack
the ability to faithfully reproduce the speaking style of the target speaker.
In this work, we propose a novel method to extract rich style information from
target utterances and to efficiently transfer it to source speech content
without requiring text transcriptions or speaker labeling. Our proposed
approach introduces an attention mechanism utilizing a self-supervised learning
(SSL) model to collect the speaking styles of a target speaker each
corresponding to the different phonetic content. The styles are represented
with a set of embeddings called stylebook. In the next step, the stylebook is
attended with the source speech&apos;s phonetic content to determine the final
target style for each source content. Finally, content information extracted
from the source speech and content-dependent target style embeddings are fed
into a diffusion-based decoder to generate the converted speech
mel-spectrogram. Experiment results show that our proposed method combined with
a diffusion-based generative model can achieve better speaker similarity in
any-to-any voice conversion tasks when compared to baseline models, while the
increase in computational complexity with longer utterances is suppressed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lim_H/0/1/0/all/0/1&quot;&gt;Hyungseob Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Byun_K/0/1/0/all/0/1&quot;&gt;Kyungguen Byun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Moon_S/0/1/0/all/0/1&quot;&gt;Sunkuk Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Visser_E/0/1/0/all/0/1&quot;&gt;Erik Visser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.04339">
<title>Online Submodular Maximization via Online Convex Optimization. (arXiv:2309.04339v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.04339</link>
<description rdf:parseType="Literal">&lt;p&gt;We study monotone submodular maximization under general matroid constraints
in the online setting. We prove that online optimization of a large class of
submodular functions, namely, weighted threshold potential functions, reduces
to online convex optimization (OCO). This is precisely because functions in
this class admit a concave relaxation; as a result, OCO policies, coupled with
an appropriate rounding scheme, can be used to achieve sublinear regret in the
combinatorial setting. We show that our reduction extends to many different
versions of the online learning problem, including the dynamic regret, bandit,
and optimistic-learning settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Si_Salem_T/0/1/0/all/0/1&quot;&gt;Tareq Si-Salem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozcan_G/0/1/0/all/0/1&quot;&gt;G&amp;#xf6;zde &amp;#xd6;zcan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikolaou_I/0/1/0/all/0/1&quot;&gt;Iasonas Nikolaou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Terzi_E/0/1/0/all/0/1&quot;&gt;Evimaria Terzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ioannidis_S/0/1/0/all/0/1&quot;&gt;Stratis Ioannidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07473">
<title>Where2Explore: Few-shot Affordance Learning for Unseen Novel Categories of Articulated Objects. (arXiv:2309.07473v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2309.07473</link>
<description rdf:parseType="Literal">&lt;p&gt;Articulated object manipulation is a fundamental yet challenging task in
robotics. Due to significant geometric and semantic variations across object
categories, previous manipulation models struggle to generalize to novel
categories. Few-shot learning is a promising solution for alleviating this
issue by allowing robots to perform a few interactions with unseen objects.
However, extant approaches often necessitate costly and inefficient test-time
interactions with each unseen instance. Recognizing this limitation, we observe
that despite their distinct shapes, different categories often share similar
local geometries essential for manipulation, such as pullable handles and
graspable edges - a factor typically underutilized in previous few-shot
learning works. To harness this commonality, we introduce &apos;Where2Explore&apos;, an
affordance learning framework that effectively explores novel categories with
minimal interactions on a limited number of instances. Our framework explicitly
estimates the geometric similarity across different categories, identifying
local areas that differ from shapes in the training categories for efficient
exploration while concurrently transferring affordance knowledge to similar
parts of the objects. Extensive experiments in simulated and real-world
environments demonstrate our framework&apos;s capacity for efficient few-shot
exploration and generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_C/0/1/0/all/0/1&quot;&gt;Chuanruo Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1&quot;&gt;Ruihai Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Haoran Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mo_K/0/1/0/all/0/1&quot;&gt;Kaichun Mo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hao Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07597">
<title>C-Pack: Packaged Resources To Advance General Chinese Embedding. (arXiv:2309.07597v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.07597</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce C-Pack, a package of resources that significantly advance the
field of general Chinese embeddings. C-Pack includes three critical resources.
1) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6
tasks and 35 datasets. 2) C-MTP is a massive text embedding dataset curated
from labeled and unlabeled Chinese corpora for training embedding models. 3)
C-TEM is a family of embedding models covering multiple sizes. Our models
outperform all prior Chinese text embeddings on C-MTEB by up to +10% upon the
time of the release. We also integrate and optimize the entire suite of
training methods for C-TEM. Along with our resources on general Chinese
embedding, we release our data and models for English text embeddings. The
English models achieve state-of-the-art performance on MTEB benchmark;
meanwhile, our released English data is 2 times larger than the Chinese data.
All these resources are made publicly available at
https://github.com/FlagOpen/FlagEmbedding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1&quot;&gt;Shitao Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Peitian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muennighoff_N/0/1/0/all/0/1&quot;&gt;Niklas Muennighoff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11236">
<title>Colour Passing Revisited: Lifted Model Construction with Commutative Factors. (arXiv:2309.11236v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2309.11236</link>
<description rdf:parseType="Literal">&lt;p&gt;Lifted probabilistic inference exploits symmetries in a probabilistic model
to allow for tractable probabilistic inference with respect to domain sizes. To
apply lifted inference, a lifted representation has to be obtained, and to do
so, the so-called colour passing algorithm is the state of the art. The colour
passing algorithm, however, is bound to a specific inference algorithm and we
found that it ignores commutativity of factors while constructing a lifted
representation. We contribute a modified version of the colour passing
algorithm that uses logical variables to construct a lifted representation
independent of a specific inference algorithm while at the same time exploiting
commutativity of factors during an offline-step. Our proposed algorithm
efficiently detects more symmetries than the state of the art and thereby
drastically increases compression, yielding significantly faster online query
times for probabilistic inference when the resulting model is applied.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luttermann_M/0/1/0/all/0/1&quot;&gt;Malte Luttermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braun_T/0/1/0/all/0/1&quot;&gt;Tanya Braun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moller_R/0/1/0/all/0/1&quot;&gt;Ralf M&amp;#xf6;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gehrke_M/0/1/0/all/0/1&quot;&gt;Marcel Gehrke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03708">
<title>Beyond One-Preference-Fits-All Alignment: Multi-Objective Direct Preference Optimization. (arXiv:2310.03708v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03708</link>
<description rdf:parseType="Literal">&lt;p&gt;A single language model (LM), despite aligning well with an average labeler
through reinforcement learning from human feedback (RLHF), may not universally
suit diverse human preferences. Recent approaches therefore opt for
customization by collecting multi-dimensional feedback and creating distinct
reward models (RMs) for each dimension (e.g., helpfulness, harmlessness, or
honesty). Different LMs can then be optimized for different preferences using
multi-objective RLHF (MORLHF) with different reward weightings. Yet, RL
fine-tuning is unstable and resource-heavy, especially for MORLHF with diverse
and usually conflicting objectives. In this paper, we present Multi-Objective
Direct Preference Optimization (MODPO), an RL-free algorithm that extends
Direct Preference Optimization (DPO) for multiple alignment objectives with
minimal overheads. Essentially, MODPO folds language modeling directly into
reward modeling, training LMs as implicit collective reward models (cRMs) that
combine all objectives with specific weightings. While theoretically guaranteed
to produce the same optimal solutions as MORLHF, MODPO is practically more
stable and computationally efficient. Empirical results from safety alignment
and long-form question answering confirm that MODPO matches or outperforms
existing methods, consistently producing a Pareto front of LMs that cater to
diverse preferences with 3 times less computational resources compared to
MORLHF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhanhui Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1&quot;&gt;Jing Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1&quot;&gt;Xiangyu Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1&quot;&gt;Wanli Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04687">
<title>Understanding and Improving Adversarial Attacks on Latent Diffusion Model. (arXiv:2310.04687v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04687</link>
<description rdf:parseType="Literal">&lt;p&gt;Latent Diffusion Model (LDM) achieves state-of-the-art performances in image
generation yet raising copyright and privacy concerns. Adversarial attacks on
LDM are then born to protect unauthorized images from being used in LDM-driven
few-shot generation. However, these attacks suffer from moderate performance
and excessive computational cost, especially in GPU memory. In this paper, we
propose an effective adversarial attack on LDM that shows superior performance
against state-of-the-art few-shot generation pipeline of LDM, for example,
LoRA. We implement the attack with memory efficiency by introducing several
mechanisms and decrease the memory cost of the attack to less than 6GB, which
allows individual users to run the attack on a majority of consumer GPUs. Our
proposed attack can be a practical tool for people facing the copyright and
privacy risk brought by LDM to protect themselves.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1&quot;&gt;Boyang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1&quot;&gt;Chumeng Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15539">
<title>SteloCoder: a Decoder-Only LLM for Multi-Language to Python Code Translation. (arXiv:2310.15539v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15539</link>
<description rdf:parseType="Literal">&lt;p&gt;With the recent focus on Large Language Models (LLMs), both StarCoder (Li et
al., 2023) and Code Llama (Rozi\`ere et al., 2023) have demonstrated remarkable
performance in code generation. However, there is still a need for improvement
in code translation functionality with efficient training techniques. In
response to this, we introduce SteloCoder, a decoder-only StarCoder-based LLM
designed specifically for multi-programming language-to-Python code
translation. In particular, SteloCoder achieves C++, C#, JavaScript, Java, or
PHP-to-Python code translation without specifying the input programming
language. We modified StarCoder model architecture by incorporating a
Mixture-of-Experts (MoE) technique featuring five experts and a gating network
for multi-task handling. Experts are obtained by StarCoder fine-tuning.
Specifically, we use a Low-Rank Adaptive Method (LoRA) technique, limiting each
expert size as only 0.06% of number of StarCoder&apos;s parameters. At the same
time, to enhance training efficiency in terms of time, we adopt curriculum
learning strategy and use self-instruct data for efficient fine-tuning. As a
result, each expert takes only 6 hours to train on one single 80Gb A100 HBM.
With experiments on XLCoST datasets, SteloCoder achieves an average of 73.76
CodeBLEU score in multi-programming language-to-Python translation, surpassing
the top performance from the leaderboard by at least 3.5. This accomplishment
is attributed to only 45M extra parameters with StarCoder as the backbone and
32 hours of valid training on one 80GB A100 HBM. The source code is release
here: https://github.com/sade-adrien/SteloCoder.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Jialing Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sade_A/0/1/0/all/0/1&quot;&gt;Adrien Sad&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soriano_E/0/1/0/all/0/1&quot;&gt;Eric Soriano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sole_G/0/1/0/all/0/1&quot;&gt;Guillem Sole&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flamant_S/0/1/0/all/0/1&quot;&gt;Sylvain Flamant&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15950">
<title>Representation Learning with Large Language Models for Recommendation. (arXiv:2310.15950v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15950</link>
<description rdf:parseType="Literal">&lt;p&gt;Recommender systems have seen significant advancements with the influence of
deep learning and graph neural networks, particularly in capturing complex
user-item relationships. However, these graph-based recommenders heavily depend
on ID-based data, potentially disregarding valuable textual information
associated with users and items, resulting in less informative learned
representations. Moreover, the utilization of implicit feedback data introduces
potential noise and bias, posing challenges for the effectiveness of user
preference learning. While the integration of large language models (LLMs) into
traditional ID-based recommenders has gained attention, challenges such as
scalability issues, limitations in text-only reliance, and prompt input
constraints need to be addressed for effective implementation in practical
recommender systems. To address these challenges, we propose a model-agnostic
framework RLMRec that aims to enhance existing recommenders with LLM-empowered
representation learning. It proposes a recommendation paradigm that integrates
representation learning with LLMs to capture intricate semantic aspects of user
behaviors and preferences. RLMRec incorporates auxiliary textual signals,
develops a user/item profiling paradigm empowered by LLMs, and aligns the
semantic space of LLMs with the representation space of collaborative
relational signals through a cross-view alignment framework. This work further
establish a theoretical foundation demonstrating that incorporating textual
signals through mutual information maximization enhances the quality of
representations. In our evaluation, we integrate RLMRec with state-of-the-art
recommender models, while also analyzing its efficiency and robustness to noise
data. Our implementation codes are available at
https://github.com/HKUDS/RLMRec.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xubin Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1&quot;&gt;Wei Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_L/0/1/0/all/0/1&quot;&gt;Lianghao Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1&quot;&gt;Lixin Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;Suqi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junfeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1&quot;&gt;Dawei Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chao Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16218">
<title>Knowledge Editing for Large Language Models: A Survey. (arXiv:2310.16218v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.16218</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have recently transformed both the academic and
industrial landscapes due to their remarkable capacity to understand, analyze,
and generate texts based on their vast knowledge and reasoning ability.
Nevertheless, one major drawback of LLMs is their substantial computational
cost for pre-training due to their unprecedented amounts of parameters. The
disadvantage is exacerbated when new knowledge frequently needs to be
introduced into the pre-trained model. Therefore, it is imperative to develop
effective and efficient techniques to update pre-trained LLMs. Traditional
methods encode new knowledge in pre-trained LLMs through direct fine-tuning.
However, naively re-training LLMs can be computationally intensive and risks
degenerating valuable pre-trained knowledge irrelevant to the update in the
model. Recently, Knowledge-based Model Editing (KME) has attracted increasing
attention, which aims to precisely modify the LLMs to incorporate specific
knowledge, without negatively influencing other irrelevant knowledge. In this
survey, we aim to provide a comprehensive and in-depth overview of recent
advances in the field of KME. We first introduce a general formulation of KME
to encompass different KME strategies. Afterward, we provide an innovative
taxonomy of KME techniques based on how the new knowledge is introduced into
pre-trained LLMs, and investigate existing KME strategies while analyzing key
insights, advantages, and limitations of methods from each category. Moreover,
representative metrics, datasets, and applications of KME are introduced
accordingly. Finally, we provide an in-depth analysis regarding the
practicality and remaining challenges of KME and suggest promising research
directions for further advancement in this field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Song Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yaochen Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haochen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1&quot;&gt;Zaiyi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jundong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18333">
<title>She had Cobalt Blue Eyes: Prompt Testing to Create Aligned and Sustainable Language Models. (arXiv:2310.18333v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18333</link>
<description rdf:parseType="Literal">&lt;p&gt;As the use of large language models (LLMs) increases within society, as does
the risk of their misuse. Appropriate safeguards must be in place to ensure LLM
outputs uphold the ethical standards of society, highlighting the positive role
that artificial intelligence technologies can have. Recent events indicate
ethical concerns around conventionally trained LLMs, leading to overall unsafe
user experiences. This motivates our research question: how do we ensure LLM
alignment? In this work, we introduce a test suite of unique prompts to foster
the development of aligned LLMs that are fair, safe, and robust. We show that
prompting LLMs at every step of the development pipeline, including data
curation, pre-training, and fine-tuning, will result in an overall more
responsible model. Our test suite evaluates outputs from four state-of-the-art
language models: GPT-3.5, GPT-4, OPT, and LLaMA-2. The assessment presented in
this paper highlights a gap between societal alignment and the capabilities of
current LLMs. Additionally, implementing a test suite such as ours lowers the
environmental overhead of making models safe and fair.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatrath_V/0/1/0/all/0/1&quot;&gt;Veronica Chatrath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bamgbose_O/0/1/0/all/0/1&quot;&gt;Oluwanifemi Bamgbose&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raza_S/0/1/0/all/0/1&quot;&gt;Shaina Raza&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02544">
<title>Nonlinear Multi-objective Reinforcement Learning with Provable Guarantees. (arXiv:2311.02544v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02544</link>
<description rdf:parseType="Literal">&lt;p&gt;We describe RA-E3 (Reward-Aware Explicit Explore or Exploit), an algorithm
with provable guarantees for solving a single or multi-objective Markov
Decision Process (MDP) where we want to maximize the expected value of a
nonlinear function over accumulated rewards. This allows us to model
fairness-aware welfare optimization for multi-objective reinforcement learning
as well as risk-aware reinforcement learning with nonlinear Von
Neumann-Morgenstern utility functions in the single objective setting. RA-E3
extends the classic E3 algorithm that solves MDPs with scalar rewards and
linear preferences. We first state a distinct reward-aware version of value
iteration that calculates a non-stationary policy that is approximately optimal
for a given model of the environment. This sub-procedure is based on an
extended form of Bellman optimality for nonlinear optimization that explicitly
considers time and current accumulated reward. We then describe how to use this
optimization procedure in a larger algorithm that must simultaneously learn a
model of the environment. The algorithm learns an approximately optimal policy
in time that depends polynomially on the MDP size, desired approximation, and
smoothness of the nonlinear function, and exponentially on the number of
objectives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1&quot;&gt;Nianli Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fain_B/0/1/0/all/0/1&quot;&gt;Brandon Fain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05741">
<title>Efficiently Adapting Pretrained Language Models To New Languages. (arXiv:2311.05741v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.05741</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent large language models (LLM) exhibit sub-optimal performance on
low-resource languages, as the training data of these models is usually
dominated by English and other high-resource languages. Furthermore, it is
challenging to train models for low-resource languages, especially from
scratch, due to a lack of high quality training data. Adapting pretrained LLMs
reduces the need for data in the new language while also providing cross
lingual transfer capabilities. However, naively adapting to new languages leads
to catastrophic forgetting and poor tokenizer efficiency. In this work, we
study how to efficiently adapt any existing pretrained LLM to a new language
without running into these issues. In particular, we improve the encoding
efficiency of the tokenizer by adding new tokens from the target language and
study the data mixing recipe to mitigate forgetting. Our experiments on
adapting an English LLM to Hungarian and Thai show that our recipe can reach
better performance than open source models on the target language, with minimal
regressions on English.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Csaki_Z/0/1/0/all/0/1&quot;&gt;Zoltan Csaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pawakapan_P/0/1/0/all/0/1&quot;&gt;Pian Pawakapan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thakker_U/0/1/0/all/0/1&quot;&gt;Urmish Thakker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qiantong Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06330">
<title>Smart Agent-Based Modeling: On the Use of Large Language Models in Computer Simulations. (arXiv:2311.06330v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.06330</link>
<description rdf:parseType="Literal">&lt;p&gt;Computer simulations offer a robust toolset for exploring complex systems
across various disciplines. A particularly impactful approach within this realm
is Agent-Based Modeling (ABM), which harnesses the interactions of individual
agents to emulate intricate system dynamics. ABM&apos;s strength lies in its
bottom-up methodology, illuminating emergent phenomena by modeling the
behaviors of individual components of a system. Yet, ABM has its own set of
challenges, notably its struggle with modeling natural language instructions
and common sense in mathematical equations or rules. This paper seeks to
transcend these boundaries by integrating Large Language Models (LLMs) like GPT
into ABM. This amalgamation gives birth to a novel framework, Smart Agent-Based
Modeling (SABM). Building upon the concept of smart agents -- entities
characterized by their intelligence, adaptability, and computation ability --
we explore in the direction of utilizing LLM-powered agents to simulate
real-world scenarios with increased nuance and realism. In this comprehensive
exploration, we elucidate the state of the art of ABM, introduce SABM&apos;s
potential and methodology, and present three case studies (source codes
available at https://github.com/Roihn/SABM), demonstrating the SABM methodology
and validating its effectiveness in modeling real-world systems. Furthermore,
we cast a vision towards several aspects of the future of SABM, anticipating a
broader horizon for its applications. Through this endeavor, we aspire to
redefine the boundaries of computer simulations, enabling a more profound
understanding of complex systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zengqing Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_R/0/1/0/all/0/1&quot;&gt;Run Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xu Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Shuyuan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yixin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Chuan Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10329">
<title>High-fidelity Person-centric Subject-to-Image Synthesis. (arXiv:2311.10329v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10329</link>
<description rdf:parseType="Literal">&lt;p&gt;Current subject-driven image generation methods encounter significant
challenges in person-centric image generation. The reason is that they learn
the semantic scene and person generation by fine-tuning a common pre-trained
diffusion, which involves an irreconcilable training imbalance. Precisely, to
generate realistic persons, they need to sufficiently tune the pre-trained
model, which inevitably causes the model to forget the rich semantic scene
prior and makes scene generation over-fit to the training data. Moreover, even
with sufficient fine-tuning, these methods can still not generate high-fidelity
persons since joint learning of the scene and person generation also lead to
quality compromise. In this paper, we propose Face-diffuser, an effective
collaborative generation pipeline to eliminate the above training imbalance and
quality compromise. Specifically, we first develop two specialized pre-trained
diffusion models, i.e., Text-driven Diffusion Model (TDM) and Subject-augmented
Diffusion Model (SDM), for scene and person generation, respectively. The
sampling process is divided into three sequential stages, i.e., semantic scene
construction, subject-scene fusion, and subject enhancement. The first and last
stages are performed by TDM and SDM respectively. The subject-scene fusion
stage, that is the collaboration achieved through a novel and highly effective
mechanism, Saliency-adaptive Noise Fusion (SNF). Specifically, it is based on
our key observation that there exists a robust link between classifier-free
guidance responses and the saliency of generated images. In each time step, SNF
leverages the unique strengths of each model and allows for the spatial
blending of predicted noises from both models automatically in a saliency-aware
manner. Extensive experiments confirm the impressive effectiveness and
robustness of the Face-diffuser.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yibin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weizhong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Jianwei Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1&quot;&gt;Cheng Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15786">
<title>YUAN 2.0: A Large Language Model with Localized Filtering-based Attention. (arXiv:2311.15786v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.15786</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we develop and release Yuan 2.0, a series of large language
models with parameters ranging from 2.1 billion to 102.6 billion. The Localized
Filtering-based Attention (LFA) is introduced to incorporate prior knowledge of
local dependencies of natural language into Attention. A data filtering and
generating system is presented to build pre-training and fine-tuning dataset in
high quality. A distributed training method with non-uniform pipeline parallel,
data parallel, and optimizer parallel is proposed, which greatly reduces the
bandwidth requirements of intra-node communication, and achieves good
performance in large-scale distributed training. Yuan 2.0 models display
impressive ability in code generation, math problem-solving, and chatting
compared with existing models. The latest version of YUAN 2.0, including model
weights and source code, is accessible at Github.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Shaohua Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xudong Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shenling Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Jiangang Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lingjun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1&quot;&gt;Bing Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1&quot;&gt;Tong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rongguo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiahua Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16973">
<title>DemoFusion: Democratising High-Resolution Image Generation With No $$$. (arXiv:2311.16973v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16973</link>
<description rdf:parseType="Literal">&lt;p&gt;High-resolution image generation with Generative Artificial Intelligence
(GenAI) has immense potential but, due to the enormous capital investment
required for training, it is increasingly centralised to a few large
corporations, and hidden behind paywalls. This paper aims to democratise
high-resolution GenAI by advancing the frontier of high-resolution generation
while remaining accessible to a broad audience. We demonstrate that existing
Latent Diffusion Models (LDMs) possess untapped potential for higher-resolution
image generation. Our novel DemoFusion framework seamlessly extends open-source
GenAI models, employing Progressive Upscaling, Skip Residual, and Dilated
Sampling mechanisms to achieve higher-resolution image generation. The
progressive nature of DemoFusion requires more passes, but the intermediate
results can serve as &quot;previews&quot;, facilitating rapid prompt iteration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_R/0/1/0/all/0/1&quot;&gt;Ruoyi Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1&quot;&gt;Dongliang Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1&quot;&gt;Timothy Hospedales&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yi-Zhe Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zhanyu Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17104">
<title>Single-Cell Deep Clustering Method Assisted by Exogenous Gene Information: A Novel Approach to Identifying Cell Types. (arXiv:2311.17104v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17104</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the field of single-cell data analysis has seen a marked
advancement in the development of clustering methods. Despite advancements,
most of these algorithms still concentrate on analyzing the provided
single-cell matrix data. However, in medical applications, single-cell data
often involves a wealth of exogenous information, including gene networks.
Overlooking this aspect could lead to information loss and clustering results
devoid of significant clinical relevance. An innovative single-cell deep
clustering method, incorporating exogenous gene information, has been proposed
to overcome this limitation. This model leverages exogenous gene network
information to facilitate the clustering process, generating discriminative
representations. Specifically, we have developed an attention-enhanced graph
autoencoder, which is designed to efficiently capture the topological features
between cells. Concurrently, we conducted a random walk on an exogenous
Protein-Protein Interaction (PPI) network, thereby acquiring the gene&apos;s
topological features. Ultimately, during the clustering process, we integrated
both sets of information and reconstructed the features of both cells and genes
to generate a discriminative representation. Extensive experiments have
validated the effectiveness of our proposed method. This research offers
enhanced insights into the characteristics and distribution of cells, thereby
laying the groundwork for early diagnosis and treatment of diseases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1&quot;&gt;Dayu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1&quot;&gt;Ke Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinwang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17401">
<title>Gene-MOE: A Sparsely-gated Framework for Pan-Cancer Genomic Analysis. (arXiv:2311.17401v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17401</link>
<description rdf:parseType="Literal">&lt;p&gt;Benefiting from the advancements in deep learning, various genomic analytical
techniques, such as survival analysis, classification of tumors and their
subtypes, and exploration of specific pathways, have significantly enhanced our
understanding of the biological mechanisms driving cancer. However, the
overfitting issue, arising from the limited number of patient samples, poses a
challenge in improving the accuracy of genome analysis by deepening the neural
network. Furthermore, it remains uncertain whether novel approaches such as the
sparsely gated mixture of expert (MOE) and self-attention mechanisms can
improve the accuracy of genomic analysis. In this paper, we introduce a novel
sparsely gated RNA-seq analysis framework called Gene-MOE. This framework
exploits the potential of the MOE layers and the proposed mixture of attention
expert (MOAE) layers to enhance the analysis accuracy. Additionally, it
addresses overfitting challenges by integrating pan-cancer information from 33
distinct cancer types through pre-training.We pre-trained Gene-MOE on TCGA
pan-cancer RNA-seq dataset with 33 cancer types. Subsequently, we conducted
experiments involving cancer classification and survival analysis based on the
pre-trained Gene-MOE. According to the survival analysis results on 14 cancer
types, Gene-MOE outperformed state-of-the-art models on 12 cancer types.
Through detailed feature analysis, we found that the Gene-MOE model could learn
rich feature representations of high-dimensional genes. According to the
classification results, the total accuracy of the classification model for 33
cancer classifications reached 95.8%, representing the best performance
compared to state-of-the-art models. These results indicate that Gene-MOE holds
strong potential for use in cancer classification and survival analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1&quot;&gt;Xiangyu Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xue Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1&quot;&gt;Huanhuan Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_L/0/1/0/all/0/1&quot;&gt;Lian Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Hongzhen Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_L/0/1/0/all/0/1&quot;&gt;Long Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xun Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04103">
<title>Enhancing the Rationale-Input Alignment for Self-explaining Rationalization. (arXiv:2312.04103v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04103</link>
<description rdf:parseType="Literal">&lt;p&gt;Rationalization empowers deep learning models with self-explaining
capabilities through a cooperative game, where a generator selects a
semantically consistent subset of the input as a rationale, and a subsequent
predictor makes predictions based on the selected rationale. In this paper, we
discover that rationalization is prone to a problem named \emph{rationale
shift}, which arises from the algorithmic bias of the cooperative game.
Rationale shift refers to a situation where the semantics of the selected
rationale may deviate from the original input, but the predictor still produces
accurate predictions based on the deviation, resulting in a compromised
generator with misleading feedback.
&lt;/p&gt;
&lt;p&gt;To address this issue, we first demonstrate the importance of the alignment
between the rationale and the full input through both empirical observations
and theoretical analysis. Subsequently, we introduce a novel approach called
DAR (\textbf{D}iscriminatively \textbf{A}ligned \textbf{R}ationalization),
which utilizes an auxiliary module pretrained on the full input to
discriminatively align the selected rationale and the original input. We
theoretically illustrate how DAR accomplishes the desired alignment, thereby
overcoming the rationale shift problem. The experiments on two widely used
real-world benchmarks show that the proposed method significantly improves the
explanation quality (measured by the overlap between the model-selected
explanation and the human-annotated rationale) as compared to state-of-the-art
techniques. Additionally, results on two synthetic settings further validate
the effectiveness of DAR in addressing the rationale shift problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haozhao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1&quot;&gt;Zhiying Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;YuanKai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Cheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ruixuan Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06371">
<title>BAT: Behavior-Aware Human-Like Trajectory Prediction for Autonomous Driving. (arXiv:2312.06371v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06371</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to accurately predict the trajectory of surrounding vehicles is a
critical hurdle to overcome on the journey to fully autonomous vehicles. To
address this challenge, we pioneer a novel behavior-aware trajectory prediction
model (BAT) that incorporates insights and findings from traffic psychology,
human behavior, and decision-making. Our model consists of behavior-aware,
interaction-aware, priority-aware, and position-aware modules that perceive and
understand the underlying interactions and account for uncertainty and
variability in prediction, enabling higher-level learning and flexibility
without rigid categorization of driving behavior. Importantly, this approach
eliminates the need for manual labeling in the training process and addresses
the challenges of non-continuous behavior labeling and the selection of
appropriate time windows. We evaluate BAT&apos;s performance across the Next
Generation Simulation (NGSIM), Highway Drone (HighD), Roundabout Drone (RounD),
and Macao Connected Autonomous Driving (MoCAD) datasets, showcasing its
superiority over prevailing state-of-the-art (SOTA) benchmarks in terms of
prediction accuracy and efficiency. Remarkably, even when trained on reduced
portions of the training data (25%), our model outperforms most of the
baselines, demonstrating its robustness and efficiency in predicting vehicle
trajectories, and the potential to reduce the amount of data required to train
autonomous vehicles, especially in corner cases. In conclusion, the
behavior-aware model represents a significant advancement in the development of
autonomous vehicles capable of predicting trajectories with the same level of
proficiency as human drivers. The project page is available at
https://github.com/Petrichor625/BATraj-Behavior-aware-Model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1&quot;&gt;Haicheng Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenning Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Huanming Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1&quot;&gt;Wenxuan Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_D/0/1/0/all/0/1&quot;&gt;Dongping Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guofa Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shengbo Eben Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chengzhong Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06718">
<title>Large Scale Foundation Models for Intelligent Manufacturing Applications: A Survey. (arXiv:2312.06718v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06718</link>
<description rdf:parseType="Literal">&lt;p&gt;Although the applications of artificial intelligence especially deep learning
had greatly improved various aspects of intelligent manufacturing, they still
face challenges for wide employment due to the poor generalization ability,
difficulties to establish high-quality training datasets, and unsatisfactory
performance of deep learning methods. The emergence of large scale foundational
models(LSFMs) had triggered a wave in the field of artificial intelligence,
shifting deep learning models from single-task, single-modal, limited data
patterns to a paradigm encompassing diverse tasks, multimodal, and pre-training
on massive datasets. Although LSFMs had demonstrated powerful generalization
capabilities, automatic high-quality training dataset generation and superior
performance across various domains, applications of LSFMs on intelligent
manufacturing were still in their nascent stage. A systematic overview of this
topic was lacking, especially regarding which challenges of deep learning can
be addressed by LSFMs and how these challenges can be systematically tackled.
To fill this gap, this paper systematically expounded current statue of LSFMs
and their advantages in the context of intelligent manufacturing. and compared
comprehensively with the challenges faced by current deep learning models in
various intelligent manufacturing applications. We also outlined the roadmaps
for utilizing LSFMs to address these challenges. Finally, case studies of
applications of LSFMs in real-world intelligent manufacturing scenarios were
presented to illustrate how LSFMs could help industries, improve their
efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haotian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dereck_S/0/1/0/all/0/1&quot;&gt;Semujju Stuart Dereck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhicheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1&quot;&gt;Xianwei Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Liang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1&quot;&gt;Ye Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jing Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_Z/0/1/0/all/0/1&quot;&gt;Zhuo Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1&quot;&gt;Wensheng Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;X.G. Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_R/0/1/0/all/0/1&quot;&gt;Ruiyan Zhuang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07559">
<title>PaperQA: Retrieval-Augmented Generative Agent for Scientific Research. (arXiv:2312.07559v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07559</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) generalize well across language tasks, but
suffer from hallucinations and uninterpretability, making it difficult to
assess their accuracy without ground-truth. Retrieval-Augmented Generation
(RAG) models have been proposed to reduce hallucinations and provide provenance
for how an answer was generated. Applying such models to the scientific
literature may enable large-scale, systematic processing of scientific
knowledge. We present PaperQA, a RAG agent for answering questions over the
scientific literature. PaperQA is an agent that performs information retrieval
across full-text scientific articles, assesses the relevance of sources and
passages, and uses RAG to provide answers. Viewing this agent as a question
answering model, we find it exceeds performance of existing LLMs and LLM agents
on current science QA benchmarks. To push the field closer to how humans
perform research on scientific literature, we also introduce LitQA, a more
complex benchmark that requires retrieval and synthesis of information from
full-text scientific papers across the literature. Finally, we demonstrate
PaperQA&apos;s matches expert human researchers on LitQA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lala_J/0/1/0/all/0/1&quot;&gt;Jakub L&amp;#xe1;la&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+ODonoghue_O/0/1/0/all/0/1&quot;&gt;Odhran O&amp;#x27;Donoghue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shtedritski_A/0/1/0/all/0/1&quot;&gt;Aleksandar Shtedritski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cox_S/0/1/0/all/0/1&quot;&gt;Sam Cox&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriques_S/0/1/0/all/0/1&quot;&gt;Samuel G. Rodriques&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+White_A/0/1/0/all/0/1&quot;&gt;Andrew D. White&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08078">
<title>Fine-Grained Image-Text Alignment in Medical Imaging Enables Cyclic Image-Report Generation. (arXiv:2312.08078v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08078</link>
<description rdf:parseType="Literal">&lt;p&gt;To address these issues, we propose a novel Adaptive patch-word Matching
(AdaMatch) model to correlate chest X-ray (CXR) image regions with words in
medical reports and apply it to CXR-report generation to provide explainability
for the generation process. AdaMatch exploits the fine-grained relation between
adaptive patches and words to provide explanations of specific image regions
with corresponding words. To capture the abnormal regions of varying sizes and
positions, we introduce the Adaptive Patch extraction (AdaPatch) module to
acquire the adaptive patches for these regions adaptively. In order to provide
explicit explainability for CXR-report generation task, we propose an
AdaMatch-based bidirectional large language model for Cyclic CXR-report
generation (AdaMatch-Cyclic). It employs the AdaMatch to obtain the keywords
for CXR images and `keypatches&apos; for medical reports as hints to guide
CXR-report generation. Extensive experiments on two publicly available CXR
datasets prove the effectiveness of our method and its superior performance to
existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenting Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Linlin Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yixuan Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08084">
<title>A Novel Energy based Model Mechanism for Multi-modal Aspect-Based Sentiment Analysis. (arXiv:2312.08084v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08084</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-modal aspect-based sentiment analysis (MABSA) has recently attracted
increasing attention. The span-based extraction methods, such as FSUIE,
demonstrate strong performance in sentiment analysis due to their joint
modeling of input sequences and target labels. However, previous methods still
have certain limitations: (i) They ignore the difference in the focus of visual
information between different analysis targets (aspect or sentiment). (ii)
Combining features from uni-modal encoders directly may not be sufficient to
eliminate the modal gap and can cause difficulties in capturing the image-text
pairwise relevance. (iii) Existing span-based methods for MABSA ignore the
pairwise relevance of target span boundaries. To tackle these limitations, we
propose a novel framework called DQPSA for multi-modal sentiment analysis.
Specifically, our model contains a Prompt as Dual Query (PDQ) module that uses
the prompt as both a visual query and a language query to extract prompt-aware
visual information and strengthen the pairwise relevance between visual
information and the analysis target. Additionally, we introduce an Energy-based
Pairwise Expert (EPE) module that models the boundaries pairing of the analysis
target from the perspective of an Energy-based Model. This expert predicts
aspect or sentiment span based on pairwise stability. Experiments on three
widely used benchmarks demonstrate that DQPSA outperforms previous approaches
and achieves a new state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_T/0/1/0/all/0/1&quot;&gt;Tianshuo Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zuchao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Ping Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lefei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hai Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08274">
<title>High-throughput Biomedical Relation Extraction for Semi-Structured Web Articles Empowered by Large Language Models. (arXiv:2312.08274v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08274</link>
<description rdf:parseType="Literal">&lt;p&gt;Objective: To develop a high-throughput biomedical relation extraction system
that takes advantage of the large language models&apos; (LLMs) reading comprehension
ability and biomedical world knowledge in a scalable and evidential manner.
Methods: We formulate the relation extraction task as a simple binary
classification problem for large language models such as ChatGPT. Specifically,
LLMs make the decision based on the external corpus and its world knowledge,
giving the reason for the judgment to factual verification. This method is
tailored for semi-structured web articles, wherein we designate the main title
as the tail entity and explicitly incorporate it into the context, and the
potential head entities are matched based on a biomedical thesaurus. Moreover,
lengthy contents are sliced into text chunks, embedded, and retrieved with
additional embedding models, ensuring compatibility with the context window
size constraints of available open-source LLMs. Results: Using an open-source
LLM, we extracted 304315 relation triplets of three distinct relation types
from four reputable biomedical websites. To assess the efficacy of the basic
pipeline employed for biomedical relation extraction, we curated a benchmark
dataset annotated by a medical expert. Evaluation results indicate that the
pipeline exhibits performance comparable to that of GPT-4. Case studies further
illuminate challenges faced by contemporary LLMs in the context of biomedical
relation extraction for semi-structured web articles. Conclusion: The proposed
method has demonstrated its effectiveness in leveraging the strengths of LLMs
for high-throughput biomedical relation extraction. Its adaptability is
evident, as it can be seamlessly extended to diverse semi-structured biomedical
websites, facilitating the extraction of various types of biomedical relations
with ease.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Songchi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Sheng Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08282">
<title>Prompting LLMs with content plans to enhance the summarization of scientific articles. (arXiv:2312.08282v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08282</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents novel prompting techniques to improve the performance of
automatic summarization systems for scientific articles. Scientific article
summarization is highly challenging due to the length and complexity of these
documents. We conceive, implement, and evaluate prompting techniques that
provide additional contextual information to guide summarization systems.
Specifically, we feed summarizers with lists of key terms extracted from
articles, such as author keywords or automatically generated keywords. Our
techniques are tested with various summarization models and input texts.
Results show performance gains, especially for smaller models summarizing
sections separately. This evidences that prompting is a promising approach to
overcoming the limitations of less powerful systems. Our findings introduce a
new research direction of using prompts to aid smaller models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Creo_A/0/1/0/all/0/1&quot;&gt;Aldan Creo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lama_M/0/1/0/all/0/1&quot;&gt;Manuel Lama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vidal_J/0/1/0/all/0/1&quot;&gt;Juan C. Vidal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08374">
<title>Unsupervised Social Event Detection via Hybrid Graph Contrastive Learning and Reinforced Incremental Clustering. (arXiv:2312.08374v2 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08374</link>
<description rdf:parseType="Literal">&lt;p&gt;Detecting events from social media data streams is gradually attracting
researchers. The innate challenge for detecting events is to extract
discriminative information from social media data thereby assigning the data
into different events. Due to the excessive diversity and high updating
frequency of social data, using supervised approaches to detect events from
social messages is hardly achieved. To this end, recent works explore learning
discriminative information from social messages by leveraging graph contrastive
learning (GCL) and embedding clustering in an unsupervised manner. However, two
intrinsic issues exist in benchmark methods: conventional GCL can only roughly
explore partial attributes, thereby insufficiently learning the discriminative
information of social messages; for benchmark methods, the learned embeddings
are clustered in the latent space by taking advantage of certain specific prior
knowledge, which conflicts with the principle of unsupervised learning
paradigm. In this paper, we propose a novel unsupervised social media event
detection method via hybrid graph contrastive learning and reinforced
incremental clustering (HCRC), which uses hybrid graph contrastive learning to
comprehensively learn semantic and structural discriminative information from
social messages and reinforced incremental clustering to perform efficient
clustering in a solidly unsupervised manner. We conduct comprehensive
experiments to evaluate HCRC on the Twitter and Maven datasets. The
experimental results demonstrate that our approach yields consistent
significant performance boosts. In traditional incremental setting,
semi-supervised incremental setting and solidly unsupervised setting, the model
performance has achieved maximum improvements of 53%, 45%, and 37%,
respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yuanyuan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zang_Z/0/1/0/all/0/1&quot;&gt;Zehua Zang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1&quot;&gt;Hang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lixiang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiangmeng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08672">
<title>CAT: A Causally Graph Attention Network for Trimming Heterophilic Graph. (arXiv:2312.08672v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08672</link>
<description rdf:parseType="Literal">&lt;p&gt;Local Attention-guided Message Passing Mechanism (LAMP) adopted in Graph
Attention Networks (GATs) is designed to adaptively learn the importance of
neighboring nodes for better local aggregation on the graph, which can bring
the representations of similar neighbors closer effectively, thus showing
stronger discrimination ability. However, existing GATs suffer from a
significant discrimination ability decline in heterophilic graphs because the
high proportion of dissimilar neighbors can weaken the self-attention of the
central node, jointly resulting in the deviation of the central node from
similar nodes in the representation space. This kind of effect generated by
neighboring nodes is called the Distraction Effect (DE) in this paper. To
estimate and weaken the DE of neighboring nodes, we propose a Causally graph
Attention network for Trimming heterophilic graph (CAT). To estimate the DE,
since the DE are generated through two paths (grab the attention assigned to
neighbors and reduce the self-attention of the central node), we use Total
Effect to model DE, which is a kind of causal estimand and can be estimated
from intervened data; To weaken the DE, we identify the neighbors with the
highest DE (we call them Distraction Neighbors) and remove them. We adopt three
representative GATs as the base model within the proposed CAT framework and
conduct experiments on seven heterophilic datasets in three different sizes.
Comparative experiments show that CAT can improve the node classification
accuracy of all base GAT models. Ablation experiments and visualization further
validate the enhancement of discrimination ability brought by CAT. The source
code is available at https://github.com/GeoX-Lab/CAT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1&quot;&gt;Silu He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Q/0/1/0/all/0/1&quot;&gt;Qinyao Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1&quot;&gt;Xinsha Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Ling Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_R/0/1/0/all/0/1&quot;&gt;Ronghua Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haifeng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08688">
<title>TigerBot: An Open Multilingual Multitask LLM. (arXiv:2312.08688v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08688</link>
<description rdf:parseType="Literal">&lt;p&gt;We release and introduce the TigerBot family of large language models (LLMs),
consisting of base and chat models, sized from 7, 13, 70 and 180 billion
parameters. We develop our models embarking from Llama-2 and BLOOM, and push
the boundary further in data, training algorithm, infrastructure, and
application tools. Our models yield meaningful performance gain over SOTA
open-source models, e.g., Llama-2, specifically 6% gain in English and 20% gain
in Chinese. TigerBot model family also achieves leading performance in major
academic and industrial benchmarks and leaderboards. We believe that TigerBot
represents just a snapshot of lightning-fast progression in LLM open-source
community. Therefore, we are thrilled to give back by publicly releasing our
models and reporting our approach behind, with additional emphases on building
SOTA LLMs in a democratized way and making LLMs of use in real-world
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Ye Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1&quot;&gt;Wei Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Liangmin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaowei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xin_Z/0/1/0/all/0/1&quot;&gt;Zhanxuan Xin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1&quot;&gt;Cong Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08702">
<title>Rational Sensibility: LLM Enhanced Empathetic Response Generation Guided by Self-presentation Theory. (arXiv:2312.08702v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08702</link>
<description rdf:parseType="Literal">&lt;p&gt;Having the ability to empathize is crucial for accurately representing human
behavior during conversations. Despite numerous research aim to improve the
cognitive capability of models by incorporating external knowledge, there has
been limited attention on the sensible and rational expression of the
conversation itself, which are crucial components of the cognitive empathy.
Guided by self-presentation theory in sociology, we have designed an innovative
categorical approach that segregates historical dialogues into sensible and
rational sentences and subsequently elucidate the context through the designed
attention mechanism. However, the rational information within the conversation
is restricted and the external knowledge used in previous methods have
limitations of semantic contradiction and narrow vision field. Considering the
impressive performance of LLM in the domain of intelligent agent. We employ
LLaMA2-70b as a rational brain to analyze the profound logical information
maintained in conversations, which assists the model assessing the balance of
sensibility and rationality to produce quality empathetic responses.
Experimental evaluations demonstrate that our method outperforms other
comparable methods on both automatic and human evaluations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Linzhuang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1&quot;&gt;Nan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1&quot;&gt;Jingxuan Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Bihui Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bu_L/0/1/0/all/0/1&quot;&gt;Liping Bu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yin Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08782">
<title>Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis. (arXiv:2312.08782v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08782</link>
<description rdf:parseType="Literal">&lt;p&gt;Building general-purpose robots that can operate seamlessly, in any
environment, with any object, and utilizing various skills to complete diverse
tasks has been a long-standing goal in Artificial Intelligence. Unfortunately,
however, most existing robotic systems have been constrained - having been
designed for specific tasks, trained on specific datasets, and deployed within
specific environments. These systems usually require extensively-labeled data,
rely on task-specific models, have numerous generalization issues when deployed
in real-world scenarios, and struggle to remain robust to distribution shifts.
Motivated by the impressive open-set performance and content generation
capabilities of web-scale, large-capacity pre-trained models (i.e., foundation
models) in research fields such as Natural Language Processing (NLP) and
Computer Vision (CV), we devote this survey to exploring (i) how these existing
foundation models from NLP and CV can be applied to the field of robotics, and
also exploring (ii) what a robotics-specific foundation model would look like.
We begin by providing an overview of what constitutes a conventional robotic
system and the fundamental barriers to making it universally applicable. Next,
we establish a taxonomy to discuss current work exploring ways to leverage
existing foundation models for robotics and develop ones catered to robotics.
Finally, we discuss key challenges and promising future directions in using
foundation models for enabling general-purpose robotic systems. We encourage
readers to view our living GitHub repository of resources, including papers
reviewed in this survey as well as related projects and repositories for
developing foundation models for robotics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yafei Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1&quot;&gt;Quanting Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1&quot;&gt;Vidhi Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1&quot;&gt;Jonathan Francis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patrikar_J/0/1/0/all/0/1&quot;&gt;Jay Patrikar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keetha_N/0/1/0/all/0/1&quot;&gt;Nikhil Keetha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seungchan Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yaqi Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shibo Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chong_Y/0/1/0/all/0/1&quot;&gt;Yu Quan Chong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sycara_K/0/1/0/all/0/1&quot;&gt;Katia Sycara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johnson_Roberson_M/0/1/0/all/0/1&quot;&gt;Matthew Johnson-Roberson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1&quot;&gt;Dhruv Batra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaolong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1&quot;&gt;Sebastian Scherer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1&quot;&gt;Zsolt Kira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1&quot;&gt;Fei Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1&quot;&gt;Yonatan Bisk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08931">
<title>N-Gram Unsupervised Compoundation and Feature Injection for Better Symbolic Music Understanding. (arXiv:2312.08931v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08931</link>
<description rdf:parseType="Literal">&lt;p&gt;The first step to apply deep learning techniques for symbolic music
understanding is to transform musical pieces (mainly in MIDI format) into
sequences of predefined tokens like note pitch, note velocity, and chords.
Subsequently, the sequences are fed into a neural sequence model to accomplish
specific tasks. Music sequences exhibit strong correlations between adjacent
elements, making them prime candidates for N-gram techniques from Natural
Language Processing (NLP). Consider classical piano music: specific melodies
might recur throughout a piece, with subtle variations each time. In this
paper, we propose a novel method, NG-Midiformer, for understanding symbolic
music sequences that leverages the N-gram approach. Our method involves first
processing music pieces into word-like sequences with our proposed unsupervised
compoundation, followed by using our N-gram Transformer encoder, which can
effectively incorporate N-gram information to enhance the primary encoder part
for better understanding of music sequences. The pre-training process on
large-scale music datasets enables the model to thoroughly learn the N-gram
information contained within music sequences, and subsequently apply this
information for making inferences during the fine-tuning stage. Experiment on
various datasets demonstrate the effectiveness of our method and achieved
state-of-the-art performance on a series of music understanding downstream
tasks. The code and model weights will be released at
https://github.com/CinqueOrigin/NG-Midiformer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1&quot;&gt;Jinhao Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zuchao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiajia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Ping Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08948">
<title>LSTM Network Analysis of Vehicle-Type Fatalities on Great Britain&apos;s Roads. (arXiv:2312.08948v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08948</link>
<description rdf:parseType="Literal">&lt;p&gt;This study harnesses the predictive capabilities of Long Short-Term Memory
(LSTM) networks to analyse and predict road traffic accidents in Great Britain.
It addresses the challenge of traffic accident forecasting, which is paramount
for devising effective preventive measures. We utilised an extensive dataset
encompassing reported collisions, casualties, and vehicles involvements from
1926 to 2022, provided by the Department for Transport (DfT). The data
underwent stringent processing to rectify missing values and normalise
features, ensuring robust LSTM network input.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oketunji_A/0/1/0/all/0/1&quot;&gt;Abiodun Finbarrs Oketunji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanify_J/0/1/0/all/0/1&quot;&gt;James Hanify&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heffron_Smith_S/0/1/0/all/0/1&quot;&gt;Salter Heffron-Smith&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09162">
<title>Approximation Algorithms for Preference Aggregation Using CP-Nets. (arXiv:2312.09162v2 [cs.CC] UPDATED)</title>
<link>http://arxiv.org/abs/2312.09162</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies the design and analysis of approximation algorithms for
aggregating preferences over combinatorial domains, represented using
Conditional Preference Networks (CP-nets). Its focus is on aggregating
preferences over so-called \emph{swaps}, for which optimal solutions in general
are already known to be of exponential size. We first analyze a trivial
2-approximation algorithm that simply outputs the best of the given input
preferences, and establish a structural condition under which the approximation
ratio of this algorithm is improved to $4/3$. We then propose a polynomial-time
approximation algorithm whose outputs are provably no worse than those of the
trivial algorithm, but often substantially better. A family of problem
instances is presented for which our improved algorithm produces optimal
solutions, while, for any $\varepsilon$, the trivial algorithm can\emph{not}\/
attain a $(2-\varepsilon)$-approximation. These results may lead to the first
polynomial-time approximation algorithm that solves the CP-net aggregation
problem for swaps with an approximation ratio substantially better than $2$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1&quot;&gt;Abu Mohammmad Hammad Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Boting Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zilles_S/0/1/0/all/0/1&quot;&gt;Sandra Zilles&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>