<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-08-31T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16215" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16246" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16258" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16269" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16271" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16274" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16278" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16280" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16316" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16323" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16325" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16354" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16355" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16376" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16380" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16383" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16386" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16404" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16435" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16454" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16460" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16461" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16463" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16466" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16477" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16481" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16484" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16486" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16490" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16510" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16512" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16518" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16527" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16528" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16530" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16532" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16551" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16552" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16555" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16561" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16567" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16568" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16571" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16572" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16573" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16576" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16582" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16598" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16611" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16612" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16632" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16633" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16635" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16637" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16648" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16649" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16651" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16682" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16684" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16689" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16714" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16725" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16735" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16738" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16739" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16741" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16742" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16758" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16777" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16801" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16819" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16825" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16847" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16863" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16875" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16876" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16880" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16890" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16891" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16893" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16894" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16896" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16905" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16906" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16909" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16911" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1912.10122" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.08060" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.04053" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.07741" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.13085" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.00780" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.01887" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.00646" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.07864" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.10892" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.11827" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.02611" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.00752" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.03022" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.07669" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.00262" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.05102" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.05800" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.06681" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.07806" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.08566" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.12059" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.12743" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.13241" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.03384" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.05821" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.06028" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.06720" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.09949" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.13455" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.14505" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05602" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05970" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.07283" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.08396" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10474" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15777" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.20091" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15142" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17595" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00038" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07873" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.12751" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14863" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.06791" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.09331" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.10161" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.10280" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.10902" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12319" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.14500" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.14604" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.15690" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.15791" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.15816" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.15975" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16139" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16145" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16154" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2308.16215">
<title>Deep Video Codec Control. (arXiv:2308.16215v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.16215</link>
<description rdf:parseType="Literal">&lt;p&gt;Lossy video compression is commonly used when transmitting and storing video
data. Unified video codecs (e.g., H.264 or H.265) remain the \emph{de facto}
standard, despite the availability of advanced (neural) compression approaches.
Transmitting videos in the face of dynamic network bandwidth conditions
requires video codecs to adapt to vastly different compression strengths. Rate
control modules augment the codec&apos;s compression such that bandwidth constraints
are satisfied and video distortion is minimized. While, both standard video
codes and their rate control modules are developed to minimize video distortion
w.r.t. human quality assessment, preserving the downstream performance of deep
vision models is not considered. In this paper, we present the first end-to-end
learnable deep video codec control considering both bandwidth constraints and
downstream vision performance, while not breaking existing standardization. We
demonstrate for two common vision tasks (semantic segmentation and optical flow
estimation) and on two different datasets that our deep codec control better
preserves downstream performance than using 2-pass average bit rate control
while meeting dynamic bandwidth constraints and adhering to standardizations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Reich_C/0/1/0/all/0/1&quot;&gt;Christoph Reich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Debnath_B/0/1/0/all/0/1&quot;&gt;Biplob Debnath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Patel_D/0/1/0/all/0/1&quot;&gt;Deep Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Prangemeier_T/0/1/0/all/0/1&quot;&gt;Tim Prangemeier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chakradhar_S/0/1/0/all/0/1&quot;&gt;Srimat Chakradhar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16246">
<title>Active Neural Mapping. (arXiv:2308.16246v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16246</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the problem of active mapping with a continually-learned neural
scene representation, namely Active Neural Mapping. The key lies in actively
finding the target space to be explored with efficient agent movement, thus
minimizing the map uncertainty on-the-fly within a previously unseen
environment. In this paper, we examine the weight space of the
continually-learned neural field, and show empirically that the neural
variability, the prediction robustness against random weight perturbation, can
be directly utilized to measure the instant uncertainty of the neural map.
Together with the continuous geometric information inherited in the neural map,
the agent can be guided to find a traversable path to gradually gain knowledge
of the environment. We present for the first time an active mapping system with
a coordinate-based implicit neural representation for online scene
reconstruction. Experiments in the visually-realistic Gibson and Matterport3D
environment demonstrate the efficacy of the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zike Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Haoxiang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1&quot;&gt;Hongbin Zha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16258">
<title>Robust Principles: Architectural Design Principles for Adversarially Robust CNNs. (arXiv:2308.16258v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16258</link>
<description rdf:parseType="Literal">&lt;p&gt;Our research aims to unify existing works&apos; diverging opinions on how
architectural components affect the adversarial robustness of CNNs. To
accomplish our goal, we synthesize a suite of three generalizable robust
architectural design principles: (a) optimal range for depth and width
configurations, (b) preferring convolutional over patchify stem stage, and (c)
robust residual block design through adopting squeeze and excitation blocks and
non-parametric smooth activation functions. Through extensive experiments
across a wide spectrum of dataset scales, adversarial training methods, model
parameters, and network design spaces, our principles consistently and markedly
improve AutoAttack accuracy: 1-3 percentage points (pp) on CIFAR-10 and
CIFAR-100, and 4-9 pp on ImageNet. The code is publicly available at
https://github.com/poloclub/robust-principles.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1&quot;&gt;ShengYun Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Weilin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cornelius_C/0/1/0/all/0/1&quot;&gt;Cory Cornelius&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hull_M/0/1/0/all/0/1&quot;&gt;Matthew Hull&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kevin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duggal_R/0/1/0/all/0/1&quot;&gt;Rahul Duggal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phute_M/0/1/0/all/0/1&quot;&gt;Mansi Phute&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_J/0/1/0/all/0/1&quot;&gt;Jason Martin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1&quot;&gt;Duen Horng Chau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16269">
<title>Can Prompt Learning Benefit Radiology Report Generation?. (arXiv:2308.16269v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16269</link>
<description rdf:parseType="Literal">&lt;p&gt;Radiology report generation aims to automatically provide clinically
meaningful descriptions of radiology images such as MRI and X-ray. Although
great success has been achieved in natural scene image captioning tasks,
radiology report generation remains challenging and requires prior medical
knowledge. In this paper, we propose PromptRRG, a method that utilizes prompt
learning to activate a pretrained model and incorporate prior knowledge. Since
prompt learning for radiology report generation has not been explored before,
we begin with investigating prompt designs and categorise them based on varying
levels of knowledge: common, domain-specific and disease-enriched prompts.
Additionally, we propose an automatic prompt learning mechanism to alleviate
the burden of manual prompt engineering. This is the first work to
systematically examine the effectiveness of prompt learning for radiology
report generation. Experimental results on the largest radiology report
generation benchmark, MIMIC-CXR, demonstrate that our proposed method achieves
state-of-the-art performance. Code will be available upon the acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Lixing Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhalerao_A/0/1/0/all/0/1&quot;&gt;Abhir Bhalerao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yulan He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16271">
<title>Emergence of Segmentation with Minimalistic White-Box Transformers. (arXiv:2308.16271v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16271</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer-like models for vision tasks have recently proven effective for a
wide range of downstream applications such as segmentation and detection.
Previous works have shown that segmentation properties emerge in vision
transformers (ViTs) trained using self-supervised methods such as DINO, but not
in those trained on supervised classification tasks. In this study, we probe
whether segmentation emerges in transformer-based models solely as a result of
intricate self-supervised learning mechanisms, or if the same emergence can be
achieved under much broader conditions through proper design of the model
architecture. Through extensive experimental results, we demonstrate that when
employing a white-box transformer-like architecture known as CRATE, whose
design explicitly models and pursues low-dimensional structures in the data
distribution, segmentation properties, at both the whole and parts levels,
already emerge with a minimalistic supervised training recipe. Layer-wise
finer-grained analysis reveals that the emergent properties strongly
corroborate the designed mathematical functions of the white-box network. Our
results suggest a path to design white-box foundation models that are
simultaneously highly performant and mathematically fully interpretable. Code
is at \url{https://github.com/Ma-Lab-Berkeley/CRATE}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yaodong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_T/0/1/0/all/0/1&quot;&gt;Tianzhe Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_S/0/1/0/all/0/1&quot;&gt;Shengbang Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Ziyang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pai_D/0/1/0/all/0/1&quot;&gt;Druv Pai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buchanan_S/0/1/0/all/0/1&quot;&gt;Sam Buchanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yi Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16274">
<title>Learning Diverse Features in Vision Transformers for Improved Generalization. (arXiv:2308.16274v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16274</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models often rely only on a small set of features even when
there is a rich set of predictive signals in the training data. This makes
models brittle and sensitive to distribution shifts. In this work, we first
examine vision transformers (ViTs) and find that they tend to extract robust
and spurious features with distinct attention heads. As a result of this
modularity, their performance under distribution shifts can be significantly
improved at test time by pruning heads corresponding to spurious features,
which we demonstrate using an &quot;oracle selection&quot; on validation data. Second, we
propose a method to further enhance the diversity and complementarity of the
learned features by encouraging orthogonality of the attention heads&apos; input
gradients. We observe improved out-of-distribution performance on diagnostic
benchmarks (MNIST-CIFAR, Waterbirds) as a consequence of the enhanced diversity
of features and the pruning of undesirable heads.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nicolicioiu_A/0/1/0/all/0/1&quot;&gt;Armand Mihai Nicolicioiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nicolicioiu_A/0/1/0/all/0/1&quot;&gt;Andrei Liviu Nicolicioiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alexe_B/0/1/0/all/0/1&quot;&gt;Bogdan Alexe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teney_D/0/1/0/all/0/1&quot;&gt;Damien Teney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16278">
<title>Autonomous damage assessment of structural columns using low-cost micro aerial vehicles and multi-view computer vision. (arXiv:2308.16278v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16278</link>
<description rdf:parseType="Literal">&lt;p&gt;Structural columns are the crucial load-carrying components of buildings and
bridges. Early detection of column damage is important for the assessment of
the residual performance and the prevention of system-level collapse. This
research proposes an innovative end-to-end micro aerial vehicles (MAVs)-based
approach to automatically scan and inspect columns. First, an MAV-based
automatic image collection method is proposed. The MAV is programmed to sense
the structural columns and their surrounding environment. During the
navigation, the MAV first detects and approaches the structural columns. Then,
it starts to collect image data at multiple viewpoints around every detected
column. Second, the collected images will be used to assess the damage types
and damage locations. Third, the damage state of the structural column will be
determined by fusing the evaluation outcomes from multiple camera views. In
this study, reinforced concrete (RC) columns are selected to demonstrate the
effectiveness of the approach. Experimental results indicate that the proposed
MAV-based inspection approach can effectively collect images from multiple
viewing angles, and accurately assess critical RC column damages. The approach
improves the level of autonomy during the inspection. In addition, the
evaluation outcomes are more comprehensive than the existing 2D vision methods.
The concept of the proposed inspection approach can be extended to other
structural columns such as bridge piers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tavasoli_S/0/1/0/all/0/1&quot;&gt;Sina Tavasoli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1&quot;&gt;Xiao Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;T. Y. Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gazi_S/0/1/0/all/0/1&quot;&gt;Saudah Gazi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azimi_M/0/1/0/all/0/1&quot;&gt;Mohsen Azimi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16280">
<title>A reinforcement learning based construction material supply strategy using robotic crane and computer vision for building reconstruction after an earthquake. (arXiv:2308.16280v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16280</link>
<description rdf:parseType="Literal">&lt;p&gt;After an earthquake, it is particularly important to provide the necessary
resources on site because a large number of infrastructures need to be repaired
or newly constructed. Due to the complex construction environment after the
disaster, there are potential safety hazards for human labors working in this
environment. With the advancement of robotic technology and artificial
intelligent (AI) algorithms, smart robotic technology is the potential solution
to provide construction resources after an earthquake. In this paper, the
robotic crane with advanced AI algorithms is proposed to provide resources for
infrastructure reconstruction after an earthquake. The proximal policy
optimization (PPO), a reinforcement learning (RL) algorithm, is implemented for
3D lift path planning when transporting the construction materials. The state
and reward function are designed in detail for RL model training. Two models
are trained through a loading task in different environments by using PPO
algorithm, one considering the influence of obstacles and the other not
considering obstacles. Then, the two trained models are compared and evaluated
through an unloading task and a loading task in simulation environments. For
each task, two different cases are considered. One is that there is no obstacle
between the initial position where the construction material is lifted and the
target position, and the other is that there are obstacles between the initial
position and the target position. The results show that the model that
considering the obstacles during training can generate proper actions for the
robotic crane to execute so that the crane can automatically transport the
construction materials to the desired location with swing suppression, short
time consumption and collision avoidance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Yifei Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;T.Y. Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1&quot;&gt;Xiao Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_F/0/1/0/all/0/1&quot;&gt;Fan Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhongwei Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16316">
<title>Ten Years of Generative Adversarial Nets (GANs): A survey of the state-of-the-art. (arXiv:2308.16316v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.16316</link>
<description rdf:parseType="Literal">&lt;p&gt;Since their inception in 2014, Generative Adversarial Networks (GANs) have
rapidly emerged as powerful tools for generating realistic and diverse data
across various domains, including computer vision and other applied areas.
Consisting of a discriminative network and a generative network engaged in a
Minimax game, GANs have revolutionized the field of generative modeling. In
February 2018, GAN secured the leading spot on the ``Top Ten Global
Breakthrough Technologies List&apos;&apos; issued by the Massachusetts Science and
Technology Review. Over the years, numerous advancements have been proposed,
leading to a rich array of GAN variants, such as conditional GAN, Wasserstein
GAN, CycleGAN, and StyleGAN, among many others. This survey aims to provide a
general overview of GANs, summarizing the latent architecture, validation
metrics, and application areas of the most widely recognized variants. We also
delve into recent theoretical developments, exploring the profound connection
between the adversarial principle underlying GAN and Jensen-Shannon divergence,
while discussing the optimality characteristics of the GAN framework. The
efficiency of GAN variants and their model architectures will be evaluated
along with training obstacles as well as training solutions. In addition, a
detailed discussion will be provided, examining the integration of GANs with
newly developed deep learning frameworks such as Transformers, Physics-Informed
Neural Networks, Large Language models, and Diffusion models. Finally, we
reveal several issues as well as future research outlines in this field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1&quot;&gt;Tanujit Chakraborty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+S_U/0/1/0/all/0/1&quot;&gt;Ujjwal Reddy K S&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naik_S/0/1/0/all/0/1&quot;&gt;Shraddha M. Naik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panja_M/0/1/0/all/0/1&quot;&gt;Madhurima Panja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manvitha_B/0/1/0/all/0/1&quot;&gt;Bayapureddy Manvitha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16323">
<title>Software multiplataforma para a segmenta\c{c}\~ao de vasos sangu\&apos;ineos em imagens da retina. (arXiv:2308.16323v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.16323</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we utilize image segmentation to visually identify blood
vessels in retinal examination images. This process is typically carried out
manually. However, we can employ heuristic methods and machine learning to
automate or at least expedite the process. In this context, we propose a
cross-platform, open-source, and responsive software that allows users to
manually segment a retinal image. The purpose is to use the user-segmented
image to retrain machine learning algorithms, thereby enhancing future
automated segmentation results. Moreover, the software also incorporates and
applies certain image filters established in the literature to improve vessel
visualization. We propose the first solution of this kind in the literature.
This is the inaugural integrated software that embodies the aforementioned
attributes: open-source, responsive, and cross-platform. It offers a
comprehensive solution encompassing manual vessel segmentation, as well as the
automated execution of classification algorithms to refine predictive models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Machado_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o Henrique Pereira Machado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Oliveira_G/0/1/0/all/0/1&quot;&gt;Gilson Adamczuk Oliveira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rodrigues_E/0/1/0/all/0/1&quot;&gt;&amp;#xc9;rick Oliveira Rodrigues&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16325">
<title>Two-Stage Violence Detection Using ViTPose and Classification Models at Smart Airports. (arXiv:2308.16325v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16325</link>
<description rdf:parseType="Literal">&lt;p&gt;This study introduces an innovative violence detection framework tailored to
the unique requirements of smart airports, where prompt responses to violent
situations are crucial. The proposed framework harnesses the power of ViTPose
for human pose estimation. It employs a CNN - BiLSTM network to analyse spatial
and temporal information within keypoints sequences, enabling the accurate
classification of violent behaviour in real time. Seamlessly integrated within
the SAFE (Situational Awareness for Enhanced Security framework of SAAB, the
solution underwent integrated testing to ensure robust performance in real
world scenarios. The AIRTLab dataset, characterized by its high video quality
and relevance to surveillance scenarios, is utilized in this study to enhance
the model&apos;s accuracy and mitigate false positives. As airports face increased
foot traffic in the post pandemic era, implementing AI driven violence
detection systems, such as the one proposed, is paramount for improving
security, expediting response times, and promoting data informed decision
making. The implementation of this framework not only diminishes the
probability of violent events but also assists surveillance teams in
effectively addressing potential threats, ultimately fostering a more secure
and protected aviation sector. Codes are available at:
https://github.com/Asami-1/GDP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ustek_I/0/1/0/all/0/1&quot;&gt;&amp;#x130;rem &amp;#xdc;stek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Desai_J/0/1/0/all/0/1&quot;&gt;Jay Desai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torrecillas_I/0/1/0/all/0/1&quot;&gt;Iv&amp;#xe1;n L&amp;#xf3;pez Torrecillas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abadou_S/0/1/0/all/0/1&quot;&gt;Sofiane Abadou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinjie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fever_Q/0/1/0/all/0/1&quot;&gt;Quentin Fever&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasthuri_S/0/1/0/all/0/1&quot;&gt;Sandhya Rani Kasthuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1&quot;&gt;Yang Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1&quot;&gt;Weisi Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsourdos_A/0/1/0/all/0/1&quot;&gt;Antonios Tsourdos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16354">
<title>Catalog Phrase Grounding (CPG): Grounding of Product Textual Attributes in Product Images for e-commerce Vision-Language Applications. (arXiv:2308.16354v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16354</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Catalog Phrase Grounding (CPG), a model that can associate product
textual data (title, brands) into corresponding regions of product images
(isolated product region, brand logo region) for e-commerce vision-language
applications. We use a state-of-the-art modulated multimodal transformer
encoder-decoder architecture unifying object detection and phrase-grounding. We
train the model in self-supervised fashion with 2.3 million image-text pairs
synthesized from an e-commerce site. The self-supervision data is annotated
with high-confidence pseudo-labels generated with a combination of teacher
models: a pre-trained general domain phrase grounding model (e.g. MDETR) and a
specialized logo detection model. This allows CPG, as a student model, to
benefit from transfer knowledge from these base models combining general-domain
knowledge and specialized knowledge. Beyond immediate catalog phrase grounding
tasks, we can benefit from CPG representations by incorporating them as ML
features into downstream catalog applications that require deep semantic
understanding of products. Our experiments on product-brand matching, a
challenging e-commerce application, show that incorporating CPG representations
into the existing production ensemble system leads to on average 5% recall
improvement across all countries globally (with the largest lift of 11% in a
single country) at fixed 95% precision, outperforming other alternatives
including a logo detection teacher model and ResNet50.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wenyi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouyarmane_K/0/1/0/all/0/1&quot;&gt;Karim Bouyarmane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tutar_I/0/1/0/all/0/1&quot;&gt;Ismail Tutar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16355">
<title>A Recycling Training Strategy for Medical Image Segmentation with Diffusion Denoising Models. (arXiv:2308.16355v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.16355</link>
<description rdf:parseType="Literal">&lt;p&gt;Denoising diffusion models have found applications in image segmentation by
generating segmented masks conditioned on images. Existing studies
predominantly focus on adjusting model architecture or improving inference such
as test-time sampling strategies. In this work, we focus on training strategy
improvements and propose a novel recycling method. During each training step, a
segmentation mask is first predicted given an image and a random noise. This
predicted mask, replacing the conventional ground truth mask, is used for
denoising task during training. This approach can be interpreted as aligning
the training strategy with inference by eliminating the dependence on ground
truth masks for generating noisy samples. Our proposed method significantly
outperforms standard diffusion training, self-conditioning, and existing
recycling strategies across multiple medical imaging data sets: muscle
ultrasound, abdominal CT, prostate MR, and brain MR. This holds true for two
widely adopted sampling strategies: denoising diffusion probabilistic model and
denoising diffusion implicit model. Importantly, existing diffusion models
often display a declining or unstable performance during inference, whereas our
novel recycling consistently enhances or maintains performance. Furthermore, we
show for the first time that, under a fair comparison with the same network
architectures and computing budget, the proposed recycling-based diffusion
models achieved on-par performance with non-diffusion-based supervised
training. This paper summarises these quantitative results and discusses their
values, with a fully reproducible JAX-based implementation, released at
https://github.com/mathpluscode/ImgX-DiffSeg.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yunguan Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yiwen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Saeed_S/0/1/0/all/0/1&quot;&gt;Shaheer U Saeed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Clarkson_M/0/1/0/all/0/1&quot;&gt;Matthew J Clarkson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yipeng Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16376">
<title>Improving Multiple Sclerosis Lesion Segmentation Across Clinical Sites: A Federated Learning Approach with Noise-Resilient Training. (arXiv:2308.16376v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.16376</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurately measuring the evolution of Multiple Sclerosis (MS) with magnetic
resonance imaging (MRI) critically informs understanding of disease progression
and helps to direct therapeutic strategy. Deep learning models have shown
promise for automatically segmenting MS lesions, but the scarcity of accurately
annotated data hinders progress in this area. Obtaining sufficient data from a
single clinical site is challenging and does not address the heterogeneous need
for model robustness. Conversely, the collection of data from multiple sites
introduces data privacy concerns and potential label noise due to varying
annotation standards. To address this dilemma, we explore the use of the
federated learning framework while considering label noise. Our approach
enables collaboration among multiple clinical sites without compromising data
privacy under a federated learning paradigm that incorporates a noise-robust
training strategy based on label correction. Specifically, we introduce a
Decoupled Hard Label Correction (DHLC) strategy that considers the imbalanced
distribution and fuzzy boundaries of MS lesions, enabling the correction of
false annotations based on prediction confidence. We also introduce a Centrally
Enhanced Label Correction (CELC) strategy, which leverages the aggregated
central model as a correction teacher for all sites, enhancing the reliability
of the correction process. Extensive experiments conducted on two multi-site
datasets demonstrate the effectiveness and robustness of our proposed methods,
indicating their potential for clinical applications in multi-site
collaborations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bai_L/0/1/0/all/0/1&quot;&gt;Lei Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dongang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Barnett_M/0/1/0/all/0/1&quot;&gt;Michael Barnett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cabezas_M/0/1/0/all/0/1&quot;&gt;Mariano Cabezas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cai_W/0/1/0/all/0/1&quot;&gt;Weidong Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Calamante_F/0/1/0/all/0/1&quot;&gt;Fernando Calamante&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kyle_K/0/1/0/all/0/1&quot;&gt;Kain Kyle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dongnan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ly_L/0/1/0/all/0/1&quot;&gt;Linda Ly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nguyen_A/0/1/0/all/0/1&quot;&gt;Aria Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shieh_C/0/1/0/all/0/1&quot;&gt;Chun-Chien Shieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sullivan_R/0/1/0/all/0/1&quot;&gt;Ryan Sullivan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hengrui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhan_G/0/1/0/all/0/1&quot;&gt;Geng Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ouyang_W/0/1/0/all/0/1&quot;&gt;Wanli Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chenyu Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16380">
<title>3D vision-based structural masonry damage detection. (arXiv:2308.16380v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16380</link>
<description rdf:parseType="Literal">&lt;p&gt;The detection of masonry damage is essential for preventing potentially
disastrous outcomes. Manual inspection can, however, take a long time and be
hazardous to human inspectors. Automation of the inspection process using novel
computer vision and machine learning algorithms can be a more efficient and
safe solution to prevent further deterioration of the masonry structures. Most
existing 2D vision-based methods are limited to qualitative damage
classification, 2D localization, and in-plane quantification. In this study, we
present a 3D vision-based methodology for accurate masonry damage detection,
which offers a more robust solution with a greater field of view, depth of
vision, and the ability to detect failures in complex environments. First,
images of the masonry specimens are collected to generate a 3D point cloud.
Second, 3D point clouds processing methods are developed to evaluate the
masonry damage. We demonstrate the effectiveness of our approach through
experiments on structural masonry components. Our experiments showed the
proposed system can effectively classify damage states and localize and
quantify critical damage features. The result showed the proposed method can
improve the level of autonomy during the inspection of masonry structures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zonouz_E/0/1/0/all/0/1&quot;&gt;Elmira Faraji Zonouz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1&quot;&gt;Xiao Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1&quot;&gt;Yu-Cheng Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Tony Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16383">
<title>Separate and Locate: Rethink the Text in Text-based Visual Question Answering. (arXiv:2308.16383v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16383</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-based Visual Question Answering (TextVQA) aims at answering questions
about the text in images. Most works in this field focus on designing network
structures or pre-training tasks. All these methods list the OCR texts in
reading order (from left to right and top to bottom) to form a sequence, which
is treated as a natural language ``sentence&apos;&apos;. However, they ignore the fact
that most OCR words in the TextVQA task do not have a semantical contextual
relationship. In addition, these approaches use 1-D position embedding to
construct the spatial relation between OCR tokens sequentially, which is not
reasonable. The 1-D position embedding can only represent the left-right
sequence relationship between words in a sentence, but not the complex spatial
position relationship. To tackle these problems, we propose a novel method
named Separate and Locate (SaL) that explores text contextual cues and designs
spatial position embedding to construct spatial relations between OCR texts.
Specifically, we propose a Text Semantic Separate (TSS) module that helps the
model recognize whether words have semantic contextual relations. Then, we
introduce a Spatial Circle Position (SCP) module that helps the model better
construct and reason the spatial position relationships between OCR texts. Our
SaL model outperforms the baseline model by 4.44% and 3.96% accuracy on TextVQA
and ST-VQA datasets. Compared with the pre-training state-of-the-art method
pre-trained on 64 million pre-training samples, our method, without any
pre-training tasks, still achieves 2.68% and 2.52% accuracy improvement on
TextVQA and ST-VQA. Our code and models will be released at
https://github.com/fangbufang/SaL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1&quot;&gt;Chengyang Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiangnan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Liang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Can Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1&quot;&gt;Dayong Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16386">
<title>RGB-T Tracking via Multi-Modal Mutual Prompt Learning. (arXiv:2308.16386v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16386</link>
<description rdf:parseType="Literal">&lt;p&gt;Object tracking based on the fusion of visible and thermal im-ages, known as
RGB-T tracking, has gained increasing atten-tion from researchers in recent
years. How to achieve a more comprehensive fusion of information from the two
modalities with fewer computational costs has been a problem that re-searchers
have been exploring. Recently, with the rise of prompt learning in computer
vision, we can better transfer knowledge from visual large models to downstream
tasks. Considering the strong complementarity between visible and thermal
modalities, we propose a tracking architecture based on mutual prompt learning
between the two modalities. We also design a lightweight prompter that
incorporates attention mechanisms in two dimensions to transfer information
from one modality to the other with lower computational costs, embedding it
into each layer of the backbone. Extensive ex-periments have demonstrated that
our proposed tracking ar-chitecture is effective and efficient, achieving
state-of-the-art performance while maintaining high running speeds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yang Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xiqing Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1&quot;&gt;Hui Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ao_L/0/1/0/all/0/1&quot;&gt;Lei Ao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16404">
<title>Deformation Robust Text Spotting with Geometric Prior. (arXiv:2308.16404v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16404</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of text spotting is to perform text detection and recognition in an
end-to-end manner. Although the diversity of luminosity and orientation in
scene texts has been widely studied, the font diversity and shape variance of
the same character are ignored in recent works, since most characters in
natural images are rendered in standard fonts. To solve this problem, we
present a Chinese Artistic Dataset, termed as ARText, which contains 33,000
artistic images with rich shape deformation and font diversity. Based on this
database, we develop a deformation robust text spotting method (DR TextSpotter)
to solve the recognition problem of complex deformation of characters in
different fonts. Specifically, we propose a geometric prior module to highlight
the important features based on the unsupervised landmark detection
sub-network. A graph convolution network is further constructed to fuse the
character features and landmark features, and then performs semantic reasoning
to enhance the discrimination for different characters. The experiments are
conducted on ARText and IC19-ReCTS datasets. Our results demonstrate the
effectiveness of our proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_X/0/1/0/all/0/1&quot;&gt;Xixuan Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Aozhong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1&quot;&gt;Xianze Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_B/0/1/0/all/0/1&quot;&gt;Bin Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16435">
<title>Njobvu-AI: An open-source tool for collaborative image labeling and implementation of computer vision models. (arXiv:2308.16435v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16435</link>
<description rdf:parseType="Literal">&lt;p&gt;Practitioners interested in using computer vision models lack user-friendly
and open-source software that combines features to label training data, allow
multiple users, train new algorithms, review output, and implement new models.
Labeling training data, such as images, is a key step to developing accurate
object detection algorithms using computer vision. This step is often not
compatible with many cloud-based services for marking or labeling image and
video data due to limited internet bandwidth in many regions of the world.
Desktop tools are useful for groups working in remote locations, but users
often do not have the capability to combine projects developed locally by
multiple collaborators. Furthermore, many tools offer features for labeling
data or using pre-trained models for classification, but few allow researchers
to combine these steps to create and apply custom models. Free, open-source,
and user-friendly software that offers a full suite of features (e.g., ability
to work locally and online, and train custom models) is desirable to field
researchers and conservationists that may have limited coding skills. We
developed Njobvu-AI, a free, open-source tool that can be run on both desktop
and server hardware using Node.js, allowing users to label data, combine
projects for collaboration and review, train custom algorithms, and implement
new computer vision models. The name Njobvu-AI (pronounced N-joh-voo AI),
incorporating the Chichewa word for elephant, is inspired by a wildlife
monitoring program in Malawi that was a primary impetus for the development of
this tool and references similarities between the powerful memory of elephants
and properties of computer vision models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koning_J/0/1/0/all/0/1&quot;&gt;Jonathan S. Koning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramanian_A/0/1/0/all/0/1&quot;&gt;Ashwin Subramanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alotaibi_M/0/1/0/all/0/1&quot;&gt;Mazen Alotaibi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Appel_C/0/1/0/all/0/1&quot;&gt;Cara L. Appel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sullivan_C/0/1/0/all/0/1&quot;&gt;Christopher M. Sullivan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chao_T/0/1/0/all/0/1&quot;&gt;Thon Chao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Truong_L/0/1/0/all/0/1&quot;&gt;Lisa Truong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanguay_R/0/1/0/all/0/1&quot;&gt;Robyn L. Tanguay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaiswal_P/0/1/0/all/0/1&quot;&gt;Pankaj Jaiswal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levi_T/0/1/0/all/0/1&quot;&gt;Taal Levi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lesmeister_D/0/1/0/all/0/1&quot;&gt;Damon B. Lesmeister&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16454">
<title>Adversarial Finetuning with Latent Representation Constraint to Mitigate Accuracy-Robustness Tradeoff. (arXiv:2308.16454v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16454</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper addresses the tradeoff between standard accuracy on clean examples
and robustness against adversarial examples in deep neural networks (DNNs).
Although adversarial training (AT) improves robustness, it degrades the
standard accuracy, thus yielding the tradeoff. To mitigate this tradeoff, we
propose a novel AT method called ARREST, which comprises three components: (i)
adversarial finetuning (AFT), (ii) representation-guided knowledge distillation
(RGKD), and (iii) noisy replay (NR). AFT trains a DNN on adversarial examples
by initializing its parameters with a DNN that is standardly pretrained on
clean examples. RGKD and NR respectively entail a regularization term and an
algorithm to preserve latent representations of clean examples during AFT. RGKD
penalizes the distance between the representations of the standardly pretrained
and AFT DNNs. NR switches input adversarial examples to nonadversarial ones
when the representation changes significantly during AFT. By combining these
components, ARREST achieves both high standard accuracy and robustness.
Experimental results demonstrate that ARREST mitigates the tradeoff more
effectively than previous AT-based methods do.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suzuki_S/0/1/0/all/0/1&quot;&gt;Satoshi Suzuki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamaguchi_S/0/1/0/all/0/1&quot;&gt;Shin&amp;#x27;ya Yamaguchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takeda_S/0/1/0/all/0/1&quot;&gt;Shoichiro Takeda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanai_S/0/1/0/all/0/1&quot;&gt;Sekitoshi Kanai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Makishima_N/0/1/0/all/0/1&quot;&gt;Naoki Makishima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ando_A/0/1/0/all/0/1&quot;&gt;Atsushi Ando&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Masumura_R/0/1/0/all/0/1&quot;&gt;Ryo Masumura&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16460">
<title>Improving Lens Flare Removal with General Purpose Pipeline and Multiple Light Sources Recovery. (arXiv:2308.16460v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.16460</link>
<description rdf:parseType="Literal">&lt;p&gt;When taking images against strong light sources, the resulting images often
contain heterogeneous flare artifacts. These artifacts can importantly affect
image visual quality and downstream computer vision tasks. While collecting
real data pairs of flare-corrupted/flare-free images for training flare removal
models is challenging, current methods utilize the direct-add approach to
synthesize data. However, these methods do not consider automatic exposure and
tone mapping in image signal processing pipeline (ISP), leading to the limited
generalization capability of deep models training using such data. Besides,
existing methods struggle to handle multiple light sources due to the different
sizes, shapes and illuminance of various light sources. In this paper, we
propose a solution to improve the performance of lens flare removal by
revisiting the ISP and remodeling the principle of automatic exposure in the
synthesis pipeline and design a more reliable light sources recovery strategy.
The new pipeline approaches realistic imaging by discriminating the local and
global illumination through convex combination, avoiding global illumination
shifting and local over-saturation. Our strategy for recovering multiple light
sources convexly averages the input and output of the neural network based on
illuminance levels, thereby avoiding the need for a hard threshold in
identifying light sources. We also contribute a new flare removal testing
dataset containing the flare-corrupted images captured by ten types of consumer
electronics. The dataset facilitates the verification of the generalization
capability of flare removal methods. Extensive experiments show that our
solution can effectively improve the performance of lens flare removal and push
the frontier toward more general situations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuyan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liang_D/0/1/0/all/0/1&quot;&gt;Dong Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Songcan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Sheng-Jun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shuo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chongyi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16461">
<title>Domain Adaptive Synapse Detection with Weak Point Annotations. (arXiv:2308.16461v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16461</link>
<description rdf:parseType="Literal">&lt;p&gt;The development of learning-based methods has greatly improved the detection
of synapses from electron microscopy (EM) images. However, training a model for
each dataset is time-consuming and requires extensive annotations.
Additionally, it is difficult to apply a learned model to data from different
brain regions due to variations in data distributions. In this paper, we
present AdaSyn, a two-stage segmentation-based framework for domain adaptive
synapse detection with weak point annotations. In the first stage, we address
the detection problem by utilizing a segmentation-based pipeline to obtain
synaptic instance masks. In the second stage, we improve model generalizability
on target data by regenerating square masks to get high-quality pseudo labels.
Benefiting from our high-accuracy detection results, we introduce the distance
nearest principle to match paired pre-synapses and post-synapses. In the
WASPSYN challenge at ISBI 2023, our method ranks the 1st place.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yueyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Xiong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16463">
<title>Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models. (arXiv:2308.16463v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16463</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models exhibit enhanced zero-shot performance on various tasks
when fine-tuned with instruction-following data. Multimodal
instruction-following models extend these capabilities by integrating both text
and images. However, existing models such as MiniGPT-4 face challenges in
maintaining dialogue coherence in scenarios involving multiple images. A
primary reason is the lack of a specialized dataset for this critical
application. To bridge these gaps, we present SparklesChat, a multimodal
instruction-following model for open-ended dialogues across multiple images. To
support the training, we introduce SparklesDialogue, the first
machine-generated dialogue dataset tailored for word-level interleaved
multi-image and text interactions. Furthermore, we construct SparklesEval, a
GPT-assisted benchmark for quantitatively assessing a model&apos;s conversational
competence across multiple images and dialogue turns. Our experiments validate
the effectiveness of SparklesChat in understanding and reasoning across
multiple images and dialogue turns. Specifically, SparklesChat outperformed
MiniGPT-4 on established vision-and-language benchmarks, including the BISON
binary image selection task and the NLVR2 visual reasoning task. Moreover,
SparklesChat scored 8.56 out of 10 on SparklesEval, substantially exceeding
MiniGPT-4&apos;s score of 3.91 and nearing GPT-4&apos;s score of 9.26. Qualitative
evaluations further demonstrate SparklesChat&apos;s generality in handling
real-world applications. All resources will be available at
https://github.com/HYPJUDY/Sparkles.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yupan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1&quot;&gt;Zaiqiao Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fangyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yixuan Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1&quot;&gt;Nigel Collier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yutong Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16466">
<title>Self-Sampling Meta SAM: Enhancing Few-shot Medical Image Segmentation with Meta-Learning. (arXiv:2308.16466v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16466</link>
<description rdf:parseType="Literal">&lt;p&gt;While the Segment Anything Model (SAM) excels in semantic segmentation for
general-purpose images, its performance significantly deteriorates when applied
to medical images, primarily attributable to insufficient representation of
medical images in its training dataset. Nonetheless, gathering comprehensive
datasets and training models that are universally applicable is particularly
challenging due to the long-tail problem common in medical images. To address
this gap, here we present a Self-Sampling Meta SAM (SSM-SAM) framework for
few-shot medical image segmentation. Our innovation lies in the design of three
key modules: 1) An online fast gradient descent optimizer, further optimized by
a meta-learner, which ensures swift and robust adaptation to new tasks. 2) A
Self-Sampling module designed to provide well-aligned visual prompts for
improved attention allocation; and 3) A robust attention-based decoder
specifically designed for medical few-shot learning to capture relationship
between different slices. Extensive experiments on a popular abdominal CT
dataset and an MRI dataset demonstrate that the proposed method achieves
significant improvements over state-of-the-art methods in few-shot
segmentation, with an average improvements of 10.21% and 1.80% in terms of DSC,
respectively. In conclusion, we present a novel approach for rapid online
adaptation in interactive image segmentation, adapting to a new organ in just
0.83 minutes. Code is publicly available on GitHub upon acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yiming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leng_T/0/1/0/all/0/1&quot;&gt;Tianang Leng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1&quot;&gt;Kun Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiaohui Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16477">
<title>PivotNet: Vectorized Pivot Learning for End-to-end HD Map Construction. (arXiv:2308.16477v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16477</link>
<description rdf:parseType="Literal">&lt;p&gt;Vectorized high-definition map online construction has garnered considerable
attention in the field of autonomous driving research. Most existing approaches
model changeable map elements using a fixed number of points, or predict local
maps in a two-stage autoregressive manner, which may miss essential details and
lead to error accumulation. Towards precise map element learning, we propose a
simple yet effective architecture named PivotNet, which adopts unified
pivot-based map representations and is formulated as a direct set prediction
paradigm. Concretely, we first propose a novel Point-to-Line Mask module to
encode both the subordinate and geometrical point-line priors in the network.
Then, a well-designed Pivot Dynamic Matching module is proposed to model the
topology in dynamic point sequences by introducing the concept of sequence
matching. Furthermore, to supervise the position and topology of the vectorized
point predictions, we propose a Dynamic Vectorized Sequence loss. Extensive
experiments and ablations show that PivotNet is remarkably superior to other
SOTAs by 5.9 mAP at least. The code will be available soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1&quot;&gt;Wenjie Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_L/0/1/0/all/0/1&quot;&gt;Limeng Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1&quot;&gt;Xi Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16481">
<title>Point-TTA: Test-Time Adaptation for Point Cloud Registration Using Multitask Meta-Auxiliary Learning. (arXiv:2308.16481v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16481</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Point-TTA, a novel test-time adaptation framework for point cloud
registration (PCR) that improves the generalization and the performance of
registration models. While learning-based approaches have achieved impressive
progress, generalization to unknown testing environments remains a major
challenge due to the variations in 3D scans. Existing methods typically train a
generic model and the same trained model is applied on each instance during
testing. This could be sub-optimal since it is difficult for the same model to
handle all the variations during testing. In this paper, we propose a test-time
adaptation approach for PCR. Our model can adapt to unseen distributions at
test-time without requiring any prior knowledge of the test data. Concretely,
we design three self-supervised auxiliary tasks that are optimized jointly with
the primary PCR task. Given a test instance, we adapt our model using these
auxiliary tasks and the updated model is used to perform the inference. During
training, our model is trained using a meta-auxiliary learning approach, such
that the adapted model via auxiliary tasks improves the accuracy of the primary
task. Experimental results demonstrate the effectiveness of our approach in
improving generalization of point cloud registration and outperforming other
state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hatem_A/0/1/0/all/0/1&quot;&gt;Ahmed Hatem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1&quot;&gt;Yiming Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16484">
<title>Test-Time Adaptation for Point Cloud Upsampling Using Meta-Learning. (arXiv:2308.16484v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16484</link>
<description rdf:parseType="Literal">&lt;p&gt;Affordable 3D scanners often produce sparse and non-uniform point clouds that
negatively impact downstream applications in robotic systems. While existing
point cloud upsampling architectures have demonstrated promising results on
standard benchmarks, they tend to experience significant performance drops when
the test data have different distributions from the training data. To address
this issue, this paper proposes a test-time adaption approach to enhance model
generality of point cloud upsampling. The proposed approach leverages
meta-learning to explicitly learn network parameters for test-time adaption.
Our method does not require any prior information about the test data. During
meta-training, the model parameters are learned from a collection of
instance-level tasks, each of which consists of a sparse-dense pair of point
clouds from the training data. During meta-testing, the trained model is
fine-tuned with a few gradient updates to produce a unique set of network
parameters for each test instance. The updated model is then used for the final
prediction. Our framework is generic and can be applied in a plug-and-play
manner with existing backbone networks in point cloud upsampling. Extensive
experiments demonstrate that our approach improves the performance of
state-of-the-art models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hatem_A/0/1/0/all/0/1&quot;&gt;Ahmed Hatem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1&quot;&gt;Yiming Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16486">
<title>Illumination Distillation Framework for Nighttime Person Re-Identification and A New Benchmark. (arXiv:2308.16486v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16486</link>
<description rdf:parseType="Literal">&lt;p&gt;Nighttime person Re-ID (person re-identification in the nighttime) is a very
important and challenging task for visual surveillance but it has not been
thoroughly investigated. Under the low illumination condition, the performance
of person Re-ID methods usually sharply deteriorates. To address the low
illumination challenge in nighttime person Re-ID, this paper proposes an
Illumination Distillation Framework (IDF), which utilizes illumination
enhancement and illumination distillation schemes to promote the learning of
Re-ID models. Specifically, IDF consists of a master branch, an illumination
enhancement branch, and an illumination distillation module. The master branch
is used to extract the features from a nighttime image. The illumination
enhancement branch first estimates an enhanced image from the nighttime image
using a nonlinear curve mapping method and then extracts the enhanced features.
However, nighttime and enhanced features usually contain data noise due to
unstable lighting conditions and enhancement failures. To fully exploit the
complementary benefits of nighttime and enhanced features while suppressing
data noise, we propose an illumination distillation module. In particular, the
illumination distillation module fuses the features from two branches through a
bottleneck fusion model and then uses the fused features to guide the learning
of both branches in a distillation manner. In addition, we build a real-world
nighttime person Re-ID dataset, named Night600, which contains 600 identities
captured from different viewpoints and nighttime illumination conditions under
complex outdoor environments. Experimental results demonstrate that our IDF can
achieve state-of-the-art performance on two nighttime person Re-ID datasets
(i.e., Night600 and Knight ). We will release our code and dataset at
https://github.com/Alexadlu/IDF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_A/0/1/0/all/0/1&quot;&gt;Andong Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yifan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenglong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jin Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16490">
<title>Latent Painter. (arXiv:2308.16490v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16490</link>
<description rdf:parseType="Literal">&lt;p&gt;Latent diffusers revolutionized the generative AI and inspired creative art.
When denoising the latent, the predicted original image at each step
collectively animates the formation. However, the animation is limited by the
denoising nature of the diffuser, and only renders a sharpening process. This
work presents Latent Painter, which uses the latent as the canvas, and the
diffuser predictions as the plan, to generate painting animation. Latent
Painter also transits one generated image to another, which can happen between
images from two different sets of checkpoints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1&quot;&gt;Shih-Chieh Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16510">
<title>Robust GAN inversion. (arXiv:2308.16510v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16510</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in real image editing have been attributed to the
exploration of Generative Adversarial Networks (GANs) latent space. However,
the main challenge of this procedure is GAN inversion, which aims to map the
image to the latent space accurately. Existing methods that work on extended
latent space $W+$ are unable to achieve low distortion and high editability
simultaneously. To address this issue, we propose an approach which works in
native latent space $W$ and tunes the generator network to restore missing
image details. We introduce a novel regularization strategy with learnable
coefficients obtained by training randomized StyleGAN 2 model - WRanGAN. This
method outperforms traditional approaches in terms of reconstruction quality
and computational efficiency, achieving the lowest distortion with 4 times
fewer parameters. Furthermore, we observe a slight improvement in the quality
of constructing hyperplanes corresponding to binary image attributes. We
demonstrate the effectiveness of our approach on two complex datasets:
Flickr-Faces-HQ and LSUN Church.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sevriugov_E/0/1/0/all/0/1&quot;&gt;Egor Sevriugov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oseledets_I/0/1/0/all/0/1&quot;&gt;Ivan Oseledets&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16512">
<title>MVDream: Multi-view Diffusion for 3D Generation. (arXiv:2308.16512v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16512</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose MVDream, a multi-view diffusion model that is able to generate
geometrically consistent multi-view images from a given text prompt. By
leveraging image diffusion models pre-trained on large-scale web datasets and a
multi-view dataset rendered from 3D assets, the resulting multi-view diffusion
model can achieve both the generalizability of 2D diffusion and the consistency
of 3D data. Such a model can thus be applied as a multi-view prior for 3D
generation via Score Distillation Sampling, where it greatly improves the
stability of existing 2D-lifting methods by solving the 3D consistency problem.
Finally, we show that the multi-view diffusion model can also be fine-tuned
under a few shot setting for personalized 3D generation, i.e. DreamBooth3D
application, where the consistency can be maintained after learning the subject
identity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yichun Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jianglong Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1&quot;&gt;Mai Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kejie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiao Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16518">
<title>MS23D: A 3D Object Detection Method Using Multi-Scale Semantic Feature Points to Construct 3D Feature Layers. (arXiv:2308.16518v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16518</link>
<description rdf:parseType="Literal">&lt;p&gt;Lidar point clouds, as a type of data with accurate distance perception, can
effectively represent the motion and posture of objects in three-dimensional
space. However, the sparsity and disorderliness of point clouds make it
challenging to extract features directly from them. Many studies have addressed
this issue by transforming point clouds into regular voxel representations.
However, these methods often lead to the loss of fine-grained local feature
information due to downsampling. Moreover, the sparsity of point clouds poses
difficulties in efficiently aggregating features in 3D feature layers using
voxel-based two-stage methods. To address these issues, this paper proposes a
two-stage 3D detection framework called MS$^{2}$3D. In MS$^{2}$3D, we utilize
small-sized voxels to extract fine-grained local features and large-sized
voxels to capture long-range local features. Additionally, we propose a method
for constructing 3D feature layers using multi-scale semantic feature points,
enabling the transformation of sparse 3D feature layers into more compact
representations. Furthermore, we compute the offset between feature points in
the 3D feature layers and the centroid of objects, aiming to bring them as
close as possible to the object&apos;s center. It significantly enhances the
efficiency of feature aggregation. To validate the effectiveness of our method,
we evaluated our method on the KITTI dataset and ONCE dataset together.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1&quot;&gt;Yongxin Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_A/0/1/0/all/0/1&quot;&gt;Aihong Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_T/0/1/0/all/0/1&quot;&gt;Tianhong Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhetao Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16527">
<title>Unsupervised Recognition of Unknown Objects for Open-World Object Detection. (arXiv:2308.16527v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16527</link>
<description rdf:parseType="Literal">&lt;p&gt;Open-World Object Detection (OWOD) extends object detection problem to a
realistic and dynamic scenario, where a detection model is required to be
capable of detecting both known and unknown objects and incrementally learning
newly introduced knowledge. Current OWOD models, such as ORE and OW-DETR, focus
on pseudo-labeling regions with high objectness scores as unknowns, whose
performance relies heavily on the supervision of known objects. While they can
detect the unknowns that exhibit similar features to the known objects, they
suffer from a severe label bias problem that they tend to detect all regions
(including unknown object regions) that are dissimilar to the known objects as
part of the background. To eliminate the label bias, this paper proposes a
novel approach that learns an unsupervised discriminative model to recognize
true unknown objects from raw pseudo labels generated by unsupervised region
proposal methods. The resulting model can be further refined by a
classification-free self-training method which iteratively extends pseudo
unknown objects to the unlabeled regions. Experimental results show that our
method 1) significantly outperforms the prior SOTA in detecting unknown objects
while maintaining competitive performance of detecting known object classes on
the MS COCO dataset, and 2) achieves better generalization ability on the LVIS
and Objects365 datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_R/0/1/0/all/0/1&quot;&gt;Ruohuan Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1&quot;&gt;Guansong Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Lei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1&quot;&gt;Xiao Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Jin Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16528">
<title>SA6D: Self-Adaptive Few-Shot 6D Pose Estimator for Novel and Occluded Objects. (arXiv:2308.16528v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16528</link>
<description rdf:parseType="Literal">&lt;p&gt;To enable meaningful robotic manipulation of objects in the real-world, 6D
pose estimation is one of the critical aspects. Most existing approaches have
difficulties to extend predictions to scenarios where novel object instances
are continuously introduced, especially with heavy occlusions. In this work, we
propose a few-shot pose estimation (FSPE) approach called SA6D, which uses a
self-adaptive segmentation module to identify the novel target object and
construct a point cloud model of the target object using only a small number of
cluttered reference images. Unlike existing methods, SA6D does not require
object-centric reference images or any additional object information, making it
a more generalizable and scalable solution across categories. We evaluate SA6D
on real-world tabletop object datasets and demonstrate that SA6D outperforms
existing FSPE methods, particularly in cluttered scenes with occlusions, while
requiring fewer reference images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_N/0/1/0/all/0/1&quot;&gt;Ning Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vien_N/0/1/0/all/0/1&quot;&gt;Ngo Anh Vien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziesche_H/0/1/0/all/0/1&quot;&gt;Hanna Ziesche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1&quot;&gt;Gerhard Neumann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16530">
<title>Privacy-Preserving Medical Image Classification through Deep Learning and Matrix Decomposition. (arXiv:2308.16530v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2308.16530</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning (DL)-based solutions have been extensively researched in the
medical domain in recent years, enhancing the efficacy of diagnosis, planning,
and treatment. Since the usage of health-related data is strictly regulated,
processing medical records outside the hospital environment for developing and
using DL models demands robust data protection measures. At the same time, it
can be challenging to guarantee that a DL solution delivers a minimum level of
performance when being trained on secured data, without being specifically
designed for the given task. Our approach uses singular value decomposition
(SVD) and principal component analysis (PCA) to obfuscate the medical images
before employing them in the DL analysis. The capability of DL algorithms to
extract relevant information from secured data is assessed on a task of
angiographic view classification based on obfuscated frames. The security level
is probed by simulated artificial intelligence (AI)-based reconstruction
attacks, considering two threat actors with different prior knowledge of the
targeted data. The degree of privacy is quantitatively measured using
similarity indices. Although a trade-off between privacy and accuracy should be
considered, the proposed technique allows for training the angiographic view
classifier exclusively on secured data with satisfactory performance and with
no computational overhead, model adaptation, or hyperparameter tuning. While
the obfuscated medical image content is well protected against human
perception, the hypothetical reconstruction attack proved that it is also
difficult to recover the complete information of the original frames.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Popescu_A/0/1/0/all/0/1&quot;&gt;Andreea Bianca Popescu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nita_C/0/1/0/all/0/1&quot;&gt;Cosmin Ioan Nita&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taca_I/0/1/0/all/0/1&quot;&gt;Ioana Antonia Taca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vizitiu_A/0/1/0/all/0/1&quot;&gt;Anamaria Vizitiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Itu_L/0/1/0/all/0/1&quot;&gt;Lucian Mihai Itu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16532">
<title>Decoupled Local Aggregation for Point Cloud Learning. (arXiv:2308.16532v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16532</link>
<description rdf:parseType="Literal">&lt;p&gt;The unstructured nature of point clouds demands that local aggregation be
adaptive to different local structures. Previous methods meet this by
explicitly embedding spatial relations into each aggregation process. Although
this coupled approach has been shown effective in generating clear semantics,
aggregation can be greatly slowed down due to repeated relation learning and
redundant computation to mix directional and point features. In this work, we
propose to decouple the explicit modelling of spatial relations from local
aggregation. We theoretically prove that basic neighbor pooling operations can
too function without loss of clarity in feature fusion, so long as essential
spatial information has been encoded in point features. As an instantiation of
decoupled local aggregation, we present DeLA, a lightweight point network,
where in each learning stage relative spatial encodings are first formed, and
only pointwise convolutions plus edge max-pooling are used for local
aggregation then. Further, a regularization term is employed to reduce
potential ambiguity through the prediction of relative coordinates.
Conceptually simple though, experimental results on five classic benchmarks
demonstrate that DeLA achieves state-of-the-art performance with reduced or
comparable latency. Specifically, DeLA achieves over 90\% overall accuracy on
ScanObjectNN and 74\% mIoU on S3DIS Area 5. Our code is available at
https://github.com/Matrix-ASC/DeLA .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Binjie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yunzhou Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zang_Y/0/1/0/all/0/1&quot;&gt;Yu Zang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Cheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jonathan Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16551">
<title>Object Detection for Caries or Pit and Fissure Sealing Requirement in Children&apos;s First Permanent Molars. (arXiv:2308.16551v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.16551</link>
<description rdf:parseType="Literal">&lt;p&gt;Dental caries is one of the most common oral diseases that, if left
untreated, can lead to a variety of oral problems. It mainly occurs inside the
pits and fissures on the occlusal/buccal/palatal surfaces of molars and
children are a high-risk group for pit and fissure caries in permanent molars.
Pit and fissure sealing is one of the most effective methods that is widely
used in prevention of pit and fissure caries. However, current detection of
pits and fissures or caries depends primarily on the experienced dentists,
which ordinary parents do not have, and children may miss the remedial
treatment without timely detection. To address this issue, we present a method
to autodetect caries and pit and fissure sealing requirements using oral photos
taken by smartphones. We use the YOLOv5 and YOLOX models and adopt a tiling
strategy to reduce information loss during image pre-processing. The best
result for YOLOXs model with tiling strategy is 72.3 mAP.5, while the best
result without tiling strategy is 71.2. YOLOv5s6 model with/without tiling
attains 70.9/67.9 mAP.5, respectively. We deploy the pre-trained network to
mobile devices as a WeChat applet, allowing in-home detection by parents or
children guardian.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jiang_C/0/1/0/all/0/1&quot;&gt;Chenyao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhai_S/0/1/0/all/0/1&quot;&gt;Shiyao Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Song_H/0/1/0/all/0/1&quot;&gt;Hengrui Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yuqing Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yachen Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yancheng Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yu_D/0/1/0/all/0/1&quot;&gt;Dongmei Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Canyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Han_S/0/1/0/all/0/1&quot;&gt;Sanyang Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Runming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianbo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qin_P/0/1/0/all/0/1&quot;&gt;Peiwu Qin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16552">
<title>Prompt-enhanced Hierarchical Transformer Elevating Cardiopulmonary Resuscitation Instruction via Temporal Action Segmentation. (arXiv:2308.16552v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16552</link>
<description rdf:parseType="Literal">&lt;p&gt;The vast majority of people who suffer unexpected cardiac arrest are
performed cardiopulmonary resuscitation (CPR) by passersby in a desperate
attempt to restore life, but endeavors turn out to be fruitless on account of
disqualification. Fortunately, many pieces of research manifest that
disciplined training will help to elevate the success rate of resuscitation,
which constantly desires a seamless combination of novel techniques to yield
further advancement. To this end, we collect a custom CPR video dataset in
which trainees make efforts to behave resuscitation on mannequins independently
in adherence to approved guidelines, thereby devising an auxiliary toolbox to
assist supervision and rectification of intermediate potential issues via
modern deep learning methodologies. Our research empirically views this problem
as a temporal action segmentation (TAS) task in computer vision, which aims to
segment an untrimmed video at a frame-wise level. Here, we propose a
Prompt-enhanced hierarchical Transformer (PhiTrans) that integrates three
indispensable modules, including a textual prompt-based Video Features
Extractor (VFE), a transformer-based Action Segmentation Executor (ASE), and a
regression-based Prediction Refinement Calibrator (PRC). The backbone of the
model preferentially derives from applications in three approved public
datasets (GTEA, 50Salads, and Breakfast) collected for TAS tasks, which
accounts for the excavation of the segmentation pipeline on the CPR dataset. In
general, we unprecedentedly probe into a feasible pipeline that genuinely
elevates the CPR instruction qualification via action segmentation in
conjunction with cutting-edge deep learning techniques. Associated experiments
advocate our implementation with multiple metrics surpassing 91.0%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1&quot;&gt;Xiaoyun Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_S/0/1/0/all/0/1&quot;&gt;Shiyao Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1&quot;&gt;Zhicheng Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1&quot;&gt;Zhenyuan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qiming Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Canyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1&quot;&gt;Bin Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandey_V/0/1/0/all/0/1&quot;&gt;Vijay Kumar Pandey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1&quot;&gt;Sanyang Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Runming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yuxing Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_P/0/1/0/all/0/1&quot;&gt;Peiwu Qin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16555">
<title>E3CM: Epipolar-Constrained Cascade Correspondence Matching. (arXiv:2308.16555v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16555</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate and robust correspondence matching is of utmost importance for
various 3D computer vision tasks. However, traditional explicit
programming-based methods often struggle to handle challenging scenarios, and
deep learning-based methods require large well-labeled datasets for network
training. In this article, we introduce Epipolar-Constrained Cascade
Correspondence (E3CM), a novel approach that addresses these limitations.
Unlike traditional methods, E3CM leverages pre-trained convolutional neural
networks to match correspondence, without requiring annotated data for any
network training or fine-tuning. Our method utilizes epipolar constraints to
guide the matching process and incorporates a cascade structure for progressive
refinement of matches. We extensively evaluate the performance of E3CM through
comprehensive experiments and demonstrate its superiority over existing
methods. To promote further research and facilitate reproducibility, we make
our source code publicly available at https://mias.group/E3CM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chenbo Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1&quot;&gt;Shuai Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qijun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1&quot;&gt;Rui Fan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16561">
<title>MoMA: Momentum Contrastive Learning with Multi-head Attention-based Knowledge Distillation for Histopathology Image Analysis. (arXiv:2308.16561v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.16561</link>
<description rdf:parseType="Literal">&lt;p&gt;There is no doubt that advanced artificial intelligence models and high
quality data are the keys to success in developing computational pathology
tools. Although the overall volume of pathology data keeps increasing, a lack
of quality data is a common issue when it comes to a specific task due to
several reasons including privacy and ethical issues with patient data. In this
work, we propose to exploit knowledge distillation, i.e., utilize the existing
model to learn a new, target model, to overcome such issues in computational
pathology. Specifically, we employ a student-teacher framework to learn a
target model from a pre-trained, teacher model without direct access to source
data and distill relevant knowledge via momentum contrastive learning with
multi-head attention mechanism, which provides consistent and context-aware
feature representations. This enables the target model to assimilate
informative representations of the teacher model while seamlessly adapting to
the unique nuances of the target data. The proposed method is rigorously
evaluated across different scenarios where the teacher model was trained on the
same, relevant, and irrelevant classification tasks with the target model.
Experimental results demonstrate the accuracy and robustness of our approach in
transferring knowledge to different domains and tasks, outperforming other
related methods. Moreover, the results provide a guideline on the learning
strategy for different types of tasks and scenarios in computational pathology.
Code is available at: \url{https://github.com/trinhvg/MoMA}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vuong_T/0/1/0/all/0/1&quot;&gt;Trinh Thi Le Vuong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kwak_J/0/1/0/all/0/1&quot;&gt;Jin Tae Kwak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16567">
<title>ScrollNet: Dynamic Weight Importance for Continual Learning. (arXiv:2308.16567v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16567</link>
<description rdf:parseType="Literal">&lt;p&gt;The principle underlying most existing continual learning (CL) methods is to
prioritize stability by penalizing changes in parameters crucial to old tasks,
while allowing for plasticity in other parameters. The importance of weights
for each task can be determined either explicitly through learning a
task-specific mask during training (e.g., parameter isolation-based approaches)
or implicitly by introducing a regularization term (e.g., regularization-based
approaches). However, all these methods assume that the importance of weights
for each task is unknown prior to data exposure. In this paper, we propose
ScrollNet as a scrolling neural network for continual learning. ScrollNet can
be seen as a dynamic network that assigns the ranking of weight importance for
each task before data exposure, thus achieving a more favorable
stability-plasticity tradeoff during sequential task learning by reassigning
this ranking for different tasks. Additionally, we demonstrate that ScrollNet
can be combined with various CL methods, including regularization-based and
replay-based approaches. Experimental results on CIFAR100 and TinyImagenet
datasets show the effectiveness of our proposed method. We release our code at
https://github.com/FireFYF/ScrollNet.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Fei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1&quot;&gt;Joost van de Weijer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16568">
<title>Shape of my heart: Cardiac models through learned signed distance functions. (arXiv:2308.16568v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.16568</link>
<description rdf:parseType="Literal">&lt;p&gt;The efficient construction of an anatomical model is one of the major
challenges of patient-specific in-silico models of the human heart. Current
methods frequently rely on linear statistical models, allowing no advanced
topological changes, or requiring medical image segmentation followed by a
meshing pipeline, which strongly depends on image resolution, quality, and
modality. These approaches are therefore limited in their transferability to
other imaging domains. In this work, the cardiac shape is reconstructed by
means of three-dimensional deep signed distance functions with Lipschitz
regularity. For this purpose, the shapes of cardiac MRI reconstructions are
learned from public databases to model the spatial relation of multiple
chambers in Cartesian space. We demonstrate that this approach is also capable
of reconstructing anatomical models from partial data, such as point clouds
from a single ventricle, or modalities different from the trained MRI, such as
electroanatomical mapping, and in addition, allows us to generate new
anatomical shapes by randomly sampling latent vectors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Verhulsdonk_J/0/1/0/all/0/1&quot;&gt;Jan Verh&amp;#xfc;lsdonk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Grandits_T/0/1/0/all/0/1&quot;&gt;Thomas Grandits&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Costabal_F/0/1/0/all/0/1&quot;&gt;Francisco Sahli Costabal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Krause_R/0/1/0/all/0/1&quot;&gt;Rolf Krause&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Auricchio_A/0/1/0/all/0/1&quot;&gt;Angelo Auricchio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Haase_G/0/1/0/all/0/1&quot;&gt;Gundolf Haase&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pezzuto_S/0/1/0/all/0/1&quot;&gt;Simone Pezzuto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Effland_A/0/1/0/all/0/1&quot;&gt;Alexander Effland&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16571">
<title>Document Layout Analysis on BaDLAD Dataset: A Comprehensive MViTv2 Based Approach. (arXiv:2308.16571v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16571</link>
<description rdf:parseType="Literal">&lt;p&gt;In the rapidly evolving digital era, the analysis of document layouts plays a
pivotal role in automated information extraction and interpretation. In our
work, we have trained MViTv2 transformer model architecture with cascaded mask
R-CNN on BaDLAD dataset to extract text box, paragraphs, images and tables from
a document. After training on 20365 document images for 36 epochs in a 3 phase
cycle, we achieved a training loss of 0.2125 and a mask loss of 0.19. Our work
extends beyond training, delving into the exploration of potential enhancement
avenues. We investigate the impact of rotation and flip augmentation, the
effectiveness of slicing input images pre-inference, the implications of
varying the resolution of the transformer backbone, and the potential of
employing a dual-pass inference to uncover missed text-boxes. Through these
explorations, we observe a spectrum of outcomes, where some modifications
result in tangible performance improvements, while others offer unique insights
for future endeavors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Ashrafur Rahman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azad_A/0/1/0/all/0/1&quot;&gt;Asif Azad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16572">
<title>CL-MAE: Curriculum-Learned Masked Autoencoders. (arXiv:2308.16572v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16572</link>
<description rdf:parseType="Literal">&lt;p&gt;Masked image modeling has been demonstrated as a powerful pretext task for
generating robust representations that can be effectively generalized across
multiple downstream tasks. Typically, this approach involves randomly masking
patches (tokens) in input images, with the masking strategy remaining unchanged
during training. In this paper, we propose a curriculum learning approach that
updates the masking strategy to continually increase the complexity of the
self-supervised reconstruction task. We conjecture that, by gradually
increasing the task complexity, the model can learn more sophisticated and
transferable representations. To facilitate this, we introduce a novel
learnable masking module that possesses the capability to generate masks of
different complexities, and integrate the proposed module into masked
autoencoders (MAE). Our module is jointly trained with the MAE, while adjusting
its behavior during training, transitioning from a partner to the MAE
(optimizing the same reconstruction loss) to an adversary (optimizing the
opposite loss), while passing through a neutral state. The transition between
these behaviors is smooth, being regulated by a factor that is multiplied with
the reconstruction loss of the masking module. The resulting training procedure
generates an easy-to-hard curriculum. We train our Curriculum-Learned Masked
Autoencoder (CL-MAE) on ImageNet and show that it exhibits superior
representation learning capabilities compared to MAE. The empirical results on
five downstream tasks confirm our conjecture, demonstrating that curriculum
learning can be successfully used to self-supervise masked autoencoders.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madan_N/0/1/0/all/0/1&quot;&gt;Neelu Madan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ristea_N/0/1/0/all/0/1&quot;&gt;Nicolae-Catalin Ristea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nasrollahi_K/0/1/0/all/0/1&quot;&gt;Kamal Nasrollahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moeslund_T/0/1/0/all/0/1&quot;&gt;Thomas B. Moeslund&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1&quot;&gt;Radu Tudor Ionescu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16573">
<title>Dual-Decoder Consistency via Pseudo-Labels Guided Data Augmentation for Semi-Supervised Medical Image Segmentation. (arXiv:2308.16573v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.16573</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical image segmentation methods often rely on fully supervised approaches
to achieve excellent performance, which is contingent upon having an extensive
set of labeled images for training. However, annotating medical images is both
expensive and time-consuming. Semi-supervised learning offers a solution by
leveraging numerous unlabeled images alongside a limited set of annotated ones.
In this paper, we introduce a semi-supervised medical image segmentation method
based on the mean-teacher model, referred to as Dual-Decoder Consistency via
Pseudo-Labels Guided Data Augmentation (DCPA). This method combines consistency
regularization, pseudo-labels, and data augmentation to enhance the efficacy of
semi-supervised segmentation. Firstly, the proposed model comprises both
student and teacher models with a shared encoder and two distinct decoders
employing different up-sampling strategies. Minimizing the output discrepancy
between decoders enforces the generation of consistent representations, serving
as regularization during student model training. Secondly, we introduce mixup
operations to blend unlabeled data with labeled data, creating mixed data and
thereby achieving data augmentation. Lastly, pseudo-labels are generated by the
teacher model and utilized as labels for mixed data to compute unsupervised
loss. We compare the segmentation results of the DCPA model with six
state-of-the-art semi-supervised methods on three publicly available medical
datasets. Beyond classical 10\% and 20\% semi-supervised settings, we
investigate performance with less supervision (5\% labeled data). Experimental
outcomes demonstrate that our approach consistently outperforms existing
semi-supervised medical image segmentation methods across the three
semi-supervised settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuanbin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Hui Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Longxuan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zong_R/0/1/0/all/0/1&quot;&gt;Ruige Zong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tan_T/0/1/0/all/0/1&quot;&gt;Tao Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinlin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tong_T/0/1/0/all/0/1&quot;&gt;Tong Tong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16576">
<title>GHuNeRF: Generalizable Human NeRF from a Monocular Video. (arXiv:2308.16576v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16576</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we tackle the challenging task of learning a generalizable
human NeRF model from a monocular video. Although existing generalizable human
NeRFs have achieved impressive results, they require muti-view images or videos
which might not be always available. On the other hand, some works on
free-viewpoint rendering of human from monocular videos cannot be generalized
to unseen identities. In view of these limitations, we propose GHuNeRF to learn
a generalizable human NeRF model from a monocular video of the human performer.
We first introduce a visibility-aware aggregation scheme to compute vertex-wise
features, which is used to construct a 3D feature volume. The feature volume
can only represent the overall geometry of the human performer with
insufficient accuracy due to the limited resolution. To solve this, we further
enhance the volume feature with temporally aligned point-wise features using an
attention mechanism. Finally, the enhanced feature is used for predicting
density and color for each sampled point. A surface-guided sampling strategy is
also introduced to improve the efficiency for both training and inference. We
validate our approach on the widely-used ZJU-MoCap dataset, where we achieve
comparable performance with existing multi-view video based approaches. We also
test on the monocular People-Snapshot dataset and achieve better performance
than existing works when only monocular video is used.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jihao Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1&quot;&gt;Gim Hee Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16582">
<title>Any-Size-Diffusion: Toward Efficient Text-Driven Synthesis for Any-Size HD Images. (arXiv:2308.16582v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16582</link>
<description rdf:parseType="Literal">&lt;p&gt;Stable diffusion, a generative model used in text-to-image synthesis,
frequently encounters resolution-induced composition problems when generating
images of varying sizes. This issue primarily stems from the model being
trained on pairs of single-scale images and their corresponding text
descriptions. Moreover, direct training on images of unlimited sizes is
unfeasible, as it would require an immense number of text-image pairs and
entail substantial computational expenses. To overcome these challenges, we
propose a two-stage pipeline named Any-Size-Diffusion (ASD), designed to
efficiently generate well-composed images of any size, while minimizing the
need for high-memory GPU resources. Specifically, the initial stage, dubbed Any
Ratio Adaptability Diffusion (ARAD), leverages a selected set of images with a
restricted range of ratios to optimize the text-conditional diffusion model,
thereby improving its ability to adjust composition to accommodate diverse
image sizes. To support the creation of images at any desired size, we further
introduce a technique called Fast Seamless Tiled Diffusion (FSTD) at the
subsequent stage. This method allows for the rapid enlargement of the ASD
output to any high-resolution size, avoiding seaming artifacts or memory
overloads. Experimental results on the LAION-COCO and MM-CelebA-HQ benchmarks
demonstrate that ASD can produce well-structured images of arbitrary sizes,
cutting down the inference time by 2x compared to the traditional tiled
algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1&quot;&gt;Qingping Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yuanfan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1&quot;&gt;Jiankang Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jianhua Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Ying Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Songcen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hang Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16598">
<title>Towards Optimal Patch Size in Vision Transformers for Tumor Segmentation. (arXiv:2308.16598v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.16598</link>
<description rdf:parseType="Literal">&lt;p&gt;Detection of tumors in metastatic colorectal cancer (mCRC) plays an essential
role in the early diagnosis and treatment of liver cancer. Deep learning models
backboned by fully convolutional neural networks (FCNNs) have become the
dominant model for segmenting 3D computerized tomography (CT) scans. However,
since their convolution layers suffer from limited kernel size, they are not
able to capture long-range dependencies and global context. To tackle this
restriction, vision transformers have been introduced to solve FCNN&apos;s locality
of receptive fields. Although transformers can capture long-range features,
their segmentation performance decreases with various tumor sizes due to the
model sensitivity to the input patch size. While finding an optimal patch size
improves the performance of vision transformer-based models on segmentation
tasks, it is a time-consuming and challenging procedure. This paper proposes a
technique to select the vision transformer&apos;s optimal input multi-resolution
image patch size based on the average volume size of metastasis lesions. We
further validated our suggested framework using a transfer-learning technique,
demonstrating that the highest Dice similarity coefficient (DSC) performance
was obtained by pre-training on training data with a larger tumour volume using
the suggested ideal patch size and then training with a smaller one. We
experimentally evaluate this idea through pre-training our model on a
multi-resolution public dataset. Our model showed consistent and improved
results when applied to our private multi-resolution mCRC dataset with a
smaller average tumor volume. This study lays the groundwork for optimizing
semantic segmentation of small objects using vision transformers. The
implementation source code is available
at:https://github.com/Ramtin-Mojtahedi/OVTPS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mojtahedi_R/0/1/0/all/0/1&quot;&gt;Ramtin Mojtahedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hamghalam_M/0/1/0/all/0/1&quot;&gt;Mohammad Hamghalam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Do_R/0/1/0/all/0/1&quot;&gt;Richard K. G. Do&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Simpson_A/0/1/0/all/0/1&quot;&gt;Amber L. Simpson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16611">
<title>Detecting Out-of-Context Image-Caption Pairs in News: A Counter-Intuitive Method. (arXiv:2308.16611v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16611</link>
<description rdf:parseType="Literal">&lt;p&gt;The growth of misinformation and re-contextualized media in social media and
news leads to an increasing need for fact-checking methods. Concurrently, the
advancement in generative models makes cheapfakes and deepfakes both easier to
make and harder to detect. In this paper, we present a novel approach using
generative image models to our advantage for detecting Out-of-Context (OOC) use
of images-caption pairs in news. We present two new datasets with a total of
$6800$ images generated using two different generative models including (1)
DALL-E 2, and (2) Stable-Diffusion. We are confident that the method proposed
in this paper can further research on generative models in the field of
cheapfake detection, and that the resulting datasets can be used to train and
evaluate new models aimed at detecting cheapfakes. We run a preliminary
qualitative and quantitative analysis to evaluate the performance of each image
generation model for this task, and evaluate a handful of methods for computing
image similarity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moholdt_E/0/1/0/all/0/1&quot;&gt;Eivind Moholdt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Sohail Ahmed Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dang_Nguyen_D/0/1/0/all/0/1&quot;&gt;Duc-Tien Dang-Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16612">
<title>Neural Gradient Regularizer. (arXiv:2308.16612v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16612</link>
<description rdf:parseType="Literal">&lt;p&gt;Owing to its significant success, the prior imposed on gradient maps has
consistently been a subject of great interest in the field of image processing.
Total variation (TV), one of the most representative regularizers, is known for
its ability to capture the sparsity of gradient maps. Nonetheless, TV and its
variants often underestimate the gradient maps, leading to the weakening of
edges and details whose gradients should not be zero in the original image.
Recently, total deep variation (TDV) has been introduced, assuming the sparsity
of feature maps, which provides a flexible regularization learned from
large-scale datasets for a specific task. However, TDV requires retraining when
the image or task changes, limiting its versatility. In this paper, we propose
a neural gradient regularizer (NGR) that expresses the gradient map as the
output of a neural network. Unlike existing methods, NGR does not rely on the
sparsity assumption, thereby avoiding the underestimation of gradient maps. NGR
is applicable to various image types and different image processing tasks,
functioning in a zero-shot learning fashion, making it a versatile and
plug-and-play regularizer. Extensive experimental results demonstrate the
superior performance of NGR over state-of-the-art counterparts for a range of
different tasks, further validating its effectiveness and versatility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shuang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yifan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zixiang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jiangjun Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xiangyong Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1&quot;&gt;Deyu Meng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16632">
<title>3D-STMN: Dependency-Driven Superpoint-Text Matching Network for End-to-End 3D Referring Expression Segmentation. (arXiv:2308.16632v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16632</link>
<description rdf:parseType="Literal">&lt;p&gt;In 3D Referring Expression Segmentation (3D-RES), the earlier approach adopts
a two-stage paradigm, extracting segmentation proposals and then matching them
with referring expressions. However, this conventional paradigm encounters
significant challenges, most notably in terms of the generation of lackluster
initial proposals and a pronounced deceleration in inference speed. Recognizing
these limitations, we introduce an innovative end-to-end Superpoint-Text
Matching Network (3D-STMN) that is enriched by dependency-driven insights. One
of the keystones of our model is the Superpoint-Text Matching (STM) mechanism.
Unlike traditional methods that navigate through instance proposals, STM
directly correlates linguistic indications with their respective superpoints,
clusters of semantically related points. This architectural decision empowers
our model to efficiently harness cross-modal semantic relationships, primarily
leveraging densely annotated superpoint-text pairs, as opposed to the more
sparse instance-text pairs. In pursuit of enhancing the role of text in guiding
the segmentation process, we further incorporate the Dependency-Driven
Interaction (DDI) module to deepen the network&apos;s semantic comprehension of
referring expressions. Using the dependency trees as a beacon, this module
discerns the intricate relationships between primary terms and their associated
descriptors in expressions, thereby elevating both the localization and
segmentation capacities of our model. Comprehensive experiments on the
ScanRefer benchmark reveal that our model not only set new performance
standards, registering an mIoU gain of 11.7 points but also achieve a
staggering enhancement in inference speed, surpassing traditional methods by
95.7 times. The code and models are available at
https://github.com/sosppxo/3D-STMN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Changli Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yiwei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haowei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1&quot;&gt;Gen Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1&quot;&gt;Jiayi Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xiaoshuai Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16633">
<title>Semi-Supervised SAR ATR Framework with Transductive Auxiliary Segmentation. (arXiv:2308.16633v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16633</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional neural networks (CNNs) have achieved high performance in
synthetic aperture radar (SAR) automatic target recognition (ATR). However, the
performance of CNNs depends heavily on a large amount of training data. The
insufficiency of labeled training SAR images limits the recognition performance
and even invalidates some ATR methods. Furthermore, under few labeled training
data, many existing CNNs are even ineffective. To address these challenges, we
propose a Semi-supervised SAR ATR Framework with transductive Auxiliary
Segmentation (SFAS). The proposed framework focuses on exploiting the
transductive generalization on available unlabeled samples with an auxiliary
loss serving as a regularizer. Through auxiliary segmentation of unlabeled SAR
samples and information residue loss (IRL) in training, the framework can
employ the proposed training loop process and gradually exploit the information
compilation of recognition and segmentation to construct a helpful inductive
bias and achieve high performance. Experiments conducted on the MSTAR dataset
have shown the effectiveness of our proposed SFAS for few-shot learning. The
recognition performance of 94.18\% can be achieved under 20 training samples in
each class with simultaneous accurate segmentation results. Facing variances of
EOCs, the recognition ratios are higher than 88.00\% when 10 training samples
each class.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chenwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yulin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1&quot;&gt;Siyi Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1&quot;&gt;Jifang Pei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jianyu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_D/0/1/0/all/0/1&quot;&gt;Deqing Mao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16635">
<title>MFR-Net: Multi-faceted Responsive Listening Head Generation via Denoising Diffusion Model. (arXiv:2308.16635v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16635</link>
<description rdf:parseType="Literal">&lt;p&gt;Face-to-face communication is a common scenario including roles of speakers
and listeners. Most existing research methods focus on producing speaker
videos, while the generation of listener heads remains largely overlooked.
Responsive listening head generation is an important task that aims to model
face-to-face communication scenarios by generating a listener head video given
a speaker video and a listener head image. An ideal generated responsive
listening video should respond to the speaker with attitude or viewpoint
expressing while maintaining diversity in interaction patterns and accuracy in
listener identity information. To achieve this goal, we propose the
\textbf{M}ulti-\textbf{F}aceted \textbf{R}esponsive Listening Head Generation
Network (MFR-Net). Specifically, MFR-Net employs the probabilistic denoising
diffusion model to predict diverse head pose and expression features. In order
to perform multi-faceted response to the speaker video, while maintaining
accurate listener identity preservation, we design the Feature Aggregation
Module to boost listener identity features and fuse them with other
speaker-related features. Finally, a renderer finetuned with identity
consistency loss produces the final listening head videos. Our extensive
experiments demonstrate that MFR-Net not only achieves multi-faceted responses
in diversity and speaker identity information but also in attitude and
viewpoint expression.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1&quot;&gt;Xiaomeng Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1&quot;&gt;Yesheng Chai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Cai Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1&quot;&gt;Jiao Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jizhong Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16637">
<title>Learning Channel Importance for High Content Imaging with Interpretable Deep Input Channel Mixing. (arXiv:2308.16637v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16637</link>
<description rdf:parseType="Literal">&lt;p&gt;Uncovering novel drug candidates for treating complex diseases remain one of
the most challenging tasks in early discovery research. To tackle this
challenge, biopharma research established a standardized high content imaging
protocol that tags different cellular compartments per image channel. In order
to judge the experimental outcome, the scientist requires knowledge about the
channel importance with respect to a certain phenotype for decoding the
underlying biology. In contrast to traditional image analysis approaches, such
experiments are nowadays preferably analyzed by deep learning based approaches
which, however, lack crucial information about the channel importance. To
overcome this limitation, we present a novel approach which utilizes
multi-spectral information of high content images to interpret a certain aspect
of cellular biology. To this end, we base our method on image blending concepts
with alpha compositing for an arbitrary number of channels. More specifically,
we introduce DCMIX, a lightweight, scaleable and end-to-end trainable mixing
layer which enables interpretable predictions in high content imaging while
retaining the benefits of deep learning based methods. We employ an extensive
set of experiments on both MNIST and RXRX1 datasets, demonstrating that DCMIX
learns the biologically relevant channel importance without scarifying
prediction performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siegismund_D/0/1/0/all/0/1&quot;&gt;Daniel Siegismund&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wieser_M/0/1/0/all/0/1&quot;&gt;Mario Wieser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heyse_S/0/1/0/all/0/1&quot;&gt;Stephan Heyse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steigele_S/0/1/0/all/0/1&quot;&gt;Stephan Steigele&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16648">
<title>Generate Your Own Scotland: Satellite Image Generation Conditioned on Maps. (arXiv:2308.16648v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16648</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite recent advancements in image generation, diffusion models still
remain largely underexplored in Earth Observation. In this paper we show that
state-of-the-art pretrained diffusion models can be conditioned on cartographic
data to generate realistic satellite images. We provide two large datasets of
paired OpenStreetMap images and satellite views over the region of Mainland
Scotland and the Central Belt. We train a ControlNet model and qualitatively
evaluate the results, demonstrating that both image quality and map fidelity
are possible. Finally, we provide some insights on the opportunities and
challenges of applying these models for remote sensing. Our model weights and
code for creating the dataset are publicly available at
https://github.com/miquel-espinosa/map-sat.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Espinosa_M/0/1/0/all/0/1&quot;&gt;Miguel Espinosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crowley_E/0/1/0/all/0/1&quot;&gt;Elliot J. Crowley&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16649">
<title>Learning with Multi-modal Gradient Attention for Explainable Composed Image Retrieval. (arXiv:2308.16649v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16649</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of composed image retrieval that takes an input query
consisting of an image and a modification text indicating the desired changes
to be made on the image and retrieves images that match these changes. Current
state-of-the-art techniques that address this problem use global features for
the retrieval, resulting in incorrect localization of the regions of interest
to be modified because of the global nature of the features, more so in cases
of real-world, in-the-wild images. Since modifier texts usually correspond to
specific local changes in an image, it is critical that models learn local
features to be able to both localize and retrieve better. To this end, our key
novelty is a new gradient-attention-based learning objective that explicitly
forces the model to focus on the local regions of interest being modified in
each retrieval step. We achieve this by first proposing a new visual image
attention computation technique, which we call multi-modal gradient attention
(MMGrad) that is explicitly conditioned on the modifier text. We next
demonstrate how MMGrad can be incorporated into an end-to-end model training
strategy with a new learning objective that explicitly forces these MMGrad
attention maps to highlight the correct local regions corresponding to the
modifier text. By training retrieval models with this new loss function, we
show improved grounding by means of better visual attention maps, leading to
better explainability of the models as well as competitive quantitative
retrieval performance on standard benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Udhayanan_P/0/1/0/all/0/1&quot;&gt;Prateksha Udhayanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karanam_S/0/1/0/all/0/1&quot;&gt;Srikrishna Karanam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivasan_B/0/1/0/all/0/1&quot;&gt;Balaji Vasan Srinivasan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16651">
<title>SoccerNet 2023 Tracking Challenge -- 3rd place MOT4MOT Team Technical Report. (arXiv:2308.16651v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16651</link>
<description rdf:parseType="Literal">&lt;p&gt;The SoccerNet 2023 tracking challenge requires the detection and tracking of
soccer players and the ball. In this work, we present our approach to tackle
these tasks separately. We employ a state-of-the-art online multi-object
tracker and a contemporary object detector for player tracking. To overcome the
limitations of our online approach, we incorporate a post-processing stage
using interpolation and appearance-free track merging. Additionally, an
appearance-based track merging technique is used to handle the termination and
creation of tracks far from the image boundaries. Ball tracking is formulated
as single object detection, and a fine-tuned YOLOv8l detector with proprietary
filtering improves the detection precision. Our method achieves 3rd place on
the SoccerNet 2023 tracking challenge with a HOTA score of 66.27.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shitrit_G/0/1/0/all/0/1&quot;&gt;Gal Shitrit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beery_I/0/1/0/all/0/1&quot;&gt;Ishay Be&amp;#x27;ery&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yerhushalmy_I/0/1/0/all/0/1&quot;&gt;Ido Yerhushalmy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16682">
<title>Diffusion Inertial Poser: Human Motion Reconstruction from Arbitrary Sparse IMU Configurations. (arXiv:2308.16682v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16682</link>
<description rdf:parseType="Literal">&lt;p&gt;Motion capture from a limited number of inertial measurement units (IMUs) has
important applications in health, human performance, and virtual reality.
Real-world limitations and application-specific goals dictate different IMU
configurations (i.e., number of IMUs and chosen attachment body segments),
trading off accuracy and practicality. Although recent works were successful in
accurately reconstructing whole-body motion from six IMUs, these systems only
work with a specific IMU configuration. Here we propose a single diffusion
generative model, Diffusion Inertial Poser (DiffIP), which reconstructs human
motion in real-time from arbitrary IMU configurations. We show that DiffIP has
the benefit of flexibility with respect to the IMU configuration while being as
accurate as the state-of-the-art for the commonly used six IMU configuration.
Our system enables selecting an optimal configuration for different
applications without retraining the model. For example, when only four IMUs are
available, DiffIP found that the configuration that minimizes errors in joint
kinematics instruments the thighs and forearms. However, global translation
reconstruction is better when instrumenting the feet instead of the thighs.
Although our approach is agnostic to the underlying model, we built DiffIP
based on physiologically realistic musculoskeletal models to enable use in
biomedical research and health applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wouwe_T/0/1/0/all/0/1&quot;&gt;Tom Van Wouwe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seunghwan Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Falisse_A/0/1/0/all/0/1&quot;&gt;Antoine Falisse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delp_S/0/1/0/all/0/1&quot;&gt;Scott Delp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;C. Karen Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16684">
<title>Everyone Can Attack: Repurpose Lossy Compression as a Natural Backdoor Attack. (arXiv:2308.16684v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2308.16684</link>
<description rdf:parseType="Literal">&lt;p&gt;The vulnerabilities to backdoor attacks have recently threatened the
trustworthiness of machine learning models in practical applications.
Conventional wisdom suggests that not everyone can be an attacker since the
process of designing the trigger generation algorithm often involves
significant effort and extensive experimentation to ensure the attack&apos;s
stealthiness and effectiveness. Alternatively, this paper shows that there
exists a more severe backdoor threat: anyone can exploit an easily-accessible
algorithm for silent backdoor attacks. Specifically, this attacker can employ
the widely-used lossy image compression from a plethora of compression tools to
effortlessly inject a trigger pattern into an image without leaving any
noticeable trace; i.e., the generated triggers are natural artifacts. One does
not require extensive knowledge to click on the &quot;convert&quot; or &quot;save as&quot; button
while using tools for lossy image compression. Via this attack, the adversary
does not need to design a trigger generator as seen in prior works and only
requires poisoning the data. Empirically, the proposed attack consistently
achieves 100% attack success rate in several benchmark datasets such as MNIST,
CIFAR-10, GTSRB and CelebA. More significantly, the proposed attack can still
achieve almost 100% attack success rate with very small (approximately 10%)
poisoning rates in the clean label setting. The generated trigger of the
proposed attack using one lossy compression algorithm is also transferable
across other related compression algorithms, exacerbating the severity of this
backdoor threat. This work takes another crucial step toward understanding the
extensive risks of backdoor attacks in practice, urging practitioners to
investigate similar attacks and relevant backdoor mitigation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Sze Jue Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1&quot;&gt;Quang Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1&quot;&gt;Chee Seng Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doan_K/0/1/0/all/0/1&quot;&gt;Khoa Doan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16689">
<title>ViLTA: Enhancing Vision-Language Pre-training through Textual Augmentation. (arXiv:2308.16689v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16689</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision-language pre-training (VLP) methods are blossoming recently, and its
crucial goal is to jointly learn visual and textual features via a
transformer-based architecture, demonstrating promising improvements on a
variety of vision-language tasks. Prior arts usually focus on how to align
visual and textual features, but strategies for improving the robustness of
model and speeding up model convergence are left insufficiently explored.
&lt;/p&gt;
&lt;p&gt;In this paper, we propose a novel method ViLTA, comprising of two components
to further facilitate the model to learn fine-grained representations among
image-text pairs. For Masked Language Modeling (MLM), we propose a
cross-distillation method to generate soft labels to enhance the robustness of
model, which alleviates the problem of treating synonyms of masked words as
negative samples in one-hot labels. For Image-Text Matching (ITM), we leverage
the current language encoder to synthesize hard negatives based on the context
of language input, encouraging the model to learn high-quality representations
by increasing the difficulty of the ITM task. By leveraging the above
techniques, our ViLTA can achieve better performance on various vision-language
tasks. Extensive experiments on benchmark datasets demonstrate that the
effectiveness of ViLTA and its promising potential for vision-language
pre-training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weihan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1&quot;&gt;Bin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Juanzi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yankui Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16714">
<title>Towards Vehicle-to-everything Autonomous Driving: A Survey on Collaborative Perception. (arXiv:2308.16714v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16714</link>
<description rdf:parseType="Literal">&lt;p&gt;Vehicle-to-everything (V2X) autonomous driving opens up a promising direction
for developing a new generation of intelligent transportation systems.
Collaborative perception (CP) as an essential component to achieve V2X can
overcome the inherent limitations of individual perception, including occlusion
and long-range perception. In this survey, we provide a comprehensive review of
CP methods for V2X scenarios, bringing a profound and in-depth understanding to
the community. Specifically, we first introduce the architecture and workflow
of typical V2X systems, which affords a broader perspective to understand the
entire V2X system and the role of CP within it. Then, we thoroughly summarize
and analyze existing V2X perception datasets and CP methods. Particularly, we
introduce numerous CP methods from various crucial perspectives, including
collaboration stages, roadside sensors placement, latency compensation,
performance-bandwidth trade-off, attack/defense, pose alignment, etc. Moreover,
we conduct extensive experimental analyses to compare and examine current CP
methods, revealing some essential and unexplored insights. Specifically, we
analyze the performance changes of different methods under different
bandwidths, providing a deep insight into the performance-bandwidth trade-off
issue. Also, we examine methods under different LiDAR ranges. To study the
model robustness, we further investigate the effects of various simulated
real-world noises on the performance of different CP methods, covering
communication latency, lossy communication, localization errors, and mixed
noises. In addition, we look into the sim-to-real generalization ability of
existing CP methods. At last, we thoroughly discuss issues and challenges,
highlighting promising directions for future efforts. Our codes for
experimental analysis will be public at
https://github.com/memberRE/Collaborative-Perception.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Si Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Chen Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1&quot;&gt;Xingyu Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1&quot;&gt;Xianghao Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Runsheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1&quot;&gt;Wentao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_H/0/1/0/all/0/1&quot;&gt;Hao Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jiaqi Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Miao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16725">
<title>Terrain Diffusion Network: Climatic-Aware Terrain Generation with Geological Sketch Guidance. (arXiv:2308.16725v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16725</link>
<description rdf:parseType="Literal">&lt;p&gt;Sketch-based terrain generation seeks to create realistic landscapes for
virtual environments in various applications such as computer games, animation
and virtual reality. Recently, deep learning based terrain generation has
emerged, notably the ones based on generative adversarial networks (GAN).
However, these methods often struggle to fulfill the requirements of flexible
user control and maintain generative diversity for realistic terrain.
Therefore, we propose a novel diffusion-based method, namely terrain diffusion
network (TDN), which actively incorporates user guidance for enhanced
controllability, taking into account terrain features like rivers, ridges,
basins, and peaks. Instead of adhering to a conventional monolithic denoising
process, which often compromises the fidelity of terrain details or the
alignment with user control, a multi-level denoising scheme is proposed to
generate more realistic terrains by taking into account fine-grained details,
particularly those related to climatic patterns influenced by erosion and
tectonic activities. Specifically, three terrain synthesisers are designed for
structural, intermediate, and fine-grained level denoising purposes, which
allow each synthesiser concentrate on a distinct terrain aspect. Moreover, to
maximise the efficiency of our TDN, we further introduce terrain and sketch
latent spaces for the synthesizers with pre-trained terrain autoencoders.
Comprehensive experiments on a new dataset constructed from NASA Topology
Images clearly demonstrate the effectiveness of our proposed method, achieving
the state-of-the-art performance. Our code and dataset will be publicly
available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zexin Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1&quot;&gt;Kun Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mo_C/0/1/0/all/0/1&quot;&gt;Clinton Mo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1&quot;&gt;Lei Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16735">
<title>Post-Deployment Adaptation with Access to Source Data via Federated Learning and Source-Target Remote Gradient Alignment. (arXiv:2308.16735v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16735</link>
<description rdf:parseType="Literal">&lt;p&gt;Deployment of Deep Neural Networks in medical imaging is hindered by
distribution shift between training data and data processed after deployment,
causing performance degradation. Post-Deployment Adaptation (PDA) addresses
this by tailoring a pre-trained, deployed model to the target data distribution
using limited labelled or entirely unlabelled target data, while assuming no
access to source training data as they cannot be deployed with the model due to
privacy concerns and their large size. This makes reliable adaptation
challenging due to limited learning signal. This paper challenges this
assumption and introduces FedPDA, a novel adaptation framework that brings the
utility of learning from remote data from Federated Learning into PDA. FedPDA
enables a deployed model to obtain information from source data via remote
gradient exchange, while aiming to optimize the model specifically for the
target domain. Tailored for FedPDA, we introduce a novel optimization method
StarAlign (Source-Target Remote Gradient Alignment) that aligns gradients
between source-target domain pairs by maximizing their inner product, to
facilitate learning a target-specific model. We demonstrate the method&apos;s
effectiveness using multi-center databases for the tasks of cancer metastases
detection and skin lesion classification, where our method compares favourably
to previous work. Code is available at: https://github.com/FelixWag/StarAlign
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagner_F/0/1/0/all/0/1&quot;&gt;Felix Wagner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zeju Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_P/0/1/0/all/0/1&quot;&gt;Pramit Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamnitsas_K/0/1/0/all/0/1&quot;&gt;Konstantinos Kamnitsas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16738">
<title>US-SFNet: A Spatial-Frequency Domain-based Multi-branch Network for Cervical Lymph Node Lesions Diagnoses in Ultrasound Images. (arXiv:2308.16738v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.16738</link>
<description rdf:parseType="Literal">&lt;p&gt;Ultrasound imaging serves as a pivotal tool for diagnosing cervical lymph
node lesions. However, the diagnoses of these images largely hinge on the
expertise of medical practitioners, rendering the process susceptible to
misdiagnoses. Although rapidly developing deep learning has substantially
improved the diagnoses of diverse ultrasound images, there remains a
conspicuous research gap concerning cervical lymph nodes. The objective of our
work is to accurately diagnose cervical lymph node lesions by leveraging a deep
learning model. To this end, we first collected 3392 images containing normal
lymph nodes, benign lymph node lesions, malignant primary lymph node lesions,
and malignant metastatic lymph node lesions. Given that ultrasound images are
generated by the reflection and scattering of sound waves across varied bodily
tissues, we proposed the Conv-FFT Block. It integrates convolutional operations
with the fast Fourier transform to more astutely model the images. Building
upon this foundation, we designed a novel architecture, named US-SFNet. This
architecture not only discerns variances in ultrasound images from the spatial
domain but also adeptly captures microstructural alterations across various
lesions in the frequency domain. To ascertain the potential of US-SFNet, we
benchmarked it against 12 popular architectures through five-fold
cross-validation. The results show that US-SFNet is SOTA and can achieve 92.89%
accuracy, 90.46% precision, 89.95% sensitivity and 97.49% specificity,
respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yue_Y/0/1/0/all/0/1&quot;&gt;Yubiao Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xue_J/0/1/0/all/0/1&quot;&gt;Jun Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liang_H/0/1/0/all/0/1&quot;&gt;Haihua Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Luo_B/0/1/0/all/0/1&quot;&gt;Bingchun Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenzhang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16739">
<title>Parsing is All You Need for Accurate Gait Recognition in the Wild. (arXiv:2308.16739v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16739</link>
<description rdf:parseType="Literal">&lt;p&gt;Binary silhouettes and keypoint-based skeletons have dominated human gait
recognition studies for decades since they are easy to extract from video
frames. Despite their success in gait recognition for in-the-lab environments,
they usually fail in real-world scenarios due to their low information entropy
for gait representations. To achieve accurate gait recognition in the wild,
this paper presents a novel gait representation, named Gait Parsing Sequence
(GPS). GPSs are sequences of fine-grained human segmentation, i.e., human
parsing, extracted from video frames, so they have much higher information
entropy to encode the shapes and dynamics of fine-grained human parts during
walking. Moreover, to effectively explore the capability of the GPS
representation, we propose a novel human parsing-based gait recognition
framework, named ParsingGait. ParsingGait contains a Convolutional Neural
Network (CNN)-based backbone and two light-weighted heads. The first head
extracts global semantic features from GPSs, while the other one learns mutual
information of part-level features through Graph Convolutional Networks to
model the detailed dynamics of human walking. Furthermore, due to the lack of
suitable datasets, we build the first parsing-based dataset for gait
recognition in the wild, named Gait3D-Parsing, by extending the large-scale and
challenging Gait3D dataset. Based on Gait3D-Parsing, we comprehensively
evaluate our method and existing gait recognition methods. The experimental
results show a significant improvement in accuracy brought by the GPS
representation and the superiority of ParsingGait. The code and dataset are
available at https://gait3d.github.io/gait3d-parsing-hp .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Jinkai Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinchen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lihao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1&quot;&gt;Chenggang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wu Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16741">
<title>Socratis: Are large multimodal models emotionally aware?. (arXiv:2308.16741v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.16741</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing emotion prediction benchmarks contain coarse emotion labels which do
not consider the diversity of emotions that an image and text can elicit in
humans due to various reasons. Learning diverse reactions to multimodal content
is important as intelligent machines take a central role in generating and
delivering content to society. To address this gap, we propose Socratis, a
\underline{soc}ietal \underline{r}e\underline{a}c\underline{ti}on\underline{s}
benchmark, where each image-caption (IC) pair is annotated with multiple
emotions and the reasons for feeling them. Socratis contains 18K free-form
reactions for 980 emotions on 2075 image-caption pairs from 5 widely-read news
and image-caption (IC) datasets. We benchmark the capability of
state-of-the-art multimodal large language models to generate the reasons for
feeling an emotion given an IC pair. Based on a preliminary human study, we
observe that humans prefer human-written reasons over 2 times more often than
machine-generated ones. This shows our task is harder than standard generation
tasks because it starkly contrasts recent findings where humans cannot tell
apart machine vs human-written news articles, for instance. We further see that
current captioning metrics based on large vision-language models also fail to
correlate with human preferences. We hope that these findings and our benchmark
will inspire further research on training emotionally aware models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_K/0/1/0/all/0/1&quot;&gt;Katherine Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1&quot;&gt;Arijit Ray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_R/0/1/0/all/0/1&quot;&gt;Reuben Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gabriel_S/0/1/0/all/0/1&quot;&gt;Saadia Gabriel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plummer_B/0/1/0/all/0/1&quot;&gt;Bryan A. Plummer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1&quot;&gt;Kate Saenko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16742">
<title>Unsupervised CT Metal Artifact Reduction by Plugging Diffusion Priors in Dual Domains. (arXiv:2308.16742v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.16742</link>
<description rdf:parseType="Literal">&lt;p&gt;During the process of computed tomography (CT), metallic implants often cause
disruptive artifacts in the reconstructed images, impeding accurate diagnosis.
Several supervised deep learning-based approaches have been proposed for
reducing metal artifacts (MAR). However, these methods heavily rely on training
with simulated data, as obtaining paired metal artifact CT and clean CT data in
clinical settings is challenging. This limitation can lead to decreased
performance when applying these methods in clinical practice. Existing
unsupervised MAR methods, whether based on learning or not, typically operate
within a single domain, either in the image domain or the sinogram domain. In
this paper, we propose an unsupervised MAR method based on the diffusion model,
a generative model with a high capacity to represent data distributions.
Specifically, we first train a diffusion model using CT images without metal
artifacts. Subsequently, we iteratively utilize the priors embedded within the
pre-trained diffusion model in both the sinogram and image domains to restore
the degraded portions caused by metal artifacts. This dual-domain processing
empowers our approach to outperform existing unsupervised MAR methods,
including another MAR method based on the diffusion model, which we have
qualitatively and quantitatively validated using synthetic datasets. Moreover,
our method demonstrates superior visual results compared to both supervised and
unsupervised methods on clinical datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yaoqin Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Diao_S/0/1/0/all/0/1&quot;&gt;Songhui Diao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tan_S/0/1/0/all/0/1&quot;&gt;Shan Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xiaokun Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16758">
<title>Towards High-Fidelity Text-Guided 3D Face Generation and Manipulation Using only Images. (arXiv:2308.16758v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16758</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating 3D faces from textual descriptions has a multitude of
applications, such as gaming, movie, and robotics. Recent progresses have
demonstrated the success of unconditional 3D face generation and text-to-3D
shape generation. However, due to the limited text-3D face data pairs,
text-driven 3D face generation remains an open problem. In this paper, we
propose a text-guided 3D faces generation method, refer as TG-3DFace, for
generating realistic 3D faces using text guidance. Specifically, we adopt an
unconditional 3D face generation framework and equip it with text conditions,
which learns the text-guided 3D face generation with only text-2D face data. On
top of that, we propose two text-to-face cross-modal alignment techniques,
including the global contrastive learning and the fine-grained alignment
module, to facilitate high semantic consistency between generated 3D faces and
input texts. Besides, we present directional classifier guidance during the
inference process, which encourages creativity for out-of-domain generations.
Compared to the existing methods, TG-3DFace creates more realistic and
aesthetically pleasing 3D faces, boosting 9% multi-view consistency (MVIC) over
Latent3D. The rendered face images generated by TG-3DFace achieve higher FID
and CLIP score than text-to-2D face/image generation models, demonstrating our
superiority in generating realistic and semantic-consistent textures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Cuican Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1&quot;&gt;Guansong Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1&quot;&gt;Yihan Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jian Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huibin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zongben Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Songcen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hang Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16777">
<title>Ref-Diff: Zero-shot Referring Image Segmentation with Generative Models. (arXiv:2308.16777v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16777</link>
<description rdf:parseType="Literal">&lt;p&gt;Zero-shot referring image segmentation is a challenging task because it aims
to find an instance segmentation mask based on the given referring
descriptions, without training on this type of paired data. Current zero-shot
methods mainly focus on using pre-trained discriminative models (e.g., CLIP).
However, we have observed that generative models (e.g., Stable Diffusion) have
potentially understood the relationships between various visual elements and
text descriptions, which are rarely investigated in this task. In this work, we
introduce a novel Referring Diffusional segmentor (Ref-Diff) for this task,
which leverages the fine-grained multi-modal information from generative
models. We demonstrate that without a proposal generator, a generative model
alone can achieve comparable performance to existing SOTA weakly-supervised
models. When we combine both generative and discriminative models, our Ref-Diff
outperforms these competing methods by a significant margin. This indicates
that generative models are also beneficial for this task and can complement
discriminative models for better referring segmentation. Our code is publicly
available at https://github.com/kodenii/Ref-Diff.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_M/0/1/0/all/0/1&quot;&gt;Minheng Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yabo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_K/0/1/0/all/0/1&quot;&gt;Kailai Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yiwen Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1&quot;&gt;Wangmeng Zuo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16801">
<title>Multiscale Residual Learning of Graph Convolutional Sequence Chunks for Human Motion Prediction. (arXiv:2308.16801v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16801</link>
<description rdf:parseType="Literal">&lt;p&gt;A new method is proposed for human motion prediction by learning temporal and
spatial dependencies. Recently, multiscale graphs have been developed to model
the human body at higher abstraction levels, resulting in more stable motion
prediction. Current methods however predetermine scale levels and combine
spatially proximal joints to generate coarser scales based on human priors,
even though movement patterns in different motion sequences vary and do not
fully comply with a fixed graph of spatially connected joints. Another problem
with graph convolutional methods is mode collapse, in which predicted poses
converge around a mean pose with no discernible movements, particularly in
long-term predictions. To tackle these issues, we propose ResChunk, an
end-to-end network which explores dynamically correlated body components based
on the pairwise relationships between all joints in individual sequences.
ResChunk is trained to learn the residuals between target sequence chunks in an
autoregressive manner to enforce the temporal connectivities between
consecutive chunks. It is hence a sequence-to-sequence prediction network which
considers dynamic spatio-temporal features of sequences at multiple levels. Our
experiments on two challenging benchmark datasets, CMU Mocap and Human3.6M,
demonstrate that our proposed method is able to effectively model the sequence
information for motion prediction and outperform other techniques to set a new
state-of-the-art. Our code is available at
https://github.com/MohsenZand/ResChunk.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zand_M/0/1/0/all/0/1&quot;&gt;Mohsen Zand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1&quot;&gt;Ali Etemad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Greenspan_M/0/1/0/all/0/1&quot;&gt;Michael Greenspan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16819">
<title>BTSeg: Barlow Twins Regularization for Domain Adaptation in Semantic Segmentation. (arXiv:2308.16819v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16819</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic image segmentation is a critical component in many computer vision
systems, such as autonomous driving. In such applications, adverse conditions
(heavy rain, night time, snow, extreme lighting) on the one hand pose specific
challenges, yet are typically underrepresented in the available datasets.
Generating more training data is cumbersome and expensive, and the process
itself is error-prone due to the inherent aleatoric uncertainty. To address
this challenging problem, we propose BTSeg, which exploits image-level
correspondences as weak supervision signal to learn a segmentation model that
is agnostic to adverse conditions. To this end, our approach uses the Barlow
twins loss from the field of unsupervised learning and treats images taken at
the same location but under different adverse conditions as &quot;augmentations&quot; of
the same unknown underlying base image. This allows the training of a
segmentation model that is robust to appearance changes introduced by different
adverse conditions. We evaluate our approach on ACDC and the new challenging
ACG benchmark to demonstrate its robustness and generalization capabilities.
Our approach performs favorably when compared to the current state-of-the-art
methods, while also being simpler to implement and train. The code will be
released upon acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kunzel_J/0/1/0/all/0/1&quot;&gt;Johannes K&amp;#xfc;nzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hilsmann_A/0/1/0/all/0/1&quot;&gt;Anna Hilsmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eisert_P/0/1/0/all/0/1&quot;&gt;Peter Eisert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16825">
<title>Coarse-to-Fine Amodal Segmentation with Shape Prior. (arXiv:2308.16825v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16825</link>
<description rdf:parseType="Literal">&lt;p&gt;Amodal object segmentation is a challenging task that involves segmenting
both visible and occluded parts of an object. In this paper, we propose a novel
approach, called Coarse-to-Fine Segmentation (C2F-Seg), that addresses this
problem by progressively modeling the amodal segmentation. C2F-Seg initially
reduces the learning space from the pixel-level image space to the
vector-quantized latent space. This enables us to better handle long-range
dependencies and learn a coarse-grained amodal segment from visual features and
visible segments. However, this latent space lacks detailed information about
the object, which makes it difficult to provide a precise segmentation
directly. To address this issue, we propose a convolution refine module to
inject fine-grained information and provide a more precise amodal object
segmentation based on visual features and coarse-predicted segmentation. To
help the studies of amodal object segmentation, we create a synthetic amodal
dataset, named as MOViD-Amodal (MOViD-A), which can be used for both image and
video amodal object segmentation. We extensively evaluate our model on two
benchmark datasets: KINS and COCO-A. Our empirical results demonstrate the
superiority of C2F-Seg. Moreover, we exhibit the potential of our approach for
video amodal object segmentation tasks on FISHBOWL and our proposed MOViD-A.
Project page at: &lt;a href=&quot;http://jianxgao.github.io/C2F-Seg.&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jianxiong Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1&quot;&gt;Xuelin Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yikai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1&quot;&gt;Tianjun Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1&quot;&gt;Tong He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yanwei Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16847">
<title>Diffusion Models for Interferometric Satellite Aperture Radar. (arXiv:2308.16847v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16847</link>
<description rdf:parseType="Literal">&lt;p&gt;Probabilistic Diffusion Models (PDMs) have recently emerged as a very
promising class of generative models, achieving high performance in natural
image generation. However, their performance relative to non-natural images,
like radar-based satellite data, remains largely unknown. Generating large
amounts of synthetic (and especially labelled) satellite data is crucial to
implement deep-learning approaches for the processing and analysis of
(interferometric) satellite aperture radar data. Here, we leverage PDMs to
generate several radar-based satellite image datasets. We show that PDMs
succeed in generating images with complex and realistic structures, but that
sampling time remains an issue. Indeed, accelerated sampling strategies, which
work well on simple image datasets like MNIST, fail on our radar datasets. We
provide a simple and versatile open-source
https://github.com/thomaskerdreux/PDM_SAR_InSAR_generation to train, sample and
evaluate PDMs using any dataset on a single GPU.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuel_A/0/1/0/all/0/1&quot;&gt;Alexandre Tuel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kerdreux_T/0/1/0/all/0/1&quot;&gt;Thomas Kerdreux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hulbert_C/0/1/0/all/0/1&quot;&gt;Claudia Hulbert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rouet_Leduc_B/0/1/0/all/0/1&quot;&gt;Bertrand Rouet-Leduc&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16863">
<title>Self-pruning Graph Neural Network for Predicting Inflammatory Disease Activity in Multiple Sclerosis from Brain MR Images. (arXiv:2308.16863v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.16863</link>
<description rdf:parseType="Literal">&lt;p&gt;Multiple Sclerosis (MS) is a severe neurological disease characterized by
inflammatory lesions in the central nervous system. Hence, predicting
inflammatory disease activity is crucial for disease assessment and treatment.
However, MS lesions can occur throughout the brain and vary in shape, size and
total count among patients. The high variance in lesion load and locations
makes it challenging for machine learning methods to learn a globally effective
representation of whole-brain MRI scans to assess and predict disease.
Technically it is non-trivial to incorporate essential biomarkers such as
lesion load or spatial proximity. Our work represents the first attempt to
utilize graph neural networks (GNN) to aggregate these biomarkers for a novel
global representation. We propose a two-stage MS inflammatory disease activity
prediction approach. First, a 3D segmentation network detects lesions, and a
self-supervised algorithm extracts their image features. Second, the detected
lesions are used to build a patient graph. The lesions act as nodes in the
graph and are initialized with image features extracted in the first stage.
Finally, the lesions are connected based on their spatial proximity and the
inflammatory disease activity prediction is formulated as a graph
classification task. Furthermore, we propose a self-pruning strategy to
auto-select the most critical lesions for prediction. Our proposed method
outperforms the existing baseline by a large margin (AUCs of 0.67 vs. 0.61 and
0.66 vs. 0.60 for one-year and two-year inflammatory disease activity,
respectively). Finally, our proposed method enjoys inherent explainability by
assigning an importance score to each lesion for the overall prediction. Code
is available at https://github.com/chinmay5/ms_ida.git
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Prabhakar_C/0/1/0/all/0/1&quot;&gt;Chinmay Prabhakar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongwei Bran Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Paetzold_J/0/1/0/all/0/1&quot;&gt;Johannes C. Paetzold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Loehr_T/0/1/0/all/0/1&quot;&gt;Timo Loehr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Niu_C/0/1/0/all/0/1&quot;&gt;Chen Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Muhlau_M/0/1/0/all/0/1&quot;&gt;Mark M&amp;#xfc;hlau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1&quot;&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wiestler_B/0/1/0/all/0/1&quot;&gt;Benedikt Wiestler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Menze_B/0/1/0/all/0/1&quot;&gt;Bjoern Menze&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16875">
<title>Holistic Processing of Colour Images Using Novel Quaternion-Valued Wavelets on the Plane. (arXiv:2308.16875v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16875</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the applicability of quaternion-valued wavelets on the plane
to holistic colour image processing. We present a methodology for decomposing
and reconstructing colour images using quaternionic wavelet filters associated
to recently developed quaternion-valued wavelets on the plane. We consider
compression, enhancement, segmentation, and denoising techniques to demonstrate
quaternion-valued wavelets as a promising tool for holistic colour image
processing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dizon_N/0/1/0/all/0/1&quot;&gt;Neil D. Dizon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hogan_J/0/1/0/all/0/1&quot;&gt;Jeffrey A. Hogan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16876">
<title>SportsSloMo: A New Benchmark and Baselines for Human-centric Video Frame Interpolation. (arXiv:2308.16876v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16876</link>
<description rdf:parseType="Literal">&lt;p&gt;Human-centric video frame interpolation has great potential for improving
people&apos;s entertainment experiences and finding commercial applications in the
sports analysis industry, e.g., synthesizing slow-motion videos. Although there
are multiple benchmark datasets available in the community, none of them is
dedicated for human-centric scenarios. To bridge this gap, we introduce
SportsSloMo, a benchmark consisting of more than 130K video clips and 1M video
frames of high-resolution ($\geq$720p) slow-motion sports videos crawled from
YouTube. We re-train several state-of-the-art methods on our benchmark, and the
results show a decrease in their accuracy compared to other datasets. It
highlights the difficulty of our benchmark and suggests that it poses
significant challenges even for the best-performing methods, as human bodies
are highly deformable and occlusions are frequent in sports videos. To improve
the accuracy, we introduce two loss terms considering the human-aware priors,
where we add auxiliary supervision to panoptic segmentation and human keypoints
detection, respectively. The loss terms are model agnostic and can be easily
plugged into any video frame interpolation approaches. Experimental results
validate the effectiveness of our proposed loss terms, leading to consistent
performance improvement over 5 existing models, which establish strong baseline
models on our benchmark. The dataset and code can be found at:
https://neu-vi.github.io/SportsSlomo/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiaben Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Huaizu Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16880">
<title>Text2Scene: Text-driven Indoor Scene Stylization with Part-aware Details. (arXiv:2308.16880v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16880</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose Text2Scene, a method to automatically create realistic textures
for virtual scenes composed of multiple objects. Guided by a reference image
and text descriptions, our pipeline adds detailed texture on labeled 3D
geometries in the room such that the generated colors respect the hierarchical
structure or semantic parts that are often composed of similar materials.
Instead of applying flat stylization on the entire scene at a single step, we
obtain weak semantic cues from geometric segmentation, which are further
clarified by assigning initial colors to segmented parts. Then we add texture
details for individual objects such that their projections on image space
exhibit feature embedding aligned with the embedding of the input. The
decomposition makes the entire pipeline tractable to a moderate amount of
computation resources and memory. As our framework utilizes the existing
resources of image and text embedding, it does not require dedicated datasets
with high-quality textures designed by skillful artists. To the best of our
knowledge, it is the first practical and scalable approach that can create
detailed and realistic textures of the desired style that maintain structural
context for scenes with multiple objects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_I/0/1/0/all/0/1&quot;&gt;Inwoo Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyeonwoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Young Min Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16890">
<title>TouchStone: Evaluating Vision-Language Models by Language Models. (arXiv:2308.16890v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16890</link>
<description rdf:parseType="Literal">&lt;p&gt;Large vision-language models (LVLMs) have recently witnessed rapid
advancements, exhibiting a remarkable capacity for perceiving, understanding,
and processing visual information by connecting visual receptor with large
language models (LLMs). However, current assessments mainly focus on
recognizing and reasoning abilities, lacking direct evaluation of
conversational skills and neglecting visual storytelling abilities. In this
paper, we propose an evaluation method that uses strong LLMs as judges to
comprehensively evaluate the various abilities of LVLMs. Firstly, we construct
a comprehensive visual dialogue dataset TouchStone, consisting of open-world
images and questions, covering five major categories of abilities and 27
subtasks. This dataset not only covers fundamental recognition and
comprehension but also extends to literary creation. Secondly, by integrating
detailed image annotations we effectively transform the multimodal input
content into a form understandable by LLMs. This enables us to employ advanced
LLMs for directly evaluating the quality of the multimodal dialogue without
requiring human intervention. Through validation, we demonstrate that powerful
LVLMs, such as GPT-4, can effectively score dialogue quality by leveraging
their textual capabilities alone, aligning with human preferences. We hope our
work can serve as a touchstone for LVLMs&apos; evaluation and pave the way for
building stronger LVLMs. The evaluation code is available at
https://github.com/OFA-Sys/TouchStone.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1&quot;&gt;Shuai Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shusheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1&quot;&gt;Jinze Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xingxuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Junyang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinggang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingren Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16891">
<title>GNFactor: Multi-Task Real Robot Learning with Generalizable Neural Feature Fields. (arXiv:2308.16891v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2308.16891</link>
<description rdf:parseType="Literal">&lt;p&gt;It is a long-standing problem in robotics to develop agents capable of
executing diverse manipulation tasks from visual observations in unstructured
real-world environments. To achieve this goal, the robot needs to have a
comprehensive understanding of the 3D structure and semantics of the scene. In
this work, we present $\textbf{GNFactor}$, a visual behavior cloning agent for
multi-task robotic manipulation with $\textbf{G}$eneralizable $\textbf{N}$eural
feature $\textbf{F}$ields. GNFactor jointly optimizes a generalizable neural
field (GNF) as a reconstruction module and a Perceiver Transformer as a
decision-making module, leveraging a shared deep 3D voxel representation. To
incorporate semantics in 3D, the reconstruction module utilizes a
vision-language foundation model ($\textit{e.g.}$, Stable Diffusion) to distill
rich semantic information into the deep 3D voxel. We evaluate GNFactor on 3
real robot tasks and perform detailed ablations on 10 RLBench tasks with a
limited number of demonstrations. We observe a substantial improvement of
GNFactor over current state-of-the-art methods in seen and unseen tasks,
demonstrating the strong generalization ability of GNFactor. Our project
website is https://yanjieze.com/GNFactor/ .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ze_Y/0/1/0/all/0/1&quot;&gt;Yanjie Ze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_G/0/1/0/all/0/1&quot;&gt;Ge Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yueh-Hua Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Macaluso_A/0/1/0/all/0/1&quot;&gt;Annabella Macaluso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yuying Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jianglong Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hansen_N/0/1/0/all/0/1&quot;&gt;Nicklas Hansen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Li Erran Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaolong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16893">
<title>Language-Conditioned Path Planning. (arXiv:2308.16893v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2308.16893</link>
<description rdf:parseType="Literal">&lt;p&gt;Contact is at the core of robotic manipulation. At times, it is desired (e.g.
manipulation and grasping), and at times, it is harmful (e.g. when avoiding
obstacles). However, traditional path planning algorithms focus solely on
collision-free paths, limiting their applicability in contact-rich tasks. To
address this limitation, we propose the domain of Language-Conditioned Path
Planning, where contact-awareness is incorporated into the path planning
problem. As a first step in this domain, we propose Language-Conditioned
Collision Functions (LACO) a novel approach that learns a collision function
using only a single-view image, language prompt, and robot configuration. LACO
predicts collisions between the robot and the environment, enabling flexible,
conditional path planning without the need for manual object annotations, point
cloud data, or ground-truth object meshes. In both simulation and the real
world, we demonstrate that LACO can facilitate complex, nuanced path plans that
allow for interaction with objects that are safe to collide, rather than
prohibiting any collision.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_A/0/1/0/all/0/1&quot;&gt;Amber Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Youngwoon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1&quot;&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1&quot;&gt;Stephen James&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16894">
<title>EMDB: The Electromagnetic Database of Global 3D Human Pose and Shape in the Wild. (arXiv:2308.16894v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16894</link>
<description rdf:parseType="Literal">&lt;p&gt;We present EMDB, the Electromagnetic Database of Global 3D Human Pose and
Shape in the Wild. EMDB is a novel dataset that contains high-quality 3D SMPL
pose and shape parameters with global body and camera trajectories for
in-the-wild videos. We use body-worn, wireless electromagnetic (EM) sensors and
a hand-held iPhone to record a total of 58 minutes of motion data, distributed
over 81 indoor and outdoor sequences and 10 participants. Together with
accurate body poses and shapes, we also provide global camera poses and body
root trajectories. To construct EMDB, we propose a multi-stage optimization
procedure, which first fits SMPL to the 6-DoF EM measurements and then refines
the poses via image observations. To achieve high-quality results, we leverage
a neural implicit avatar model to reconstruct detailed human surface geometry
and appearance, which allows for improved alignment and smoothness via a dense
pixel-level objective. Our evaluations, conducted with a multi-view volumetric
capture system, indicate that EMDB has an expected accuracy of 2.3 cm
positional and 10.6 degrees angular error, surpassing the accuracy of previous
in-the-wild datasets. We evaluate existing state-of-the-art monocular RGB
methods for camera-relative and global pose estimation on EMDB. EMDB is
publicly available under https://ait.ethz.ch/emdb
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaufmann_M/0/1/0/all/0/1&quot;&gt;Manuel Kaufmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jie Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1&quot;&gt;Chen Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_K/0/1/0/all/0/1&quot;&gt;Kaiyue Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1&quot;&gt;Tianjian Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1&quot;&gt;Chengcheng Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zarate_J/0/1/0/all/0/1&quot;&gt;Juan Zarate&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1&quot;&gt;Otmar Hilliges&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16896">
<title>PointOcc: Cylindrical Tri-Perspective View for Point-based 3D Semantic Occupancy Prediction. (arXiv:2308.16896v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16896</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic segmentation in autonomous driving has been undergoing an evolution
from sparse point segmentation to dense voxel segmentation, where the objective
is to predict the semantic occupancy of each voxel in the concerned 3D space.
The dense nature of the prediction space has rendered existing efficient
2D-projection-based methods (e.g., bird&apos;s eye view, range view, etc.)
ineffective, as they can only describe a subspace of the 3D scene. To address
this, we propose a cylindrical tri-perspective view to represent point clouds
effectively and comprehensively and a PointOcc model to process them
efficiently. Considering the distance distribution of LiDAR point clouds, we
construct the tri-perspective view in the cylindrical coordinate system for
more fine-grained modeling of nearer areas. We employ spatial group pooling to
maintain structural details during projection and adopt 2D backbones to
efficiently process each TPV plane. Finally, we obtain the features of each
point by aggregating its projected features on each of the processed TPV planes
without the need for any post-processing. Extensive experiments on both 3D
occupancy prediction and LiDAR segmentation benchmarks demonstrate that the
proposed PointOcc achieves state-of-the-art performance with much faster speed.
Specifically, despite only using LiDAR, PointOcc significantly outperforms all
other methods, including multi-modal methods, with a large margin on the
OpenOccupancy benchmark. Code: https://github.com/wzzheng/PointOcc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_S/0/1/0/all/0/1&quot;&gt;Sicheng Zuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1&quot;&gt;Wenzhao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yuanhui Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jie Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiwen Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16905">
<title>InterDiff: Generating 3D Human-Object Interactions with Physics-Informed Diffusion. (arXiv:2308.16905v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16905</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper addresses a novel task of anticipating 3D human-object
interactions (HOIs). Most existing research on HOI synthesis lacks
comprehensive whole-body interactions with dynamic objects, e.g., often limited
to manipulating small or static objects. Our task is significantly more
challenging, as it requires modeling dynamic objects with various shapes,
capturing whole-body motion, and ensuring physically valid interactions. To
this end, we propose InterDiff, a framework comprising two key steps: (i)
interaction diffusion, where we leverage a diffusion model to encode the
distribution of future human-object interactions; (ii) interaction correction,
where we introduce a physics-informed predictor to correct denoised HOIs in a
diffusion step. Our key insight is to inject prior knowledge that the
interactions under reference with respect to contact points follow a simple
pattern and are easily predictable. Experiments on multiple human-object
interaction datasets demonstrate the effectiveness of our method for this task,
capable of producing realistic, vivid, and remarkably long-term 3D HOI
predictions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Sirui Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhengyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Xiong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gui_L/0/1/0/all/0/1&quot;&gt;Liang-Yan Gui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16906">
<title>Fine-Grained Cross-View Geo-Localization Using a Correlation-Aware Homography Estimator. (arXiv:2308.16906v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16906</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce a novel approach to fine-grained cross-view
geo-localization. Our method aligns a warped ground image with a corresponding
GPS-tagged satellite image covering the same area using homography estimation.
We first employ a differentiable spherical transform, adhering to geometric
principles, to accurately align the perspective of the ground image with the
satellite map. This transformation effectively places ground and aerial images
in the same view and on the same plane, reducing the task to an image alignment
problem. To address challenges such as occlusion, small overlapping range, and
seasonal variations, we propose a robust correlation-aware homography estimator
to align similar parts of the transformed ground image with the satellite
image. Our method achieves sub-pixel resolution and meter-level GPS accuracy by
mapping the center point of the transformed ground image to the satellite image
using a homography matrix and determining the orientation of the ground camera
using a point above the central axis. Operating at a speed of 30 FPS, our
method outperforms state-of-the-art techniques, reducing the mean metric
localization error by 21.3% and 32.4% in same-area and cross-area
generalization tasks on the VIGOR benchmark, respectively, and by 34.4% on the
KITTI benchmark in same-area evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaolong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Runsen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1&quot;&gt;Zuofan Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1&quot;&gt;Zeyu Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yu Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16909">
<title>StyleInV: A Temporal Style Modulated Inversion Network for Unconditional Video Generation. (arXiv:2308.16909v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16909</link>
<description rdf:parseType="Literal">&lt;p&gt;Unconditional video generation is a challenging task that involves
synthesizing high-quality videos that are both coherent and of extended
duration. To address this challenge, researchers have used pretrained StyleGAN
image generators for high-quality frame synthesis and focused on motion
generator design. The motion generator is trained in an autoregressive manner
using heavy 3D convolutional discriminators to ensure motion coherence during
video generation. In this paper, we introduce a novel motion generator design
that uses a learning-based inversion network for GAN. The encoder in our method
captures rich and smooth priors from encoding images to latents, and given the
latent of an initially generated frame as guidance, our method can generate
smooth future latent by modulating the inversion encoder temporally. Our method
enjoys the advantage of sparse training and naturally constrains the generation
space of our motion generator with the inversion network guided by the initial
frame, eliminating the need for heavy discriminators. Moreover, our method
supports style transfer with simple fine-tuning when the encoder is paired with
a pretrained StyleGAN generator. Extensive experiments conducted on various
benchmarks demonstrate the superiority of our method in generating long and
high-resolution videos with decent single-frame quality and temporal
consistency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuhan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1&quot;&gt;Liming Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1&quot;&gt;Chen Change Loy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16911">
<title>PointLLM: Empowering Large Language Models to Understand Point Clouds. (arXiv:2308.16911v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16911</link>
<description rdf:parseType="Literal">&lt;p&gt;The unprecedented advancements in Large Language Models (LLMs) have created a
profound impact on natural language processing but are yet to fully embrace the
realm of 3D understanding. This paper introduces PointLLM, a preliminary effort
to fill this gap, thereby enabling LLMs to understand point clouds and offering
a new avenue beyond 2D visual data. PointLLM processes colored object point
clouds with human instructions and generates contextually appropriate
responses, illustrating its grasp of point clouds and common sense.
Specifically, it leverages a point cloud encoder with a powerful LLM to
effectively fuse geometric, appearance, and linguistic information. We collect
a novel dataset comprising 660K simple and 70K complex point-text instruction
pairs to enable a two-stage training strategy: initially aligning latent spaces
and subsequently instruction-tuning the unified model. To rigorously evaluate
our model&apos;s perceptual abilities and its generalization capabilities, we
establish two benchmarks: Generative 3D Object Classification and 3D Object
Captioning, assessed through three different methods, including human
evaluation, GPT-4/ChatGPT evaluation, and traditional metrics. Experiment
results show that PointLLM demonstrates superior performance over existing 2D
baselines. Remarkably, in human-evaluated object captioning tasks, PointLLM
outperforms human annotators in over 50% of the samples. Codes, datasets, and
benchmarks are available at https://github.com/OpenRobotLab/PointLLM .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Runsen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaolong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yilun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1&quot;&gt;Jiangmiao Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dahua Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1912.10122">
<title>A Region-based Randers Geodesic Approach for Image Segmentation. (arXiv:1912.10122v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1912.10122</link>
<description rdf:parseType="Literal">&lt;p&gt;The geodesic model based on the eikonal partial differential equation (PDE)
has served as a fundamental tool for the applications of image segmentation and
boundary detection in the past two decades. However, the existing approaches
commonly only exploit the image edge-based features for computing minimal
geodesic paths, potentially limiting their performance in complicated
segmentation situations. In this paper, we introduce a new variational image
segmentation model based on the minimal geodesic path framework and the eikonal
PDE, where the region-based appearance term that defines then regional
homogeneity features can be taken into account for estimating the associated
minimal geodesic paths. This is done by constructing a Randers geodesic metric
interpretation of the region-based active contour energy functional. As a
result, the minimization of the active contour energy functional is transformed
into finding the solution to the Randers eikonal PDE.
&lt;/p&gt;
&lt;p&gt;We also suggest a practical interactive image segmentation strategy, where
the target boundary can be delineated by the concatenation of several piecewise
geodesic paths. We invoke the Finsler variant of the fast marching method to
estimate the geodesic distance map, yielding an efficient implementation of the
proposed region-based Randers geodesic model for image segmentation.
Experimental results on both synthetic and real images exhibit that our model
indeed achieves encouraging segmentation performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Da Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirebeau_J/0/1/0/all/0/1&quot;&gt;Jean-Marie Mirebeau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_H/0/1/0/all/0/1&quot;&gt;Huazhong Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_L/0/1/0/all/0/1&quot;&gt;Laurent D. Cohen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.08060">
<title>Leveraging Image-based Generative Adversarial Networks for Time Series Generation. (arXiv:2112.08060v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2112.08060</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative models for images have gained significant attention in computer
vision and natural language processing due to their ability to generate
realistic samples from complex data distributions. To leverage the advances of
image-based generative models for the time series domain, we propose a
two-dimensional image representation for time series, the Extended
Intertemporal Return Plot (XIRP). Our approach captures the intertemporal time
series dynamics in a scale-invariant and invertible way, reducing training time
and improving sample quality. We benchmark synthetic XIRPs obtained by an
off-the-shelf Wasserstein GAN with gradient penalty (WGAN-GP) to other image
representations and models regarding similarity and predictive ability metrics.
Our novel, validated image representation for time series consistently and
significantly outperforms a state-of-the-art RNN-based generative model
regarding predictive ability. Further, we introduce an improved stochastic
inversion to substantially improve simulation quality regardless of the
representation and provide the prospect of transfer potentials in other
domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hellermann_J/0/1/0/all/0/1&quot;&gt;Justin Hellermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lessmann_S/0/1/0/all/0/1&quot;&gt;Stefan Lessmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.04053">
<title>DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generation Models. (arXiv:2202.04053v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2202.04053</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, DALL-E, a multimodal transformer language model, and its variants,
including diffusion models, have shown high-quality text-to-image generation
capabilities. However, despite the realistic image generation results, there
has not been a detailed analysis of how to evaluate such models. In this work,
we investigate the visual reasoning capabilities and social biases of different
text-to-image models, covering both multimodal transformer language models and
diffusion models. First, we measure three visual reasoning skills: object
recognition, object counting, and spatial relation understanding. For this, we
propose PaintSkills, a compositional diagnostic evaluation dataset that
measures these skills. Despite the high-fidelity image generation capability, a
large gap exists between the performance of recent models and the upper bound
accuracy in object counting and spatial relation understanding skills. Second,
we assess the gender and skin tone biases by measuring the gender/skin tone
distribution of generated images across various professions and attributes. We
demonstrate that recent text-to-image generation models learn specific biases
about gender and skin tone from web image-text pairs. We hope our work will
help guide future progress in improving text-to-image generation models on
visual reasoning skills and learning socially unbiased representations. Code
and data: https://github.com/j-min/DallEval
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1&quot;&gt;Jaemin Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zala_A/0/1/0/all/0/1&quot;&gt;Abhay Zala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1&quot;&gt;Mohit Bansal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.07741">
<title>Edge Inference with Fully Differentiable Quantized Mixed Precision Neural Networks. (arXiv:2206.07741v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.07741</link>
<description rdf:parseType="Literal">&lt;p&gt;The large computing and memory cost of deep neural networks (DNNs) often
precludes their use in resource-constrained devices. Quantizing the parameters
and operations to lower bit-precision offers substantial memory and energy
savings for neural network inference, facilitating the use of DNNs on edge
computing platforms. Recent efforts at quantizing DNNs have employed a range of
techniques encompassing progressive quantization, step-size adaptation, and
gradient scaling. This paper proposes a new quantization approach for mixed
precision convolutional neural networks (CNNs) targeting edge-computing. Our
method establishes a new pareto frontier in model accuracy and memory footprint
demonstrating a range of quantized models, delivering best-in-class accuracy
below 4.3 MB of weights (wgts.) and activations (acts.). Our main contributions
are: (i) hardware-aware heterogeneous differentiable quantization with
tensor-sliced learned precision, (ii) targeted gradient modification for wgts.
and acts. to mitigate quantization errors, and (iii) a multi-phase learning
schedule to address instability in learning arising from updates to the learned
quantizer and model parameters. We demonstrate the effectiveness of our
techniques on the ImageNet dataset across a range of models including
EfficientNet-Lite0 (e.g., 4.14MB of wgts. and acts. at 67.66% accuracy) and
MobileNetV2 (e.g., 3.51MB wgts. and acts. at 65.39% accuracy).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaefer_C/0/1/0/all/0/1&quot;&gt;Clemens JS Schaefer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1&quot;&gt;Siddharth Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blazquez_R/0/1/0/all/0/1&quot;&gt;Raul Blazquez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.13085">
<title>Group DETR: Fast DETR Training with Group-Wise One-to-Many Assignment. (arXiv:2207.13085v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.13085</link>
<description rdf:parseType="Literal">&lt;p&gt;Detection transformer (DETR) relies on one-to-one assignment, assigning one
ground-truth object to one prediction, for end-to-end detection without NMS
post-processing. It is known that one-to-many assignment, assigning one
ground-truth object to multiple predictions, succeeds in detection methods such
as Faster R-CNN and FCOS. While the naive one-to-many assignment does not work
for DETR, and it remains challenging to apply one-to-many assignment for DETR
training. In this paper, we introduce Group DETR, a simple yet efficient DETR
training approach that introduces a group-wise way for one-to-many assignment.
This approach involves using multiple groups of object queries, conducting
one-to-one assignment within each group, and performing decoder self-attention
separately. It resembles data augmentation with automatically-learned object
query augmentation. It is also equivalent to simultaneously training
parameter-sharing networks of the same architecture, introducing more
supervision and thus improving DETR training. The inference process is the same
as DETR trained normally and only needs one group of queries without any
architecture modification. Group DETR is versatile and is applicable to various
DETR variants. The experiments show that Group DETR significantly speeds up the
training convergence and improves the performance of various DETR-based models.
Code will be available at \url{https://github.com/Atten4Vis/GroupDETR}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qiang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaokang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_K/0/1/0/all/0/1&quot;&gt;Kun Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1&quot;&gt;Haocheng Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Junyu Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1&quot;&gt;Errui Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1&quot;&gt;Gang Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingdong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.00780">
<title>Visual correspondence-based explanations improve AI robustness and human-AI team accuracy. (arXiv:2208.00780v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2208.00780</link>
<description rdf:parseType="Literal">&lt;p&gt;Explaining artificial intelligence (AI) predictions is increasingly important
and even imperative in many high-stakes applications where humans are the
ultimate decision-makers. In this work, we propose two novel architectures of
self-interpretable image classifiers that first explain, and then predict (as
opposed to post-hoc explanations) by harnessing the visual correspondences
between a query image and exemplars. Our models consistently improve (by 1 to 4
points) on out-of-distribution (OOD) datasets while performing marginally worse
(by 1 to 2 points) on in-distribution tests than ResNet-50 and a $k$-nearest
neighbor classifier (kNN). Via a large-scale, human study on ImageNet and CUB,
our correspondence-based explanations are found to be more useful to users than
kNN explanations. Our explanations help users more accurately reject AI&apos;s wrong
decisions than all other tested methods. Interestingly, for the first time, we
show that it is possible to achieve complementary human-AI team accuracy (i.e.,
that is higher than either AI-alone or human-alone), in ImageNet and CUB image
classification tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_G/0/1/0/all/0/1&quot;&gt;Giang Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taesiri_M/0/1/0/all/0/1&quot;&gt;Mohammad Reza Taesiri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1&quot;&gt;Anh Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.01887">
<title>Collecting The Puzzle Pieces: Disentangled Self-Driven Human Pose Transfer by Permuting Textures. (arXiv:2210.01887v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.01887</link>
<description rdf:parseType="Literal">&lt;p&gt;Human pose transfer synthesizes new view(s) of a person for a given pose.
Recent work achieves this via self-reconstruction, which disentangles a
person&apos;s pose and texture information by breaking the person down into parts,
then recombines them for reconstruction. However, part-level disentanglement
preserves some pose information that can create unwanted artifacts. In this
paper, we propose Pose Transfer by Permuting Textures (PT$^2$), an approach for
self-driven human pose transfer that disentangles pose from texture at the
patch-level. Specifically, we remove pose from an input image by permuting
image patches so only texture information remains. Then we reconstruct the
input image by sampling from the permuted textures for patch-level
disentanglement. To reduce noise and recover clothing shape information from
the permuted patches, we employ encoders with multiple kernel sizes in a triple
branch network. On DeepFashion and Market-1501, PT$^2$ reports significant
gains on automatic metrics over other self-driven methods, and even outperforms
some fully-supervised methods. A user study also reports images generated by
our method are preferred in 68% of cases over self-driven approaches from prior
work. Code is available at https://github.com/NannanLi999/pt_square.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1&quot;&gt;Nannan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shih_K/0/1/0/all/0/1&quot;&gt;Kevin J. Shih&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plummer_B/0/1/0/all/0/1&quot;&gt;Bryan A. Plummer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.00646">
<title>Learning Melanocytic Cell Masks from Adjacent Stained Tissue. (arXiv:2211.00646v3 [q-bio.QM] UPDATED)</title>
<link>http://arxiv.org/abs/2211.00646</link>
<description rdf:parseType="Literal">&lt;p&gt;Melanoma is one of the most aggressive forms of skin cancer, causing a large
proportion of skin cancer deaths. However, melanoma diagnoses by pathologists
shows low interrater reliability. As melanoma is a cancer of the melanocyte,
there is a clear need to develop a melanocytic cell segmentation tool that is
agnostic to pathologist variability and automates pixel-level annotation.
Gigapixel-level pathologist labeling, however, is impractical. Herein, we
propose a means to train deep neural networks for melanocytic cell segmentation
from hematoxylin and eosin (H&amp;amp;E) stained sections and paired
immunohistochemistry (IHC) of adjacent tissue sections, achieving a mean IOU of
0.64 despite imperfect ground-truth labels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Tada_M/0/1/0/all/0/1&quot;&gt;Mikio Tada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lang_U/0/1/0/all/0/1&quot;&gt;Ursula E. Lang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Yeh_I/0/1/0/all/0/1&quot;&gt;Iwei Yeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wei_M/0/1/0/all/0/1&quot;&gt;Maria L. Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Keiser_M/0/1/0/all/0/1&quot;&gt;Michael J. Keiser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.07864">
<title>Federated Adaptive Prompt Tuning for Multi-domain Collaborative Learning. (arXiv:2211.07864v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.07864</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) enables multiple clients to collaboratively train a
global model without disclosing their data. Previous researches often require
training the complete model parameters. However, the emergence of powerful
pre-trained models makes it possible to achieve higher performance with fewer
learnable parameters in FL. In this paper, we propose a federated adaptive
prompt tuning algorithm, FedAPT, for multi-domain collaborative image
classification with powerful foundation models, like CLIP. Compared with direct
federated prompt tuning, our core idea is to adaptively unlock specific domain
knowledge for each test sample in order to provide them with personalized
prompts. To implement this idea, we design an adaptive prompt tuning module,
which consists of a meta prompt, an adaptive network, and some keys. The server
randomly generates a set of keys and assigns a unique key to each client. Then
all clients cooperatively train the global adaptive network and meta prompt
with the local datasets and the frozen keys. Ultimately, the global aggregation
model can assign a personalized prompt to CLIP based on the domain features of
each test sample. We perform extensive experiments on two multi-domain image
classification datasets across two different settings - supervised and
unsupervised. The results show that FedAPT can achieve better performance with
less than 10\% of the number of parameters of the fully trained model, and the
global model can perform well in diverse client domains simultaneously.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1&quot;&gt;Shangchao Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Mingzhao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1&quot;&gt;Xiangyang Xue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.10892">
<title>Towards Realistic Out-of-Distribution Detection: A Novel Evaluation Framework for Improving Generalization in OOD Detection. (arXiv:2211.10892v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.10892</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel evaluation framework for Out-of-Distribution
(OOD) detection that aims to assess the performance of machine learning models
in more realistic settings. We observed that the real-world requirements for
testing OOD detection methods are not satisfied by the current testing
protocols. They usually encourage methods to have a strong bias towards a low
level of diversity in normal data. To address this limitation, we propose new
OOD test datasets (CIFAR-10-R, CIFAR-100-R, and ImageNet-30-R) that can allow
researchers to benchmark OOD detection performance under realistic distribution
shifts. Additionally, we introduce a Generalizability Score (GS) to measure the
generalization ability of a model during OOD detection. Our experiments
demonstrate that improving the performance on existing benchmark datasets does
not necessarily improve the usability of OOD detection models in real-world
scenarios. While leveraging deep pre-trained features has been identified as a
promising avenue for OOD detection research, our experiments show that
state-of-the-art pre-trained models tested on our proposed datasets suffer a
significant drop in performance. To address this issue, we propose a
post-processing stage for adapting pre-trained features under these
distribution shifts before calculating the OOD scores, which significantly
enhances the performance of state-of-the-art pre-trained models on our
benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khazaie_V/0/1/0/all/0/1&quot;&gt;Vahid Reza Khazaie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1&quot;&gt;Anthony Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabokrou_M/0/1/0/all/0/1&quot;&gt;Mohammad Sabokrou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.11827">
<title>High-Perceptual Quality JPEG Decoding via Posterior Sampling. (arXiv:2211.11827v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.11827</link>
<description rdf:parseType="Literal">&lt;p&gt;JPEG is arguably the most popular image coding format, achieving high
compression ratios via lossy quantization that may create visual artifacts
degradation. Numerous attempts to remove these artifacts were conceived over
the years, and common to most of these is the use of deterministic
post-processing algorithms that optimize some distortion measure (e.g., PSNR,
SSIM). In this paper we propose a different paradigm for JPEG artifact
correction: Our method is stochastic, and the objective we target is high
perceptual quality -- striving to obtain sharp, detailed and visually pleasing
reconstructed images, while being consistent with the compressed input. These
goals are achieved by training a stochastic conditional generator (conditioned
on the compressed input), accompanied by a theoretically well-founded loss
term, resulting in a sampler from the posterior distribution. Our solution
offers a diverse set of plausible and fast reconstructions for a given input
with perfect consistency. We demonstrate our scheme&apos;s unique properties and its
superiority to a variety of alternative methods on the FFHQ and ImageNet
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Man_S/0/1/0/all/0/1&quot;&gt;Sean Man&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ohayon_G/0/1/0/all/0/1&quot;&gt;Guy Ohayon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Adrai_T/0/1/0/all/0/1&quot;&gt;Theo Adrai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Elad_M/0/1/0/all/0/1&quot;&gt;Michael Elad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.02611">
<title>StyleGAN as a Utility-Preserving Face De-identification Method. (arXiv:2212.02611v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.02611</link>
<description rdf:parseType="Literal">&lt;p&gt;Face de-identification methods have been proposed to preserve users&apos; privacy
by obscuring their faces. These methods, however, can degrade the quality of
photos, and they usually do not preserve the utility of faces, i.e., their age,
gender, pose, and facial expression. Recently, GANs, such as StyleGAN, have
been proposed, which generate realistic, high-quality imaginary faces. In this
paper, we investigate the use of StyleGAN in generating de-identified faces
through style mixing. We examined this de-identification method for preserving
utility and privacy by implementing several face detection, verification, and
identification attacks and conducting a user study. The results from our
extensive experiments, human evaluation, and comparison with two
state-of-the-art methods, i.e., CIAGAN and DeepPrivacy, show that StyleGAN
performs on par or better than these methods, preserving users&apos; privacy and
images&apos; utility. In particular, the results of the machine learning-based
experiments show that StyleGAN0-4 preserves utility better than CIAGAN and
DeepPrivacy while preserving privacy at the same level. StyleGAN0-3 preserves
utility at the same level while providing more privacy. In this paper, for the
first time, we also performed a carefully designed user study to examine both
privacy and utility-preserving properties of StyleGAN0-3, 0-4, and 0-5, as well
as CIAGAN and DeepPrivacy from the human observers&apos; perspectives. Our
statistical tests showed that participants tend to verify and identify
StyleGAN0-5 images more easily than DeepPrivacy images. All the methods but
StyleGAN0-5 had significantly lower identification rates than CIAGAN. Regarding
utility, as expected, StyleGAN0-5 performed significantly better in preserving
some attributes. Among all methods, on average, participants believe gender has
been preserved the most while naturalness has been preserved the least.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khorzooghi_S/0/1/0/all/0/1&quot;&gt;Seyyed Mohammad Sadegh Moosavi Khorzooghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nilizadeh_S/0/1/0/all/0/1&quot;&gt;Shirin Nilizadeh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.00752">
<title>Point Cloud-based Proactive Link Quality Prediction for Millimeter-wave Communications. (arXiv:2301.00752v3 [cs.NI] UPDATED)</title>
<link>http://arxiv.org/abs/2301.00752</link>
<description rdf:parseType="Literal">&lt;p&gt;This study demonstrates the feasibility of point cloud-based proactive link
quality prediction for millimeter-wave (mmWave) communications. Previous
studies have proposed machine learning-based methods to predict received signal
strength for future time periods using time series of depth images to mitigate
the line-of-sight (LOS) path blockage by pedestrians in mmWave communication.
However, these image-based methods have limited applicability due to privacy
concerns as camera images may contain sensitive information. This study
proposes a point cloud-based method for mmWave link quality prediction and
demonstrates its feasibility through experiments. Point clouds represent
three-dimensional (3D) spaces as a set of points and are sparser and less
likely to contain sensitive information than camera images. Additionally, point
clouds provide 3D position and motion information, which is necessary for
understanding the radio propagation environment involving pedestrians. This
study designs the mmWave link quality prediction method and conducts realistic
indoor experiments, where the link quality fluctuates significantly due to
human blockage, using commercially available IEEE 802.11ad-based 60 GHz
wireless LAN devices and Kinect v2 RGB-D camera and Velodyne VLP-16 light
detection and ranging (LiDAR) for point cloud acquisition. The experimental
results showed that our proposed method can predict future large attenuation of
mmWave received signal strength and throughput induced by the LOS path blockage
by pedestrians with comparable or superior accuracy to image-based prediction
methods. Hence, our point cloud-based method can serve as a viable alternative
to image-based methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ohta_S/0/1/0/all/0/1&quot;&gt;Shoki Ohta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nishio_T/0/1/0/all/0/1&quot;&gt;Takayuki Nishio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kudo_R/0/1/0/all/0/1&quot;&gt;Riichi Kudo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takahashi_K/0/1/0/all/0/1&quot;&gt;Kahoko Takahashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagata_H/0/1/0/all/0/1&quot;&gt;Hisashi Nagata&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.03022">
<title>SurgT challenge: Benchmark of Soft-Tissue Trackers for Robotic Surgery. (arXiv:2302.03022v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.03022</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces the ``SurgT: Surgical Tracking&quot; challenge which was
organised in conjunction with MICCAI 2022. There were two purposes for the
creation of this challenge: (1) the establishment of the first standardised
benchmark for the research community to assess soft-tissue trackers; and (2) to
encourage the development of unsupervised deep learning methods, given the lack
of annotated data in surgery. A dataset of 157 stereo endoscopic videos from 20
clinical cases, along with stereo camera calibration parameters, have been
provided. Participants were assigned the task of developing algorithms to track
the movement of soft tissues, represented by bounding boxes, in stereo
endoscopic videos. At the end of the challenge, the developed methods were
assessed on a previously hidden test subset. This assessment uses benchmarking
metrics that were purposely developed for this challenge, to verify the
efficacy of unsupervised deep learning algorithms in tracking soft-tissue. The
metric used for ranking the methods was the Expected Average Overlap (EAO)
score, which measures the average overlap between a tracker&apos;s and the ground
truth bounding boxes. Coming first in the challenge was the deep learning
submission by ICVS-2Ai with a superior EAO score of 0.617. This method employs
ARFlow to estimate unsupervised dense optical flow from cropped images, using
photometric and regularization losses. Second, Jmees with an EAO of 0.583, uses
deep learning for surgical tool segmentation on top of a non-deep learning
baseline method: CSRT. CSRT by itself scores a similar EAO of 0.563. The
results from this challenge show that currently, non-deep learning methods are
still competitive. The dataset and benchmarking tool created for this challenge
have been made publicly available at https://surgt.grand-challenge.org/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cartucho_J/0/1/0/all/0/1&quot;&gt;Joao Cartucho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weld_A/0/1/0/all/0/1&quot;&gt;Alistair Weld&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tukra_S/0/1/0/all/0/1&quot;&gt;Samyakh Tukra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Haozheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matsuzaki_H/0/1/0/all/0/1&quot;&gt;Hiroki Matsuzaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ishikawa_T/0/1/0/all/0/1&quot;&gt;Taiyo Ishikawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_M/0/1/0/all/0/1&quot;&gt;Minjun Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1&quot;&gt;Yong Eun Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kwang-Ju Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1&quot;&gt;Gwang Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_B/0/1/0/all/0/1&quot;&gt;Bizhe Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kahrs_L/0/1/0/all/0/1&quot;&gt;Lueder Kahrs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boecking_L/0/1/0/all/0/1&quot;&gt;Lars Boecking&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allmendinger_S/0/1/0/all/0/1&quot;&gt;Simeon Allmendinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_L/0/1/0/all/0/1&quot;&gt;Leopold Muller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yitong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yueming Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bano_S/0/1/0/all/0/1&quot;&gt;Sophia Bano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasconcelos_F/0/1/0/all/0/1&quot;&gt;Francisco Vasconcelos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reiter_W/0/1/0/all/0/1&quot;&gt;Wolfgang Reiter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hajek_J/0/1/0/all/0/1&quot;&gt;Jonas Hajek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_B/0/1/0/all/0/1&quot;&gt;Bruno Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lima_E/0/1/0/all/0/1&quot;&gt;Estevao Lima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vilaca_J/0/1/0/all/0/1&quot;&gt;Joao L. Vilaca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Queiros_S/0/1/0/all/0/1&quot;&gt;Sandro Queiros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giannarou_S/0/1/0/all/0/1&quot;&gt;Stamatia Giannarou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.07669">
<title>Unsupervised Hashing with Similarity Distribution Calibration. (arXiv:2302.07669v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.07669</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised hashing methods typically aim to preserve the similarity between
data points in a feature space by mapping them to binary hash codes. However,
these methods often overlook the fact that the similarity between data points
in the continuous feature space may not be preserved in the discrete hash code
space, due to the limited similarity range of hash codes. The similarity range
is bounded by the code length and can lead to a problem known as similarity
collapse. That is, the positive and negative pairs of data points become less
distinguishable from each other in the hash space. To alleviate this problem,
in this paper a novel Similarity Distribution Calibration (SDC) method is
introduced. SDC aligns the hash code similarity distribution towards a
calibration distribution (e.g., beta distribution) with sufficient spread
across the entire similarity range, thus alleviating the similarity collapse
problem. Extensive experiments show that our SDC outperforms significantly the
state-of-the-art alternatives on coarse category-level and instance-level image
retrieval. Code is available at https://github.com/kamwoh/sdc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_K/0/1/0/all/0/1&quot;&gt;Kam Woh Ng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiatian Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoe_J/0/1/0/all/0/1&quot;&gt;Jiun Tian Hoe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1&quot;&gt;Chee Seng Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yi-Zhe Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1&quot;&gt;Tao Xiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.00262">
<title>Collage Diffusion. (arXiv:2303.00262v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.00262</link>
<description rdf:parseType="Literal">&lt;p&gt;We seek to give users precise control over diffusion-based image generation
by modeling complex scenes as sequences of layers, which define the desired
spatial arrangement and visual attributes of objects in the scene. Collage
Diffusion harmonizes the input layers to make objects fit together -- the key
challenge involves minimizing changes in the positions and key visual
attributes of the input layers while allowing other attributes to change in the
harmonization process. We ensure that objects are generated in the correct
locations by modifying text-image cross-attention with the layers&apos; alpha masks.
We preserve key visual attributes of input layers by learning specialized text
representations per layer and by extending ControlNet to operate on layers.
Layer input allows users to control the extent of image harmonization on a
per-object basis, and users can even iteratively edit individual objects in
generated images while keeping other objects fixed. By leveraging the rich
information present in layer input, Collage Diffusion generates globally
harmonized images that maintain desired object characteristics better than
prior approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarukkai_V/0/1/0/all/0/1&quot;&gt;Vishnu Sarukkai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Linden Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_A/0/1/0/all/0/1&quot;&gt;Arden Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1&quot;&gt;Christopher R&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fatahalian_K/0/1/0/all/0/1&quot;&gt;Kayvon Fatahalian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.05102">
<title>StyleDiff: Attribute Comparison Between Unlabeled Datasets in Latent Disentangled Space. (arXiv:2303.05102v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2303.05102</link>
<description rdf:parseType="Literal">&lt;p&gt;One major challenge in machine learning applications is coping with
mismatches between the datasets used in the development and those obtained in
real-world applications. These mismatches may lead to inaccurate predictions
and errors, resulting in poor product quality and unreliable systems. In this
study, we propose StyleDiff to inform developers of the differences between the
two datasets for the steady development of machine learning systems. Using
disentangled image spaces obtained from recently proposed generative models,
StyleDiff compares the two datasets by focusing on attributes in the images and
provides an easy-to-understand analysis of the differences between the
datasets. The proposed StyleDiff performs in $O (d N\log N)$, where $N$ is the
size of the datasets and $d$ is the number of attributes, enabling the
application to large datasets. We demonstrate that StyleDiff accurately detects
differences between datasets and presents them in an understandable format
using, for example, driving scenes datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kawano_K/0/1/0/all/0/1&quot;&gt;Keisuke Kawano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kutsuna_T/0/1/0/all/0/1&quot;&gt;Takuro Kutsuna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tokuhisa_R/0/1/0/all/0/1&quot;&gt;Ryoko Tokuhisa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nakamura_A/0/1/0/all/0/1&quot;&gt;Akihiro Nakamura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Esaki_Y/0/1/0/all/0/1&quot;&gt;Yasushi Esaki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.05800">
<title>Enhancing the accuracies by performing pooling decisions adjacent to the output layer. (arXiv:2303.05800v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.05800</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning classification tasks of (2^nx2^n) inputs typically consist of \le n
(2x2) max-pooling (MP) operators along the entire feedforward deep
architecture. Here we show, using the CIFAR-10 database, that pooling decisions
adjacent to the last convolutional layer significantly enhance accuracies. In
particular, average accuracies of the advanced-VGG with m layers (A-VGGm)
architectures are 0.936, 0.940, 0.954, 0.955, and 0.955 for m=6, 8, 14, 13, and
16, respectively. The results indicate A-VGG8s&apos; accuracy is superior to
VGG16s&apos;, and that the accuracies of A-VGG13 and A-VGG16 are equal, and
comparable to that of Wide-ResNet16. In addition, replacing the three fully
connected (FC) layers with one FC layer, A-VGG6 and A-VGG14, or with several
linear activation FC layers, yielded similar accuracies. These significantly
enhanced accuracies stem from training the most influential input-output
routes, in comparison to the inferior routes selected following multiple MP
decisions along the deep architecture. In addition, accuracies are sensitive to
the order of the non-commutative MP and average pooling operators adjacent to
the output layer, varying the number and location of training routes. The
results call for the reexamination of previously proposed deep architectures
and their accuracies by utilizing the proposed pooling strategy adjacent to the
output layer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meir_Y/0/1/0/all/0/1&quot;&gt;Yuval Meir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tzach_Y/0/1/0/all/0/1&quot;&gt;Yarden Tzach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gross_R/0/1/0/all/0/1&quot;&gt;Ronit D. Gross&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tevet_O/0/1/0/all/0/1&quot;&gt;Ofek Tevet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vardi_R/0/1/0/all/0/1&quot;&gt;Roni Vardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanter_I/0/1/0/all/0/1&quot;&gt;Ido Kanter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.06681">
<title>Learning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction. (arXiv:2303.06681v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.06681</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparse-view cone-beam CT (CBCT) reconstruction is an important direction to
reduce radiation dose and benefit clinical applications. Previous voxel-based
generation methods represent the CT as discrete voxels, resulting in high
memory requirements and limited spatial resolution due to the use of 3D
decoders. In this paper, we formulate the CT volume as a continuous intensity
field and develop a novel DIF-Net to perform high-quality CBCT reconstruction
from extremely sparse (fewer than 10) projection views at an ultrafast speed.
The intensity field of a CT can be regarded as a continuous function of 3D
spatial points. Therefore, the reconstruction can be reformulated as regressing
the intensity value of an arbitrary 3D point from given sparse projections.
Specifically, for a point, DIF-Net extracts its view-specific features from
different 2D projection views. These features are subsequently aggregated by a
fusion module for intensity estimation. Notably, thousands of points can be
processed in parallel to improve efficiency during training and testing. In
practice, we collect a knee CBCT dataset to train and evaluate DIF-Net.
Extensive experiments show that our approach can reconstruct CBCT with high
image quality and high spatial resolution from extremely sparse views within
1.6 seconds, significantly outperforming state-of-the-art methods. Our code
will be available at https://github.com/xmed-lab/DIF-Net.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yiqun Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Zhongjin Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wei Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaomeng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.07806">
<title>USAGE: A Unified Seed Area Generation Paradigm for Weakly Supervised Semantic Segmentation. (arXiv:2303.07806v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.07806</link>
<description rdf:parseType="Literal">&lt;p&gt;Seed area generation is usually the starting point of weakly supervised
semantic segmentation (WSSS). Computing the Class Activation Map (CAM) from a
multi-label classification network is the de facto paradigm for seed area
generation, but CAMs generated from Convolutional Neural Networks (CNNs) and
Transformers are prone to be under- and over-activated, respectively, which
makes the strategies to refine CAMs for CNNs usually inappropriate for
Transformers, and vice versa. In this paper, we propose a Unified optimization
paradigm for Seed Area GEneration (USAGE) for both types of networks, in which
the objective function to be optimized consists of two terms: One is a
generation loss, which controls the shape of seed areas by a temperature
parameter following a deterministic principle for different types of networks;
The other is a regularization loss, which ensures the consistency between the
seed areas that are generated by self-adaptive network adjustment from
different views, to overturn false activation in seed areas. Experimental
results show that USAGE consistently improves seed area generation for both
CNNs and Transformers by large margins, e.g., outperforming state-of-the-art
methods by a mIoU of 4.1% on PASCAL VOC. Moreover, based on the USAGE-generated
seed areas on Transformers, we achieve state-of-the-art WSSS results on both
PASCAL VOC and MS COCO.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1&quot;&gt;Zelin Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guanchun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1&quot;&gt;Lingxi Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1&quot;&gt;Dongsheng Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1&quot;&gt;Wei Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1&quot;&gt;Qi Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.08566">
<title>Sensitivity-Aware Visual Parameter-Efficient Fine-Tuning. (arXiv:2303.08566v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.08566</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual Parameter-Efficient Fine-Tuning (PEFT) has become a powerful
alternative for full fine-tuning so as to adapt pre-trained vision models to
downstream tasks, which only tunes a small number of parameters while freezing
the vast majority ones to ease storage burden and optimization difficulty.
However, existing PEFT methods introduce trainable parameters to the same
positions across different tasks depending solely on human heuristics and
neglect the domain gaps. To this end, we study where to introduce and how to
allocate trainable parameters by proposing a novel Sensitivity-aware visual
Parameter-efficient fine-Tuning (SPT) scheme, which adaptively allocates
trainable parameters to task-specific important positions given a desired
tunable parameter budget. Specifically, our SPT first quickly identifies the
sensitive parameters that require tuning for a given task in a data-dependent
way. Next, our SPT further boosts the representational capability for the
weight matrices whose number of sensitive parameters exceeds a pre-defined
threshold by utilizing existing structured tuning methods, e.g., LoRA [23] or
Adapter [22], to replace directly tuning the selected sensitive parameters
(unstructured tuning) under the budget. Extensive experiments on a wide range
of downstream recognition tasks show that our SPT is complementary to the
existing PEFT methods and largely boosts their performance, e.g., SPT improves
Adapter with supervised pre-trained ViT-B/16 backbone by 4.2% and 1.4% mean
Top-1 accuracy, reaching SOTA performance on FGVC and VTAB-1k benchmarks,
respectively. Source code is at https://github.com/ziplab/SPT
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;Haoyu He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1&quot;&gt;Jianfei Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1&quot;&gt;Bohan Zhuang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.12059">
<title>Motion Matters: Neural Motion Transfer for Better Camera Physiological Measurement. (arXiv:2303.12059v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.12059</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning models for camera-based physiological measurement can have
weak generalization due to a lack of representative training data. Body motion
is one of the most significant sources of noise when attempting to recover the
subtle cardiac pulse from a video. We explore motion transfer as a form of data
augmentation to introduce motion variation while preserving physiological
changes of interest. We adapt a neural video synthesis approach to augment
videos for the task of remote photoplethysmography (rPPG) and study the effects
of motion augmentation with respect to 1) the magnitude and 2) the type of
motion. After training on motion-augmented versions of publicly available
datasets, we demonstrate a 47% improvement over existing inter-dataset results
using various state-of-the-art methods on the PURE dataset. We also present
inter-dataset results on five benchmark datasets to show improvements of up to
79% using TS-CAN, a neural rPPG estimation method. Our findings illustrate the
usefulness of motion transfer as a data augmentation technique for improving
the generalization of models for camera-based physiological sensing. We release
our code for using motion transfer as a data augmentation technique on three
publicly available datasets, UBFC-rPPG, PURE, and SCAMPS, and models
pre-trained on motion-augmented data here: https://motion-matters.github.io/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paruchuri_A/0/1/0/all/0/1&quot;&gt;Akshay Paruchuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yulu Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1&quot;&gt;Shwetak Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McDuff_D/0/1/0/all/0/1&quot;&gt;Daniel McDuff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1&quot;&gt;Soumyadip Sengupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.12743">
<title>DR.CPO: Diversified and Realistic 3D Augmentation via Iterative Construction, Random Placement, and HPR Occlusion. (arXiv:2303.12743v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.12743</link>
<description rdf:parseType="Literal">&lt;p&gt;In autonomous driving, data augmentation is commonly used for improving 3D
object detection. The most basic methods include insertion of copied objects
and rotation and scaling of the entire training frame. Numerous variants have
been developed as well. The existing methods, however, are considerably limited
when compared to the variety of the real world possibilities. In this work, we
develop a diversified and realistic augmentation method that can flexibly
construct a whole-body object, freely locate and rotate the object, and apply
self-occlusion and external-occlusion accordingly. To improve the diversity of
the whole-body object construction, we develop an iterative method that
stochastically combines multiple objects observed from the real world into a
single object. Unlike the existing augmentation methods, the constructed
objects can be randomly located and rotated in the training frame because
proper occlusions can be reflected to the whole-body objects in the final step.
Finally, proper self-occlusion at each local object level and
external-occlusion at the global frame level are applied using the Hidden Point
Removal (HPR) algorithm that is computationally efficient. HPR is also used for
adaptively controlling the point density of each object according to the
object&apos;s distance from the LiDAR. Experiment results show that the proposed
DR.CPO algorithm is data-efficient and model-agnostic without incurring any
computational overhead. Also, DR.CPO can improve mAP performance by 2.08% when
compared to the best 3D detection result known for KITTI dataset. The code is
available at https://github.com/SNU-DRL/DRCPO.git
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1&quot;&gt;Jungwook Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jaeill Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kyungeun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1&quot;&gt;Hyunghun Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rhee_W/0/1/0/all/0/1&quot;&gt;Wonjong Rhee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.13241">
<title>6D Object Pose Estimation from Approximate 3D Models for Orbital Robotics. (arXiv:2303.13241v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.13241</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel technique to estimate the 6D pose of objects from single
images where the 3D geometry of the object is only given approximately and not
as a precise 3D model. To achieve this, we employ a dense 2D-to-3D
correspondence predictor that regresses 3D model coordinates for every pixel.
In addition to the 3D coordinates, our model also estimates the pixel-wise
coordinate error to discard correspondences that are likely wrong. This allows
us to generate multiple 6D pose hypotheses of the object, which we then refine
iteratively using a highly efficient region-based approach. We also introduce a
novel pixel-wise posterior formulation by which we can estimate the probability
for each hypothesis and select the most likely one. As we show in experiments,
our approach is capable of dealing with extreme visual conditions including
overexposure, high contrast, or low signal-to-noise ratio. This makes it a
powerful technique for the particularly challenging task of estimating the pose
of tumbling satellites for in-orbit robotic applications. Our method achieves
state-of-the-art performance on the SPEED+ dataset and has won the SPEC2021
post-mortem competition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ulmer_M/0/1/0/all/0/1&quot;&gt;Maximilian Ulmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durner_M/0/1/0/all/0/1&quot;&gt;Maximilian Durner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sundermeyer_M/0/1/0/all/0/1&quot;&gt;Martin Sundermeyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoiber_M/0/1/0/all/0/1&quot;&gt;Manuel Stoiber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Triebel_R/0/1/0/all/0/1&quot;&gt;Rudolph Triebel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.03384">
<title>Beyond NeRF Underwater: Learning Neural Reflectance Fields for True Color Correction of Marine Imagery. (arXiv:2304.03384v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.03384</link>
<description rdf:parseType="Literal">&lt;p&gt;Underwater imagery often exhibits distorted coloration as a result of
light-water interactions, which complicates the study of benthic environments
in marine biology and geography. In this research, we propose an algorithm to
restore the true color (albedo) in underwater imagery by jointly learning the
effects of the medium and neural scene representations. Our approach models
water effects as a combination of light attenuation with distance and
backscattered light. The proposed neural scene representation is based on a
neural reflectance field model, which learns albedos, normals, and volume
densities of the underwater environment. We introduce a logistic regression
model to separate water from the scene and apply distinct light physics during
training. Our method avoids the need to estimate complex backscatter effects in
water by employing several approximations, enhancing sampling efficiency and
numerical stability during training. The proposed technique integrates
underwater light effects into a volume rendering framework with end-to-end
differentiability. Experimental results on both synthetic and real-world data
demonstrate that our method effectively restores true color from underwater
imagery, outperforming existing approaches in terms of color consistency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johnson_Roberson_M/0/1/0/all/0/1&quot;&gt;Matthew Johnson-Roberson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.05821">
<title>DUFormer: Solving Power Line Detection Task in Aerial Images using Semantic Segmentation. (arXiv:2304.05821v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.05821</link>
<description rdf:parseType="Literal">&lt;p&gt;Unmanned aerial vehicles (UAVs) are frequently used for inspecting power
lines and capturing high-resolution aerial images. However, detecting power
lines in aerial images is difficult,as the foreground data(i.e, power lines) is
small and the background information is abundant.To tackle this problem, we
introduce DUFormer, a semantic segmentation algorithm explicitly designed to
detect power lines in aerial images. We presuppose that it is advantageous to
train an efficient Transformer model with sufficient feature extraction using a
convolutional neural network(CNN) with a strong inductive bias.With this goal
in mind, we introduce a heavy token encoder that performs overlapping feature
remodeling and tokenization. The encoder comprises a pyramid CNN feature
extraction module and a power line feature enhancement module.After successful
local feature extraction for power lines, feature fusion is conducted.Then,the
Transformer block is used for global modeling. The final segmentation result is
achieved by amalgamating local and global features in the decode head.Moreover,
we demonstrate the importance of the joint multi-weight loss function in power
line segmentation. Our experimental results show that our proposed method
outperforms all state-of-the-art methods in power line segmentation on the
publicly accessible TTPLA dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_D/0/1/0/all/0/1&quot;&gt;Deyu An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chao_J/0/1/0/all/0/1&quot;&gt;Jianshu Chao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Ting Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_F/0/1/0/all/0/1&quot;&gt;Feng Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yong Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_Z/0/1/0/all/0/1&quot;&gt;Zhenpeng Bian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.06028">
<title>RECLIP: Resource-efficient CLIP by Training with Small Images. (arXiv:2304.06028v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.06028</link>
<description rdf:parseType="Literal">&lt;p&gt;We present RECLIP (Resource-efficient CLIP), a simple method that minimizes
computational resource footprint for CLIP (Contrastive Language Image
Pretraining). Inspired by the notion of coarse-to-fine in computer vision, we
leverage small images to learn from large-scale language supervision
efficiently, and finetune the model with high-resolution data in the end. Since
the complexity of the vision transformer heavily depends on input image size,
our approach significantly reduces the training resource requirements both in
theory and in practice. Using the same batch size and training epoch, RECLIP
achieves highly competitive zero-shot classification and image-text retrieval
accuracy with 6 to 8x less computational resources and 7 to 9x fewer FLOPs than
the baseline. Compared to the state-of-the-art contrastive learning methods,
RECLIP demonstrates 5 to 59x training resource savings while maintaining highly
competitive zero-shot classification and retrieval performance. Finally, RECLIP
matches the state of the art in transfer learning to open-vocabulary detection
tasks, achieving 32 APr on LVIS. We hope this work will pave the path for the
broader research community to explore language supervised pretraining in
resource-friendly settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Runze Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Dahun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhanu_B/0/1/0/all/0/1&quot;&gt;Bir Bhanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuo_W/0/1/0/all/0/1&quot;&gt;Weicheng Kuo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.06720">
<title>Expressive Text-to-Image Generation with Rich Text. (arXiv:2304.06720v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.06720</link>
<description rdf:parseType="Literal">&lt;p&gt;Plain text has become a prevalent interface for text-to-image synthesis.
However, its limited customization options hinder users from accurately
describing desired outputs. For example, plain text makes it hard to specify
continuous quantities, such as the precise RGB color value or importance of
each word. Furthermore, creating detailed text prompts for complex scenes is
tedious for humans to write and challenging for text encoders to interpret. To
address these challenges, we propose using a rich-text editor supporting
formats such as font style, size, color, and footnote. We extract each word&apos;s
attributes from rich text to enable local style control, explicit token
reweighting, precise color rendering, and detailed region synthesis. We achieve
these capabilities through a region-based diffusion process. We first obtain
each word&apos;s region based on attention maps of a diffusion process using plain
text. For each region, we enforce its text attributes by creating
region-specific detailed prompts and applying region-specific guidance, and
maintain its fidelity against plain-text generation through region-based
injections. We present various examples of image generation from rich text and
demonstrate that our method outperforms strong baselines with quantitative
evaluations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1&quot;&gt;Songwei Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_T/0/1/0/all/0/1&quot;&gt;Taesung Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun-Yan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jia-Bin Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.09949">
<title>Learning Temporal Distribution and Spatial Correlation for Universal Moving Object Segmentation. (arXiv:2304.09949v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.09949</link>
<description rdf:parseType="Literal">&lt;p&gt;Universal moving object segmentation aims to provide a general model for
videos from all types of natural scenes, as previous approaches are usually
effective for specific or similar scenes. In this paper, we propose a method
called Learning Temporal Distribution and Spatial Correlation (LTS) that has
the potential to be a general solution for universal moving object
segmentation. In the proposed approach, the distribution from temporal pixels
is first learned by our Defect Iterative Distribution Learning (DIDL) network
for a scene-independent segmentation. Then, the Stochastic Bayesian Refinement
(SBR) Network, which learns the spatial correlation, is proposed to improve the
binary mask generated by the DIDL network. Benefiting from the scene
independence of the temporal distribution and the accuracy improvement
resulting from the spatial correlation, the proposed approach performs well for
almost all videos from diverse and complex natural scenes with fixed
parameters. Comprehensive experiments on standard datasets including LASIESTA,
CDNet2014, BMC, SBMI2015 and 128 real world videos demonstrate the superiority
of proposed approach compared to state-of-the-art methods with or without the
use of deep learning networks. To the best of our knowledge, this work has high
potential to be a general solution for moving object segmentation in real world
environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1&quot;&gt;Guanfang Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1&quot;&gt;Chenqiu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1&quot;&gt;Xichen Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basu_A/0/1/0/all/0/1&quot;&gt;Anup Basu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.13455">
<title>From Chaos Comes Order: Ordering Event Representations for Object Recognition and Detection. (arXiv:2304.13455v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.13455</link>
<description rdf:parseType="Literal">&lt;p&gt;Today, state-of-the-art deep neural networks that process events first
convert them into dense, grid-like input representations before using an
off-the-shelf network. However, selecting the appropriate representation for
the task traditionally requires training a neural network for each
representation and selecting the best one based on the validation score, which
is very time-consuming. This work eliminates this bottleneck by selecting
representations based on the Gromov-Wasserstein Discrepancy (GWD) between raw
events and their representation. It is about 200 times faster to compute than
training a neural network and preserves the task performance ranking of event
representations across multiple representations, network backbones, datasets,
and tasks. Thus finding representations with high task scores is equivalent to
finding representations with a low GWD. We use this insight to, for the first
time, perform a hyperparameter search on a large family of event
representations, revealing new and powerful representations that exceed the
state-of-the-art. Our optimized representations outperform existing
representations by 1.7 mAP on the 1 Mpx dataset and 0.3 mAP on the Gen1
dataset, two established object detection benchmarks, and reach a 3.8% higher
classification score on the mini N-ImageNet benchmark. Moreover, we outperform
state-of-the-art by 2.1 mAP on Gen1 and state-of-the-art feed-forward methods
by 6.0 mAP on the 1 Mpx datasets. This work opens a new unexplored field of
explicit representation optimization for event-based learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zubic_N/0/1/0/all/0/1&quot;&gt;Nikola Zubi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gehrig_D/0/1/0/all/0/1&quot;&gt;Daniel Gehrig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gehrig_M/0/1/0/all/0/1&quot;&gt;Mathias Gehrig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scaramuzza_D/0/1/0/all/0/1&quot;&gt;Davide Scaramuzza&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.14505">
<title>Transformer-based interpretable multi-modal data fusion for skin lesion classification. (arXiv:2304.14505v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.14505</link>
<description rdf:parseType="Literal">&lt;p&gt;A lot of deep learning (DL) research these days is mainly focused on
improving quantitative metrics regardless of other factors. In human-centered
applications, like skin lesion classification in dermatology, DL-driven
clinical decision support systems are still in their infancy due to the limited
transparency of their decision-making process. Moreover, the lack of procedures
that can explain the behavior of trained DL algorithms leads to almost no trust
from clinical physicians. To diagnose skin lesions, dermatologists rely on
visual assessment of the disease and the data gathered from the patient&apos;s
anamnesis. Data-driven algorithms dealing with multi-modal data are limited by
the separation of feature-level and decision-level fusion procedures required
by convolutional architectures. To address this issue, we enable single-stage
multi-modal data fusion via the attention mechanism of transformer-based
architectures to aid in diagnosing skin diseases. Our method beats other
state-of-the-art single- and multi-modal DL architectures in image-rich and
patient-data-rich environments. Additionally, the choice of the architecture
enables native interpretability support for the classification task both in the
image and metadata domain with no additional modifications necessary.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cheslerean_Boghiu_T/0/1/0/all/0/1&quot;&gt;Theodor Cheslerean-Boghiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fleischmann_M/0/1/0/all/0/1&quot;&gt;Melia-Evelina Fleischmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Willem_T/0/1/0/all/0/1&quot;&gt;Theresa Willem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lasser_T/0/1/0/all/0/1&quot;&gt;Tobias Lasser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05602">
<title>Collaborative Chinese Text Recognition with Personalized Federated Learning. (arXiv:2305.05602v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05602</link>
<description rdf:parseType="Literal">&lt;p&gt;In Chinese text recognition, to compensate for the insufficient local data
and improve the performance of local few-shot character recognition, it is
often necessary for one organization to collect a large amount of data from
similar organizations. However, due to the natural presence of private
information in text data, such as addresses and phone numbers, different
organizations are unwilling to share private data. Therefore, it becomes
increasingly important to design a privacy-preserving collaborative training
framework for the Chinese text recognition task. In this paper, we introduce
personalized federated learning (pFL) into the Chinese text recognition task
and propose the pFedCR algorithm, which significantly improves the model
performance of each client (organization) without sharing private data.
Specifically, pFedCR comprises two stages: multiple rounds of global model
training stage and the the local personalization stage. During stage 1, an
attention mechanism is incorporated into the CRNN model to adapt to various
client data distributions. Leveraging inherent character data characteristics,
a balanced dataset is created on the server to mitigate character imbalance. In
the personalization phase, the global model is fine-tuned for one epoch to
create a local model. Parameter averaging between local and global models
combines personalized and global feature extraction capabilities. Finally, we
fine-tune only the attention layers to enhance its focus on local personalized
features. The experimental results on three real-world industrial scenario
datasets show that the pFedCR algorithm can improve the performance of local
personalized models by about 20\% while also improving their generalization
performance on other client data domains. Compared to other state-of-the-art
personalized federated learning methods, pFedCR improves performance by 6\%
$\sim$ 8\%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1&quot;&gt;Shangchao Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Haiyang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1&quot;&gt;Xiangyang Xue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05970">
<title>FusionBooster: A Unified Image Fusion Boosting Paradigm. (arXiv:2305.05970v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05970</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, numerous ideas have emerged for designing a mutually
reinforcing mechanism or extra stages for the image fusion task, ignoring the
inevitable gaps between different vision tasks and the computational burden. We
argue that there is a scope to improve the fusion performance with the help of
the FusionBooster, a model specifically designed for the fusion task. In
particular, our booster is based on the divide-and-conquer strategy controlled
by an information probe. The booster is composed of three building blocks: the
probe units, the booster layer, and the assembling module. Given the result
produced by a backbone method, the probe units assess the fused image and
divide the results according to their information content. This is instrumental
in identifying missing information, as a step to its recovery. The recovery of
the degraded components along with the fusion guidance are the role of the
booster layer. Lastly, the assembling module is responsible for piecing these
advanced components together to deliver the output. We use concise
reconstruction loss functions in conjunction with lightweight autoencoder
models to formulate the learning task, with marginal computational complexity
increase. The experimental results obtained in various fusion tasks, as well as
downstream detection tasks, consistently demonstrate that the proposed
FusionBooster significantly improves the performance. Our code will be publicly
available on the project homepage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1&quot;&gt;Chunyang Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Tianyang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiao-Jun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kittler_J/0/1/0/all/0/1&quot;&gt;Josef Kittler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.07283">
<title>Quaternion-valued Correlation Learning for Few-Shot Semantic Segmentation. (arXiv:2305.07283v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.07283</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot segmentation (FSS) aims to segment unseen classes given only a few
annotated samples. Encouraging progress has been made for FSS by leveraging
semantic features learned from base classes with sufficient training samples to
represent novel classes. The correlation-based methods lack the ability to
consider interaction of the two subspace matching scores due to the inherent
nature of the real-valued 2D convolutions. In this paper, we introduce a
quaternion perspective on correlation learning and propose a novel
Quaternion-valued Correlation Learning Network (QCLNet), with the aim to
alleviate the computational burden of high-dimensional correlation tensor and
explore internal latent interaction between query and support images by
leveraging operations defined by the established quaternion algebra.
Specifically, our QCLNet is formulated as a hyper-complex valued network and
represents correlation tensors in the quaternion domain, which uses
quaternion-valued convolution to explore the external relations of query
subspace when considering the hidden relationship of the support sub-dimension
in the quaternion space. Extensive experiments on the PASCAL-5i and COCO-20i
datasets demonstrate that our method outperforms the existing state-of-the-art
methods effectively. Our code is available at
https://github.com/zwzheng98/QCLNet and our article &quot;Quaternion-valued
Correlation Learning for Few-Shot Semantic Segmentation&quot; was published in IEEE
Transactions on Circuits and Systems for Video Technology, vol.
33,no.5,pp.2102-2115,May 2023,doi: 10.1109/TCSVT.&lt;a href=&quot;/abs/2022.32231&quot;&gt;2022.32231&lt;/a&gt;50.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1&quot;&gt;Zewen Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1&quot;&gt;Guoheng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xiaochen Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pun_C/0/1/0/all/0/1&quot;&gt;Chi-Man Pun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hongrui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_W/0/1/0/all/0/1&quot;&gt;Wing-Kuen Ling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.08396">
<title>MaxViT-UNet: Multi-Axis Attention for Medical Image Segmentation. (arXiv:2305.08396v4 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.08396</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we present MaxViT-UNet, an Encoder-Decoder based hybrid vision
transformer (CNN-Transformer) for medical image segmentation. The proposed
Hybrid Decoder, based on MaxViT-block, is designed to harness the power of both
the convolution and self-attention mechanisms at each decoding stage with a
nominal memory and computational burden. The inclusion of multi-axis
self-attention, within each decoder stage, significantly enhances the
discriminating capacity between the object and background regions, thereby
helping in improving the segmentation efficiency. In the Hybrid Decoder block,
the fusion process commences by integrating the upsampled lower-level decoder
features, obtained through transpose convolution, with the skip-connection
features derived from the hybrid encoder. Subsequently, the fused features
undergo refinement through the utilization of a multi-axis attention mechanism.
The proposed decoder block is repeated multiple times to progressively segment
the nuclei regions. Experimental results on MoNuSeg18 and MoNuSAC20 dataset
demonstrates the effectiveness of the proposed technique. Our MaxViT-UNet
outperformed the previous CNN-based (UNet) and Transformer-based (Swin-UNet)
techniques by a considerable margin on both of the standard datasets. The
following github (https://github.com/PRLAB21/MaxViT-UNet) contains the
implementation and trained weights.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Abdul Rehman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Asifullah Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10474">
<title>Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models. (arXiv:2305.10474v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10474</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite tremendous progress in generating high-quality images using diffusion
models, synthesizing a sequence of animated frames that are both photorealistic
and temporally coherent is still in its infancy. While off-the-shelf
billion-scale datasets for image generation are available, collecting similar
video data of the same scale is still challenging. Also, training a video
diffusion model is computationally much more expensive than its image
counterpart. In this work, we explore finetuning a pretrained image diffusion
model with video data as a practical solution for the video synthesis task. We
find that naively extending the image noise prior to video noise prior in video
diffusion leads to sub-optimal performance. Our carefully designed video noise
prior leads to substantially better performance. Extensive experimental
validation shows that our model, Preserve Your Own Correlation (PYoCo), attains
SOTA zero-shot text-to-video results on the UCF-101 and MSR-VTT benchmarks. It
also achieves SOTA video generation quality on the small-scale UCF-101
benchmark with a $10\times$ smaller model using significantly less computation
than the prior art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1&quot;&gt;Songwei Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nah_S/0/1/0/all/0/1&quot;&gt;Seungjun Nah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guilin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poon_T/0/1/0/all/0/1&quot;&gt;Tyler Poon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_A/0/1/0/all/0/1&quot;&gt;Andrew Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1&quot;&gt;Bryan Catanzaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacobs_D/0/1/0/all/0/1&quot;&gt;David Jacobs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jia-Bin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Ming-Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balaji_Y/0/1/0/all/0/1&quot;&gt;Yogesh Balaji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15777">
<title>Dynamic Data Augmentation via MCTS for Prostate MRI Segmentation. (arXiv:2305.15777v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15777</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical image data are often limited due to the expensive acquisition and
annotation process. Hence, training a deep-learning model with only raw data
can easily lead to overfitting. One solution to this problem is to augment the
raw data with various transformations, improving the model&apos;s ability to
generalize to new data. However, manually configuring a generic augmentation
combination and parameters for different datasets is non-trivial due to
inconsistent acquisition approaches and data distributions. Therefore,
automatic data augmentation is proposed to learn favorable augmentation
strategies for different datasets while incurring large GPU overhead. To this
end, we present a novel method, called Dynamic Data Augmentation (DDAug), which
is efficient and has negligible computation cost. Our DDAug develops a
hierarchical tree structure to represent various augmentations and utilizes an
efficient Monte-Carlo tree searching algorithm to update, prune, and sample the
tree. As a result, the augmentation pipeline can be optimized for each dataset
automatically. Experiments on multiple Prostate MRI datasets show that our
method outperforms the current state-of-the-art data augmentation strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xinyue Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hsi_Y/0/1/0/all/0/1&quot;&gt;Yuhan Hsi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haonan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaomeng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.20091">
<title>Humans in 4D: Reconstructing and Tracking Humans with Transformers. (arXiv:2305.20091v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.20091</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an approach to reconstruct humans and track them over time. At the
core of our approach, we propose a fully &quot;transformerized&quot; version of a network
for human mesh recovery. This network, HMR 2.0, advances the state of the art
and shows the capability to analyze unusual poses that have in the past been
difficult to reconstruct from single images. To analyze video, we use 3D
reconstructions from HMR 2.0 as input to a tracking system that operates in 3D.
This enables us to deal with multiple people and maintain identities through
occlusion events. Our complete approach, 4DHumans, achieves state-of-the-art
results for tracking people from monocular video. Furthermore, we demonstrate
the effectiveness of HMR 2.0 on the downstream task of action recognition,
achieving significant improvements over previous pose-based action recognition
approaches. Our code and models are available on the project website:
https://shubham-goel.github.io/4dhumans/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goel_S/0/1/0/all/0/1&quot;&gt;Shubham Goel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavlakos_G/0/1/0/all/0/1&quot;&gt;Georgios Pavlakos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajasegaran_J/0/1/0/all/0/1&quot;&gt;Jathushan Rajasegaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanazawa_A/0/1/0/all/0/1&quot;&gt;Angjoo Kanazawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1&quot;&gt;Jitendra Malik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15142">
<title>LRANet: Towards Accurate and Efficient Scene Text Detection with Low-Rank Approximation Network. (arXiv:2306.15142v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15142</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, regression-based methods, which predict parameterized text shapes
for text localization, have gained popularity in scene text detection. However,
the existing parameterized text shape methods still have limitations in
modeling arbitrary-shaped texts due to ignoring the utilization of
text-specific shape information. Moreover, the time consumption of the entire
pipeline has been largely overlooked, leading to a suboptimal overall inference
speed. To address these issues, we first propose a novel parameterized text
shape method based on low-rank approximation. Unlike other shape representation
methods that employ data-irrelevant parameterization, our approach utilizes
singular value decomposition and reconstructs the text shape using a few
eigenvectors learned from labeled text contours. By exploring the shape
correlation among different text contours, our method achieves consistency,
compactness, simplicity, and robustness in shape representation. Next, we
propose a dual assignment scheme for speed acceleration. It adopts a sparse
assignment branch to accelerate the inference speed, and meanwhile, provides
ample supervised signals for training through a dense assignment branch.
Building upon these designs, we implement an accurate and efficient
arbitrary-shaped text detector named LRANet. Extensive experiments are
conducted on several challenging benchmarks, demonstrating the superior
accuracy and efficiency of LRANet compared to state-of-the-art methods. Code
will be released soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yuchen Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhineng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1&quot;&gt;Zhiwen Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yuning Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1&quot;&gt;Zhilong Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1&quot;&gt;Jinfeng Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yong Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yu-Gang Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17595">
<title>RBSR: Efficient and Flexible Recurrent Network for Burst Super-Resolution. (arXiv:2306.17595v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17595</link>
<description rdf:parseType="Literal">&lt;p&gt;Burst super-resolution (BurstSR) aims at reconstructing a high-resolution
(HR) image from a sequence of low-resolution (LR) and noisy images, which is
conducive to enhancing the imaging effects of smartphones with limited sensors.
The main challenge of BurstSR is to effectively combine the complementary
information from input frames, while existing methods still struggle with it.
In this paper, we suggest fusing cues frame-by-frame with an efficient and
flexible recurrent network. In particular, we emphasize the role of the
base-frame and utilize it as a key prompt to guide the knowledge acquisition
from other frames in every recurrence. Moreover, we introduce an implicit
weighting loss to improve the model&apos;s flexibility in facing input frames with
variable numbers. Extensive experiments on both synthetic and real-world
datasets demonstrate that our method achieves better results than
state-of-the-art ones. Codes and pre-trained models are available at
https://github.com/ZcsrenlongZ/RBSR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1&quot;&gt;Renlong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhilu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shuohao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongzhi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1&quot;&gt;Wangmeng Zuo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00038">
<title>Training-free Object Counting with Prompts. (arXiv:2307.00038v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00038</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper tackles the problem of object counting in images. Existing
approaches rely on extensive training data with point annotations for each
object, making data collection labor-intensive and time-consuming. To overcome
this, we propose a training-free object counter that treats the counting task
as a segmentation problem. Our approach leverages the Segment Anything Model
(SAM), known for its high-quality masks and zero-shot segmentation capability.
However, the vanilla mask generation method of SAM lacks class-specific
information in the masks, resulting in inferior counting accuracy. To overcome
this limitation, we introduce a prior-guided mask generation method that
incorporates three types of priors into the segmentation process, enhancing
efficiency and accuracy. Additionally, we tackle the issue of counting objects
specified through text by proposing a two-stage approach that combines
reference object selection and prior-guided mask generation. Extensive
experiments on standard datasets demonstrate the competitive performance of our
training-free counter compared to learning-based approaches. This paper
presents a promising solution for counting objects in various scenarios without
the need for extensive data collection and counting-specific training. Code is
available at \url{https://github.com/shizenglin/training-free-object-counter}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zenglin Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Ying Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mengmi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07873">
<title>Why Does Little Robustness Help? Understanding and Improving Adversarial Transferability from Surrogate Training. (arXiv:2307.07873v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07873</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial examples (AEs) for DNNs have been shown to be transferable: AEs
that successfully fool white-box surrogate models can also deceive other
black-box models with different architectures. Although a bunch of empirical
studies have provided guidance on generating highly transferable AEs, many of
these findings lack explanations and even lead to inconsistent advice. In this
paper, we take a further step towards understanding adversarial
transferability, with a particular focus on surrogate aspects. Starting from
the intriguing little robustness phenomenon, where models adversarially trained
with mildly perturbed adversarial samples can serve as better surrogates, we
attribute it to a trade-off between two predominant factors: model smoothness
and gradient similarity. Our investigations focus on their joint effects,
rather than their separate correlations with transferability. Through a series
of theoretical and empirical analyses, we conjecture that the data distribution
shift in adversarial training explains the degradation of gradient similarity.
Building on these insights, we explore the impacts of data augmentation and
gradient regularization on transferability and identify that the trade-off
generally exists in the various training mechanisms, thus building a
comprehensive blueprint for the regulation mechanism behind transferability.
Finally, we provide a general route for constructing better surrogates to boost
transferability which optimizes both model smoothness and gradient similarity
simultaneously, e.g., the combination of input gradient regularization and
sharpness-aware minimization (SAM), validated by extensive experiments. In
summary, we call for attention to the united impacts of these two factors for
launching effective transfer attacks, rather than optimizing one while ignoring
the other, and emphasize the crucial role of manipulating surrogate models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yechao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shengshan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Leo Yu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Junyu Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Minghui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaogeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_W/0/1/0/all/0/1&quot;&gt;Wei Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1&quot;&gt;Hai Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.12751">
<title>ICF-SRSR: Invertible scale-Conditional Function for Self-Supervised Real-world Single Image Super-Resolution. (arXiv:2307.12751v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.12751</link>
<description rdf:parseType="Literal">&lt;p&gt;Single image super-resolution (SISR) is a challenging ill-posed problem that
aims to up-sample a given low-resolution (LR) image to a high-resolution (HR)
counterpart. Due to the difficulty in obtaining real LR-HR training pairs,
recent approaches are trained on simulated LR images degraded by simplified
down-sampling operators, e.g., bicubic. Such an approach can be problematic in
practice because of the large gap between the synthesized and real-world LR
images. To alleviate the issue, we propose a novel Invertible scale-Conditional
Function (ICF), which can scale an input image and then restore the original
input with different scale conditions. By leveraging the proposed ICF, we
construct a novel self-supervised SISR framework (ICF-SRSR) to handle the
real-world SR task without using any paired/unpaired training data.
Furthermore, our ICF-SRSR can generate realistic and feasible LR-HR pairs,
which can make existing supervised SISR networks more robust. Extensive
experiments demonstrate the effectiveness of the proposed method in handling
SISR in a fully self-supervised manner. Our ICF-SRSR demonstrates superior
performance compared to the existing methods trained on synthetic paired images
in real-world scenarios and exhibits comparable performance compared to
state-of-the-art supervised/unsupervised methods on public benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Neshatavar_R/0/1/0/all/0/1&quot;&gt;Reyhaneh Neshatavar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yavartanoo_M/0/1/0/all/0/1&quot;&gt;Mohsen Yavartanoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Son_S/0/1/0/all/0/1&quot;&gt;Sanghyun Son&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kyoung Mu Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14863">
<title>IML-ViT: Benchmarking Image Manipulation Localization by Vision Transformer. (arXiv:2307.14863v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.14863</link>
<description rdf:parseType="Literal">&lt;p&gt;Advanced image tampering techniques are increasingly challenging the
trustworthiness of multimedia, leading to the development of Image Manipulation
Localization (IML). But what makes a good IML model? The answer lies in the way
to capture artifacts. Exploiting artifacts requires the model to extract
non-semantic discrepancies between manipulated and authentic regions,
necessitating explicit comparisons between the two areas. With the
self-attention mechanism, naturally, the Transformer should be a better
candidate to capture artifacts. However, due to limited datasets, there is
currently no pure ViT-based approach for IML to serve as a benchmark, and CNNs
dominate the entire task. Nevertheless, CNNs suffer from weak long-range and
non-semantic modeling. To bridge this gap, based on the fact that artifacts are
sensitive to image resolution, amplified under multi-scale features, and
massive at the manipulation border, we formulate the answer to the former
question as building a ViT with high-resolution capacity, multi-scale feature
extraction capability, and manipulation edge supervision that could converge
with a small amount of data. We term this simple but effective ViT paradigm
IML-ViT, which has significant potential to become a new benchmark for IML.
Extensive experiments on five benchmark datasets verified our model outperforms
the state-of-the-art manipulation localization methods.Code and models are
available at \url{https://github.com/SunnyHaze/IML-ViT}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiaochen Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1&quot;&gt;Bo Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhuohang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hammadi_A/0/1/0/all/0/1&quot;&gt;Ahmed Y. Al Hammadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jizhe Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.06791">
<title>PV-SSD: A Projection and Voxel-based Double Branch Single-Stage 3D Object Detector. (arXiv:2308.06791v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.06791</link>
<description rdf:parseType="Literal">&lt;p&gt;LIDAR-based 3D object detection and classification is crucial for autonomous
driving. However, inference in real-time from extremely sparse 3D data poses a
formidable challenge. To address this issue, a common approach is to project
point clouds onto a bird&apos;s-eye or perspective view, effectively converting them
into an image-like data format. However, this excessive compression of point
cloud data often leads to the loss of information. This paper proposes a 3D
object detector based on voxel and projection double branch feature extraction
(PV-SSD) to address the problem of information loss. We add voxel features
input containing rich local semantic information, which is fully fused with the
projected features in the feature extraction stage to reduce the local
information loss caused by projection. A good performance is achieved compared
to the previous work. In addition, this paper makes the following
contributions: 1) a voxel feature extraction method with variable receptive
fields is proposed; 2) a feature point sampling method by weight sampling is
used to filter out the feature points that are more conducive to the detection
task; 3) the MSSFA module is proposed based on the SSFA module. To verify the
effectiveness of our method, we designed comparison experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1&quot;&gt;Yongxin Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_A/0/1/0/all/0/1&quot;&gt;Aihong Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhetao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_E/0/1/0/all/0/1&quot;&gt;Enhui Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_T/0/1/0/all/0/1&quot;&gt;Tianhong Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.09331">
<title>SAMedOCT: Adapting Segment Anything Model (SAM) for Retinal OCT. (arXiv:2308.09331v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.09331</link>
<description rdf:parseType="Literal">&lt;p&gt;The Segment Anything Model (SAM) has gained significant attention in the
field of image segmentation due to its impressive capabilities and prompt-based
interface. While SAM has already been extensively evaluated in various domains,
its adaptation to retinal OCT scans remains unexplored. To bridge this research
gap, we conduct a comprehensive evaluation of SAM and its adaptations on a
large-scale public dataset of OCTs from RETOUCH challenge. Our evaluation
covers diverse retinal diseases, fluid compartments, and device vendors,
comparing SAM against state-of-the-art retinal fluid segmentation methods.
Through our analysis, we showcase adapted SAM&apos;s efficacy as a powerful
segmentation model in retinal OCT scans, although still lagging behind
established methods in some circumstances. The findings highlight SAM&apos;s
adaptability and robustness, showcasing its utility as a valuable tool in
retinal OCT image analysis and paving the way for further advancements in this
domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fazekas_B/0/1/0/all/0/1&quot;&gt;Botond Fazekas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Morano_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Morano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lachinov_D/0/1/0/all/0/1&quot;&gt;Dmitrii Lachinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Aresta_G/0/1/0/all/0/1&quot;&gt;Guilherme Aresta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bogunovic_H/0/1/0/all/0/1&quot;&gt;Hrvoje Bogunovi&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.10161">
<title>ThermRad: A Multi-modal Dataset for Robust 3D Object Detection under Challenging Conditions. (arXiv:2308.10161v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.10161</link>
<description rdf:parseType="Literal">&lt;p&gt;Robust 3D object detection in extreme weather and illumination conditions is
a challenging task. While radars and thermal cameras are known for their
resilience to these conditions, few studies have been conducted on
radar-thermal fusion due to the lack of corresponding datasets. To address this
gap, we first present a new multi-modal dataset called ThermRad, which includes
a 3D LiDAR, a 4D radar, an RGB camera and a thermal camera. This dataset is
unique because it includes data from all four sensors in extreme weather
conditions, providing a valuable resource for future research in this area. To
validate the robustness of 4D radars and thermal cameras for 3D object
detection in challenging weather conditions, we propose a new multi-modal
fusion method called RTDF-RCNN, which leverages the complementary strengths of
4D radars and thermal cameras to boost object detection performance. To further
prove the effectiveness of our proposed framework, we re-implement
state-of-the-art (SOTA) 3D detectors on our dataset as benchmarks for
evaluation. Our method achieves significant enhancements in detecting cars,
pedestrians, and cyclists, with improvements of over 7.98%, 24.27%, and 27.15%,
respectively, while achieving comparable results to LiDAR-based approaches. Our
contributions in both the ThermRad dataset and the new multi-modal fusion
method provide a new approach to robust 3D object detection in adverse weather
and illumination conditions. The ThermRad dataset will be released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1&quot;&gt;Qiao Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yihan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.10280">
<title>MacFormer: Map-Agent Coupled Transformer for Real-time and Robust Trajectory Prediction. (arXiv:2308.10280v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.10280</link>
<description rdf:parseType="Literal">&lt;p&gt;Predicting the future behavior of agents is a fundamental task in autonomous
vehicle domains. Accurate prediction relies on comprehending the surrounding
map, which significantly regularizes agent behaviors. However, existing methods
have limitations in exploiting the map and exhibit a strong dependence on
historical trajectories, which yield unsatisfactory prediction performance and
robustness. Additionally, their heavy network architectures impede real-time
applications. To tackle these problems, we propose Map-Agent Coupled
Transformer (MacFormer) for real-time and robust trajectory prediction. Our
framework explicitly incorporates map constraints into the network via two
carefully designed modules named coupled map and reference extractor. A novel
multi-task optimization strategy (MTOS) is presented to enhance learning of
topology and rule constraints. We also devise bilateral query scheme in context
fusion for a more efficient and lightweight network. We evaluated our approach
on Argoverse 1, Argoverse 2, and nuScenes real-world benchmarks, where it all
achieved state-of-the-art performance with the lowest inference latency and
smallest model size. Experiments also demonstrate that our framework is
resilient to imperfect tracklet inputs. Furthermore, we show that by combining
with our proposed strategies, classical models outperform their baselines,
further validating the versatility of our framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1&quot;&gt;Chen Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hangning Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1&quot;&gt;Huadong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhigang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Ziyao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Boyu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1&quot;&gt;Shaojie Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.10902">
<title>CamP: Camera Preconditioning for Neural Radiance Fields. (arXiv:2308.10902v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.10902</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Fields (NeRF) can be optimized to obtain high-fidelity 3D
scene reconstructions of objects and large-scale scenes. However, NeRFs require
accurate camera parameters as input -- inaccurate camera parameters result in
blurry renderings. Extrinsic and intrinsic camera parameters are usually
estimated using Structure-from-Motion (SfM) methods as a pre-processing step to
NeRF, but these techniques rarely yield perfect estimates. Thus, prior works
have proposed jointly optimizing camera parameters alongside a NeRF, but these
methods are prone to local minima in challenging settings. In this work, we
analyze how different camera parameterizations affect this joint optimization
problem, and observe that standard parameterizations exhibit large differences
in magnitude with respect to small perturbations, which can lead to an
ill-conditioned optimization problem. We propose using a proxy problem to
compute a whitening transform that eliminates the correlation between camera
parameters and normalizes their effects, and we propose to use this transform
as a preconditioner for the camera parameters during joint optimization. Our
preconditioned camera optimization significantly improves reconstruction
quality on scenes from the Mip-NeRF 360 dataset: we reduce error rates (RMSE)
by 67% compared to state-of-the-art NeRF approaches that do not optimize for
cameras like Zip-NeRF, and by 29% relative to state-of-the-art joint
optimization approaches using the camera parameterization of SCNeRF. Our
approach is easy to implement, does not significantly increase runtime, can be
applied to a wide variety of camera parameterizations, and can
straightforwardly be incorporated into other NeRF-like models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1&quot;&gt;Keunhong Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henzler_P/0/1/0/all/0/1&quot;&gt;Philipp Henzler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mildenhall_B/0/1/0/all/0/1&quot;&gt;Ben Mildenhall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1&quot;&gt;Jonathan T. Barron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_Brualla_R/0/1/0/all/0/1&quot;&gt;Ricardo Martin-Brualla&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12319">
<title>RemovalNet: DNN Fingerprint Removal Attacks. (arXiv:2308.12319v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12319</link>
<description rdf:parseType="Literal">&lt;p&gt;With the performance of deep neural networks (DNNs) remarkably improving,
DNNs have been widely used in many areas. Consequently, the DNN model has
become a valuable asset, and its intellectual property is safeguarded by
ownership verification techniques (e.g., DNN fingerprinting). However, the
feasibility of the DNN fingerprint removal attack and its potential influence
remains an open problem. In this paper, we perform the first comprehensive
investigation of DNN fingerprint removal attacks. Generally, the knowledge
contained in a DNN model can be categorized into general semantic and
fingerprint-specific knowledge. To this end, we propose a min-max bilevel
optimization-based DNN fingerprint removal attack named RemovalNet, to evade
model ownership verification. The lower-level optimization is designed to
remove fingerprint-specific knowledge. While in the upper-level optimization,
we distill the victim model&apos;s general semantic knowledge to maintain the
surrogate model&apos;s performance. We conduct extensive experiments to evaluate the
fidelity, effectiveness, and efficiency of the RemovalNet against four advanced
defense methods on six metrics. The empirical results demonstrate that (1) the
RemovalNet is effective. After our DNN fingerprint removal attack, the model
distance between the target and surrogate models is x100 times higher than that
of the baseline attacks, (2) the RemovalNet is efficient. It uses only 0.2%
(400 samples) of the substitute dataset and 1,000 iterations to conduct our
attack. Besides, compared with advanced model stealing attacks, the RemovalNet
saves nearly 85% of computational resources at most, (3) the RemovalNet
achieves high fidelity that the created surrogate model maintains high accuracy
after the DNN fingerprint removal process. Our code is available at:
https://github.com/grasses/RemovalNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1&quot;&gt;Hongwei Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kunzhe Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1&quot;&gt;Jian Lou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1&quot;&gt;Zhan Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1&quot;&gt;Kui Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.14500">
<title>LAC: Latent Action Composition for Skeleton-based Action Segmentation. (arXiv:2308.14500v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.14500</link>
<description rdf:parseType="Literal">&lt;p&gt;Skeleton-based action segmentation requires recognizing composable actions in
untrimmed videos. Current approaches decouple this problem by first extracting
local visual features from skeleton sequences and then processing them by a
temporal model to classify frame-wise actions. However, their performances
remain limited as the visual features cannot sufficiently express composable
actions. In this context, we propose Latent Action Composition (LAC), a novel
self-supervised framework aiming at learning from synthesized composable
motions for skeleton-based action segmentation. LAC is composed of a novel
generation module towards synthesizing new sequences. Specifically, we design a
linear latent space in the generator to represent primitive motion. New
composed motions can be synthesized by simply performing arithmetic operations
on latent representations of multiple input skeleton sequences. LAC leverages
such synthesized sequences, which have large diversity and complexity, for
learning visual representations of skeletons in both sequence and frame spaces
via contrastive learning. The resulting visual encoder has a high expressive
power and can be effectively transferred onto action segmentation tasks by
end-to-end fine-tuning without the need for additional temporal models. We
conduct a study focusing on transfer-learning and we show that representations
learned from pre-trained LAC outperform the state-of-the-art by a large margin
on TSU, Charades, PKU-MMD datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Di Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaohui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dantcheva_A/0/1/0/all/0/1&quot;&gt;Antitza Dantcheva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1&quot;&gt;Quan Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garattoni_L/0/1/0/all/0/1&quot;&gt;Lorenzo Garattoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Francesca_G/0/1/0/all/0/1&quot;&gt;Gianpiero Francesca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bremond_F/0/1/0/all/0/1&quot;&gt;Francois Bremond&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.14604">
<title>SAM-PARSER: Fine-tuning SAM Efficiently by Parameter Space Reconstruction. (arXiv:2308.14604v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.14604</link>
<description rdf:parseType="Literal">&lt;p&gt;Segment Anything Model (SAM) has received remarkable attention as it offers a
powerful and versatile solution for object segmentation in images. However,
fine-tuning SAM for downstream segmentation tasks under different scenarios
remains a challenge, as the varied characteristics of different scenarios
naturally requires diverse model parameter spaces. Most existing fine-tuning
methods attempt to bridge the gaps among different scenarios by introducing a
set of new parameters to modify SAM&apos;s original parameter space. Unlike these
works, in this paper, we propose fine-tuning SAM efficiently by parameter space
reconstruction (SAM-PARSER), which introduce nearly zero trainable parameters
during fine-tuning. In SAM-PARSER, we assume that SAM&apos;s original parameter
space is relatively complete, so that its bases are able to reconstruct the
parameter space of a new scenario. We obtain the bases by matrix decomposition,
and fine-tuning the coefficients to reconstruct the parameter space tailored to
the new scenario by an optimal linear combination of the bases. Experimental
results show that SAM-PARSER exhibits superior segmentation performance across
various scenarios, while reducing the number of trainable parameters by
$\approx 290$ times compared with current parameter-efficient fine-tuning
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1&quot;&gt;Zelin Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhengqin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1&quot;&gt;Zhilin Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaokang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1&quot;&gt;Wei Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.15690">
<title>CongNaMul: A Dataset for Advanced Image Processing of Soybean Sprouts. (arXiv:2308.15690v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.15690</link>
<description rdf:parseType="Literal">&lt;p&gt;We present &apos;CongNaMul&apos;, a comprehensive dataset designed for various tasks in
soybean sprouts image analysis. The CongNaMul dataset is curated to facilitate
tasks such as image classification, semantic segmentation, decomposition, and
measurement of length and weight. The classification task provides four classes
to determine the quality of soybean sprouts: normal, broken, spotted, and
broken and spotted, for the development of AI-aided automatic quality
inspection technology. For semantic segmentation, images with varying
complexity, from single sprout images to images with multiple sprouts, along
with human-labelled mask images, are included. The label has 4 different
classes: background, head, body, tail. The dataset also provides images and
masks for the image decomposition task, including two separate sprout images
and their combined form. Lastly, 5 physical features of sprouts (head length,
body length, body thickness, tail length, weight) are provided for image-based
measurement tasks. This dataset is expected to be a valuable resource for a
wide range of research and applications in the advanced analysis of images of
soybean sprouts. Also, we hope that this dataset can assist researchers
studying classification, semantic segmentation, decomposition, and physical
feature measurement in other industrial fields, in evaluating their models. The
dataset is available at the authors&apos; repository. (https://bhban.kr/data)
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ban_B/0/1/0/all/0/1&quot;&gt;Byunghyun Ban&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryu_D/0/1/0/all/0/1&quot;&gt;Donghun Ryu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1&quot;&gt;Su-won Hwang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.15791">
<title>Neural Video Compression with Temporal Layer-Adaptive Hierarchical B-frame Coding. (arXiv:2308.15791v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.15791</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural video compression (NVC) is a rapidly evolving video coding research
area, with some models achieving superior coding efficiency compared to the
latest video coding standard Versatile Video Coding (VVC). In conventional
video coding standards, the hierarchical B-frame coding, which utilizes a
bidirectional prediction structure for higher compression, had been
well-studied and exploited. In NVC, however, limited research has investigated
the hierarchical B scheme. In this paper, we propose an NVC model exploiting
hierarchical B-frame coding with temporal layer-adaptive optimization. We first
extend an existing unidirectional NVC model to a bidirectional model, which
achieves -21.13% BD-rate gain over the unidirectional baseline model. However,
this model faces challenges when applied to sequences with complex or large
motions, leading to performance degradation. To address this, we introduce
temporal layer-adaptive optimization, incorporating methods such as temporal
layer-adaptive quality scaling (TAQS) and temporal layer-adaptive latent
scaling (TALS). The final model with the proposed methods achieves an
impressive BD-rate gain of -39.86% against the baseline. It also resolves the
challenges in sequences with large or complex motions with up to -49.13% more
BD-rate gains than the simple bidirectional extension. This improvement is
attributed to the allocation of more bits to lower temporal layers, thereby
enhancing overall reconstruction quality with smaller bits. Since our method
has little dependency on a specific NVC model architecture, it can serve as a
general tool for extending unidirectional NVC models to the ones with
hierarchical B-frame coding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yeongwoong Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bahk_S/0/1/0/all/0/1&quot;&gt;Suyong Bahk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seungeon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1&quot;&gt;Won Hee Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_D/0/1/0/all/0/1&quot;&gt;Dokwan Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hui Yong Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.15816">
<title>Improving Underwater Visual Tracking With a Large Scale Dataset and Image Enhancement. (arXiv:2308.15816v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.15816</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a new dataset and general tracker enhancement method for
Underwater Visual Object Tracking (UVOT). Despite its significance, underwater
tracking has remained unexplored due to data inaccessibility. It poses distinct
challenges; the underwater environment exhibits non-uniform lighting
conditions, low visibility, lack of sharpness, low contrast, camouflage, and
reflections from suspended particles. Performance of traditional tracking
methods designed primarily for terrestrial or open-air scenarios drops in such
conditions. We address the problem by proposing a novel underwater image
enhancement algorithm designed specifically to boost tracking quality. The
method has resulted in a significant performance improvement, of up to 5.0%
AUC, of state-of-the-art (SOTA) visual trackers. To develop robust and accurate
UVOT methods, large-scale datasets are required. To this end, we introduce a
large-scale UVOT benchmark dataset consisting of 400 video segments and 275,000
manually annotated frames enabling underwater training and evaluation of deep
trackers. The videos are labelled with several underwater-specific tracking
attributes including watercolor variation, target distractors, camouflage,
target relative size, and low visibility conditions. The UVOT400 dataset,
tracking results, and the code are publicly available on:
https://github.com/BasitAlawode/UWVOT400.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alawode_B/0/1/0/all/0/1&quot;&gt;Basit Alawode&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dharejo_F/0/1/0/all/0/1&quot;&gt;Fayaz Ali Dharejo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ummar_M/0/1/0/all/0/1&quot;&gt;Mehnaz Ummar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yuhang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahmood_A/0/1/0/all/0/1&quot;&gt;Arif Mahmood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Werghi_N/0/1/0/all/0/1&quot;&gt;Naoufel Werghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1&quot;&gt;Fahad Shahbaz Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1&quot;&gt;Jiri Matas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Javed_S/0/1/0/all/0/1&quot;&gt;Sajid Javed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.15975">
<title>RoboTAP: Tracking Arbitrary Points for Few-Shot Visual Imitation. (arXiv:2308.15975v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2308.15975</link>
<description rdf:parseType="Literal">&lt;p&gt;For robots to be useful outside labs and specialized factories we need a way
to teach them new useful behaviors quickly. Current approaches lack either the
generality to onboard new tasks without task-specific engineering, or else lack
the data-efficiency to do so in an amount of time that enables practical use.
In this work we explore dense tracking as a representational vehicle to allow
faster and more general learning from demonstration. Our approach utilizes
Track-Any-Point (TAP) models to isolate the relevant motion in a demonstration,
and parameterize a low-level controller to reproduce this motion across changes
in the scene configuration. We show this results in robust robot policies that
can solve complex object-arrangement tasks such as shape-matching, stacking,
and even full path-following tasks such as applying glue and sticking objects
together, all from demonstrations that can be collected in minutes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vecerik_M/0/1/0/all/0/1&quot;&gt;Mel Vecerik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doersch_C/0/1/0/all/0/1&quot;&gt;Carl Doersch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davchev_T/0/1/0/all/0/1&quot;&gt;Todor Davchev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aytar_Y/0/1/0/all/0/1&quot;&gt;Yusuf Aytar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1&quot;&gt;Guangyao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hadsell_R/0/1/0/all/0/1&quot;&gt;Raia Hadsell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agapito_L/0/1/0/all/0/1&quot;&gt;Lourdes Agapito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholz_J/0/1/0/all/0/1&quot;&gt;Jon Scholz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16139">
<title>MedShapeNet -- A Large-Scale Dataset of 3D Medical Shapes for Computer Vision. (arXiv:2308.16139v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.16139</link>
<description rdf:parseType="Literal">&lt;p&gt;We present MedShapeNet, a large collection of anatomical shapes (e.g., bones,
organs, vessels) and 3D surgical instrument models. Prior to the deep learning
era, the broad application of statistical shape models (SSMs) in medical image
analysis is evidence that shapes have been commonly used to describe medical
data. Nowadays, however, state-of-the-art (SOTA) deep learning algorithms in
medical imaging are predominantly voxel-based. In computer vision, on the
contrary, shapes (including, voxel occupancy grids, meshes, point clouds and
implicit surface models) are preferred data representations in 3D, as seen from
the numerous shape-related publications in premier vision conferences, such as
the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), as
well as the increasing popularity of ShapeNet (about 51,300 models) and
Princeton ModelNet (127,915 models) in computer vision research. MedShapeNet is
created as an alternative to these commonly used shape benchmarks to facilitate
the translation of data-driven vision algorithms to medical applications, and
it extends the opportunities to adapt SOTA vision algorithms to solve critical
medical problems. Besides, the majority of the medical shapes in MedShapeNet
are modeled directly on the imaging data of real patients, and therefore it
complements well existing shape benchmarks comprising of computer-aided design
(CAD) models. MedShapeNet currently includes more than 100,000 medical shapes,
and provides annotations in the form of paired data. It is therefore also a
freely available repository of 3D models for extended reality (virtual reality
- VR, augmented reality - AR, mixed reality - MR) and medical 3D printing. This
white paper describes in detail the motivations behind MedShapeNet, the shape
acquisition procedures, the use cases, as well as the usage of the online shape
search portal: https://medshapenet.ikim.nrw/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianning Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pepe_A/0/1/0/all/0/1&quot;&gt;Antonio Pepe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gsaxner_C/0/1/0/all/0/1&quot;&gt;Christina Gsaxner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luijten_G/0/1/0/all/0/1&quot;&gt;Gijs Luijten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yuan Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ambigapathy_N/0/1/0/all/0/1&quot;&gt;Narmada Ambigapathy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nasca_E/0/1/0/all/0/1&quot;&gt;Enrico Nasca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solak_N/0/1/0/all/0/1&quot;&gt;Naida Solak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melito_G/0/1/0/all/0/1&quot;&gt;Gian Marco Melito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Memon_A/0/1/0/all/0/1&quot;&gt;Afaque R. Memon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaojun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirschke_J/0/1/0/all/0/1&quot;&gt;Jan Stefan Kirschke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosa_E/0/1/0/all/0/1&quot;&gt;Ezequiel de la Rosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christ_P/0/1/0/all/0/1&quot;&gt;Patrich Ferndinand Christ&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongwei Bran Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ellis_D/0/1/0/all/0/1&quot;&gt;David G. Ellis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aizenberg_M/0/1/0/all/0/1&quot;&gt;Michele R. Aizenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gatidis_S/0/1/0/all/0/1&quot;&gt;Sergios Gatidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuestner_T/0/1/0/all/0/1&quot;&gt;Thomas Kuestner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shusharina_N/0/1/0/all/0/1&quot;&gt;Nadya Shusharina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heller_N/0/1/0/all/0/1&quot;&gt;Nicholas Heller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andrearczyk_V/0/1/0/all/0/1&quot;&gt;Vincent Andrearczyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Depeursinge_A/0/1/0/all/0/1&quot;&gt;Adrien Depeursinge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hatt_M/0/1/0/all/0/1&quot;&gt;Mathieu Hatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sekuboyina_A/0/1/0/all/0/1&quot;&gt;Anjany Sekuboyina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loeffler_M/0/1/0/all/0/1&quot;&gt;Maximilian Loeffler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liebl_H/0/1/0/all/0/1&quot;&gt;Hans Liebl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dorent_R/0/1/0/all/0/1&quot;&gt;Reuben Dorent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vercauteren_T/0/1/0/all/0/1&quot;&gt;Tom Vercauteren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shapey_J/0/1/0/all/0/1&quot;&gt;Jonathan Shapey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kujawa_A/0/1/0/all/0/1&quot;&gt;Aaron Kujawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cornelissen_S/0/1/0/all/0/1&quot;&gt;Stefan Cornelissen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Langenhuizen_P/0/1/0/all/0/1&quot;&gt;Patrick Langenhuizen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ben_Hamadou_A/0/1/0/all/0/1&quot;&gt;Achraf Ben-Hamadou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rekik_A/0/1/0/all/0/1&quot;&gt;Ahmed Rekik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pujades_S/0/1/0/all/0/1&quot;&gt;Sergi Pujades&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boyer_E/0/1/0/all/0/1&quot;&gt;Edmond Boyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bolelli_F/0/1/0/all/0/1&quot;&gt;Federico Bolelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grana_C/0/1/0/all/0/1&quot;&gt;Costantino Grana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lumetti_L/0/1/0/all/0/1&quot;&gt;Luca Lumetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salehi_H/0/1/0/all/0/1&quot;&gt;Hamidreza Salehi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jun Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gharleghi_R/0/1/0/all/0/1&quot;&gt;Ramtin Gharleghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beier_S/0/1/0/all/0/1&quot;&gt;Susann Beier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sowmya_A/0/1/0/all/0/1&quot;&gt;Arcot Sowmya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garza_Villarreal_E/0/1/0/all/0/1&quot;&gt;Eduardo A. Garza-Villarreal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balducci_T/0/1/0/all/0/1&quot;&gt;Thania Balducci&lt;/a&gt;, et al. (68 additional authors not shown)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16145">
<title>CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention. (arXiv:2308.16145v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.16145</link>
<description rdf:parseType="Literal">&lt;p&gt;Both CNN-based and Transformer-based object detection with bounding box
representation have been extensively studied in computer vision and medical
image analysis, but circular object detection in medical images is still
underexplored. Inspired by the recent anchor free CNN-based circular object
detection method (CircleNet) for ball-shape glomeruli detection in renal
pathology, in this paper, we present CircleFormer, a Transformer-based circular
medical object detection with dynamic anchor circles. Specifically, queries
with circle representation in Transformer decoder iteratively refine the
circular object detection results, and a circle cross attention module is
introduced to compute the similarity between circular queries and image
features. A generalized circle IoU (gCIoU) is proposed to serve as a new
regression loss of circular object detection as well. Moreover, our approach is
easy to generalize to the segmentation task by adding a simple segmentation
branch to CircleFormer. We evaluate our method in circular nuclei detection and
segmentation on the public MoNuSeg dataset, and the experimental results show
that our method achieves promising performance compared with the
state-of-the-art approaches. The effectiveness of each component is validated
via ablation studies as well. Our code is released at
https://github.com/zhanghx-iim-ahu/CircleFormer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hengxu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Pengpeng Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1&quot;&gt;Bo Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_E/0/1/0/all/0/1&quot;&gt;Erkang Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16154">
<title>MMVP: Motion-Matrix-based Video Prediction. (arXiv:2308.16154v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.16154</link>
<description rdf:parseType="Literal">&lt;p&gt;A central challenge of video prediction lies where the system has to reason
the objects&apos; future motions from image frames while simultaneously maintaining
the consistency of their appearances across frames. This work introduces an
end-to-end trainable two-stream video prediction framework, Motion-Matrix-based
Video Prediction (MMVP), to tackle this challenge. Unlike previous methods that
usually handle motion prediction and appearance maintenance within the same set
of modules, MMVP decouples motion and appearance information by constructing
appearance-agnostic motion matrices. The motion matrices represent the temporal
similarity of each and every pair of feature patches in the input frames, and
are the sole input of the motion prediction module in MMVP. This design
improves video prediction in both accuracy and efficiency, and reduces the
model size. Results of extensive experiments demonstrate that MMVP outperforms
state-of-the-art systems on public data sets by non-negligible large margins
(about 1 db in PSNR, UCF Sports) in significantly smaller model sizes (84% the
size or smaller).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yiqi Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1&quot;&gt;Luming Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zharkov_I/0/1/0/all/0/1&quot;&gt;Ilya Zharkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_U/0/1/0/all/0/1&quot;&gt;Ulrich Neumann&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>