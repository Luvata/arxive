<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-11-08T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04226" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04231" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04235" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04236" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04239" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04240" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04241" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04244" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04245" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04247" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04250" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04251" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04254" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04256" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04258" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04262" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04284" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04301" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04313" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04325" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04326" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04335" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04345" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04348" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04372" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04386" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04397" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04400" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04403" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04412" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04418" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04419" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04420" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04441" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04457" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04459" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04467" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04474" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04491" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04498" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04512" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04524" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04535" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04544" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04562" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04569" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04588" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04589" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04614" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04640" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04645" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04653" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04659" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04666" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04693" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04698" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04710" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04744" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04765" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04770" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04778" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04799" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04816" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04817" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04837" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04850" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04879" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04886" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04888" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04894" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04898" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04902" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2104.09864" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.00907" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.03860" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.07074" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.08349" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.05981" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.07849" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.07867" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.07143" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.06026" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10406" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10782" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.11243" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.11414" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14979" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15296" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18869" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.03286" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04031" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08013" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15063" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16750" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03718" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09342" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09933" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14953" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00113" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00158" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00530" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01098" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.06202" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.06597" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07670" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.08589" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.08637" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.08646" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.08872" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12871" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00034" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03912" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05036" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.09689" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.13032" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18471" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18626" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18679" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.20654" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00322" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01007" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02382" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02847" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02926" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03084" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03340" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03429" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03989" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2311.04226">
<title>Assessing Upper Limb Motor Function in the Immediate Post-Stroke Perioud Using Accelerometry. (arXiv:2311.04226v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.04226</link>
<description rdf:parseType="Literal">&lt;p&gt;Accelerometry has been extensively studied as an objective means of measuring
upper limb function in patients post-stroke. The objective of this paper is to
determine whether the accelerometry-derived measurements frequently used in
more long-term rehabilitation studies can also be used to monitor and rapidly
detect sudden changes in upper limb motor function in more recently
hospitalized stroke patients. Six binary classification models were created by
training on variable data window times of paretic upper limb accelerometer
feature data. The models were assessed on their effectiveness for
differentiating new input data into two classes: severe or moderately severe
motor function. The classification models yielded Area Under the Curve (AUC)
scores that ranged from 0.72 to 0.82 for 15-minute data windows to 0.77 to 0.94
for 120-minute data windows. These results served as a preliminary assessment
and a basis on which to further investigate the efficacy of using accelerometry
and machine learning to alert healthcare professionals to rapid changes in
motor function in the days immediately following a stroke.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wallich_M/0/1/0/all/0/1&quot;&gt;Mackenzie Wallich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_K/0/1/0/all/0/1&quot;&gt;Kenneth Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yanushkevich_S/0/1/0/all/0/1&quot;&gt;Svetlana Yanushkevich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04231">
<title>A Practical Large-Scale Roadside Multi-View Multi-Sensor Spatial Synchronization Framework for Intelligent Transportation Systems. (arXiv:2311.04231v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2311.04231</link>
<description rdf:parseType="Literal">&lt;p&gt;Spatial synchronization in roadside scenarios is essential for integrating
data from multiple sensors at different locations. Current methods using
cascading spatial transformation (CST) often lead to cumulative errors in
large-scale deployments. Manual camera calibration is insufficient and requires
extensive manual work, and existing methods are limited to controlled or
single-view scenarios. To address these challenges, our research introduces a
parallel spatial transformation (PST)-based framework for large-scale,
multi-view, multi-sensor scenarios. PST parallelizes sensor coordinate system
transformation, reducing cumulative errors. We incorporate deep learning for
precise roadside monocular global localization, reducing manual work.
Additionally, we use geolocation cues and an optimization algorithm for
improved synchronization accuracy. Our framework has been tested in real-world
scenarios, outperforming CST-based methods. It significantly enhances
large-scale roadside multi-perspective, multi-sensor spatial synchronization,
reducing deployment costs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhiguo Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yunli Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tian_R/0/1/0/all/0/1&quot;&gt;Rui Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04235">
<title>Can LLMs Follow Simple Rules?. (arXiv:2311.04235v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.04235</link>
<description rdf:parseType="Literal">&lt;p&gt;As Large Language Models (LLMs) are deployed with increasing real-world
responsibilities, it is important to be able to specify and constrain the
behavior of these systems in a reliable manner. Model developers may wish to
set explicit rules for the model, such as &quot;do not generate abusive content&quot;,
but these may be circumvented by jailbreaking techniques. Evaluating how well
LLMs follow developer-provided rules in the face of adversarial inputs
typically requires manual review, which slows down monitoring and methods
development. To address this issue, we propose Rule-following Language
Evaluation Scenarios (RuLES), a programmatic framework for measuring
rule-following ability in LLMs. RuLES consists of 15 simple text scenarios in
which the model is instructed to obey a set of rules in natural language while
interacting with the human user. Each scenario has a concise evaluation program
to determine whether the model has broken any rules in a conversation. Through
manual exploration of model behavior in our scenarios, we identify 6 categories
of attack strategies and collect two suites of test cases: one consisting of
unique conversations from manual testing and one that systematically implements
strategies from the 6 categories. Across various popular proprietary and open
models such as GPT-4 and Llama 2, we find that all models are susceptible to a
wide variety of adversarial hand-crafted user inputs, though GPT-4 is the
best-performing model. Additionally, we evaluate open models under
gradient-based attacks and find significant vulnerabilities. We propose RuLES
as a challenging new setting for research into exploring and defending against
both manual and automatic attacks on LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_N/0/1/0/all/0/1&quot;&gt;Norman Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Sarah Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zifan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Sizhe Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karamardian_D/0/1/0/all/0/1&quot;&gt;David Karamardian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aljeraisy_L/0/1/0/all/0/1&quot;&gt;Lulwa Aljeraisy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1&quot;&gt;Dan Hendrycks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagner_D/0/1/0/all/0/1&quot;&gt;David Wagner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04236">
<title>Distributed Agent-Based Collaborative Learning in Cross-Individual Wearable Sensor-Based Human Activity Recognition. (arXiv:2311.04236v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2311.04236</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid growth of wearable sensor technologies holds substantial promise
for the field of personalized and context-aware Human Activity Recognition.
Given the inherently decentralized nature of data sources within this domain,
the utilization of multi-agent systems with their inherent decentralization
capabilities presents an opportunity to facilitate the development of scalable,
adaptable, and privacy-conscious methodologies. This paper introduces a
collaborative distributed learning approach rooted in multi-agent principles,
wherein individual users of sensor-equipped devices function as agents within a
distributed network, collectively contributing to the comprehensive process of
learning and classifying human activities. In this proposed methodology, not
only is the privacy of activity monitoring data upheld for each individual,
eliminating the need for an external server to oversee the learning process,
but the system also exhibits the potential to surmount the limitations of
conventional centralized models and adapt to the unique attributes of each
user. The proposed approach has been empirically tested on two publicly
accessible human activity recognition datasets, specifically PAMAP2 and HARTH,
across varying settings. The provided empirical results conclusively highlight
the efficacy of inter-individual collaborative learning when contrasted with
centralized configurations, both in terms of local and global generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Esmaeili_A/0/1/0/all/0/1&quot;&gt;Ahmad Esmaeili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ghorrati_Z/0/1/0/all/0/1&quot;&gt;Zahra Ghorrati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Matson_E/0/1/0/all/0/1&quot;&gt;Eric T. Matson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04239">
<title>Kindness in Multi-Agent Reinforcement Learning. (arXiv:2311.04239v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.04239</link>
<description rdf:parseType="Literal">&lt;p&gt;In human societies, people often incorporate fairness in their decisions and
treat reciprocally by being kind to those who act kindly. They evaluate the
kindness of others&apos; actions not only by monitoring the outcomes but also by
considering the intentions. This behavioral concept can be adapted to train
cooperative agents in Multi-Agent Reinforcement Learning (MARL). We propose the
KindMARL method, where agents&apos; intentions are measured by counterfactual
reasoning over the environmental impact of the actions that were available to
the agents. More specifically, the current environment state is compared with
the estimation of the current environment state provided that the agent had
chosen another action. The difference between each agent&apos;s reward, as the
outcome of its action, with that of its fellow, multiplied by the intention of
the fellow is then taken as the fellow&apos;s &quot;kindness&quot;. If the result of each
reward-comparison confirms the agent&apos;s superiority, it perceives the fellow&apos;s
kindness and reduces its own reward. Experimental results in the Cleanup and
Harvest environments show that training based on the KindMARL method enabled
the agents to earn 89\% (resp. 37\%) and 44% (resp. 43\%) more total rewards
than training based on the Inequity Aversion and Social Influence methods. The
effectiveness of KindMARL is further supported by experiments in a traffic
light control problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alamiyan_Harandi_F/0/1/0/all/0/1&quot;&gt;Farinaz Alamiyan-Harandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassanjani_M/0/1/0/all/0/1&quot;&gt;Mersad Hassanjani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramazi_P/0/1/0/all/0/1&quot;&gt;Pouria Ramazi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04240">
<title>Environmental-Impact Based Multi-Agent Reinforcement Learning. (arXiv:2311.04240v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.04240</link>
<description rdf:parseType="Literal">&lt;p&gt;To promote cooperation and strengthen the individual impact on the collective
outcome in social dilemmas, we propose the Environmental-impact Multi-Agent
Reinforcement Learning (EMuReL) method where each agent estimates the
&quot;environmental impact&quot; of every other agent, that is, the difference in the
current environment state compared to the hypothetical environment in the
absence of that other agent. Inspired by the Inequity Aversion model, the agent
then compares its own reward with those of its fellows multiplied by their
environmental impacts. If its reward exceeds the scaled reward of one of its
fellows, the agent takes &quot;social responsibility&quot; toward that fellow by reducing
its own reward. Therefore, the less influential an agent is in reaching the
current state, the more social responsibility is taken by other agents.
Experiments in the Cleanup (resp. Harvest) test environment demonstrate that
agents trained based on EMuReL learn to cooperate more effectively and obtain
$54\%$ ($39\%$) and $20\%$ ($44\%$) more total rewards while preserving the
same cooperation levels compared to when they are trained based on the two
state-of-the-art reward reshaping methods inequity aversion and social
influence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alamiyan_Harandi_F/0/1/0/all/0/1&quot;&gt;Farinaz Alamiyan-Harandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramazi_P/0/1/0/all/0/1&quot;&gt;Pouria Ramazi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04241">
<title>AI-Enabled Unmanned Vehicle-Assisted Reconfigurable Intelligent Surfaces: Deployment, Prototyping, Experiments, and Opportunities. (arXiv:2311.04241v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2311.04241</link>
<description rdf:parseType="Literal">&lt;p&gt;The requirement of wireless data demands is increasingly high as the
sixth-generation (6G) technology evolves. Reconfigurable intelligent surface
(RIS) is promisingly deemed to be one of 6G techniques for extending service
coverage, reducing power consumption, and enhancing spectral efficiency. In
this article, we have provided some fundamentals of RIS deployment in theory
and hardware perspectives as well as utilization of artificial intelligence
(AI) and machine learning. We conducted an intelligent deployment of RIS
(i-Dris) prototype, including dual-band auto-guided vehicle (AGV) assisted RISs
associated with an mmWave base station (BS) and a receiver. The RISs are
deployed on the AGV with configured incident/reflection angles. While, both the
mmWave BS and receiver are associated with an edge server monitoring downlink
packets for obtaining system throughput. We have designed a federated
multi-agent reinforcement learning scheme associated with several AGV-RIS
agents and sub-agents per AGV-RIS consisting of the deployment of position,
height, orientation and elevation angles. The experimental results presented
the stationary measurement in different aspects and scenarios. The i-Dris can
reach up to 980 Mbps transmission throughput under a bandwidth of 100 MHz with
comparably low complexity as well as rapid deployment, which outperforms the
other existing works. At last, we highlight some opportunities and future
issues in leveraging RIS-empowered wireless communication networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Li-Hsiang Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Feng_K/0/1/0/all/0/1&quot;&gt;Kai-Ten Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_T/0/1/0/all/0/1&quot;&gt;Ta-Sung Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yuan-Chun Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lin_S/0/1/0/all/0/1&quot;&gt;Shih-Cheng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chang_C/0/1/0/all/0/1&quot;&gt;Chia-Chan Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chang_S/0/1/0/all/0/1&quot;&gt;Sheng-Fuh Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04244">
<title>HKTGNN: Hierarchical Knowledge Transferable Graph Neural Network-based Supply Chain Risk Assessment. (arXiv:2311.04244v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.04244</link>
<description rdf:parseType="Literal">&lt;p&gt;The strength of a supply chain is an important measure of a country&apos;s or
region&apos;s technical advancement and overall competitiveness. Establishing supply
chain risk assessment models for effective management and mitigation of
potential risks has become increasingly crucial. As the number of businesses
grows, the important relationships become more complicated and difficult to
measure. This emphasizes the need of extracting relevant information from graph
data. Previously, academics mostly employed knowledge inference to increase the
visibility of links between nodes in the supply chain. However, they have not
solved the data hunger problem of single node feature characteristics. We
propose a hierarchical knowledge transferable graph neural network-based
(HKTGNN) supply chain risk assessment model to address these issues. Our
approach is based on current graph embedding methods for assessing corporate
investment risk assessment. We embed the supply chain network corresponding to
individual goods in the supply chain using the graph embedding module,
resulting in a directed homogeneous graph with just product nodes. This reduces
the complicated supply chain network into a basic product network. It addresses
difficulties using the domain difference knowledge transferable module based on
centrality, which is presented by the premise that supply chain feature
characteristics may be biased in the actual world. Meanwhile, the feature
complement and message passing will alleviate the data hunger problem, which is
driven by domain differences. Our model outperforms in experiments on a
real-world supply chain dataset. We will give an equation to prove that our
comparative experiment is both effective and fair.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhanting Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_K/0/1/0/all/0/1&quot;&gt;Kejun Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yuyanzhen Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1&quot;&gt;Chao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dongfen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ying_S/0/1/0/all/0/1&quot;&gt;Shi Ying&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruijin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04245">
<title>GPT-ST: Generative Pre-Training of Spatio-Temporal Graph Neural Networks. (arXiv:2311.04245v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.04245</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, there has been a rapid development of spatio-temporal
prediction techniques in response to the increasing demands of traffic
management and travel planning. While advanced end-to-end models have achieved
notable success in improving predictive performance, their integration and
expansion pose significant challenges. This work aims to address these
challenges by introducing a spatio-temporal pre-training framework that
seamlessly integrates with downstream baselines and enhances their performance.
The framework is built upon two key designs: (i) We propose a spatio-temporal
mask autoencoder as a pre-training model for learning spatio-temporal
dependencies. The model incorporates customized parameter learners and
hierarchical spatial pattern encoding networks. These modules are specifically
designed to capture spatio-temporal customized representations and intra- and
inter-cluster region semantic relationships, which have often been neglected in
existing approaches. (ii) We introduce an adaptive mask strategy as part of the
pre-training mechanism. This strategy guides the mask autoencoder in learning
robust spatio-temporal representations and facilitates the modeling of
different relationships, ranging from intra-cluster to inter-cluster, in an
easy-to-hard training manner. Extensive experiments conducted on representative
benchmarks demonstrate the effectiveness of our proposed method. We have made
our model implementation publicly available at https://github.com/HKUDS/GPT-ST.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhonghang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_L/0/1/0/all/0/1&quot;&gt;Lianghao Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chao Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04247">
<title>Analysis and Applications of Deep Learning with Finite Samples in Full Life-Cycle Intelligence of Nuclear Power Generation. (arXiv:2311.04247v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.04247</link>
<description rdf:parseType="Literal">&lt;p&gt;The advent of Industry 4.0 has precipitated the incorporation of Artificial
Intelligence (AI) methods within industrial contexts, aiming to realize
intelligent manufacturing, operation as well as maintenance, also known as
industrial intelligence. However, intricate industrial milieus, particularly
those relating to energy exploration and production, frequently encompass data
characterized by long-tailed class distribution, sample imbalance, and domain
shift. These attributes pose noteworthy challenges to data-centric Deep
Learning (DL) techniques, crucial for the realization of industrial
intelligence. The present study centers on the intricate and distinctive
industrial scenarios of Nuclear Power Generation (NPG), meticulously
scrutinizing the application of DL techniques under the constraints of finite
data samples. Initially, the paper expounds on potential employment scenarios
for AI across the full life-cycle of NPG. Subsequently, we delve into an
evaluative exposition of DL&apos;s advancement, grounded in the finite sample
perspective. This encompasses aspects such as small-sample learning, few-shot
learning, zero-shot learning, and open-set recognition, also referring to the
unique data characteristics of NPG. The paper then proceeds to present two
specific case studies. The first revolves around the automatic recognition of
zirconium alloy metallography, while the second pertains to open-set
recognition for signal diagnosis of machinery sensors. These cases, spanning
the entirety of NPG&apos;s life-cycle, are accompanied by constructive outcomes and
insightful deliberations. By exploring and applying DL methodologies within the
constraints of finite sample availability, this paper not only furnishes a
robust technical foundation but also introduces a fresh perspective toward the
secure and efficient advancement and exploitation of this advanced energy
source.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1&quot;&gt;Chenwei Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wenqiang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Caiyang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zhenan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jizhe Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shudong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yi Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jianming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1&quot;&gt;Wentao Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1&quot;&gt;Jiancheng Lv&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04250">
<title>Unifying Structure and Language Semantic for Efficient Contrastive Knowledge Graph Completion with Structured Entity Anchors. (arXiv:2311.04250v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.04250</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of knowledge graph completion (KGC) is to predict missing links in a
KG using trained facts that are already known. In recent, pre-trained language
model (PLM) based methods that utilize both textual and structural information
are emerging, but their performances lag behind state-of-the-art (SOTA)
structure-based methods or some methods lose their inductive inference
capabilities in the process of fusing structure embedding to text encoder. In
this paper, we propose a novel method to effectively unify structure
information and language semantics without losing the power of inductive
reasoning. We adopt entity anchors and these anchors and textual description of
KG elements are fed together into the PLM-based encoder to learn unified
representations. In addition, the proposed method utilizes additional random
negative samples which can be reused in the each mini-batch during contrastive
learning to learn a generalized entity representations. We verify the
effectiveness of the our proposed method through various experiments and
analysis. The experimental results on standard benchmark widely used in link
prediction task show that the proposed model outperforms existing the SOTA KGC
models. Especially, our method show the largest performance improvement on
FB15K-237, which is competitive to the SOTA of structure-based KGC methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Je_S/0/1/0/all/0/1&quot;&gt;Sang-Hyun Je&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_W/0/1/0/all/0/1&quot;&gt;Wontae Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_K/0/1/0/all/0/1&quot;&gt;Kwangjin Oh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04251">
<title>MixtureGrowth: Growing Neural Networks by Recombining Learned Parameters. (arXiv:2311.04251v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.04251</link>
<description rdf:parseType="Literal">&lt;p&gt;Most deep neural networks are trained under fixed network architectures and
require retraining when the architecture changes. If expanding the network&apos;s
size is needed, it is necessary to retrain from scratch, which is expensive. To
avoid this, one can grow from a small network by adding random weights over
time to gradually achieve the target network size. However, this naive approach
falls short in practice as it brings too much noise to the growing process.
Prior work tackled this issue by leveraging the already learned weights and
training data for generating new weights through conducting a computationally
expensive analysis step. In this paper, we introduce MixtureGrowth, a new
approach to growing networks that circumvents the initialization overhead in
prior work. Before growing, each layer in our model is generated with a linear
combination of parameter templates. Newly grown layer weights are generated by
using a new linear combination of existing templates for a layer. On one hand,
these templates are already trained for the task, providing a strong
initialization. On the other, the new coefficients provide flexibility for the
added layer weights to learn something new. We show that our approach boosts
top-1 accuracy over the state-of-the-art by 2-2.5% on CIFAR-100 and ImageNet
datasets, while achieving comparable performance with fewer FLOPs to a larger
network trained from scratch. Code is available at
https://github.com/chaudatascience/mixturegrowth.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_C/0/1/0/all/0/1&quot;&gt;Chau Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teterwak_P/0/1/0/all/0/1&quot;&gt;Piotr Teterwak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nelson_S/0/1/0/all/0/1&quot;&gt;Soren Nelson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plummer_B/0/1/0/all/0/1&quot;&gt;Bryan A. Plummer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04254">
<title>Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation. (arXiv:2311.04254v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.04254</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in Large Language Models (LLMs) have revolutionized
decision-making by breaking down complex problems into more manageable language
sequences referred to as ``thoughts&apos;&apos;. An effective thought design should
consider three key perspectives: performance, efficiency, and flexibility.
However, existing thought can at most exhibit two of these attributes. To
address these limitations, we introduce a novel thought prompting approach
called ``Everything of Thoughts&apos;&apos; (XoT) to defy the law of ``Penrose triangle
of existing thought paradigms. XoT leverages pretrained reinforcement learning
and Monte Carlo Tree Search (MCTS) to incorporate external domain knowledge
into thoughts, thereby enhancing LLMs&apos; capabilities and enabling them to
generalize to unseen problems efficiently. Through the utilization of the
MCTS-LLM collaborative thought revision framework, this approach autonomously
produces high-quality comprehensive cognitive mappings with minimal LLM
interactions. Additionally, XoT empowers LLMs to engage in unconstrained
thinking, allowing for flexible cognitive mappings for problems with multiple
solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1&quot;&gt;Ruomeng Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chaoyun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1&quot;&gt;Minghua Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_S/0/1/0/all/0/1&quot;&gt;Si Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajmohan_S/0/1/0/all/0/1&quot;&gt;Saravan Rajmohan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1&quot;&gt;Qingwei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dongmei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04256">
<title>Foundational propositions of hesitant fuzzy sets and parameter reductions of hesitant fuzzy information systems. (arXiv:2311.04256v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.04256</link>
<description rdf:parseType="Literal">&lt;p&gt;Hesitant fuzzy sets are widely used in the instances of uncertainty and
hesitation. The inclusion relationship is an important and foundational
definition for sets. Hesitant fuzzy set, as a kind of set, needs explicit
definition of inclusion relationship. Base on the hesitant fuzzy membership
degree of discrete form, several kinds of inclusion relationships for hesitant
fuzzy sets are proposed. And then some foundational propositions of hesitant
fuzzy sets and the families of hesitant fuzzy sets are presented. Finally, some
foundational propositions of hesitant fuzzy information systems with respect to
parameter reductions are put forward, and an example and an algorithm are given
to illustrate the processes of parameter reductions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1&quot;&gt;Shizhan Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04258">
<title>IoT-Based Environmental Control System for Fish Farms with Sensor Integration and Machine Learning Decision Support. (arXiv:2311.04258v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2311.04258</link>
<description rdf:parseType="Literal">&lt;p&gt;In response to the burgeoning global demand for seafood and the challenges of
managing fish farms, we introduce an innovative IoT based environmental control
system that integrates sensor technology and advanced machine learning decision
support. Deploying a network of wireless sensors within the fish farm, we
continuously collect real-time data on crucial environmental parameters,
including water temperature, pH levels, humidity, and fish behavior. This data
undergoes meticulous preprocessing to ensure its reliability, including
imputation, outlier detection, feature engineering, and synchronization. At the
heart of our system are four distinct machine learning algorithms: Random
Forests predict and optimize water temperature and pH levels for the fish,
fostering their health and growth; Support Vector Machines (SVMs) function as
an early warning system, promptly detecting diseases and parasites in fish;
Gradient Boosting Machines (GBMs) dynamically fine-tune the feeding schedule
based on real-time environmental conditions, promoting resource efficiency and
fish productivity; Neural Networks manage the operation of critical equipment
like water pumps and heaters to maintain the desired environmental conditions
within the farm. These machine learning algorithms collaboratively make
real-time decisions to ensure that the fish farm&apos;s environmental conditions
align with predefined specifications, leading to improved fish health and
productivity while simultaneously reducing resource wastage, thereby
contributing to increased profitability and sustainability. This research
article showcases the power of data-driven decision support in fish farming,
promising to meet the growing demand for seafood while emphasizing
environmental responsibility and economic viability, thus revolutionizing the
future of fish farming.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dhinakaran_D/0/1/0/all/0/1&quot;&gt;D. Dhinakaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gopalakrishnan_S/0/1/0/all/0/1&quot;&gt;S. Gopalakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Manigandan_M/0/1/0/all/0/1&quot;&gt;M.D. Manigandan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Anish_T/0/1/0/all/0/1&quot;&gt;T. P. Anish&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04262">
<title>ETDPC: A Multimodality Framework for Classifying Pages in Electronic Theses and Dissertations. (arXiv:2311.04262v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04262</link>
<description rdf:parseType="Literal">&lt;p&gt;Electronic theses and dissertations (ETDs) have been proposed, advocated, and
generated for more than 25 years. Although ETDs are hosted by commercial or
institutional digital library repositories, they are still an understudied type
of scholarly big data, partially because they are usually longer than
conference proceedings and journals. Segmenting ETDs will allow researchers to
study sectional content. Readers can navigate to particular pages of interest,
discover, and explore the content buried in these long documents. Most existing
frameworks on document page classification are designed for classifying general
documents and perform poorly on ETDs. In this paper, we propose ETDPC. Its
backbone is a two-stream multimodal model with a cross-attention network to
classify ETD pages into 13 categories. To overcome the challenge of imbalanced
labeled samples, we augmented data for minority categories and employed a
hierarchical classifier. ETDPC outperforms the state-of-the-art models in all
categories, achieving an F1 of 0.84 -- 0.96 for 9 out of 13 categories. We also
demonstrated its data efficiency. The code and data can be found on GitHub
(https://github.com/lamps-lab/ETDMiner/tree/master/etd_segmentation).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1&quot;&gt;Muntabir Hasan Choudhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salsabil_L/0/1/0/all/0/1&quot;&gt;Lamia Salsabil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ingram_W/0/1/0/all/0/1&quot;&gt;William A. Ingram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fox_E/0/1/0/all/0/1&quot;&gt;Edward A. Fox&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jian Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04284">
<title>CRAB: Assessing the Strength of Causal Relationships Between Real-world Events. (arXiv:2311.04284v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04284</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding narratives requires reasoning about the cause-and-effect
relationships between events mentioned in the text. While existing foundation
models yield impressive results in many NLP tasks requiring reasoning, it is
unclear whether they understand the complexity of the underlying network of
causal relationships of events in narratives. In this work, we present CRAB, a
new Causal Reasoning Assessment Benchmark designed to evaluate causal
understanding of events in real-world narratives. CRAB contains fine-grained,
contextual causality annotations for ~2.7K pairs of real-world events that
describe various newsworthy event timelines (e.g., the acquisition of Twitter
by Elon Musk). Using CRAB, we measure the performance of several large language
models, demonstrating that most systems achieve poor performance on the task.
Motivated by classical causal principles, we also analyze the causal structures
of groups of events in CRAB, and find that models perform worse on causal
reasoning when events are derived from complex causal structures compared to
simple linear causal chains. We make our dataset and code available to the
research community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romanou_A/0/1/0/all/0/1&quot;&gt;Angelika Romanou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montariol_S/0/1/0/all/0/1&quot;&gt;Syrielle Montariol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paul_D/0/1/0/all/0/1&quot;&gt;Debjit Paul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laugier_L/0/1/0/all/0/1&quot;&gt;Leo Laugier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aberer_K/0/1/0/all/0/1&quot;&gt;Karl Aberer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1&quot;&gt;Antoine Bosselut&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04301">
<title>Class-Incremental Continual Learning for General Purpose Healthcare Models. (arXiv:2311.04301v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.04301</link>
<description rdf:parseType="Literal">&lt;p&gt;Healthcare clinics regularly encounter dynamic data that changes due to
variations in patient populations, treatment policies, medical devices, and
emerging disease patterns. Deep learning models can suffer from catastrophic
forgetting when fine-tuned in such scenarios, causing poor performance on
previously learned tasks. Continual learning allows learning on new tasks
without performance drop on previous tasks. In this work, we investigate the
performance of continual learning models on four different medical imaging
scenarios involving ten classification datasets from diverse modalities,
clinical specialties, and hospitals. We implement various continual learning
approaches and evaluate their performance in these scenarios. Our results
demonstrate that a single model can sequentially learn new tasks from different
specialties and achieve comparable performance to naive methods. These findings
indicate the feasibility of recycling or sharing models across the same or
different medical specialties, offering another step towards the development of
general-purpose medical imaging AI that can be shared across institutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Amritpal Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurbuz_M/0/1/0/all/0/1&quot;&gt;Mustafa Burak Gurbuz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gantha_S/0/1/0/all/0/1&quot;&gt;Shiva Souhith Gantha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jasti_P/0/1/0/all/0/1&quot;&gt;Prahlad Jasti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04313">
<title>Improved Child Text-to-Speech Synthesis through Fastpitch-based Transfer Learning. (arXiv:2311.04313v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2311.04313</link>
<description rdf:parseType="Literal">&lt;p&gt;Speech synthesis technology has witnessed significant advancements in recent
years, enabling the creation of natural and expressive synthetic speech. One
area of particular interest is the generation of synthetic child speech, which
presents unique challenges due to children&apos;s distinct vocal characteristics and
developmental stages. This paper presents a novel approach that leverages the
Fastpitch text-to-speech (TTS) model for generating high-quality synthetic
child speech. This study uses the transfer learning training pipeline. The
approach involved finetuning a multi-speaker TTS model to work with child
speech. We use the cleaned version of the publicly available MyST dataset (55
hours) for our finetuning experiments. We also release a prototype dataset of
synthetic speech samples generated from this research together with model code
to support further research. By using a pretrained MOSNet, we conducted an
objective assessment that showed a significant correlation between real and
synthetic child voices. Additionally, to validate the intelligibility of the
generated speech, we employed an automatic speech recognition (ASR) model to
compare the word error rates (WER) of real and synthetic child voices. The
speaker similarity between the real and generated speech is also measured using
a pretrained speaker encoder.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1&quot;&gt;Rishabh Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corcoran_P/0/1/0/all/0/1&quot;&gt;Peter Corcoran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04325">
<title>Extending Machine Learning-Based Early Sepsis Detection to Different Demographics. (arXiv:2311.04325v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.04325</link>
<description rdf:parseType="Literal">&lt;p&gt;Sepsis requires urgent diagnosis, but research is predominantly focused on
Western datasets. In this study, we perform a comparative analysis of two
ensemble learning methods, LightGBM and XGBoost, using the public eICU-CRD
dataset and a private South Korean St. Mary&apos;s Hospital&apos;s dataset. Our analysis
reveals the effectiveness of these methods in addressing healthcare data
imbalance and enhancing sepsis detection. Specifically, LightGBM shows a slight
edge in computational efficiency and scalability. The study paves the way for
the broader application of machine learning in critical care, thereby expanding
the reach of predictive analytics in healthcare globally.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parmar_S/0/1/0/all/0/1&quot;&gt;Surajsinh Parmar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_T/0/1/0/all/0/1&quot;&gt;Tao Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;San Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yonghwan Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jang Yong Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04326">
<title>Educating for AI Cybersecurity Work and Research: Ethics, Systems Thinking, and Communication Requirements. (arXiv:2311.04326v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2311.04326</link>
<description rdf:parseType="Literal">&lt;p&gt;The present study explored managerial and instructor perceptions of their
freshly employed cybersecurity workers&apos; or students&apos; preparedness to work
effectively in a changing cybersecurity environment that includes AI tools.
Specifically, we related perceptions of technical preparedness to ethical,
systems thinking, and communication skills. We found that managers and
professors perceive preparedness to use AI tools in cybersecurity to be
significantly associated with all three non-technical skill sets. Most
important, ethics is a clear leader in the network of relationships. Contrary
to expectations that ethical concerns are left behind in the rush to adopt the
most advanced AI tools in security, both higher education instructors and
managers appreciate their role and see them closely associated with technical
prowess. Another significant finding is that professors over-estimate students&apos;
preparedness for ethical, system thinking, and communication abilities compared
to IT managers&apos; perceptions of their newly employed IT workers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matei_S/0/1/0/all/0/1&quot;&gt;Sorin Adam Matei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertino_E/0/1/0/all/0/1&quot;&gt;Elisa Bertino&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04335">
<title>Sub-Sentence Encoder: Contrastive Learning of Propositional Semantic Representations. (arXiv:2311.04335v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04335</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce sub-sentence encoder, a contrastively-learned contextual
embedding model for fine-grained semantic representation of text. In contrast
to the standard practice with sentence embeddings, where the meaning of an
entire sequence of text is encoded into a fixed-length vector, the sub-sentence
encoder learns to produce distinct contextual embeddings corresponding to
different atomic propositions, i.e. atomic units of meaning expressed within a
text sequence. The sub-sentence embeddings are contrastively learned to
recognize (inferred) semantic equivalence between propositions across different
text sequences. Our experiments show the effectiveness of sub-sentence encoders
in applications, such as retrieving supporting facts for fine-grained text
attribution or recognizing the conditional semantic similarity between texts.
In practice, we demonstrate that sub-sentence encoders keep the same level of
inference cost and space complexity compared to sentence encoders.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Sihao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Ben Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1&quot;&gt;Wenhao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1&quot;&gt;Dian Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1&quot;&gt;Baolin Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1&quot;&gt;Dan Roth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1&quot;&gt;Dong Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04345">
<title>A Taxonomy of Rater Disagreements: Surveying Challenges &amp; Opportunities from the Perspective of Annotating Online Toxicity. (arXiv:2311.04345v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04345</link>
<description rdf:parseType="Literal">&lt;p&gt;Toxicity is an increasingly common and severe issue in online spaces.
Consequently, a rich line of machine learning research over the past decade has
focused on computationally detecting and mitigating online toxicity. These
efforts crucially rely on human-annotated datasets that identify toxic content
of various kinds in social media texts. However, such annotations historically
yield low inter-rater agreement, which was often dealt with by taking the
majority vote or other such approaches to arrive at a single ground truth
label. Recent research has pointed out the importance of accounting for the
subjective nature of this task when building and utilizing these datasets, and
this has triggered work on analyzing and better understanding rater
disagreements, and how they could be effectively incorporated into the machine
learning developmental pipeline. While these efforts are filling an important
gap, there is a lack of a broader framework about the root causes of rater
disagreement, and therefore, we situate this work within that broader
landscape. In this survey paper, we analyze a broad set of literature on the
reasons behind rater disagreements focusing on online toxicity, and propose a
detailed taxonomy for the same. Further, we summarize and discuss the potential
solutions targeting each reason for disagreement. We also discuss several open
issues, which could promote the future development of online toxicity research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenbo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1&quot;&gt;Hangzhi Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kivlichan_I/0/1/0/all/0/1&quot;&gt;Ian D Kivlichan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabhakaran_V/0/1/0/all/0/1&quot;&gt;Vinodkumar Prabhakaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yadav_D/0/1/0/all/0/1&quot;&gt;Davis Yadav&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yadav_A/0/1/0/all/0/1&quot;&gt;Amulya Yadav&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04348">
<title>Evaluating the Effectiveness of Retrieval-Augmented Large Language Models in Scientific Document Reasoning. (arXiv:2311.04348v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04348</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the dramatic progress in Large Language Model (LLM) development, LLMs
often provide seemingly plausible but not factual information, often referred
to as hallucinations. Retrieval-augmented LLMs provide a non-parametric
approach to solve these issues by retrieving relevant information from external
data sources and augment the training process. These models help to trace
evidence from an externally provided knowledge base allowing the model
predictions to be better interpreted and verified. In this work, we critically
evaluate these models in their ability to perform in scientific document
reasoning tasks. To this end, we tuned multiple such model variants with
science-focused instructions and evaluated them on a scientific document
reasoning benchmark for the usefulness of the retrieved document passages. Our
findings suggest that models justify predictions in science tasks with
fabricated evidence and leveraging scientific corpus as pretraining data does
not alleviate the risk of evidence fabrication.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munikoti_S/0/1/0/all/0/1&quot;&gt;Sai Munikoti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1&quot;&gt;Anurag Acharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagle_S/0/1/0/all/0/1&quot;&gt;Sridevi Wagle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horawalavithana_S/0/1/0/all/0/1&quot;&gt;Sameera Horawalavithana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04372">
<title>Enhancing Malware Detection by Integrating Machine Learning with Cuckoo Sandbox. (arXiv:2311.04372v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2311.04372</link>
<description rdf:parseType="Literal">&lt;p&gt;In the modern era, malware is experiencing a significant increase in both its
variety and quantity, aligning with the widespread adoption of the digital
world. This surge in malware has emerged as a critical challenge in the realm
of cybersecurity, prompting numerous research endeavors and contributions to
address the issue. Machine learning algorithms have been leveraged for malware
detection due to their ability to uncover concealed patterns within vast
datasets. However, deep learning algorithms, characterized by their
multi-layered structure, surpass the limitations of traditional machine
learning approaches. By employing deep learning techniques such as CNN
(Convolutional Neural Network) and RNN (Recurrent Neural Network), this study
aims to classify and identify malware extracted from a dataset containing API
call sequences. The performance of these algorithms is compared with that of
conventional machine learning methods, including SVM (Support Vector Machine),
RF (Random Forest), KNN (K-Nearest Neighbors), XGB (Extreme Gradient Boosting),
and GBC (Gradient Boosting Classifier), all using the same dataset. The
outcomes of this research demonstrate that both deep learning and machine
learning algorithms achieve remarkably high levels of accuracy, reaching up to
99% in certain cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alshmarni_A/0/1/0/all/0/1&quot;&gt;Amaal F. Alshmarni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alliheedi_M/0/1/0/all/0/1&quot;&gt;Mohammed A. Alliheedi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04386">
<title>Harnessing Manycore Processors with Distributed Memory for Accelerated Training of Sparse and Recurrent Models. (arXiv:2311.04386v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2311.04386</link>
<description rdf:parseType="Literal">&lt;p&gt;Current AI training infrastructure is dominated by single instruction
multiple data (SIMD) and systolic array architectures, such as Graphics
Processing Units (GPUs) and Tensor Processing Units (TPUs), that excel at
accelerating parallel workloads and dense vector matrix multiplications.
Potentially more efficient neural network models utilizing sparsity and
recurrence cannot leverage the full power of SIMD processor and are thus at a
severe disadvantage compared to today&apos;s prominent parallel architectures like
Transformers and CNNs, thereby hindering the path towards more sustainable AI.
To overcome this limitation, we explore sparse and recurrent model training on
a massively parallel multiple instruction multiple data (MIMD) architecture
with distributed local memory. We implement a training routine based on
backpropagation through time (BPTT) for the brain-inspired class of Spiking
Neural Networks (SNNs) that feature binary sparse activations. We observe a
massive advantage in using sparse activation tensors with a MIMD processor, the
Intelligence Processing Unit (IPU) compared to GPUs. On training workloads, our
results demonstrate 5-10x throughput gains compared to A100 GPUs and up to 38x
gains for higher levels of activation sparsity, without a significant slowdown
in training convergence or reduction in final model performance. Furthermore,
our results show highly promising trends for both single and multi IPU
configurations as we scale up to larger model sizes. Our work paves the way
towards more efficient, non-standard models via AI training hardware beyond
GPUs, and competitive large scale SNN models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finkbeiner_J/0/1/0/all/0/1&quot;&gt;Jan Finkbeiner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gmeinder_T/0/1/0/all/0/1&quot;&gt;Thomas Gmeinder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pupilli_M/0/1/0/all/0/1&quot;&gt;Mark Pupilli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Titterton_A/0/1/0/all/0/1&quot;&gt;Alexander Titterton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neftci_E/0/1/0/all/0/1&quot;&gt;Emre Neftci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04397">
<title>ToP-ToM: Trust-aware Robot Policy with Theory of Mind. (arXiv:2311.04397v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.04397</link>
<description rdf:parseType="Literal">&lt;p&gt;Theory of Mind (ToM) is a fundamental cognitive architecture that endows
humans with the ability to attribute mental states to others. Humans infer the
desires, beliefs, and intentions of others by observing their behavior and, in
turn, adjust their actions to facilitate better interpersonal communication and
team collaboration. In this paper, we investigated trust-aware robot policy
with the theory of mind in a multiagent setting where a human collaborates with
a robot against another human opponent. We show that by only focusing on team
performance, the robot may resort to the reverse psychology trick, which poses
a significant threat to trust maintenance. The human&apos;s trust in the robot will
collapse when they discover deceptive behavior by the robot. To mitigate this
problem, we adopt the robot theory of mind model to infer the human&apos;s trust
beliefs, including true belief and false belief (an essential element of ToM).
We designed a dynamic trust-aware reward function based on different trust
beliefs to guide the robot policy learning, which aims to balance between
avoiding human trust collapse due to robot reverse psychology. The experimental
results demonstrate the importance of the ToM-based robot policy for
human-robot trust and the effectiveness of our robot ToM-based robot policy in
multiagent interaction settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Chuang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serhan_B/0/1/0/all/0/1&quot;&gt;Baris Serhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cangelosi_A/0/1/0/all/0/1&quot;&gt;Angelo Cangelosi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04400">
<title>LRM: Large Reconstruction Model for Single Image to 3D. (arXiv:2311.04400v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04400</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose the first Large Reconstruction Model (LRM) that predicts the 3D
model of an object from a single input image within just 5 seconds. In contrast
to many previous methods that are trained on small-scale datasets such as
ShapeNet in a category-specific fashion, LRM adopts a highly scalable
transformer-based architecture with 500 million learnable parameters to
directly predict a neural radiance field (NeRF) from the input image. We train
our model in an end-to-end manner on massive multi-view data containing around
1 million objects, including both synthetic renderings from Objaverse and real
captures from MVImgNet. This combination of a high-capacity model and
large-scale training data empowers our model to be highly generalizable and
produce high-quality 3D reconstructions from various testing inputs including
real-world in-the-wild captures and images from generative models. Video demos
and interactable 3D meshes can be found on this website:
https://yiconghong.me/LRM/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1&quot;&gt;Yicong Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jiuxiang Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_S/0/1/0/all/0/1&quot;&gt;Sai Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Difan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Feng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sunkavalli_K/0/1/0/all/0/1&quot;&gt;Kalyan Sunkavalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1&quot;&gt;Trung Bui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1&quot;&gt;Hao Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04403">
<title>Human-Centered Planning. (arXiv:2311.04403v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.04403</link>
<description rdf:parseType="Literal">&lt;p&gt;LLMs have recently made impressive inroads on tasks whose output is
structured, such as coding, robotic planning and querying databases. The vision
of creating AI-powered personal assistants also involves creating structured
outputs, such as a plan for one&apos;s day, or for an overseas trip. Here, since the
plan is executed by a human, the output doesn&apos;t have to satisfy strict
syntactic constraints. A useful assistant should also be able to incorporate
vague constraints specified by the user in natural language. This makes LLMs an
attractive option for planning.
&lt;/p&gt;
&lt;p&gt;We consider the problem of planning one&apos;s day. We develop an LLM-based
planner (LLMPlan) extended with the ability to self-reflect on its output and a
symbolic planner (SymPlan) with the ability to translate text constraints into
a symbolic representation. Despite no formal specification of constraints, we
find that LLMPlan performs explicit constraint satisfaction akin to the
traditional symbolic planners on average (2% performance difference), while
retaining the reasoning of implicit requirements. Consequently, LLM-based
planners outperform their symbolic counterparts in user satisfaction (70.5% vs.
40.4%) during interactive evaluation with 40 users.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuliang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamra_N/0/1/0/all/0/1&quot;&gt;Nitin Kamra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Desai_R/0/1/0/all/0/1&quot;&gt;Ruta Desai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Halevy_A/0/1/0/all/0/1&quot;&gt;Alon Halevy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04412">
<title>Human Conditional Reasoning in Answer Set Programming. (arXiv:2311.04412v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.04412</link>
<description rdf:parseType="Literal">&lt;p&gt;Given a conditional sentence P=&amp;gt;Q (if P then Q) and respective facts, four
different types of inferences are observed in human reasoning. Affirming the
antecedent (AA) (or modus ponens) reasons Q from P; affirming the consequent
(AC) reasons P from Q; denying the antecedent (DA) reasons -Q from -P; and
denying the consequent (DC) (or modus tollens) reasons -P from -Q. Among them,
AA and DC are logically valid, while AC and DA are logically invalid and often
called logical fallacies. Nevertheless, humans often perform AC or DA as
pragmatic inference in daily life. In this paper, we realize AC, DA and DC
inferences in answer set programming. Eight different types of completion are
introduced and their semantics are given by answer sets. We investigate formal
properties and characterize human reasoning tasks in cognitive psychology.
Those completions are also applied to commonsense reasoning in AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sakama_C/0/1/0/all/0/1&quot;&gt;Chiaki Sakama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04418">
<title>AI-accelerated Discovery of Altermagnetic Materials. (arXiv:2311.04418v1 [cond-mat.mtrl-sci])</title>
<link>http://arxiv.org/abs/2311.04418</link>
<description rdf:parseType="Literal">&lt;p&gt;Altermagnetism, a new magnetic phase, has been theoretically proposed and
experimentally verified to be distinct from ferromagnetism and
antiferromagnetism. Although altermagnets have been found to possess many
exotic physical properties, the very limited availability of known
altermagnetic materials~(e.g., 14 confirmed materials) hinders the study of
such properties. Hence, discovering more types of altermagnetic materials is
crucial for a comprehensive understanding of altermagnetism and thus
facilitating new applications in the next generation information technologies,
e.g., storage devices and high-sensitivity sensors. Here, we report 25 new
altermagnetic materials that cover metals, semiconductors, and insulators,
discovered by an AI search engine unifying symmetry analysis, graph neural
network pre-training, optimal transport theory, and first-principles electronic
structure calculation. The wide range of electronic structural characteristics
reveals that various innovative physical properties manifest in these newly
discovered altermagnetic materials, e.g., anomalous Hall effect, anomalous Kerr
effect, and topological property. Noteworthy, we discovered 8 $i$-wave
altermagnetic materials for the first time. Overall, the AI search engine
performs much better than human experts and suggests a set of new altermagnetic
materials with unique properties, outlining its potential for accelerated
discovery of altermagnetic materials.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Gao_Z/0/1/0/all/0/1&quot;&gt;Ze-Feng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Qu_S/0/1/0/all/0/1&quot;&gt;Shuai Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Zeng_B/0/1/0/all/0/1&quot;&gt;Bocheng Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Ji-Rong Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Hao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Guo_P/0/1/0/all/0/1&quot;&gt;Pengjie Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zhong-Yi Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04419">
<title>PepLand: a large-scale pre-trained peptide representation model for a comprehensive landscape of both canonical and non-canonical amino acids. (arXiv:2311.04419v1 [q-bio.BM])</title>
<link>http://arxiv.org/abs/2311.04419</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the scientific community has become increasingly interested
on peptides with non-canonical amino acids due to their superior stability and
resistance to proteolytic degradation. These peptides present promising
modifications to biological, pharmacological, and physiochemical attributes in
both endogenous and engineered peptides. Notwithstanding their considerable
advantages, the scientific community exhibits a conspicuous absence of an
effective pre-trained model adept at distilling feature representations from
such complex peptide sequences. We herein propose PepLand, a novel pre-training
architecture for representation and property analysis of peptides spanning both
canonical and non-canonical amino acids. In essence, PepLand leverages a
comprehensive multi-view heterogeneous graph neural network tailored to unveil
the subtle structural representations of peptides. Empirical validations
underscore PepLand&apos;s effectiveness across an array of peptide property
predictions, encompassing protein-protein interactions, permeability,
solubility, and synthesizability. The rigorous evaluation confirms PepLand&apos;s
unparalleled capability in capturing salient synthetic peptide features,
thereby laying a robust foundation for transformative advances in
peptide-centric research domains. We have made all the source code utilized in
this study publicly accessible via GitHub at
https://github.com/zhangruochi/pepland
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruochi Zhang&lt;/a&gt; (1,2,3), &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Haoran Wu&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Xiu_Y/0/1/0/all/0/1&quot;&gt;Yuting Xiu&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kewei Li&lt;/a&gt; (1,4), &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Chen_N/0/1/0/all/0/1&quot;&gt;Ningning Chen&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu Wang&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yan Wang&lt;/a&gt; (1,2,4), &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xin Gao&lt;/a&gt; (5,6,7), &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhou_F/0/1/0/all/0/1&quot;&gt;Fengfeng Zhou&lt;/a&gt; (1,4,7) ((1) Key Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education, Jilin University, Changchun, China. (2) School of Artificial Intelligence, Jilin University, Changchun, China. (3) Syneron Technology, Guangzhou, China. (4) College of Computer Science and Technology, Jilin University, Changchun, China. (5) Computational Bioscience Research Center, King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia. (6) Computer Science Program, Computer, Electrical and Mathematical Sciences and Engineering Division, King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia. (7) Corresponding Authors)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04420">
<title>Data Factors for Better Compositional Generalization. (arXiv:2311.04420v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04420</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent diagnostic datasets on compositional generalization, such as SCAN
(Lake and Baroni, 2018) and COGS (Kim and Linzen, 2020), expose severe problems
in models trained from scratch on these datasets. However, in contrast to this
poor performance, state-of-the-art models trained on larger and more general
datasets show better generalization ability. In this work, to reconcile this
inconsistency, we conduct an empirical analysis by training Transformer models
on a variety of training sets with different data factors, including dataset
scale, pattern complexity, example difficulty, etc. First, we show that
increased dataset complexity can lead to better generalization behavior on
multiple different generalization challenges. To further understand this
improvement, we show two axes of the benefit from more complex datasets: they
provide more diverse examples so compositional understanding becomes more
effective, and they also prevent ungeneralizable memorization of the examples
due to reduced example repetition frequency. Finally, we explore how training
examples of different difficulty levels influence generalization differently.
On synthetic datasets, simple examples invoke stronger compositionality than
hard examples do. On larger-scale real language datasets, while hard examples
become more important potentially to ensure decent data coverage, a balanced
mixture of simple and hard examples manages to induce the strongest
generalizability. The code and data for this work are available at
https://github.com/owenzx/data4comp
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xiang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yichen Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1&quot;&gt;Mohit Bansal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04441">
<title>MixTEA: Semi-supervised Entity Alignment with Mixture Teaching. (arXiv:2311.04441v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.04441</link>
<description rdf:parseType="Literal">&lt;p&gt;Semi-supervised entity alignment (EA) is a practical and challenging task
because of the lack of adequate labeled mappings as training data. Most works
address this problem by generating pseudo mappings for unlabeled entities.
However, they either suffer from the erroneous (noisy) pseudo mappings or
largely ignore the uncertainty of pseudo mappings. In this paper, we propose a
novel semi-supervised EA method, termed as MixTEA, which guides the model
learning with an end-to-end mixture teaching of manually labeled mappings and
probabilistic pseudo mappings. We firstly train a student model using few
labeled mappings as standard. More importantly, in pseudo mapping learning, we
propose a bi-directional voting (BDV) strategy that fuses the alignment
decisions in different directions to estimate the uncertainty via the joint
matching confidence score. Meanwhile, we also design a matching diversity-based
rectification (MDR) module to adjust the pseudo mapping learning, thus reducing
the negative influence of noisy mappings. Extensive results on benchmark
datasets as well as further analyses demonstrate the superiority and the
effectiveness of our proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_F/0/1/0/all/0/1&quot;&gt;Feng Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1&quot;&gt;Xin Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1&quot;&gt;Xiang Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xuechen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1&quot;&gt;Lei Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Bin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1&quot;&gt;Yusong Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04457">
<title>Evaluating Uncertainty Quantification approaches for Neural PDEs in scientific applications. (arXiv:2311.04457v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.04457</link>
<description rdf:parseType="Literal">&lt;p&gt;The accessibility of spatially distributed data, enabled by affordable
sensors, field, and numerical experiments, has facilitated the development of
data-driven solutions for scientific problems, including climate change,
weather prediction, and urban planning. Neural Partial Differential Equations
(Neural PDEs), which combine deep learning (DL) techniques with domain
expertise (e.g., governing equations) for parameterization, have proven to be
effective in capturing valuable correlations within spatiotemporal datasets.
However, sparse and noisy measurements coupled with modeling approximation
introduce aleatoric and epistemic uncertainties. Therefore, quantifying
uncertainties propagated from model inputs to outputs remains a challenge and
an essential goal for establishing the trustworthiness of Neural PDEs. This
work evaluates various Uncertainty Quantification (UQ) approaches for both
Forward and Inverse Problems in scientific applications. Specifically, we
investigate the effectiveness of Bayesian methods, such as Hamiltonian Monte
Carlo (HMC) and Monte-Carlo Dropout (MCD), and a more conventional approach,
Deep Ensembles (DE). To illustrate their performance, we take two canonical
PDEs: Burger&apos;s equation and the Navier-Stokes equation. Our results indicate
that Neural PDEs can effectively reconstruct flow systems and predict the
associated unknown parameters. However, it is noteworthy that the results
derived from Bayesian methods, based on our observations, tend to display a
higher degree of certainty in their predictions as compared to those obtained
using the DE. This elevated certainty in predictions suggests that Bayesian
techniques might underestimate the true underlying uncertainty, thereby
appearing more confident in their predictions than the DE approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dongre_V/0/1/0/all/0/1&quot;&gt;Vardhan Dongre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hora_G/0/1/0/all/0/1&quot;&gt;Gurpreet Singh Hora&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04459">
<title>Improving Pacing in Long-Form Story Planning. (arXiv:2311.04459v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04459</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing LLM-based systems for writing long-form stories or story outlines
frequently suffer from unnatural pacing, whether glossing over important events
or over-elaborating on insignificant details, resulting in a jarring experience
for the reader. We propose a CONCrete Outline ConTrol (CONCOCT) system to
improve pacing when automatically generating story outlines. We first train a
concreteness evaluator to judge which of two events is more concrete
(low-level-detailed). This evaluator can then be used to control pacing in
hierarchical outline generation; in this work, we explore a vaguest-first
expansion procedure that aims for uniform pacing. We further use the evaluator
to filter new outline items based on predicted concreteness. Compared to a
baseline hierarchical outline generator, humans judge CONCOCT&apos;s pacing to be
more consistent over 57% of the time across multiple outline lengths; the gains
also translate to downstream stories. All code, data, and models are
open-sourced.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yichen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kevin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1&quot;&gt;Dan Klein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04467">
<title>RDGCN: Reinforced Dependency Graph Convolutional Network for Aspect-based Sentiment Analysis. (arXiv:2311.04467v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04467</link>
<description rdf:parseType="Literal">&lt;p&gt;Aspect-based sentiment analysis (ABSA) is dedicated to forecasting the
sentiment polarity of aspect terms within sentences. Employing graph neural
networks to capture structural patterns from syntactic dependency parsing has
been confirmed as an effective approach for boosting ABSA. In most works, the
topology of dependency trees or dependency-based attention coefficients is
often loosely regarded as edges between aspects and opinions, which can result
in insufficient and ambiguous syntactic utilization. To address these problems,
we propose a new reinforced dependency graph convolutional network (RDGCN) that
improves the importance calculation of dependencies in both distance and type
views. Initially, we propose an importance calculation criterion for the
minimum distances over dependency trees. Under the criterion, we design a
distance-importance function that leverages reinforcement learning for weight
distribution search and dissimilarity control. Since dependency types often do
not have explicit syntax like tree distances, we use global attention and mask
mechanisms to design type-importance functions. Finally, we merge these weights
and implement feature aggregation and classification. Comprehensive experiments
on three popular datasets demonstrate the effectiveness of the criterion and
importance functions. RDGCN outperforms state-of-the-art GNN-based baselines in
all validations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xusheng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1&quot;&gt;Hao Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1&quot;&gt;Qiong Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1&quot;&gt;Xu Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1&quot;&gt;Huailiang Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yanbing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qinglang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1&quot;&gt;Philip S. Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04474">
<title>Emergent Communication for Rules Reasoning. (arXiv:2311.04474v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.04474</link>
<description rdf:parseType="Literal">&lt;p&gt;Research on emergent communication between deep-learning-based agents has
received extensive attention due to its inspiration for linguistics and
artificial intelligence. However, previous attempts have hovered around
emerging communication under perception-oriented environmental settings, that
forces agents to describe low-level perceptual features intra image or symbol
contexts. In this work, inspired by the classic human reasoning test (namely
Raven&apos;s Progressive Matrix), we propose the Reasoning Game, a
cognition-oriented environment that encourages agents to reason and communicate
high-level rules, rather than perceived low-level contexts. Moreover, we
propose 1) an unbiased dataset (namely rule-RAVEN) as a benchmark to avoid
overfitting, 2) and a two-stage curriculum agent training method as a baseline
for more stable convergence in the Reasoning Game, where contexts and semantics
are bilaterally drifting. Experimental results show that, in the Reasoning
Game, a semantically stable and compositional language emerges to solve
reasoning problems. The emerged language helps agents apply the extracted rules
to the generalization of unseen context attributes, and to the transfer between
different context attributes or even tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1&quot;&gt;Yifan Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1&quot;&gt;Enshuai Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1&quot;&gt;Zidong Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xishan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1&quot;&gt;Xinkai Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yuanbo Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yongwei Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xuehai Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jiaming Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_Q/0/1/0/all/0/1&quot;&gt;Qi Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1&quot;&gt;Shaohui Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1&quot;&gt;Di Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Ruizhi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qi Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yunji Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04491">
<title>Explainable AI for Earth Observation: Current Methods, Open Challenges, and Opportunities. (arXiv:2311.04491v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.04491</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has taken by storm all fields involved in data analysis,
including remote sensing for Earth observation. However, despite significant
advances in terms of performance, its lack of explainability and
interpretability, inherent to neural networks in general since their inception,
remains a major source of criticism. Hence it comes as no surprise that the
expansion of deep learning methods in remote sensing is being accompanied by
increasingly intensive efforts oriented towards addressing this drawback
through the exploration of a wide spectrum of Explainable Artificial
Intelligence techniques. This chapter, organized according to prominent Earth
observation application fields, presents a panorama of the state-of-the-art in
explainable remote sensing image analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taskin_G/0/1/0/all/0/1&quot;&gt;Gulsen Taskin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aptoula_E/0/1/0/all/0/1&quot;&gt;Erchan Aptoula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erturk_A/0/1/0/all/0/1&quot;&gt;Alp Ert&amp;#xfc;rk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04498">
<title>NExT-Chat: An LMM for Chat, Detection and Segmentation. (arXiv:2311.04498v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04498</link>
<description rdf:parseType="Literal">&lt;p&gt;The development of large language models (LLMs) has greatly advanced the
field of multimodal understanding, leading to the emergence of large multimodal
models (LMMs). In order to enhance the level of visual comprehension, recent
studies have equipped LMMs with region-level understanding capabilities by
representing object bounding box coordinates as a series of text sequences
(pixel2seq). In this paper, we introduce a novel paradigm for object location
modeling called pixel2emb method, where we ask the LMM to output the location
embeddings and then decoded by different decoders. This paradigm allows for
different location formats (such as bounding boxes and masks) to be used in
multimodal conversations Furthermore, this kind of embedding based location
modeling enables the utilization of existing practices in localization tasks,
such as detection and segmentation. In scenarios with limited resources, our
pixel2emb demonstrates superior performance compared to existing
state-of-the-art (SOTA) approaches in both the location input and output tasks
under fair comparison. Leveraging the proposed pixel2emb method, we train an
LMM named NExT-Chat and demonstrate its capability of handling multiple tasks
like visual grounding, region caption, and grounded reasoning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Ao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Liming Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1&quot;&gt;Chen-Wei Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yun Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1&quot;&gt;Wei Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1&quot;&gt;Tat-Seng Chua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04512">
<title>FFINet: Future Feedback Interaction Network for Motion Forecasting. (arXiv:2311.04512v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04512</link>
<description rdf:parseType="Literal">&lt;p&gt;Motion forecasting plays a crucial role in autonomous driving, with the aim
of predicting the future reasonable motions of traffic agents. Most existing
methods mainly model the historical interactions between agents and the
environment, and predict multi-modal trajectories in a feedforward process,
ignoring potential trajectory changes caused by future interactions between
agents. In this paper, we propose a novel Future Feedback Interaction Network
(FFINet) to aggregate features the current observations and potential future
interactions for trajectory prediction. Firstly, we employ different
spatial-temporal encoders to embed the decomposed position vectors and the
current position of each scene, providing rich features for the subsequent
cross-temporal aggregation. Secondly, the relative interaction and
cross-temporal aggregation strategies are sequentially adopted to integrate
features in the current fusion module, observation interaction module, future
feedback module and global fusion module, in which the future feedback module
can enable the understanding of pre-action by feeding the influence of preview
information to feedforward prediction. Thirdly, the comprehensive interaction
features are further fed into final predictor to generate the joint predicted
trajectories of multiple agents. Extensive experimental results show that our
FFINet achieves the state-of-the-art performance on Argoverse 1 and Argoverse 2
motion forecasting benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1&quot;&gt;Miao Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shengqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Sanping Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_K/0/1/0/all/0/1&quot;&gt;Ke Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jingjing Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1&quot;&gt;Nanning Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04524">
<title>Validating ChatGPT Facts through RDF Knowledge Graphs and Sentence Similarity. (arXiv:2311.04524v1 [cs.DB])</title>
<link>http://arxiv.org/abs/2311.04524</link>
<description rdf:parseType="Literal">&lt;p&gt;Since ChatGPT offers detailed responses without justifications, and erroneous
facts even for popular persons, events and places, in this paper we present a
novel pipeline that retrieves the response of ChatGPT in RDF and tries to
validate the ChatGPT facts using one or more RDF Knowledge Graphs (KGs). To
this end we leverage DBpedia and LODsyndesis (an aggregated Knowledge Graph
that contains 2 billion triples from 400 RDF KGs of many domains) and short
sentence embeddings, and introduce an algorithm that returns the more relevant
triple(s) accompanied by their provenance and a confidence score. This enables
the validation of ChatGPT responses and their enrichment with justifications
and provenance. To evaluate this service (such services in general), we create
an evaluation benchmark that includes 2,000 ChatGPT facts; specifically 1,000
facts for famous Greek Persons, 500 facts for popular Greek Places, and 500
facts for Events related to Greece. The facts were manually labelled
(approximately 73% of ChatGPT facts were correct and 27% of facts were
erroneous). The results are promising; indicatively for the whole benchmark, we
managed to verify the 85.3% of the correct facts of ChatGPT and to find the
correct answer for the 62.6% of the erroneous ChatGPT facts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mountantonakis_M/0/1/0/all/0/1&quot;&gt;Michalis Mountantonakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tzitzikas_Y/0/1/0/all/0/1&quot;&gt;Yannis Tzitzikas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04535">
<title>RankAug: Augmented data ranking for text classification. (arXiv:2311.04535v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04535</link>
<description rdf:parseType="Literal">&lt;p&gt;Research on data generation and augmentation has been focused majorly on
enhancing generation models, leaving a notable gap in the exploration and
refinement of methods for evaluating synthetic data. There are several text
similarity metrics within the context of generated data filtering which can
impact the performance of specific Natural Language Understanding (NLU) tasks,
specifically focusing on intent and sentiment classification. In this study, we
propose RankAug, a text-ranking approach that detects and filters out the top
augmented texts in terms of being most similar in meaning with lexical and
syntactical diversity. Through experiments conducted on multiple datasets, we
demonstrate that the judicious selection of filtering techniques can yield a
substantial improvement of up to 35% in classification accuracy for
under-represented classes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_T/0/1/0/all/0/1&quot;&gt;Tiasa Singha Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basu_P/0/1/0/all/0/1&quot;&gt;Priyam Basu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04544">
<title>Local Differential Privacy for Smart Meter Data Sharing. (arXiv:2311.04544v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2311.04544</link>
<description rdf:parseType="Literal">&lt;p&gt;Energy disaggregation techniques, which use smart meter data to infer
appliance energy usage, can provide consumers and energy companies valuable
insights into energy management. However, these techniques also present privacy
risks, such as the potential for behavioral profiling. Local differential
privacy (LDP) methods provide strong privacy guarantees with high efficiency in
addressing privacy concerns. However, existing LDP methods focus on protecting
aggregated energy consumption data rather than individual appliances.
Furthermore, these methods do not consider the fact that smart meter data are a
form of streaming data, and its processing methods should account for time
windows. In this paper, we propose a novel LDP approach (named LDP-SmartEnergy)
that utilizes randomized response techniques with sliding windows to facilitate
the sharing of appliance-level energy consumption data over time while not
revealing individual users&apos; appliance usage patterns. Our evaluations show that
LDP-SmartEnergy runs efficiently compared to baseline methods. The results also
demonstrate that our solution strikes a balance between protecting privacy and
maintaining the utility of data for effective analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shanmugarasa_Y/0/1/0/all/0/1&quot;&gt;Yashothara Shanmugarasa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chamikara_M/0/1/0/all/0/1&quot;&gt;M.A.P. Chamikara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paik_H/0/1/0/all/0/1&quot;&gt;Hye-young Paik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanhere_S/0/1/0/all/0/1&quot;&gt;Salil S. Kanhere&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Liming Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04562">
<title>CAIS-DMA: A Decision-Making Assistant for Collaborative AI Systems. (arXiv:2311.04562v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2311.04562</link>
<description rdf:parseType="Literal">&lt;p&gt;A Collaborative Artificial Intelligence System (CAIS) is a cyber-physical
system that learns actions in collaboration with humans in a shared environment
to achieve a common goal. In particular, a CAIS is equipped with an AI model to
support the decision-making process of this collaboration. When an event
degrades the performance of CAIS (i.e., a disruptive event), this
decision-making process may be hampered or even stopped. Thus, it is of
paramount importance to monitor the learning of the AI model, and eventually
support its decision-making process in such circumstances. This paper
introduces a new methodology to automatically support the decision-making
process in CAIS when the system experiences performance degradation after a
disruptive event. To this aim, we develop a framework that consists of three
components: one manages or simulates CAIS&apos;s environment and disruptive events,
the second automates the decision-making process, and the third provides a
visual analysis of CAIS behavior. Overall, our framework automatically monitors
the decision-making process, intervenes whenever a performance degradation
occurs, and recommends the next action. We demonstrate our framework by
implementing an example with a real-world collaborative robot, where the
framework recommends the next action that balances between minimizing the
recovery time (i.e., resilience), and minimizing the energy adverse effects
(i.e., greenness).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rimawi_D/0/1/0/all/0/1&quot;&gt;Diaeddin Rimawi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lotta_A/0/1/0/all/0/1&quot;&gt;Antonio Lotta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Todescato_M/0/1/0/all/0/1&quot;&gt;Marco Todescato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russo_B/0/1/0/all/0/1&quot;&gt;Barbara Russo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04569">
<title>GResilience: Trading Off Between the Greenness and the Resilience of Collaborative AI Systems. (arXiv:2311.04569v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2311.04569</link>
<description rdf:parseType="Literal">&lt;p&gt;A Collaborative Artificial Intelligence System (CAIS) works with humans in a
shared environment to achieve a common goal. To recover from a disruptive event
that degrades its performance and ensures its resilience, a CAIS may then need
to perform a set of actions either by the system, by the humans, or
collaboratively together. As for any other system, recovery actions may cause
energy adverse effects due to the additional required energy. Therefore, it is
of paramount importance to understand which of the above actions can better
trade-off between resilience and greenness. In this in-progress work, we
propose an approach to automatically evaluate CAIS recovery actions for their
ability to trade-off between the resilience and greenness of the system. We
have also designed an experiment protocol and its application to a real CAIS
demonstrator. Our approach aims to attack the problem from two perspectives: as
a one-agent decision problem through optimization, which takes the decision
based on the score of resilience and greenness, and as a two-agent decision
problem through game theory, which takes the decision based on the payoff
computed for resilience and greenness as two players of a cooperative game.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rimawi_D/0/1/0/all/0/1&quot;&gt;Diaeddin Rimawi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liotta_A/0/1/0/all/0/1&quot;&gt;Antonio Liotta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Todescato_M/0/1/0/all/0/1&quot;&gt;Marco Todescato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russo_B/0/1/0/all/0/1&quot;&gt;Barbara Russo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04588">
<title>Army of Thieves: Enhancing Black-Box Model Extraction via Ensemble based sample selection. (arXiv:2311.04588v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.04588</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine Learning (ML) models become vulnerable to Model Stealing Attacks
(MSA) when they are deployed as a service. In such attacks, the deployed model
is queried repeatedly to build a labelled dataset. This dataset allows the
attacker to train a thief model that mimics the original model. To maximize
query efficiency, the attacker has to select the most informative subset of
data points from the pool of available data. Existing attack strategies utilize
approaches like Active Learning and Semi-Supervised learning to minimize costs.
However, in the black-box setting, these approaches may select sub-optimal
samples as they train only one thief model. Depending on the thief model&apos;s
capacity and the data it was pretrained on, the model might even select noisy
samples that harm the learning process. In this work, we explore the usage of
an ensemble of deep learning models as our thief model. We call our attack Army
of Thieves(AOT) as we train multiple models with varying complexities to
leverage the crowd&apos;s wisdom. Based on the ensemble&apos;s collective decision,
uncertain samples are selected for querying, while the most confident samples
are directly included in the training data. Our approach is the first one to
utilize an ensemble of thief models to perform model extraction. We outperform
the base approaches of existing state-of-the-art methods by at least 3% and
achieve a 21% higher adversarial sample transferability than previous work for
models trained on the CIFAR-10 dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jindal_A/0/1/0/all/0/1&quot;&gt;Akshit Jindal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_V/0/1/0/all/0/1&quot;&gt;Vikram Goyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anand_S/0/1/0/all/0/1&quot;&gt;Saket Anand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_C/0/1/0/all/0/1&quot;&gt;Chetan Arora&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04589">
<title>TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models. (arXiv:2311.04589v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04589</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite Multi-modal Large Language Models (MM-LLMs) have made exciting
strides recently, they are still struggling to efficiently model the
interactions among multi-modal inputs and the generation in non-textual
modalities. In this work, we propose TEAL (Tokenize and Embed ALl)}, an
approach to treat the input from any modality as a token sequence and learn a
joint embedding space for all modalities. Specifically, for the input from any
modality, TEAL first discretizes it into a token sequence with the
off-the-shelf tokenizer and embeds the token sequence into a joint embedding
space with a learnable embedding matrix. MM-LLMs just need to predict the
multi-modal tokens autoregressively as the textual LLMs do. Finally, the
corresponding de-tokenizer is applied to generate the output in each modality
based on the predicted token sequence. With the joint embedding space, TEAL
enables the frozen LLMs to perform both understanding and generation tasks
involving non-textual modalities, such as image and audio. Thus, the textual
LLM can just work as an interface and maintain its high performance in textual
understanding and generation. Experiments show that TEAL achieves substantial
improvements in multi-modal understanding, and implements a simple scheme for
multi-modal generations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yingxue Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1&quot;&gt;Fandong Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jie Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04614">
<title>LuminanceL1Loss: A loss function which measures percieved brightness and colour differences. (arXiv:2311.04614v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04614</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce LuminanceL1Loss, a novel loss function designed to enhance the
performance of image restoration tasks. We demonstrate its superiority over MSE
when applied to the Retinexformer, BUIFD and DnCNN architectures. Our proposed
LuminanceL1Loss leverages a unique approach by transforming images into
grayscale and subsequently computing the MSE loss for both grayscale and color
channels. Experimental results demonstrate that this innovative loss function
consistently outperforms traditional methods, showcasing its potential in image
denoising and other related tasks in image reconstruction. It demonstrates
gains up to 4.7dB. The results presented in this study highlight the efficacy
of LuminanceL1Loss for various image restoration tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jonge_D/0/1/0/all/0/1&quot;&gt;Dominic De Jonge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04640">
<title>Object-Centric Learning with Slot Mixture Module. (arXiv:2311.04640v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.04640</link>
<description rdf:parseType="Literal">&lt;p&gt;Object-centric architectures usually apply a differentiable module to the
entire feature map to decompose it into sets of entity representations called
slots. Some of these methods structurally resemble clustering algorithms, where
the cluster&apos;s center in latent space serves as a slot representation. Slot
Attention is an example of such a method, acting as a learnable analog of the
soft k-means algorithm. Our work employs a learnable clustering method based on
the Gaussian Mixture Model. Unlike other approaches, we represent slots not
only as centers of clusters but also incorporate information about the distance
between clusters and assigned vectors, leading to more expressive slot
representations. Our experiments demonstrate that using this approach instead
of Slot Attention improves performance in object-centric scenarios, achieving
state-of-the-art results in the set property prediction task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirilenko_D/0/1/0/all/0/1&quot;&gt;Daniil Kirilenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vorobyov_V/0/1/0/all/0/1&quot;&gt;Vitaliy Vorobyov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kovalev_A/0/1/0/all/0/1&quot;&gt;Alexey K. Kovalev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panov_A/0/1/0/all/0/1&quot;&gt;Aleksandr I. Panov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04645">
<title>SKU-Patch: Towards Efficient Instance Segmentation for Unseen Objects in Auto-Store. (arXiv:2311.04645v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04645</link>
<description rdf:parseType="Literal">&lt;p&gt;In large-scale storehouses, precise instance masks are crucial for robotic
bin picking but are challenging to obtain. Existing instance segmentation
methods typically rely on a tedious process of scene collection, mask
annotation, and network fine-tuning for every single Stock Keeping Unit (SKU).
This paper presents SKU-Patch, a new patch-guided instance segmentation
solution, leveraging only a few image patches for each incoming new SKU to
predict accurate and robust masks, without tedious manual effort and model
re-training. Technical-wise, we design a novel transformer-based network with
(i) a patch-image correlation encoder to capture multi-level image features
calibrated by patch information and (ii) a patch-aware transformer decoder with
parallel task heads to generate instance masks. Extensive experiments on four
storehouse benchmarks manifest that SKU-Patch is able to achieve the best
performance over the state-of-the-art methods. Also, SKU-Patch yields an
average of nearly 100% grasping success rate on more than 50 unseen SKUs in a
robot-aided auto-store logistic pipeline, showing its effectiveness and
practicality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Biqi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1&quot;&gt;Weiliang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xiaojie Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xianzhi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yun-Hui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1&quot;&gt;Chi-Wing Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1&quot;&gt;Pheng-Ann Heng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04653">
<title>Hybrid Focal and Full-Range Attention Based Graph Transformers. (arXiv:2311.04653v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.04653</link>
<description rdf:parseType="Literal">&lt;p&gt;The paradigm of Transformers using the self-attention mechanism has
manifested its advantage in learning graph-structured data. Yet, Graph
Transformers are capable of modeling full range dependencies but are often
deficient in extracting information from locality. A common practice is to
utilize Message Passing Neural Networks (MPNNs) as an auxiliary to capture
local information, which however are still inadequate for comprehending
substructures. In this paper, we present a purely attention-based architecture,
namely Focal and Full-Range Graph Transformer (FFGT), which can mitigate the
loss of local information in learning global correlations. The core component
of FFGT is a new mechanism of compound attention, which combines the
conventional full-range attention with K-hop focal attention on ego-nets to
aggregate both global and local information. Beyond the scope of canonical
Transformers, the FFGT has the merit of being more substructure-aware. Our
approach enhances the performance of existing Graph Transformers on various
open datasets, while achieves compatible SOTA performance on several Long-Range
Graph Benchmark (LRGB) datasets even with a vanilla transformer. We further
examine influential factors on the optimal focal length of attention via
introducing a novel synthetic dataset based on SBM-PATTERN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1&quot;&gt;Minhong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhenhao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1&quot;&gt;Weiran Cai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04659">
<title>Pragmatic Reasoning Unlocks Quantifier Semantics for Foundation Models. (arXiv:2311.04659v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.04659</link>
<description rdf:parseType="Literal">&lt;p&gt;Generalized quantifiers (e.g., few, most) are used to indicate the
proportions predicates are satisfied (for example, some apples are red). One
way to interpret quantifier semantics is to explicitly bind these satisfactions
with percentage scopes (e.g., 30%-40% of apples are red). This approach can be
helpful for tasks like logic formalization and surface-form quantitative
reasoning (Gordon and Schubert, 2010; Roy et al., 2015). However, it remains
unclear if recent foundation models possess this ability, as they lack direct
training signals. To explore this, we introduce QuRe, a crowd-sourced dataset
of human-annotated generalized quantifiers in Wikipedia sentences featuring
percentage-equipped predicates. We explore quantifier comprehension in language
models using PRESQUE, a framework that combines natural language inference and
the Rational Speech Acts framework. Experimental results on the HVD dataset and
QuRe illustrate that PRESQUE, employing pragmatic reasoning, performs 20%
better than a literal reasoning baseline when predicting quantifier percentage
scopes, with no additional training required.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yiyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menon_R/0/1/0/all/0/1&quot;&gt;Rakesh R. Menon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Sayan Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1&quot;&gt;Shashank Srivastava&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04666">
<title>Pre-training LLMs using human-like development data corpus. (arXiv:2311.04666v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04666</link>
<description rdf:parseType="Literal">&lt;p&gt;Pre-trained Large Language Models (LLMs) have shown success in a diverse set
of language inference and understanding tasks. The pre-training stage of LLMs
looks at a large corpus of raw textual data. The BabyLM shared task compares
LLM pre-training to human language acquisition, where the number of tokens seen
by 13-year-old kids is magnitudes smaller than the number of tokens seen by
LLMs. In this work, we pre-train and evaluate LLMs on their ability to learn
contextual word representations using roughly the same number of tokens as seen
by children. We provide a strong set of baselines; with different
architectures, evaluation of changes in performance across epochs, and reported
pre-training metrics for the strict small and strict tracks of the task. We
also try to loosely replicate the RoBERTa baseline given by the task organizers
to observe the training robustness to hyperparameter selection and
replicability. We provide the submission details to the strict and strict-small
tracks in this report.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhardwaj_K/0/1/0/all/0/1&quot;&gt;Khushi Bhardwaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1&quot;&gt;Raj Sanjay Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varma_S/0/1/0/all/0/1&quot;&gt;Sashank Varma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04693">
<title>Diff-HierVC: Diffusion-based Hierarchical Voice Conversion with Robust Pitch Generation and Masked Prior for Zero-shot Speaker Adaptation. (arXiv:2311.04693v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2311.04693</link>
<description rdf:parseType="Literal">&lt;p&gt;Although voice conversion (VC) systems have shown a remarkable ability to
transfer voice style, existing methods still have an inaccurate pitch and low
speaker adaptation quality. To address these challenges, we introduce
Diff-HierVC, a hierarchical VC system based on two diffusion models. We first
introduce DiffPitch, which can effectively generate F0 with the target voice
style. Subsequently, the generated F0 is fed to DiffVoice to convert the speech
with a target voice style. Furthermore, using the source-filter encoder, we
disentangle the speech and use the converted Mel-spectrogram as a data-driven
prior in DiffVoice to improve the voice style transfer capacity. Finally, by
using the masked prior in diffusion models, our model can improve the speaker
adaptation quality. Experimental results verify the superiority of our model in
pitch generation and voice style transfer performance, and our model also
achieves a CER of 0.83% and EER of 3.29% in zero-shot VC scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Choi_H/0/1/0/all/0/1&quot;&gt;Ha-Yeong Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sang-Hoon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seong-Whan Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04698">
<title>Challenging Common Assumptions in Multi-task Learning. (arXiv:2311.04698v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.04698</link>
<description rdf:parseType="Literal">&lt;p&gt;While multi-task learning (MTL) has gained significant attention in recent
years, its underlying mechanisms remain poorly understood. Recent methods did
not yield consistent performance improvements over single task learning (STL)
baselines, underscoring the importance of gaining more profound insights about
challenges specific to MTL. In our study, we challenge common assumptions in
MTL in the context of STL: First, the choice of optimizer has only been mildly
investigated in MTL. We show the pivotal role of common STL tools such as the
Adam optimizer in MTL. We deduce the effectiveness of Adam to its partial
loss-scale invariance. Second, the notion of gradient conflicts has often been
phrased as a specific problem in MTL. We delve into the role of gradient
conflicts in MTL and compare it to STL. For angular gradient alignment we find
no evidence that this is a unique problem in MTL. We emphasize differences in
gradient magnitude as the main distinguishing factor. Lastly, we compare the
transferability of features learned through MTL and STL on common image
corruptions, and find no conclusive evidence that MTL leads to superior
transferability. Overall, we find surprising similarities between STL and MTL
suggesting to consider methods from both fields in a broader context.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elich_C/0/1/0/all/0/1&quot;&gt;Cathrin Elich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirchdorfer_L/0/1/0/all/0/1&quot;&gt;Lukas Kirchdorfer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kohler_J/0/1/0/all/0/1&quot;&gt;Jan M. K&amp;#xf6;hler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schott_L/0/1/0/all/0/1&quot;&gt;Lukas Schott&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04710">
<title>The Quest for Content: A Survey of Search-Based Procedural Content Generation for Video Games. (arXiv:2311.04710v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2311.04710</link>
<description rdf:parseType="Literal">&lt;p&gt;Video games demand is constantly increasing, which requires the costly
production of large amounts of content. Towards this challenge, researchers
have developed Search-Based Procedural Content Generation (SBPCG), that is, the
(semi-)automated creation of content through search algorithms. We survey the
current state of SBPCG, reporting work appeared in the field between 2011-2022
and identifying open research challenges. The results lead to recommendations
for practitioners and to the identification of several potential future
research avenues for SBPCG.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zamorano_M/0/1/0/all/0/1&quot;&gt;Mar Zamorano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cetina_C/0/1/0/all/0/1&quot;&gt;Carlos Cetina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarro_F/0/1/0/all/0/1&quot;&gt;Federica Sarro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04744">
<title>Euclidean, Projective, Conformal: Choosing a Geometric Algebra for Equivariant Transformers. (arXiv:2311.04744v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.04744</link>
<description rdf:parseType="Literal">&lt;p&gt;The Geometric Algebra Transformer (GATr) is a versatile architecture for
geometric deep learning based on projective geometric algebra. We generalize
this architecture into a blueprint that allows one to construct a scalable
transformer architecture given any geometric (or Clifford) algebra. We study
versions of this architecture for Euclidean, projective, and conformal
algebras, all of which are suited to represent 3D data, and evaluate them in
theory and practice. The simplest Euclidean architecture is computationally
cheap, but has a smaller symmetry group and is not as sample-efficient, while
the projective model is not sufficiently expressive. Both the conformal algebra
and an improved version of the projective algebra define powerful, performant
architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haan_P/0/1/0/all/0/1&quot;&gt;Pim de Haan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_T/0/1/0/all/0/1&quot;&gt;Taco Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brehmer_J/0/1/0/all/0/1&quot;&gt;Johann Brehmer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04765">
<title>The voraus-AD Dataset for Anomaly Detection in Robot Applications. (arXiv:2311.04765v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.04765</link>
<description rdf:parseType="Literal">&lt;p&gt;During the operation of industrial robots, unusual events may endanger the
safety of humans and the quality of production. When collecting data to detect
such cases, it is not ensured that data from all potentially occurring errors
is included as unforeseeable events may happen over time. Therefore, anomaly
detection (AD) delivers a practical solution, using only normal data to learn
to detect unusual events. We introduce a dataset that allows training and
benchmarking of anomaly detection methods for robotic applications based on
machine data which will be made publicly available to the research community.
As a typical robot task the dataset includes a pick-and-place application which
involves movement, actions of the end effector and interactions with the
objects of the environment. Since several of the contained anomalies are not
task-specific but general, evaluations on our dataset are transferable to other
robotics applications as well. Additionally, we present MVT-Flow (multivariate
time-series flow) as a new baseline method for anomaly detection: It relies on
deep-learning-based density estimation with normalizing flows, tailored to the
data domain by taking its structure into account for the architecture. Our
evaluation shows that MVT-Flow outperforms baselines from previous work by a
large margin of 6.2% in area under ROC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brockmann_J/0/1/0/all/0/1&quot;&gt;Jan Thie&amp;#xdf; Brockmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rudolph_M/0/1/0/all/0/1&quot;&gt;Marco Rudolph&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1&quot;&gt;Bodo Rosenhahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wandt_B/0/1/0/all/0/1&quot;&gt;Bastian Wandt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04770">
<title>Vital Sign Forecasting for Sepsis Patients in ICUs. (arXiv:2311.04770v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.04770</link>
<description rdf:parseType="Literal">&lt;p&gt;Sepsis and septic shock are a critical medical condition affecting millions
globally, with a substantial mortality rate. This paper uses state-of-the-art
deep learning (DL) architectures to introduce a multi-step forecasting system
to predict vital signs indicative of septic shock progression in Intensive Care
Units (ICUs). Our approach utilizes a short window of historical vital sign
data to forecast future physiological conditions. We introduce a DL-based vital
sign forecasting system that predicts up to 3 hours of future vital signs from
6 hours of past data. We further adopt the DILATE loss function to capture
better the shape and temporal dynamics of vital signs, which are critical for
clinical decision-making. We compare three DL models, N-BEATS, N-HiTS, and
Temporal Fusion Transformer (TFT), using the publicly available eICU
Collaborative Research Database (eICU-CRD), highlighting their forecasting
capabilities in a critical care setting. We evaluate the performance of our
models using mean squared error (MSE) and dynamic time warping (DTW) metrics.
Our findings show that while TFT excels in capturing overall trends, N-HiTS is
superior in retaining short-term fluctuations within a predefined range. This
paper demonstrates the potential of deep learning in transforming the
monitoring systems in ICUs, potentially leading to significant improvements in
patient care and outcomes by accurately forecasting vital signs to assist
healthcare providers in detecting early signs of physiological instability and
anticipating septic shock.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatti_A/0/1/0/all/0/1&quot;&gt;Anubhav Bhatti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dan_C/0/1/0/all/0/1&quot;&gt;Chen Dan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1&quot;&gt;Bingjie Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;San Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yonghwan Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jang Yong Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04778">
<title>On the Multiple Roles of Ontologies in Explainable AI. (arXiv:2311.04778v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.04778</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper discusses the different roles that explicit knowledge, in
particular ontologies, can play in Explainable AI and in the development of
human-centric explainable systems and intelligible explanations. We consider
three main perspectives in which ontologies can contribute significantly,
namely reference modelling, common-sense reasoning, and knowledge refinement
and complexity management. We overview some of the existing approaches in the
literature, and we position them according to these three proposed
perspectives. The paper concludes by discussing what challenges still need to
be addressed to enable ontology-based approaches to explanation and to evaluate
their human-understandability and effectiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Confalonieri_R/0/1/0/all/0/1&quot;&gt;Roberto Confalonieri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guizzardi_G/0/1/0/all/0/1&quot;&gt;Giancarlo Guizzardi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04799">
<title>DACBERT: Leveraging Dependency Agreement for Cost-Efficient Bert Pretraining. (arXiv:2311.04799v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04799</link>
<description rdf:parseType="Literal">&lt;p&gt;Building on the cost-efficient pretraining advancements brought about by
Crammed BERT, we enhance its performance and interpretability further by
introducing a novel pretrained model Dependency Agreement Crammed BERT
(DACBERT) and its two-stage pretraining framework - Dependency Agreement
Pretraining. This framework, grounded by linguistic theories, seamlessly weaves
syntax and semantic information into the pretraining process. The first stage
employs four dedicated submodels to capture representative dependency
agreements at the chunk level, effectively converting these agreements into
embeddings. The second stage uses these refined embeddings, in tandem with
conventional BERT embeddings, to guide the pretraining of the rest of the
model. Evaluated on the GLUE benchmark, our DACBERT demonstrates notable
improvement across various tasks, surpassing Crammed BERT by 3.13% in the RTE
task and by 2.26% in the MRPC task. Furthermore, our method boosts the average
GLUE score by 0.83%, underscoring its significant potential. The pretraining
process can be efficiently executed on a single GPU within a 24-hour cycle,
necessitating no supplementary computational resources or extending the
pretraining duration compared with the Crammed BERT. Extensive studies further
illuminate our approach&apos;s instrumental role in bolstering the interpretability
of pretrained language models for natural language understanding tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuo_M/0/1/0/all/0/1&quot;&gt;Martin Kuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yiran Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04816">
<title>MTGER: Multi-view Temporal Graph Enhanced Temporal Reasoning over Time-Involved Document. (arXiv:2311.04816v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04816</link>
<description rdf:parseType="Literal">&lt;p&gt;The facts and time in the document are intricately intertwined, making
temporal reasoning over documents challenging. Previous work models time
implicitly, making it difficult to handle such complex relationships. To
address this issue, we propose MTGER, a novel Multi-view Temporal Graph
Enhanced Temporal Reasoning framework for temporal reasoning over time-involved
documents. Concretely, MTGER explicitly models the temporal relationships among
facts by multi-view temporal graphs. On the one hand, the heterogeneous
temporal graphs explicitly model the temporal and discourse relationships among
facts; on the other hand, the multi-view mechanism captures both time-focused
and fact-focused information, allowing the two views to complement each other
through adaptive fusion. To further improve the implicit reasoning capability
of the model, we design a self-supervised time-comparing objective. Extensive
experimental results demonstrate the effectiveness of our method on the TimeQA
and SituatedQA datasets. Furthermore, MTGER gives more consistent answers under
question perturbations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1&quot;&gt;Zheng Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zekun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Jiafeng Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Ming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1&quot;&gt;Bing Qin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04817">
<title>Decentralized Personalized Online Federated Learning. (arXiv:2311.04817v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.04817</link>
<description rdf:parseType="Literal">&lt;p&gt;Vanilla federated learning does not support learning in an online
environment, learning a personalized model on each client, and learning in a
decentralized setting. There are existing methods extending federated learning
in each of the three aspects. However, some important applications on
enterprise edge servers (e.g. online item recommendation at global scale)
involve the three aspects at the same time. Therefore, we propose a new
learning setting \textit{Decentralized Personalized Online Federated Learning}
that considers all the three aspects at the same time.
&lt;/p&gt;
&lt;p&gt;In this new setting for learning, the first technical challenge is how to
aggregate the shared model parameters from neighboring clients to obtain a
personalized local model with good performance on each client. We propose to
directly learn an aggregation by optimizing the performance of the local model
with respect to the aggregation weights. This not only improves personalization
of each local model but also helps the local model adapting to potential data
shift by intelligently incorporating the right amount of information from its
neighbors. The second challenge is how to select the neighbors for each client.
We propose a peer selection method based on the learned aggregation weights
enabling each client to select the most helpful neighbors and reduce
communication cost at the same time. We verify the effectiveness and robustness
of our proposed method on three real-world item recommendation datasets and one
air quality prediction dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1&quot;&gt;Renzhi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitra_S/0/1/0/all/0/1&quot;&gt;Saayan Mitra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1&quot;&gt;Anup Rao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04837">
<title>Identifying Semantic Component for Robust Molecular Property Prediction. (arXiv:2311.04837v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.04837</link>
<description rdf:parseType="Literal">&lt;p&gt;Although graph neural networks have achieved great success in the task of
molecular property prediction in recent years, their generalization ability
under out-of-distribution (OOD) settings is still under-explored. Different
from existing methods that learn discriminative representations for prediction,
we propose a generative model with semantic-components identifiability, named
SCI. We demonstrate that the latent variables in this generative model can be
explicitly identified into semantic-relevant (SR) and semantic-irrelevant (SI)
components, which contributes to better OOD generalization by involving minimal
change properties of causal mechanisms. Specifically, we first formulate the
data generation process from the atom level to the molecular level, where the
latent space is split into SI substructures, SR substructures, and SR atom
variables. Sequentially, to reduce misidentification, we restrict the minimal
changes of the SR atom variables and add a semantic latent substructure
regularization to mitigate the variance of the SR substructure under augmented
domain changes. Under mild assumptions, we prove the block-wise identifiability
of the SR substructure and the comment-wise identifiability of SR atom
variables. Experimental studies achieve state-of-the-art performance and show
general improvement on 21 datasets in 3 mainstream benchmarks. Moreover, the
visualization results of the proposed SCI method provide insightful case
studies and explanations for the prediction results. The code is available at:
https://github.com/DMIRLAB-Group/SCI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zijian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zunhong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1&quot;&gt;Ruichu Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhenhui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yuguang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_Z/0/1/0/all/0/1&quot;&gt;Zhifeng Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guangyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04850">
<title>Rethinking Benchmark and Contamination for Language Models with Rephrased Samples. (arXiv:2311.04850v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04850</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models are increasingly trained on all the data ever produced
by humans. Many have raised concerns about the trustworthiness of public
benchmarks due to potential contamination in pre-training or fine-tuning
datasets. While most data decontamination efforts apply string matching (e.g.,
n-gram overlap) to remove benchmark data, we show that these methods are
insufficient, and simple variations of test data (e.g., paraphrasing,
translation) can easily bypass these decontamination measures. Furthermore, we
demonstrate that if such variation of test data is not eliminated, a 13B model
can easily overfit a test benchmark and achieve drastically high performance,
on par with GPT-4. We validate such observations in widely used benchmarks such
as MMLU, GSK8k, and HumanEval. To address this growing risk, we propose a
stronger LLM-based decontamination method and apply it to widely used
pre-training and fine-tuning datasets, revealing significant previously unknown
test overlap. For example, in pre-training sets such as RedPajama-Data-1T and
StarCoder-Data, we identified that 8-18\% of the HumanEval benchmark overlaps.
Interestingly, we also find such contamination in synthetic dataset generated
by GPT-3.5/4, suggesting a potential risk of unintentional contamination. We
urge the community to adopt stronger decontamination approaches when using
public benchmarks. Moreover, we call for the community to actively develop
fresh one-time exams to evaluate models accurately. Our decontamination tool is
publicly available at https://github.com/lm-sys/llm-decontaminator.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shuo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiang_W/0/1/0/all/0/1&quot;&gt;Wei-Lin Chiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1&quot;&gt;Lianmin Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1&quot;&gt;Joseph E. Gonzalez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1&quot;&gt;Ion Stoica&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04879">
<title>LongQLoRA: Efficient and Effective Method to Extend Context Length of Large Language Models. (arXiv:2311.04879v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04879</link>
<description rdf:parseType="Literal">&lt;p&gt;We present LongQLoRA, an efficient and effective method to extend context
length of large language models with less training resources. LongQLoRA
combines the advantages of Position Interpolation, QLoRA and Shift Short
Attention of LongLoRA. With a single 32GB V100 GPU, LongQLoRA can extend the
context length of LLaMA2 7B and 13B from 4096 to 8192 and even to 12k within
1000 finetuning steps. LongQLoRA achieves competitive perplexity performance on
PG19 and Proof-pile datasets, our model outperforms LongLoRA and is very close
to MPT-7B-8K within the evaluation context length of 8192. We collect and build
39k long instruction data to extend context length of Vicuna-13B from 4096 to
8192 and achieve good performance both in long and short context generation
task. We also do some ablation experiments to study the effect of LoRA rank,
finetuning steps and attention patterns in inference.The model weights,
training data and code are avaliable at
https://github.com/yangjianxin1/LongQLoRA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jianxin Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04886">
<title>SEMQA: Semi-Extractive Multi-Source Question Answering. (arXiv:2311.04886v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04886</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently proposed long-form question answering (QA) systems, supported by
large language models (LLMs), have shown promising capabilities. Yet,
attributing and verifying their generated abstractive answers can be difficult,
and automatically evaluating their accuracy remains an ongoing challenge.
&lt;/p&gt;
&lt;p&gt;In this work, we introduce a new QA task for answering multi-answer questions
by summarizing multiple diverse sources in a semi-extractive fashion.
Specifically, Semi-extractive Multi-source QA (SEMQA) requires models to output
a comprehensive answer, while mixing factual quoted spans -- copied verbatim
from given input sources -- and non-factual free-text connectors that glue
these spans together into a single cohesive passage. This setting bridges the
gap between the outputs of well-grounded but constrained extractive QA systems
and more fluent but harder to attribute fully abstractive answers.
Particularly, it enables a new mode for language models that leverages their
advanced language generation capabilities, while also producing fine in-line
attributions by-design that are easy to verify, interpret, and evaluate.
&lt;/p&gt;
&lt;p&gt;To study this task, we create the first dataset of this kind, QuoteSum, with
human-written semi-extractive answers to natural and generated questions, and
define text-based evaluation metrics. Experimenting with several LLMs in
various settings, we find this task to be surprisingly challenging,
demonstrating the importance of QuoteSum for developing and studying such
consolidation capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuster_T/0/1/0/all/0/1&quot;&gt;Tal Schuster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lelkes_A/0/1/0/all/0/1&quot;&gt;Adam D. Lelkes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Haitian Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1&quot;&gt;Jai Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1&quot;&gt;Jonathan Berant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1&quot;&gt;William W. Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1&quot;&gt;Donald Metzler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04888">
<title>Towards Few-Annotation Learning in Computer Vision: Application to Image Classification and Object Detection tasks. (arXiv:2311.04888v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04888</link>
<description rdf:parseType="Literal">&lt;p&gt;In this thesis, we develop theoretical, algorithmic and experimental
contributions for Machine Learning with limited labels, and more specifically
for the tasks of Image Classification and Object Detection in Computer Vision.
In a first contribution, we are interested in bridging the gap between theory
and practice for popular Meta-Learning algorithms used in Few-Shot
Classification. We make connections to Multi-Task Representation Learning,
which benefits from solid theoretical foundations, to verify the best
conditions for a more efficient meta-learning. Then, to leverage unlabeled data
when training object detectors based on the Transformer architecture, we
propose both an unsupervised pretraining and a semi-supervised learning method
in two other separate contributions. For pretraining, we improve Contrastive
Learning for object detectors by introducing the localization information.
Finally, our semi-supervised method is the first tailored to transformer-based
detectors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouniot_Q/0/1/0/all/0/1&quot;&gt;Quentin Bouniot&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04894">
<title>DAMEX: Dataset-aware Mixture-of-Experts for visual understanding of mixture-of-datasets. (arXiv:2311.04894v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04894</link>
<description rdf:parseType="Literal">&lt;p&gt;Construction of a universal detector poses a crucial question: How can we
most effectively train a model on a large mixture of datasets? The answer lies
in learning dataset-specific features and ensembling their knowledge but do all
this in a single model. Previous methods achieve this by having separate
detection heads on a common backbone but that results in a significant increase
in parameters. In this work, we present Mixture-of-Experts as a solution,
highlighting that MoEs are much more than a scalability tool. We propose
Dataset-Aware Mixture-of-Experts, DAMEX where we train the experts to become an
`expert&apos; of a dataset by learning to route each dataset tokens to its mapped
expert. Experiments on Universal Object-Detection Benchmark show that we
outperform the existing state-of-the-art by average +10.2 AP score and improve
over our non-MoE baseline by average +2.0 AP score. We also observe consistent
gains while mixing datasets with (1) limited availability, (2) disparate
domains and (3) divergent label sets. Further, we qualitatively show that DAMEX
is robust against expert representation collapse.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_Y/0/1/0/all/0/1&quot;&gt;Yash Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Behl_H/0/1/0/all/0/1&quot;&gt;Harkirat Behl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1&quot;&gt;Zsolt Kira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vineet_V/0/1/0/all/0/1&quot;&gt;Vibhav Vineet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04898">
<title>Two Complementary Perspectives to Continual Learning: Ask Not Only What to Optimize, But Also How. (arXiv:2311.04898v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.04898</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years have seen considerable progress in the continual training of
deep neural networks, predominantly thanks to approaches that add replay or
regularization terms to the loss function to approximate the joint loss over
all tasks so far. However, we show that even with a perfect approximation to
the joint loss, these approaches still suffer from temporary but substantial
forgetting when starting to train on a new task. Motivated by this &apos;stability
gap&apos;, we propose that continual learning strategies should focus not only on
the optimization objective, but also on the way this objective is optimized.
While there is some continual learning work that alters the optimization
trajectory (e.g., using gradient projection techniques), this line of research
is positioned as alternative to improving the optimization objective, while we
argue it should be complementary. To evaluate the merits of our proposition, we
plan to combine replay-approximated joint objectives with gradient
projection-based optimization routines to test whether the addition of the
latter provides benefits in terms of (1) alleviating the stability gap, (2)
increasing the learning efficiency and (3) improving the final learning
outcome.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hess_T/0/1/0/all/0/1&quot;&gt;Timm Hess&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1&quot;&gt;Tinne Tuytelaars&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ven_G/0/1/0/all/0/1&quot;&gt;Gido M. van de Ven&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04902">
<title>Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models. (arXiv:2311.04902v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04902</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) with a billion or more parameters are prime
targets for network pruning, which aims to reduce a portion of the network
weights without compromising performance. Prior approaches such as Weights
Magnitude, SparseGPT, and Wanda, either concentrated solely on weights or
integrated weights with activations for sparsity. However, they overlooked the
informative gradients derived from pretrained large language models. In this
paper, we present a novel sparsity-centric pruning method for pretrained LLMs,
termed Gradient-based Language Model Pruner (GBLM-Pruner). GBLM-Pruner
leverages the first-order term of the Taylor expansion, operating in a
training-free manner by harnessing properly normalized gradients from a few
calibration samples to determine the importance pruning score, and
substantially outperforms competitive counterparts like SparseGPT and Wanda in
multiple benchmarks. Intriguing, after incorporating gradients, the
unstructured pruning method tends to reveal some structural patterns
post-pruning, which mirrors the geometric interdependence inherent in the LLMs&apos;
parameter structure. Additionally, GBLM-Pruner functions without any subsequent
retraining or weight updates to maintain its simplicity as other counterparts.
Extensive evaluations on LLaMA-1 and LLaMA-2 across various language benchmarks
and perplexity show that GBLM-Pruner surpasses magnitude pruning, Wanda
(weights+activations) and SparseGPT (weights+activations+weight update) by
significant margins. Our code and models are available at
https://github.com/RocktimJyotiDas/GBLM-Pruner.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1&quot;&gt;Rocktim Jyoti Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Liqun Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2104.09864">
<title>RoFormer: Enhanced Transformer with Rotary Position Embedding. (arXiv:2104.09864v5 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2104.09864</link>
<description rdf:parseType="Literal">&lt;p&gt;Position encoding recently has shown effective in the transformer
architecture. It enables valuable supervision for dependency modeling between
elements at different positions of the sequence. In this paper, we first
investigate various methods to integrate positional information into the
learning process of transformer-based language models. Then, we propose a novel
method named Rotary Position Embedding(RoPE) to effectively leverage the
positional information. Specifically, the proposed RoPE encodes the absolute
position with a rotation matrix and meanwhile incorporates the explicit
relative position dependency in self-attention formulation. Notably, RoPE
enables valuable properties, including the flexibility of sequence length,
decaying inter-token dependency with increasing relative distances, and the
capability of equipping the linear self-attention with relative position
encoding. Finally, we evaluate the enhanced transformer with rotary position
embedding, also called RoFormer, on various long text classification benchmark
datasets. Our experiments show that it consistently overcomes its alternatives.
Furthermore, we provide a theoretical analysis to explain some experimental
results. RoFormer is already integrated into Huggingface:
\url{https://huggingface.co/docs/transformers/model_doc/roformer}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1&quot;&gt;Jianlin Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yu Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1&quot;&gt;Shengfeng Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murtadha_A/0/1/0/all/0/1&quot;&gt;Ahmed Murtadha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1&quot;&gt;Bo Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yunfeng Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.00907">
<title>Split Semantic Detection in Sandplay Images. (arXiv:2203.00907v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.00907</link>
<description rdf:parseType="Literal">&lt;p&gt;Sandplay image, as an important psychoanalysis carrier, is a visual scene
constructed by the client selecting and placing sand objects (e.g., sand,
river, human figures, animals, vegetation, buildings, etc.). As the projection
of the client&apos;s inner world, it contains high-level semantic information
reflecting the client&apos;s subjective psychological states, which is different
from the common natural image scene that only contains the objective basic
semantics (e.g., object&apos;s name, attribute, bounding box, etc.). In this work,
we take &quot;split&quot; which is a typical psychological semantics related to many
emotional and personality problems as the research goal, and we propose an
automatic detection model, which can replace the time-consuming and expensive
manual analysis process. To achieve that, we design a distribution map
generation method projecting the semantic judgment problem into a visual
problem, and a feature dimensionality reduction and extraction algorithm which
can provide a good representation of split semantics. Besides, we built a
sandplay datasets by collecting one sample from each client and inviting 5
therapists to label each sample, which has a large data cost. Experimental
results demonstrated the effectiveness of our proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1&quot;&gt;Xiaokun Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaotang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1&quot;&gt;Jian Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kaiqi Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.03860">
<title>CCMB: A Large-scale Chinese Cross-modal Benchmark. (arXiv:2205.03860v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2205.03860</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision-language pre-training (VLP) on large-scale datasets has shown premier
performance on various downstream tasks. In contrast to plenty of available
benchmarks with English corpus, large-scale pre-training datasets and
downstream datasets with Chinese corpus remain largely unexplored. In this
work, we build a large-scale high-quality Chinese Cross-Modal Benchmark named
CCMB for the research community, which contains the currently largest public
pre-training dataset Zero and five human-annotated fine-tuning datasets for
downstream tasks. Zero contains 250 million images paired with 750 million text
descriptions, plus two of the five fine-tuning datasets are also currently the
largest ones for Chinese cross-modal downstream tasks. Along with the CCMB, we
also develop a VLP framework named R2D2, applying a pre-Ranking + Ranking
strategy to learn powerful vision-language representations and a two-way
distillation method (i.e., target-guided Distillation and feature-guided
Distillation) to further enhance the learning capability. With the Zero and the
R2D2 VLP framework, we achieve state-of-the-art performance on twelve
downstream datasets from five broad categories of tasks including image-text
retrieval, image-text matching, image caption, text-to-image generation, and
zero-shot image classification. The datasets, models, and codes are available
at https://github.com/yuxie11/R2D2
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1&quot;&gt;Chunyu Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1&quot;&gt;Heng Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jincheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1&quot;&gt;Fanjing Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jianfei Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morimitsu_H/0/1/0/all/0/1&quot;&gt;Henrique Morimitsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1&quot;&gt;Lin Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dexin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiangzheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leng_D/0/1/0/all/0/1&quot;&gt;Dawei Leng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Baochang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1&quot;&gt;Xiangyang Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yafeng Deng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.07074">
<title>Sound and Relatively Complete Belief Hoare Logic for Statistical Hypothesis Testing Programs. (arXiv:2208.07074v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2208.07074</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new approach to formally describing the requirement for
statistical inference and checking whether a program uses the statistical
method appropriately. Specifically, we define belief Hoare logic (BHL) for
formalizing and reasoning about the statistical beliefs acquired via hypothesis
testing. This program logic is sound and relatively complete with respect to a
Kripke model for hypothesis tests. We demonstrate by examples that BHL is
useful for reasoning about practical issues in hypothesis testing. In our
framework, we clarify the importance of prior beliefs in acquiring statistical
beliefs through hypothesis testing, and discuss the whole picture of the
justification of statistical inference inside and outside the program logic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawamoto_Y/0/1/0/all/0/1&quot;&gt;Yusuke Kawamoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sato_T/0/1/0/all/0/1&quot;&gt;Tetsuya Sato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suenaga_K/0/1/0/all/0/1&quot;&gt;Kohei Suenaga&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.08349">
<title>When to Update Your Model: Constrained Model-based Reinforcement Learning. (arXiv:2210.08349v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.08349</link>
<description rdf:parseType="Literal">&lt;p&gt;Designing and analyzing model-based RL (MBRL) algorithms with guaranteed
monotonic improvement has been challenging, mainly due to the interdependence
between policy optimization and model learning. Existing discrepancy bounds
generally ignore the impacts of model shifts, and their corresponding
algorithms are prone to degrade performance by drastic model updating. In this
work, we first propose a novel and general theoretical scheme for a
non-decreasing performance guarantee of MBRL. Our follow-up derived bounds
reveal the relationship between model shifts and performance improvement. These
discoveries encourage us to formulate a constrained lower-bound optimization
problem to permit the monotonicity of MBRL. A further example demonstrates that
learning models from a dynamically-varying number of explorations benefit the
eventual returns. Motivated by these analyses, we design a simple but effective
algorithm CMLO (Constrained Model-shift Lower-bound Optimization), by
introducing an event-triggered mechanism that flexibly determines when to
update the model. Experiments show that CMLO surpasses other state-of-the-art
methods and produces a boost when various policy optimization methods are
employed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_T/0/1/0/all/0/1&quot;&gt;Tianying Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yu Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1&quot;&gt;Fuchun Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_M/0/1/0/all/0/1&quot;&gt;Mingxuan Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1&quot;&gt;Fengxiang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wenbing Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.05981">
<title>MarioGPT: Open-Ended Text2Level Generation through Large Language Models. (arXiv:2302.05981v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2302.05981</link>
<description rdf:parseType="Literal">&lt;p&gt;Procedural Content Generation (PCG) is a technique to generate complex and
diverse environments in an automated way. However, while generating content
with PCG methods is often straightforward, generating meaningful content that
reflects specific intentions and constraints remains challenging. Furthermore,
many PCG algorithms lack the ability to generate content in an open-ended
manner. Recently, Large Language Models (LLMs) have shown to be incredibly
effective in many diverse domains. These trained LLMs can be fine-tuned,
re-using information and accelerating training for new tasks. Here, we
introduce MarioGPT, a fine-tuned GPT2 model trained to generate tile-based game
levels, in our case Super Mario Bros levels. MarioGPT can not only generate
diverse levels, but can be text-prompted for controllable level generation,
addressing one of the key challenges of current PCG techniques. As far as we
know, MarioGPT is the first text-to-level model and combined with novelty
search it enables the generation of diverse levels with varying play-style
dynamics (i.e. player paths) and the open-ended discovery of an increasingly
diverse range of content. Code available at
https://github.com/shyamsn97/mario-gpt.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sudhakaran_S/0/1/0/all/0/1&quot;&gt;Shyam Sudhakaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_Duque_M/0/1/0/all/0/1&quot;&gt;Miguel Gonz&amp;#xe1;lez-Duque&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glanois_C/0/1/0/all/0/1&quot;&gt;Claire Glanois&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freiberger_M/0/1/0/all/0/1&quot;&gt;Matthias Freiberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Najarro_E/0/1/0/all/0/1&quot;&gt;Elias Najarro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Risi_S/0/1/0/all/0/1&quot;&gt;Sebastian Risi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.07849">
<title>Zero-Shot Anomaly Detection via Batch Normalization. (arXiv:2302.07849v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.07849</link>
<description rdf:parseType="Literal">&lt;p&gt;Anomaly detection (AD) plays a crucial role in many safety-critical
application domains. The challenge of adapting an anomaly detector to drift in
the normal data distribution, especially when no training data is available for
the &quot;new normal,&quot; has led to the development of zero-shot AD techniques. In
this paper, we propose a simple yet effective method called Adaptive Centered
Representations (ACR) for zero-shot batch-level AD. Our approach trains
off-the-shelf deep anomaly detectors (such as deep SVDD) to adapt to a set of
inter-related training data distributions in combination with batch
normalization, enabling automatic zero-shot generalization for unseen AD tasks.
This simple recipe, batch normalization plus meta-training, is a highly
effective and versatile tool. Our theoretical results guarantee the zero-shot
generalization for unseen AD tasks; our empirical results demonstrate the first
zero-shot AD results for tabular data and outperform existing methods in
zero-shot anomaly detection and segmentation on image data from specialized
domains. Code is at https://github.com/aodongli/zero-shot-ad-via-batch-norm
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1&quot;&gt;Aodong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_C/0/1/0/all/0/1&quot;&gt;Chen Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kloft_M/0/1/0/all/0/1&quot;&gt;Marius Kloft&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smyth_P/0/1/0/all/0/1&quot;&gt;Padhraic Smyth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rudolph_M/0/1/0/all/0/1&quot;&gt;Maja Rudolph&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mandt_S/0/1/0/all/0/1&quot;&gt;Stephan Mandt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.07867">
<title>Learning Performance-Improving Code Edits. (arXiv:2302.07867v4 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2302.07867</link>
<description rdf:parseType="Literal">&lt;p&gt;With the waning of Moore&apos;s law, optimizing program performance has become a
major focus of software research. However, high-level optimizations such as API
and algorithm changes remain elusive due to the difficulty of understanding the
semantics of code. Simultaneously, pretrained large language models (LLMs) have
demonstrated strong capabilities at solving a wide range of programming tasks.
To that end, we introduce a framework for adapting LLMs to high-level program
optimization. First, we curate a dataset of performance-improving edits made by
human programmers of over 77K competitive C++ programming submission pairs,
accompanied by extensive unit tests. A major challenge is the significant
variability of measuring performance on commodity hardware, which can lead to
spurious &quot;improvements&quot;. To isolate and reliably evaluate the impact of program
optimizations, we design an environment based on the gem5 full system
simulator, the de facto simulator used in academia and industry. Next, we
propose a broad range of adaptation strategies for code optimization; for
prompting, these include retrieval-based few-shot prompting and
chain-of-thought, and for finetuning, these include performance-conditioned
generation and synthetic data augmentation based on self-play. A combination of
these techniques achieves an average speedup of 5.65X on CodeLlama-13B and
6.86X on GPT-3.5, surpassing the best human performance (4.06X). We find our
proposed performance-conditioned generation is particularly effective at
improving performance as well as increasing the fraction of optimized programs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shypula_A/0/1/0/all/0/1&quot;&gt;Alexander Shypula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madaan_A/0/1/0/all/0/1&quot;&gt;Aman Madaan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1&quot;&gt;Yimeng Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alon_U/0/1/0/all/0/1&quot;&gt;Uri Alon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gardner_J/0/1/0/all/0/1&quot;&gt;Jacob Gardner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hashemi_M/0/1/0/all/0/1&quot;&gt;Milad Hashemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1&quot;&gt;Graham Neubig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ranganathan_P/0/1/0/all/0/1&quot;&gt;Parthasarathy Ranganathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bastani_O/0/1/0/all/0/1&quot;&gt;Osbert Bastani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yazdanbakhsh_A/0/1/0/all/0/1&quot;&gt;Amir Yazdanbakhsh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.07143">
<title>A Review on Car-Following Model. (arXiv:2304.07143v2 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2304.07143</link>
<description rdf:parseType="Literal">&lt;p&gt;The car-following (CF) model is the core component for traffic simulations
and has been built-in in many production vehicles with Advanced Driving
Assistance Systems (ADAS). Research of CF behavior allows us to identify the
sources of different macro phenomena induced by the basic process of pairwise
vehicle interaction. The CF behavior and control model encompasses various
fields, such as traffic engineering, physics, cognitive science, machine
learning, and reinforcement learning. This paper provides a comprehensive
survey highlighting differences, complementarities, and overlaps among various
CF models according to their underlying logic and principles. We reviewed
representative algorithms, ranging from the theory-based kinematic models,
stimulus-response models, and cruise control models to data-driven Behavior
Cloning (BC) and Imitation Learning (IL) and outlined their strengths and
limitations. This review categorizes CF models that are conceptualized in
varying principles and summarize the vast literature with a holistic framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianya Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jin_P/0/1/0/all/0/1&quot;&gt;Peter J. Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bayen_A/0/1/0/all/0/1&quot;&gt;Alexandre Bayen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+D%2E_P/0/1/0/all/0/1&quot;&gt;Ph.D.&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Piccoli_B/0/1/0/all/0/1&quot;&gt;Benedetto Piccoli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.06026">
<title>Uncertainty in GNN Learning Evaluations: The Importance of a Consistent Benchmark for Community Detection. (arXiv:2305.06026v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.06026</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) have improved unsupervised community detection
of clustered nodes due to their ability to encode the dual dimensionality of
the connectivity and feature information spaces of graphs. Identifying the
latent communities has many practical applications from social networks to
genomics. Current benchmarks of real world performance are confusing due to the
variety of decisions influencing the evaluation of GNNs at this task. To
address this, we propose a framework to establish a common evaluation protocol.
We motivate and justify it by demonstrating the differences with and without
the protocol. The W Randomness Coefficient is a metric proposed for assessing
the consistency of algorithm rankings to quantify the reliability of results
under the presence of randomness. We find that by ensuring the same evaluation
criteria is followed, there may be significant differences from the reported
performance of methods at this task, but a more complete evaluation and
comparison of methods is possible.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leeney_W/0/1/0/all/0/1&quot;&gt;William Leeney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McConville_R/0/1/0/all/0/1&quot;&gt;Ryan McConville&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10406">
<title>Variational Classification. (arXiv:2305.10406v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10406</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a latent variable model for classification that provides a novel
probabilistic interpretation of neural network softmax classifiers. We derive a
variational objective to train the model, analogous to the evidence lower bound
(ELBO) used to train variational auto-encoders, that generalises the
cross-entropy loss used to train classification models. Treating inputs to the
softmax layer as samples of a latent variable, our abstracted perspective
reveals a potential inconsistency between their anticipated distribution,
required for accurate label predictions to be output, and the empirical
distribution found in practice. We augment the variational objective to
mitigate such inconsistency and encourage a chosen latent distribution, instead
of the implicit assumption in off-the-shelf softmax classifiers. Overall, we
provide new theoretical insight into the inner workings of widely-used softmax
classification. Empirical evaluation on image and text classification datasets
demonstrates that our proposed approach, variational classification, maintains
classification accuracy while the reshaped latent space improves other
desirable properties of a classifier, such as calibration, adversarial
robustness, robustness to distribution shift and sample efficiency useful in
low data settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhuliawala_S/0/1/0/all/0/1&quot;&gt;Shehzaad Dhuliawala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1&quot;&gt;Mrinmaya Sachan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allen_C/0/1/0/all/0/1&quot;&gt;Carl Allen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10782">
<title>Human Behavioral Benchmarking: Numeric Magnitude Comparison Effects in Large Language Models. (arXiv:2305.10782v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10782</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) do not differentially represent numbers, which
are pervasive in text. In contrast, neuroscience research has identified
distinct neural representations for numbers and words. In this work, we
investigate how well popular LLMs capture the magnitudes of numbers (e.g., that
$4 &amp;lt; 5$) from a behavioral lens. Prior research on the representational
capabilities of LLMs evaluates whether they show human-level performance, for
instance, high overall accuracy on standard benchmarks. Here, we ask a
different question, one inspired by cognitive science: How closely do the
number representations of LLMscorrespond to those of human language users, who
typically demonstrate the distance, size, and ratio effects? We depend on a
linking hypothesis to map the similarities among the model embeddings of number
words and digits to human response times. The results reveal surprisingly
human-like representations across language models of different architectures,
despite the absence of the neural circuitry that directly supports these
representations in the human brain. This research shows the utility of
understanding LLMs using behavioral benchmarks and points the way to future
work on the number representations of LLMs and their cognitive plausibility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1&quot;&gt;Raj Sanjay Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marupudi_V/0/1/0/all/0/1&quot;&gt;Vijay Marupudi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koenen_R/0/1/0/all/0/1&quot;&gt;Reba Koenen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhardwaj_K/0/1/0/all/0/1&quot;&gt;Khushi Bhardwaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varma_S/0/1/0/all/0/1&quot;&gt;Sashank Varma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.11243">
<title>Comparing Machines and Children: Using Developmental Psychology Experiments to Assess the Strengths and Weaknesses of LaMDA Responses. (arXiv:2305.11243v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.11243</link>
<description rdf:parseType="Literal">&lt;p&gt;Developmental psychologists have spent decades devising experiments to test
the intelligence and knowledge of infants and children, tracing the origin of
crucial concepts and capacities. Moreover, experimental techniques in
developmental psychology have been carefully designed to discriminate the
cognitive capacities that underlie particular behaviors. We propose that using
classical experiments from child development is a particularly effective way to
probe the computational abilities of AI models, in general, and LLMs in
particular. First, the methodological techniques of developmental psychology,
such as the use of novel stimuli to control for past experience or control
conditions to determine whether children are using simple associations, can be
equally helpful for assessing the capacities of LLMs. In parallel, testing LLMs
in this way can tell us whether the information that is encoded in text is
sufficient to enable particular responses, or whether those responses depend on
other kinds of information, such as information from exploration of the
physical world. In this work we adapt classical developmental experiments to
evaluate the capabilities of LaMDA, a large language model from Google. We
propose a novel LLM Response Score (LRS) metric which can be used to evaluate
other language models, such as GPT. We find that LaMDA generates appropriate
responses that are similar to those of children in experiments involving social
understanding, perhaps providing evidence that knowledge of these domains is
discovered through language. On the other hand, LaMDA&apos;s responses in early
object and action understanding, theory of mind, and especially causal
reasoning tasks are very different from those of young children, perhaps
showing that these domains require more real-world, self-initiated exploration
and cannot simply be learned from patterns in language input.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kosoy_E/0/1/0/all/0/1&quot;&gt;Eliza Kosoy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reagan_E/0/1/0/all/0/1&quot;&gt;Emily Rose Reagan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_L/0/1/0/all/0/1&quot;&gt;Leslie Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gopnik_A/0/1/0/all/0/1&quot;&gt;Alison Gopnik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cobb_D/0/1/0/all/0/1&quot;&gt;Danielle Krettek Cobb&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.11414">
<title>Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models. (arXiv:2305.11414v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.11414</link>
<description rdf:parseType="Literal">&lt;p&gt;Foundation Models (FMs), such as LLaMA, BERT, GPT, ViT, and CLIP, have
demonstrated remarkable success in a wide range of applications, driven by
their ability to leverage vast amounts of data for pre-training. However,
optimizing FMs often requires access to sensitive data, raising privacy
concerns and limiting their applicability in many domains. In this paper, we
propose the Federated Foundation Models (FFMs) paradigm, which combines the
benefits of FMs and Federated Learning (FL) to enable privacy-preserving and
collaborative learning across multiple end-users. We discuss the potential
benefits and challenges of integrating FL into the lifespan of FMs, covering
pre-training, fine-tuning, and application. We further outline potential future
research avenues in FFM, including FFM pre-training, FFM fine-tuning, and
federated prompt tuning, which allow the development of more personalized and
context-aware models while ensuring data privacy. Moreover, we explore the
possibility of continual/lifelong learning in FFMs, as increased computational
power at the edge may unlock the potential for optimizing FMs using newly
generated private data close to the data source. The proposed FFM concepts
offer a flexible and scalable framework for training large language models in a
privacy-preserving manner, setting the stage for subsequent advancements in
both FM training and federated learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Sixing Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munoz_J/0/1/0/all/0/1&quot;&gt;J. Pablo Mu&amp;#xf1;oz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jannesari_A/0/1/0/all/0/1&quot;&gt;Ali Jannesari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14979">
<title>Assessment of the Reliablity of a Model&apos;s Decision by Generalizing Attribution to the Wavelet Domain. (arXiv:2305.14979v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14979</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks have shown remarkable performance in computer vision, but
their deployment in numerous scientific and technical fields is challenging due
to their black-box nature. Scientists and practitioners need to evaluate the
reliability of a decision, i.e., to know simultaneously if a model relies on
the relevant features and whether these features are robust to image
corruptions. Existing attribution methods aim to provide human-understandable
explanations by highlighting important regions in the image domain, but fail to
fully characterize a decision process&apos;s reliability. To bridge this gap, we
introduce the Wavelet sCale Attribution Method (WCAM), a generalization of
attribution from the pixel domain to the space-scale domain using wavelet
transforms. Attribution in the wavelet domain reveals where and on what scales
the model focuses, thus enabling us to assess whether a decision is reliable.
Our code is accessible here:
\url{https://github.com/gabrielkasmi/spectral-attribution}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasmi_G/0/1/0/all/0/1&quot;&gt;Gabriel Kasmi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubus_L/0/1/0/all/0/1&quot;&gt;Laurent Dubus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Drenan_Y/0/1/0/all/0/1&quot;&gt;Yves-Marie Saint Drenan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blanc_P/0/1/0/all/0/1&quot;&gt;Philippe Blanc&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15296">
<title>MultiFusion: Fusing Pre-Trained Models for Multi-Lingual, Multi-Modal Image Generation. (arXiv:2305.15296v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15296</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent popularity of text-to-image diffusion models (DM) can largely be
attributed to the intuitive interface they provide to users. The intended
generation can be expressed in natural language, with the model producing
faithful interpretations of text prompts. However, expressing complex or
nuanced ideas in text alone can be difficult. To ease image generation, we
propose MultiFusion that allows one to express complex and nuanced concepts
with arbitrarily interleaved inputs of multiple modalities and languages.
MutliFusion leverages pre-trained models and aligns them for integration into a
cohesive system, thereby avoiding the need for extensive training from scratch.
Our experimental results demonstrate the efficient transfer of capabilities
from individual modules to the downstream model. Specifically, the fusion of
all independent components allows the image generation module to utilize
multilingual, interleaved multimodal inputs despite being trained solely on
monomodal data in a single language.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellagente_M/0/1/0/all/0/1&quot;&gt;Marco Bellagente&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brack_M/0/1/0/all/0/1&quot;&gt;Manuel Brack&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teufel_H/0/1/0/all/0/1&quot;&gt;Hannah Teufel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Friedrich_F/0/1/0/all/0/1&quot;&gt;Felix Friedrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deiseroth_B/0/1/0/all/0/1&quot;&gt;Bj&amp;#xf6;rn Deiseroth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eichenberg_C/0/1/0/all/0/1&quot;&gt;Constantin Eichenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1&quot;&gt;Andrew Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baldock_R/0/1/0/all/0/1&quot;&gt;Robert Baldock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nanda_S/0/1/0/all/0/1&quot;&gt;Souradeep Nanda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oostermeijer_K/0/1/0/all/0/1&quot;&gt;Koen Oostermeijer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cruz_Salinas_A/0/1/0/all/0/1&quot;&gt;Andres Felipe Cruz-Salinas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1&quot;&gt;Patrick Schramowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1&quot;&gt;Kristian Kersting&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinbach_S/0/1/0/all/0/1&quot;&gt;Samuel Weinbach&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18869">
<title>Dissecting Chain-of-Thought: Compositionality through In-Context Filtering and Learning. (arXiv:2305.18869v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18869</link>
<description rdf:parseType="Literal">&lt;p&gt;Chain-of-thought (CoT) is a method that enables language models to handle
complex reasoning tasks by decomposing them into simpler steps. Despite its
success, the underlying mechanics of CoT are not yet fully understood. In an
attempt to shed light on this, our study investigates the impact of CoT on the
ability of transformers to in-context learn a simple to study, yet general
family of compositional functions: multi-layer perceptrons (MLPs). In this
setting, we find that the success of CoT can be attributed to breaking down
in-context learning of a compositional function into two distinct phases:
focusing on and filtering data related to each step of the composition and
in-context learning the single-step composition function. Through both
experimental and theoretical evidence, we demonstrate how CoT significantly
reduces the sample complexity of in-context learning (ICL) and facilitates the
learning of complex functions that non-CoT methods struggle with. Furthermore,
we illustrate how transformers can transition from vanilla in-context learning
to mastering a compositional function with CoT by simply incorporating
additional layers that perform the necessary data-filtering for CoT via the
attention mechanism. In addition to these test-time benefits, we show CoT helps
accelerate pretraining by learning shortcuts to represent complex functions and
filtering plays an important role in this process. These findings collectively
provide insights into the mechanics of CoT, inviting further investigation of
its role in complex reasoning tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yingcong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sreenivasan_K/0/1/0/all/0/1&quot;&gt;Kartik Sreenivasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giannou_A/0/1/0/all/0/1&quot;&gt;Angeliki Giannou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papailiopoulos_D/0/1/0/all/0/1&quot;&gt;Dimitris Papailiopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oymak_S/0/1/0/all/0/1&quot;&gt;Samet Oymak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.03286">
<title>Survival Instinct in Offline Reinforcement Learning. (arXiv:2306.03286v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.03286</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel observation about the behavior of offline reinforcement
learning (RL) algorithms: on many benchmark datasets, offline RL can produce
well-performing and safe policies even when trained with &quot;wrong&quot; reward labels,
such as those that are zero everywhere or are negatives of the true rewards.
This phenomenon cannot be easily explained by offline RL&apos;s return maximization
objective. Moreover, it gives offline RL a degree of robustness that is
uncharacteristic of its online RL counterparts, which are known to be sensitive
to reward design. We demonstrate that this surprising robustness property is
attributable to an interplay between the notion of pessimism in offline RL
algorithms and certain implicit biases in common data collection practices. As
we prove in this work, pessimism endows the agent with a &quot;survival instinct&quot;,
i.e., an incentive to stay within the data support in the long term, while the
limited and biased data coverage further constrains the set of survival
policies. Formally, given a reward class -- which may not even contain the true
reward -- we identify conditions on the training data distribution that enable
offline RL to learn a near-optimal and safe policy from any reward within the
class. We argue that the survival instinct should be taken into account when
interpreting results from existing offline RL benchmarks and when creating
future ones. Our empirical and theoretical results suggest a new paradigm for
RL, whereby an agent is nudged to learn a desirable behavior with imperfect
reward but purposely biased data coverage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1&quot;&gt;Anqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1&quot;&gt;Dipendra Misra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolobov_A/0/1/0/all/0/1&quot;&gt;Andrey Kolobov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1&quot;&gt;Ching-An Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04031">
<title>Certified Deductive Reasoning with Language Models. (arXiv:2306.04031v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04031</link>
<description rdf:parseType="Literal">&lt;p&gt;Language models often achieve higher accuracy when reasoning step-by-step in
complex tasks. However, even when arriving at a correct final answer, their
rationales are often logically unsound or inconsistent. This is a major issue
when reliable reasoning traces are needed, such when fine-tuning on
model-generated reasoning for self-improvement. To tackle these issues, we
introduce a class of tools for language models called \emph{guides}, that use
state and incremental constraints to guide generation. A guide can be invoked
by the model to constrain its own generation to a set of valid statements given
by the tool. In turn, the model&apos;s choices can change the guide&apos;s state. We show
how a general system for logical reasoning can be used as a guide, which we
call \textsc{LogicGuide}. Given a reasoning problem in natural language, a
model can formalize its assumptions for \textsc{LogicGuide} and guarantee that
its step-by-step reasoning is sound. In experiments on PrOntoQA, ProofWriter
and Syllogism Validity datasets, \textsc{LogicGuide} significantly improves the
performance of GPT-3, GPT-3.5 Turbo and LLaMA (accuracy gains up to 35\%),
while drastically reducing \emph{content effects} -- the interference between
unwanted prior assumptions and reasoning, which humans and language models
suffer from. We then explore bootstrapping GPT-3.5 Turbo and LLaMA using their
own reasoning traces. We find that LogicGuide is critical: by training only on
certified self-generated reasoning, models can self-improve, avoiding learning
from their own hallucinations. Moreover, bootstrapped models enjoy significant
boosts on ReClor, a challenging real-world reasoning dataset, even when not
relying on formalization at inference time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poesia_G/0/1/0/all/0/1&quot;&gt;Gabriel Poesia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gandhi_K/0/1/0/all/0/1&quot;&gt;Kanishk Gandhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zelikman_E/0/1/0/all/0/1&quot;&gt;Eric Zelikman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1&quot;&gt;Noah D. Goodman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08013">
<title>TopP&amp;R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models. (arXiv:2306.08013v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08013</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a robust and reliable evaluation metric for generative models by
introducing topological and statistical treatments for rigorous support
estimation. Existing metrics, such as Inception Score (IS), Frechet Inception
Distance (FID), and the variants of Precision and Recall (P&amp;amp;R), heavily rely on
supports that are estimated from sample features. However, the reliability of
their estimation has not been seriously discussed (and overlooked) even though
the quality of the evaluation entirely depends on it. In this paper, we propose
Topological Precision and Recall (TopP&amp;amp;R, pronounced &apos;topper&apos;), which provides
a systematic approach to estimating supports, retaining only topologically and
statistically important features with a certain level of confidence. This not
only makes TopP&amp;amp;R strong for noisy features, but also provides statistical
consistency. Our theoretical and experimental results show that TopP&amp;amp;R is
robust to outliers and non-independent and identically distributed (Non-IID)
perturbations, while accurately capturing the true trend of change in samples.
To the best of our knowledge, this is the first evaluation metric focused on
the robust estimation of the support and provides its statistical consistency
under noise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_P/0/1/0/all/0/1&quot;&gt;Pum Jun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1&quot;&gt;Yoojin Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jisu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1&quot;&gt;Jaejun Yoo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15063">
<title>Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression. (arXiv:2306.15063v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15063</link>
<description rdf:parseType="Literal">&lt;p&gt;Pretrained transformers exhibit the remarkable ability of in-context learning
(ICL): they can learn tasks from just a few examples provided in the prompt
without updating any weights. This raises a foundational question: can ICL
solve fundamentally $\textit{new}$ tasks that are very different from those
seen during pretraining? To probe this question, we examine ICL&apos;s performance
on linear regression while varying the diversity of tasks in the pretraining
dataset. We empirically demonstrate a $\textit{task diversity threshold}$ for
the emergence of ICL. Below this threshold, the pretrained transformer cannot
solve unseen regression tasks, instead behaving like a Bayesian estimator with
the $\textit{non-diverse pretraining task distribution}$ as the prior. Beyond
this threshold, the transformer significantly outperforms this estimator; its
behavior aligns with that of ridge regression, corresponding to a Gaussian
prior over $\textit{all tasks}$, including those not seen during pretraining.
Thus, when pretrained on data with task diversity greater than the threshold,
transformers $\textit{can}$ optimally solve fundamentally new tasks in-context.
Importantly, this capability hinges on it deviating from the Bayes optimal
estimator with the pretraining distribution as the prior. This study also
explores the effect of regularization, model capacity and task structure and
underscores, in a concrete example, the critical role of task diversity,
alongside data and model scale, in the emergence of ICL. Code is available at
https://github.com/mansheej/icl-task-diversity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raventos_A/0/1/0/all/0/1&quot;&gt;Allan Ravent&amp;#xf3;s&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paul_M/0/1/0/all/0/1&quot;&gt;Mansheej Paul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1&quot;&gt;Feng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1&quot;&gt;Surya Ganguli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16750">
<title>Eigensubspace of Temporal-Difference Dynamics and How It Improves Value Approximation in Reinforcement Learning. (arXiv:2306.16750v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16750</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel value approximation method, namely Eigensubspace
Regularized Critic (ERC) for deep reinforcement learning (RL). ERC is motivated
by an analysis of the dynamics of Q-value approximation error in the
Temporal-Difference (TD) method, which follows a path defined by the
1-eigensubspace of the transition kernel associated with the Markov Decision
Process (MDP). It reveals a fundamental property of TD learning that has
remained unused in previous deep RL approaches. In ERC, we propose a
regularizer that guides the approximation error tending towards the
1-eigensubspace, resulting in a more efficient and stable path of value
approximation. Moreover, we theoretically prove the convergence of the ERC
method. Besides, theoretical analysis and experiments demonstrate that ERC
effectively reduces the variance of value functions. Among 26 tasks in the
DMControl benchmark, ERC outperforms state-of-the-art methods for 20. Besides,
it shows significant advantages in Q-value approximation and variance
reduction. Our code is available at https://sites.google.com/view/erc-ecml23/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1&quot;&gt;Qiang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1&quot;&gt;Tianyi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1&quot;&gt;Meng Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maghsudi_S/0/1/0/all/0/1&quot;&gt;Setareh Maghsudi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03718">
<title>Frontier AI Regulation: Managing Emerging Risks to Public Safety. (arXiv:2307.03718v4 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03718</link>
<description rdf:parseType="Literal">&lt;p&gt;Advanced AI models hold the promise of tremendous benefits for humanity, but
society needs to proactively manage the accompanying risks. In this paper, we
focus on what we term &quot;frontier AI&quot; models: highly capable foundation models
that could possess dangerous capabilities sufficient to pose severe risks to
public safety. Frontier AI models pose a distinct regulatory challenge:
dangerous capabilities can arise unexpectedly; it is difficult to robustly
prevent a deployed model from being misused; and, it is difficult to stop a
model&apos;s capabilities from proliferating broadly. To address these challenges,
at least three building blocks for the regulation of frontier models are
needed: (1) standard-setting processes to identify appropriate requirements for
frontier AI developers, (2) registration and reporting requirements to provide
regulators with visibility into frontier AI development processes, and (3)
mechanisms to ensure compliance with safety standards for the development and
deployment of frontier AI models. Industry self-regulation is an important
first step. However, wider societal discussions and government intervention
will be needed to create standards and to ensure compliance with them. We
consider several options to this end, including granting enforcement powers to
supervisory authorities and licensure regimes for frontier AI models. Finally,
we propose an initial set of safety standards. These include conducting
pre-deployment risk assessments; external scrutiny of model behavior; using
risk assessments to inform deployment decisions; and monitoring and responding
to new information about model capabilities and uses post-deployment. We hope
this discussion contributes to the broader conversation on how to balance
public safety risks and innovation benefits from advances at the frontier of AI
development.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anderljung_M/0/1/0/all/0/1&quot;&gt;Markus Anderljung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barnhart_J/0/1/0/all/0/1&quot;&gt;Joslyn Barnhart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korinek_A/0/1/0/all/0/1&quot;&gt;Anton Korinek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leung_J/0/1/0/all/0/1&quot;&gt;Jade Leung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OKeefe_C/0/1/0/all/0/1&quot;&gt;Cullen O&amp;#x27;Keefe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whittlestone_J/0/1/0/all/0/1&quot;&gt;Jess Whittlestone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Avin_S/0/1/0/all/0/1&quot;&gt;Shahar Avin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brundage_M/0/1/0/all/0/1&quot;&gt;Miles Brundage&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bullock_J/0/1/0/all/0/1&quot;&gt;Justin Bullock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cass_Beggs_D/0/1/0/all/0/1&quot;&gt;Duncan Cass-Beggs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1&quot;&gt;Ben Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Collins_T/0/1/0/all/0/1&quot;&gt;Tantum Collins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fist_T/0/1/0/all/0/1&quot;&gt;Tim Fist&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hadfield_G/0/1/0/all/0/1&quot;&gt;Gillian Hadfield&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayes_A/0/1/0/all/0/1&quot;&gt;Alan Hayes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_L/0/1/0/all/0/1&quot;&gt;Lewis Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1&quot;&gt;Sara Hooker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1&quot;&gt;Eric Horvitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolt_N/0/1/0/all/0/1&quot;&gt;Noam Kolt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuett_J/0/1/0/all/0/1&quot;&gt;Jonas Schuett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shavit_Y/0/1/0/all/0/1&quot;&gt;Yonadav Shavit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siddarth_D/0/1/0/all/0/1&quot;&gt;Divya Siddarth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trager_R/0/1/0/all/0/1&quot;&gt;Robert Trager&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolf_K/0/1/0/all/0/1&quot;&gt;Kevin Wolf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09342">
<title>Learning to Select SAT Encodings for Pseudo-Boolean and Linear Integer Constraints. (arXiv:2307.09342v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09342</link>
<description rdf:parseType="Literal">&lt;p&gt;Many constraint satisfaction and optimisation problems can be solved
effectively by encoding them as instances of the Boolean Satisfiability problem
(SAT). However, even the simplest types of constraints have many encodings in
the literature with widely varying performance, and the problem of selecting
suitable encodings for a given problem instance is not trivial. We explore the
problem of selecting encodings for pseudo-Boolean and linear constraints using
a supervised machine learning approach. We show that it is possible to select
encodings effectively using a standard set of features for constraint problems;
however we obtain better performance with a new set of features specifically
designed for the pseudo-Boolean and linear constraints. In fact, we achieve
good results when selecting encodings for unseen problem classes. Our results
compare favourably to AutoFolio when using the same feature set. We discuss the
relative importance of instance features to the task of selecting the best
encodings, and compare several variations of the machine learning method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ulrich_Oltean_F/0/1/0/all/0/1&quot;&gt;Felix Ulrich-Oltean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nightingale_P/0/1/0/all/0/1&quot;&gt;Peter Nightingale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walker_J/0/1/0/all/0/1&quot;&gt;James Alfred Walker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09933">
<title>Spuriosity Didn&apos;t Kill the Classifier: Using Invariant Predictions to Harness Spurious Features. (arXiv:2307.09933v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09933</link>
<description rdf:parseType="Literal">&lt;p&gt;To avoid failures on out-of-distribution data, recent works have sought to
extract features that have an invariant or stable relationship with the label
across domains, discarding &quot;spurious&quot; or unstable features whose relationship
with the label changes across domains. However, unstable features often carry
complementary information that could boost performance if used correctly in the
test domain. In this work, we show how this can be done without test-domain
labels. In particular, we prove that pseudo-labels based on stable features
provide sufficient guidance for doing so, provided that stable and unstable
features are conditionally independent given the label. Based on this
theoretical insight, we propose Stable Feature Boosting (SFB), an algorithm
for: (i) learning a predictor that separates stable and
conditionally-independent unstable features; and (ii) using the stable-feature
predictions to adapt the unstable-feature predictions in the test domain.
Theoretically, we prove that SFB can learn an asymptotically-optimal predictor
without test-domain labels. Empirically, we demonstrate the effectiveness of
SFB on real and synthetic data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eastwood_C/0/1/0/all/0/1&quot;&gt;Cian Eastwood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Shashank Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nicolicioiu_A/0/1/0/all/0/1&quot;&gt;Andrei Liviu Nicolicioiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vlastelica_M/0/1/0/all/0/1&quot;&gt;Marin Vlastelica&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kugelgen_J/0/1/0/all/0/1&quot;&gt;Julius von K&amp;#xfc;gelgen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14953">
<title>Multi-Source Domain Adaptation through Dataset Dictionary Learning in Wasserstein Space. (arXiv:2307.14953v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.14953</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper seeks to solve Multi-Source Domain Adaptation (MSDA), which aims
to mitigate data distribution shifts when transferring knowledge from multiple
labeled source domains to an unlabeled target domain. We propose a novel MSDA
framework based on dictionary learning and optimal transport. We interpret each
domain in MSDA as an empirical distribution. As such, we express each domain as
a Wasserstein barycenter of dictionary atoms, which are empirical
distributions. We propose a novel algorithm, DaDiL, for learning via
mini-batches: (i) atom distributions; (ii) a matrix of barycentric coordinates.
Based on our dictionary, we propose two novel methods for MSDA: DaDil-R, based
on the reconstruction of labeled samples in the target domain, and DaDiL-E,
based on the ensembling of classifiers learned on atom distributions. We
evaluate our methods in 3 benchmarks: Caltech-Office, Office 31, and CRWU,
where we improved previous state-of-the-art by 3.15%, 2.29%, and 7.71% in
classification performance. Finally, we show that interpolations in the
Wasserstein hull of learned atoms provide data that can generalize to the
target domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montesuma_E/0/1/0/all/0/1&quot;&gt;Eduardo Fernandes Montesuma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mboula_F/0/1/0/all/0/1&quot;&gt;Fred Ngol&amp;#xe8; Mboula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Souloumiac_A/0/1/0/all/0/1&quot;&gt;Antoine Souloumiac&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00113">
<title>Three Bricks to Consolidate Watermarks for Large Language Models. (arXiv:2308.00113v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.00113</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of discerning between generated and natural texts is increasingly
challenging. In this context, watermarking emerges as a promising technique for
ascribing generated text to a specific model. It alters the sampling generation
process so as to leave an invisible trace in the generated output, facilitating
later detection. This research consolidates watermarks for large language
models based on three theoretical and empirical considerations. First, we
introduce new statistical tests that offer robust theoretical guarantees which
remain valid even at low false-positive rates (less than 10$^{\text{-6}}$).
Second, we compare the effectiveness of watermarks using classical benchmarks
in the field of natural language processing, gaining insights into their
real-world applicability. Third, we develop advanced detection schemes for
scenarios where access to the LLM is available, as well as multi-bit
watermarking.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_P/0/1/0/all/0/1&quot;&gt;Pierre Fernandez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaffin_A/0/1/0/all/0/1&quot;&gt;Antoine Chaffin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tit_K/0/1/0/all/0/1&quot;&gt;Karim Tit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chappelier_V/0/1/0/all/0/1&quot;&gt;Vivien Chappelier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furon_T/0/1/0/all/0/1&quot;&gt;Teddy Furon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00158">
<title>Predictive Data Analytics with AI: assessing the need for post-editing of MT output by fine-tuning OpenAI LLMs. (arXiv:2308.00158v5 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.00158</link>
<description rdf:parseType="Literal">&lt;p&gt;Translation Quality Evaluation (TQE) is an essential step of the modern
translation production process. TQE is critical in assessing both machine
translation (MT) and human translation (HT) quality without reference
translations. The ability to evaluate or even simply estimate the quality of
translation automatically may open significant efficiency gains through process
optimisation. This work examines whether the state-of-the-art large language
models (LLMs) can be used for this purpose. We take OpenAI models as the best
state-of-the-art technology and approach TQE as a binary classification task.
On eight language pairs including English to Italian, German, French, Japanese,
Dutch, Portuguese, Turkish, and Chinese, our experimental results show that
fine-tuned gpt3.5 can demonstrate good performance on translation quality
prediction tasks, i.e. whether the translation needs to be edited. Another
finding is that simply increasing the sizes of LLMs does not lead to apparent
better performances on this task by comparing the performance of three
different versions of OpenAI models: curie, davinci, and gpt3.5 with 13B, 175B,
and 175B parameters, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gladkoff_S/0/1/0/all/0/1&quot;&gt;Serge Gladkoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erofeev_G/0/1/0/all/0/1&quot;&gt;Gleb Erofeev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sorokina_I/0/1/0/all/0/1&quot;&gt;Irina Sorokina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1&quot;&gt;Lifeng Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nenadic_G/0/1/0/all/0/1&quot;&gt;Goran Nenadic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00530">
<title>Tolerating Annotation Displacement in Dense Object Counting via Point Annotation Probability Map. (arXiv:2308.00530v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.00530</link>
<description rdf:parseType="Literal">&lt;p&gt;Counting objects in crowded scenes remains a challenge to computer vision.
The current deep learning based approach often formulate it as a Gaussian
density regression problem. Such a brute-force regression, though effective,
may not consider the annotation displacement properly which arises from the
human annotation process and may lead to different distributions. We conjecture
that it would be beneficial to consider the annotation displacement in the
dense object counting task. To obtain strong robustness against annotation
displacement, generalized Gaussian distribution (GGD) function with a tunable
bandwidth and shape parameter is exploited to form the learning target point
annotation probability map, PAPM. Specifically, we first present a
hand-designed PAPM method (HD-PAPM), in which we design a function based on GGD
to tolerate the annotation displacement. For end-to-end training, the
hand-designed PAPM may not be optimal for the particular network and dataset.
An adaptively learned PAPM method (AL-PAPM) is proposed. To improve the
robustness to annotation displacement, we design an effective transport cost
function based on GGD. The proposed PAPM is capable of integration with other
methods. We also combine PAPM with P2PNet through modifying the matching cost
matrix, forming P2P-PAPM. This could also improve the robustness to annotation
displacement of P2PNet. Extensive experiments show the superiority of our
proposed methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuehai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Badong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gang_H/0/1/0/all/0/1&quot;&gt;Hua Gang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1&quot;&gt;Shaoyi Du&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01098">
<title>Towards Better Query Classification with Multi-Expert Knowledge Condensation in JD Ads Search. (arXiv:2308.01098v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2308.01098</link>
<description rdf:parseType="Literal">&lt;p&gt;Search query classification, as an effective way to understand user intents,
is of great importance in real-world online ads systems. To ensure a lower
latency, a shallow model (e.g. FastText) is widely used for efficient online
inference. However, the representation ability of the FastText model is
insufficient, resulting in poor classification performance, especially on some
low-frequency queries and tailed categories. Using a deeper and more complex
model (e.g. BERT) is an effective solution, but it will cause a higher online
inference latency and more expensive computing costs. Thus, how to juggle both
inference efficiency and classification performance is obviously of great
practical importance. To overcome this challenge, in this paper, we propose
knowledge condensation (KC), a simple yet effective knowledge distillation
framework to boost the classification performance of the online FastText model
under strict low latency constraints. Specifically, we propose to train an
offline BERT model to retrieve more potentially relevant data. Benefiting from
its powerful semantic representation, more relevant labels not exposed in the
historical data will be added into the training set for better FastText model
training. Moreover, a novel distribution-diverse multi-expert learning strategy
is proposed to further improve the mining ability of relevant data. By training
multiple BERT models from different data distributions, it can respectively
perform better at high, middle, and low-frequency search queries. The model
ensemble from multi-distribution makes its retrieval ability more powerful. We
have deployed two versions of this framework in JD search, and both offline
experiments and online A/B testing from multiple datasets have validated the
effectiveness of the proposed approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_K/0/1/0/all/0/1&quot;&gt;Kun-Peng Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_M/0/1/0/all/0/1&quot;&gt;Ming Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1&quot;&gt;Zheng Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xue Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xi-Wei Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1&quot;&gt;Chang-Ping Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhan-Gang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jing-He Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1&quot;&gt;Jing-Ping Shao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.06202">
<title>Exploring Predicate Visual Context in Detecting Human-Object Interactions. (arXiv:2308.06202v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.06202</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the DETR framework has emerged as the dominant approach for
human--object interaction (HOI) research. In particular, two-stage
transformer-based HOI detectors are amongst the most performant and
training-efficient approaches. However, these often condition HOI
classification on object features that lack fine-grained contextual
information, eschewing pose and orientation information in favour of visual
cues about object identity and box extremities. This naturally hinders the
recognition of complex or ambiguous interactions. In this work, we study these
issues through visualisations and carefully designed experiments. Accordingly,
we investigate how best to re-introduce image features via cross-attention.
With an improved query design, extensive exploration of keys and values, and
box pair positional embeddings as spatial guidance, our model with enhanced
predicate visual context (PViC) outperforms state-of-the-art methods on the
HICO-DET and V-COCO benchmarks, while maintaining low training cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Frederic Z. Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yuhui Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Campbell_D/0/1/0/all/0/1&quot;&gt;Dylan Campbell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1&quot;&gt;Zhuoyao Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1&quot;&gt;Stephen Gould&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.06597">
<title>Rank2Tell: A Multimodal Driving Dataset for Joint Importance Ranking and Reasoning. (arXiv:2309.06597v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.06597</link>
<description rdf:parseType="Literal">&lt;p&gt;The widespread adoption of commercial autonomous vehicles (AVs) and advanced
driver assistance systems (ADAS) may largely depend on their acceptance by
society, for which their perceived trustworthiness and interpretability to
riders are crucial. In general, this task is challenging because modern
autonomous systems software relies heavily on black-box artificial intelligence
models. Towards this goal, this paper introduces a novel dataset, Rank2Tell, a
multi-modal ego-centric dataset for Ranking the importance level and Telling
the reason for the importance. Using various close and open-ended visual
question answering, the dataset provides dense annotations of various semantic,
spatial, temporal, and relational attributes of various important objects in
complex traffic scenarios. The dense annotations and unique attributes of the
dataset make it a valuable resource for researchers working on visual scene
understanding and related fields. Furthermore, we introduce a joint model for
joint importance level ranking and natural language captions generation to
benchmark our dataset and demonstrate performance with quantitative
evaluations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sachdeva_E/0/1/0/all/0/1&quot;&gt;Enna Sachdeva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_N/0/1/0/all/0/1&quot;&gt;Nakul Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chundi_S/0/1/0/all/0/1&quot;&gt;Suhas Chundi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roelofs_S/0/1/0/all/0/1&quot;&gt;Sean Roelofs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiachen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1&quot;&gt;Mykel Kochenderfer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_C/0/1/0/all/0/1&quot;&gt;Chiho Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dariush_B/0/1/0/all/0/1&quot;&gt;Behzad Dariush&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07670">
<title>Federated Dataset Dictionary Learning for Multi-Source Domain Adaptation. (arXiv:2309.07670v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.07670</link>
<description rdf:parseType="Literal">&lt;p&gt;In this article, we propose an approach for federated domain adaptation, a
setting where distributional shift exists among clients and some have unlabeled
data. The proposed framework, FedDaDiL, tackles the resulting challenge through
dictionary learning of empirical distributions. In our setting, clients&apos;
distributions represent particular domains, and FedDaDiL collectively trains a
federated dictionary of empirical distributions. In particular, we build upon
the Dataset Dictionary Learning framework by designing collaborative
communication protocols and aggregation operations. The chosen protocols keep
clients&apos; data private, thus enhancing overall privacy compared to its
centralized counterpart. We empirically demonstrate that our approach
successfully generates labeled data on the target domain with extensive
experiments on (i) Caltech-Office, (ii) TEP, and (iii) CWRU benchmarks.
Furthermore, we compare our method to its centralized counterpart and other
benchmarks in federated domain adaptation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castellon_F/0/1/0/all/0/1&quot;&gt;Fabiola Espinoza Castellon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montesuma_E/0/1/0/all/0/1&quot;&gt;Eduardo Fernandes Montesuma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mboula_F/0/1/0/all/0/1&quot;&gt;Fred Ngol&amp;#xe8; Mboula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mayoue_A/0/1/0/all/0/1&quot;&gt;Aur&amp;#xe9;lien Mayoue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Souloumiac_A/0/1/0/all/0/1&quot;&gt;Antoine Souloumiac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gouy_Pailler_C/0/1/0/all/0/1&quot;&gt;C&amp;#xe9;dric Gouy-Pailler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.08589">
<title>Chain-of-Thought Reasoning is a Policy Improvement Operator. (arXiv:2309.08589v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.08589</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models have astounded the world with fascinating new
capabilities. However, they currently lack the ability to teach themselves new
skills, relying instead on large amounts of human-generated training data. We
introduce SECToR (Self-Education via Chain-of-Thought Reasoning), a
proof-of-concept demonstration that language models can teach themselves new
skills using chain-of-thought reasoning. During the self-learning loop, SECToR
asks models to solve addition problems using chain-of-thought reasoning before
training the next version of the model to solve those same problems directly
without using such reasoning. This process often results in an improved model
which can, when again augmented with chain-of-thought reasoning, solve even
harder problems than the original model, allowing the self-learning loop to
continue. Language models trained via SECToR autonomously learn to add up to
the longest-length-digit numbers without access to any ground truth examples
beyond an initial supervised fine-tuning phase consisting only of numbers with
6 or fewer digits. Our central hypothesis is that chain-of-thought reasoning
can act as a policy improvement operator, similarly to how Monte-Carlo Tree
Search is used in AlphaZero (Silver et al., 2017). We hope that this research
can lead to new directions in which language models can learn to teach
themselves without the need for human demonstrations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hugh Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parkes_D/0/1/0/all/0/1&quot;&gt;David C. Parkes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.08637">
<title>TextBind: Multi-turn Interleaved Multimodal Instruction-following in the Wild. (arXiv:2309.08637v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.08637</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models with instruction-following abilities have
revolutionized the field of artificial intelligence. These models show
exceptional generalizability to tackle various real-world tasks through their
natural language interfaces. However, their performance heavily relies on
high-quality exemplar data, which is often difficult to obtain. This challenge
is further exacerbated when it comes to multimodal instruction following. We
introduce TextBind, an almost annotation-free framework for empowering larger
language models with the multi-turn interleaved multimodal
instruction-following capabilities. Our approach requires only image-caption
pairs and generates multi-turn multimodal instruction-response conversations
from a language model. To accommodate interleaved image-text inputs and
outputs, we devise MIM, a language model-centric architecture that seamlessly
integrates image encoder and decoder models. We release our dataset, model, and
demo to foster future research in the area of multimodal instruction following.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huayang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Siheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1&quot;&gt;Deng Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Longyue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lemao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watanabe_T/0/1/0/all/0/1&quot;&gt;Taro Watanabe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yujiu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1&quot;&gt;Shuming Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.08646">
<title>CoCA: Fusing position embedding with Collinear Constrained Attention for fine-tuning free context window extending. (arXiv:2309.08646v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.08646</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-attention and position embedding are two key modules in Transformer
based LLMs. The potential relationship among them are far from well studied,
especially for context window extending. In this paper, we introduce collinear
constrained relationship to fuse RoPE and self-attention, and name it as
Collinear Constrained Attention (CoCA). We&apos;ve analyzed the computational and
spatial complexity of CoCA and have determined that it adds only minimal
additional overhead compared to the original Transformer-based models. We
provide an efficient implementation of CoCA, and make it drop-in replacement
for any existing position embedding and attention modules in Transformer based
models. Experiments show that CoCA performs extraordinary well on context
window extending. For instance, a CoCA based GPT model trained with 512 context
length can extend the context window up to 8K without perplexity diverging.
This indicates more than 16x context window extending without any fine-tuning.
Our code is released here:
https://github.com/codefuse-ai/Collinear-Constrained-Attention
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Shiyi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jing Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1&quot;&gt;Wei Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yifan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianguo Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.08872">
<title>PDFTriage: Question Answering over Long, Structured Documents. (arXiv:2309.08872v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.08872</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have issues with document question answering
(QA) in situations where the document is unable to fit in the small context
length of an LLM. To overcome this issue, most existing works focus on
retrieving the relevant context from the document, representing them as plain
text. However, documents such as PDFs, web pages, and presentations are
naturally structured with different pages, tables, sections, and so on.
Representing such structured documents as plain text is incongruous with the
user&apos;s mental model of these documents with rich structure. When a system has
to query the document for context, this incongruity is brought to the fore, and
seemingly trivial questions can trip up the QA system. To bridge this
fundamental gap in handling structured documents, we propose an approach called
PDFTriage that enables models to retrieve the context based on either structure
or content. Our experiments demonstrate the effectiveness of the proposed
PDFTriage-augmented models across several classes of questions where existing
retrieval-augmented LLMs fail. To facilitate further research on this
fundamental problem, we release our benchmark dataset consisting of 900+
human-generated questions over 80 structured documents from 10 different
categories of question types for document QA. Our code and datasets will be
released soon on Github.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saad_Falcon_J/0/1/0/all/0/1&quot;&gt;Jon Saad-Falcon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barrow_J/0/1/0/all/0/1&quot;&gt;Joe Barrow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siu_A/0/1/0/all/0/1&quot;&gt;Alexa Siu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nenkova_A/0/1/0/all/0/1&quot;&gt;Ani Nenkova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_D/0/1/0/all/0/1&quot;&gt;David Seunghyun Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rossi_R/0/1/0/all/0/1&quot;&gt;Ryan A. Rossi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1&quot;&gt;Franck Dernoncourt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12871">
<title>AnglE-optimized Text Embeddings. (arXiv:2309.12871v6 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.12871</link>
<description rdf:parseType="Literal">&lt;p&gt;High-quality text embedding is pivotal in improving semantic textual
similarity (STS) tasks, which are crucial components in Large Language Model
(LLM) applications. However, a common challenge existing text embedding models
face is the problem of vanishing gradients, primarily due to their reliance on
the cosine function in the optimization objective, which has saturation zones.
To address this issue, this paper proposes a novel angle-optimized text
embedding model called AnglE. The core idea of AnglE is to introduce angle
optimization in a complex space. This novel approach effectively mitigates the
adverse effects of the saturation zone in the cosine function, which can impede
gradient and hinder optimization processes. To set up a comprehensive STS
evaluation, we experimented on existing short-text STS datasets and a newly
collected long-text STS dataset from GitHub Issues. Furthermore, we examine
domain-specific STS scenarios with limited labeled data and explore how AnglE
works with LLM-annotated data. Extensive experiments were conducted on various
tasks including short-text STS, long-text STS, and domain-specific STS tasks.
The results show that AnglE outperforms the state-of-the-art (SOTA) STS models
that ignore the cosine saturation zone. These findings demonstrate the ability
of AnglE to generate high-quality text embeddings and the usefulness of angle
optimization in STS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xianming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jing Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00034">
<title>PB-LLM: Partially Binarized Large Language Models. (arXiv:2310.00034v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00034</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores network binarization, a radical form of quantization,
compressing model weights to a single bit, specifically for Large Language
Models (LLMs) compression. Due to previous binarization methods collapsing
LLMs, we propose a novel approach, Partially-Binarized LLM (PB-LLM), which can
achieve extreme low-bit quantization while maintaining the linguistic reasoning
capacity of quantized LLMs. Specifically, our exploration first uncovers the
ineffectiveness of naive applications of existing binarization algorithms and
highlights the imperative role of salient weights in achieving low-bit
quantization. Thus, PB-LLM filters a small ratio of salient weights during
binarization, allocating them to higher-bit storage, i.e.,
partially-binarization. PB-LLM is extended to recover the capacities of
quantized LMMs, by analyzing from the perspective of post-training quantization
(PTQ) and quantization-aware training (QAT). Under PTQ, combining the concepts
from GPTQ, we reconstruct the binarized weight matrix guided by the Hessian
matrix and successfully recover the reasoning capacity of PB-LLM in low-bit.
Under QAT, we freeze the salient weights during training, explore the
derivation of optimal scaling factors crucial for minimizing the quantization
error, and propose a scaling mechanism based on this derived scaling strategy
for residual binarized weights. Those explorations and the developed
methodologies significantly contribute to rejuvenating the performance of
low-bit quantized LLMs and present substantial advancements in the field of
network binarization for LLMs.The code is available at
https://github.com/hahnyuan/BinaryLLM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_Y/0/1/0/all/0/1&quot;&gt;Yuzhang Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1&quot;&gt;Zhihang Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qiang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1&quot;&gt;Zhen Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03912">
<title>RTDK-BO: High Dimensional Bayesian Optimization with Reinforced Transformer Deep kernels. (arXiv:2310.03912v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03912</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian Optimization (BO), guided by Gaussian process (GP) surrogates, has
proven to be an invaluable technique for efficient, high-dimensional, black-box
optimization, a critical problem inherent to many applications such as
industrial design and scientific computing. Recent contributions have
introduced reinforcement learning (RL) to improve the optimization performance
on both single function optimization and \textit{few-shot} multi-objective
optimization. However, even few-shot techniques fail to exploit similarities
shared between closely related objectives. In this paper, we combine recent
developments in Deep Kernel Learning (DKL) and attention-based Transformer
models to improve the modeling powers of GP surrogates with meta-learning. We
propose a novel method for improving meta-learning BO surrogates by
incorporating attention mechanisms into DKL, empowering the surrogates to adapt
to contextual information gathered during the BO process. We combine this
Transformer Deep Kernel with a learned acquisition function trained with
continuous Soft Actor-Critic Reinforcement Learning to aid in exploration. This
Reinforced Transformer Deep Kernel (RTDK-BO) approach yields state-of-the-art
results in continuous high-dimensional optimization problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shmakov_A/0/1/0/all/0/1&quot;&gt;Alexander Shmakov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naug_A/0/1/0/all/0/1&quot;&gt;Avisek Naug&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gundecha_V/0/1/0/all/0/1&quot;&gt;Vineet Gundecha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghorbanpour_S/0/1/0/all/0/1&quot;&gt;Sahand Ghorbanpour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gutierrez_R/0/1/0/all/0/1&quot;&gt;Ricardo Luna Gutierrez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babu_A/0/1/0/all/0/1&quot;&gt;Ashwin Ramesh Babu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guillen_A/0/1/0/all/0/1&quot;&gt;Antonio Guillen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1&quot;&gt;Soumyendu Sarkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05036">
<title>AvalonBench: Evaluating LLMs Playing the Game of Avalon. (arXiv:2310.05036v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05036</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we explore the potential of Large Language Models (LLMs)
Agents in playing the strategic social deduction game, Resistance Avalon.
Players in Avalon are challenged not only to make informed decisions based on
dynamically evolving game phases, but also to engage in discussions where they
must deceive, deduce, and negotiate with other players. These characteristics
make Avalon a compelling test-bed to study the decision-making and
language-processing capabilities of LLM Agents. To facilitate research in this
line, we introduce AvalonBench - a comprehensive game environment tailored for
evaluating multi-agent LLM Agents. This benchmark incorporates: (1) a game
environment for Avalon, (2) rule-based bots as baseline opponents, and (3)
ReAct-style LLM agents with tailored prompts for each role. Notably, our
evaluations based on AvalonBench highlight a clear capability gap. For
instance, models like ChatGPT playing good-role got a win rate of 22.2% against
rule-based bots playing evil, while good-role bot achieves 38.2% win rate in
the same setting. We envision AvalonBench could be a good test-bed for
developing more advanced LLMs (with self-playing) and agent frameworks that can
effectively model the layered complexities of such game environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Light_J/0/1/0/all/0/1&quot;&gt;Jonathan Light&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1&quot;&gt;Min Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1&quot;&gt;Sheng Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Ziniu Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.09689">
<title>A Partially Supervised Reinforcement Learning Framework for Visual Active Search. (arXiv:2310.09689v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.09689</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual active search (VAS) has been proposed as a modeling framework in which
visual cues are used to guide exploration, with the goal of identifying regions
of interest in a large geospatial area. Its potential applications include
identifying hot spots of rare wildlife poaching activity, search-and-rescue
scenarios, identifying illegal trafficking of weapons, drugs, or people, and
many others. State of the art approaches to VAS include applications of deep
reinforcement learning (DRL), which yield end-to-end search policies, and
traditional active search, which combines predictions with custom algorithmic
approaches. While the DRL framework has been shown to greatly outperform
traditional active search in such domains, its end-to-end nature does not make
full use of supervised information attained either during training, or during
actual search, a significant limitation if search tasks differ significantly
from those in the training distribution. We propose an approach that combines
the strength of both DRL and conventional active search by decomposing the
search policy into a prediction module, which produces a geospatial
distribution of regions of interest based on task embedding and search history,
and a search module, which takes the predictions and search history as input
and outputs the search distribution. We develop a novel meta-learning approach
for jointly learning the resulting combined policy that can make effective use
of supervised information obtained both at training and decision time. Our
extensive experiments demonstrate that the proposed representation and
meta-learning frameworks significantly outperform state of the art in visual
active search on several problem domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1&quot;&gt;Anindya Sarkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacobs_N/0/1/0/all/0/1&quot;&gt;Nathan Jacobs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vorobeychik_Y/0/1/0/all/0/1&quot;&gt;Yevgeniy Vorobeychik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.13032">
<title>Quality-Diversity through AI Feedback. (arXiv:2310.13032v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.13032</link>
<description rdf:parseType="Literal">&lt;p&gt;In many text-generation problems, users may prefer not only a single
response, but a diverse range of high-quality outputs from which to choose.
Quality-diversity (QD) search algorithms aim at such outcomes, by continually
improving and diversifying a population of candidates. However, the
applicability of QD to qualitative domains, like creative writing, has been
limited by the difficulty of algorithmically specifying measures of quality and
diversity. Interestingly, recent developments in language models (LMs) have
enabled guiding search through AI feedback, wherein LMs are prompted in natural
language to evaluate qualitative aspects of text. Leveraging this development,
we introduce Quality-Diversity through AI Feedback (QDAIF), wherein an
evolutionary algorithm applies LMs to both generate variation and evaluate the
quality and diversity of candidate text. When assessed on creative writing
domains, QDAIF covers more of a specified search space with high-quality
samples than do non-QD controls. Further, human evaluation of QDAIF-generated
creative texts validates reasonable agreement between AI and human evaluation.
Our results thus highlight the potential of AI feedback to guide open-ended
search for creative and original solutions, providing a recipe that seemingly
generalizes to many domains and modalities. In this way, QDAIF is a step
towards AI systems that can independently search, diversify, evaluate, and
improve, which are among the core skills underlying human society&apos;s capacity
for innovation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bradley_H/0/1/0/all/0/1&quot;&gt;Herbie Bradley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1&quot;&gt;Andrew Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teufel_H/0/1/0/all/0/1&quot;&gt;Hannah Teufel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jenny Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oostermeijer_K/0/1/0/all/0/1&quot;&gt;Koen Oostermeijer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellagente_M/0/1/0/all/0/1&quot;&gt;Marco Bellagente&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clune_J/0/1/0/all/0/1&quot;&gt;Jeff Clune&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stanley_K/0/1/0/all/0/1&quot;&gt;Kenneth Stanley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schott_G/0/1/0/all/0/1&quot;&gt;Gr&amp;#xe9;gory Schott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lehman_J/0/1/0/all/0/1&quot;&gt;Joel Lehman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18471">
<title>Causal disentanglement of multimodal data. (arXiv:2310.18471v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18471</link>
<description rdf:parseType="Literal">&lt;p&gt;Causal representation learning algorithms discover lower-dimensional
representations of data that admit a decipherable interpretation of cause and
effect; as achieving such interpretable representations is challenging, many
causal learning algorithms utilize elements indicating prior information, such
as (linear) structural causal models, interventional data, or weak supervision.
Unfortunately, in exploratory causal representation learning, such elements and
prior information may not be available or warranted. Alternatively, scientific
datasets often have multiple modalities or physics-based constraints, and the
use of such scientific, multimodal data has been shown to improve
disentanglement in fully unsupervised settings. Consequently, we introduce a
causal representation learning algorithm (causalPIMA) that can use multimodal
data and known physics to discover important features with causal
relationships. Our innovative algorithm utilizes a new differentiable
parametrization to learn a directed acyclic graph (DAG) together with a latent
space of a variational autoencoder in an end-to-end differentiable framework
via a single, tractable evidence lower bound loss function. We place a Gaussian
mixture prior on the latent space and identify each of the mixtures with an
outcome of the DAG nodes; this novel identification enables feature discovery
with causal relationships. Tested against a synthetic and a scientific dataset,
our results demonstrate the capability of learning an interpretable causal
structure while simultaneously discovering key features in a fully unsupervised
setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walker_E/0/1/0/all/0/1&quot;&gt;Elise Walker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Actor_J/0/1/0/all/0/1&quot;&gt;Jonas A. Actor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_C/0/1/0/all/0/1&quot;&gt;Carianne Martinez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trask_N/0/1/0/all/0/1&quot;&gt;Nathaniel Trask&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18626">
<title>Benchmark Generation Framework with Customizable Distortions for Image Classifier Robustness. (arXiv:2310.18626v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18626</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel framework for generating adversarial benchmarks to
evaluate the robustness of image classification models. Our framework allows
users to customize the types of distortions to be optimally applied to images,
which helps address the specific distortions relevant to their deployment. The
benchmark can generate datasets at various distortion levels to assess the
robustness of different image classifiers. Our results show that the
adversarial samples generated by our framework with any of the image
classification models, like ResNet-50, Inception-V3, and VGG-16, are effective
and transferable to other models causing them to fail. These failures happen
even when these models are adversarially retrained using state-of-the-art
techniques, demonstrating the generalizability of our adversarial samples. We
achieve competitive performance in terms of net $L_2$ distortion compared to
state-of-the-art benchmark techniques on CIFAR-10 and ImageNet; however, we
demonstrate our framework achieves such results with simple distortions like
Gaussian noise without introducing unnatural artifacts or color bleeds. This is
made possible by a model-based reinforcement learning (RL) agent and a
technique that reduces a deep tree search of the image for model sensitivity to
perturbations, to a one-level analysis and action. The flexibility of choosing
distortions and setting classification probability thresholds for multiple
classes makes our framework suitable for algorithmic audits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1&quot;&gt;Soumyendu Sarkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babu_A/0/1/0/all/0/1&quot;&gt;Ashwin Ramesh Babu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mousavi_S/0/1/0/all/0/1&quot;&gt;Sajad Mousavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carmichael_Z/0/1/0/all/0/1&quot;&gt;Zachariah Carmichael&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gundecha_V/0/1/0/all/0/1&quot;&gt;Vineet Gundecha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghorbanpour_S/0/1/0/all/0/1&quot;&gt;Sahand Ghorbanpour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luna_R/0/1/0/all/0/1&quot;&gt;Ricardo Luna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guillen_G/0/1/0/all/0/1&quot;&gt;Gutierrez Antonio Guillen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naug_A/0/1/0/all/0/1&quot;&gt;Avisek Naug&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18679">
<title>N-Critics: Self-Refinement of Large Language Models with Ensemble of Critics. (arXiv:2310.18679v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18679</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a self-correction mechanism for Large Language Models (LLMs) to
mitigate issues such as toxicity and fact hallucination. This method involves
refining model outputs through an ensemble of critics and the model&apos;s own
feedback. Drawing inspiration from human behavior, we explore whether LLMs can
emulate the self-correction process observed in humans who often engage in
self-reflection and seek input from others to refine their understanding of
complex topics. Our approach is model-agnostic and can be applied across
various domains to enhance trustworthiness by addressing fairness, bias, and
robustness concerns. We consistently observe performance improvements in LLMs
for reducing toxicity and correcting factual errors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mousavi_S/0/1/0/all/0/1&quot;&gt;Sajad Mousavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gutierrez_R/0/1/0/all/0/1&quot;&gt;Ricardo Luna Guti&amp;#xe9;rrez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rengarajan_D/0/1/0/all/0/1&quot;&gt;Desik Rengarajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gundecha_V/0/1/0/all/0/1&quot;&gt;Vineet Gundecha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babu_A/0/1/0/all/0/1&quot;&gt;Ashwin Ramesh Babu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naug_A/0/1/0/all/0/1&quot;&gt;Avisek Naug&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guillen_A/0/1/0/all/0/1&quot;&gt;Antonio Guillen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1&quot;&gt;Soumyendu Sarkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.20654">
<title>Interpretability, Generalizability, and Memory of Reinforcement Learning Agents in Closed Drafting Games. (arXiv:2310.20654v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.20654</link>
<description rdf:parseType="Literal">&lt;p&gt;Closed drafting or &quot;pick and pass&quot; is a popular game mechanic where each
round players select a card or other playable element from their hand and pass
the rest to the next player. In this paper, we establish first-principle
interpretability, generalizability, and memory benchmarks for studying
model-free reinforcement learning (RL) algorithms playing closed drafting
games. Specifically in a popular family of closed drafting games called &quot;Sushi
Go Party!&quot;, in which we achieve state-of-the-art performance. We fit decision
rules to interpret the strategy of trained RL agents and compare these to the
ranking preferences of different types of human players, finding easily
understandable explanations of the disparate performance of RL agents in this
environment. As Sushi Go Party! can be expressed as a set of closely-related
games based on the set of cards in play, we quantify the generalizability of RL
models trained on various sets of cards, establishing key trends between
performance and the set distance between the train and evaluation game
configurations. Using the explicitly calculable memory of other player&apos;s hands
in closed drafting games, we create measures of the ability of RL models to
learn memory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rezai_R/0/1/0/all/0/1&quot;&gt;Ryan Rezai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jason Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00322">
<title>Robust Graph Clustering via Meta Weighting for Noisy Graphs. (arXiv:2311.00322v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00322</link>
<description rdf:parseType="Literal">&lt;p&gt;How can we find meaningful clusters in a graph robustly against noise edges?
Graph clustering (i.e., dividing nodes into groups of similar ones) is a
fundamental problem in graph analysis with applications in various fields.
Recent studies have demonstrated that graph neural network (GNN) based
approaches yield promising results for graph clustering. However, we observe
that their performance degenerates significantly on graphs with noise edges,
which are prevalent in practice. In this work, we propose MetaGC for robust
GNN-based graph clustering. MetaGC employs a decomposable clustering loss
function, which can be rephrased as a sum of losses over node pairs. We add a
learnable weight to each node pair, and MetaGC adaptively adjusts the weights
of node pairs using meta-weighting so that the weights of meaningful node pairs
increase and the weights of less-meaningful ones (e.g., noise edges) decrease.
We show empirically that MetaGC learns weights as intended and consequently
outperforms the state-of-the-art GNN-based competitors, even when they are
equipped with separate denoising schemes, on five real-world graphs under
varying levels of noise. Our code and datasets are available at
https://github.com/HyeonsooJo/MetaGC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jo_H/0/1/0/all/0/1&quot;&gt;Hyeonsoo Jo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bu_F/0/1/0/all/0/1&quot;&gt;Fanchen Bu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_K/0/1/0/all/0/1&quot;&gt;Kijung Shin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01007">
<title>Effective Human-AI Teams via Learned Natural Language Rules and Onboarding. (arXiv:2311.01007v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01007</link>
<description rdf:parseType="Literal">&lt;p&gt;People are relying on AI agents to assist them with various tasks. The human
must know when to rely on the agent, collaborate with the agent, or ignore its
suggestions. In this work, we propose to learn rules, grounded in data regions
and described in natural language, that illustrate how the human should
collaborate with the AI. Our novel region discovery algorithm finds local
regions in the data as neighborhoods in an embedding space where prior human
behavior should be corrected. Each region is then described using a large
language model in an iterative and contrastive procedure. We then teach these
rules to the human via an onboarding stage. Through user studies on object
detection and question-answering tasks, we show that our method can lead to
more accurate human-AI teams. We also evaluate our region discovery and
description algorithms separately.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mozannar_H/0/1/0/all/0/1&quot;&gt;Hussein Mozannar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jimin J Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1&quot;&gt;Dennis Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sattigeri_P/0/1/0/all/0/1&quot;&gt;Prasanna Sattigeri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1&quot;&gt;Subhro Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sontag_D/0/1/0/all/0/1&quot;&gt;David Sontag&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02382">
<title>Ultra-Long Sequence Distributed Transformer. (arXiv:2311.02382v2 [cs.DC] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02382</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer models trained on long sequences often achieve higher accuracy
than short sequences. Unfortunately, conventional transformers struggle with
long sequence training due to the overwhelming computation and memory
requirements. Existing methods for long sequence training offer limited speedup
and memory reduction, and may compromise accuracy. This paper presents a novel
and efficient distributed training method, the Long Short-Sequence Transformer
(LSS Transformer), for training transformer with long sequences. It distributes
a long sequence into segments among GPUs, with each GPU computing a partial
self-attention for its segment. Then, it uses a fused communication and a novel
double gradient averaging technique to avoid the need to aggregate partial
self-attention and minimize communication overhead. We evaluated the
performance between LSS Transformer and the state-of-the-art Nvidia sequence
parallelism on a Wikipedia enwik8 dataset. Results show that our proposed
method lead to 5.6x faster and 10.2x more memory-efficient implementation
compared to state-of-the-art sequence parallelism on 144 Nvidia V100 GPUs.
Moreover, our algorithm scales to an extreme sequence length of 50,112 at 3,456
GPUs, achieving 161% super-linear parallel efficiency and a throughput of 32
petaflops.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyngaas_I/0/1/0/all/0/1&quot;&gt;Isaac Lyngaas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsaris_A/0/1/0/all/0/1&quot;&gt;Aristeidis Tsaris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Peng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dash_S/0/1/0/all/0/1&quot;&gt;Sajal Dash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shekar_M/0/1/0/all/0/1&quot;&gt;Mayanka Chandra Shekar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1&quot;&gt;Tao Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_H/0/1/0/all/0/1&quot;&gt;Hong-Jun Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wahib_M/0/1/0/all/0/1&quot;&gt;Mohamed Wahib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gouley_J/0/1/0/all/0/1&quot;&gt;John Gouley&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02847">
<title>Kinematic-aware Prompting for Generalizable Articulated Object Manipulation with LLMs. (arXiv:2311.02847v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02847</link>
<description rdf:parseType="Literal">&lt;p&gt;Generalizable articulated object manipulation is essential for home-assistant
robots. Recent efforts focus on imitation learning from demonstrations or
reinforcement learning in simulation, however, due to the prohibitive costs of
real-world data collection and precise object simulation, it still remains
challenging for these works to achieve broad adaptability across diverse
articulated objects. Recently, many works have tried to utilize the strong
in-context learning ability of Large Language Models (LLMs) to achieve
generalizable robotic manipulation, but most of these researches focus on
high-level task planning, sidelining low-level robotic control. In this work,
building on the idea that the kinematic structure of the object determines how
we can manipulate it, we propose a kinematic-aware prompting framework that
prompts LLMs with kinematic knowledge of objects to generate low-level motion
trajectory waypoints, supporting various object manipulation. To effectively
prompt LLMs with the kinematic structure of different objects, we design a
unified kinematic knowledge parser, which represents various articulated
objects as a unified textual description containing kinematic joints and
contact location. Building upon this unified description, a kinematic-aware
planner model is proposed to generate precise 3D manipulation waypoints via a
designed kinematic-aware chain-of-thoughts prompting method. Our evaluation
spanned 48 instances across 16 distinct categories, revealing that our
framework not only outperforms traditional methods on 8 seen categories but
also shows a powerful zero-shot capability for 8 unseen articulated object
categories. Moreover, the real-world experiments on 7 different object
categories prove our framework&apos;s adaptability in practical scenarios. Code is
released at
\href{https://github.com/GeWu-Lab/LLM_articulated_object_manipulation/tree/main}{here}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1&quot;&gt;Wenke Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_X/0/1/0/all/0/1&quot;&gt;Xincheng Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhigang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1&quot;&gt;Bin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1&quot;&gt;Di Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02926">
<title>Deep Image Semantic Communication Model for Artificial Intelligent Internet of Things. (arXiv:2311.02926v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02926</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rapid development of Artificial Intelligent Internet of Things
(AIoT), the image data from AIoT devices has been witnessing the explosive
increasing. In this paper, a novel deep image semantic communication model is
proposed for the efficient image communication in AIoT. Particularly, at the
transmitter side, a high-precision image semantic segmentation algorithm is
proposed to extract the semantic information of the image to achieve
significant compression of the image data. At the receiver side, a semantic
image restoration algorithm based on Generative Adversarial Network (GAN) is
proposed to convert the semantic image to a real scene image with detailed
information. Simulation results demonstrate that the proposed image semantic
communication model can improve the image compression ratio and recovery
accuracy by 71.93% and 25.07% on average in comparison with WebP and CycleGAN,
respectively. More importantly, our demo experiment shows that the proposed
model reduces the total delay by 95.26% in the image communication, when
comparing with the original image transmission.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_L/0/1/0/all/0/1&quot;&gt;Li Ping Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1&quot;&gt;Sikai Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Huijie Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1&quot;&gt;Xuemin Sherman Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaoniu Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03084">
<title>A Simple yet Efficient Ensemble Approach for AI-generated Text Detection. (arXiv:2311.03084v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03084</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent Large Language Models (LLMs) have demonstrated remarkable capabilities
in generating text that closely resembles human writing across wide range of
styles and genres. However, such capabilities are prone to potential abuse,
such as fake news generation, spam email creation, and misuse in academic
assignments. Hence, it is essential to build automated approaches capable of
distinguishing between artificially generated text and human-authored text. In
this paper, we propose a simple yet efficient solution to this problem by
ensembling predictions from multiple constituent LLMs. Compared to previous
state-of-the-art approaches, which are perplexity-based or uses ensembles with
a number of LLMs, our condensed ensembling approach uses only two constituent
LLMs to achieve comparable performance. Experiments conducted on four benchmark
datasets for generative text classification show performance improvements in
the range of 0.5 to 100\% compared to previous state-of-the-art approaches. We
also study the influence that the training data from individual LLMs have on
model performance. We found that substituting commercially-restrictive
Generative Pre-trained Transformer (GPT) data with data generated from other
open language models such as Falcon, Large Language Model Meta AI (LLaMA2), and
Mosaic Pretrained Transformers (MPT) is a feasible alternative when developing
generative text detectors. Furthermore, to demonstrate zero-shot
generalization, we experimented with an English essays dataset, and results
suggest that our ensembling approach can handle new data effectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abburi_H/0/1/0/all/0/1&quot;&gt;Harika Abburi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1&quot;&gt;Kalyani Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suesserman_M/0/1/0/all/0/1&quot;&gt;Michael Suesserman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pudota_N/0/1/0/all/0/1&quot;&gt;Nirmala Pudota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veeramani_B/0/1/0/all/0/1&quot;&gt;Balaji Veeramani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bowen_E/0/1/0/all/0/1&quot;&gt;Edward Bowen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1&quot;&gt;Sanmitra Bhattacharya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03340">
<title>Multitask Kernel-based Learning with First-Order Logic Constraints. (arXiv:2311.03340v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03340</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we propose a general framework to integrate supervised and
unsupervised examples with background knowledge expressed by a collection of
first-order logic clauses into kernel machines. In particular, we consider a
multi-task learning scheme where multiple predicates defined on a set of
objects are to be jointly learned from examples, enforcing a set of FOL
constraints on the admissible configurations of their values. The predicates
are defined on the feature spaces, in which the input objects are represented,
and can be either known a priori or approximated by an appropriate kernel-based
learner. A general approach is presented to convert the FOL clauses into a
continuous implementation that can deal with the outputs computed by the
kernel-based predicates. The learning problem is formulated as a
semi-supervised task that requires the optimization in the primal of a loss
function that combines a fitting loss measure on the supervised examples, a
regularization term, and a penalty term that enforces the constraints on both
the supervised and unsupervised examples. Unfortunately, the penalty term is
not convex and it can hinder the optimization process. However, it is possible
to avoid poor solutions by using a two stage learning schema, in which the
supervised examples are learned first and then the constraints are enforced.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diligenti_M/0/1/0/all/0/1&quot;&gt;Michelangelo Diligenti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gori_M/0/1/0/all/0/1&quot;&gt;Marco Gori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maggini_M/0/1/0/all/0/1&quot;&gt;Marco Maggini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rigutini_L/0/1/0/all/0/1&quot;&gt;Leonardo Rigutini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03429">
<title>ProPath: Disease-Specific Protein Language Model for Variant Pathogenicity. (arXiv:2311.03429v2 [q-bio.GN] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03429</link>
<description rdf:parseType="Literal">&lt;p&gt;Clinical variant classification of pathogenic versus benign genetic variants
remains a pivotal challenge in clinical genetics. Recently, the proposition of
protein language models has improved the generic variant effect prediction
(VEP) accuracy via weakly-supervised or unsupervised training. However, these
VEPs are not disease-specific, limiting their adaptation at point-of-care. To
address this problem, we propose a disease-specific \textsc{pro}tein language
model for variant \textsc{path}ogenicity, termed ProPath, to capture the
pseudo-log-likelihood ratio in rare missense variants through a siamese
network. We evaluate the performance of ProPath against pre-trained language
models, using clinical variant sets in inherited cardiomyopathies and
arrhythmias that were not seen during training. Our results demonstrate that
ProPath surpasses the pre-trained ESM1b with an over $5\%$ improvement in AUC
across both datasets. Furthermore, our model achieved the highest performances
across all baselines for both datasets. Thus, our ProPath offers a potent
disease-specific variant effect prediction, particularly valuable for disease
associations and clinical applicability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhan_H/0/1/0/all/0/1&quot;&gt;Huixin Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zijun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03989">
<title>Learned Causal Method Prediction. (arXiv:2311.03989v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03989</link>
<description rdf:parseType="Literal">&lt;p&gt;For a given causal question, it is important to efficiently decide which
causal inference method to use for a given dataset. This is challenging because
causal methods typically rely on complex and difficult-to-verify assumptions,
and cross-validation is not applicable since ground truth causal quantities are
unobserved. In this work, we propose CAusal Method Predictor (CAMP), a
framework for predicting the best method for a given dataset. To this end, we
generate datasets from a diverse set of synthetic causal models, score the
candidate methods, and train a model to directly predict the highest-scoring
method for that dataset. Next, by formulating a self-supervised pre-training
objective centered on dataset assumptions relevant for causal inference, we
significantly reduce the need for costly labeled data and enhance training
efficiency. Our strategy learns to map implicit dataset properties to the best
method in a data-driven manner. In our experiments, we focus on method
prediction for causal discovery. CAMP outperforms selecting any individual
candidate method and demonstrates promising generalization to unseen
semi-synthetic and real-world benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1&quot;&gt;Shantanu Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Cheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hilmkil_A/0/1/0/all/0/1&quot;&gt;Agrin Hilmkil&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>