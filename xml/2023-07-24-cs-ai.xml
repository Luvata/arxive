<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-23T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11099" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11105" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11114" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11128" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11133" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11137" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11166" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11206" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11224" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11242" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11253" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11286" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11288" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11308" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11317" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11319" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11323" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11325" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11335" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11341" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11343" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11346" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11373" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11388" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11397" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11411" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11413" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11434" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11449" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11452" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11457" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11462" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11468" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11469" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11471" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11494" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11499" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11503" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11516" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11519" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11525" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11544" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11554" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11563" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11610" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11621" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11637" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11643" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11650" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11655" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11661" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11688" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11694" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11709" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11719" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11730" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.08227" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.10736" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.07392" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.11723" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.09648" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.09559" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.04973" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.11630" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.15471" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.17555" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12851" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18451" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.03933" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07458" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14096" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15079" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03070" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03512" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04541" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06092" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08167" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09004" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09782" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10490" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10577" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10617" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10711" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10991" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.11099">
<title>Solving multiphysics-based inverse problems with learned surrogates and constraints. (arXiv:2307.11099v1 [physics.geo-ph])</title>
<link>http://arxiv.org/abs/2307.11099</link>
<description rdf:parseType="Literal">&lt;p&gt;Solving multiphysics-based inverse problems for geological carbon storage
monitoring can be challenging when multimodal time-lapse data are expensive to
collect and costly to simulate numerically. We overcome these challenges by
combining computationally cheap learned surrogates with learned constraints.
Not only does this combination lead to vastly improved inversions for the
important fluid-flow property, permeability, it also provides a natural
platform for inverting multimodal data including well measurements and
active-source time-lapse seismic data. By adding a learned constraint, we
arrive at a computationally feasible inversion approach that remains accurate.
This is accomplished by including a trained deep neural network, known as a
normalizing flow, which forces the model iterates to remain in-distribution,
thereby safeguarding the accuracy of trained Fourier neural operators that act
as surrogates for the computationally expensive multiphase flow simulations
involving partial differential equation solves. By means of carefully selected
experiments, centered around the problem of geological carbon storage, we
demonstrate the efficacy of the proposed constrained optimization method on two
different data modalities, namely time-lapse well and time-lapse seismic data.
While permeability inversions from both these two modalities have their pluses
and minuses, their joint inversion benefits from either, yielding valuable
superior permeability inversions and CO2 plume predictions near, and far away,
from the monitoring wells.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Ziyi Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Orozco_R/0/1/0/all/0/1&quot;&gt;Rafael Orozco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Louboutin_M/0/1/0/all/0/1&quot;&gt;Mathias Louboutin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Herrmann_F/0/1/0/all/0/1&quot;&gt;Felix J. Herrmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11105">
<title>Technical Challenges of Deploying Reinforcement Learning Agents for Game Testing in AAA Games. (arXiv:2307.11105v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2307.11105</link>
<description rdf:parseType="Literal">&lt;p&gt;Going from research to production, especially for large and complex software
systems, is fundamentally a hard problem. In large-scale game production, one
of the main reasons is that the development environment can be very different
from the final product. In this technical paper we describe an effort to add an
experimental reinforcement learning system to an existing automated game
testing solution based on scripted bots in order to increase its capacity. We
report on how this reinforcement learning system was integrated with the aim to
increase test coverage similar to [1] in a set of AAA games including
Battlefield 2042 and Dead Space (2023). The aim of this technical paper is to
show a use-case of leveraging reinforcement learning in game production and
cover some of the largest time sinks anyone who wants to make the same journey
for their game may encounter. Furthermore, to help the game industry to adopt
this technology faster, we propose a few research directions that we believe
will be valuable and necessary for making machine learning, and especially
reinforcement learning, an effective tool in game production.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gillberg_J/0/1/0/all/0/1&quot;&gt;Jonas Gillberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bergdahl_J/0/1/0/all/0/1&quot;&gt;Joakim Bergdahl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sestini_A/0/1/0/all/0/1&quot;&gt;Alessandro Sestini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eakins_A/0/1/0/all/0/1&quot;&gt;Andrew Eakins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gisslen_L/0/1/0/all/0/1&quot;&gt;Linus Gisslen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11114">
<title>Nature of Intelligence. (arXiv:2307.11114v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/2307.11114</link>
<description rdf:parseType="Literal">&lt;p&gt;The human brain is the substrate for human intelligence. By simulating the
human brain, artificial intelligence builds computational models that have
learning capabilities and perform intelligent tasks approaching the human
level. Deep neural networks consist of multiple computation layers to learn
representations of data and improve the state-of-the-art in many recognition
domains. However, the essence of intelligence commonly represented by both
humans and AI is unknown. Here, we show that the nature of intelligence is a
series of mathematically functional processes that minimize system entropy by
establishing functional relationships between datasets over space and time.
Humans and AI have achieved intelligence by implementing these entropy-reducing
processes in a reinforced manner that consumes energy. With this hypothesis, we
establish mathematical models of language, unconsciousness and consciousness,
predicting the evidence to be found by neuroscience and achieved by AI
engineering. Furthermore, a conclusion is made that the total entropy of the
universe is conservative, and intelligence counters the spontaneous processes
to decrease entropy by physically or informationally connecting datasets that
originally exist in the universe but are separated across space and time. This
essay should be a starting point for a deeper understanding of the universe and
us as human beings and for achieving sophisticated AI models that are
tantamount to human intelligence or even superior. Furthermore, this essay
argues that more advanced intelligence than humans should exist if only it
reduces entropy in a more efficient energy-consuming way.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+You_B/0/1/0/all/0/1&quot;&gt;Barco Jie You&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11128">
<title>Approximate Computing Survey, Part II: Application-Specific &amp; Architectural Approximation Techniques and Applications. (arXiv:2307.11128v1 [cs.AR])</title>
<link>http://arxiv.org/abs/2307.11128</link>
<description rdf:parseType="Literal">&lt;p&gt;The challenging deployment of compute-intensive applications from domains
such Artificial Intelligence (AI) and Digital Signal Processing (DSP), forces
the community of computing systems to explore new design approaches.
Approximate Computing appears as an emerging solution, allowing to tune the
quality of results in the design of a system in order to improve the energy
efficiency and/or performance. This radical paradigm shift has attracted
interest from both academia and industry, resulting in significant research on
approximation techniques and methodologies at different design layers (from
system down to integrated circuits). Motivated by the wide appeal of
Approximate Computing over the last 10 years, we conduct a two-part survey to
cover key aspects (e.g., terminology and applications) and review the
state-of-the art approximation techniques from all layers of the traditional
computing stack. In Part II of our survey, we classify and present the
technical details of application-specific and architectural approximation
techniques, which both target the design of resource-efficient
processors/accelerators &amp;amp; systems. Moreover, we present a detailed analysis of
the application spectrum of Approximate Computing and discuss open challenges
and future directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leon_V/0/1/0/all/0/1&quot;&gt;Vasileios Leon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanif_M/0/1/0/all/0/1&quot;&gt;Muhammad Abdullah Hanif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Armeniakos_G/0/1/0/all/0/1&quot;&gt;Giorgos Armeniakos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_X/0/1/0/all/0/1&quot;&gt;Xun Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shafique_M/0/1/0/all/0/1&quot;&gt;Muhammad Shafique&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pekmestzi_K/0/1/0/all/0/1&quot;&gt;Kiamal Pekmestzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soudris_D/0/1/0/all/0/1&quot;&gt;Dimitrios Soudris&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11133">
<title>Contrastive Graph Pooling for Explainable Classification of Brain Networks. (arXiv:2307.11133v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/2307.11133</link>
<description rdf:parseType="Literal">&lt;p&gt;Functional magnetic resonance imaging (fMRI) is a commonly used technique to
measure neural activation. Its application has been particularly important in
identifying underlying neurodegenerative conditions such as Parkinson&apos;s,
Alzheimer&apos;s, and Autism. Recent analysis of fMRI data models the brain as a
graph and extracts features by graph neural networks (GNNs). However, the
unique characteristics of fMRI data require a special design of GNN. Tailoring
GNN to generate effective and domain-explainable features remains challenging.
In this paper, we propose a contrastive dual-attention block and a
differentiable graph pooling method called ContrastPool to better utilize GNN
for brain networks, meeting fMRI-specific requirements. We apply our method to
5 resting-state fMRI brain network datasets of 3 diseases and demonstrate its
superiority over state-of-the-art baselines. Our case study confirms that the
patterns extracted by our method match the domain knowledge in neuroscience
literature, and disclose direct and interesting insights. Our contributions
underscore the potential of ContrastPool for advancing the understanding of
brain networks and neurodegenerative conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiaxing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bian_Q/0/1/0/all/0/1&quot;&gt;Qingtian Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xinhang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Aihu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ke_Y/0/1/0/all/0/1&quot;&gt;Yiping Ke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Qiao_M/0/1/0/all/0/1&quot;&gt;Miao Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Sim_W/0/1/0/all/0/1&quot;&gt;Wei Khang Jeremy Sim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gulyas_B/0/1/0/all/0/1&quot;&gt;Bal&amp;#xe1;zs Guly&amp;#xe1;s&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11137">
<title>Of Models and Tin Men -- a behavioural economics study of principal-agent problems in AI alignment using large-language models. (arXiv:2307.11137v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.11137</link>
<description rdf:parseType="Literal">&lt;p&gt;AI Alignment is often presented as an interaction between a single designer
and an artificial agent in which the designer attempts to ensure the agent&apos;s
behavior is consistent with its purpose, and risks arise solely because of
conflicts caused by inadvertent misalignment between the utility function
intended by the designer and the resulting internal utility function of the
agent. With the advent of agents instantiated with large-language models
(LLMs), which are typically pre-trained, we argue this does not capture the
essential aspects of AI safety because in the real world there is not a
one-to-one correspondence between designer and agent, and the many agents, both
artificial and human, have heterogeneous values. Therefore, there is an
economic aspect to AI safety and the principal-agent problem is likely to
arise. In a principal-agent problem conflict arises because of information
asymmetry together with inherent misalignment between the utility of the agent
and its principal, and this inherent misalignment cannot be overcome by
coercing the agent into adopting a desired utility function through training.
We argue the assumptions underlying principal-agent problems are crucial to
capturing the essence of safety problems involving pre-trained AI models in
real-world situations. Taking an empirical approach to AI safety, we
investigate how GPT models respond in principal-agent conflicts. We find that
agents based on both GPT-3.5 and GPT-4 override their principal&apos;s objectives in
a simple online shopping task, showing clear evidence of principal-agent
conflict. Surprisingly, the earlier GPT-3.5 model exhibits more nuanced
behaviour in response to changes in information asymmetry, whereas the later
GPT-4 model is more rigid in adhering to its prior alignment. Our results
highlight the importance of incorporating principles from economics into the
alignment process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phelps_S/0/1/0/all/0/1&quot;&gt;Steve Phelps&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ranson_R/0/1/0/all/0/1&quot;&gt;Rebecca Ranson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11166">
<title>Exploring reinforcement learning techniques for discrete and continuous control tasks in the MuJoCo environment. (arXiv:2307.11166v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11166</link>
<description rdf:parseType="Literal">&lt;p&gt;We leverage the fast physics simulator, MuJoCo to run tasks in a continuous
control environment and reveal details like the observation space, action
space, rewards, etc. for each task. We benchmark value-based methods for
continuous control by comparing Q-learning and SARSA through a discretization
approach, and using them as baselines, progressively moving into one of the
state-of-the-art deep policy gradient method DDPG. Over a large number of
episodes, Qlearning outscored SARSA, but DDPG outperformed both in a small
number of episodes. Lastly, we also fine-tuned the model hyper-parameters
expecting to squeeze more performance but using lesser time and resources. We
anticipated that the new design for DDPG would vastly improve performance, yet
after only a few episodes, we were able to achieve decent average rewards. We
expect to improve the performance provided adequate time and computational
resources.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahul_V/0/1/0/all/0/1&quot;&gt;Vaddadi Sai Rahul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_D/0/1/0/all/0/1&quot;&gt;Debajyoti Chakraborty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11206">
<title>Towards Ontologically Grounded and Language-Agnostic Knowledge Graphs. (arXiv:2307.11206v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.11206</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge graphs (KGs) have become the standard technology for the
representation of factual information in applications such as recommendation
engines, search, and question-answering systems. However, the continual
updating of KGs, as well as the integration of KGs from different domains and
KGs in different languages, remains to be a major challenge. What we suggest
here is that by a reification of abstract objects and by acknowledging the
ontological distinction between concepts and types, we arrive at an
ontologically grounded and language-agnostic representation that can alleviate
the difficulties in KG integration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saba_W/0/1/0/all/0/1&quot;&gt;Walid S. Saba&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11224">
<title>Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models. (arXiv:2307.11224v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.11224</link>
<description rdf:parseType="Literal">&lt;p&gt;Jina Embeddings constitutes a set of high-performance sentence embedding
models adept at translating various textual inputs into numerical
representations, thereby capturing the semantic essence of the text. While
these models are not exclusively designed for text generation, they excel in
applications such as dense retrieval and semantic textual similarity. This
paper details the development of Jina Embeddings, starting with the creation of
a high-quality pairwise and triplet dataset. It underlines the crucial role of
data cleaning in dataset preparation, gives in-depth insights into the model
training process, and concludes with a comprehensive performance evaluation
using the Massive Textual Embedding Benchmark (MTEB).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunther_M/0/1/0/all/0/1&quot;&gt;Michael G&amp;#xfc;nther&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milliken_L/0/1/0/all/0/1&quot;&gt;Louis Milliken&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geuter_J/0/1/0/all/0/1&quot;&gt;Jonathan Geuter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mastrapas_G/0/1/0/all/0/1&quot;&gt;Georgios Mastrapas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1&quot;&gt;Han Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11242">
<title>On-Sensor Data Filtering using Neuromorphic Computing for High Energy Physics Experiments. (arXiv:2307.11242v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2307.11242</link>
<description rdf:parseType="Literal">&lt;p&gt;This work describes the investigation of neuromorphic computing-based spiking
neural network (SNN) models used to filter data from sensor electronics in high
energy physics experiments conducted at the High Luminosity Large Hadron
Collider. We present our approach for developing a compact neuromorphic model
that filters out the sensor data based on the particle&apos;s transverse momentum
with the goal of reducing the amount of data being sent to the downstream
electronics. The incoming charge waveforms are converted to streams of
binary-valued events, which are then processed by the SNN. We present our
insights on the various system design choices - from data encoding to optimal
hyperparameters of the training algorithm - for an accurate and compact SNN
optimized for hardware deployment. Our results show that an SNN trained with an
evolutionary algorithm and an optimized set of hyperparameters obtains a signal
efficiency of about 91% with nearly half as many parameters as a deep neural
network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulkarni_S/0/1/0/all/0/1&quot;&gt;Shruti R. Kulkarni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Young_A/0/1/0/all/0/1&quot;&gt;Aaron Young&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Date_P/0/1/0/all/0/1&quot;&gt;Prasanna Date&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miniskar_N/0/1/0/all/0/1&quot;&gt;Narasinga Rao Miniskar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vetter_J/0/1/0/all/0/1&quot;&gt;Jeffrey S. Vetter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fahim_F/0/1/0/all/0/1&quot;&gt;Farah Fahim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parpillon_B/0/1/0/all/0/1&quot;&gt;Benjamin Parpillon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dickinson_J/0/1/0/all/0/1&quot;&gt;Jennet Dickinson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_N/0/1/0/all/0/1&quot;&gt;Nhan Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1&quot;&gt;Jieun Yoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mills_C/0/1/0/all/0/1&quot;&gt;Corrinne Mills&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swartz_M/0/1/0/all/0/1&quot;&gt;Morris Swartz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maksimovic_P/0/1/0/all/0/1&quot;&gt;Petar Maksimovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuman_C/0/1/0/all/0/1&quot;&gt;Catherine D. Schuman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bean_A/0/1/0/all/0/1&quot;&gt;Alice Bean&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11253">
<title>Joint one-sided synthetic unpaired image translation and segmentation for colorectal cancer prevention. (arXiv:2307.11253v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11253</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has shown excellent performance in analysing medical images.
However, datasets are difficult to obtain due privacy issues, standardization
problems, and lack of annotations. We address these problems by producing
realistic synthetic images using a combination of 3D technologies and
generative adversarial networks. We propose CUT-seg, a joint training where a
segmentation model and a generative model are jointly trained to produce
realistic images while learning to segment polyps. We take advantage of recent
one-sided translation models because they use significantly less memory,
allowing us to add a segmentation model in the training loop. CUT-seg performs
better, is computationally less expensive, and requires less real images than
other memory-intensive image translation approaches that require two stage
training. Promising results are achieved on five real polyp segmentation
datasets using only one real image and zero real annotations. As a part of this
study we release Synth-Colon, an entirely synthetic dataset that includes 20000
realistic colon images and additional details about depth and 3D geometry:
https://enric1994.github.io/synth-colon
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moreu_E/0/1/0/all/0/1&quot;&gt;Enric Moreu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arazo_E/0/1/0/all/0/1&quot;&gt;Eric Arazo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McGuinness_K/0/1/0/all/0/1&quot;&gt;Kevin McGuinness&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OConnor_N/0/1/0/all/0/1&quot;&gt;Noel E. O&amp;#x27;Connor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11286">
<title>Eliminating Unintended Stable Fixpoints for Hybrid Reasoning Systems. (arXiv:2307.11286v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.11286</link>
<description rdf:parseType="Literal">&lt;p&gt;A wide variety of nonmonotonic semantics can be expressed as approximators
defined under AFT (Approximation Fixpoint Theory). Using traditional AFT
theory, it is not possible to define approximators that rely on information
computed in previous iterations of stable revision. However, this information
is rich for semantics that incorporate classical negation into nonmonotonic
reasoning. In this work, we introduce a methodology resembling AFT that can
utilize priorly computed upper bounds to more precisely capture semantics. We
demonstrate our framework&apos;s applicability to hybrid MKNF (minimal knowledge and
negation as failure) knowledge bases by extending the state-of-the-art
approximator.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Killen_S/0/1/0/all/0/1&quot;&gt;Spencer Killen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1&quot;&gt;Jia-Huai You&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11288">
<title>Kernelized Offline Contextual Dueling Bandits. (arXiv:2307.11288v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11288</link>
<description rdf:parseType="Literal">&lt;p&gt;Preference-based feedback is important for many applications where direct
evaluation of a reward function is not feasible. A notable recent example
arises in reinforcement learning from human feedback on large language models.
For many of these applications, the cost of acquiring the human feedback can be
substantial or even prohibitive. In this work, we take advantage of the fact
that often the agent can choose contexts at which to obtain human feedback in
order to most efficiently identify a good policy, and introduce the offline
contextual dueling bandit setting. We give an upper-confidence-bound style
algorithm for this setting and prove a regret bound. We also give empirical
confirmation that this method outperforms a similar strategy that uses
uniformly sampled contexts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehta_V/0/1/0/all/0/1&quot;&gt;Viraj Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neopane_O/0/1/0/all/0/1&quot;&gt;Ojash Neopane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_V/0/1/0/all/0/1&quot;&gt;Vikramjeet Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1&quot;&gt;Sen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1&quot;&gt;Jeff Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neiswanger_W/0/1/0/all/0/1&quot;&gt;Willie Neiswanger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11308">
<title>DPM-OT: A New Diffusion Probabilistic Model Based on Optimal Transport. (arXiv:2307.11308v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11308</link>
<description rdf:parseType="Literal">&lt;p&gt;Sampling from diffusion probabilistic models (DPMs) can be viewed as a
piecewise distribution transformation, which generally requires hundreds or
thousands of steps of the inverse diffusion trajectory to get a high-quality
image. Recent progress in designing fast samplers for DPMs achieves a trade-off
between sampling speed and sample quality by knowledge distillation or
adjusting the variance schedule or the denoising equation. However, it can&apos;t be
optimal in both aspects and often suffer from mode mixture in short steps. To
tackle this problem, we innovatively regard inverse diffusion as an optimal
transport (OT) problem between latents at different stages and propose the
DPM-OT, a unified learning framework for fast DPMs with a direct expressway
represented by OT map, which can generate high-quality samples within around 10
function evaluations. By calculating the semi-discrete optimal transport map
between the data latents and the white noise, we obtain an expressway from the
prior distribution to the data distribution, while significantly alleviating
the problem of mode mixture. In addition, we give the error bound of the
proposed method, which theoretically guarantees the stability of the algorithm.
Extensive experiments validate the effectiveness and advantages of DPM-OT in
terms of speed and quality (FID and mode mixture), thus representing an
efficient solution for generative modeling. Source codes are available at
https://github.com/cognaclee/DPM-OT
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zezeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;ShengHao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhanpeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_N/0/1/0/all/0/1&quot;&gt;Na Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Zhongxuan Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1&quot;&gt;Xianfeng Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11317">
<title>XLDA: Linear Discriminant Analysis for Scaling Continual Learning to Extreme Classification at the Edge. (arXiv:2307.11317v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11317</link>
<description rdf:parseType="Literal">&lt;p&gt;Streaming Linear Discriminant Analysis (LDA) while proven in
Class-incremental Learning deployments at the edge with limited classes (upto
1000), has not been proven for deployment in extreme classification scenarios.
In this paper, we present: (a) XLDA, a framework for Class-IL in edge
deployment where LDA classifier is proven to be equivalent to FC layer
including in extreme classification scenarios, and (b) optimizations to enable
XLDA-based training and inference for edge deployment where there is a
constraint on available compute resources. We show up to 42x speed up using a
batched training approach and up to 5x inference speedup with nearest neighbor
search on extreme datasets like AliProducts (50k classes) and Google Landmarks
V2 (81k classes)
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_K/0/1/0/all/0/1&quot;&gt;Karan Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veerendranath_V/0/1/0/all/0/1&quot;&gt;Vishruth Veerendranath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hebbar_A/0/1/0/all/0/1&quot;&gt;Anushka Hebbar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhat_R/0/1/0/all/0/1&quot;&gt;Raghavendra Bhat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11319">
<title>How to Tidy Up a Table: Fusing Visual and Semantic Commonsense Reasoning for Robotic Tasks with Vague Objectives. (arXiv:2307.11319v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.11319</link>
<description rdf:parseType="Literal">&lt;p&gt;Vague objectives in many real-life scenarios pose long-standing challenges
for robotics, as defining rules, rewards, or constraints for optimization is
difficult. Tasks like tidying a messy table may appear simple for humans, but
articulating the criteria for tidiness is complex due to the ambiguity and
flexibility in commonsense reasoning. Recent advancement in Large Language
Models (LLMs) offers us an opportunity to reason over these vague objectives:
learned from extensive human data, LLMs capture meaningful common sense about
human behavior. However, as LLMs are trained solely on language input, they may
struggle with robotic tasks due to their limited capacity to account for
perception and low-level controls. In this work, we propose a simple approach
to solve the task of table tidying, an example of robotic tasks with vague
objectives. Specifically, the task of tidying a table involves not just
clustering objects by type and functionality for semantic tidiness but also
considering spatial-visual relations of objects for a visually pleasing
arrangement, termed as visual tidiness. We propose to learn a lightweight,
image-based tidiness score function to ground the semantically tidy policy of
LLMs to achieve visual tidiness. We innovatively train the tidiness score using
synthetic data gathered using random walks from a few tidy configurations. Such
trajectories naturally encode the order of tidiness, thereby eliminating the
need for laborious and expensive human demonstrations. Our empirical results
show that our pipeline can be applied to unseen objects and complex 3D
arrangements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yiqing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_D/0/1/0/all/0/1&quot;&gt;David Hsu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11323">
<title>HVDetFusion: A Simple and Robust Camera-Radar Fusion Framework. (arXiv:2307.11323v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11323</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of autonomous driving, 3D object detection is a very important
perception module. Although the current SOTA algorithm combines Camera and
Lidar sensors, limited by the high price of Lidar, the current mainstream
landing schemes are pure Camera sensors or Camera+Radar sensors. In this study,
we propose a new detection algorithm called HVDetFusion, which is a multi-modal
detection algorithm that not only supports pure camera data as input for
detection, but also can perform fusion input of radar data and camera data. The
camera stream does not depend on the input of Radar data, thus addressing the
downside of previous methods. In the pure camera stream, we modify the
framework of Bevdet4D for better perception and more efficient inference, and
this stream has the whole 3D detection output. Further, to incorporate the
benefits of Radar signals, we use the prior information of different object
positions to filter the false positive information of the original radar data,
according to the positioning information and radial velocity information
recorded by the radar sensors to supplement and fuse the BEV features generated
by the original camera data, and the effect is further improved in the process
of fusion training. Finally, HVDetFusion achieves the new state-of-the-art
67.4\% NDS on the challenging nuScenes test set among all camera-radar 3D
object detectors. The code is available at
https://github.com/HVXLab/HVDetFusion
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_K/0/1/0/all/0/1&quot;&gt;Kai Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_S/0/1/0/all/0/1&quot;&gt;Shuman Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoteng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11325">
<title>Analysis of Elephant Movement in Sub-Saharan Africa: Ecological, Climatic, and Conservation Perspectives. (arXiv:2307.11325v1 [q-bio.PE])</title>
<link>http://arxiv.org/abs/2307.11325</link>
<description rdf:parseType="Literal">&lt;p&gt;The interaction between elephants and their environment has profound
implications for both ecology and conservation strategies. This study presents
an analytical approach to decipher the intricate patterns of elephant movement
in Sub-Saharan Africa, concentrating on key ecological drivers such as seasonal
variations and rainfall patterns. Despite the complexities surrounding these
influential factors, our analysis provides a holistic view of elephant
migratory behavior in the context of the dynamic African landscape. Our
comprehensive approach enables us to predict the potential impact of these
ecological determinants on elephant migration, a critical step in establishing
informed conservation strategies. This projection is particularly crucial given
the impacts of global climate change on seasonal and rainfall patterns, which
could substantially influence elephant movements in the future. The findings of
our work aim to not only advance the understanding of movement ecology but also
foster a sustainable coexistence of humans and elephants in Sub-Saharan Africa.
By predicting potential elephant routes, our work can inform strategies to
minimize human-elephant conflict, effectively manage land use, and enhance
anti-poaching efforts. This research underscores the importance of integrating
movement ecology and climatic variables for effective wildlife management and
conservation planning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hines_M/0/1/0/all/0/1&quot;&gt;Matthew Hines&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Glatzer_G/0/1/0/all/0/1&quot;&gt;Gregory Glatzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Shreya Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Mitra_P/0/1/0/all/0/1&quot;&gt;Prasenjit Mitra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11335">
<title>Tri-MipRF: Tri-Mip Representation for Efficient Anti-Aliasing Neural Radiance Fields. (arXiv:2307.11335v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11335</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the tremendous progress in neural radiance fields (NeRF), we still
face a dilemma of the trade-off between quality and efficiency, e.g., MipNeRF
presents fine-detailed and anti-aliased renderings but takes days for training,
while Instant-ngp can accomplish the reconstruction in a few minutes but
suffers from blurring or aliasing when rendering at various distances or
resolutions due to ignoring the sampling area. To this end, we propose a novel
Tri-Mip encoding that enables both instant reconstruction and anti-aliased
high-fidelity rendering for neural radiance fields. The key is to factorize the
pre-filtered 3D feature spaces in three orthogonal mipmaps. In this way, we can
efficiently perform 3D area sampling by taking advantage of 2D pre-filtered
feature maps, which significantly elevates the rendering quality without
sacrificing efficiency. To cope with the novel Tri-Mip representation, we
propose a cone-casting rendering technique to efficiently sample anti-aliased
3D features with the Tri-Mip encoding considering both pixel imaging and
observing distance. Extensive experiments on both synthetic and real-world
datasets demonstrate our method achieves state-of-the-art rendering quality and
reconstruction speed while maintaining a compact representation that reduces
25% model size compared against Instant-ngp.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1&quot;&gt;Wenbo Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuling Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bangbang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1&quot;&gt;Lin Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yuewen Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11341">
<title>OpenGDA: Graph Domain Adaptation Benchmark for Cross-network Learning. (arXiv:2307.11341v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.11341</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph domain adaptation models are widely adopted in cross-network learning
tasks, with the aim of transferring labeling or structural knowledge.
Currently, there mainly exist two limitations in evaluating graph domain
adaptation models. On one side, they are primarily tested for the specific
cross-network node classification task, leaving tasks at edge-level and
graph-level largely under-explored. Moreover, they are primarily tested in
limited scenarios, such as social networks or citation networks, lacking
validation of model&apos;s capability in richer scenarios. As comprehensively
assessing models could enhance model practicality in real-world applications,
we propose a benchmark, known as OpenGDA. It provides abundant pre-processed
and unified datasets for different types of tasks (node, edge, graph). They
originate from diverse scenarios, covering web information systems, urban
systems and natural systems. Furthermore, it integrates state-of-the-art models
with standardized and end-to-end pipelines. Overall, OpenGDA provides a
user-friendly, scalable and reproducible benchmark for evaluating graph domain
adaptation models. The benchmark experiments highlight the challenges of
applying GDA models to real-world applications with consistent good
performance, and potentially provide insights to future research. As an
emerging project, OpenGDA will be regularly updated with new datasets and
models. It could be accessed from https://github.com/Skyorca/OpenGDA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Boshen Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yongqing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1&quot;&gt;Fangda Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1&quot;&gt;Jiangli Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Huawei Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xueqi Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11343">
<title>A Two-stage Fine-tuning Strategy for Generalizable Manipulation Skill of Embodied AI. (arXiv:2307.11343v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.11343</link>
<description rdf:parseType="Literal">&lt;p&gt;The advent of Chat-GPT has led to a surge of interest in Embodied AI.
However, many existing Embodied AI models heavily rely on massive interactions
with training environments, which may not be practical in real-world
situations. To this end, the Maniskill2 has introduced a full-physics
simulation benchmark for manipulating various 3D objects. This benchmark
enables agents to be trained using diverse datasets of demonstrations and
evaluates their ability to generalize to unseen scenarios in testing
environments. In this paper, we propose a novel two-stage fine-tuning strategy
that aims to further enhance the generalization capability of our model based
on the Maniskill2 benchmark. Through extensive experiments, we demonstrate the
effectiveness of our approach by achieving the 1st prize in all three tracks of
the ManiSkill2 Challenge. Our findings highlight the potential of our method to
improve the generalization abilities of Embodied AI models and pave the way for
their ractical applications in real-world scenarios. All codes and models of
our solution is available at https://github.com/xtli12/GXU-LIPE.git
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1&quot;&gt;Fang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;XueTao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jun Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaung_F/0/1/0/all/0/1&quot;&gt;Feng Shaung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11346">
<title>CohortGPT: An Enhanced GPT for Participant Recruitment in Clinical Study. (arXiv:2307.11346v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.11346</link>
<description rdf:parseType="Literal">&lt;p&gt;Participant recruitment based on unstructured medical texts such as clinical
notes and radiology reports has been a challenging yet important task for the
cohort establishment in clinical research. Recently, Large Language Models
(LLMs) such as ChatGPT have achieved tremendous success in various downstream
tasks thanks to their promising performance in language understanding,
inference, and generation. It is then natural to test their feasibility in
solving the cohort recruitment task, which involves the classification of a
given paragraph of medical text into disease label(s). However, when applied to
knowledge-intensive problem settings such as medical text classification, where
the LLMs are expected to understand the decision made by human experts and
accurately identify the implied disease labels, the LLMs show a mediocre
performance. A possible explanation is that, by only using the medical text,
the LLMs neglect to use the rich context of additional information that
languages afford. To this end, we propose to use a knowledge graph as auxiliary
information to guide the LLMs in making predictions. Moreover, to further boost
the LLMs adapt to the problem setting, we apply a chain-of-thought (CoT) sample
selection strategy enhanced by reinforcement learning, which selects a set of
CoT samples given each individual medical report. Experimental results and
various ablation studies show that our few-shot learning method achieves
satisfactory performance compared with fine-tuning strategies and gains superb
advantages when the available data is limited. The code and sample dataset of
the proposed CohortGPT model is available at:
https://anonymous.4open.science/r/CohortGPT-4872/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_Z/0/1/0/all/0/1&quot;&gt;Zihan Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zihao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhengliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Dufan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1&quot;&gt;Hui Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Quanzheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Ninghao Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11373">
<title>Diverse Offline Imitation via Fenchel Duality. (arXiv:2307.11373v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11373</link>
<description rdf:parseType="Literal">&lt;p&gt;There has been significant recent progress in the area of unsupervised skill
discovery, with various works proposing mutual information based objectives, as
a source of intrinsic motivation. Prior works predominantly focused on
designing algorithms that require online access to the environment. In
contrast, we develop an \textit{offline} skill discovery algorithm. Our problem
formulation considers the maximization of a mutual information objective
constrained by a KL-divergence. More precisely, the constraints ensure that the
state occupancy of each skill remains close to the state occupancy of an
expert, within the support of an offline dataset with good state-action
coverage. Our main contribution is to connect Fenchel duality, reinforcement
learning and unsupervised skill discovery, and to give a simple offline
algorithm for learning diverse skills that are aligned with an expert.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vlastelica_M/0/1/0/all/0/1&quot;&gt;Marin Vlastelica&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolev_P/0/1/0/all/0/1&quot;&gt;Pavel Kolev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jin Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martius_G/0/1/0/all/0/1&quot;&gt;Georg Martius&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11388">
<title>Large Language Model-based System to Provide Immediate Feedback to Students in Flipped Classroom Preparation Learning. (arXiv:2307.11388v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2307.11388</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a system that uses large language models to provide
immediate feedback to students in flipped classroom preparation learning. This
study aimed to solve challenges in the flipped classroom model, such as
ensuring that students are emotionally engaged and motivated to learn. Students
often have questions about the content of lecture videos in the preparation of
flipped classrooms, but it is difficult for teachers to answer them
immediately. The proposed system was developed using the ChatGPT API on a
video-watching support system for preparation learning that is being used in
real practice. Answers from ChatGPT often do not align with the context of the
student&apos;s question. Therefore, this paper also proposes a method to align the
answer with the context. This paper also proposes a method to collect the
teacher&apos;s answers to the students&apos; questions and use them as additional guides
for the students. This paper discusses the design and implementation of the
proposed system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uchiyama_S/0/1/0/all/0/1&quot;&gt;Shintaro Uchiyama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Umemura_K/0/1/0/all/0/1&quot;&gt;Kyoji Umemura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morita_Y/0/1/0/all/0/1&quot;&gt;Yusuke Morita&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11397">
<title>Probabilistic Modeling of Inter- and Intra-observer Variability in Medical Image Segmentation. (arXiv:2307.11397v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.11397</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical image segmentation is a challenging task, particularly due to inter-
and intra-observer variability, even between medical experts. In this paper, we
propose a novel model, called Probabilistic Inter-Observer and iNtra-Observer
variation NetwOrk (Pionono). It captures the labeling behavior of each rater
with a multidimensional probability distribution and integrates this
information with the feature maps of the image to produce probabilistic
segmentation predictions. The model is optimized by variational inference and
can be trained end-to-end. It outperforms state-of-the-art models such as
STAPLE, Probabilistic U-Net, and models based on confusion matrices.
Additionally, Pionono predicts multiple coherent segmentation maps that mimic
the rater&apos;s expert opinion, which provides additional valuable information for
the diagnostic process. Experiments on real-world cancer segmentation datasets
demonstrate the high accuracy and efficiency of Pionono, making it a powerful
tool for medical image analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schmidt_A/0/1/0/all/0/1&quot;&gt;Arne Schmidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Morales_Alvarez_P/0/1/0/all/0/1&quot;&gt;Pablo Morales-&amp;#xc1;lvarez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Molina_R/0/1/0/all/0/1&quot;&gt;Rafael Molina&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11411">
<title>Deep Directly-Trained Spiking Neural Networks for Object Detection. (arXiv:2307.11411v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11411</link>
<description rdf:parseType="Literal">&lt;p&gt;Spiking neural networks (SNNs) are brain-inspired energy-efficient models
that encode information in spatiotemporal dynamics. Recently, deep SNNs trained
directly have shown great success in achieving high performance on
classification tasks with very few time steps. However, how to design a
directly-trained SNN for the regression task of object detection still remains
a challenging problem. To address this problem, we propose EMS-YOLO, a novel
directly-trained SNN framework for object detection, which is the first trial
to train a deep SNN with surrogate gradients for object detection rather than
ANN-SNN conversion strategies. Specifically, we design a full-spike residual
block, EMS-ResNet, which can effectively extend the depth of the
directly-trained SNN with low power consumption. Furthermore, we theoretically
analyze and prove the EMS-ResNet could avoid gradient vanishing or exploding.
The results demonstrate that our approach outperforms the state-of-the-art
ANN-SNN conversion methods (at least 500 time steps) in extremely fewer time
steps (only 4 time steps). It is shown that our model could achieve comparable
performance to the ANN with the same architecture while consuming 5.83 times
less energy on the frame-based COCO Dataset and the event-based Gen1 Dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1&quot;&gt;Qiaoyi Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chou_Y/0/1/0/all/0/1&quot;&gt;Yuhong Chou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yifan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_S/0/1/0/all/0/1&quot;&gt;Shijie Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Ziyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guoqi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11413">
<title>A Video-based Detector for Suspicious Activity in Examination with OpenPose. (arXiv:2307.11413v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11413</link>
<description rdf:parseType="Literal">&lt;p&gt;Examinations are a crucial part of the learning process, and academic
institutions invest significant resources into maintaining their integrity by
preventing cheating from students or facilitators. However, cheating has become
rampant in examination setups, compromising their integrity. The traditional
method of relying on invigilators to monitor every student is impractical and
ineffective. To address this issue, there is a need to continuously record exam
sessions to monitor students for suspicious activities. However, these
recordings are often too lengthy for invigilators to analyze effectively, and
fatigue may cause them to miss significant details. To widen the coverage,
invigilators could use fixed overhead or wearable cameras. This paper
introduces a framework that uses automation to analyze videos and detect
suspicious activities during examinations efficiently and effectively. We
utilized the OpenPose framework and Convolutional Neural Network (CNN) to
identify students exchanging objects during exams. This detection system is
vital in preventing cheating and promoting academic integrity, fairness, and
quality education for institutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moyo_R/0/1/0/all/0/1&quot;&gt;Reuben Moyo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ndebvu_S/0/1/0/all/0/1&quot;&gt;Stanley Ndebvu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zimba_M/0/1/0/all/0/1&quot;&gt;Michael Zimba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mbelwa_J/0/1/0/all/0/1&quot;&gt;Jimmy Mbelwa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11434">
<title>Batching for Green AI -- An Exploratory Study on Inference. (arXiv:2307.11434v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11434</link>
<description rdf:parseType="Literal">&lt;p&gt;The batch size is an essential parameter to tune during the development of
new neural networks. Amongst other quality indicators, it has a large degree of
influence on the model&apos;s accuracy, generalisability, training times and
parallelisability. This fact is generally known and commonly studied. However,
during the application phase of a deep learning model, when the model is
utilised by an end-user for inference, we find that there is a disregard for
the potential benefits of introducing a batch size. In this study, we examine
the effect of input batching on the energy consumption and response times of
five fully-trained neural networks for computer vision that were considered
state-of-the-art at the time of their publication. The results suggest that
batching has a significant effect on both of these metrics. Furthermore, we
present a timeline of the energy efficiency and accuracy of neural networks
over the past decade. We find that in general, energy consumption rises at a
much steeper pace than accuracy and question the necessity of this evolution.
Additionally, we highlight one particular network, ShuffleNetV2(2018), that
achieved a competitive performance for its time while maintaining a much lower
energy consumption. Nevertheless, we highlight that the results are model
dependent.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yarally_T/0/1/0/all/0/1&quot;&gt;Tim Yarally&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cruz_L/0/1/0/all/0/1&quot;&gt;Lu&amp;#xed;s Cruz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feitosa_D/0/1/0/all/0/1&quot;&gt;Daniel Feitosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sallou_J/0/1/0/all/0/1&quot;&gt;June Sallou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deursen_A/0/1/0/all/0/1&quot;&gt;Arie van Deursen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11449">
<title>AIGC Empowering Telecom Sector White Paper. (arXiv:2307.11449v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.11449</link>
<description rdf:parseType="Literal">&lt;p&gt;In the global craze of GPT, people have deeply realized that AI, as a
transformative technology and key force in economic and social development,
will bring great leaps and breakthroughs to the global industry and profoundly
influence the future world competition pattern. As the builder and operator of
information and communication infrastructure, the telecom sector provides
infrastructure support for the development of AI, and even takes the lead in
the implementation of AI applications. How to enable the application of AIGC
(GPT) and implement AIGC in the telecom sector are questions that telecom
practitioners must ponder and answer. Through the study of GPT, a typical
representative of AIGC, the authors have analyzed how GPT empowers the telecom
sector in the form of scenarios, discussed the gap between the current GPT
general model and telecom services, proposed for the first time a Telco
Augmented Cognition capability system, provided answers to how to construct a
telecom service GPT in the telecom sector, and carried out various practices.
Our counterparts in the industry are expected to focus on collaborative
innovation around telecom and AI, build an open and shared innovation
ecosystem, promote the deep integration of AI and telecom sector, and
accelerate the construction of next-generation information infrastructure, in
an effort to facilitate the digital transformation of the economy and society.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_Y/0/1/0/all/0/1&quot;&gt;Ye Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yaqin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1&quot;&gt;Xiaozhou Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yunxin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yong Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_S/0/1/0/all/0/1&quot;&gt;Sen Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11452">
<title>Providing personalized Explanations: a Conversational Approach. (arXiv:2307.11452v1 [cs.MA])</title>
<link>http://arxiv.org/abs/2307.11452</link>
<description rdf:parseType="Literal">&lt;p&gt;The increasing applications of AI systems require personalized explanations
for their behaviors to various stakeholders since the stakeholders may have
various knowledge and backgrounds. In general, a conversation between
explainers and explainees not only allows explainers to obtain the explainees&apos;
background, but also allows explainees to better understand the explanations.
In this paper, we propose an approach for an explainer to communicate
personalized explanations to an explainee through having consecutive
conversations with the explainee. We prove that the conversation terminates due
to the explainee&apos;s justification of the initial claim as long as there exists
an explanation for the initial claim that the explainee understands and the
explainer is aware of.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Jieting Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Studer_T/0/1/0/all/0/1&quot;&gt;Thomas Studer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dastani_M/0/1/0/all/0/1&quot;&gt;Mehdi Dastani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11457">
<title>Incorporating Human Translator Style into English-Turkish Literary Machine Translation. (arXiv:2307.11457v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.11457</link>
<description rdf:parseType="Literal">&lt;p&gt;Although machine translation systems are mostly designed to serve in the
general domain, there is a growing tendency to adapt these systems to other
domains like literary translation. In this paper, we focus on English-Turkish
literary translation and develop machine translation models that take into
account the stylistic features of translators. We fine-tune a pre-trained
machine translation model by the manually-aligned works of a particular
translator. We make a detailed analysis of the effects of manual and automatic
alignments, data augmentation methods, and corpus size on the translations. We
propose an approach based on stylistic features to evaluate the style of a
translator in the output translations. We show that the human translator style
can be highly recreated in the target machine translations by adapting the
models to the style of the translator.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yirmibesoglu_Z/0/1/0/all/0/1&quot;&gt;Zeynep Yirmibe&amp;#x15f;o&amp;#x11f;lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dursun_O/0/1/0/all/0/1&quot;&gt;Olgun Dursun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dalli_H/0/1/0/all/0/1&quot;&gt;Harun Dall&amp;#x131;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sahin_M/0/1/0/all/0/1&quot;&gt;Mehmet &amp;#x15e;ahin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hodzik_E/0/1/0/all/0/1&quot;&gt;Ena Hodzik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurses_S/0/1/0/all/0/1&quot;&gt;Sabri G&amp;#xfc;rses&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gungor_T/0/1/0/all/0/1&quot;&gt;Tunga G&amp;#xfc;ng&amp;#xf6;r&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11462">
<title>Improve Long-term Memory Learning Through Rescaling the Error Temporally. (arXiv:2307.11462v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11462</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies the error metric selection for long-term memory learning
in sequence modelling. We examine the bias towards short-term memory in
commonly used errors, including mean absolute/squared error. Our findings show
that all temporally positive-weighted errors are biased towards short-term
memory in learning linear functionals. To reduce this bias and improve
long-term memory learning, we propose the use of a temporally rescaled error.
In addition to reducing the bias towards short-term memory, this approach can
also alleviate the vanishing gradient issue. We conduct numerical experiments
on different long-memory tasks and sequence models to validate our claims.
Numerical results confirm the importance of appropriate temporally rescaled
error for effective long-term memory learning. To the best of our knowledge,
this is the first work that quantitatively analyzes different errors&apos; memory
bias towards short-term memory in sequence modelling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shida Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zhanglu Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11468">
<title>Zero-touch realization of Pervasive Artificial Intelligence-as-a-service in 6G networks. (arXiv:2307.11468v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.11468</link>
<description rdf:parseType="Literal">&lt;p&gt;The vision of the upcoming 6G technologies, characterized by ultra-dense
network, low latency, and fast data rate is to support Pervasive AI (PAI) using
zero-touch solutions enabling self-X (e.g., self-configuration,
self-monitoring, and self-healing) services. However, the research on 6G is
still in its infancy, and only the first steps have been taken to conceptualize
its design, investigate its implementation, and plan for use cases. Toward this
end, academia and industry communities have gradually shifted from theoretical
studies of AI distribution to real-world deployment and standardization. Still,
designing an end-to-end framework that systematizes the AI distribution by
allowing easier access to the service using a third-party application assisted
by a zero-touch service provisioning has not been well explored. In this
context, we introduce a novel platform architecture to deploy a zero-touch
PAI-as-a-Service (PAIaaS) in 6G networks supported by a blockchain-based smart
system. This platform aims to standardize the pervasive AI at all levels of the
architecture and unify the interfaces in order to facilitate the service
deployment across application and infrastructure domains, relieve the users
worries about cost, security, and resource allocation, and at the same time,
respect the 6G stringent performance requirements. As a proof of concept, we
present a Federated Learning-as-a-service use case where we evaluate the
ability of our proposed system to self-optimize and self-adapt to the dynamics
of 6G networks in addition to minimizing the users&apos; perceived costs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baccour_E/0/1/0/all/0/1&quot;&gt;Emna Baccour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allahham_M/0/1/0/all/0/1&quot;&gt;Mhd Saria Allahham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erbad_A/0/1/0/all/0/1&quot;&gt;Aiman Erbad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1&quot;&gt;Amr Mohamed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hussein_A/0/1/0/all/0/1&quot;&gt;Ahmed Refaey Hussein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamdi_M/0/1/0/all/0/1&quot;&gt;Mounir Hamdi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11469">
<title>Distribution Shift Matters for Knowledge Distillation with Webly Collected Images. (arXiv:2307.11469v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11469</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge distillation aims to learn a lightweight student network from a
pre-trained teacher network. In practice, existing knowledge distillation
methods are usually infeasible when the original training data is unavailable
due to some privacy issues and data management considerations. Therefore,
data-free knowledge distillation approaches proposed to collect training
instances from the Internet. However, most of them have ignored the common
distribution shift between the instances from original training data and webly
collected data, affecting the reliability of the trained student network. To
solve this problem, we propose a novel method dubbed ``Knowledge Distillation
between Different Distributions&quot; (KD$^{3}$), which consists of three
components. Specifically, we first dynamically select useful training instances
from the webly collected data according to the combined predictions of teacher
network and student network. Subsequently, we align both the weighted features
and classifier parameters of the two networks for knowledge memorization.
Meanwhile, we also build a new contrastive learning block called
MixDistribution to generate perturbed data with a new distribution for instance
alignment, so that the student network can further learn a
distribution-invariant representation. Intensive experiments on various
benchmark datasets demonstrate that our proposed KD$^{3}$ can outperform the
state-of-the-art data-free knowledge distillation approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jialiang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1&quot;&gt;Gang Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1&quot;&gt;Masashi Sugiyama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1&quot;&gt;Chen Gong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11471">
<title>Robust Visual Question Answering: Datasets, Methods, and Future Challenges. (arXiv:2307.11471v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11471</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual question answering requires a system to provide an accurate natural
language answer given an image and a natural language question. However, it is
widely recognized that previous generic VQA methods often exhibit a tendency to
memorize biases present in the training data rather than learning proper
behaviors, such as grounding images before predicting answers. Therefore, these
methods usually achieve high in-distribution but poor out-of-distribution
performance. In recent years, various datasets and debiasing methods have been
proposed to evaluate and enhance the VQA robustness, respectively. This paper
provides the first comprehensive survey focused on this emerging fashion.
Specifically, we first provide an overview of the development process of
datasets from in-distribution and out-of-distribution perspectives. Then, we
examine the evaluation metrics employed by these datasets. Thirdly, we propose
a typology that presents the development process, similarities and differences,
robustness comparison, and technical features of existing debiasing methods.
Furthermore, we analyze and discuss the robustness of representative
vision-and-language pre-training models on VQA. Finally, through a thorough
review of the available literature and experimental analysis, we discuss the
key areas for future research from various viewpoints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jie Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Pinghui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1&quot;&gt;Dechen Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zewei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_H/0/1/0/all/0/1&quot;&gt;Hongbin Pei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Junzhou Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11494">
<title>Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting. (arXiv:2307.11494v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11494</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have achieved state-of-the-art performance in generative
modeling tasks across various domains. Prior works on time series diffusion
models have primarily focused on developing conditional models tailored to
specific forecasting or imputation tasks. In this work, we explore the
potential of task-agnostic, unconditional diffusion models for several time
series applications. We propose TSDiff, an unconditionally trained diffusion
model for time series. Our proposed self-guidance mechanism enables
conditioning TSDiff for downstream tasks during inference, without requiring
auxiliary networks or altering the training procedure. We demonstrate the
effectiveness of our method on three different time series tasks: forecasting,
refinement, and synthetic data generation. First, we show that TSDiff is
competitive with several task-specific conditional forecasting methods
(predict). Second, we leverage the learned implicit probability density of
TSDiff to iteratively refine the predictions of base forecasters with reduced
computational overhead over reverse diffusion (refine). Notably, the generative
performance of the model remains intact -- downstream forecasters trained on
synthetic samples from TSDiff outperform forecasters that are trained on
samples from other state-of-the-art generative time series models, occasionally
even outperforming models trained on real data (synthesize).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kollovieh_M/0/1/0/all/0/1&quot;&gt;Marcel Kollovieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ansari_A/0/1/0/all/0/1&quot;&gt;Abdul Fatir Ansari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bohlke_Schneider_M/0/1/0/all/0/1&quot;&gt;Michael Bohlke-Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zschiegner_J/0/1/0/all/0/1&quot;&gt;Jasper Zschiegner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuyang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11499">
<title>Adaptive ResNet Architecture for Distributed Inference in Resource-Constrained IoT Systems. (arXiv:2307.11499v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.11499</link>
<description rdf:parseType="Literal">&lt;p&gt;As deep neural networks continue to expand and become more complex, most edge
devices are unable to handle their extensive processing requirements.
Therefore, the concept of distributed inference is essential to distribute the
neural network among a cluster of nodes. However, distribution may lead to
additional energy consumption and dependency among devices that suffer from
unstable transmission rates. Unstable transmission rates harm real-time
performance of IoT devices causing low latency, high energy usage, and
potential failures. Hence, for dynamic systems, it is necessary to have a
resilient DNN with an adaptive architecture that can downsize as per the
available resources. This paper presents an empirical study that identifies the
connections in ResNet that can be dropped without significantly impacting the
model&apos;s performance to enable distribution in case of resource shortage. Based
on the results, a multi-objective optimization problem is formulated to
minimize latency and maximize accuracy as per available resources. Our
experiments demonstrate that an adaptive ResNet architecture can reduce shared
data, energy consumption, and latency throughout the distribution while
maintaining high accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1&quot;&gt;Fazeela Mazhar Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baccour_E/0/1/0/all/0/1&quot;&gt;Emna Baccour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erbad_A/0/1/0/all/0/1&quot;&gt;Aiman Erbad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamdi_M/0/1/0/all/0/1&quot;&gt;Mounir Hamdi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11503">
<title>General regularization in covariate shift adaptation. (arXiv:2307.11503v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11503</link>
<description rdf:parseType="Literal">&lt;p&gt;Sample reweighting is one of the most widely used methods for correcting the
error of least squares learning algorithms in reproducing kernel Hilbert spaces
(RKHS), that is caused by future data distributions that are different from the
training data distribution. In practical situations, the sample weights are
determined by values of the estimated Radon-Nikod\&apos;ym derivative, of the future
data distribution w.r.t.~the training data distribution. In this work, we
review known error bounds for reweighted kernel regression in RKHS and obtain,
by combination, novel results. We show under weak smoothness conditions, that
the amount of samples, needed to achieve the same order of accuracy as in the
standard supervised learning without differences in data distributions, is
smaller than proven by state-of-the-art analyses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Duc Hoan Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pereverzyev_S/0/1/0/all/0/1&quot;&gt;Sergei V. Pereverzyev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zellinger_W/0/1/0/all/0/1&quot;&gt;Werner Zellinger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11516">
<title>IndigoVX: Where Human Intelligence Meets AI for Optimal Decision Making. (arXiv:2307.11516v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.11516</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper defines a new approach for augmenting human intelligence with AI
for optimal goal solving. Our proposed AI, Indigo, is an acronym for Informed
Numerical Decision-making through Iterative Goal-Oriented optimization. When
combined with a human collaborator, we term the joint system IndigoVX, for
Virtual eXpert. The system is conceptually simple. We envisage this method
being applied to games or business strategies, with the human providing
strategic context and the AI offering optimal, data-driven moves. Indigo
operates through an iterative feedback loop, harnessing the human expert&apos;s
contextual knowledge and the AI&apos;s data-driven insights to craft and refine
strategies towards a well-defined goal. Using a quantified three-score schema,
this hybridization allows the combined team to evaluate strategies and refine
their plan, while adapting to challenges and changes in real-time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dukes_K/0/1/0/all/0/1&quot;&gt;Kais Dukes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11519">
<title>Multi-modal Hate Speech Detection using Machine Learning. (arXiv:2307.11519v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.11519</link>
<description rdf:parseType="Literal">&lt;p&gt;With the continuous growth of internet users and media content, it is very
hard to track down hateful speech in audio and video. Converting video or audio
into text does not detect hate speech accurately as human sometimes uses
hateful words as humorous or pleasant in sense and also uses different voice
tones or show different action in the video. The state-ofthe-art hate speech
detection models were mostly developed on a single modality. In this research,
a combined approach of multimodal system has been proposed to detect hate
speech from video contents by extracting feature images, feature values
extracted from the audio, text and used machine learning and Natural language
processing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boishakhi_F/0/1/0/all/0/1&quot;&gt;Fariha Tahosin Boishakhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shill_P/0/1/0/all/0/1&quot;&gt;Ponkoj Chandra Shill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1&quot;&gt;Md. Golam Rabiul Alam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11525">
<title>Model Reporting for Certifiable AI: A Proposal from Merging EU Regulation into AI Development. (arXiv:2307.11525v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.11525</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite large progress in Explainable and Safe AI, practitioners suffer from
a lack of regulation and standards for AI safety. In this work we merge recent
regulation efforts by the European Union and first proposals for AI guidelines
with recent trends in research: data and model cards. We propose the use of
standardized cards to document AI applications throughout the development
process. Our main contribution is the introduction of use-case and operation
cards, along with updates for data and model cards to cope with regulatory
requirements. We reference both recent research as well as the source of the
regulation in our cards and provide references to additional support material
and toolboxes whenever possible. The goal is to design cards that help
practitioners develop safe AI systems throughout the development process, while
enabling efficient third-party auditing of AI applications, being easy to
understand, and building trust in the system. Our work incorporates insights
from interviews with certification experts as well as developers and
individuals working with the developed AI applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brajovic_D/0/1/0/all/0/1&quot;&gt;Danilo Brajovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Renner_N/0/1/0/all/0/1&quot;&gt;Niclas Renner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goebels_V/0/1/0/all/0/1&quot;&gt;Vincent Philipp Goebels&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagner_P/0/1/0/all/0/1&quot;&gt;Philipp Wagner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fresz_B/0/1/0/all/0/1&quot;&gt;Benjamin Fresz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biller_M/0/1/0/all/0/1&quot;&gt;Martin Biller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klaeb_M/0/1/0/all/0/1&quot;&gt;Mara Klaeb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kutz_J/0/1/0/all/0/1&quot;&gt;Janika Kutz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neuhuettler_J/0/1/0/all/0/1&quot;&gt;Jens Neuhuettler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huber_M/0/1/0/all/0/1&quot;&gt;Marco F. Huber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11544">
<title>Identifying Relevant Features of CSE-CIC-IDS2018 Dataset for the Development of an Intrusion Detection System. (arXiv:2307.11544v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.11544</link>
<description rdf:parseType="Literal">&lt;p&gt;Intrusion detection systems (IDSs) are essential elements of IT systems.
Their key component is a classification module that continuously evaluates some
features of the network traffic and identifies possible threats. Its efficiency
is greatly affected by the right selection of the features to be monitored.
Therefore, the identification of a minimal set of features that are necessary
to safely distinguish malicious traffic from benign traffic is indispensable in
the course of the development of an IDS. This paper presents the preprocessing
and feature selection workflow as well as its results in the case of the
CSE-CIC-IDS2018 on AWS dataset, focusing on five attack types. To identify the
relevant features, six feature selection methods were applied, and the final
ranking of the features was elaborated based on their average score. Next,
several subsets of the features were formed based on different ranking
threshold values, and each subset was tried with five classification algorithms
to determine the optimal feature set for each attack type. During the
evaluation, four widely used metrics were taken into consideration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gocs_L/0/1/0/all/0/1&quot;&gt;L&amp;#xe1;szl&amp;#xf3; G&amp;#xf6;cs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johanyak_Z/0/1/0/all/0/1&quot;&gt;Zsolt Csaba Johany&amp;#xe1;k&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11554">
<title>CycleIK: Neuro-inspired Inverse Kinematics. (arXiv:2307.11554v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.11554</link>
<description rdf:parseType="Literal">&lt;p&gt;The paper introduces CycleIK, a neuro-robotic approach that wraps two novel
neuro-inspired methods for the inverse kinematics (IK) task, a Generative
Adversarial Network (GAN), and a Multi-Layer Perceptron architecture. These
methods can be used in a standalone fashion, but we also show how embedding
these into a hybrid neuro-genetic IK pipeline allows for further optimization
via sequential least-squares programming (SLSQP) or a genetic algorithm (GA).
The models are trained and tested on dense datasets that were collected from
random robot configurations of the new Neuro-Inspired COLlaborator (NICOL), a
semi-humanoid robot with two redundant 8-DoF manipulators. We utilize the
weighted multi-objective function from the state-of-the-art BioIK method to
support the training process and our hybrid neuro-genetic architecture. We show
that the neural models can compete with state-of-the-art IK approaches, which
allows for deployment directly to robotic hardware. Additionally, it is shown
that the incorporation of the genetic algorithm improves the precision while
simultaneously reducing the overall runtime.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Habekost_J/0/1/0/all/0/1&quot;&gt;Jan-Gerrit Habekost&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strahl_E/0/1/0/all/0/1&quot;&gt;Erik Strahl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allgeuer_P/0/1/0/all/0/1&quot;&gt;Philipp Allgeuer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kerzel_M/0/1/0/all/0/1&quot;&gt;Matthias Kerzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1&quot;&gt;Stefan Wermter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11563">
<title>Feature Map Testing for Deep Neural Networks. (arXiv:2307.11563v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2307.11563</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the widespread application of deep neural networks~(DNNs) in
safety-critical tasks, deep learning testing has drawn increasing attention.
During the testing process, test cases that have been fuzzed or selected using
test metrics are fed into the model to find fault-inducing test units (e.g.,
neurons and feature maps, activating which will almost certainly result in a
model error) and report them to the DNN developer, who subsequently repair
them~(e.g., retraining the model with test cases). Current test metrics,
however, are primarily concerned with the neurons, which means that test cases
that are discovered either by guided fuzzing or selection with these metrics
focus on detecting fault-inducing neurons while failing to detect
fault-inducing feature maps.
&lt;/p&gt;
&lt;p&gt;In this work, we propose DeepFeature, which tests DNNs from the feature map
level. When testing is conducted, DeepFeature will scrutinize every internal
feature map in the model and identify vulnerabilities that can be enhanced
through repairing to increase the model&apos;s overall performance. Exhaustive
experiments are conducted to demonstrate that (1) DeepFeature is a strong tool
for detecting the model&apos;s vulnerable feature maps; (2) DeepFeature&apos;s test case
selection has a high fault detection rate and can detect more types of
faults~(comparing DeepFeature to coverage-guided selection techniques, the
fault detection rate is increased by 49.32\%). (3) DeepFeature&apos;s fuzzer also
outperforms current fuzzing techniques and generates valuable test cases more
efficiently.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1&quot;&gt;Dong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bu_Q/0/1/0/all/0/1&quot;&gt;Qingwen Bu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qing_Y/0/1/0/all/0/1&quot;&gt;Yahao Qing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yichao Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1&quot;&gt;Heming Cui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11610">
<title>CausE: Towards Causal Knowledge Graph Embedding. (arXiv:2307.11610v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.11610</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge graph embedding (KGE) focuses on representing the entities and
relations of a knowledge graph (KG) into the continuous vector spaces, which
can be employed to predict the missing triples to achieve knowledge graph
completion (KGC). However, KGE models often only briefly learn structural
correlations of triple data and embeddings would be misled by the trivial
patterns and noisy links in real-world KGs. To address this issue, we build the
new paradigm of KGE in the context of causality and embedding disentanglement.
We further propose a Causality-enhanced knowledge graph Embedding (CausE)
framework. CausE employs causal intervention to estimate the causal effect of
the confounder embeddings and design new training objectives to make stable
predictions. Experimental results demonstrate that CausE could outperform the
baseline models and achieve state-of-the-art KGC performance. We release our
code in https://github.com/zjukg/CausE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yichi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wen Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11621">
<title>On the Complexity of the Bipartite Polarization Problem: from Neutral to Highly Polarized Discussions. (arXiv:2307.11621v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.11621</link>
<description rdf:parseType="Literal">&lt;p&gt;The Bipartite Polarization Problem is an optimization problem where the goal
is to find the highest polarized bipartition on a weighted and labelled graph
that represents a debate developed through some social network, where nodes
represent user&apos;s opinions and edges agreement or disagreement between users.
This problem can be seen as a generalization of the maxcut problem, and in
previous work approximate solutions and exact solutions have been obtained for
real instances obtained from Reddit discussions, showing that such real
instances seem to be very easy to solve. In this paper, we investigate further
the complexity of this problem, by introducing an instance generation model
where a single parameter controls the polarization of the instances in such a
way that this correlates with the average complexity to solve those instances.
The average complexity results we obtain are consistent with our hypothesis:
the higher the polarization of the instance, the easier is to find the
corresponding polarized bipartition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alsinet_T/0/1/0/all/0/1&quot;&gt;Teresa Alsinet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Argelich_J/0/1/0/all/0/1&quot;&gt;Josep Argelich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bejar_R/0/1/0/all/0/1&quot;&gt;Ram&amp;#xf3;n B&amp;#xe9;jar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_S/0/1/0/all/0/1&quot;&gt;Santi Mart&amp;#xed;nez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11637">
<title>Integration of Domain Expert-Centric Ontology Design into the CRISP-DM for Cyber-Physical Production Systems. (arXiv:2307.11637v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.11637</link>
<description rdf:parseType="Literal">&lt;p&gt;In the age of Industry 4.0 and Cyber-Physical Production Systems (CPPSs) vast
amounts of potentially valuable data are being generated. Methods from Machine
Learning (ML) and Data Mining (DM) have proven to be promising in extracting
complex and hidden patterns from the data collected. The knowledge obtained can
in turn be used to improve tasks like diagnostics or maintenance planning.
However, such data-driven projects, usually performed with the Cross-Industry
Standard Process for Data Mining (CRISP-DM), often fail due to the
disproportionate amount of time needed for understanding and preparing the
data. The application of domain-specific ontologies has demonstrated its
advantageousness in a wide variety of Industry 4.0 application scenarios
regarding the aforementioned challenges. However, workflows and artifacts from
ontology design for CPPSs have not yet been systematically integrated into the
CRISP-DM. Accordingly, this contribution intends to present an integrated
approach so that data scientists are able to more quickly and reliably gain
insights into the CPPS. The result is exemplarily applied to an anomaly
detection use case.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gill_M/0/1/0/all/0/1&quot;&gt;Milapji Singh Gill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Westermann_T/0/1/0/all/0/1&quot;&gt;Tom Westermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schieseck_M/0/1/0/all/0/1&quot;&gt;Marvin Schieseck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fay_A/0/1/0/all/0/1&quot;&gt;Alexander Fay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11643">
<title>Morphological Image Analysis and Feature Extraction for Reasoning with AI-based Defect Detection and Classification Models. (arXiv:2307.11643v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11643</link>
<description rdf:parseType="Literal">&lt;p&gt;As the use of artificial intelligent (AI) models becomes more prevalent in
industries such as engineering and manufacturing, it is essential that these
models provide transparent reasoning behind their predictions. This paper
proposes the AI-Reasoner, which extracts the morphological characteristics of
defects (DefChars) from images and utilises decision trees to reason with the
DefChar values. Thereafter, the AI-Reasoner exports visualisations (i.e.
charts) and textual explanations to provide insights into outputs made by
masked-based defect detection and classification models. It also provides
effective mitigation strategies to enhance data pre-processing and overall
model performance. The AI-Reasoner was tested on explaining the outputs of an
IE Mask R-CNN model using a set of 366 images containing defects. The results
demonstrated its effectiveness in explaining the IE Mask R-CNN model&apos;s
predictions. Overall, the proposed AI-Reasoner provides a solution for
improving the performance of AI models in industrial applications that require
defect analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiajun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cosma_G/0/1/0/all/0/1&quot;&gt;Georgina Cosma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bugby_S/0/1/0/all/0/1&quot;&gt;Sarah Bugby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finke_A/0/1/0/all/0/1&quot;&gt;Axel Finke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watkins_J/0/1/0/all/0/1&quot;&gt;Jason Watkins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11650">
<title>Alleviating the Long-Tail Problem in Conversational Recommender Systems. (arXiv:2307.11650v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2307.11650</link>
<description rdf:parseType="Literal">&lt;p&gt;Conversational recommender systems (CRS) aim to provide the recommendation
service via natural language conversations. To develop an effective CRS,
high-quality CRS datasets are very crucial. However, existing CRS datasets
suffer from the long-tail issue, \ie a large proportion of items are rarely (or
even never) mentioned in the conversations, which are called long-tail items.
As a result, the CRSs trained on these datasets tend to recommend frequent
items, and the diversity of the recommended items would be largely reduced,
making users easier to get bored.
&lt;/p&gt;
&lt;p&gt;To address this issue, this paper presents \textbf{LOT-CRS}, a novel
framework that focuses on simulating and utilizing a balanced CRS dataset (\ie
covering all the items evenly) for improving \textbf{LO}ng-\textbf{T}ail
recommendation performance of CRSs. In our approach, we design two pre-training
tasks to enhance the understanding of simulated conversation for long-tail
items, and adopt retrieval-augmented fine-tuning with label smoothness strategy
to further improve the recommendation of long-tail items. Extensive experiments
on two public CRS datasets have demonstrated the effectiveness and
extensibility of our approach, especially on long-tail recommendation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhipeng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1&quot;&gt;Kun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaolei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wayne Xin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1&quot;&gt;Fan Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1&quot;&gt;Zhao Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Ji-Rong Wen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11655">
<title>Bandits with Deterministically Evolving States. (arXiv:2307.11655v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11655</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a model for learning with bandit feedback while accounting for
deterministically evolving and unobservable states that we call Bandits with
Deterministically Evolving States. The workhorse applications of our model are
learning for recommendation systems and learning for online ads. In both cases,
the reward that the algorithm obtains at each round is a function of the
short-term reward of the action chosen and how ``healthy&apos;&apos; the system is (i.e.,
as measured by its state). For example, in recommendation systems, the reward
that the platform obtains from a user&apos;s engagement with a particular type of
content depends not only on the inherent features of the specific content, but
also on how the user&apos;s preferences have evolved as a result of interacting with
other types of content on the platform. Our general model accounts for the
different rate $\lambda \in [0,1]$ at which the state evolves (e.g., how fast a
user&apos;s preferences shift as a result of previous content consumption) and
encompasses standard multi-armed bandits as a special case. The goal of the
algorithm is to minimize a notion of regret against the best-fixed sequence of
arms pulled. We analyze online learning algorithms for any possible
parametrization of the evolution rate $\lambda$. Specifically, the regret rates
obtained are: for $\lambda \in [0, 1/T^2]$: $\widetilde O(\sqrt{KT})$; for
$\lambda = T^{-a/b}$ with $b &amp;lt; a &amp;lt; 2b$: $\widetilde O (T^{b/a})$; for $\lambda
\in (1/T, 1 - 1/\sqrt{T}): \widetilde O (K^{1/3}T^{2/3})$; and for $\lambda \in
[1 - 1/\sqrt{T}, 1]: \widetilde O (K\sqrt{T})$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khosravi_K/0/1/0/all/0/1&quot;&gt;Khashayar Khosravi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leme_R/0/1/0/all/0/1&quot;&gt;Renato Paes Leme&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Podimata_C/0/1/0/all/0/1&quot;&gt;Chara Podimata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsorvantzis_A/0/1/0/all/0/1&quot;&gt;Apostolis Tsorvantzis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11661">
<title>Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts. (arXiv:2307.11661v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11661</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive pretrained large Vision-Language Models (VLMs) like CLIP have
revolutionized visual representation learning by providing good performance on
downstream datasets. VLMs are 0-shot adapted to a downstream dataset by
designing prompts that are relevant to the dataset. Such prompt engineering
makes use of domain expertise and a validation dataset. Meanwhile, recent
developments in generative pretrained models like GPT-4 mean they can be used
as advanced internet search tools. They can also be manipulated to provide
visual information in any structure. In this work, we show that GPT-4 can be
used to generate text that is visually descriptive and how this can be used to
adapt CLIP to downstream tasks. We show considerable improvements in 0-shot
transfer accuracy on specialized fine-grained datasets like EuroSAT (~7%), DTD
(~7%), SUN397 (~4.6%), and CUB (~3.3%) when compared to CLIP&apos;s default prompt.
We also design a simple few-shot adapter that learns to choose the best
possible sentences to construct generalizable classifiers that outperform the
recently proposed CoCoOP by ~2% on average and by over 4% on 4 specialized
fine-grained datasets. We will release the code, prompts, and auxiliary text
dataset upon acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maniparambil_M/0/1/0/all/0/1&quot;&gt;Mayug Maniparambil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vorster_C/0/1/0/all/0/1&quot;&gt;Chris Vorster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molloy_D/0/1/0/all/0/1&quot;&gt;Derek Molloy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murphy_N/0/1/0/all/0/1&quot;&gt;Noel Murphy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McGuinness_K/0/1/0/all/0/1&quot;&gt;Kevin McGuinness&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OConnor_N/0/1/0/all/0/1&quot;&gt;Noel E. O&amp;#x27;Connor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11688">
<title>Interpretable Graph Networks Formulate Universal Algebra Conjectures. (arXiv:2307.11688v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11688</link>
<description rdf:parseType="Literal">&lt;p&gt;The rise of Artificial Intelligence (AI) recently empowered researchers to
investigate hard mathematical problems which eluded traditional approaches for
decades. Yet, the use of AI in Universal Algebra (UA) -- one of the fields
laying the foundations of modern mathematics -- is still completely unexplored.
This work proposes the first use of AI to investigate UA&apos;s conjectures with an
equivalent equational and topological characterization. While topological
representations would enable the analysis of such properties using graph neural
networks, the limited transparency and brittle explainability of these models
hinder their straightforward use to empirically validate existing conjectures
or to formulate new ones. To bridge these gaps, we propose a general algorithm
generating AI-ready datasets based on UA&apos;s conjectures, and introduce a novel
neural layer to build fully interpretable graph networks. The results of our
experiments demonstrate that interpretable graph networks: (i) enhance
interpretability without sacrificing task accuracy, (ii) strongly generalize
when predicting universal algebra&apos;s properties, (iii) generate simple
explanations that empirically validate existing conjectures, and (iv) identify
subgraphs suggesting the formulation of novel conjectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giannini_F/0/1/0/all/0/1&quot;&gt;Francesco Giannini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fioravanti_S/0/1/0/all/0/1&quot;&gt;Stefano Fioravanti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keskin_O/0/1/0/all/0/1&quot;&gt;Oguzhan Keskin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lupidi_A/0/1/0/all/0/1&quot;&gt;Alisia Maria Lupidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magister_L/0/1/0/all/0/1&quot;&gt;Lucie Charlotte Magister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1&quot;&gt;Pietro Lio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barbiero_P/0/1/0/all/0/1&quot;&gt;Pietro Barbiero&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11694">
<title>SynerGPT: In-Context Learning for Personalized Drug Synergy Prediction and Drug Design. (arXiv:2307.11694v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.11694</link>
<description rdf:parseType="Literal">&lt;p&gt;Predicting synergistic drug combinations can help accelerate discovery of
cancer treatments, particularly therapies personalized to a patient&apos;s specific
tumor via biopsied cells. In this paper, we propose a novel setting and models
for in-context drug synergy learning. We are given a small &quot;personalized
dataset&quot; of 10-20 drug synergy relationships in the context of specific cancer
cell targets. Our goal is to predict additional drug synergy relationships in
that context. Inspired by recent work that pre-trains a GPT language model (LM)
to &quot;in-context learn&quot; common function classes, we devise novel pre-training
schemes that enable a GPT model to in-context learn &quot;drug synergy functions&quot;.
Our model -- which does not use any textual corpora, molecular fingerprints,
protein interaction or any other domain-specific knowledge -- is able to
achieve competitive results. We further integrate our in-context approach with
a genetic algorithm to optimize model prompts and select synergy candidates to
test after conducting a patient biopsy. Finally, we explore a novel task of
inverse drug design which can potentially enable the design of drugs that
synergize specifically to target a given patient&apos;s &quot;personalized dataset&quot;. Our
findings can potentially have an important impact on precision cancer medicine,
and also raise intriguing questions on non-textual pre-training for LMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Edwards_C/0/1/0/all/0/1&quot;&gt;Carl Edwards&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naik_A/0/1/0/all/0/1&quot;&gt;Aakanksha Naik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khot_T/0/1/0/all/0/1&quot;&gt;Tushar Khot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burke_M/0/1/0/all/0/1&quot;&gt;Martin Burke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1&quot;&gt;Heng Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1&quot;&gt;Tom Hope&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11709">
<title>Statement-based Memory for Neural Source Code Summarization. (arXiv:2307.11709v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.11709</link>
<description rdf:parseType="Literal">&lt;p&gt;Source code summarization is the task of writing natural language
descriptions of source code behavior. Code summarization underpins software
documentation for programmers. Short descriptions of code help programmers
understand the program quickly without having to read the code itself. Lately,
neural source code summarization has emerged as the frontier of research into
automated code summarization techniques. By far the most popular targets for
summarization are program subroutines. The idea, in a nutshell, is to train an
encoder-decoder neural architecture using large sets of examples of subroutines
extracted from code repositories. The encoder represents the code and the
decoder represents the summary. However, most current approaches attempt to
treat the subroutine as a single unit. For example, by taking the entire
subroutine as input to a Transformer or RNN-based encoder. But code behavior
tends to depend on the flow from statement to statement. Normally dynamic
analysis may shed light on this flow, but dynamic analysis on hundreds of
thousands of examples in large datasets is not practical. In this paper, we
present a statement-based memory encoder that learns the important elements of
flow during training, leading to a statement-based subroutine representation
without the need for dynamic analysis. We implement our encoder for code
summarization and demonstrate a significant improvement over the
state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_A/0/1/0/all/0/1&quot;&gt;Aakash Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1&quot;&gt;Siyuan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haque_S/0/1/0/all/0/1&quot;&gt;Sakib Haque&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McMillan_C/0/1/0/all/0/1&quot;&gt;Collin McMillan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11719">
<title>Benchmark datasets for biomedical knowledge graphs with negative statements. (arXiv:2307.11719v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.11719</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge graphs represent facts about real-world entities. Most of these
facts are defined as positive statements. The negative statements are scarce
but highly relevant under the open-world assumption. Furthermore, they have
been demonstrated to improve the performance of several applications, namely in
the biomedical domain. However, no benchmark dataset supports the evaluation of
the methods that consider these negative statements.
&lt;/p&gt;
&lt;p&gt;We present a collection of datasets for three relation prediction tasks -
protein-protein interaction prediction, gene-disease association prediction and
disease prediction - that aim at circumventing the difficulties in building
benchmarks for knowledge graphs with negative statements. These datasets
include data from two successful biomedical ontologies, Gene Ontology and Human
Phenotype Ontology, enriched with negative statements.
&lt;/p&gt;
&lt;p&gt;We also generate knowledge graph embeddings for each dataset with two popular
path-based methods and evaluate the performance in each task. The results show
that the negative statements can improve the performance of knowledge graph
embeddings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sousa_R/0/1/0/all/0/1&quot;&gt;Rita T. Sousa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_S/0/1/0/all/0/1&quot;&gt;Sara Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pesquita_C/0/1/0/all/0/1&quot;&gt;Catia Pesquita&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11730">
<title>Mitigating Communications Threats in Decentralized Federated Learning through Moving Target Defense. (arXiv:2307.11730v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2307.11730</link>
<description rdf:parseType="Literal">&lt;p&gt;The rise of Decentralized Federated Learning (DFL) has enabled the training
of machine learning models across federated participants, fostering
decentralized model aggregation and reducing dependence on a server. However,
this approach introduces unique communication security challenges that have yet
to be thoroughly addressed in the literature. These challenges primarily
originate from the decentralized nature of the aggregation process, the varied
roles and responsibilities of the participants, and the absence of a central
authority to oversee and mitigate threats. Addressing these challenges, this
paper first delineates a comprehensive threat model, highlighting the potential
risks of DFL communications. In response to these identified risks, this work
introduces a security module designed for DFL platforms to counter
communication-based attacks. The module combines security techniques such as
symmetric and asymmetric encryption with Moving Target Defense (MTD)
techniques, including random neighbor selection and IP/port switching. The
security module is implemented in a DFL platform called Fedstellar, allowing
the deployment and monitoring of the federation. A DFL scenario has been
deployed, involving eight physical devices implementing three security
configurations: (i) a baseline with no security, (ii) an encrypted
configuration, and (iii) a configuration integrating both encryption and MTD
techniques. The effectiveness of the security module is validated through
experiments with the MNIST dataset and eclipse attacks. The results indicated
an average F1 score of 95%, with moderate increases in CPU usage (up to 63.2%
+-3.5%) and network traffic (230 MB +-15 MB) under the most secure
configuration, mitigating the risks posed by eavesdropping or eclipse attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beltran_E/0/1/0/all/0/1&quot;&gt;Enrique Tom&amp;#xe1;s Mart&amp;#xed;nez Beltr&amp;#xe1;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanchez_P/0/1/0/all/0/1&quot;&gt;Pedro Miguel S&amp;#xe1;nchez S&amp;#xe1;nchez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernal_S/0/1/0/all/0/1&quot;&gt;Sergio L&amp;#xf3;pez Bernal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bovet_G/0/1/0/all/0/1&quot;&gt;G&amp;#xe9;r&amp;#xf4;me Bovet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_M/0/1/0/all/0/1&quot;&gt;Manuel Gil P&amp;#xe9;rez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_G/0/1/0/all/0/1&quot;&gt;Gregorio Mart&amp;#xed;nez P&amp;#xe9;rez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Celdran_A/0/1/0/all/0/1&quot;&gt;Alberto Huertas Celdr&amp;#xe1;n&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.08227">
<title>Learning Multi-agent Skills for Tabular Reinforcement Learning using Factor Graphs. (arXiv:2201.08227v3 [cs.MA] UPDATED)</title>
<link>http://arxiv.org/abs/2201.08227</link>
<description rdf:parseType="Literal">&lt;p&gt;Covering skill (a.k.a., option) discovery has been developed to improve the
exploration of reinforcement learning in single-agent scenarios with sparse
reward signals, through connecting the most distant states in the embedding
space provided by the Fiedler vector of the state transition graph. However,
these option discovery methods cannot be directly extended to multi-agent
scenarios, since the joint state space grows exponentially with the number of
agents in the system. Thus, existing researches on adopting options in
multi-agent scenarios still rely on single-agent option discovery and fail to
directly discover the joint options that can improve the connectivity of the
joint state space of agents. In this paper, we show that it is indeed possible
to directly compute multi-agent options with collaborative exploratory
behaviors among the agents, while still enjoying the ease of decomposition. Our
key idea is to approximate the joint state space as a Kronecker graph -- the
Kronecker product of individual agents&apos; state transition graphs, based on which
we can directly estimate the Fiedler vector of the joint state space using the
Laplacian spectrum of individual agents&apos; transition graphs. This decomposition
enables us to efficiently construct multi-agent joint options by encouraging
agents to connect the sub-goal joint states which are corresponding to the
minimum or maximum values of the estimated joint Fiedler vector. The evaluation
based on multi-agent collaborative tasks shows that the proposed algorithm can
successfully identify multi-agent options, and significantly outperforms prior
works using single-agent options or no options, in terms of both faster
exploration and higher cumulative rewards.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiayu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jingdi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_T/0/1/0/all/0/1&quot;&gt;Tian Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aggarwal_V/0/1/0/all/0/1&quot;&gt;Vaneet Aggarwal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.10736">
<title>The activity-weight duality in feed forward neural networks: The geometric determinants of generalization. (arXiv:2203.10736v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2203.10736</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the fundamental problems in machine learning is generalization. In
neural network models with a large number of weights (parameters), many
solutions can be found to fit the training data equally well. The key question
is which solution can describe testing data not in the training set. Here, we
report the discovery of an exact duality (equivalence) between changes in
activities in a given layer of neurons and changes in weights that connect to
the next layer of neurons in a densely connected layer in any feed forward
neural network. The activity-weight (A-W) duality allows us to map variations
in inputs (data) to variations of the corresponding dual weights. By using this
mapping, we show that the generalization loss can be decomposed into a sum of
contributions from different eigen-directions of the Hessian matrix of the loss
function at the solution in weight space. The contribution from a given
eigen-direction is the product of two geometric factors (determinants): the
sharpness of the loss landscape and the standard deviation of the dual weights,
which is found to scale with the weight norm of the solution. Our results
provide an unified framework, which we used to reveal how different
regularization schemes (weight decay, stochastic gradient descent with
different batch sizes and learning rates, dropout), training data size, and
labeling noise affect generalization performance by controlling either one or
both of these two geometric determinants for generalization. These insights can
be used to guide development of algorithms for finding more generalizable
solutions in overparametrized neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yu Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Y/0/1/0/all/0/1&quot;&gt;Yuhai Tu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.07392">
<title>A simple declarative model of the Federal Disaster Assistance Policy -- modelling and measuring transparency. (arXiv:2207.07392v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2207.07392</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we will provide a quantitative analysis of a simple model of
the Federal Disaster Assistance policy from the viewpoint of three different
stakeholders. This quantitative methodology is new and has applications to
other areas such as business and healthcare processes. The stakeholders are
interested in process transparency but each has a different opinion on
precisely what constitutes transparency. We will also consider three
modifications to the Federal Disaster Assistance policy and analyse, from a
stakeholder viewpoint, how stakeholder satisfaction changes from process to
process. This analysis is used to rank the favourability of four policies with
respect to all collective stakeholder preferences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dukes_M/0/1/0/all/0/1&quot;&gt;Mark Dukes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.11723">
<title>Evidence of Vocal Tract Articulation in Self-Supervised Learning of Speech. (arXiv:2210.11723v3 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2210.11723</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent self-supervised learning (SSL) models have proven to learn rich
representations of speech, which can readily be utilized by diverse downstream
tasks. To understand such utilities, various analyses have been done for speech
SSL models to reveal which and how information is encoded in the learned
representations. Although the scope of previous analyses is extensive in
acoustic, phonetic, and semantic perspectives, the physical grounding by speech
production has not yet received full attention. To bridge this gap, we conduct
a comprehensive analysis to link speech representations to articulatory
trajectories measured by electromagnetic articulography (EMA). Our analysis is
based on a linear probing approach where we measure articulatory score as an
average correlation of linear mapping to EMA. We analyze a set of SSL models
selected from the leaderboard of the SUPERB benchmark and perform further
layer-wise analyses on two most successful models, Wav2Vec 2.0 and HuBERT.
Surprisingly, representations from the recent speech SSL models are highly
correlated with EMA traces (best: r = 0.81), and only 5 minutes are sufficient
to train a linear model with high performance (r = 0.77). Our findings suggest
that SSL models learn to align closely with continuous articulations, and
provide a novel insight into speech SSL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cho_C/0/1/0/all/0/1&quot;&gt;Cheol Jun Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_P/0/1/0/all/0/1&quot;&gt;Peter Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mohamed_A/0/1/0/all/0/1&quot;&gt;Abdelrahman Mohamed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Anumanchipalli_G/0/1/0/all/0/1&quot;&gt;Gopala K. Anumanchipalli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.09648">
<title>NusaCrowd: Open Source Initiative for Indonesian NLP Resources. (arXiv:2212.09648v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2212.09648</link>
<description rdf:parseType="Literal">&lt;p&gt;We present NusaCrowd, a collaborative initiative to collect and unify
existing resources for Indonesian languages, including opening access to
previously non-public resources. Through this initiative, we have brought
together 137 datasets and 118 standardized data loaders. The quality of the
datasets has been assessed manually and automatically, and their value is
demonstrated through multiple experiments. NusaCrowd&apos;s data collection enables
the creation of the first zero-shot benchmarks for natural language
understanding and generation in Indonesian and the local languages of
Indonesia. Furthermore, NusaCrowd brings the creation of the first multilingual
automatic speech recognition benchmark in Indonesian and the local languages of
Indonesia. Our work strives to advance natural language processing (NLP)
research for languages that are under-represented despite being widely spoken.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1&quot;&gt;Samuel Cahyawijaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lovenia_H/0/1/0/all/0/1&quot;&gt;Holy Lovenia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1&quot;&gt;Alham Fikri Aji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1&quot;&gt;Genta Indra Winata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilie_B/0/1/0/all/0/1&quot;&gt;Bryan Wilie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahendra_R/0/1/0/all/0/1&quot;&gt;Rahmad Mahendra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wibisono_C/0/1/0/all/0/1&quot;&gt;Christian Wibisono&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romadhony_A/0/1/0/all/0/1&quot;&gt;Ade Romadhony&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vincentio_K/0/1/0/all/0/1&quot;&gt;Karissa Vincentio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koto_F/0/1/0/all/0/1&quot;&gt;Fajri Koto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santoso_J/0/1/0/all/0/1&quot;&gt;Jennifer Santoso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moeljadi_D/0/1/0/all/0/1&quot;&gt;David Moeljadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wirawan_C/0/1/0/all/0/1&quot;&gt;Cahya Wirawan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hudi_F/0/1/0/all/0/1&quot;&gt;Frederikus Hudi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parmonangan_I/0/1/0/all/0/1&quot;&gt;Ivan Halim Parmonangan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alfina_I/0/1/0/all/0/1&quot;&gt;Ika Alfina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wicaksono_M/0/1/0/all/0/1&quot;&gt;Muhammad Satrio Wicaksono&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Putra_I/0/1/0/all/0/1&quot;&gt;Ilham Firdausi Putra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahmadani_S/0/1/0/all/0/1&quot;&gt;Samsul Rahmadani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oenang_Y/0/1/0/all/0/1&quot;&gt;Yulianti Oenang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Septiandri_A/0/1/0/all/0/1&quot;&gt;Ali Akbar Septiandri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaya_J/0/1/0/all/0/1&quot;&gt;James Jaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhole_K/0/1/0/all/0/1&quot;&gt;Kaustubh D. Dhole&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suryani_A/0/1/0/all/0/1&quot;&gt;Arie Ardiyanti Suryani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Putri_R/0/1/0/all/0/1&quot;&gt;Rifki Afina Putri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1&quot;&gt;Dan Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stevens_K/0/1/0/all/0/1&quot;&gt;Keith Stevens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nityasya_M/0/1/0/all/0/1&quot;&gt;Made Nindyatama Nityasya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adilazuarda_M/0/1/0/all/0/1&quot;&gt;Muhammad Farid Adilazuarda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ignatius_R/0/1/0/all/0/1&quot;&gt;Ryan Ignatius&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diandaru_R/0/1/0/all/0/1&quot;&gt;Ryandito Diandaru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1&quot;&gt;Tiezheng Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghifari_V/0/1/0/all/0/1&quot;&gt;Vito Ghifari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1&quot;&gt;Wenliang Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damapuspita_D/0/1/0/all/0/1&quot;&gt;Dyah Damapuspita&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tho_C/0/1/0/all/0/1&quot;&gt;Cuk Tho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karo_I/0/1/0/all/0/1&quot;&gt;Ichwanul Muslim Karo Karo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fatyanosa_T/0/1/0/all/0/1&quot;&gt;Tirana Noor Fatyanosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1&quot;&gt;Ziwei Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1&quot;&gt;Pascale Fung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1&quot;&gt;Graham Neubig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1&quot;&gt;Timothy Baldwin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1&quot;&gt;Sebastian Ruder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sujaini_H/0/1/0/all/0/1&quot;&gt;Herry Sujaini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sakti_S/0/1/0/all/0/1&quot;&gt;Sakriani Sakti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Purwarianti_A/0/1/0/all/0/1&quot;&gt;Ayu Purwarianti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.09559">
<title>SpArX: Sparse Argumentative Explanations for Neural Networks. (arXiv:2301.09559v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2301.09559</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks (NNs) have various applications in AI, but explaining their
decisions remains challenging. Existing approaches often focus on explaining
how changing individual inputs affects NNs&apos; outputs. However, an explanation
that is consistent with the input-output behaviour of an NN is not necessarily
faithful to the actual mechanics thereof. In this paper, we exploit
relationships between multi-layer perceptrons (MLPs) and quantitative
argumentation frameworks (QAFs) to create argumentative explanations for the
mechanics of MLPs. Our SpArX method first sparsifies the MLP while maintaining
as much of the original structure as possible. It then translates the sparse
MLP into an equivalent QAF to shed light on the underlying decision process of
the MLP, producing global and/or local explanations. We demonstrate
experimentally that SpArX can give more faithful explanations than existing
approaches, while simultaneously providing deeper insights into the actual
reasoning process of MLPs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayoobi_H/0/1/0/all/0/1&quot;&gt;Hamed Ayoobi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Potyka_N/0/1/0/all/0/1&quot;&gt;Nico Potyka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toni_F/0/1/0/all/0/1&quot;&gt;Francesca Toni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.04973">
<title>Invariant Slot Attention: Object Discovery with Slot-Centric Reference Frames. (arXiv:2302.04973v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.04973</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatically discovering composable abstractions from raw perceptual data is
a long-standing challenge in machine learning. Recent slot-based neural
networks that learn about objects in a self-supervised manner have made
exciting progress in this direction. However, they typically fall short at
adequately capturing spatial symmetries present in the visual world, which
leads to sample inefficiency, such as when entangling object appearance and
pose. In this paper, we present a simple yet highly effective method for
incorporating spatial symmetries via slot-centric reference frames. We
incorporate equivariance to per-object pose transformations into the attention
and generation mechanism of Slot Attention by translating, scaling, and
rotating position encodings. These changes result in little computational
overhead, are easy to implement, and can result in large gains in terms of data
efficiency and overall improvements to object discovery. We evaluate our method
on a wide range of synthetic object discovery benchmarks namely CLEVR,
Tetrominoes, CLEVRTex, Objects Room and MultiShapeNet, and show promising
improvements on the challenging real-world Waymo Open dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biza_O/0/1/0/all/0/1&quot;&gt;Ondrej Biza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steenkiste_S/0/1/0/all/0/1&quot;&gt;Sjoerd van Steenkiste&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sajjadi_M/0/1/0/all/0/1&quot;&gt;Mehdi S. M. Sajjadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elsayed_G/0/1/0/all/0/1&quot;&gt;Gamaleldin F. Elsayed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahendran_A/0/1/0/all/0/1&quot;&gt;Aravindh Mahendran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kipf_T/0/1/0/all/0/1&quot;&gt;Thomas Kipf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.11630">
<title>BoxSnake: Polygonal Instance Segmentation with Box Supervision. (arXiv:2303.11630v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.11630</link>
<description rdf:parseType="Literal">&lt;p&gt;Box-supervised instance segmentation has gained much attention as it requires
only simple box annotations instead of costly mask or polygon annotations.
However, existing box-supervised instance segmentation models mainly focus on
mask-based frameworks. We propose a new end-to-end training technique, termed
BoxSnake, to achieve effective polygonal instance segmentation using only box
annotations for the first time. Our method consists of two loss functions: (1)
a point-based unary loss that constrains the bounding box of predicted polygons
to achieve coarse-grained segmentation; and (2) a distance-aware pairwise loss
that encourages the predicted polygons to fit the object boundaries. Compared
with the mask-based weakly-supervised methods, BoxSnake further reduces the
performance gap between the predicted segmentation and the bounding box, and
shows significant superiority on the Cityscapes dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1&quot;&gt;Rui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1&quot;&gt;Lin Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yixiao Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.15471">
<title>Embedding Contextual Information through Reward Shaping in Multi-Agent Learning: A Case Study from Google Football. (arXiv:2303.15471v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.15471</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial Intelligence has been used to help human complete difficult tasks
in complicated environments by providing optimized strategies for
decision-making or replacing the manual labour. In environments including
multiple agents, such as football, the most common methods to train agents are
Imitation Learning and Multi-Agent Reinforcement Learning (MARL). However, the
agents trained by Imitation Learning cannot outperform the expert demonstrator,
which makes humans hardly get new insights from the learnt policy. Besides,
MARL is prone to the credit assignment problem. In environments with sparse
reward signal, this method can be inefficient. The objective of our research is
to create a novel reward shaping method by embedding contextual information in
reward function to solve the aforementioned challenges. We demonstrate this in
the Google Research Football (GRF) environment. We quantify the contextual
information extracted from game state observation and use this quantification
together with original sparse reward to create the shaped reward. The
experiment results in the GRF environment prove that our reward shaping method
is a useful addition to state-of-the-art MARL algorithms for training agents in
environments with sparse reward signal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_C/0/1/0/all/0/1&quot;&gt;Chaoyi Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_V/0/1/0/all/0/1&quot;&gt;Varuna De Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Artaud_C/0/1/0/all/0/1&quot;&gt;Corentin Artaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pina_R/0/1/0/all/0/1&quot;&gt;Rafael Pina&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.17555">
<title>Factoring the Matrix of Domination: A Critical Review and Reimagination of Intersectionality in AI Fairness. (arXiv:2303.17555v2 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2303.17555</link>
<description rdf:parseType="Literal">&lt;p&gt;Intersectionality is a critical framework that, through inquiry and praxis,
allows us to examine how social inequalities persist through domains of
structure and discipline. Given AI fairness&apos; raison d&apos;etre of &quot;fairness&quot;, we
argue that adopting intersectionality as an analytical framework is pivotal to
effectively operationalizing fairness. Through a critical review of how
intersectionality is discussed in 30 papers from the AI fairness literature, we
deductively and inductively: 1) map how intersectionality tenets operate within
the AI fairness paradigm and 2) uncover gaps between the conceptualization and
operationalization of intersectionality. We find that researchers
overwhelmingly reduce intersectionality to optimizing for fairness metrics over
demographic subgroups. They also fail to discuss their social context and when
mentioning power, they mostly situate it only within the AI pipeline. We: 3)
outline and assess the implications of these gaps for critical inquiry and
praxis, and 4) provide actionable recommendations for AI fairness researchers
to engage with intersectionality in their work by grounding it in AI
epistemology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ovalle_A/0/1/0/all/0/1&quot;&gt;Anaelia Ovalle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramonian_A/0/1/0/all/0/1&quot;&gt;Arjun Subramonian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gautam_V/0/1/0/all/0/1&quot;&gt;Vagrant Gautam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gee_G/0/1/0/all/0/1&quot;&gt;Gilbert Gee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1&quot;&gt;Kai-Wei Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12851">
<title>Enhancing Coherence of Extractive Summarization with Multitask Learning. (arXiv:2305.12851v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12851</link>
<description rdf:parseType="Literal">&lt;p&gt;This study proposes a multitask learning architecture for extractive
summarization with coherence boosting. The architecture contains an extractive
summarizer and coherent discriminator module. The coherent discriminator is
trained online on the sentence vectors of the augmented textual input, thus
improving its general ability of judging whether the input sentences are
coherent. Meanwhile, we maximize the coherent scores from the coherent
discriminator by updating the parameters of the summarizer. To make the
extractive sentences trainable in a differentiable manner, we introduce two
strategies, including pre-trained converting model (model-based) and converting
matrix (MAT-based) that merge sentence representations. Experiments show that
our proposed method significantly improves the proportion of consecutive
sentences in the extracted summaries based on their positions in the original
article (i.e., automatic sentence-level coherence metric), while the goodness
in terms of other automatic metrics (i.e., Rouge scores and BertScores) are
preserved. Human evaluation also evidences the improvement of coherence and
consistency of the extracted summaries given by our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jie_R/0/1/0/all/0/1&quot;&gt;Renlong Jie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1&quot;&gt;Xiaojun Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1&quot;&gt;Lifeng Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xin Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qun Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18451">
<title>Shift-Robust Molecular Relational Learning with Causal Substructure. (arXiv:2305.18451v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18451</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, molecular relational learning, whose goal is to predict the
interaction behavior between molecular pairs, got a surge of interest in
molecular sciences due to its wide range of applications. In this work, we
propose CMRL that is robust to the distributional shift in molecular relational
learning by detecting the core substructure that is causally related to
chemical reactions. To do so, we first assume a causal relationship based on
the domain knowledge of molecular sciences and construct a structural causal
model (SCM) that reveals the relationship between variables. Based on the SCM,
we introduce a novel conditional intervention framework whose intervention is
conditioned on the paired molecule. With the conditional intervention
framework, our model successfully learns from the causal substructure and
alleviates the confounding effect of shortcut substructures that are spuriously
correlated to chemical reactions. Extensive experiments on various tasks with
real-world and synthetic datasets demonstrate the superiority of CMRL over
state-of-the-art baseline models. Our code is available at
https://github.com/Namkyeong/CMRL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1&quot;&gt;Namkyeong Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1&quot;&gt;Kanghoon Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Na_G/0/1/0/all/0/1&quot;&gt;Gyoung S. Na&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sein Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1&quot;&gt;Chanyoung Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.03933">
<title>High-dimensional and Permutation Invariant Anomaly Detection. (arXiv:2306.03933v2 [hep-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2306.03933</link>
<description rdf:parseType="Literal">&lt;p&gt;Methods for anomaly detection of new physics processes are often limited to
low-dimensional spaces due to the difficulty of learning high-dimensional
probability densities. Particularly at the constituent level, incorporating
desirable properties such as permutation invariance and variable-length inputs
becomes difficult within popular density estimation methods. In this work, we
introduce a permutation-invariant density estimator for particle physics data
based on diffusion models, specifically designed to handle variable-length
inputs. We demonstrate the efficacy of our methodology by utilizing the learned
density as a permutation-invariant anomaly detection score, effectively
identifying jets with low likelihood under the background-only hypothesis. To
validate our density estimation method, we investigate the ratio of learned
densities and compare to those obtained by a supervised classification
algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Mikuni_V/0/1/0/all/0/1&quot;&gt;Vinicius Mikuni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Nachman_B/0/1/0/all/0/1&quot;&gt;Benjamin Nachman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07458">
<title>Adaptive interventions for both accuracy and time in AI-assisted human decision making. (arXiv:2306.07458v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07458</link>
<description rdf:parseType="Literal">&lt;p&gt;In settings where users are both time-pressured and need high accuracy, such
as doctors working in Emergency Rooms, we want to provide AI assistance that
both increases accuracy and reduces time. However, different types of AI
assistance have different benefits: some reduce time taken while increasing
overreliance on AI, while others do the opposite. We therefore want to adapt
what AI assistance we show depending on various properties (of the question and
of the user) in order to best tradeoff our two objectives. We introduce a study
where users have to prescribe medicines to aliens, and use it to explore the
potential for adapting AI assistance. We find evidence that it is beneficial to
adapt our AI assistance depending on the question, leading to good tradeoffs
between time taken and accuracy. Future work would consider machine-learning
algorithms (such as reinforcement learning) to automatically adapt quickly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swaroop_S/0/1/0/all/0/1&quot;&gt;Siddharth Swaroop&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bucinca_Z/0/1/0/all/0/1&quot;&gt;Zana Bu&amp;#xe7;inca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doshi_Velez_F/0/1/0/all/0/1&quot;&gt;Finale Doshi-Velez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14096">
<title>Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models. (arXiv:2306.14096v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14096</link>
<description rdf:parseType="Literal">&lt;p&gt;Entity-level fine-grained sentiment analysis in the financial domain is a
crucial subtask of sentiment analysis and currently faces numerous challenges.
The primary challenge stems from the lack of high-quality and large-scale
annotated corpora specifically designed for financial text sentiment analysis,
which in turn limits the availability of data necessary for developing
effective text processing techniques. Recent advancements in large language
models (LLMs) have yielded remarkable performance in natural language
processing tasks, primarily centered around language pattern matching. In this
paper, we propose a novel and extensive Chinese fine-grained financial
sentiment analysis dataset, FinChina SA, for enterprise early warning. We
thoroughly evaluate and experiment with well-known existing open-source LLMs
using our dataset. We firmly believe that our dataset will serve as a valuable
resource to advance the exploration of real-world financial sentiment analysis
tasks, which should be the focus of future research. Our dataset and all code
to replicate the experimental results will be released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1&quot;&gt;Yinyu Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yanru Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Wang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1&quot;&gt;Weiqiang Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Youhao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15079">
<title>A direct optimization algorithm for input-constrained MPC. (arXiv:2306.15079v4 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15079</link>
<description rdf:parseType="Literal">&lt;p&gt;One challenge of running a model predictive control (MPC) algorithm in a
production-embedded platform is to provide the certificate of worst-case
computation complexity, that is, its maximum execution time has to always be
smaller than sampling time. This paper proposes for the first time a
\textit{direct} optimization algorithm for input-constrained MPC: the number of
iterations is data-independent and dependent on the problem dimension $n$, with
exact value
$\left\lceil\frac{\log\left(\frac{2n}{\epsilon}\right)}{-2\log(1-\frac{1}{4\sqrt{2n}})}\right\rceil+1$,
where $\epsilon$ denotes a given stopping accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Liang Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03070">
<title>Hybrid Knowledge-Data Driven Channel Semantic Acquisition and Beamforming for Cell-Free Massive MIMO. (arXiv:2307.03070v2 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03070</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper focuses on advancing outdoor wireless systems to better support
ubiquitous extended reality (XR) applications, and close the gap with current
indoor wireless transmission capabilities. We propose a hybrid knowledge-data
driven method for channel semantic acquisition and multi-user beamforming in
cell-free massive multiple-input multiple-output (MIMO) systems. Specifically,
we firstly propose a data-driven multiple layer perceptron (MLP)-Mixer-based
auto-encoder for channel semantic acquisition, where the pilot signals, CSI
quantizer for channel semantic embedding, and CSI reconstruction for channel
semantic extraction are jointly optimized in an end-to-end manner. Moreover,
based on the acquired channel semantic, we further propose a knowledge-driven
deep-unfolding multi-user beamformer, which is capable of achieving good
spectral efficiency with robustness to imperfect CSI in outdoor XR scenarios.
By unfolding conventional successive over-relaxation (SOR)-based linear
beamforming scheme with deep learning, the proposed beamforming scheme is
capable of adaptively learning the optimal parameters to accelerate convergence
and improve the robustness to imperfect CSI. The proposed deep unfolding
beamforming scheme can be used for access points (APs) with fully-digital array
and APs with hybrid analog-digital array. Simulation results demonstrate the
effectiveness of our proposed scheme in improving the accuracy of channel
acquisition, as well as reducing complexity in both CSI acquisition and
beamformer design. The proposed beamforming method achieves approximately 96%
of the converged spectrum efficiency performance after only three iterations in
downlink transmission, demonstrating its efficacy and potential to improve
outdoor XR applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gao_Z/0/1/0/all/0/1&quot;&gt;Zhen Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shicong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yu Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhongxiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zheng_D/0/1/0/all/0/1&quot;&gt;Dezhi Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03512">
<title>Tranfer Learning of Semantic Segmentation Methods for Identifying Buried Archaeological Structures on LiDAR Data. (arXiv:2307.03512v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03512</link>
<description rdf:parseType="Literal">&lt;p&gt;When applying deep learning to remote sensing data in archaeological
research, a notable obstacle is the limited availability of suitable datasets
for training models. The application of transfer learning is frequently
employed to mitigate this drawback. However, there is still a need to explore
its effectiveness when applied across different archaeological datasets. This
paper compares the performance of various transfer learning configurations
using two semantic segmentation deep neural networks on two LiDAR datasets. The
experimental results indicate that transfer learning-based approaches in
archaeology can lead to performance improvements, although a systematic
enhancement has not yet been observed. We provide specific insights about the
validity of such techniques that can serve as a baseline for future works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soleni_P/0/1/0/all/0/1&quot;&gt;Paolo Soleni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaart_W/0/1/0/all/0/1&quot;&gt;Wouter B. Verschoof-van der Vaart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kokalj_Z/0/1/0/all/0/1&quot;&gt;&amp;#x17d;iga Kokalj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Traviglia_A/0/1/0/all/0/1&quot;&gt;Arianna Traviglia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fiorucci_M/0/1/0/all/0/1&quot;&gt;Marco Fiorucci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04541">
<title>Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis. (arXiv:2307.04541v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04541</link>
<description rdf:parseType="Literal">&lt;p&gt;Fueled by deep learning, computer-aided diagnosis achieves huge advances.
However, out of controlled lab environments, algorithms could face multiple
challenges. Open set recognition (OSR), as an important one, states that
categories unseen in training could appear in testing. In medical fields, it
could derive from incompletely collected training datasets and the constantly
emerging new or rare diseases. OSR requires an algorithm to not only correctly
classify known classes, but also recognize unknown classes and forward them to
experts for further diagnosis. To tackle OSR, we assume that known classes
could densely occupy small parts of the embedding space and the remaining
sparse regions could be recognized as unknowns. Following it, we propose Open
Margin Cosine Loss (OMCL) unifying two mechanisms. The former, called Margin
Loss with Adaptive Scale (MLAS), introduces angular margin for reinforcing
intra-class compactness and inter-class separability, together with an adaptive
scaling factor to strengthen the generalization capacity. The latter, called
Open-Space Suppression (OSS), opens the classifier by recognizing sparse
embedding space as unknowns using proposed feature space descriptors. Besides,
since medical OSR is still a nascent field, two publicly available benchmark
datasets are proposed for comparison. Extensive ablation studies and feature
visualization demonstrate the effectiveness of each design. Compared with
state-of-the-art methods, MLAS achieves superior performances, measured by ACC,
AUROC, and OSCR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mingyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Lu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jicong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06092">
<title>Quantitative CLTs in Deep Neural Networks. (arXiv:2307.06092v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06092</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the distribution of a fully connected neural network with random
Gaussian weights and biases in which the hidden layer widths are proportional
to a large constant $n$. Under mild assumptions on the non-linearity, we obtain
quantitative bounds on normal approximations valid at large but finite $n$ and
any fixed network depth. Our theorems show both for the finite-dimensional
distributions and the entire process, that the distance between a random fully
connected network (and its derivatives) to the corresponding infinite width
Gaussian process scales like $n^{-\gamma}$ for $\gamma&amp;gt;0$, with the exponent
depending on the metric used to measure discrepancy. Our bounds are strictly
stronger in terms of their dependence on network width than any previously
available in the literature; in the one-dimensional case, we also prove that
they are optimal, i.e., we establish matching lower bounds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Favaro_S/0/1/0/all/0/1&quot;&gt;Stefano Favaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanin_B/0/1/0/all/0/1&quot;&gt;Boris Hanin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marinucci_D/0/1/0/all/0/1&quot;&gt;Domenico Marinucci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nourdin_I/0/1/0/all/0/1&quot;&gt;Ivan Nourdin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peccati_G/0/1/0/all/0/1&quot;&gt;Giovanni Peccati&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08167">
<title>Computing the gradients with respect to all parameters of a quantum neural network using a single circuit. (arXiv:2307.08167v2 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08167</link>
<description rdf:parseType="Literal">&lt;p&gt;When computing the gradients of a quantum neural network using the
parameter-shift rule, the cost function needs to be calculated twice for the
gradient with respect to a single adjustable parameter of the network. When the
total number of parameters is high, the quantum circuit for the computation has
to be adjusted and run for many times. Here we propose an approach to compute
all the gradients using a single circuit only, with a much reduced circuit
depth and less classical registers. We also demonstrate experimentally, on both
real quantum hardware and simulator, that our approach has the advantages that
the circuit takes a significantly shorter time to compile than the conventional
approach, resulting in a speedup on the total runtime.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+He_G/0/1/0/all/0/1&quot;&gt;Guang Ping He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09004">
<title>Ord2Seq: Regarding Ordinal Regression as Label Sequence Prediction. (arXiv:2307.09004v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09004</link>
<description rdf:parseType="Literal">&lt;p&gt;Ordinal regression refers to classifying object instances into ordinal
categories. It has been widely studied in many scenarios, such as medical
disease grading, movie rating, etc. Known methods focused only on learning
inter-class ordinal relationships, but still incur limitations in
distinguishing adjacent categories thus far. In this paper, we propose a simple
sequence prediction framework for ordinal regression called Ord2Seq, which, for
the first time, transforms each ordinal category label into a special label
sequence and thus regards an ordinal regression task as a sequence prediction
process. In this way, we decompose an ordinal regression task into a series of
recursive binary classification steps, so as to subtly distinguish adjacent
categories. Comprehensive experiments show the effectiveness of distinguishing
adjacent categories for performance improvement and our new approach exceeds
state-of-the-art performances in four different scenarios. Codes are available
at https://github.com/wjh892521292/Ord2Seq.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinhong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jintai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tingting Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Danny Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jian Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09782">
<title>ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats. (arXiv:2307.09782v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09782</link>
<description rdf:parseType="Literal">&lt;p&gt;In the complex domain of large language models (LLMs), striking a balance
between computational efficiency and maintaining model quality is a formidable
challenge. Navigating the inherent limitations of uniform quantization,
particularly when dealing with outliers, and motivated by the launch of
NVIDIA&apos;s H100 hardware, this study delves into the viability of floating-point
(FP) quantization, particularly focusing on FP8 and FP4, as a potential
solution. Our comprehensive investigation reveals that for LLMs, FP8 activation
consistently outshines its integer (INT8) equivalent, with the performance edge
becoming more noticeable in models possessing parameters beyond one billion.
For weight quantization, our findings indicate that FP4 exhibits comparable, if
not superior, performance to INT4, simplifying deployment on FP-supported
hardware like H100. To mitigate the overhead from precision alignment caused by
the disparity between weights and activations, we propose two scaling
constraints for weight quantization that negligibly impact the performance
compared to the standard W4A8 model. We additionally enhance our quantization
methods by integrating the Low Rank Compensation (LoRC) strategy, yielding
improvements especially in smaller models. The results of our investigation
emphasize the immense potential of FP quantization for LLMs, paving the way for
high-efficiency deployment in resource-limited settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaoxia Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1&quot;&gt;Zhewei Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yuxiong He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10490">
<title>(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs. (arXiv:2307.10490v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10490</link>
<description rdf:parseType="Literal">&lt;p&gt;We demonstrate how images and sounds can be used for indirect prompt and
instruction injection in multi-modal LLMs. An attacker generates an adversarial
perturbation corresponding to the prompt and blends it into an image or audio
recording. When the user asks the (unmodified, benign) model about the
perturbed image or audio, the perturbation steers the model to output the
attacker-chosen text and/or make the subsequent dialog follow the attacker&apos;s
instruction. We illustrate this attack with several proof-of-concept examples
targeting LLaVa and PandaGPT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagdasaryan_E/0/1/0/all/0/1&quot;&gt;Eugene Bagdasaryan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsieh_T/0/1/0/all/0/1&quot;&gt;Tsung-Yin Hsieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nassi_B/0/1/0/all/0/1&quot;&gt;Ben Nassi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shmatikov_V/0/1/0/all/0/1&quot;&gt;Vitaly Shmatikov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10577">
<title>Ethosight: A Reasoning-Guided Iterative Learning System for Nuanced Perception based on Joint-Embedding &amp; Contextual Label Affinity. (arXiv:2307.10577v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10577</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional computer vision models often require extensive manual effort for
data acquisition, annotation and validation, particularly when detecting subtle
behavioral nuances or events. The difficulty in distinguishing routine
behaviors from potential risks in real-world applications, such as
differentiating routine shopping from potential shoplifting, further
complicates the process. Moreover, these models may demonstrate high false
positive rates and imprecise event detection when exposed to real-world
scenarios that differ significantly from the conditions of the training data.
&lt;/p&gt;
&lt;p&gt;To overcome these hurdles, we present Ethosight, a novel zero-shot computer
vision system. Ethosight initiates with a clean slate based on user
requirements and semantic knowledge of interest. Using localized label affinity
calculations and a reasoning-guided iterative learning loop, Ethosight infers
scene details and iteratively refines the label set. Reasoning mechanisms can
be derived from large language models like GPT4, symbolic reasoners like
OpenNARS\cite{wang2013}\cite{wang2006}, or hybrid systems.
&lt;/p&gt;
&lt;p&gt;Our evaluations demonstrate Ethosight&apos;s efficacy across 40 complex use cases,
spanning domains such as health, safety, and security. Detailed results and
case studies within the main body of this paper and an appendix underscore a
promising trajectory towards enhancing the adaptability and resilience of
computer vision models in detecting and extracting subtle and nuanced
behaviors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Latapie_H/0/1/0/all/0/1&quot;&gt;Hugo Latapie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thorisson_K/0/1/0/all/0/1&quot;&gt;Kristinn R. Thorisson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Shan Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petrosyan_V/0/1/0/all/0/1&quot;&gt;Vahagn Petrosyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hammer_P/0/1/0/all/0/1&quot;&gt;Patrick Hammer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Pei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kynoch_B/0/1/0/all/0/1&quot;&gt;Brandon Kynoch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hanning Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tangrui Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10617">
<title>Unmasking Falsehoods in Reviews: An Exploration of NLP Techniques. (arXiv:2307.10617v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10617</link>
<description rdf:parseType="Literal">&lt;p&gt;In the contemporary digital landscape, online reviews have become an
indispensable tool for promoting products and services across various
businesses. Marketers, advertisers, and online businesses have found incentives
to create deceptive positive reviews for their products and negative reviews
for their competitors&apos; offerings. As a result, the writing of deceptive reviews
has become an unavoidable practice for businesses seeking to promote themselves
or undermine their rivals. Detecting such deceptive reviews has become an
intense and ongoing area of research. This research paper proposes a machine
learning model to identify deceptive reviews, with a particular focus on
restaurants. This study delves into the performance of numerous experiments
conducted on a dataset of restaurant reviews known as the Deceptive Opinion
Spam Corpus. To accomplish this, an n-gram model and max features are developed
to effectively identify deceptive content, particularly focusing on fake
reviews. A benchmark study is undertaken to explore the performance of two
different feature extraction techniques, which are then coupled with five
distinct machine learning classification algorithms. The experimental results
reveal that the passive aggressive classifier stands out among the various
algorithms, showcasing the highest accuracy not only in text classification but
also in identifying fake reviews. Moreover, the research delves into data
augmentation and implements various deep learning techniques to further enhance
the process of detecting deceptive reviews. The findings shed light on the
efficacy of the proposed machine learning approach and offer valuable insights
into dealing with deceptive reviews in the realm of online businesses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnan_A/0/1/0/all/0/1&quot;&gt;Anusuya Baby Hari Krishnan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10711">
<title>AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models. (arXiv:2307.10711v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10711</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing customization methods require access to multiple reference examples
to align pre-trained diffusion probabilistic models (DPMs) with user-provided
concepts. This paper aims to address the challenge of DPM customization when
the only available supervision is a differentiable metric defined on the
generated contents. Since the sampling procedure of DPMs involves recursive
calls to the denoising UNet, na\&quot;ive gradient backpropagation requires storing
the intermediate states of all iterations, resulting in extremely high memory
consumption. To overcome this issue, we propose a novel method AdjointDPM,
which first generates new samples from diffusion models by solving the
corresponding probability-flow ODEs. It then uses the adjoint sensitivity
method to backpropagate the gradients of the loss to the models&apos; parameters
(including conditioning signals, network weights, and initial noises) by
solving another augmented ODE. To reduce numerical errors in both the forward
generation and gradient backpropagation processes, we further reparameterize
the probability-flow ODE and augmented ODE as simple non-stiff ODEs using
exponential integration. Finally, we demonstrate the effectiveness of
AdjointDPM on three interesting tasks: converting visual effects into
identification text embeddings, finetuning DPMs for specific types of
stylization, and optimizing initial noise to generate adversarial samples for
security auditing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Jiachun Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liew_J/0/1/0/all/0/1&quot;&gt;Jun Hao Liew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1&quot;&gt;Vincent Y. F. Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jiashi Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1&quot;&gt;Hanshu Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10991">
<title>Dense Sample Deep Learning. (arXiv:2307.10991v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10991</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Learning (DL) , a variant of the neural network algorithms originally
proposed in the 1980s, has made surprising progress in Artificial Intelligence
(AI), ranging from language translation, protein folding, autonomous cars, and
more recently human-like language models (CHATbots), all that seemed
intractable until very recently. Despite the growing use of Deep Learning (DL)
networks, little is actually understood about the learning mechanisms and
representations that makes these networks effective across such a diverse range
of applications. Part of the answer must be the huge scale of the architecture
and of course the large scale of the data, since not much has changed since
1987. But the nature of deep learned representations remain largely unknown.
Unfortunately training sets with millions or billions of tokens have unknown
combinatorics and Networks with millions or billions of hidden units cannot
easily be visualized and their mechanisms cannot be easily revealed. In this
paper, we explore these questions with a large (1.24M weights; VGG) DL in a
novel high density sample task (5 unique tokens with at minimum 500 exemplars
per token) which allows us to more carefully follow the emergence of category
structure and feature construction. We use various visualization methods for
following the emergence of the classification and the development of the
coupling of feature detectors and structures that provide a type of graphical
bootstrapping, From these results we harvest some basic observations of the
learning dynamics of DL and propose a new theory of complex feature
construction based on our results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanson_S/0/1/0/all/0/1&quot;&gt;Stephen Jos&amp;#xe8; Hanson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yadav_V/0/1/0/all/0/1&quot;&gt;Vivek Yadav&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanson_C/0/1/0/all/0/1&quot;&gt;Catherine Hanson&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>