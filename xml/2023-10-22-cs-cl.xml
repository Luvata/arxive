<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CL updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-10-19T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computation and Language</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12172" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12236" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12274" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12300" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12303" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12318" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12321" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12342" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12344" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12352" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12360" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12362" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12379" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12404" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12406" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12418" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12426" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12430" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12439" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12442" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12443" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12444" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12450" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12454" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12462" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12467" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12477" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12481" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12489" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12490" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12505" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12516" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12522" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12531" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12537" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12541" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12557" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12558" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12575" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12580" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12585" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12611" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12620" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12640" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12648" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12664" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12727" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12751" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12766" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12774" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12778" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12794" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12798" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12803" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12808" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12815" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12818" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12821" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12823" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12836" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12851" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12860" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12864" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12874" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12892" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12902" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2105.14082" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.14276" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.03251" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.02147" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.07025" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.06348" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.13623" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.08238" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.09561" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.09724" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.09730" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.10784" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.01328" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.06132" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.06674" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.11084" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.06623" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.06762" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.12410" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.03495" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.08281" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.09955" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.11171" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.11790" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12295" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12634" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12818" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14232" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14281" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14610" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14766" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14994" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15020" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16986" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19713" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00477" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05644" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09821" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14790" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15687" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07362" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02463" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.00857" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.05270" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.10444" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12871" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15630" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00378" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01448" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02031" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02954" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05069" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05161" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05199" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05991" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.06165" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07091" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07488" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08099" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08395" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.09168" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.09342" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.09430" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10191" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10638" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10765" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.11097" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.11368" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.11670" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.11878" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.04934" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2310.12172">
<title>Overview of ImageArg-2023: The First Shared Task in Multimodal Argument Mining. (arXiv:2310.12172v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12172</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents an overview of the ImageArg shared task, the first
multimodal Argument Mining shared task co-located with the 10th Workshop on
Argument Mining at EMNLP 2023. The shared task comprises two classification
subtasks - (1) Subtask-A: Argument Stance Classification; (2) Subtask-B: Image
Persuasiveness Classification. The former determines the stance of a tweet
containing an image and a piece of text toward a controversial topic (e.g., gun
control and abortion). The latter determines whether the image makes the tweet
text more persuasive. The shared task received 31 submissions for Subtask-A and
21 submissions for Subtask-B from 9 different teams across 6 countries. The top
submission in Subtask-A achieved an F1-score of 0.8647 while the best
submission in Subtask-B achieved an F1-score of 0.5561.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhexiong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elarby_M/0/1/0/all/0/1&quot;&gt;Mohamed Elarby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yang Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Litman_D/0/1/0/all/0/1&quot;&gt;Diane Litman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12236">
<title>Direct Neural Machine Translation with Task-level Mixture of Experts models. (arXiv:2310.12236v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12236</link>
<description rdf:parseType="Literal">&lt;p&gt;Direct neural machine translation (direct NMT) is a type of NMT system that
translates text between two non-English languages. Direct NMT systems often
face limitations due to the scarcity of parallel data between non-English
language pairs. Several approaches have been proposed to address this
limitation, such as multilingual NMT and pivot NMT (translation between two
languages via English). Task-level Mixture of expert models (Task-level MoE),
an inference-efficient variation of Transformer-based models, has shown
promising NMT performance for a large number of language pairs. In Task-level
MoE, different language groups can use different routing strategies to optimize
cross-lingual learning and inference speed. In this work, we examine Task-level
MoE&apos;s applicability in direct NMT and propose a series of high-performing
training and evaluation configurations, through which Task-level MoE-based
direct NMT systems outperform bilingual and pivot-based models for a large
number of low and high-resource direct pairs, and translation directions. Our
Task-level MoE with 16 experts outperforms bilingual NMT, Pivot NMT models for
7 language pairs, while pivot-based models still performed better in 9 pairs
and directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tourni_I/0/1/0/all/0/1&quot;&gt;Isidora Chara Tourni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naskar_S/0/1/0/all/0/1&quot;&gt;Subhajit Naskar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12274">
<title>An Image is Worth Multiple Words: Learning Object Level Concepts using Multi-Concept Prompt Learning. (arXiv:2310.12274v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12274</link>
<description rdf:parseType="Literal">&lt;p&gt;Textural Inversion, a prompt learning method, learns a singular embedding for
a new &quot;word&quot; to represent image style and appearance, allowing it to be
integrated into natural language sentences to generate novel synthesised
images. However, identifying and integrating multiple object-level concepts
within one scene poses significant challenges even when embeddings for
individual concepts are attainable. This is further confirmed by our empirical
tests. To address this challenge, we introduce a framework for Multi-Concept
Prompt Learning (MCPL), where multiple new &quot;words&quot; are simultaneously learned
from a single sentence-image pair. To enhance the accuracy of word-concept
correlation, we propose three regularisation techniques: Attention Masking
(AttnMask) to concentrate learning on relevant areas; Prompts Contrastive Loss
(PromptCL) to separate the embeddings of different concepts; and Bind adjective
(Bind adj.) to associate new &quot;words&quot; with known words. We evaluate via image
generation, editing, and attention visualisation with diverse images. Extensive
quantitative comparisons demonstrate that our method can learn more
semantically disentangled concepts with enhanced word-concept correlation.
Additionally, we introduce a novel dataset and evaluation protocol tailored for
this new task of learning object-level concepts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1&quot;&gt;Chen Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanno_R/0/1/0/all/0/1&quot;&gt;Ryutaro Tanno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saseendran_A/0/1/0/all/0/1&quot;&gt;Amrutha Saseendran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diethe_T/0/1/0/all/0/1&quot;&gt;Tom Diethe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teare_P/0/1/0/all/0/1&quot;&gt;Philip Teare&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12300">
<title>Measuring Pointwise $\mathcal{V}$-Usable Information In-Context-ly. (arXiv:2310.12300v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12300</link>
<description rdf:parseType="Literal">&lt;p&gt;In-context learning (ICL) is a new learning paradigm that has gained
popularity along with the development of large language models. In this work,
we adapt a recently proposed hardness metric, pointwise $\mathcal{V}$-usable
information (PVI), to an in-context version (in-context PVI). Compared to the
original PVI, in-context PVI is more efficient in that it requires only a few
exemplars and does not require fine-tuning. We conducted a comprehensive
empirical analysis to evaluate the reliability of in-context PVI. Our findings
indicate that in-context PVI estimates exhibit similar characteristics to the
original PVI. Specific to the in-context setting, we show that in-context PVI
estimates remain consistent across different exemplar selections and numbers of
shots. The variance of in-context PVI estimates across different exemplar
selections is insignificant, which suggests that in-context PVI are stable.
Furthermore, we demonstrate how in-context PVI can be employed to identify
challenging instances. Our work highlights the potential of in-context PVI and
provides new insights into the capabilities of ICL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1&quot;&gt;Sheng Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yingya Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bitterman_D/0/1/0/all/0/1&quot;&gt;Danielle Bitterman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savova_G/0/1/0/all/0/1&quot;&gt;Guergana Savova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1&quot;&gt;Iryna Gurevych&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12303">
<title>Document-Level Language Models for Machine Translation. (arXiv:2310.12303v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12303</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the known limitations, most machine translation systems today still
operate on the sentence-level. One reason for this is, that most parallel
training data is only sentence-level aligned, without document-level meta
information available. In this work, we set out to build context-aware
translation systems utilizing document-level monolingual data instead. This can
be achieved by combining any existing sentence-level translation model with a
document-level language model. We improve existing approaches by leveraging
recent advancements in model combination. Additionally, we propose novel
weighting techniques that make the system combination more flexible and
significantly reduce computational overhead. In a comprehensive evaluation on
four diverse translation tasks, we show that our extensions improve
document-targeted scores substantially and are also computationally more
efficient. However, we also find that in most scenarios, back-translation gives
even better results, at the cost of having to re-train the translation system.
Finally, we explore language model fusion in the light of recent advancements
in large language models. Our findings suggest that there might be strong
potential in utilizing large language models via model combination.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petrick_F/0/1/0/all/0/1&quot;&gt;Frithjof Petrick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herold_C/0/1/0/all/0/1&quot;&gt;Christian Herold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petrushkov_P/0/1/0/all/0/1&quot;&gt;Pavel Petrushkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khadivi_S/0/1/0/all/0/1&quot;&gt;Shahram Khadivi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1&quot;&gt;Hermann Ney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12318">
<title>The Sentiment Problem: A Critical Survey towards Deconstructing Sentiment Analysis. (arXiv:2310.12318v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12318</link>
<description rdf:parseType="Literal">&lt;p&gt;We conduct an inquiry into the sociotechnical aspects of sentiment analysis
(SA) by critically examining 189 peer-reviewed papers on their applications,
models, and datasets. Our investigation stems from the recognition that SA has
become an integral component of diverse sociotechnical systems, exerting
influence on both social and technical users. By delving into sociological and
technological literature on sentiment, we unveil distinct conceptualizations of
this term in domains such as finance, government, and medicine. Our study
exposes a lack of explicit definitions and frameworks for characterizing
sentiment, resulting in potential challenges and biases. To tackle this issue,
we propose an ethics sheet encompassing critical inquiries to guide
practitioners in ensuring equitable utilization of SA. Our findings underscore
the significance of adopting an interdisciplinary approach to defining
sentiment in SA and offer a pragmatic solution for its implementation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkit_P/0/1/0/all/0/1&quot;&gt;Pranav Narayanan Venkit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinath_M/0/1/0/all/0/1&quot;&gt;Mukund Srinath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gautam_S/0/1/0/all/0/1&quot;&gt;Sanjana Gautam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkatraman_S/0/1/0/all/0/1&quot;&gt;Saranya Venkatraman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1&quot;&gt;Vipul Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Passonneau_R/0/1/0/all/0/1&quot;&gt;Rebecca J. Passonneau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_S/0/1/0/all/0/1&quot;&gt;Shomir Wilson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12321">
<title>A Survey of GPT-3 Family Large Language Models Including ChatGPT and GPT-4. (arXiv:2310.12321v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12321</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) are a special class of pretrained language
models obtained by scaling model size, pretraining corpus and computation.
LLMs, because of their large size and pretraining on large volumes of text
data, exhibit special abilities which allow them to achieve remarkable
performances without any task-specific training in many of the natural language
processing tasks. The era of LLMs started with OpenAI GPT-3 model, and the
popularity of LLMs is increasing exponentially after the introduction of models
like ChatGPT and GPT4. We refer to GPT-3 and its successor OpenAI models,
including ChatGPT and GPT4, as GPT-3 family large language models (GLLMs). With
the ever-rising popularity of GLLMs, especially in the research community,
there is a strong need for a comprehensive survey which summarizes the recent
research progress in multiple dimensions and can guide the research community
with insightful future research directions. We start the survey paper with
foundation concepts like transformers, transfer learning, self-supervised
learning, pretrained language models and large language models. We then present
a brief overview of GLLMs and discuss the performances of GLLMs in various
downstream tasks, specific domains and multiple languages. We also discuss the
data labelling and data augmentation abilities of GLLMs, the robustness of
GLLMs, the effectiveness of GLLMs as evaluators, and finally, conclude with
multiple insightful future research directions. To summarize, this
comprehensive survey paper will serve as a good resource for both academic and
industry people to stay updated with the latest research related to GPT-3
family large language models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalyan_K/0/1/0/all/0/1&quot;&gt;Katikapalli Subramanyam Kalyan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12342">
<title>Eliminating Reasoning via Inferring with Planning: A New Framework to Guide LLMs&apos; Non-linear Thinking. (arXiv:2310.12342v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12342</link>
<description rdf:parseType="Literal">&lt;p&gt;Chain-of-Thought(CoT) prompting and its variants explore equipping large
language models (LLMs) with high-level reasoning abilities by emulating
human-like linear cognition and logic. However, the human mind is complicated
and mixed with both linear and nonlinear thinking. In this work, we propose
\textbf{I}nferential \textbf{E}xclusion \textbf{P}rompting (IEP), a novel
prompting that combines the principles of elimination and inference in order to
guide LLMs to think non-linearly. IEP guides LLMs to plan and then utilize
Natural Language Inference (NLI) to deduce each possible solution&apos;s entailment
relation with context, commonsense, or facts, therefore yielding a broader
perspective by thinking back for inferring. This forward planning and backward
eliminating process allows IEP to better simulate the complex human thinking
processes compared to other CoT-based methods, which only reflect linear
cognitive processes. We conducted a series of empirical studies and have
corroborated that IEP consistently outperforms CoT across various tasks.
Additionally, we observe that integrating IEP and CoT further improves the
LLMs&apos; performance on certain tasks, highlighting the necessity of equipping
LLMs with mixed logic processes. Moreover, to better evaluate comprehensive
features inherent in human logic, we introduce \textbf{M}ental-\textbf{A}bility
\textbf{R}easoning \textbf{B}enchmark (MARB). The benchmark comprises six novel
subtasks with a total of 9,115 questions, among which 1,685 are developed with
hand-crafted rationale references. We believe both \textsc{IEP} and
\textsc{MARB} can serve as a promising direction for unveiling LLMs&apos; logic and
verbal reasoning abilities and drive further advancements. \textsc{MARB} will
be available at ~\texttt{anonymity link} soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1&quot;&gt;Yongqi Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yifan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dawei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sizhe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1&quot;&gt;Simeng Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1&quot;&gt;Jingbo Shang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12344">
<title>LACMA: Language-Aligning Contrastive Learning with Meta-Actions for Embodied Instruction Following. (arXiv:2310.12344v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12344</link>
<description rdf:parseType="Literal">&lt;p&gt;End-to-end Transformers have demonstrated an impressive success rate for
Embodied Instruction Following when the environment has been seen in training.
However, they tend to struggle when deployed in an unseen environment. This
lack of generalizability is due to the agent&apos;s insensitivity to subtle changes
in natural language instructions. To mitigate this issue, we propose explicitly
aligning the agent&apos;s hidden states with the instructions via contrastive
learning. Nevertheless, the semantic gap between high-level language
instructions and the agent&apos;s low-level action space remains an obstacle.
Therefore, we further introduce a novel concept of meta-actions to bridge the
gap. Meta-actions are ubiquitous action patterns that can be parsed from the
original action sequence. These patterns represent higher-level semantics that
are intuitively aligned closer to the instructions. When meta-actions are
applied as additional training signals, the agent generalizes better to unseen
environments. Compared to a strong multi-modal Transformer baseline, we achieve
a significant 4.5% absolute gain in success rate in unseen environments of
ALFRED Embodied Instruction Following. Additional analysis shows that the
contrastive objective and meta-actions are complementary in achieving the best
results, and the resulting agent better aligns its states with corresponding
instructions, making it more suitable for real-world embodied agents. The code
is available at: https://github.com/joeyy5588/LACMA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Cheng-Fu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yen-Chun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jianwei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1&quot;&gt;Xiyang Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1&quot;&gt;Lu Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Chiang Frank Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1&quot;&gt;Kai-Wei Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12352">
<title>knn-seq: Efficient, Extensible kNN-MT Framework. (arXiv:2310.12352v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12352</link>
<description rdf:parseType="Literal">&lt;p&gt;k-nearest-neighbor machine translation (kNN-MT) boosts the translation
quality of a pre-trained neural machine translation (NMT) model by utilizing
translation examples during decoding. Translation examples are stored in a
vector database, called a datastore, which contains one entry for each target
token from the parallel data it is made from. Due to its size, it is
computationally expensive both to construct and to retrieve examples from the
datastore. In this paper, we present an efficient and extensible kNN-MT
framework, knn-seq, for researchers and developers that is carefully designed
to run efficiently, even with a billion-scale large datastore. knn-seq is
developed as a plug-in on fairseq and easy to switch models and kNN indexes.
Experimental results show that our implemented kNN-MT achieves a comparable
gain to the original kNN-MT, and the billion-scale datastore construction took
2.21 hours in the WMT&apos;19 German-to-English translation task. We publish our
knn-seq as an MIT-licensed open-source project and the code is available on
https://github.com/naist-nlp/knn-seq . The demo video is available on
https://youtu.be/zTDzEOq80m0 .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deguchi_H/0/1/0/all/0/1&quot;&gt;Hiroyuki Deguchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hirano_H/0/1/0/all/0/1&quot;&gt;Hayate Hirano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoshino_T/0/1/0/all/0/1&quot;&gt;Tomoki Hoshino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nishida_Y/0/1/0/all/0/1&quot;&gt;Yuto Nishida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasselli_J/0/1/0/all/0/1&quot;&gt;Justin Vasselli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watanabe_T/0/1/0/all/0/1&quot;&gt;Taro Watanabe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12360">
<title>GRI: Graph-based Relative Isomorphism of Word Embedding Spaces. (arXiv:2310.12360v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12360</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated construction of bilingual dictionaries using monolingual embedding
spaces is a core challenge in machine translation. The end performance of these
dictionaries relies upon the geometric similarity of individual spaces, i.e.,
their degree of isomorphism. Existing attempts aimed at controlling the
relative isomorphism of different spaces fail to incorporate the impact of
semantically related words in the training objective. To address this, we
propose GRI that combines the distributional training objectives with attentive
graph convolutions to unanimously consider the impact of semantically similar
words required to define/compute the relative isomorphism of multiple spaces.
Experimental evaluation shows that GRI outperforms the existing research by
improving the average P@1 by a relative score of up to 63.6%. We release the
codes for GRI at https://github.com/asif6827/GRI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1&quot;&gt;Muhammad Asif Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1&quot;&gt;Jianbin Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Di Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12362">
<title>REMARK-LLM: A Robust and Efficient Watermarking Framework for Generative Large Language Models. (arXiv:2310.12362v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2310.12362</link>
<description rdf:parseType="Literal">&lt;p&gt;We present REMARK-LLM, a novel efficient, and robust watermarking framework
designed for texts generated by large language models (LLMs). Synthesizing
human-like content using LLMs necessitates vast computational resources and
extensive datasets, encapsulating critical intellectual property (IP). However,
the generated content is prone to malicious exploitation, including spamming
and plagiarism. To address the challenges, REMARK-LLM proposes three new
components: (i) a learning-based message encoding module to infuse binary
signatures into LLM-generated texts; (ii) a reparameterization module to
transform the dense distributions from the message encoding to the sparse
distribution of the watermarked textual tokens; (iii) a decoding module
dedicated for signature extraction; Furthermore, we introduce an optimized beam
search algorithm to guarantee the coherence and consistency of the generated
content. REMARK-LLM is rigorously trained to encourage the preservation of
semantic integrity in watermarked content, while ensuring effective watermark
retrieval. Extensive evaluations on multiple unseen datasets highlight
REMARK-LLM proficiency and transferability in inserting 2 times more signature
bits into the same texts when compared to prior art, all while maintaining
semantic integrity. Furthermore, REMARK-LLM exhibits better resilience against
a spectrum of watermark detection and removal attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruisi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hussain_S/0/1/0/all/0/1&quot;&gt;Shehzeen Samarah Hussain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neekhara_P/0/1/0/all/0/1&quot;&gt;Paarth Neekhara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koushanfar_F/0/1/0/all/0/1&quot;&gt;Farinaz Koushanfar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12379">
<title>Solving Hard Analogy Questions with Relation Embedding Chains. (arXiv:2310.12379v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12379</link>
<description rdf:parseType="Literal">&lt;p&gt;Modelling how concepts are related is a central topic in Lexical Semantics. A
common strategy is to rely on knowledge graphs (KGs) such as ConceptNet, and to
model the relation between two concepts as a set of paths. However, KGs are
limited to a fixed set of relation types, and they are incomplete and often
noisy. Another strategy is to distill relation embeddings from a fine-tuned
language model. However, this is less suitable for words that are only
indirectly related and it does not readily allow us to incorporate structured
domain knowledge. In this paper, we aim to combine the best of both worlds. We
model relations as paths but associate their edges with relation embeddings.
The paths are obtained by first identifying suitable intermediate words and
then selecting those words for which informative relation embeddings can be
obtained. We empirically show that our proposed representations are useful for
solving hard analogy questions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1&quot;&gt;Nitesh Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schockaert_S/0/1/0/all/0/1&quot;&gt;Steven Schockaert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12404">
<title>Loop Copilot: Conducting AI Ensembles for Music Generation and Iterative Editing. (arXiv:2310.12404v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2310.12404</link>
<description rdf:parseType="Literal">&lt;p&gt;Creating music is iterative, requiring varied methods at each stage. However,
existing AI music systems fall short in orchestrating multiple subsystems for
diverse needs. To address this gap, we introduce Loop Copilot, a novel system
that enables users to generate and iteratively refine music through an
interactive, multi-round dialogue interface. The system uses a large language
model to interpret user intentions and select appropriate AI models for task
execution. Each backend model is specialized for a specific task, and their
outputs are aggregated to meet the user&apos;s requirements. To ensure musical
coherence, essential attributes are maintained in a centralized table. We
evaluate the effectiveness of the proposed system through semi-structured
interviews and questionnaires, highlighting its utility not only in
facilitating music creation but also its potential for broader applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yixiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maezawa_A/0/1/0/all/0/1&quot;&gt;Akira Maezawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1&quot;&gt;Gus Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamamoto_K/0/1/0/all/0/1&quot;&gt;Kazuhiko Yamamoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dixon_S/0/1/0/all/0/1&quot;&gt;Simon Dixon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12406">
<title>FinEntity: Entity-level Sentiment Classification for Financial Texts. (arXiv:2310.12406v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12406</link>
<description rdf:parseType="Literal">&lt;p&gt;In the financial domain, conducting entity-level sentiment analysis is
crucial for accurately assessing the sentiment directed toward a specific
financial entity. To our knowledge, no publicly available dataset currently
exists for this purpose. In this work, we introduce an entity-level sentiment
classification dataset, called \textbf{FinEntity}, that annotates financial
entity spans and their sentiment (positive, neutral, and negative) in financial
news. We document the dataset construction process in the paper. Additionally,
we benchmark several pre-trained models (BERT, FinBERT, etc.) and ChatGPT on
entity-level sentiment classification. In a case study, we demonstrate the
practical utility of using FinEntity in monitoring cryptocurrency markets. The
data and code of FinEntity is available at
\url{https://github.com/yixuantt/FinEntity}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yixuan Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_A/0/1/0/all/0/1&quot;&gt;Allen H Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tam_A/0/1/0/all/0/1&quot;&gt;Andy Tam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Justin Z Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12418">
<title>The Shifted and The Overlooked: A Task-oriented Investigation of User-GPT Interactions. (arXiv:2310.12418v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12418</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent progress in Large Language Models (LLMs) has produced models that
exhibit remarkable performance across a variety of NLP tasks. However, it
remains unclear whether the existing focus of NLP research accurately captures
the genuine requirements of human users. This paper provides a comprehensive
analysis of the divergence between current NLP research and the needs of
real-world NLP applications via a large-scale collection of user-GPT
conversations. We analyze a large-scale collection of real user queries to GPT.
We compare these queries against existing NLP benchmark tasks and identify a
significant gap between the tasks that users frequently request from LLMs and
the tasks that are commonly studied in academic research. For example, we find
that tasks such as ``design&apos;&apos; and ``planning&apos;&apos; are prevalent in user
interactions but are largely neglected or different from traditional NLP
benchmarks. We investigate these overlooked tasks, dissect the practical
challenges they pose, and provide insights toward a roadmap to make LLMs better
aligned with user needs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1&quot;&gt;Siru Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuohang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1&quot;&gt;Ming Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1&quot;&gt;Yizhu Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iter_D/0/1/0/all/0/1&quot;&gt;Dan Iter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pryzant_R/0/1/0/all/0/1&quot;&gt;Reid Pryzant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Chenguang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1&quot;&gt;Heng Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jiawei Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12426">
<title>MAF: Multi-Aspect Feedback for Improving Reasoning in Large Language Models. (arXiv:2310.12426v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12426</link>
<description rdf:parseType="Literal">&lt;p&gt;Language Models (LMs) have shown impressive performance in various natural
language tasks. However, when it comes to natural language reasoning, LMs still
face challenges such as hallucination, generating incorrect intermediate
reasoning steps, and making mathematical errors. Recent research has focused on
enhancing LMs through self-improvement using feedback. Nevertheless, existing
approaches relying on a single generic feedback source fail to address the
diverse error types found in LM-generated reasoning chains. In this work, we
propose Multi-Aspect Feedback, an iterative refinement framework that
integrates multiple feedback modules, including frozen LMs and external tools,
each focusing on a specific error category. Our experimental results
demonstrate the efficacy of our approach to addressing several errors in the
LM-generated reasoning chain and thus improving the overall performance of an
LM in several reasoning tasks. We see a relative improvement of up to 20% in
Mathematical Reasoning and up to 18% in Logical Entailment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nathani_D/0/1/0/all/0/1&quot;&gt;Deepak Nathani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;David Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1&quot;&gt;Liangming Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;William Yang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12430">
<title>DocXChain: A Powerful Open-Source Toolchain for Document Parsing and Beyond. (arXiv:2310.12430v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12430</link>
<description rdf:parseType="Literal">&lt;p&gt;In this report, we introduce DocXChain, a powerful open-source toolchain for
document parsing, which is designed and developed to automatically convert the
rich information embodied in unstructured documents, such as text, tables and
charts, into structured representations that are readable and manipulable by
machines. Specifically, basic capabilities, including text detection, text
recognition, table structure recognition and layout analysis, are provided.
Upon these basic capabilities, we also build a set of fully functional
pipelines for document parsing, i.e., general text reading, table parsing, and
document structurization, to drive various applications related to documents in
real-world scenarios. Moreover, DocXChain is concise, modularized and flexible,
such that it can be readily integrated with existing tools, libraries or models
(such as LangChain and ChatGPT), to construct more powerful systems that can
accomplish more complicated and challenging tasks. The code of DocXChain is
publicly available
at:~\url{https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/Applications/DocXChain}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_C/0/1/0/all/0/1&quot;&gt;Cong Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12439">
<title>PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models. (arXiv:2310.12439v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12439</link>
<description rdf:parseType="Literal">&lt;p&gt;Prompts have significantly improved the performance of pretrained Large
Language Models (LLMs) on various downstream tasks recently, making them
increasingly indispensable for a diverse range of LLM application scenarios.
However, the backdoor vulnerability, a serious security threat that can
maliciously alter the victim model&apos;s normal predictions, has not been
sufficiently explored for prompt-based LLMs. In this paper, we present
POISONPROMPT, a novel backdoor attack capable of successfully compromising both
hard and soft prompt-based LLMs. We evaluate the effectiveness, fidelity, and
robustness of POISONPROMPT through extensive experiments on three popular
prompt methods, using six datasets and three widely used LLMs. Our findings
highlight the potential security threats posed by backdoor attacks on
prompt-based LLMs and emphasize the need for further research in this area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1&quot;&gt;Hongwei Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1&quot;&gt;Jian Lou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1&quot;&gt;Zhan Qin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12442">
<title>Efficient Long-Range Transformers: You Need to Attend More, but Not Necessarily at Every Layer. (arXiv:2310.12442v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12442</link>
<description rdf:parseType="Literal">&lt;p&gt;Pretrained transformer models have demonstrated remarkable performance across
various natural language processing tasks. These models leverage the attention
mechanism to capture long- and short-range dependencies in the sequence.
However, the (full) attention mechanism incurs high computational cost -
quadratic in the sequence length, which is not affordable in tasks with long
sequences, e.g., inputs with 8k tokens. Although sparse attention can be used
to improve computational efficiency, as suggested in existing work, it has
limited modeling capacity and often fails to capture complicated dependencies
in long sequences. To tackle this challenge, we propose MASFormer, an
easy-to-implement transformer variant with Mixed Attention Spans. Specifically,
MASFormer is equipped with full attention to capture long-range dependencies,
but only at a small number of layers. For the remaining layers, MASformer only
employs sparse attention to capture short-range dependencies. Our experiments
on natural language modeling and generation tasks show that a decoder-only
MASFormer model of 1.3B parameters can achieve competitive performance to
vanilla transformers with full attention while significantly reducing
computational cost (up to 75%). Additionally, we investigate the effectiveness
of continual training with long sequence data and how sequence length impacts
downstream generation performance, which may be of independent interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qingru Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ram_D/0/1/0/all/0/1&quot;&gt;Dhananjay Ram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hawkins_C/0/1/0/all/0/1&quot;&gt;Cole Hawkins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_S/0/1/0/all/0/1&quot;&gt;Sheng Zha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tuo Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12443">
<title>Know Where to Go: Make LLM a Relevant, Responsible, and Trustworthy Searcher. (arXiv:2310.12443v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2310.12443</link>
<description rdf:parseType="Literal">&lt;p&gt;The advent of Large Language Models (LLMs) has shown the potential to improve
relevance and provide direct answers in web searches. However, challenges arise
in validating the reliability of generated results and the credibility of
contributing sources, due to the limitations of traditional information
retrieval algorithms and the LLM hallucination problem. Aiming to create a
&quot;PageRank&quot; for the LLM era, we strive to transform LLM into a relevant,
responsible, and trustworthy searcher. We propose a novel generative retrieval
framework leveraging the knowledge of LLMs to foster a direct link between
queries and online sources. This framework consists of three core modules:
Generator, Validator, and Optimizer, each focusing on generating trustworthy
online sources, verifying source reliability, and refining unreliable sources,
respectively. Extensive experiments and evaluations highlight our method&apos;s
superior relevance, responsibility, and trustfulness against various SOTA
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1&quot;&gt;Xiang Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiawei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yinpeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1&quot;&gt;Qikai Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1&quot;&gt;Wei Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12444">
<title>Revisiting Sparse Retrieval for Few-shot Entity Linking. (arXiv:2310.12444v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12444</link>
<description rdf:parseType="Literal">&lt;p&gt;Entity linking aims to link ambiguous mentions to their corresponding
entities in a knowledge base. One of the key challenges comes from insufficient
labeled data for specific domains. Although dense retrievers have achieved
excellent performance on several benchmarks, their performance decreases
significantly when only a limited amount of in-domain labeled data is
available. In such few-shot setting, we revisit the sparse retrieval method,
and propose an ELECTRA-based keyword extractor to denoise the mention context
and construct a better query expression. For training the extractor, we propose
a distant supervision method to automatically generate training data based on
overlapping tokens between mention contexts and entity descriptions.
Experimental results on the ZESHEL dataset demonstrate that the proposed method
outperforms state-of-the-art models by a significant margin across all test
domains, showing the effectiveness of keyword-enhanced sparse retrieval.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yulin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhenran Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1&quot;&gt;Baotian Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Min Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12450">
<title>A Read-and-Select Framework for Zero-shot Entity Linking. (arXiv:2310.12450v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12450</link>
<description rdf:parseType="Literal">&lt;p&gt;Zero-shot entity linking (EL) aims at aligning entity mentions to unseen
entities to challenge the generalization ability. Previous methods largely
focus on the candidate retrieval stage and ignore the essential candidate
ranking stage, which disambiguates among entities and makes the final linking
prediction. In this paper, we propose a read-and-select (ReS) framework by
modeling the main components of entity disambiguation, i.e., mention-entity
matching and cross-entity comparison. First, for each candidate, the reading
module leverages mention context to output mention-aware entity
representations, enabling mention-entity matching. Then, in the selecting
module, we frame the choice of candidates as a sequence labeling problem, and
all candidate representations are fused together to enable cross-entity
comparison. Our method achieves the state-of-the-art performance on the
established zero-shot EL dataset ZESHEL with a 2.55\% micro-average accuracy
gain, with no need for laborious multi-phase pre-training used in most of the
previous work, showing the effectiveness of both mention-entity and
cross-entity interaction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhenran Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yulin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1&quot;&gt;Baotian Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Min Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12454">
<title>Rethinking the Construction of Effective Metrics for Understanding the Mechanisms of Pretrained Language Models. (arXiv:2310.12454v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12454</link>
<description rdf:parseType="Literal">&lt;p&gt;Pretrained language models are expected to effectively map input text to a
set of vectors while preserving the inherent relationships within the text.
Consequently, designing a white-box model to compute metrics that reflect the
presence of specific internal relations in these vectors has become a common
approach for post-hoc interpretability analysis of pretrained language models.
However, achieving interpretability in white-box models and ensuring the rigor
of metric computation becomes challenging when the source model lacks inherent
interpretability. Therefore, in this paper, we discuss striking a balance in
this trade-off and propose a novel line to constructing metrics for
understanding the mechanisms of pretrained language models. We have
specifically designed a family of metrics along this line of investigation, and
the model used to compute these metrics is referred to as the tree topological
probe. We conducted measurements on BERT-large by using these metrics. Based on
the experimental results, we propose a speculation regarding the working
mechanism of BERT-like pretrained language models, as well as a strategy for
enhancing fine-tuning performance by leveraging the topological probe to
improve specific submodules.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;You Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1&quot;&gt;Jinhui Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yuming Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12462">
<title>Unmasking Transformers: A Theoretical Approach to Data Recovery via Attention Weights. (arXiv:2310.12462v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12462</link>
<description rdf:parseType="Literal">&lt;p&gt;In the realm of deep learning, transformers have emerged as a dominant
architecture, particularly in natural language processing tasks. However, with
their widespread adoption, concerns regarding the security and privacy of the
data processed by these models have arisen. In this paper, we address a pivotal
question: Can the data fed into transformers be recovered using their attention
weights and outputs? We introduce a theoretical framework to tackle this
problem. Specifically, we present an algorithm that aims to recover the input
data $X \in \mathbb{R}^{d \times n}$ from given attention weights $W = QK^\top
\in \mathbb{R}^{d \times d}$ and output $B \in \mathbb{R}^{n \times n}$ by
minimizing the loss function $L(X)$. This loss function captures the
discrepancy between the expected output and the actual output of the
transformer. Our findings have significant implications for the Localized
Layer-wise Mechanism (LLM), suggesting potential vulnerabilities in the model&apos;s
design from a security and privacy perspective. This work underscores the
importance of understanding and safeguarding the internal workings of
transformers to ensure the confidentiality of processed data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yichuan Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zhao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1&quot;&gt;Shenghao Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chiwun Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12467">
<title>Contrastive Learning for Inference in Dialogue. (arXiv:2310.12467v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12467</link>
<description rdf:parseType="Literal">&lt;p&gt;Inference, especially those derived from inductive processes, is a crucial
component in our conversation to complement the information implicitly or
explicitly conveyed by a speaker. While recent large language models show
remarkable advances in inference tasks, their performance in inductive
reasoning, where not all information is present in the context, is far behind
deductive reasoning. In this paper, we analyze the behavior of the models based
on the task difficulty defined by the semantic information gap -- which
distinguishes inductive and deductive reasoning (Johnson-Laird, 1988, 1993).
Our analysis reveals that the disparity in information between dialogue
contexts and desired inferences poses a significant challenge to the inductive
inference process. To mitigate this information gap, we investigate a
contrastive learning approach by feeding negative samples. Our experiments
suggest negative samples help models understand what is wrong and improve their
inference generations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ishii_E/0/1/0/all/0/1&quot;&gt;Etsuko Ishii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilie_B/0/1/0/all/0/1&quot;&gt;Bryan Wilie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1&quot;&gt;Ziwei Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lovenia_H/0/1/0/all/0/1&quot;&gt;Holy Lovenia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_W/0/1/0/all/0/1&quot;&gt;Willy Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1&quot;&gt;Pascale Fung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12477">
<title>An Exploration of In-Context Learning for Speech Language Model. (arXiv:2310.12477v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2310.12477</link>
<description rdf:parseType="Literal">&lt;p&gt;Ever since the development of GPT-3 in the natural language processing (NLP)
field, in-context learning (ICL) has played an important role in utilizing
large language models (LLMs). By presenting the LM utterance-label
demonstrations at the input, the LM can accomplish few-shot learning without
relying on gradient descent or requiring explicit modification of its
parameters. This enables the LM to learn and adapt in a black-box manner.
Despite the success of ICL in NLP, little work is exploring the possibility of
ICL in speech processing. This study proposes the first exploration of ICL with
a speech LM without text supervision. We first show that the current speech LM
does not have the ICL capability. With the proposed warmup training, the speech
LM can, therefore, perform ICL on unseen tasks. In this work, we verify the
feasibility of ICL for speech LM on speech classification tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hsu_M/0/1/0/all/0/1&quot;&gt;Ming-Hao Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chang_K/0/1/0/all/0/1&quot;&gt;Kai-Wei Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shang-Wen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hung-yi Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12481">
<title>Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in Large Language Models. (arXiv:2310.12481v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12481</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we identify a cultural dominance issue within large language
models (LLMs) due to the predominant use of English data in model training
(e.g. ChatGPT). LLMs often provide inappropriate English-culture-related
answers that are not relevant to the expected culture when users ask in
non-English languages. To systematically evaluate the cultural dominance issue,
we build a benchmark that consists of both concrete (e.g. holidays and songs)
and abstract (e.g. values and opinions) cultural objects. Empirical results
show that the representative GPT models suffer from the culture dominance
problem, where GPT-4 is the most affected while text-davinci-003 suffers the
least from this problem. Our study emphasizes the need for critical examination
of cultural dominance and ethical consideration in their development and
deployment. We show two straightforward methods in model development (i.e.
pretraining on more diverse data) and deployment (e.g. culture-aware prompting)
can significantly mitigate the cultural dominance issue in LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenxuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1&quot;&gt;Wenxiang Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jingyuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_R/0/1/0/all/0/1&quot;&gt;Ruyi Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jen-tse Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1&quot;&gt;Zhaopeng Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1&quot;&gt;Michael R. Lyu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12489">
<title>MedAI Dialog Corpus (MEDIC): Zero-Shot Classification of Doctor and AI Responses in Health Consultations. (arXiv:2310.12489v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12489</link>
<description rdf:parseType="Literal">&lt;p&gt;Zero-shot classification has enabled the classification of text into classes
that were not seen during training. In this paper, we investigate the
effectiveness of pre-trained language models to accurately classify responses
from Doctors and AI in health consultations through zero-shot learning. Our
study aims to determine whether these models can effectively detect if a text
originates from human or AI models without specific corpus training. For our
experiments, we collected responses from doctors to patient inquiries about
their health and posed the same question/response to AI models. Our findings
revealed that while pre-trained language models demonstrate a strong
understanding of language generally, they may require specific corpus training
or other techniques to achieve accurate classification of doctor- and
AI-generated text in healthcare consultations. As a baseline approach, this
study shows the limitations of relying solely on zero-shot classification in
medical classification tasks. This research lays the groundwork for further
research into the field of medical text classification, informing the
development of more effective approaches to accurately classify doctor- and
AI-generated text in health consultations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ojo_O/0/1/0/all/0/1&quot;&gt;Olumide E. Ojo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adebanji_O/0/1/0/all/0/1&quot;&gt;Olaronke O. Adebanji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gelbukh_A/0/1/0/all/0/1&quot;&gt;Alexander Gelbukh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calvo_H/0/1/0/all/0/1&quot;&gt;Hiram Calvo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feldman_A/0/1/0/all/0/1&quot;&gt;Anna Feldman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12490">
<title>Co$^2$PT: Mitigating Bias in Pre-trained Language Models through Counterfactual Contrastive Prompt Tuning. (arXiv:2310.12490v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12490</link>
<description rdf:parseType="Literal">&lt;p&gt;Pre-trained Language Models are widely used in many important real-world
applications. However, recent studies show that these models can encode social
biases from large pre-training corpora and even amplify biases in downstream
applications. To address this challenge, we propose Co$^2$PT, an efficient and
effective debias-while-prompt tuning method for mitigating biases via
counterfactual contrastive prompt tuning on downstream tasks. Our experiments
conducted on three extrinsic bias benchmarks demonstrate the effectiveness of
Co$^2$PT on bias mitigation during the prompt tuning process and its
adaptability to existing upstream debiased language models. These findings
indicate the strength of Co$^2$PT and provide promising avenues for further
enhancement in bias mitigation on downstream tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1&quot;&gt;Xiangjue Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Ziwei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhuoer Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teleki_M/0/1/0/all/0/1&quot;&gt;Maria Teleki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caverlee_J/0/1/0/all/0/1&quot;&gt;James Caverlee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12505">
<title>Attack Prompt Generation for Red Teaming and Defending Large Language Models. (arXiv:2310.12505v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12505</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) are susceptible to red teaming attacks, which
can induce LLMs to generate harmful content. Previous research constructs
attack prompts via manual or automatic methods, which have their own
limitations on construction cost and quality. To address these issues, we
propose an integrated approach that combines manual and automatic methods to
economically generate high-quality attack prompts. Specifically, considering
the impressive capabilities of newly emerged LLMs, we propose an attack
framework to instruct LLMs to mimic human-generated prompts through in-context
learning. Furthermore, we propose a defense framework that fine-tunes victim
LLMs through iterative interactions with the attack framework to enhance their
safety against red teaming attacks. Extensive experiments on different LLMs
validate the effectiveness of our proposed attack and defense frameworks.
Additionally, we release a series of attack prompts datasets named SAP with
varying sizes, facilitating the safety evaluation and enhancement of more LLMs.
Our code and dataset is available on https://github.com/Aatrox103/SAP .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1&quot;&gt;Boyi Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenjie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1&quot;&gt;Fuli Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yang Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qifan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xiangnan He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12516">
<title>Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks. (arXiv:2310.12516v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12516</link>
<description rdf:parseType="Literal">&lt;p&gt;Although remarkable progress has been achieved in preventing large language
model (LLM) hallucinations using instruction tuning and retrieval augmentation,
it remains challenging to measure the reliability of LLMs using human-crafted
evaluation data which is not available for many tasks and domains and could
suffer from data leakage. Inspired by adversarial machine learning, this paper
aims to develop a method of automatically generating evaluation data by
appropriately modifying existing data on which LLMs behave faithfully.
Specifically, this paper presents AutoDebug, an LLM-based framework to use
prompting chaining to generate transferable adversarial attacks in the form of
question-answering examples. We seek to understand the extent to which these
examples trigger the hallucination behaviors of LLMs.
&lt;/p&gt;
&lt;p&gt;We implement AutoDebug using ChatGPT and evaluate the resulting two variants
of a popular open-domain question-answering dataset, Natural Questions (NQ), on
a collection of open-source and proprietary LLMs under various prompting
settings. Our generated evaluation data is human-readable and, as we show,
humans can answer these modified questions well. Nevertheless, we observe
pronounced accuracy drops across multiple LLMs including GPT-4. Our
experimental results show that LLMs are likely to hallucinate in two categories
of question-answering scenarios where (1) there are conflicts between knowledge
given in the prompt and their parametric knowledge, or (2) the knowledge
expressed in the prompt is complex. Finally, we find that the adversarial
examples generated by our method are transferable across all considered LLMs.
The examples generated by a small model can be used to debug a much larger
model, making our approach cost-effective.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xiaodong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1&quot;&gt;Hao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaodong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1&quot;&gt;Dan Roth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jianfeng Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12520">
<title>Lost in Translation: When GPT-4V(ision) Can&apos;t See Eye to Eye with Text. A Vision-Language-Consistency Analysis of VLLMs and Beyond. (arXiv:2310.12520v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12520</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in multimodal techniques open exciting possibilities for
models excelling in diverse tasks involving text, audio, and image processing.
Models like GPT-4V, blending computer vision and language modeling, excel in
complex text and image tasks. Numerous prior research endeavors have diligently
examined the performance of these Vision Large Language Models (VLLMs) across
tasks like object detection, image captioning and others. However, these
analyses often focus on evaluating the performance of each modality in
isolation, lacking insights into their cross-modal interactions. Specifically,
questions concerning whether these vision-language models execute vision and
language tasks consistently or independently have remained unanswered. In this
study, we draw inspiration from recent investigations into multilingualism and
conduct a comprehensive analysis of model&apos;s cross-modal interactions. We
introduce a systematic framework that quantifies the capability disparities
between different modalities in the multi-modal setting and provide a set of
datasets designed for these evaluations. Our findings reveal that models like
GPT-4V tend to perform consistently modalities when the tasks are relatively
simple. However, the trustworthiness of results derived from the vision
modality diminishes as the tasks become more challenging. Expanding on our
findings, we introduce &quot;Vision Description Prompting,&quot; a method that
effectively improves performance in challenging vision-related tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Senyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zijun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_N/0/1/0/all/0/1&quot;&gt;Ning Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12522">
<title>Named Entity Recognition for Monitoring Plant Health Threats in Tweets: a ChouBERT Approach. (arXiv:2310.12522v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12522</link>
<description rdf:parseType="Literal">&lt;p&gt;An important application scenario of precision agriculture is detecting and
measuring crop health threats using sensors and data analysis techniques.
However, the textual data are still under-explored among the existing solutions
due to the lack of labelled data and fine-grained semantic resources. Recent
research suggests that the increasing connectivity of farmers and the emergence
of online farming communities make social media like Twitter a participatory
platform for detecting unfamiliar plant health events if we can extract
essential information from unstructured textual data. ChouBERT is a French
pre-trained language model that can identify Tweets concerning observations of
plant health issues with generalizability on unseen natural hazards. This paper
tackles the lack of labelled data by further studying ChouBERT&apos;s know-how on
token-level annotation tasks over small labeled sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1&quot;&gt;Shufan Jiang&lt;/a&gt; (CRESTIC, ISEP), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Angarita_R/0/1/0/all/0/1&quot;&gt;Rafael Angarita&lt;/a&gt; (ISEP), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cormier_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Cormier&lt;/a&gt; (CRESTIC), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rousseaux_F/0/1/0/all/0/1&quot;&gt;Francis Rousseaux&lt;/a&gt; (CRESTIC)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12531">
<title>ICU: Conquering Language Barriers in Vision-and-Language Modeling by Dividing the Tasks into Image Captioning and Language Understanding. (arXiv:2310.12531v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12531</link>
<description rdf:parseType="Literal">&lt;p&gt;Most multilingual vision-and-language (V&amp;amp;L) research aims to accomplish
multilingual and multimodal capabilities within one model. However, the
scarcity of multilingual captions for images has hindered the development. To
overcome this obstacle, we propose ICU, Image Caption Understanding, which
divides a V&amp;amp;L task into two stages: a V&amp;amp;L model performs image captioning in
English, and a multilingual language model (mLM), in turn, takes the caption as
the alt text and performs crosslingual language understanding. The burden of
multilingual processing is lifted off V&amp;amp;L model and placed on mLM. Since the
multilingual text data is relatively of higher abundance and quality, ICU can
facilitate the conquering of language barriers for V&amp;amp;L models. In experiments
on two tasks across 9 languages in the IGLUE benchmark, we show that ICU can
achieve new state-of-the-art results for five languages, and comparable results
for the rest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1&quot;&gt;Guojun Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12537">
<title>Product Attribute Value Extraction using Large Language Models. (arXiv:2310.12537v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12537</link>
<description rdf:parseType="Literal">&lt;p&gt;E-commerce applications such as faceted product search or product comparison
are based on structured product descriptions like attribute/value pairs. The
vendors on e-commerce platforms do not provide structured product descriptions
but describe offers using titles or descriptions. To process such offers, it is
necessary to extract attribute/value pairs from textual product attributes.
State-of-the-art attribute/value extraction techniques rely on pre-trained
language models (PLMs), such as BERT. Two major drawbacks of these models for
attribute/value extraction are that (i) the models require significant amounts
of task-specific training data and (ii) the fine-tuned models face challenges
in generalizing to attribute values not included in the training data. This
paper explores the potential of large language models (LLMs) as a training
data-efficient and robust alternative to PLM-based attribute/value extraction
methods. We consider hosted LLMs, such as GPT-3.5 and GPT-4, as well as
open-source LLMs based on Llama2. We evaluate the models in a zero-shot
scenario and in a scenario where task-specific training data is available. In
the zero-shot scenario, we compare various prompt designs for representing
information about the target attributes of the extraction. In the scenario with
training data, we investigate (i) the provision of example attribute values,
(ii) the selection of in-context demonstrations, and (iii) the fine-tuning of
GPT-3.5. Our experiments show that GPT-4 achieves an average F1-score of 85% on
the two evaluation datasets while the best PLM-based techniques perform on
average 5% worse using the same amount of training data. GPT-4 achieves a 10%
higher F1-score than the best open-source LLM. The fine-tuned GPT-3.5 model
reaches a similar performance as GPT-4 while being significantly more
cost-efficient.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brinkmann_A/0/1/0/all/0/1&quot;&gt;Alexander Brinkmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shraga_R/0/1/0/all/0/1&quot;&gt;Roee Shraga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bizer_C/0/1/0/all/0/1&quot;&gt;Christian Bizer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12541">
<title>Large Language Model for Multi-objective Evolutionary Optimization. (arXiv:2310.12541v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2310.12541</link>
<description rdf:parseType="Literal">&lt;p&gt;Multiobjective evolutionary algorithms (MOEAs) are major methods for solving
multiobjective optimization problems (MOPs). Many MOEAs have been proposed in
the past decades, of which the operators need carefully handcrafted design with
domain knowledge. Recently, some attempts have been made to replace the
manually designed operators in MOEAs with learning-based operators (e.g.,
neural network models). However, much effort is still required for designing
and training such models, and the learned operators might not generalize well
to solve new problems. To tackle the above challenges, this work investigates a
novel approach that leverages the powerful large language model (LLM) to design
MOEA operators. With proper prompt engineering, we successfully let a general
LLM serve as a black-box search operator for decomposition-based MOEA (MOEA/D)
in a zero-shot manner. In addition, by learning from the LLM behavior, we
further design an explicit white-box operator with randomness and propose a new
version of decomposition-based MOEA, termed MOEA/D-LO. Experimental studies on
different test benchmarks show that our proposed method can achieve competitive
performance with widely used MOEAs. It is also promising to see the operator
only learned from a few instances can have robust generalization performance on
unseen problems with quite different patterns and settings. The results reveal
the potential benefits of using pre-trained LLMs in the design of MOEAs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhenkun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1&quot;&gt;Shunyu Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_X/0/1/0/all/0/1&quot;&gt;Xialiang Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1&quot;&gt;Mingxuan Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qingfu Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12557">
<title>DepWiGNN: A Depth-wise Graph Neural Network for Multi-hop Spatial Reasoning in Text. (arXiv:2310.12557v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12557</link>
<description rdf:parseType="Literal">&lt;p&gt;Spatial reasoning in text plays a crucial role in various real-world
applications. Existing approaches for spatial reasoning typically infer spatial
relations from pure text, which overlook the gap between natural language and
symbolic structures. Graph neural networks (GNNs) have showcased exceptional
proficiency in inducing and aggregating symbolic structures. However, classical
GNNs face challenges in handling multi-hop spatial reasoning due to the
over-smoothing issue, \textit{i.e.}, the performance decreases substantially as
the number of graph layers increases. To cope with these challenges, we propose
a novel \textbf{Dep}th-\textbf{Wi}se \textbf{G}raph \textbf{N}eural
\textbf{N}etwork (\textbf{DepWiGNN}). Specifically, we design a novel node
memory scheme and aggregate the information over the depth dimension instead of
the breadth dimension of the graph, which empowers the ability to collect long
dependencies without stacking multiple layers. Experimental results on two
challenging multi-hop spatial reasoning datasets show that DepWiGNN outperforms
existing spatial reasoning methods. The comparisons with the other three GNNs
further demonstrate its superiority in capturing long dependency in the graph.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuaiyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yang Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1&quot;&gt;Wai Lam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12558">
<title>Large Language Models Help Humans Verify Truthfulness -- Except When They Are Convincingly Wrong. (arXiv:2310.12558v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12558</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) are increasingly used for accessing information
on the web. Their truthfulness and factuality are thus of great interest. To
help users make the right decisions about the information they&apos;re getting, LLMs
should not only provide but also help users fact-check information. In this
paper, we conduct experiments with 80 crowdworkers in total to compare language
models with search engines (information retrieval systems) at facilitating
fact-checking by human users. We prompt LLMs to validate a given claim and
provide corresponding explanations. Users reading LLM explanations are
significantly more efficient than using search engines with similar accuracy.
However, they tend to over-rely the LLMs when the explanation is wrong. To
reduce over-reliance on LLMs, we ask LLMs to provide contrastive information -
explain both why the claim is true and false, and then we present both sides of
the explanation to users. This contrastive explanation mitigates users&apos;
over-reliance on LLMs, but cannot significantly outperform search engines.
However, showing both search engine results and LLM explanations offers no
complementary benefits as compared to search engines alone. Taken together,
natural language explanations by LLMs may not be a reliable replacement for
reading the retrieved passages yet, especially in high-stakes settings where
over-relying on wrong AI explanations could lead to critical consequences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1&quot;&gt;Chenglei Si&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1&quot;&gt;Navita Goyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Sherry Tongshuang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1&quot;&gt;Chen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1&quot;&gt;Shi Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daume_H/0/1/0/all/0/1&quot;&gt;Hal Daum&amp;#xe9; III&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boyd_Graber_J/0/1/0/all/0/1&quot;&gt;Jordan Boyd-Graber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12575">
<title>Multilingual estimation of political-party positioning: From label aggregation to long-input Transformers. (arXiv:2310.12575v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12575</link>
<description rdf:parseType="Literal">&lt;p&gt;Scaling analysis is a technique in computational political science that
assigns a political actor (e.g. politician or party) a score on a predefined
scale based on a (typically long) body of text (e.g. a parliamentary speech or
an election manifesto). For example, political scientists have often used the
left--right scale to systematically analyse political landscapes of different
countries. NLP methods for automatic scaling analysis can find broad
application provided they (i) are able to deal with long texts and (ii) work
robustly across domains and languages. In this work, we implement and compare
two approaches to automatic scaling analysis of political-party manifestos:
label aggregation, a pipeline strategy relying on annotations of individual
statements from the manifestos, and long-input-Transformer-based models, which
compute scaling values directly from raw text. We carry out the analysis of the
Comparative Manifestos Project dataset across 41 countries and 27 languages and
find that the task can be efficiently solved by state-of-the-art models, with
label aggregation producing the best results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikolaev_D/0/1/0/all/0/1&quot;&gt;Dmitry Nikolaev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ceron_T/0/1/0/all/0/1&quot;&gt;Tanise Ceron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pado_S/0/1/0/all/0/1&quot;&gt;Sebastian Pad&amp;#xf3;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12580">
<title>Pretraining Language Models with Text-Attributed Heterogeneous Graphs. (arXiv:2310.12580v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12580</link>
<description rdf:parseType="Literal">&lt;p&gt;In many real-world scenarios (e.g., academic networks, social platforms),
different types of entities are not only associated with texts but also
connected by various relationships, which can be abstracted as Text-Attributed
Heterogeneous Graphs (TAHGs). Current pretraining tasks for Language Models
(LMs) primarily focus on separately learning the textual information of each
entity and overlook the crucial aspect of capturing topological connections
among entities in TAHGs. In this paper, we present a new pretraining framework
for LMs that explicitly considers the topological and heterogeneous information
in TAHGs. Firstly, we define a context graph as neighborhoods of a target node
within specific orders and propose a topology-aware pretraining task to predict
nodes involved in the context graph by jointly optimizing an LM and an
auxiliary heterogeneous graph neural network. Secondly, based on the
observation that some nodes are text-rich while others have little text, we
devise a text augmentation strategy to enrich textless nodes with their
neighbors&apos; texts for handling the imbalance issue. We conduct link prediction
and node classification tasks on three datasets from various domains.
Experimental results demonstrate the superiority of our approach over existing
methods and the rationality of each design. Our code is available at
https://github.com/Hope-Rita/THLM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_T/0/1/0/all/0/1&quot;&gt;Tao Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Le Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yifei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Leilei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1&quot;&gt;Bowen Du&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12585">
<title>Time-Aware Representation Learning for Time-Sensitive Question Answering. (arXiv:2310.12585v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12585</link>
<description rdf:parseType="Literal">&lt;p&gt;Time is one of the crucial factors in real-world question answering (QA)
problems. However, language models have difficulty understanding the
relationships between time specifiers, such as &apos;after&apos; and &apos;before&apos;, and
numbers, since existing QA datasets do not include sufficient time expressions.
To address this issue, we propose a Time-Context aware Question Answering
(TCQA) framework. We suggest a Time-Context dependent Span Extraction (TCSE)
task, and build a time-context dependent data generation framework for model
training. Moreover, we present a metric to evaluate the time awareness of the
QA model using TCSE. The TCSE task consists of a question and four sentence
candidates classified as correct or incorrect based on time and context. The
model is trained to extract the answer span from the sentence that is both
correct in time and context. The model trained with TCQA outperforms baseline
models up to 8.5 of the F1-score in the TimeQA dataset. Our dataset and code
are available at https://github.com/sonjbin/TCQA
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Son_J/0/1/0/all/0/1&quot;&gt;Jungbin Son&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1&quot;&gt;Alice Oh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12611">
<title>Identifying and Adapting Transformer-Components Responsible for Gender Bias in an English Language Model. (arXiv:2310.12611v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12611</link>
<description rdf:parseType="Literal">&lt;p&gt;Language models (LMs) exhibit and amplify many types of undesirable biases
learned from the training data, including gender bias. However, we lack tools
for effectively and efficiently changing this behavior without hurting general
language modeling performance. In this paper, we study three methods for
identifying causal relations between LM components and particular output:
causal mediation analysis, automated circuit discovery and our novel, efficient
method called DiffMask+ based on differential masking. We apply the methods to
GPT-2 small and the problem of gender bias, and use the discovered sets of
components to perform parameter-efficient fine-tuning for bias mitigation. Our
results show significant overlap in the identified components (despite huge
differences in the computational requirements of the methods) as well as
success in mitigating gender bias, with less damage to general language
modeling compared to full model fine-tuning. However, our work also underscores
the difficulty of defining and measuring bias, and the sensitivity of causal
discovery procedures to dataset choice. We hope our work can contribute to more
attention for dataset development, and lead to more effective mitigation
strategies for other types of bias.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chintam_A/0/1/0/all/0/1&quot;&gt;Abhijith Chintam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beloch_R/0/1/0/all/0/1&quot;&gt;Rahel Beloch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuidema_W/0/1/0/all/0/1&quot;&gt;Willem Zuidema&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanna_M/0/1/0/all/0/1&quot;&gt;Michael Hanna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wal_O/0/1/0/all/0/1&quot;&gt;Oskar van der Wal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12620">
<title>Predict the Future from the Past? On the Temporal Data Distribution Shift in Financial Sentiment Classifications. (arXiv:2310.12620v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12620</link>
<description rdf:parseType="Literal">&lt;p&gt;Temporal data distribution shift is prevalent in the financial text. How can
a financial sentiment analysis system be trained in a volatile market
environment that can accurately infer sentiment and be robust to temporal data
distribution shifts? In this paper, we conduct an empirical study on the
financial sentiment analysis system under temporal data distribution shifts
using a real-world financial social media dataset that spans three years. We
find that the fine-tuned models suffer from general performance degradation in
the presence of temporal distribution shifts. Furthermore, motivated by the
unique temporal nature of the financial text, we propose a novel method that
combines out-of-distribution detection with time series modeling for temporal
financial sentiment analysis. Experimental results show that the proposed
method enhances the model&apos;s capability to adapt to evolving temporal shifts in
a volatile financial market.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yue Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1&quot;&gt;Chenxi Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12640">
<title>Non-Autoregressive Sentence Ordering. (arXiv:2310.12640v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12640</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing sentence ordering approaches generally employ encoder-decoder
frameworks with the pointer net to recover the coherence by recurrently
predicting each sentence step-by-step. Such an autoregressive manner only
leverages unilateral dependencies during decoding and cannot fully explore the
semantic dependency between sentences for ordering. To overcome these
limitations, in this paper, we propose a novel Non-Autoregressive Ordering
Network, dubbed \textit{NAON}, which explores bilateral dependencies between
sentences and predicts the sentence for each position in parallel. We claim
that the non-autoregressive manner is not just applicable but also particularly
suitable to the sentence ordering task because of two peculiar characteristics
of the task: 1) each generation target is in deterministic length, and 2) the
sentences and positions should match exclusively. Furthermore, to address the
repetition issue of the naive non-autoregressive Transformer, we introduce an
exclusive loss to constrain the exclusiveness between positions and sentences.
To verify the effectiveness of the proposed model, we conduct extensive
experiments on several common-used datasets and the experimental results show
that our method outperforms all the autoregressive approaches and yields
competitive performance compared with the state-of-the-arts. The codes are
available at:
\url{https://github.com/steven640pixel/nonautoregressive-sentence-ordering}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bin_Y/0/1/0/all/0/1&quot;&gt;Yi Bin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1&quot;&gt;Wenhao Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_B/0/1/0/all/0/1&quot;&gt;Bin Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jipeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Yujuan Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yang Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12648">
<title>Towards Real-World Streaming Speech Translation for Code-Switched Speech. (arXiv:2310.12648v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12648</link>
<description rdf:parseType="Literal">&lt;p&gt;Code-switching (CS), i.e. mixing different languages in a single sentence, is
a common phenomenon in communication and can be challenging in many Natural
Language Processing (NLP) settings. Previous studies on CS speech have shown
promising results for end-to-end speech translation (ST), but have been limited
to offline scenarios and to translation to one of the languages present in the
source (\textit{monolingual transcription}).
&lt;/p&gt;
&lt;p&gt;In this paper, we focus on two essential yet unexplored areas for real-world
CS speech translation: streaming settings, and translation to a third language
(i.e., a language not included in the source). To this end, we extend the
Fisher and Miami test and validation datasets to include new targets in Spanish
and German. Using this data, we train a model for both offline and streaming ST
and we establish baseline results for the two settings mentioned earlier.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alastruey_B/0/1/0/all/0/1&quot;&gt;Belen Alastruey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sperber_M/0/1/0/all/0/1&quot;&gt;Matthias Sperber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gollan_C/0/1/0/all/0/1&quot;&gt;Christian Gollan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Telaar_D/0/1/0/all/0/1&quot;&gt;Dominic Telaar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_T/0/1/0/all/0/1&quot;&gt;Tim Ng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agargwal_A/0/1/0/all/0/1&quot;&gt;Aashish Agargwal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12664">
<title>Is ChatGPT a Financial Expert? Evaluating Language Models on Financial Natural Language Processing. (arXiv:2310.12664v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12664</link>
<description rdf:parseType="Literal">&lt;p&gt;The emergence of Large Language Models (LLMs), such as ChatGPT, has
revolutionized general natural language preprocessing (NLP) tasks. However,
their expertise in the financial domain lacks a comprehensive evaluation. To
assess the ability of LLMs to solve financial NLP tasks, we present FinLMEval,
a framework for Financial Language Model Evaluation, comprising nine datasets
designed to evaluate the performance of language models. This study compares
the performance of encoder-only language models and the decoder-only language
models. Our findings reveal that while some decoder-only LLMs demonstrate
notable performance across most financial tasks via zero-shot prompting, they
generally lag behind the fine-tuned expert models, especially when dealing with
proprietary datasets. We hope this study provides foundation evaluations for
continuing efforts to build more advanced LLMs in the financial domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yue Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zian Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12727">
<title>Representing and Computing Uncertainty in Phonological Reconstruction. (arXiv:2310.12727v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12727</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the inherently fuzzy nature of reconstructions in historical
linguistics, most scholars do not represent their uncertainty when proposing
proto-forms. With the increasing success of recently proposed approaches to
automating certain aspects of the traditional comparative method, the formal
representation of proto-forms has also improved. This formalization makes it
possible to address both the representation and the computation of uncertainty.
Building on recent advances in supervised phonological reconstruction, during
which an algorithm learns how to reconstruct words in a given proto-language
relying on previously annotated data, and inspired by improved methods for
automated word prediction from cognate sets, we present a new framework that
allows for the representation of uncertainty in linguistic reconstruction and
also includes a workflow for the computation of fuzzy reconstructions from
linguistic data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+List_J/0/1/0/all/0/1&quot;&gt;Johann-Mattis List&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hill_N/0/1/0/all/0/1&quot;&gt;Nathan W. Hill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forkel_R/0/1/0/all/0/1&quot;&gt;Robert Forkel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blum_F/0/1/0/all/0/1&quot;&gt;Frederic Blum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12751">
<title>Character-level Chinese Backpack Language Models. (arXiv:2310.12751v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12751</link>
<description rdf:parseType="Literal">&lt;p&gt;The Backpack is a Transformer alternative shown to improve interpretability
in English language modeling by decomposing predictions into a weighted sum of
token sense components. However, Backpacks&apos; reliance on token-defined meaning
raises questions as to their potential for languages other than English, a
language for which subword tokenization provides a reasonable approximation for
lexical items. In this work, we train, evaluate, interpret, and control
Backpack language models in character-tokenized Chinese, in which words are
often composed of many characters. We find that our (134M parameter) Chinese
Backpack language model performs comparably to a (104M parameter) Transformer,
and learns rich character-level meanings that log-additively compose to form
word meanings. In SimLex-style lexical semantic evaluations, simple averages of
Backpack character senses outperform input embeddings from a Transformer. We
find that complex multi-character meanings are often formed by using the same
per-character sense weights consistently across context. Exploring
interpretability-through control, we show that we can localize a source of
gender bias in our Backpacks to specific character senses and intervene to
reduce the bias.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Hao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hewitt_J/0/1/0/all/0/1&quot;&gt;John Hewitt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12766">
<title>Transformer-based Entity Legal Form Classification. (arXiv:2310.12766v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12766</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose the application of Transformer-based language models for
classifying entity legal forms from raw legal entity names. Specifically, we
employ various BERT variants and compare their performance against multiple
traditional baselines. Our evaluation encompasses a substantial subset of
freely available Legal Entity Identifier (LEI) data, comprising over 1.1
million legal entities from 30 different legal jurisdictions. The ground truth
labels for classification per jurisdiction are taken from the Entity Legal Form
(ELF) code standard (ISO 20275). Our findings demonstrate that pre-trained BERT
variants outperform traditional text classification approaches in terms of F1
score, while also performing comparably well in the Macro F1 Score. Moreover,
the validity of our proposal is supported by the outcome of third-party expert
reviews conducted in ten selected jurisdictions. This study highlights the
significant potential of Transformer-based models in advancing data
standardization and data integration. The presented approaches can greatly
benefit financial institutions, corporations, governments and other
organizations in assessing business relationships, understanding risk exposure,
and promoting effective governance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arimond_A/0/1/0/all/0/1&quot;&gt;Alexander Arimond&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molteni_M/0/1/0/all/0/1&quot;&gt;Mauro Molteni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jany_D/0/1/0/all/0/1&quot;&gt;Dominik Jany&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manolova_Z/0/1/0/all/0/1&quot;&gt;Zornitsa Manolova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borth_D/0/1/0/all/0/1&quot;&gt;Damian Borth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoepner_A/0/1/0/all/0/1&quot;&gt;Andreas G.F. Hoepner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12774">
<title>Survival of the Most Influential Prompts: Efficient Black-Box Prompt Search via Clustering and Pruning. (arXiv:2310.12774v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12774</link>
<description rdf:parseType="Literal">&lt;p&gt;Prompt-based learning has been an effective paradigm for large pretrained
language models (LLM), enabling few-shot or even zero-shot learning. Black-box
prompt search has received growing interest recently for its distinctive
properties of gradient-free optimization, proven particularly useful and
powerful for model-as-a-service usage. However, the discrete nature and the
complexity of combinatorial optimization hinder the efficiency of modern
black-box approaches. Despite extensive research on search algorithms, the
crucial aspect of search space design and optimization has been largely
overlooked. In this paper, we first conduct a sensitivity analysis by prompting
LLM, revealing that only a small number of tokens exert a disproportionate
amount of influence on LLM predictions. Leveraging this insight, we propose the
Clustering and Pruning for Efficient Black-box Prompt Search (ClaPS), a simple
black-box search method that first clusters and prunes the search space to
focus exclusively on influential prompt tokens. By employing even simple search
methods within the pruned search space, ClaPS achieves state-of-the-art
performance across various tasks and LLMs, surpassing the performance of
complex approaches while significantly reducing search costs. Our findings
underscore the critical role of search space design and optimization in
enhancing both the usefulness and the efficiency of black-box prompt-based
learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Han Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1&quot;&gt;Xingchen Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1&quot;&gt;Ivan Vuli&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1&quot;&gt;Anna Korhonen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12778">
<title>Label-Aware Automatic Verbalizer for Few-Shot Text Classification. (arXiv:2310.12778v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12778</link>
<description rdf:parseType="Literal">&lt;p&gt;Prompt-based learning has shown its effectiveness in few-shot text
classification. One important factor in its success is a verbalizer, which
translates output from a language model into a predicted class. Notably, the
simplest and widely acknowledged verbalizer employs manual labels to represent
the classes. However, manual selection does not guarantee the optimality of the
selected words when conditioned on the chosen language model. Therefore, we
propose Label-Aware Automatic Verbalizer (LAAV), effectively augmenting the
manual labels to achieve better few-shot classification results. Specifically,
we use the manual labels along with the conjunction &quot;and&quot; to induce the model
to generate more effective words for the verbalizer. The experimental results
on five datasets across five languages demonstrate that LAAV significantly
outperforms existing verbalizers. Furthermore, our analysis reveals that LAAV
suggests more relevant words compared to similar approaches, especially in
mid-to-low resource languages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thaminkaew_T/0/1/0/all/0/1&quot;&gt;Thanakorn Thaminkaew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lertvittayakumjorn_P/0/1/0/all/0/1&quot;&gt;Piyawat Lertvittayakumjorn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vateekul_P/0/1/0/all/0/1&quot;&gt;Peerapon Vateekul&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12794">
<title>Are Structural Concepts Universal in Transformer Language Models? Towards Interpretable Cross-Lingual Generalization. (arXiv:2310.12794v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12794</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have exhibited considerable cross-lingual
generalization abilities, whereby they implicitly transfer knowledge across
languages. However, the transfer is not equally successful for all languages,
especially for low-resource ones, which poses an ongoing challenge. It is
unclear whether we have reached the limits of implicit cross-lingual
generalization and if explicit knowledge transfer is viable. In this paper, we
investigate the potential for explicitly aligning conceptual correspondence
between languages to enhance cross-lingual generalization. Using the syntactic
aspect of language as a testbed, our analyses of 43 languages reveal a high
degree of alignability among the spaces of structural concepts within each
language for both encoder-only and decoder-only LLMs. We then propose a
meta-learning-based method to learn to align conceptual spaces of different
languages, which facilitates zero-shot and few-shot generalization in concept
classification and also offers insights into the cross-lingual in-context
learning phenomenon. Experiments on syntactic analysis tasks show that our
approach achieves competitive results with state-of-the-art methods and narrows
the performance gap between languages, particularly benefiting those with
limited resources.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1&quot;&gt;Ningyu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jingting Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Menghan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xuanjing Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12798">
<title>MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter. (arXiv:2310.12798v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12798</link>
<description rdf:parseType="Literal">&lt;p&gt;Language Models (LMs) have demonstrated impressive molecule understanding
ability on various 1D text-related tasks. However, they inherently lack 2D
graph perception - a critical ability of human professionals in comprehending
molecules&apos; topological structures. To bridge this gap, we propose MolCA:
Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal
Adapter. MolCA enables an LM (e.g., Galactica) to understand both text- and
graph-based molecular contents via the cross-modal projector. Specifically, the
cross-modal projector is implemented as a Q-Former to connect a graph encoder&apos;s
representation space and an LM&apos;s text space. Further, MolCA employs a uni-modal
adapter (i.e., LoRA) for the LM&apos;s efficient adaptation to downstream tasks.
Unlike previous studies that couple an LM with a graph encoder via cross-modal
contrastive learning, MolCA retains the LM&apos;s ability of open-ended text
generation and augments it with 2D graph information. To showcase its
effectiveness, we extensively benchmark MolCA on tasks of molecule captioning,
IUPAC name prediction, and molecule-text retrieval, on which MolCA
significantly outperforms the baselines. Our codes and checkpoints can be found
at https://github.com/acharkq/MolCA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sihang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yanchen Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1&quot;&gt;Hao Fei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yixin Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1&quot;&gt;Kenji Kawaguchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1&quot;&gt;Tat-Seng Chua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12803">
<title>Causal-structure Driven Augmentations for Text OOD Generalization. (arXiv:2310.12803v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12803</link>
<description rdf:parseType="Literal">&lt;p&gt;The reliance of text classifiers on spurious correlations can lead to poor
generalization at deployment, raising concerns about their use in
safety-critical domains such as healthcare. In this work, we propose to use
counterfactual data augmentation, guided by knowledge of the causal structure
of the data, to simulate interventions on spurious features and to learn more
robust text classifiers. We show that this strategy is appropriate in
prediction problems where the label is spuriously correlated with an attribute.
Under the assumptions of such problems, we discuss the favorable sample
complexity of counterfactual data augmentation, compared to importance
re-weighting. Pragmatically, we match examples using auxiliary data, based on
diff-in-diff methodology, and use a large language model (LLM) to represent a
conditional probability of text. Through extensive experimentation on learning
caregiver-invariant predictors of clinical diagnoses from medical narratives
and on semi-synthetic data, we demonstrate that our method for simulating
interventions improves out-of-distribution (OOD) accuracy compared to baseline
invariant learning algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feder_A/0/1/0/all/0/1&quot;&gt;Amir Feder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wald_Y/0/1/0/all/0/1&quot;&gt;Yoav Wald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1&quot;&gt;Claudia Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saria_S/0/1/0/all/0/1&quot;&gt;Suchi Saria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blei_D/0/1/0/all/0/1&quot;&gt;David Blei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12808">
<title>Model Merging by Uncertainty-Based Gradient Matching. (arXiv:2310.12808v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12808</link>
<description rdf:parseType="Literal">&lt;p&gt;Models trained on different datasets can be merged by a weighted-averaging of
their parameters, but why does it work and when can it fail? Here, we connect
the inaccuracy of weighted-averaging to mismatches in the gradients and propose
a new uncertainty-based scheme to improve the performance by reducing the
mismatch. The connection also reveals implicit assumptions in other schemes
such as averaging, task arithmetic, and Fisher-weighted averaging. Our new
method gives consistent improvements for large language models and vision
transformers, both in terms of performance and robustness to hyperparameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daheim_N/0/1/0/all/0/1&quot;&gt;Nico Daheim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mollenhoff_T/0/1/0/all/0/1&quot;&gt;Thomas M&amp;#xf6;llenhoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1&quot;&gt;Edoardo Maria Ponti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1&quot;&gt;Iryna Gurevych&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Mohammad Emtiyaz Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12815">
<title>Prompt Injection Attacks and Defenses in LLM-Integrated Applications. (arXiv:2310.12815v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2310.12815</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) are increasingly deployed as the backend for a
variety of real-world applications called LLM-Integrated Applications. Multiple
recent works showed that LLM-Integrated Applications are vulnerable to prompt
injection attacks, in which an attacker injects malicious instruction/data into
the input of those applications such that they produce results as the attacker
desires. However, existing works are limited to case studies. As a result, the
literature lacks a systematic understanding of prompt injection attacks and
their defenses. We aim to bridge the gap in this work. In particular, we
propose a general framework to formalize prompt injection attacks. Existing
attacks, which are discussed in research papers and blog posts, are special
cases in our framework. Our framework enables us to design a new attack by
combining existing attacks. Moreover, we also propose a framework to
systematize defenses against prompt injection attacks. Using our frameworks, we
conduct a systematic evaluation on prompt injection attacks and their defenses
with 10 LLMs and 7 tasks. We hope our frameworks can inspire future research in
this field. Our code is available at
https://github.com/liu00222/Open-Prompt-Injection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yupei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1&quot;&gt;Yuqi Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_R/0/1/0/all/0/1&quot;&gt;Runpeng Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1&quot;&gt;Jinyuan Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1&quot;&gt;Neil Zhenqiang Gong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12818">
<title>Boosting Inference Efficiency: Unleashing the Power of Parameter-Shared Pre-trained Language Models. (arXiv:2310.12818v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12818</link>
<description rdf:parseType="Literal">&lt;p&gt;Parameter-shared pre-trained language models (PLMs) have emerged as a
successful approach in resource-constrained environments, enabling substantial
reductions in model storage and memory costs without significant performance
compromise. However, it is important to note that parameter sharing does not
alleviate computational burdens associated with inference, thus impeding its
practicality in situations characterized by limited stringent latency
requirements or computational resources. Building upon neural ordinary
differential equations (ODEs), we introduce a straightforward technique to
enhance the inference efficiency of parameter-shared PLMs. Additionally, we
propose a simple pre-training technique that leads to fully or partially shared
models capable of achieving even greater inference acceleration. The
experimental results demonstrate the effectiveness of our methods on both
autoregressive and autoencoding PLMs, providing novel insights into more
efficient utilization of parameter-shared models in resource-constrained
settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weize Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaoyue Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xu Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yankai Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1&quot;&gt;Ruobing Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;Maosong Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jie Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12821">
<title>GestureGPT: Zero-shot Interactive Gesture Understanding and Grounding with Large Language Model Agents. (arXiv:2310.12821v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12821</link>
<description rdf:parseType="Literal">&lt;p&gt;Current gesture recognition systems primarily focus on identifying gestures
within a predefined set, leaving a gap in connecting these gestures to
interactive GUI elements or system functions (e.g., linking a &apos;thumb-up&apos;
gesture to a &apos;like&apos; button). We introduce GestureGPT, a novel zero-shot gesture
understanding and grounding framework leveraging large language models (LLMs).
Gesture descriptions are formulated based on hand landmark coordinates from
gesture videos and fed into our dual-agent dialogue system. A gesture agent
deciphers these descriptions and queries about the interaction context (e.g.,
interface, history, gaze data), which a context agent organizes and provides.
Following iterative exchanges, the gesture agent discerns user intent,
grounding it to an interactive function. We validated the gesture description
module using public first-view and third-view gesture datasets and tested the
whole system in two real-world settings: video streaming and smart home IoT
control. The highest zero-shot Top-5 grounding accuracies are 80.11% for video
streaming and 90.78% for smart home tasks, showing potential of the new gesture
understanding paradigm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1&quot;&gt;Xin Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tengxiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Chun Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shengdong Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yiqiang Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12823">
<title>AgentTuning: Enabling Generalized Agent Abilities for LLMs. (arXiv:2310.12823v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12823</link>
<description rdf:parseType="Literal">&lt;p&gt;Open large language models (LLMs) with great performance in various tasks
have significantly advanced the development of LLMs. However, they are far
inferior to commercial models such as ChatGPT and GPT-4 when acting as agents
to tackle complex tasks in the real world. These agent tasks employ LLMs as the
central controller responsible for planning, memorization, and tool
utilization, necessitating both fine-grained prompting methods and robust LLMs
to achieve satisfactory performance. Though many prompting methods have been
proposed to complete particular agent tasks, there is lack of research focusing
on improving the agent capabilities of LLMs themselves without compromising
their general abilities. In this work, we present AgentTuning, a simple and
general method to enhance the agent abilities of LLMs while maintaining their
general LLM capabilities. We construct AgentInstruct, a lightweight
instruction-tuning dataset containing high-quality interaction trajectories. We
employ a hybrid instruction-tuning strategy by combining AgentInstruct with
open-source instructions from general domains. AgentTuning is used to
instruction-tune the Llama 2 series, resulting in AgentLM. Our evaluations show
that AgentTuning enables LLMs&apos; agent capabilities without compromising general
abilities. The AgentLM-70B is comparable to GPT-3.5-turbo on unseen agent
tasks, demonstrating generalized agent capabilities. We open source the
AgentInstruct and AgentLM-7B, 13B, and 70B models at
https://github.com/THUDM/AgentTuning , serving open and powerful alternatives
to commercial LLMs for agent tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1&quot;&gt;Aohan Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mingdao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1&quot;&gt;Rui Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bowen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yuxiao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jie Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12836">
<title>Knowledge-Augmented Language Model Verification. (arXiv:2310.12836v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12836</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent Language Models (LMs) have shown impressive capabilities in generating
texts with the knowledge internalized in parameters. Yet, LMs often generate
the factually incorrect responses to the given queries, since their knowledge
may be inaccurate, incomplete, and outdated. To address this problem, previous
works propose to augment LMs with the knowledge retrieved from an external
knowledge source. However, such approaches often show suboptimal text
generation performance due to two reasons: 1) the model may fail to retrieve
the knowledge relevant to the given query, or 2) the model may not faithfully
reflect the retrieved knowledge in the generated text. To overcome these, we
propose to verify the output and the knowledge of the knowledge-augmented LMs
with a separate verifier, which is a small LM that is trained to detect those
two types of errors through instruction-finetuning. Then, when the verifier
recognizes an error, we can rectify it by either retrieving new knowledge or
generating new text. Further, we use an ensemble of the outputs from different
instructions with a single verifier to enhance the reliability of the
verification processes. We validate the effectiveness of the proposed
verification steps on multiple question answering benchmarks, whose results
show that the proposed verifier effectively identifies retrieval and generation
errors, allowing LMs to provide more factually correct outputs. Our code is
available at https://github.com/JinheonBaek/KALMV.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baek_J/0/1/0/all/0/1&quot;&gt;Jinheon Baek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_S/0/1/0/all/0/1&quot;&gt;Soyeong Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1&quot;&gt;Minki Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Jong C. Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1&quot;&gt;Sung Ju Hwang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12851">
<title>EmoDiarize: Speaker Diarization and Emotion Identification from Speech Signals using Convolutional Neural Networks. (arXiv:2310.12851v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2310.12851</link>
<description rdf:parseType="Literal">&lt;p&gt;In the era of advanced artificial intelligence and human-computer
interaction, identifying emotions in spoken language is paramount. This
research explores the integration of deep learning techniques in speech emotion
recognition, offering a comprehensive solution to the challenges associated
with speaker diarization and emotion identification. It introduces a framework
that combines a pre-existing speaker diarization pipeline and an emotion
identification model built on a Convolutional Neural Network (CNN) to achieve
higher precision. The proposed model was trained on data from five speech
emotion datasets, namely, RAVDESS, CREMA-D, SAVEE, TESS, and Movie Clips, out
of which the latter is a speech emotion dataset created specifically for this
research. The features extracted from each sample include Mel Frequency
Cepstral Coefficients (MFCC), Zero Crossing Rate (ZCR), Root Mean Square (RMS),
and various data augmentation algorithms like pitch, noise, stretch, and shift.
This feature extraction approach aims to enhance prediction accuracy while
reducing computational complexity. The proposed model yields an unweighted
accuracy of 63%, demonstrating remarkable efficiency in accurately identifying
emotional states within speech signals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamza_H/0/1/0/all/0/1&quot;&gt;Hanan Hamza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gafoor_F/0/1/0/all/0/1&quot;&gt;Fiza Gafoor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sithara_F/0/1/0/all/0/1&quot;&gt;Fathima Sithara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anil_G/0/1/0/all/0/1&quot;&gt;Gayathri Anil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anoop_V/0/1/0/all/0/1&quot;&gt;V. S. Anoop&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12860">
<title>Probing LLMs for hate speech detection: strengths and vulnerabilities. (arXiv:2310.12860v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12860</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently efforts have been made by social media platforms as well as
researchers to detect hateful or toxic language using large language models.
However, none of these works aim to use explanation, additional context and
victim community information in the detection process. We utilise different
prompt variation, input information and evaluate large language models in zero
shot setting (without adding any in-context examples). We select three large
language models (GPT-3.5, text-davinci and Flan-T5) and three datasets -
HateXplain, implicit hate and ToxicSpans. We find that on average including the
target information in the pipeline improves the model performance substantially
(~20-30%) over the baseline across the datasets. There is also a considerable
effect of adding the rationales/explanations into the pipeline (~10-20%) over
the baseline across the datasets. In addition, we further provide a typology of
the error cases where these large language models fail to (i) classify and (ii)
explain the reason for the decisions they take. Such vulnerable points
automatically constitute &apos;jailbreak&apos; prompts for these models and industry
scale safeguard techniques need to be developed to make the models robust
against such prompts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1&quot;&gt;Sarthak Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harshavardhan_A/0/1/0/all/0/1&quot;&gt;Ashish Harshavardhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1&quot;&gt;Animesh Mukherjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_P/0/1/0/all/0/1&quot;&gt;Punyajoy Saha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12864">
<title>The Locality and Symmetry of Positional Encodings. (arXiv:2310.12864v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12864</link>
<description rdf:parseType="Literal">&lt;p&gt;Positional Encodings (PEs) are used to inject word-order information into
transformer-based language models. While they can significantly enhance the
quality of sentence representations, their specific contribution to language
models is not fully understood, especially given recent findings that various
positional encodings are insensitive to word order. In this work, we conduct a
systematic study of positional encodings in \textbf{Bidirectional Masked
Language Models} (BERT-style) , which complements existing work in three
aspects: (1) We uncover the core function of PEs by identifying two common
properties, Locality and Symmetry; (2) We show that the two properties are
closely correlated with the performances of downstream tasks; (3) We quantify
the weakness of current PEs by introducing two new probing tasks, on which
current PEs perform poorly. We believe that these results are the basis for
developing better PEs for transformer-based language models. The code is
available at \faGithub~ \url{https://github.com/tigerchen52/locality\_symmetry}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lihu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varoquaux_G/0/1/0/all/0/1&quot;&gt;Ga&amp;#xeb;l Varoquaux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suchanek_F/0/1/0/all/0/1&quot;&gt;Fabian M. Suchanek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12874">
<title>StoryAnalogy: Deriving Story-level Analogies from Large Language Models to Unlock Analogical Understanding. (arXiv:2310.12874v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12874</link>
<description rdf:parseType="Literal">&lt;p&gt;Analogy-making between narratives is one of the most critical abilities in
natural language understanding. In this paper, we evaluate the ability to
identify and generate analogy by building a first-of-its-kind large-scale
story-level analogy corpus, StoryAnalogy, which contains 24K story pairs from
diverse domains with human annotations on two similarities from the extended
Structure-Mapping Theory. We design a set of tests on StoryAnalogy, presenting
the first evaluation of story-level analogy identification and generation.
Interestingly, we find that the analogy identification tasks are extremely
challenging not only for the sentence embedding models but also for the recent
large language models (LLMs) such as ChatGPT and LLaMa, where ChatGPT only
achieved around 30% accuracy in multiple-choice questions (&amp;gt; 85% accuracy for
humans). Finally, we find that data in StoryAnalogy can improve LLMs analogy
generation quality, where a fine-tuned FlanT5-xxl model yields comparable
performance to zero-shot ChatGPT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiayang_C/0/1/0/all/0/1&quot;&gt;Cheng Jiayang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1&quot;&gt;Lin Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_T/0/1/0/all/0/1&quot;&gt;Tsz Ho Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_T/0/1/0/all/0/1&quot;&gt;Tianqing Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weiqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1&quot;&gt;Chunkit Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ru_D/0/1/0/all/0/1&quot;&gt;Dongyu Ru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qipeng Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yangqiu Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yue Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12892">
<title>A Systematic Study of Performance Disparities in Multilingual Task-Oriented Dialogue Systems. (arXiv:2310.12892v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12892</link>
<description rdf:parseType="Literal">&lt;p&gt;Achieving robust language technologies that can perform well across the
world&apos;s many languages is a central goal of multilingual NLP. In this work, we
take stock of and empirically analyse task performance disparities that exist
between multilingual task-oriented dialogue (ToD) systems. We first define new
quantitative measures of absolute and relative equivalence in system
performance, capturing disparities across languages and within individual
languages. Through a series of controlled experiments, we demonstrate that
performance disparities depend on a number of factors: the nature of the ToD
task at hand, the underlying pretrained language model, the target language,
and the amount of ToD annotated data. We empirically prove the existence of the
adaptation and intrinsic biases in current ToD systems: e.g., ToD systems
trained for Arabic or Turkish using annotated ToD data fully parallel to
English ToD data still exhibit diminished ToD task performance. Beyond
providing a series of insights into the performance disparities of ToD systems
in different languages, our analyses offer practical tips on how to approach
ToD data collection and system development for new languages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Songbo Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Han Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1&quot;&gt;Moy Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gritta_M/0/1/0/all/0/1&quot;&gt;Milan Gritta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guchun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iacobacci_I/0/1/0/all/0/1&quot;&gt;Ignacio Iacobacci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1&quot;&gt;Anna Korhonen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1&quot;&gt;Ivan Vuli&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12902">
<title>Experimental Narratives: A Comparison of Human Crowdsourced Storytelling and AI Storytelling. (arXiv:2310.12902v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12902</link>
<description rdf:parseType="Literal">&lt;p&gt;The paper proposes a framework that combines behavioral and computational
experiments employing fictional prompts as a novel tool for investigating
cultural artifacts and social biases in storytelling both by humans and
generative AI. The study analyzes 250 stories authored by crowdworkers in June
2019 and 80 stories generated by GPT-3.5 and GPT-4 in March 2023 by merging
methods from narratology and inferential statistics. Both crowdworkers and
large language models responded to identical prompts about creating and falling
in love with an artificial human. The proposed experimental paradigm allows a
direct comparison between human and LLM-generated storytelling. Responses to
the Pygmalionesque prompts confirm the pervasive presence of the Pygmalion myth
in the collective imaginary of both humans and large language models. All
solicited narratives present a scientific or technological pursuit. The
analysis reveals that narratives from GPT-3.5 and particularly GPT-4 are more
more progressive in terms of gender roles and sexuality than those written by
humans. While AI narratives can occasionally provide innovative plot twists,
they offer less imaginative scenarios and rhetoric than human-authored texts.
The proposed framework argues that fiction can be used as a window into human
and AI-based collective imaginary and social dimensions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Begus_N/0/1/0/all/0/1&quot;&gt;Nina Begus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2105.14082">
<title>Bhasacitra: Visualising the dialect geography of South Asia. (arXiv:2105.14082v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2105.14082</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Bhasacitra, a dialect mapping system for South Asia built on a
database of linguistic studies of languages of the region annotated for topic
and location data. We analyse language coverage and look towards applications
to typology by visualising example datasets. The application is not only meant
to be useful for feature mapping, but also serves as a new kind of interactive
bibliography for linguists of South Asian languages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1&quot;&gt;Aryaman Arora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farris_A/0/1/0/all/0/1&quot;&gt;Adam Farris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+R_G/0/1/0/all/0/1&quot;&gt;Gopalakrishnan R&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1&quot;&gt;Samopriya Basu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.14276">
<title>Example-based Hypernetworks for Out-of-Distribution Generalization. (arXiv:2203.14276v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2203.14276</link>
<description rdf:parseType="Literal">&lt;p&gt;As Natural Language Processing (NLP) algorithms continually achieve new
milestones, out-of-distribution generalization remains a significant challenge.
This paper addresses the issue of multi-source adaptation for unfamiliar
domains: We leverage labeled data from multiple source domains to generalize to
unknown target domains at training. Our innovative framework employs
example-based Hypernetwork adaptation: a T5 encoder-decoder initially generates
a unique signature from an input example, embedding it within the source
domains&apos; semantic space. This signature is subsequently utilized by a
Hypernetwork to generate the task classifier&apos;s weights. We evaluated our method
across two tasks - sentiment classification and natural language inference - in
29 adaptation scenarios, where it outpaced established algorithms. In an
advanced version, the signature also enriches the input example&apos;s
representation. We also compare our finetuned architecture to few-shot GPT-3,
demonstrating its effectiveness in essential use cases. To our knowledge, this
marks the first application of Hypernetworks to the adaptation for unknown
domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Volk_T/0/1/0/all/0/1&quot;&gt;Tomer Volk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ben_David_E/0/1/0/all/0/1&quot;&gt;Eyal Ben-David&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amosy_O/0/1/0/all/0/1&quot;&gt;Ohad Amosy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1&quot;&gt;Gal Chechik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1&quot;&gt;Roi Reichart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.03251">
<title>Towards Automatic Construction of Filipino WordNet: Word Sense Induction and Synset Induction Using Sentence Embeddings. (arXiv:2204.03251v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2204.03251</link>
<description rdf:parseType="Literal">&lt;p&gt;Wordnets are indispensable tools for various natural language processing
applications. Unfortunately, wordnets get outdated, and producing or updating
wordnets can be slow and costly in terms of time and resources. This problem
intensifies for low-resource languages. This study proposes a method for word
sense induction and synset induction using only two linguistic resources,
namely, an unlabeled corpus and a sentence embeddings-based language model. The
resulting sense inventory and synonym sets can be used in automatically
creating a wordnet. We applied this method on a corpus of Filipino text. The
sense inventory and synsets were evaluated by matching them with the sense
inventory of the machine translated Princeton WordNet, as well as comparing the
synsets to the Filipino WordNet. This study empirically shows that the 30% of
the induced word senses are valid and 40% of the induced synsets are valid in
which 20% are novel synsets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Velasco_D/0/1/0/all/0/1&quot;&gt;Dan John Velasco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alba_A/0/1/0/all/0/1&quot;&gt;Axel Alba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pelagio_T/0/1/0/all/0/1&quot;&gt;Trisha Gail Pelagio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramirez_B/0/1/0/all/0/1&quot;&gt;Bryce Anthony Ramirez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chua_U/0/1/0/all/0/1&quot;&gt;Unisse Chua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samson_B/0/1/0/all/0/1&quot;&gt;Briane Paul Samson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cruz_J/0/1/0/all/0/1&quot;&gt;Jan Christian Blaise Cruz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1&quot;&gt;Charibeth Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.02147">
<title>Dict-TTS: Learning to Pronounce with Prior Dictionary Knowledge for Text-to-Speech. (arXiv:2206.02147v3 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2206.02147</link>
<description rdf:parseType="Literal">&lt;p&gt;Polyphone disambiguation aims to capture accurate pronunciation knowledge
from natural text sequences for reliable Text-to-speech (TTS) systems. However,
previous approaches require substantial annotated training data and additional
efforts from language experts, making it difficult to extend high-quality
neural TTS systems to out-of-domain daily conversations and countless languages
worldwide. This paper tackles the polyphone disambiguation problem from a
concise and novel perspective: we propose Dict-TTS, a semantic-aware generative
text-to-speech model with an online website dictionary (the existing prior
information in the natural language). Specifically, we design a
semantics-to-pronunciation attention (S2PA) module to match the semantic
patterns between the input text sequence and the prior semantics in the
dictionary and obtain the corresponding pronunciations; The S2PA module can be
easily trained with the end-to-end TTS model without any annotated phoneme
labels. Experimental results in three languages show that our model outperforms
several strong baseline models in terms of pronunciation accuracy and improves
the prosody modeling of TTS systems. Further extensive analyses demonstrate
that each design in Dict-TTS is effective. The code is available at
\url{https://github.com/Zain-Jiang/Dict-TTS}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Ziyue Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Su_Z/0/1/0/all/0/1&quot;&gt;Zhe Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhou Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qian Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ren_Y/0/1/0/all/0/1&quot;&gt;Yi Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jinglin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ye_Z/0/1/0/all/0/1&quot;&gt;Zhenhui Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.07025">
<title>Learning to translate by learning to communicate. (arXiv:2207.07025v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2207.07025</link>
<description rdf:parseType="Literal">&lt;p&gt;We formulate and test a technique to use Emergent Communication (EC) with a
pre-trained multilingual model to improve on modern Unsupervised NMT systems,
especially for low-resource languages. It has been argued that the current
dominant paradigm in NLP of pre-training on text-only corpora will not yield
robust natural language understanding systems, and the need for grounded,
goal-oriented, and interactive language learning has been high lighted. In our
approach, we embed a multilingual model (mBART, Liu et al., 2020) into an EC
image-reference game, in which the model is incentivized to use multilingual
generations to accomplish a vision-grounded task. The hypothesis is that this
will align multiple languages to a shared task space. We present two variants
of EC Fine-Tuning (Steinert-Threlkeld et al., 2022), one of which outperforms a
backtranslation-only baseline in all four languages investigated, including the
low-resource language Nepali.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Downey_C/0/1/0/all/0/1&quot;&gt;C.M. Downey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xuhui Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Leo Z. Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steinert_Threlkeld_S/0/1/0/all/0/1&quot;&gt;Shane Steinert-Threlkeld&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.06348">
<title>Can Brain Signals Reveal Inner Alignment with Human Languages?. (arXiv:2208.06348v4 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/2208.06348</link>
<description rdf:parseType="Literal">&lt;p&gt;Brain Signals, such as Electroencephalography (EEG), and human languages have
been widely explored independently for many downstream tasks, however, the
connection between them has not been well explored. In this study, we explore
the relationship and dependency between EEG and language. To study at the
representation level, we introduced \textbf{MTAM}, a \textbf{M}ultimodal
\textbf{T}ransformer \textbf{A}lignment \textbf{M}odel, to observe coordinated
representations between the two modalities. We used various relationship
alignment-seeking techniques, such as Canonical Correlation Analysis and
Wasserstein Distance, as loss functions to transfigure features. On downstream
applications, sentiment analysis and relation detection, we achieved new
state-of-the-art results on two datasets, ZuCo and K-EmoCon. Our method
achieved an F1-score improvement of 1.7% on K-EmoCon and 9.3% on Zuco datasets
for sentiment analysis, and 7.4% on ZuCo for relation detection. In addition,
we provide interpretations of the performance improvement: (1) feature
distribution shows the effectiveness of the alignment module for discovering
and encoding the relationship between EEG and language; (2) alignment weights
show the influence of different language semantics as well as EEG frequency
features; (3) brain topographical maps provide an intuitive demonstration of
the connectivity in the brain regions. Our code is available at
\url{https://github.com/Jason-Qiu/EEG_Language_Alignment}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Han_W/0/1/0/all/0/1&quot;&gt;William Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Qiu_J/0/1/0/all/0/1&quot;&gt;Jielin Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jiacheng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Mengdi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Weber_D/0/1/0/all/0/1&quot;&gt;Douglas Weber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;Ding Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.13623">
<title>Reinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook. (arXiv:2210.13623v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2210.13623</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, reinforcement learning and bandits have transformed a wide
range of real-world applications including healthcare, finance, recommendation
systems, robotics, and last but not least, the speech and natural language
processing. While most speech and language applications of reinforcement
learning algorithms are centered around improving the training of deep neural
networks with its flexible optimization properties, there are still many
grounds to explore to utilize the benefits of reinforcement learning, such as
its reward-driven adaptability, state representations, temporal structures and
generalizability. In this survey, we present an overview of recent advancements
of reinforcement learning and bandits, and discuss how they can be effectively
employed to solve speech and natural language processing problems with models
that are adaptive, interactive and scalable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Baihan Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.08238">
<title>Exploiting Contrastive Learning and Numerical Evidence for Improving Confusing Legal Judgment Prediction. (arXiv:2211.08238v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2211.08238</link>
<description rdf:parseType="Literal">&lt;p&gt;Given the fact description text of a legal case, legal judgment prediction
(LJP) aims to predict the case&apos;s charge, law article and penalty term. A core
problem of LJP is how to distinguish confusing legal cases, where only subtle
text differences exist. Previous studies fail to distinguish different
classification errors with a standard cross-entropy classification loss, and
ignore the numbers in the fact description for predicting the term of penalty.
To tackle these issues, in this work, first, we propose a moco-based supervised
contrastive learning to learn distinguishable representations, and explore the
best strategy to construct positive example pairs to benefit all three subtasks
of LJP simultaneously. Second, in order to exploit the numbers in legal cases
for predicting the penalty terms of certain cases, we further enhance the
representation of the fact description with extracted crime amounts which are
encoded by a pre-trained numeracy model. Extensive experiments on public
benchmarks show that the proposed method achieves new state-of-the-art results,
especially on confusing legal cases. Ablation studies also demonstrate the
effectiveness of each component.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_L/0/1/0/all/0/1&quot;&gt;Leilei Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Baokui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1&quot;&gt;Kun Kuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yating Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luu_A/0/1/0/all/0/1&quot;&gt;Anh Tuan Luu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Fei Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.09561">
<title>Large Language Models are Better Reasoners with Self-Verification. (arXiv:2212.09561v5 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2212.09561</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, with the chain of thought (CoT) prompting, large language models
(LLMs), e.g., GPT-3, have shown strong reasoning ability in several natural
language processing tasks such as arithmetic, commonsense, and logical
reasoning. However, LLMs with CoT require multi-step prompting and multi-token
prediction, which is highly sensitive to individual mistakes and vulnerable to
error accumulation. The above issues make the LLMs need the ability to verify
the answers. In fact, after inferring conclusions in some thinking decision
tasks, people often check them by re-verifying steps to avoid some mistakes. In
this paper, we propose and prove that LLMs also have similar self-verification
abilities. We take the conclusion obtained by CoT as one of the conditions for
solving the original problem. By performing a backward verification of the
answers that LLM deduced for itself, we can obtain interpretable answer
validation scores to select the candidate answer with the highest score.
Experimental results demonstrate that the proposed method can improve the
reasoning performance on various arithmetic, commonsense, and logical reasoning
datasets. Our code is publicly available at:
https://github.com/WENGSYX/Self-Verification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1&quot;&gt;Yixuan Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1&quot;&gt;Minjun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1&quot;&gt;Fei Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1&quot;&gt;Shizhu He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shengping Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1&quot;&gt;Bin Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jun Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.09724">
<title>A Retrieve-and-Read Framework for Knowledge Graph Link Prediction. (arXiv:2212.09724v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2212.09724</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge graph (KG) link prediction aims to infer new facts based on
existing facts in the KG. Recent studies have shown that using the graph
neighborhood of a node via graph neural networks (GNNs) provides more useful
information compared to just using the query information. Conventional GNNs for
KG link prediction follow the standard message-passing paradigm on the entire
KG, which leads to superfluous computation, over-smoothing of node
representations, and also limits their expressive power. On a large scale, it
becomes computationally expensive to aggregate useful information from the
entire KG for inference. To address the limitations of existing KG link
prediction frameworks, we propose a novel retrieve-and-read framework, which
first retrieves a relevant subgraph context for the query and then jointly
reasons over the context and the query with a high-capacity reader. As part of
our exemplar instantiation for the new framework, we propose a novel
Transformer-based GNN as the reader, which incorporates graph-based attention
structure and cross-attention between query and context for deep fusion. This
simple yet effective design enables the model to focus on salient context
information relevant to the query. Empirical results on two standard KG link
prediction datasets demonstrate the competitive performance of the proposed
method. Furthermore, our analysis yields valuable insights for designing
improved retrievers within the framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pahuja_V/0/1/0/all/0/1&quot;&gt;Vardaan Pahuja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Boshi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Latapie_H/0/1/0/all/0/1&quot;&gt;Hugo Latapie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivasa_J/0/1/0/all/0/1&quot;&gt;Jayanth Srinivasa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yu Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.09730">
<title>Speaking Style Conversion in the Waveform Domain Using Discrete Self-Supervised Units. (arXiv:2212.09730v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2212.09730</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce DISSC, a novel, lightweight method that converts the rhythm,
pitch contour and timbre of a recording to a target speaker in a textless
manner. Unlike DISSC, most voice conversion (VC) methods focus primarily on
timbre, and ignore people&apos;s unique speaking style (prosody). The proposed
approach uses a pretrained, self-supervised model for encoding speech to
discrete units, which makes it simple, effective, and fast to train. All
conversion modules are only trained on reconstruction like tasks, thus suitable
for any-to-many VC with no paired data. We introduce a suite of quantitative
and qualitative evaluation metrics for this setup, and empirically demonstrate
that DISSC significantly outperforms the evaluated baselines. Code and samples
are available at https://pages.cs.huji.ac.il/adiyoss-lab/dissc/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maimon_G/0/1/0/all/0/1&quot;&gt;Gallil Maimon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1&quot;&gt;Yossi Adi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.10784">
<title>Can NLI Provide Proper Indirect Supervision for Low-resource Biomedical Relation Extraction?. (arXiv:2212.10784v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2212.10784</link>
<description rdf:parseType="Literal">&lt;p&gt;Two key obstacles in biomedical relation extraction (RE) are the scarcity of
annotations and the prevalence of instances without explicitly pre-defined
labels due to low annotation coverage. Existing approaches, which treat
biomedical RE as a multi-class classification task, often result in poor
generalization in low-resource settings and do not have the ability to make
selective prediction on unknown cases but give a guess from seen relations,
hindering the applicability of those approaches. We present NBR, which converts
biomedical RE as natural language inference formulation through indirect
supervision. By converting relations to natural language hypotheses, NBR is
capable of exploiting semantic cues to alleviate annotation scarcity. By
incorporating a ranking-based loss that implicitly calibrates abstinent
instances, NBR learns a clearer decision boundary and is instructed to abstain
on uncertain instances. Extensive experiments on three widely-used biomedical
RE benchmarks, namely ChemProt, DDI and GAD, verify the effectiveness of NBR in
both full-set and low-resource regimes. Our analysis demonstrates that indirect
supervision benefits biomedical RE even when a domain gap exists, and combining
NLI knowledge with biomedical knowledge leads to the best performance gains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiashu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1&quot;&gt;Mingyu Derek Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Muhao Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.01328">
<title>IC3: Image Captioning by Committee Consensus. (arXiv:2302.01328v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.01328</link>
<description rdf:parseType="Literal">&lt;p&gt;If you ask a human to describe an image, they might do so in a thousand
different ways. Traditionally, image captioning models are trained to generate
a single &quot;best&quot; (most like a reference) image caption. Unfortunately, doing so
encourages captions that are &quot;informationally impoverished,&quot; and focus on only
a subset of the possible details, while ignoring other potentially useful
information in the scene. In this work, we introduce a simple, yet novel,
method: &quot;Image Captioning by Committee Consensus&quot; (IC3), designed to generate a
single caption that captures high-level details from several annotator
viewpoints. Humans rate captions produced by IC3 at least as helpful as
baseline SOTA models more than two thirds of the time, and IC3 can improve the
performance of SOTA automated recall systems by up to 84%, outperforming single
human-generated reference captions, and indicating significant improvements
over SOTA approaches for visual description. Code is available at
https://davidmchan.github.io/caption-by-committee/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_D/0/1/0/all/0/1&quot;&gt;David M. Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Myers_A/0/1/0/all/0/1&quot;&gt;Austin Myers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vijayanarasimhan_S/0/1/0/all/0/1&quot;&gt;Sudheendra Vijayanarasimhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ross_D/0/1/0/all/0/1&quot;&gt;David A. Ross&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Canny_J/0/1/0/all/0/1&quot;&gt;John Canny&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.06132">
<title>NNKGC: Improving Knowledge Graph Completion with Node Neighborhoods. (arXiv:2302.06132v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2302.06132</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge graph completion (KGC) aims to discover missing relations of query
entities. Current text-based models utilize the entity name and description to
infer the tail entity given the head entity and a certain relation. Existing
approaches also consider the neighborhood of the head entity. However, these
methods tend to model the neighborhood using a flat structure and are only
restricted to 1-hop neighbors. In this work, we propose a node
neighborhood-enhanced framework for knowledge graph completion. It models the
head entity neighborhood from multiple hops using graph neural networks to
enrich the head node information. Moreover, we introduce an additional edge
link prediction task to improve KGC. Evaluation on two public datasets shows
that this framework is simple yet effective. The case study also shows that the
model is able to predict explainable predictions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1&quot;&gt;Irene Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Boming Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.06674">
<title>PK-ICR: Persona-Knowledge Interactive Context Retrieval for Grounded Dialogue. (arXiv:2302.06674v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2302.06674</link>
<description rdf:parseType="Literal">&lt;p&gt;Identifying relevant persona or knowledge for conversational systems is
critical to grounded dialogue response generation. However, each grounding has
been mostly researched in isolation with more practical multi-context dialogue
tasks introduced in recent works. We define Persona and Knowledge Dual Context
Identification as the task to identify persona and knowledge jointly for a
given dialogue, which could be of elevated importance in complex multi-context
dialogue settings. We develop a novel grounding retrieval method that utilizes
all contexts of dialogue simultaneously. Our method requires less computational
power via utilizing neural QA retrieval models. We further introduce our novel
null-positive rank test which measures ranking performance on semantically
dissimilar samples (i.e. hard negatives) in relation to data augmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_M/0/1/0/all/0/1&quot;&gt;Minsik Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Joosung Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guoyin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.11084">
<title>Test-Time Distribution Normalization for Contrastively Learned Vision-language Models. (arXiv:2302.11084v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.11084</link>
<description rdf:parseType="Literal">&lt;p&gt;Advances in the field of vision-language contrastive learning have made it
possible for many downstream applications to be carried out efficiently and
accurately by simply taking the dot product between image and text
representations. One of the most representative approaches proposed recently
known as CLIP has garnered widespread adoption due to its effectiveness. CLIP
is trained with an InfoNCE loss that takes into account both positive and
negative samples to help learn a much more robust representation space. This
paper reveals that the common downstream practice of taking a dot product is
only a zeroth-order approximation of the optimization goal, resulting in a loss
of information during test-time. Intuitively, since the model has been
optimized based on the InfoNCE loss, test-time procedures should also be in
alignment. The question lies in how one can retrieve any semblance of negative
samples information during inference in a computationally efficient way. To
this end, we propose Distribution Normalization (DN), where we approximate the
mean representation of a batch of test samples and use such a mean to represent
what would be analogous to negative samples in the InfoNCE loss. DN requires no
retraining or fine-tuning and can be effortlessly applied during inference.
Extensive experiments on a wide variety of downstream tasks exhibit a clear
advantage of DN over the dot product on top of other existing test-time
augmentation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yifei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1&quot;&gt;Juntao Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Fengyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zabih_R/0/1/0/all/0/1&quot;&gt;Ramin Zabih&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1&quot;&gt;Ser-Nam Lim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.06623">
<title>MWE as WSD: Solving Multiword Expression Identification with Word Sense Disambiguation. (arXiv:2303.06623v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2303.06623</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent approaches to word sense disambiguation (WSD) utilize encodings of the
sense gloss (definition), in addition to the input context, to improve
performance. In this work we demonstrate that this approach can be adapted for
use in multiword expression (MWE) identification by training models which use
gloss and context information to filter MWE candidates produced by a rule-based
extraction pipeline. Our approach substantially improves precision,
outperforming the state-of-the-art in MWE identification on the DiMSUM dataset
by up to 1.9 F1 points and achieving competitive results on the PARSEME 1.1
English dataset. Our models also retain most of their WSD performance, showing
that a single model can be used for both tasks. Finally, building on similar
approaches using Bi-encoders for WSD, we introduce a novel Poly-encoder
architecture which improves MWE identification performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanner_J/0/1/0/all/0/1&quot;&gt;Joshua Tanner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1&quot;&gt;Jacob Hoffman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.06762">
<title>Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study. (arXiv:2304.06762v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2304.06762</link>
<description rdf:parseType="Literal">&lt;p&gt;Large decoder-only language models (LMs) can be largely improved in terms of
perplexity by retrieval (e.g., RETRO), but its impact on text generation
quality and downstream task accuracy is unclear. Thus, it is still an open
question: shall we pretrain large autoregressive LMs with retrieval? To answer
it, we perform a comprehensive study on a scalable pre-trained
retrieval-augmented LM (i.e., RETRO) compared with standard GPT and
retrieval-augmented GPT incorporated at fine-tuning or inference stages. We
first provide the recipe to reproduce RETRO up to 9.5B parameters while
retrieving a text corpus with 330B tokens. Based on that, we have the following
novel findings: i) RETRO outperforms GPT on text generation with much less
degeneration (i.e., repetition), moderately higher factual accuracy, and
slightly lower toxicity with a nontoxic retrieval database. ii) On the LM
Evaluation Harness benchmark, RETRO largely outperforms GPT on
knowledge-intensive tasks, but is on par with GPT on other tasks. Furthermore,
we introduce a simple variant of the model, RETRO++, which largely improves
open-domain QA results of original RETRO (e.g., EM score +8.6 on Natural
Question) and significantly outperforms retrieval-augmented GPT in both
fine-tuning and zero-shot evaluation settings. Our findings highlight the
promising direction of pretraining autoregressive LMs with retrieval as future
foundation models. We release our implementation at:
https://github.com/NVIDIA/Megatron-LM#retro.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Boxin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1&quot;&gt;Wei Ping&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Peng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McAfee_L/0/1/0/all/0/1&quot;&gt;Lawrence McAfee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zihan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1&quot;&gt;Mohammad Shoeybi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yi Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuchaiev_O/0/1/0/all/0/1&quot;&gt;Oleksii Kuchaiev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Chaowei Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1&quot;&gt;Anima Anandkumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1&quot;&gt;Bryan Catanzaro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.12410">
<title>PEFT-Ref: A Modular Reference Architecture and Typology for Parameter-Efficient Finetuning Techniques. (arXiv:2304.12410v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2304.12410</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent parameter-efficient finetuning (PEFT) techniques aim to improve over
the considerable cost of fully finetuning large pretrained language models
(PLM). As different PEFT techniques proliferate, it is becoming difficult to
compare them, in particular in terms of (i) the structure and functionality
they add to the PLM, (ii) the different types and degrees of efficiency
improvements achieved, (iii) performance at different downstream tasks, and
(iv) how differences in structure and functionality relate to efficiency and
task performance. To facilitate such comparisons, this paper presents a
reference architecture which standardises aspects shared by different PEFT
techniques, while isolating differences to specific locations and interactions
with the standard components. Through this process of standardising and
isolating differences, a modular view of PEFT techniques emerges, supporting
not only direct comparison of different techniques and their efficiency and
task performance, but also systematic exploration of reusability and
composability of the different types of finetuned modules. We demonstrate how
the reference architecture can be applied to understand properties and relative
advantages of PEFT techniques, hence to inform selection of techniques for
specific tasks, and design choices for new PEFT techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabry_M/0/1/0/all/0/1&quot;&gt;Mohammed Sabry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belz_A/0/1/0/all/0/1&quot;&gt;Anya Belz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.03495">
<title>Automatic Prompt Optimization with &quot;Gradient Descent&quot; and Beam Search. (arXiv:2305.03495v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.03495</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have shown impressive performance as general
purpose agents, but their abilities remain highly dependent on prompts which
are hand written with onerous trial-and-error effort. We propose a simple and
nonparametric solution to this problem, Automatic Prompt Optimization (APO),
which is inspired by numerical gradient descent to automatically improve
prompts, assuming access to training data and an LLM API. The algorithm uses
minibatches of data to form natural language &quot;gradients&quot; that criticize the
current prompt. The gradients are then &quot;propagated&quot; into the prompt by editing
the prompt in the opposite semantic direction of the gradient. These gradient
descent steps are guided by a beam search and bandit selection procedure which
significantly improves algorithmic efficiency. Preliminary results across three
benchmark NLP tasks and the novel problem of LLM jailbreak detection suggest
that Automatic Prompt Optimization can outperform prior prompt editing
techniques and improve an initial prompt&apos;s performance by up to 31%, by using
data to rewrite vague task descriptions into more precise annotation
instructions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pryzant_R/0/1/0/all/0/1&quot;&gt;Reid Pryzant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iter_D/0/1/0/all/0/1&quot;&gt;Dan Iter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jerry Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yin Tat Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Chenguang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1&quot;&gt;Michael Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.08281">
<title>FactKB: Generalizable Factuality Evaluation using Language Models Enhanced with Factual Knowledge. (arXiv:2305.08281v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.08281</link>
<description rdf:parseType="Literal">&lt;p&gt;Evaluating the factual consistency of automatically generated summaries is
essential for the progress and adoption of reliable summarization systems.
Despite recent advances, existing factuality evaluation models are not robust,
being especially prone to entity and relation errors in new domains. We propose
FactKB, a simple new approach to factuality evaluation that is generalizable
across domains, in particular with respect to entities and relations. FactKB is
based on language models pretrained using facts extracted from external
knowledge bases. We introduce three types of complementary factuality
pretraining objectives based on direct entity facts, facts grounded in
auxiliary knowledge about entities, and facts constructed compositionally
through knowledge base walks. The resulting factuality evaluation model
achieves state-of-the-art performance on two in-domain news summarization
benchmarks as well as on three out-of-domain scientific literature datasets.
Further analysis of FactKB shows improved ability to detect erroneous entities
and relations in summaries and is robust and generalizable across domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1&quot;&gt;Shangbin Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balachandran_V/0/1/0/all/0/1&quot;&gt;Vidhisha Balachandran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yuyang Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1&quot;&gt;Yulia Tsvetkov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.09955">
<title>Knowledge Card: Filling LLMs&apos; Knowledge Gaps with Plug-in Specialized Language Models. (arXiv:2305.09955v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.09955</link>
<description rdf:parseType="Literal">&lt;p&gt;By design, large language models (LLMs) are static general-purpose models,
expensive to retrain or update frequently. As they are increasingly adopted for
knowledge-intensive tasks, it becomes evident that these design choices lead to
failures to generate factual, relevant, and up-to-date knowledge. To this end,
we propose \ourmethod{}, a modular framework to plug in new factual and
relevant knowledge into general-purpose LLMs. We first introduce
\emph{knowledge cards} -- specialized language models trained on corpora from
specific domains and sources. Knowledge cards serve as parametric repositories
that are selected at inference time to generate background knowledge for the
base LLM. We then propose three content selectors to dynamically select and
retain information in documents generated by knowledge cards, specifically
controlling for \emph{relevance}, \emph{brevity}, and \emph{factuality} of
outputs. Finally, we propose two complementary integration approaches to
augment the base LLM with the (relevant, factual) knowledge curated from the
specialized LMs. Through extensive experiments, we demonstrate that
\ourmethod{} achieves state-of-the-art performance on six benchmark datasets.
Ultimately, \ourmethod{} framework enables dynamic synthesis and updates of
knowledge from diverse domains. Its modularity will ensure that relevant
knowledge can be continuously updated through the collective efforts of the
research community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1&quot;&gt;Shangbin Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1&quot;&gt;Weijia Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yuyang Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balachandran_V/0/1/0/all/0/1&quot;&gt;Vidhisha Balachandran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1&quot;&gt;Tianxing He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1&quot;&gt;Yulia Tsvetkov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.11171">
<title>TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models. (arXiv:2305.11171v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.11171</link>
<description rdf:parseType="Literal">&lt;p&gt;Factual consistency evaluation is often conducted using Natural Language
Inference (NLI) models, yet these models exhibit limited success in evaluating
summaries. Previous work improved such models with synthetic training data.
However, the data is typically based on perturbed human-written summaries,
which often differ in their characteristics from real model-generated summaries
and have limited coverage of possible factual errors. Alternatively, large
language models (LLMs) have recently shown promising results in directly
evaluating generative tasks, but are too computationally expensive for
practical use. Motivated by these limitations, we introduce TrueTeacher, a
method for generating synthetic data by annotating diverse model-generated
summaries using a LLM. Unlike prior work, TrueTeacher does not rely on
human-written summaries, and is multilingual by nature. Experiments on the TRUE
benchmark show that a student model trained using our data, substantially
outperforms both the state-of-the-art model with similar capacity, and the LLM
teacher. In a systematic study, we compare TrueTeacher to existing synthetic
data generation methods and demonstrate its superiority and robustness to
domain-shift. We also show that our method generalizes to multilingual
scenarios. Lastly, we release our large scale synthetic dataset (1.4M
examples), generated using TrueTeacher, and a checkpoint trained on this data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gekhman_Z/0/1/0/all/0/1&quot;&gt;Zorik Gekhman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herzig_J/0/1/0/all/0/1&quot;&gt;Jonathan Herzig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aharoni_R/0/1/0/all/0/1&quot;&gt;Roee Aharoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elkind_C/0/1/0/all/0/1&quot;&gt;Chen Elkind&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szpektor_I/0/1/0/all/0/1&quot;&gt;Idan Szpektor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.11790">
<title>Prompting with Pseudo-Code Instructions. (arXiv:2305.11790v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.11790</link>
<description rdf:parseType="Literal">&lt;p&gt;Prompting with natural language instructions has recently emerged as a
popular method of harnessing the capabilities of large language models. Given
the inherent ambiguity present in natural language, it is intuitive to consider
the possible advantages of prompting with less ambiguous prompt styles, such as
the use of pseudo-code.
&lt;/p&gt;
&lt;p&gt;In this paper we explore if prompting via pseudo-code instructions helps
improve the performance of pre-trained language models. We manually create a
dataset of pseudo-code prompts for 132 different tasks spanning classification,
QA and generative language tasks, sourced from the Super-NaturalInstructions
dataset. Using these prompts along with their counterparts in natural language,
we study their performance on two LLM families - BLOOM and CodeGen. Our
experiments show that using pseudo-code instructions leads to better results,
with an average increase (absolute) of 7-16 points in F1 scores for
classification tasks and an improvement (relative) of 12-38% in aggregate
ROUGE-L scores across all tasks. We include detailed ablation studies which
indicate that code comments, docstrings, and the structural clues encoded in
pseudo-code all contribute towards the improvement in performance.
&lt;/p&gt;
&lt;p&gt;To the best of our knowledge our work is the first to demonstrate how
pseudo-code prompts can be helpful in improving the performance of pre-trained
LMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_M/0/1/0/all/0/1&quot;&gt;Mayank Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1&quot;&gt;Prince Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhat_R/0/1/0/all/0/1&quot;&gt;Riyaz Bhat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+V_R/0/1/0/all/0/1&quot;&gt;Rudra Murthy V&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Contractor_D/0/1/0/all/0/1&quot;&gt;Danish Contractor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tamilselvam_S/0/1/0/all/0/1&quot;&gt;Srikanth Tamilselvam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12295">
<title>Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning. (arXiv:2305.12295v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12295</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have shown human-like reasoning abilities but
still struggle with complex logical problems. This paper introduces a novel
framework, Logic-LM, which integrates LLMs with symbolic solvers to improve
logical problem-solving. Our method first utilizes LLMs to translate a natural
language problem into a symbolic formulation. Afterward, a deterministic
symbolic solver performs inference on the formulated problem. We also introduce
a self-refinement module, which utilizes the symbolic solver&apos;s error messages
to revise symbolic formalizations. We demonstrate Logic-LM&apos;s effectiveness on
five logical reasoning datasets: ProofWriter, PrOntoQA, FOLIO,
LogicalDeduction, and AR-LSAT. On average, Logic-LM achieves a significant
performance boost of 39.2% over using LLM alone with standard prompting and
18.4% over LLM with chain-of-thought prompting. Our findings suggest that
Logic-LM, by combining LLMs with symbolic logic, offers a promising avenue for
faithful logical reasoning. Code and data are publicly available at
https://github.com/teacherpeterpan/Logic-LLM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1&quot;&gt;Liangming Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albalak_A/0/1/0/all/0/1&quot;&gt;Alon Albalak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;William Yang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12634">
<title>Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training. (arXiv:2305.12634v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12634</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we propose a pragmatic method that reduces the annotation cost
for structured label spaces using active learning. Our approach leverages
partial annotation, which reduces labeling costs for structured outputs by
selecting only the most informative sub-structures for annotation. We also
utilize self-training to incorporate the current model&apos;s automatic predictions
as pseudo-labels for un-annotated sub-structures. A key challenge in
effectively combining partial annotation with self-training to reduce
annotation cost is determining which sub-structures to select to label. To
address this challenge, we adopt an error estimator to adaptively decide the
partial selection ratio according to the current model&apos;s capability. In
evaluations spanning four structured prediction tasks, we show that our
combination of partial annotation and self-training using an adaptive selection
ratio reduces annotation cost over strong full annotation baselines under a
fair comparison scheme that takes reading time into consideration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhisong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strubell_E/0/1/0/all/0/1&quot;&gt;Emma Strubell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1&quot;&gt;Eduard Hovy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12818">
<title>Crosslingual Transfer Learning for Low-Resource Languages Based on Multilingual Colexification Graphs. (arXiv:2305.12818v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12818</link>
<description rdf:parseType="Literal">&lt;p&gt;In comparative linguistics, colexification refers to the phenomenon of a
lexical form conveying two or more distinct meanings. Existing work on
colexification patterns relies on annotated word lists, limiting scalability
and usefulness in NLP. In contrast, we identify colexification patterns of more
than 2,000 concepts across 1,335 languages directly from an unannotated
parallel corpus. We then propose simple and effective methods to build
multilingual graphs from the colexification patterns: ColexNet and ColexNet+.
ColexNet&apos;s nodes are concepts and its edges are colexifications. In ColexNet+,
concept nodes are additionally linked through intermediate nodes, each
representing an ngram in one of 1,334 languages. We use ColexNet+ to train
$\overrightarrow{\mbox{ColexNet+}}$, high-quality multilingual embeddings that
are well-suited for transfer learning. In our experiments, we first show that
ColexNet achieves high recall on CLICS, a dataset of crosslingual
colexifications. We then evaluate $\overrightarrow{\mbox{ColexNet+}}$ on
roundtrip translation, sentence retrieval and sentence classification and show
that our embeddings surpass several transfer learning baselines. This
demonstrates the benefits of using colexification as a source of information in
multilingual NLP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yihong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1&quot;&gt;Haotian Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weissweiler_L/0/1/0/all/0/1&quot;&gt;Leonie Weissweiler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_R/0/1/0/all/0/1&quot;&gt;Renhao Pei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1&quot;&gt;Hinrich Sch&amp;#xfc;tze&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14232">
<title>Pre-training Multi-task Contrastive Learning Models for Scientific Literature Understanding. (arXiv:2305.14232v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14232</link>
<description rdf:parseType="Literal">&lt;p&gt;Scientific literature understanding tasks have gained significant attention
due to their potential to accelerate scientific discovery. Pre-trained language
models (LMs) have shown effectiveness in these tasks, especially when tuned via
contrastive learning. However, jointly utilizing pre-training data across
multiple heterogeneous tasks (e.g., extreme multi-label paper classification,
citation prediction, and literature search) remains largely unexplored. To
bridge this gap, we propose a multi-task contrastive learning framework,
SciMult, with a focus on facilitating common knowledge sharing across different
scientific literature understanding tasks while preventing task-specific skills
from interfering with each other. To be specific, we explore two techniques --
task-aware specialization and instruction tuning. The former adopts a
Mixture-of-Experts Transformer architecture with task-aware sub-layers; the
latter prepends task-specific instructions to the input text so as to produce
task-aware outputs. Extensive experiments on a comprehensive collection of
benchmark datasets verify the effectiveness of our task-aware specialization
strategy, where we outperform state-of-the-art scientific pre-trained LMs.
Code, datasets, and pre-trained models can be found at
https://scimult.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1&quot;&gt;Hao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1&quot;&gt;Zhihong Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaodong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Ye-Yi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jianfeng Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14281">
<title>Weakly-Supervised Learning of Visual Relations in Multimodal Pretraining. (arXiv:2305.14281v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14281</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work in vision-and-language pretraining has investigated supervised
signals from object detection data to learn better, fine-grained multimodal
representations. In this work, we take a step further and explore how we can
tap into supervision from small-scale visual relation data. In particular, we
propose two pretraining approaches to contextualise visual entities in a
multimodal setup. With verbalised scene graphs, we transform visual relation
triplets into structured captions, and treat them as additional image
descriptions. With masked relation prediction, we further encourage relating
entities from image regions with visually masked contexts. When applied to
strong baselines pretrained on large amounts of Web data, zero-shot evaluations
on both coarse-grained and fine-grained tasks show the efficacy of our methods
in learning multimodal representations from weakly-supervised relations data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bugliarello_E/0/1/0/all/0/1&quot;&gt;Emanuele Bugliarello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nematzadeh_A/0/1/0/all/0/1&quot;&gt;Aida Nematzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hendricks_L/0/1/0/all/0/1&quot;&gt;Lisa Anne Hendricks&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14610">
<title>This Land is {Your, My} Land: Evaluating Geopolitical Biases in Language Models. (arXiv:2305.14610v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14610</link>
<description rdf:parseType="Literal">&lt;p&gt;Do the Spratly Islands belong to China, the Philippines, or Vietnam? A
pretrained large language model (LLM) may answer differently if asked in the
languages of each claimant country: Chinese, Tagalog, or Vietnamese. This
contrasts with a multilingual human, who would likely answer consistently. In
this work, we show that LLMs recall geopolitical knowledge inconsistently
across languages -- a phenomenon we term geopolitical bias. As a targeted case
study, we consider territorial disputes, inherently controversial and
cross-lingual task.
&lt;/p&gt;
&lt;p&gt;We first introduce the BorderLines dataset of territorial disputes. This
covers 256 territories, each of which is associated to a set of multiple-choice
questions in the languages of each claimant country (48 languages total). We
then pose these questions to LLMs to probe their internal knowledge. Finally,
we propose a suite of evaluation metrics based on accuracy, which compares
responses with respect to the actual geopolitical situation, and consistency of
the responses in different languages. These metrics allow us to quantify
several findings, which include instruction-tuned LLMs underperforming base
ones, and geopolitical bias being amplified in stronger models. We release our
code and dataset to facilitate future investigation and mitigation of
geopolitical bias.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bryan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1&quot;&gt;Chris Callison-Burch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14766">
<title>Allies: Prompting Large Language Model with Beam Search. (arXiv:2305.14766v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14766</link>
<description rdf:parseType="Literal">&lt;p&gt;With the advance of large language models (LLMs), the research field of LLM
applications becomes more and more popular and the idea of constructing
pipelines to accomplish complex tasks by stacking LLM API calls come true.
However, this kind of methods face two limitations: narrow information coverage
and low fault tolerance. In this work, we propose a novel method called ALLIES.
Given an input query, ALLIES leverages LLMs to iteratively generate new queries
related to the original query, enabling an iterative reasoning process. By
iteratively refining and expanding the scope of the original query, ALLIES
captures and utilizes hidden knowledge that may not be directly obtainable
through retrieval. We take zero-shot open-domain question answering (ODQA) as
an application scene and evaluate ALLIES on the widely-used benchmarks, such as
NQ, WebQ and TriviaQA. The experimental results demonstrate that ALLIES
significantly outperforms other zero-shot baselines, indicating its
effectiveness in tackling those challenges. Our code is available in
https://github.com/microsoft/SimXNS/tree/main/ALLIES.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Hao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yeyun Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1&quot;&gt;Daxin Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Linjun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1&quot;&gt;Nan Duan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14994">
<title>RefGPT: Dialogue Generation of GPT, by GPT, and for GPT. (arXiv:2305.14994v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14994</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have attained the impressive capability to
resolve a wide range of NLP tasks by fine-tuning high-quality instruction data.
However, collecting human-written data of high quality, especially multi-turn
dialogues, is expensive and unattainable for most people. Though previous
studies have used powerful LLMs to generate the dialogues automatically, they
all suffer from generating untruthful dialogues because of the model
hallucination. Therefore, we propose a method called RefGPT to generate
enormous truthful and customized dialogues without worrying about factual
errors caused by the model hallucination. RefGPT solves the model hallucination
in dialogue generation by restricting the LLMs to leverage the given reference
instead of reciting their own knowledge to generate dialogues. Additionally,
RefGPT adds detailed controls on every utterance to enable high customization
capability, which previous studies have ignored. On the basis of RefGPT, we
also propose two high-quality dialogue datasets generated by GPT-4, namely
RefGPT-Fact and RefGPT-Code. RefGPT-Fact is a dataset with 100k multi-turn
dialogues based on factual knowledge and RefGPT-Code has 76k multi-turn
dialogues covering a wide range of coding scenarios. Our code and datasets are
released in https://github.com/mutonix/RefGPT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Dongjie Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_R/0/1/0/all/0/1&quot;&gt;Ruifeng Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yuantao Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yifei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zili Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shusen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hai Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15020">
<title>An Efficient Multilingual Language Model Compression through Vocabulary Trimming. (arXiv:2305.15020v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15020</link>
<description rdf:parseType="Literal">&lt;p&gt;Multilingual language model (LM) have become a powerful tool in NLP
especially for non-English languages. Nevertheless, model parameters of
multilingual LMs remain large due to the larger embedding matrix of the
vocabulary covering tokens in different languages. On the contrary, monolingual
LMs can be trained in a target language with the language-specific vocabulary
only, but this requires a large budget and availability of reliable corpora to
achieve a high-quality LM from scratch. In this paper, we propose
vocabulary-trimming (VT), a method to reduce a multilingual LM vocabulary to a
target language by deleting irrelevant tokens from its vocabulary. In theory,
VT can compress any existing multilingual LM to build monolingual LMs in any
language covered by the multilingual LM. In our experiments, we show that VT
can retain the original performance of the multilingual LM, while being smaller
in size (in general around 50% of the original vocabulary size is enough) than
the original multilingual LM. The evaluation is performed over four NLP tasks
(two generative and two classification tasks) among four widely used
multilingual LMs in seven languages. Finally, we show that this methodology can
keep the best of both monolingual and multilingual worlds by keeping a small
size as monolingual models without the need for specifically retraining them,
and even limiting potentially harmful social biases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ushio_A/0/1/0/all/0/1&quot;&gt;Asahi Ushio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1&quot;&gt;Jose Camacho-Collados&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16986">
<title>NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models. (arXiv:2305.16986v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16986</link>
<description rdf:parseType="Literal">&lt;p&gt;Trained with an unprecedented scale of data, large language models (LLMs)
like ChatGPT and GPT-4 exhibit the emergence of significant reasoning abilities
from model scaling. Such a trend underscored the potential of training LLMs
with unlimited language data, advancing the development of a universal embodied
agent. In this work, we introduce the NavGPT, a purely LLM-based
instruction-following navigation agent, to reveal the reasoning capability of
GPT models in complex embodied scenes by performing zero-shot sequential action
prediction for vision-and-language navigation (VLN). At each step, NavGPT takes
the textual descriptions of visual observations, navigation history, and future
explorable directions as inputs to reason the agent&apos;s current status, and makes
the decision to approach the target. Through comprehensive experiments, we
demonstrate NavGPT can explicitly perform high-level planning for navigation,
including decomposing instruction into sub-goal, integrating commonsense
knowledge relevant to navigation task resolution, identifying landmarks from
observed scenes, tracking navigation progress, and adapting to exceptions with
plan adjustment. Furthermore, we show that LLMs is capable of generating
high-quality navigational instructions from observations and actions along a
path, as well as drawing accurate top-down metric trajectory given the agent&apos;s
navigation history. Despite the performance of using NavGPT to zero-shot R2R
tasks still falling short of trained models, we suggest adapting multi-modality
inputs for LLMs to use as visual navigation agents and applying the explicit
reasoning of LLMs to benefit learning-based models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1&quot;&gt;Gengze Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1&quot;&gt;Yicong Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qi Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19713">
<title>Red Teaming Language Model Detectors with Language Models. (arXiv:2305.19713v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19713</link>
<description rdf:parseType="Literal">&lt;p&gt;The prevalence and strong capability of large language models (LLMs) present
significant safety and ethical risks if exploited by malicious users. To
prevent the potentially deceptive usage of LLMs, recent works have proposed
algorithms to detect LLM-generated text and protect LLMs. In this paper, we
investigate the robustness and reliability of these LLM detectors under
adversarial attacks. We study two types of attack strategies: 1) replacing
certain words in an LLM&apos;s output with their synonyms given the context; 2)
automatically searching for an instructional prompt to alter the writing style
of the generation. In both strategies, we leverage an auxiliary LLM to generate
the word replacements or the instructional prompt. Different from previous
works, we consider a challenging setting where the auxiliary LLM can also be
protected by a detector. Experiments reveal that our attacks effectively
compromise the performance of all detectors in the study with plausible
generations, underscoring the urgent need to improve the robustness of
LLM-generated text detection systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhouxing Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yihan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_F/0/1/0/all/0/1&quot;&gt;Fan Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiangning Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1&quot;&gt;Kai-Wei Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1&quot;&gt;Cho-Jui Hsieh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00477">
<title>Make Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning. (arXiv:2306.00477v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00477</link>
<description rdf:parseType="Literal">&lt;p&gt;Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs)
has emerged as a highly successful approach, with training only a small number
of parameters without sacrificing performance and becoming the de-facto
learning paradigm with the increasing size of PLMs. However, existing PEFT
methods are not memory-efficient, because they still require caching most of
the intermediate activations for the gradient calculation, akin to fine-tuning.
One effective way to reduce the activation memory is to apply a reversible
model, so the intermediate activations are not necessary to be cached and can
be recomputed. Nevertheless, modifying a PLM to its reversible variant is not
straightforward, since the reversible model has a distinct architecture from
the currently released PLMs. In this paper, we first investigate what is a key
factor for the success of existing PEFT methods, and realize that it&apos;s
essential to preserve the PLM&apos;s starting point when initializing a PEFT method.
With this finding, we propose memory-efficient fine-tuning (MEFT) that inserts
adapters into a PLM, preserving the PLM&apos;s starting point and making it
reversible without additional pre-training. We evaluate MEFT on the GLUE
benchmark and five question-answering tasks with various backbones, BERT,
RoBERTa, BART and OPT. MEFT significantly reduces the activation memory up to
84% of full fine-tuning with a negligible amount of trainable parameters.
Moreover, MEFT achieves the same score on GLUE and a comparable score on the
question-answering tasks as full fine-tuning. A similar finding is also
observed for the image classification task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_B/0/1/0/all/0/1&quot;&gt;Baohao Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1&quot;&gt;Shaomu Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monz_C/0/1/0/all/0/1&quot;&gt;Christof Monz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05644">
<title>WSPAlign: Word Alignment Pre-training via Large-Scale Weakly Supervised Span Prediction. (arXiv:2306.05644v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05644</link>
<description rdf:parseType="Literal">&lt;p&gt;Most existing word alignment methods rely on manual alignment datasets or
parallel corpora, which limits their usefulness. Here, to mitigate the
dependence on manual data, we broaden the source of supervision by relaxing the
requirement for correct, fully-aligned, and parallel sentences. Specifically,
we make noisy, partially aligned, and non-parallel paragraphs. We then use such
a large-scale weakly-supervised dataset for word alignment pre-training via
span prediction. Extensive experiments with various settings empirically
demonstrate that our approach, which is named WSPAlign, is an effective and
scalable way to pre-train word aligners without manual data. When fine-tuned on
standard benchmarks, WSPAlign has set a new state-of-the-art by improving upon
the best-supervised baseline by 3.3~6.1 points in F1 and 1.5~6.1 points in AER.
Furthermore, WSPAlign also achieves competitive performance compared with the
corresponding baselines in few-shot, zero-shot and cross-lingual tests, which
demonstrates that WSPAlign is potentially more practical for low-resource
languages than existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qiyu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagata_M/0/1/0/all/0/1&quot;&gt;Masaaki Nagata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsuruoka_Y/0/1/0/all/0/1&quot;&gt;Yoshimasa Tsuruoka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09821">
<title>Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulator to Enhance Dialogue System. (arXiv:2306.09821v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09821</link>
<description rdf:parseType="Literal">&lt;p&gt;Dialogue systems and large language models (LLMs) have gained considerable
attention. However, the direct utilization of LLMs as task-oriented dialogue
(TOD) models has been found to underperform compared to smaller task-specific
models. Nonetheless, it is crucial to acknowledge the significant potential of
LLMs and explore improved approaches for leveraging their impressive abilities.
Motivated by the goal of leveraging LLMs, we propose an alternative approach
called User-Guided Response Optimization (UGRO) to combine it with a smaller
TOD model. This approach uses LLM as annotation-free user simulator to assess
dialogue responses, combining them with smaller fine-tuned end-to-end TOD
models. By utilizing the satisfaction feedback generated by LLMs, UGRO further
optimizes the supervised fine-tuned TOD model. Specifically, the TOD model
takes the dialogue history as input and, with the assistance of the user
simulator&apos;s feedback, generates high-satisfaction responses that meet the
user&apos;s requirements. Through empirical experiments on two TOD benchmarks, we
validate the effectiveness of our method. The results demonstrate that our
approach outperforms previous state-of-the-art (SOTA) results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yue Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luu_A/0/1/0/all/0/1&quot;&gt;Anh Tuan Luu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1&quot;&gt;Bryan Hooi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipani_A/0/1/0/all/0/1&quot;&gt;Aldo Lipani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14790">
<title>Automatic Assessment of Divergent Thinking in Chinese Language with TransDis: A Transformer-Based Language Model Approach. (arXiv:2306.14790v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14790</link>
<description rdf:parseType="Literal">&lt;p&gt;Language models have been increasingly popular for automatic creativity
assessment, generating semantic distances to objectively measure the quality of
creative ideas. However, there is currently a lack of an automatic assessment
system for evaluating creative ideas in the Chinese language. To address this
gap, we developed TransDis, a scoring system using transformer-based language
models, capable of providing valid originality (quality) and flexibility
(variety) scores for Alternative Uses Task (AUT) responses in Chinese. Study 1
demonstrated that the latent model-rated originality factor, comprised of three
transformer-based models, strongly predicted human originality ratings, and the
model-rated flexibility strongly correlated with human flexibility ratings as
well. Criterion validity analyses indicated that model-rated originality and
flexibility positively correlated to other creativity measures, demonstrating
similar validity to human ratings. Study 2 &amp;amp; 3 showed that TransDis effectively
distinguished participants instructed to provide creative vs. common uses
(Study 2) and participants instructed to generate ideas in a flexible vs.
persistent way (Study 3). Our findings suggest that TransDis can be a reliable
and low-cost tool for measuring idea originality and flexibility in Chinese
language, potentially paving the way for automatic creativity assessment in
other languages. We offer an open platform to compute originality and
flexibility for AUT responses in Chinese and over 50 other languages
(https://osf.io/59jv2/).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Tianchen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qifan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhaoyang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1&quot;&gt;Yubo Hou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15687">
<title>Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale. (arXiv:2306.15687v2 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15687</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale generative models such as GPT and DALL-E have revolutionized the
research community. These models not only generate high fidelity outputs, but
are also generalists which can solve tasks not explicitly taught. In contrast,
speech generative models are still primitive in terms of scale and task
generalization. In this paper, we present Voicebox, the most versatile
text-guided generative model for speech at scale. Voicebox is a
non-autoregressive flow-matching model trained to infill speech, given audio
context and text, trained on over 50K hours of speech that are not filtered or
enhanced. Similar to GPT, Voicebox can perform many different tasks through
in-context learning, but is more flexible as it can also condition on future
context. Voicebox can be used for mono or cross-lingual zero-shot
text-to-speech synthesis, noise removal, content editing, style conversion, and
diverse sample generation. In particular, Voicebox outperforms the
state-of-the-art zero-shot TTS model VALL-E on both intelligibility (5.9% vs
1.9% word error rates) and audio similarity (0.580 vs 0.681) while being up to
20 times faster. Audio samples can be found in
\url{https://voicebox.metademolab.com}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Le_M/0/1/0/all/0/1&quot;&gt;Matthew Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vyas_A/0/1/0/all/0/1&quot;&gt;Apoorv Vyas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Bowen Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Karrer_B/0/1/0/all/0/1&quot;&gt;Brian Karrer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sari_L/0/1/0/all/0/1&quot;&gt;Leda Sari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Moritz_R/0/1/0/all/0/1&quot;&gt;Rashel Moritz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Williamson_M/0/1/0/all/0/1&quot;&gt;Mary Williamson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Manohar_V/0/1/0/all/0/1&quot;&gt;Vimal Manohar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Adi_Y/0/1/0/all/0/1&quot;&gt;Yossi Adi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mahadeokar_J/0/1/0/all/0/1&quot;&gt;Jay Mahadeokar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hsu_W/0/1/0/all/0/1&quot;&gt;Wei-Ning Hsu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07362">
<title>A scoping review on multimodal deep learning in biomedical images and texts. (arXiv:2307.07362v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07362</link>
<description rdf:parseType="Literal">&lt;p&gt;Computer-assisted diagnostic and prognostic systems of the future should be
capable of simultaneously processing multimodal data. Multimodal deep learning
(MDL), which involves the integration of multiple sources of data, such as
images and text, has the potential to revolutionize the analysis and
interpretation of biomedical data. However, it only caught researchers&apos;
attention recently. To this end, there is a critical need to conduct a
systematic review on this topic, identify the limitations of current work, and
explore future directions. In this scoping review, we aim to provide a
comprehensive overview of the current state of the field and identify key
concepts, types of studies, and research gaps with a focus on biomedical images
and texts joint learning, mainly because these two were the most commonly
available data types in MDL research. This study reviewed the current uses of
multimodal deep learning on five tasks: (1) Report generation, (2) Visual
question answering, (3) Cross-modal retrieval, (4) Computer-aided diagnosis,
and (5) Semantic segmentation. Our results highlight the diverse applications
and potential of MDL and suggest directions for future research in the field.
We hope our review will facilitate the collaboration of natural language
processing (NLP) and medical imaging communities and support the next
generation of decision-making and computer-assisted diagnostic system
development.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhaoyi Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1&quot;&gt;Mingquan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1&quot;&gt;Qingqing Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1&quot;&gt;Qianqian Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yifan Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02463">
<title>Towards Generalist Foundation Model for Radiology by Leveraging Web-scale 2D&amp;3D Medical Data. (arXiv:2308.02463v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.02463</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we aim to initiate the development of Radiology Foundation
Model, termed as RadFM.We consider the construction of foundational models from
the perspectives of dataset construction, model design, and thorough
evaluation. Our contribution can be concluded as follows: (i), we construct a
large-scale Medical Multi-modal Dataset, MedMD, which consists of 16M 2D and 3D
medical scans with high-quality text descriptions or reports across various
data formats, modalities, and tasks, covering over 5000 distinct diseases. To
the best of our knowledge, this is the first large-scale, high-quality, medical
visual-language dataset, with both 2D and 3D scans; (ii ), we propose an
architecture that enables visually conditioned generative pre-training, i.e.,
allowing for integration of text input with 2D or 3D medical scans, and
generate responses for diverse radiologic tasks. The model was initially
pre-trained on MedMD and subsequently fine-tuned on the domain-specific
dataset, which is a radiologic cleaned version of MedMD, containing 3M
radiologic visual-language pairs, termed as RadMD; (iii), we propose a new
evaluation benchmark, RadBench, that comprises five tasks, including modality
recognition, disease diagnosis, visual question answering, report generation
and rationale diagnosis, aiming to comprehensively assess the capability of
foundation models in handling practical clinical problems. We conduct both
automatic and human evaluation on RadBench, in both cases, RadFM significantly
outperforms existing multi-modal foundation models. The codes, data, and model
checkpoint will all be made publicly available to promote further research and
development in the field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chaoyi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoman Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Ya Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanfeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1&quot;&gt;Weidi Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.00857">
<title>Evaluating Transformer&apos;s Ability to Learn Mildly Context-Sensitive Languages. (arXiv:2309.00857v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.00857</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the fact that Transformers perform well in NLP tasks, recent studies
suggest that self-attention is theoretically limited in learning even some
regular and context-free languages. These findings motivated us to think about
their implications in modeling natural language, which is hypothesized to be
mildly context-sensitive. We test the Transformer&apos;s ability to learn mildly
context-sensitive languages of varying complexities, and find that they
generalize well to unseen in-distribution data, but their ability to
extrapolate to longer strings is worse than that of LSTMs. Our analyses show
that the learned self-attention patterns and representations modeled dependency
relations and demonstrated counting behavior, which may have helped the models
solve the languages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shunjie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steinert_Threlkeld_S/0/1/0/all/0/1&quot;&gt;Shane Steinert-Threlkeld&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.05270">
<title>CONFLATOR: Incorporating Switching Point based Rotatory Positional Encodings for Code-Mixed Language Modeling. (arXiv:2309.05270v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.05270</link>
<description rdf:parseType="Literal">&lt;p&gt;The mixing of two or more languages is called Code-Mixing (CM). CM is a
social norm in multilingual societies. Neural Language Models (NLMs) like
transformers have been effective on many NLP tasks. However, NLM for CM is an
under-explored area. Though transformers are capable and powerful, they cannot
always encode positional information since they are non-recurrent. Therefore,
to enrich word information and incorporate positional information, positional
encoding is defined. We hypothesize that Switching Points (SPs), i.e.,
junctions in the text where the language switches (L1 -&amp;gt; L2 or L2 -&amp;gt; L1), pose
a challenge for CM Language Models (LMs), and hence give special emphasis to
SPs in the modeling process. We experiment with several positional encoding
mechanisms and show that rotatory positional encodings along with switching
point information yield the best results.
&lt;/p&gt;
&lt;p&gt;We introduce CONFLATOR: a neural language modeling approach for code-mixed
languages. CONFLATOR tries to learn to emphasize switching points using smarter
positional encoding, both at unigram and bigram levels. CONFLATOR outperforms
the state-of-the-art on two tasks based on code-mixed Hindi and English
(Hinglish): (i) sentiment analysis and (ii) machine translation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1&quot;&gt;Mohsin Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teja_K/0/1/0/all/0/1&quot;&gt;Kandukuri Sai Teja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1&quot;&gt;Neeharika Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patwa_P/0/1/0/all/0/1&quot;&gt;Parth Patwa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatterjee_A/0/1/0/all/0/1&quot;&gt;Anubhab Chatterjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1&quot;&gt;Vinija Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1&quot;&gt;Aman Chadha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1&quot;&gt;Amitava Das&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.10444">
<title>Exploring Self-Reinforcement for Improving Learnersourced Multiple-Choice Question Explanations with Large Language Models. (arXiv:2309.10444v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2309.10444</link>
<description rdf:parseType="Literal">&lt;p&gt;Learnersourcing involves students generating and sharing learning resources
with their peers. When learnersourcing multiple-choice questions, creating
explanations for the generated questions is a crucial step as it facilitates a
deeper understanding of the related concepts. However, it is often difficult
for students to craft effective explanations due to limited subject
understanding and a tendency to merely restate the question stem, distractors,
and correct answer. To help scaffold this task, in this work we propose a
self-reinforcement large-language-model framework, with the goal of generating
and evaluating explanations automatically. Comprising three modules, the
framework generates student-aligned explanations, evaluates these explanations
to ensure their quality and iteratively enhances the explanations. If an
explanation&apos;s evaluation score falls below a defined threshold, the framework
iteratively refines and reassesses the explanation. Importantly, our framework
emulates the manner in which students compose explanations at the relevant
grade level. For evaluation, we had a human subject-matter expert compare the
explanations generated by students with the explanations created by the
open-source large language model Vicuna-13B, a version of Vicuna-13B that had
been fine-tuned using our method, and by GPT-4. We observed that, when compared
to other large language models, GPT-4 exhibited a higher level of creativity in
generating explanations. We also found that explanations generated by GPT-4
were ranked higher by the human expert than both those created by the other
models and the original student-created explanations. Our findings represent a
significant advancement in enriching the learnersourcing experience for
students and enhancing the capabilities of large language models in educational
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_Q/0/1/0/all/0/1&quot;&gt;Qiming Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leinonen_J/0/1/0/all/0/1&quot;&gt;Juho Leinonen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_A/0/1/0/all/0/1&quot;&gt;Alex Yuxuan Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1&quot;&gt;Wanjun Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pistotti_T/0/1/0/all/0/1&quot;&gt;Tim Pistotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_A/0/1/0/all/0/1&quot;&gt;Alice Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denny_P/0/1/0/all/0/1&quot;&gt;Paul Denny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Witbrock_M/0/1/0/all/0/1&quot;&gt;Michael Witbrock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiamou Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12871">
<title>AnglE-optimized Text Embeddings. (arXiv:2309.12871v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.12871</link>
<description rdf:parseType="Literal">&lt;p&gt;High-quality text embedding is pivotal in improving semantic textual
similarity (STS) tasks, which are crucial components in Large Language Model
(LLM) applications. However, a common challenge existing text embedding models
face is the problem of vanishing gradients, primarily due to their reliance on
the cosine function in the optimization objective, which has saturation zones.
To address this issue, this paper proposes a novel angle-optimized text
embedding model called AnglE. The core idea of AnglE is to introduce angle
optimization in a complex space. This novel approach effectively mitigates the
adverse effects of the saturation zone in the cosine function, which can impede
gradient and hinder optimization processes. To set up a comprehensive STS
evaluation, we experimented on existing short-text STS datasets and a newly
collected long-text STS dataset from GitHub Issues. Furthermore, we examine
domain-specific STS scenarios with limited labeled data and explore how AnglE
works with LLM-annotated data. Extensive experiments were conducted on various
tasks including short-text STS, long-text STS, and domain-specific STS tasks.
The results show that AnglE outperforms the state-of-the-art (SOTA) STS models
that ignore the cosine saturation zone. These findings demonstrate the ability
of AnglE to generate high-quality text embeddings and the usefulness of angle
optimization in STS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xianming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jing Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15630">
<title>NLPBench: Evaluating Large Language Models on Solving NLP Problems. (arXiv:2309.15630v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.15630</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent developments in large language models (LLMs) have shown promise in
enhancing the capabilities of natural language processing (NLP). Despite these
successes, there remains a dearth of research dedicated to the NLP
problem-solving abilities of LLMs. To fill the gap in this area, we present a
unique benchmarking dataset, NLPBench, comprising 378 college-level NLP
questions spanning various NLP topics sourced from Yale University&apos;s prior
final exams. NLPBench includes questions with context, in which multiple
sub-questions share the same public information, and diverse question types,
including multiple choice, short answer, and math. Our evaluation, centered on
LLMs such as GPT-3.5/4, PaLM-2, and LLAMA-2, incorporates advanced prompting
strategies like the chain-of-thought (CoT) and tree-of-thought (ToT). Our study
reveals that the effectiveness of the advanced prompting strategies can be
inconsistent, occasionally damaging LLM performance, especially in smaller
models like the LLAMA-2 (13b). Furthermore, our manual assessment illuminated
specific shortcomings in LLMs&apos; scientific problem-solving skills, with
weaknesses in logical decomposition and reasoning notably affecting results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1&quot;&gt;Linxin Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jieyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1&quot;&gt;Lechao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1&quot;&gt;Pengyuan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1&quot;&gt;Tianyi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1&quot;&gt;Irene Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00378">
<title>Measuring Value Understanding in Language Models through Discriminator-Critique Gap. (arXiv:2310.00378v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00378</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in Large Language Models (LLMs) have heightened concerns
about their potential misalignment with human values. However, evaluating their
grasp of these values is complex due to their intricate and adaptable nature.
We argue that truly understanding values in LLMs requires considering both
&quot;know what&quot; and &quot;know why&quot;. To this end, we present the Value Understanding
Measurement (VUM) framework that quantitatively assesses both &quot;know what&quot; and
&quot;know why&quot; by measuring the discriminator-critique gap related to human values.
Using the Schwartz Value Survey, we specify our evaluation values and develop a
thousand-level dialogue dataset with GPT-4. Our assessment looks at both the
value alignment of LLM&apos;s outputs compared to baseline answers and how LLM
responses align with reasons for value recognition versus GPT-4&apos;s annotations.
We evaluate five representative LLMs and provide strong evidence that the
scaling law significantly impacts &quot;know what&quot; but not much on &quot;know why&quot;, which
has consistently maintained a high level. This may further suggest that LLMs
might craft plausible explanations based on the provided context without truly
understanding their inherent value, indicating potential risks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhaowei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_F/0/1/0/all/0/1&quot;&gt;Fengshuo Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jun Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yaodong Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01448">
<title>Meta Semantic Template for Evaluation of Large Language Models. (arXiv:2310.01448v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01448</link>
<description rdf:parseType="Literal">&lt;p&gt;Do large language models (LLMs) genuinely understand the semantics of the
language, or just memorize the training data? The recent concern on potential
data contamination of LLMs has raised awareness of the community to conduct
research on LLMs evaluation. In this paper, we propose MSTemp, an approach that
creates meta semantic templates to evaluate the semantic understanding ability
of LLMs. The core of MSTemp is not to perform evaluation directly on existing
benchmark datasets, but to generate new out-of-distribution (OOD) evaluation
sets using existing datasets as seeds. Specifically, for a given sentence,
MSTemp leverages another language model to generate new samples while
preserving its semantics. The new samples are called semantic templates to the
original sentence. Then, MSTemp generates evaluation samples via sentence
parsing and random word replacement on the semantic templates. MSTemp is highly
flexible, dynamic, and cost-effective. Our initial experiments show that
MSTemp-generated samples can significantly reduce the performance of LLMs using
existing datasets as seeds. We hope this initial work can shed light on future
research of LLMs evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yachuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Liang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jindong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_Q/0/1/0/all/0/1&quot;&gt;Qiaozhu Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xing Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02031">
<title>OceanGPT: A Large Language Model for Ocean Science Tasks. (arXiv:2310.02031v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02031</link>
<description rdf:parseType="Literal">&lt;p&gt;Ocean science, which delves into the oceans that are reservoirs of life and
biodiversity, is of great significance given that oceans cover over 70% of our
planet&apos;s surface. Recently, advances in Large Language Models (LLMs) have
transformed the paradigm in science. Despite the success in other domains,
current LLMs often fall short in catering to the needs of domain experts like
oceanographers, and the potential of LLMs for ocean science is under-explored.
The intrinsic reason may be the immense and intricate nature of ocean data as
well as the necessity for higher granularity and richness in knowledge. To
alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean
domain, which is expert in various ocean science tasks. We propose DoInstruct,
a novel framework to automatically obtain a large volume of ocean domain
instruction data, which generates instructions based on multi-agent
collaboration. Additionally, we construct the first oceanography benchmark,
OceanBench, to evaluate the capabilities of LLMs in the ocean domain. Though
comprehensive experiments, OceanGPT not only shows a higher level of knowledge
expertise for oceans science tasks but also gains preliminary embodied
intelligence capabilities in ocean technology. Codes, data and checkpoints will
soon be available at https://github.com/zjunlp/KnowLM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1&quot;&gt;Zhen Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1&quot;&gt;Yida Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ou_Y/0/1/0/all/0/1&quot;&gt;Yixin Ou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_D/0/1/0/all/0/1&quot;&gt;Daxiong Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1&quot;&gt;Guozhou Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huajun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02954">
<title>DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning. (arXiv:2310.02954v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02954</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in natural language processing, primarily propelled by Large
Language Models (LLMs), have showcased their remarkable capabilities grounded
in in-context learning. A promising avenue for guiding LLMs in intricate
reasoning tasks involves the utilization of intermediate reasoning steps within
the Chain-of-Thought (CoT) paradigm. Nevertheless, the central challenge lies
in the effective selection of exemplars for facilitating in-context learning.
In this study, we introduce a framework that leverages Dual Queries and
Low-rank approximation Re-ranking (DQ-LoRe) to automatically select exemplars
for in-context learning. Dual Queries first query LLM to obtain LLM-generated
knowledge such as CoT, then query the retriever to obtain the final exemplars
via both question and the knowledge. Moreover, for the second query, LoRe
employs dimensionality reduction techniques to refine exemplar selection,
ensuring close alignment with the input question&apos;s knowledge. Through extensive
experiments, we demonstrate that DQ-LoRe significantly outperforms prior
state-of-the-art methods in the automatic selection of exemplars for GPT-4,
enhancing performance from 92.5% to 94.2%. Our comprehensive analysis further
reveals that DQ-LoRe consistently outperforms retrieval-based approaches in
terms of both performance and adaptability, especially in scenarios
characterized by distribution shifts. DQ-LoRe pushes the boundaries of
in-context learning and opens up new avenues for addressing complex reasoning
challenges. We will release the code soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1&quot;&gt;Jing Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zixuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Chuanyang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zhijiang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1&quot;&gt;Yichun Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1&quot;&gt;Enze Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhicheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1&quot;&gt;Qingxing Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haiming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xiongwei Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jing Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chengming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xiaodan Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05069">
<title>Unleashing the Multilingual Encoder Potential: Boosting Zero-Shot Performance via Probability Calibration. (arXiv:2310.05069v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05069</link>
<description rdf:parseType="Literal">&lt;p&gt;Pretrained multilingual encoder models can directly perform zero-shot
multilingual tasks or linguistic probing by reformulating the input examples
into cloze-style prompts. This is accomplished by predicting the probabilities
of the label words at the masked token position, without requiring any updates
to the model parameters. However, the performance of this method is limited by
the model&apos;s bias toward predicting label words which frequently occurred during
the pretraining. These words typically receive high probabilities. To address
this issue, we combine the models with calibration techniques which modify the
probabilities of label words predicted by the models. We first validate the
effectiveness of a proposed simple calibration method together with other
existing techniques on monolingual encoders in both zero- and few-shot
scenarios. We subsequently employ these calibration techniques on multilingual
encoders, resulting in substantial performance improvements across a wide range
of tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_E/0/1/0/all/0/1&quot;&gt;Ercong Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmid_H/0/1/0/all/0/1&quot;&gt;Helmut Schmid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1&quot;&gt;Hinrich Sch&amp;#xfc;tze&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05161">
<title>Recurrent Neural Language Models as Probabilistic Finite-state Automata. (arXiv:2310.05161v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05161</link>
<description rdf:parseType="Literal">&lt;p&gt;Studying language models (LMs) in terms of well-understood formalisms allows
us to precisely characterize their abilities and limitations. Previous work has
investigated the representational capacity of recurrent neural network (RNN)
LMs in terms of their capacity to recognize unweighted formal languages.
However, LMs do not describe unweighted formal languages -- rather, they define
probability distributions over strings. In this work, we study what classes of
such probability distributions RNN LMs can represent, which allows us to make
more direct statements about their capabilities. We show that simple RNNs are
equivalent to a subclass of probabilistic finite-state automata, and can thus
model a strict subset of probability distributions expressible by finite-state
models. Furthermore, we study the space complexity of representing finite-state
LMs with RNNs. We show that, to represent an arbitrary deterministic
finite-state LM with $N$ states over an alphabet $\Sigma$, an RNN requires
$\Omega\left(N |\Sigma|\right)$ neurons. These results present a first step
towards characterizing the classes of distributions RNN LMs can represent and
thus help us understand their capabilities and limitations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Svete_A/0/1/0/all/0/1&quot;&gt;Anej Svete&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1&quot;&gt;Ryan Cotterell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05199">
<title>Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human Feedback. (arXiv:2310.05199v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05199</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning from human feedback serves as a crucial bridge,
aligning large language models with human and societal values. This alignment
requires a vast corpus of human feedback to learn a reward model, which is
subsequently used to finetune language models. However, we have identified that
the reward model often finds shortcuts to bypass its intended objectives,
misleadingly assuming that humans prefer longer responses. The emergence of
length bias often induces the model to favor longer outputs, yet it doesn&apos;t
equate to an increase in helpful information within these outputs. In this
paper, we propose an innovative solution, applying the Product-of-Experts (PoE)
technique to separate reward modeling from the influence of sequence length. In
our framework, the main expert concentrates on understanding human intents,
while the biased expert targets the identification and capture of length bias.
To further enhance the learning of bias, we introduce perturbations into the
bias-focused expert, disrupting the flow of semantic information. Experimental
results validate the effectiveness of our approach, indicating that language
model performance is improved, irrespective of sequence length.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1&quot;&gt;Wei Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1&quot;&gt;Rui Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1&quot;&gt;Wenyu Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jun Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_S/0/1/0/all/0/1&quot;&gt;Shihan Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1&quot;&gt;Tao Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xuanjing Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05991">
<title>Enhancing Document-level Event Argument Extraction with Contextual Clues and Role Relevance. (arXiv:2310.05991v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05991</link>
<description rdf:parseType="Literal">&lt;p&gt;Document-level event argument extraction poses new challenges of long input
and cross-sentence inference compared to its sentence-level counterpart.
However, most prior works focus on capturing the relations between candidate
arguments and the event trigger in each event, ignoring two crucial points: a)
non-argument contextual clue information; b) the relevance among argument
roles. In this paper, we propose a SCPRG (Span-trigger-based Contextual Pooling
and latent Role Guidance) model, which contains two novel and effective modules
for the above problem. The Span-Trigger-based Contextual Pooling(STCP)
adaptively selects and aggregates the information of non-argument clue words
based on the context attention weights of specific argument-trigger pairs from
pre-trained model. The Role-based Latent Information Guidance (RLIG) module
constructs latent role representations, makes them interact through
role-interactive encoding to capture semantic relevance, and merges them into
candidate arguments. Both STCP and RLIG introduce no more than 1% new
parameters compared with the base model and can be easily applied to other
event extraction models, which are compact and transplantable. Experiments on
two public datasets show that our SCPRG outperforms previous state-of-the-art
methods, with 1.13 F1 and 2.64 F1 improvements on RAMS and WikiEvents
respectively. Further analyses illustrate the interpretability of our model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wanlong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;Shaohuan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1&quot;&gt;Dingyi Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1&quot;&gt;Hong Qu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.06165">
<title>CAW-coref: Conjunction-Aware Word-level Coreference Resolution. (arXiv:2310.06165v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.06165</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art coreference resolutions systems depend on multiple LLM calls
per document and are thus prohibitively expensive for many use cases (e.g.,
information extraction with large corpora). The leading word-level coreference
system (WL-coref) attains 96.6% of these SOTA systems&apos; performance while being
much more efficient. In this work, we identify a routine yet important failure
case of WL-coref: dealing with conjoined mentions such as &apos;Tom and Mary&apos;. We
offer a simple yet effective solution that improves the performance on the
OntoNotes test set by 0.9% F1, shrinking the gap between efficient word-level
coreference resolution and expensive SOTA approaches by 34.6%. Our
Conjunction-Aware Word-level coreference model (CAW-coref) and code is
available at https://github.com/KarelDO/wl-coref.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DOosterlinck_K/0/1/0/all/0/1&quot;&gt;Karel D&amp;#x27;Oosterlinck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bitew_S/0/1/0/all/0/1&quot;&gt;Semere Kiros Bitew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papineau_B/0/1/0/all/0/1&quot;&gt;Brandon Papineau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1&quot;&gt;Christopher Potts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demeester_T/0/1/0/all/0/1&quot;&gt;Thomas Demeester&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Develder_C/0/1/0/all/0/1&quot;&gt;Chris Develder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07091">
<title>Jaeger: A Concatenation-Based Multi-Transformer VQA Model. (arXiv:2310.07091v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07091</link>
<description rdf:parseType="Literal">&lt;p&gt;Document-based Visual Question Answering poses a challenging task between
linguistic sense disambiguation and fine-grained multimodal retrieval. Although
there has been encouraging progress in document-based question answering due to
the utilization of large language and open-world prior models\cite{1}, several
challenges persist, including prolonged response times, extended inference
durations, and imprecision in matching. In order to overcome these challenges,
we propose Jaegar, a concatenation-based multi-transformer VQA model. To derive
question features, we leverage the exceptional capabilities of RoBERTa
large\cite{2} and GPT2-xl\cite{3} as feature extractors. Subsequently, we
subject the outputs from both models to a concatenation process. This operation
allows the model to consider information from diverse sources concurrently,
strengthening its representational capability. By leveraging pre-trained models
for feature extraction, our approach has the potential to amplify the
performance of these models through concatenation. After concatenation, we
apply dimensionality reduction to the output features, reducing the model&apos;s
computational effectiveness and inference time. Empirical results demonstrate
that our proposed model achieves competitive performance on Task C of the
PDF-VQA Dataset. If the user adds any new data, they should make sure to style
it as per the instructions provided in previous sections.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_J/0/1/0/all/0/1&quot;&gt;Jieting Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zewei Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1&quot;&gt;Penghao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1&quot;&gt;Yidong Gan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07488">
<title>KwaiYiiMath: Technical Report. (arXiv:2310.07488v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07488</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in large language models (LLMs) have demonstrated
remarkable abilities in handling a variety of natural language processing (NLP)
downstream tasks, even on mathematical tasks requiring multi-step reasoning. In
this report, we introduce the KwaiYiiMath which enhances the mathematical
reasoning abilities of KwaiYiiBase1, by applying Supervised Fine-Tuning (SFT)
and Reinforced Learning from Human Feedback (RLHF), including on both English
and Chinese mathematical tasks. Meanwhile, we also constructed a small-scale
Chinese primary school mathematics test set (named KMath), consisting of 188
examples to evaluate the correctness of the problem-solving process generated
by the models. Empirical studies demonstrate that KwaiYiiMath can achieve
state-of-the-art (SOTA) performance on GSM8k, CMath, and KMath compared with
the similar size models, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jiayi Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Lei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xiaoyang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1&quot;&gt;Pengli Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhengzong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhirui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shengnan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1&quot;&gt;Xue Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1&quot;&gt;Xucheng Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1&quot;&gt;Yiqiao Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1&quot;&gt;Chao Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1&quot;&gt;Chengru Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1&quot;&gt;Junchen Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zijia Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fuzheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhongyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Di Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gai_K/0/1/0/all/0/1&quot;&gt;Kun Gai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08099">
<title>ClimateNLP: Analyzing Public Sentiment Towards Climate Change Using Natural Language Processing. (arXiv:2310.08099v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08099</link>
<description rdf:parseType="Literal">&lt;p&gt;Climate change&apos;s impact on human health poses unprecedented and diverse
challenges. Unless proactive measures based on solid evidence are implemented,
these threats will likely escalate and continue to endanger human well-being.
The escalating advancements in information and communication technologies have
facilitated the widespread availability and utilization of social media
platforms. Individuals utilize platforms such as Twitter and Facebook to
express their opinions, thoughts, and critiques on diverse subjects,
encompassing the pressing issue of climate change. The proliferation of climate
change-related content on social media necessitates comprehensive analysis to
glean meaningful insights. This paper employs natural language processing (NLP)
techniques to analyze climate change discourse and quantify the sentiment of
climate change-related tweets. We use ClimateBERT, a pretrained model
fine-tuned specifically for the climate change domain. The objective is to
discern the sentiment individuals express and uncover patterns in public
opinion concerning climate change. Analyzing tweet sentiments allows a deeper
comprehension of public perceptions, concerns, and emotions about this critical
global challenge. The findings from this experiment unearth valuable insights
into public sentiment and the entities associated with climate change
discourse. Policymakers, researchers, and organizations can leverage such
analyses to understand public perceptions, identify influential actors, and
devise informed strategies to address climate change challenges.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnan_A/0/1/0/all/0/1&quot;&gt;Ajay Krishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anoop_V/0/1/0/all/0/1&quot;&gt;V. S. Anoop&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08395">
<title>Prompting Large Language Models with Chain-of-Thought for Few-Shot Knowledge Base Question Generation. (arXiv:2310.08395v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08395</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of Question Generation over Knowledge Bases (KBQG) aims to convert a
logical form into a natural language question. For the sake of expensive cost
of large-scale question annotation, the methods of KBQG under low-resource
scenarios urgently need to be developed. However, current methods heavily rely
on annotated data for fine-tuning, which is not well-suited for few-shot
question generation. The emergence of Large Language Models (LLMs) has shown
their impressive generalization ability in few-shot tasks. Inspired by
Chain-of-Thought (CoT) prompting, which is an in-context learning strategy for
reasoning, we formulate KBQG task as a reasoning problem, where the generation
of a complete question is splitted into a series of sub-question generation.
Our proposed prompting method KQG-CoT first retrieves supportive logical forms
from the unlabeled data pool taking account of the characteristics of the
logical form. Then, we write a prompt to explicit the reasoning chain of
generating complicated questions based on the selected demonstrations. To
further ensure prompt quality, we extend KQG-CoT into KQG-CoT+ via sorting the
logical forms by their complexity. We conduct extensive experiments over three
public KBQG datasets. The results demonstrate that our prompting method
consistently outperforms other prompting baselines on the evaluated datasets.
Remarkably, our KQG-CoT+ method could surpass existing few-shot SoTA results of
the PathQuestions dataset by 18.25, 10.72, and 10.18 absolute points on BLEU-4,
METEOR, and ROUGE-L, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yuanyuan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Hanlun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_W/0/1/0/all/0/1&quot;&gt;Weining Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1&quot;&gt;Yunshi Lan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.09168">
<title>Explore-Instruct: Enhancing Domain-Specific Instruction Coverage through Active Exploration. (arXiv:2310.09168v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.09168</link>
<description rdf:parseType="Literal">&lt;p&gt;Instruction-tuning can be substantially optimized through enhanced diversity,
resulting in models capable of handling a broader spectrum of tasks. However,
existing data employed for such tuning often exhibit an inadequate coverage of
individual domains, limiting the scope for nuanced comprehension and
interactions within these areas. To address this deficiency, we propose
Explore-Instruct, a novel approach to enhance the data coverage to be used in
domain-specific instruction-tuning through active exploration via Large
Language Models (LLMs). Built upon representative domain use cases,
Explore-Instruct explores a multitude of variations or possibilities by
implementing a search algorithm to obtain diversified and domain-focused
instruction-tuning data. Our data-centric analysis validates the effectiveness
of this proposed approach in improving domain-specific instruction coverage.
Moreover, our model&apos;s performance demonstrates considerable advancements over
multiple baselines, including those utilizing domain-specific data enhancement.
Our findings offer a promising opportunity to improve instruction coverage,
especially in domain-specific contexts, thereby advancing the development of
adaptable language models. Our code, model weights, and data are public at
\url{https://github.com/fanqiwan/Explore-Instruct}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_F/0/1/0/all/0/1&quot;&gt;Fanqi Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xinting Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Tao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1&quot;&gt;Xiaojun Quan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_W/0/1/0/all/0/1&quot;&gt;Wei Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1&quot;&gt;Shuming Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.09342">
<title>Ranking LLM-Generated Loop Invariants for Program Verification. (arXiv:2310.09342v2 [cs.PL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.09342</link>
<description rdf:parseType="Literal">&lt;p&gt;Synthesizing inductive loop invariants is fundamental to automating program
verification. In this work, we observe that Large Language Models (such as
gpt-3.5 or gpt-4) are capable of synthesizing loop invariants for a class of
programs in a 0-shot setting, yet require several samples to generate the
correct invariants. This can lead to a large number of calls to a program
verifier to establish an invariant. To address this issue, we propose a {\it
re-ranking} approach for the generated results of LLMs. We have designed a
ranker that can distinguish between correct inductive invariants and incorrect
attempts based on the problem definition. The ranker is optimized as a
contrastive ranker. Experimental results demonstrate that this re-ranking
mechanism significantly improves the ranking of correct invariants among the
generated candidates, leading to a notable reduction in the number of calls to
a verifier.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1&quot;&gt;Saikat Chakraborty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lahiri_S/0/1/0/all/0/1&quot;&gt;Shuvendu K. Lahiri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fakhoury_S/0/1/0/all/0/1&quot;&gt;Sarah Fakhoury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Musuvathi_M/0/1/0/all/0/1&quot;&gt;Madanlal Musuvathi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lal_A/0/1/0/all/0/1&quot;&gt;Akash Lal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rastogi_A/0/1/0/all/0/1&quot;&gt;Aseem Rastogi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Senthilnathan_A/0/1/0/all/0/1&quot;&gt;Aditya Senthilnathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1&quot;&gt;Rahul Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swamy_N/0/1/0/all/0/1&quot;&gt;Nikhil Swamy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.09430">
<title>A Systematic Evaluation of Large Language Models on Out-of-Distribution Logical Reasoning Tasks. (arXiv:2310.09430v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.09430</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs), such as GPT-3.5 and GPT-4, have greatly
advanced the performance of artificial systems on various natural language
processing tasks to human-like levels. However, their generalisation and
robustness to perform logical reasoning remain under-evaluated. To probe this
ability, we propose three new logical reasoning datasets named &quot;ReClor-plus&quot;,
&quot;LogiQA-plus&quot; and &quot;LogiQAv2-plus&quot;, each featuring three subsets: the first with
randomly shuffled options, the second with the correct choices replaced by
&quot;none of the other options are correct&quot;, and a combination of the previous two
subsets. We carry out experiments on these datasets with both discriminative
and generative LLMs and show that these simple tricks greatly hinder the
performance of the language models. Despite their superior performance on the
original publicly available datasets, we find that all models struggle to
answer our newly constructed datasets. We show that introducing task variations
by perturbing a sizable training set can markedly improve the model&apos;s
generalisation and robustness in logical reasoning tasks. Moreover, applying
logic-driven data augmentation for fine-tuning, combined with prompting can
enhance the generalisation performance of both discriminative large language
models and generative large language models. These results offer insights into
assessing and improving the generalisation and robustness of large language
models for logical reasoning tasks. We make our source code and data publicly
available
\url{https://github.com/Strong-AI-Lab/Logical-and-abstract-reasoning}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_Q/0/1/0/all/0/1&quot;&gt;Qiming Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gendron_G/0/1/0/all/0/1&quot;&gt;Gael Gendron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_A/0/1/0/all/0/1&quot;&gt;Alex Yuxuan Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1&quot;&gt;Wanjun Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_N/0/1/0/all/0/1&quot;&gt;Neset Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Witbrock_M/0/1/0/all/0/1&quot;&gt;Michael Witbrock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiamou Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10191">
<title>VIBE: Topic-Driven Temporal Adaptation for Twitter Classification. (arXiv:2310.10191v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10191</link>
<description rdf:parseType="Literal">&lt;p&gt;Language features are evolving in real-world social media, resulting in the
deteriorating performance of text classification in dynamics. To address this
challenge, we study temporal adaptation, where models trained on past data are
tested in the future. Most prior work focused on continued pretraining or
knowledge updating, which may compromise their performance on noisy social
media data. To tackle this issue, we reflect feature change via modeling latent
topic evolution and propose a novel model, VIBE: Variational Information
Bottleneck for Evolutions. Concretely, we first employ two Information
Bottleneck (IB) regularizers to distinguish past and future topics. Then, the
distinguished topics work as adaptive features via multi-task training with
timestamp and class label prediction. In adaptive learning, VIBE utilizes
retrieved unlabeled data from online streams created posterior to training data
time. Substantial Twitter experiments on three classification tasks show that
our model, with only 3% of data, significantly outperforms previous
state-of-the-art continued-pretraining methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuji Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenjie Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10638">
<title>In-Context Pretraining: Language Modeling Beyond Document Boundaries. (arXiv:2310.10638v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10638</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LMs) are currently trained to predict tokens given
document prefixes, enabling them to directly perform long-form generation and
prompting-style tasks which can be reduced to document completion. Existing
pretraining pipelines train LMs by concatenating random sets of short documents
to create input contexts but the prior documents provide no signal for
predicting the next document. We instead present In-Context Pretraining, a new
approach where language models are pretrained on a sequence of related
documents, thereby explicitly encouraging them to read and reason across
document boundaries. We can do In-Context Pretraining by simply changing the
document ordering so that each context contains related documents, and directly
applying existing pretraining pipelines. However, this document sorting problem
is challenging. There are billions of documents and we would like the sort to
maximize contextual similarity for every document without repeating any data.
To do this, we introduce approximate algorithms for finding related documents
with efficient nearest neighbor search and constructing coherent input contexts
with a graph traversal algorithm. Our experiments show In-Context Pretraining
offers a simple and scalable approach to significantly enhance LMs&apos;performance:
we see notable improvements in tasks that require more complex contextual
reasoning, including in-context learning (+8%), reading comprehension (+15%),
faithfulness to previous contexts (+16%), long-context reasoning (+5%), and
retrieval augmentation (+9%).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1&quot;&gt;Weijia Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1&quot;&gt;Sewon Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lomeli_M/0/1/0/all/0/1&quot;&gt;Maria Lomeli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chunting Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Margaret Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_V/0/1/0/all/0/1&quot;&gt;Victoria Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1&quot;&gt;Noah A. Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1&quot;&gt;Luke Zettlemoyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yih_S/0/1/0/all/0/1&quot;&gt;Scott Yih&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1&quot;&gt;Mike Lewis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10765">
<title>BiomedJourney: Counterfactual Biomedical Image Generation by Instruction-Learning from Multimodal Patient Journeys. (arXiv:2310.10765v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10765</link>
<description rdf:parseType="Literal">&lt;p&gt;Rapid progress has been made in instruction-learning for image editing with
natural-language instruction, as exemplified by InstructPix2Pix. In
biomedicine, such methods can be applied to counterfactual image generation,
which helps differentiate causal structure from spurious correlation and
facilitate robust image interpretation for disease progression modeling.
However, generic image-editing models are ill-suited for the biomedical domain,
and counterfactual biomedical image generation is largely underexplored. In
this paper, we present BiomedJourney, a novel method for counterfactual
biomedical image generation by instruction-learning from multimodal patient
journeys. Given a patient with two biomedical images taken at different time
points, we use GPT-4 to process the corresponding imaging reports and generate
a natural language description of disease progression. The resulting triples
(prior image, progression description, new image) are then used to train a
latent diffusion model for counterfactual biomedical image generation. Given
the relative scarcity of image time series data, we introduce a two-stage
curriculum that first pretrains the denoising network using the much more
abundant single image-report pairs (with dummy prior image), and then continues
training using the counterfactual triples. Experiments using the standard
MIMIC-CXR dataset demonstrate the promise of our method. In a comprehensive
battery of tests on counterfactual medical image generation, BiomedJourney
substantially outperforms prior state-of-the-art methods in instruction image
editing and medical image generation such as InstructPix2Pix and RoentGen. To
facilitate future study in counterfactual medical generation, we plan to
release our instruction-learning code and pretrained models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yu Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jianwei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1&quot;&gt;Naoto Usuyama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chunyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Sheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1&quot;&gt;Matthew P. Lungren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jianfeng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1&quot;&gt;Hoifung Poon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.11097">
<title>Experimenting AI Technologies for Disinformation Combat: the IDMO Project. (arXiv:2310.11097v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.11097</link>
<description rdf:parseType="Literal">&lt;p&gt;The Italian Digital Media Observatory (IDMO) project, part of a European
initiative, focuses on countering disinformation and fake news. This report
outlines contributions from Rai-CRITS to the project, including: (i) the
creation of novel datasets for testing technologies (ii) development of an
automatic model for categorizing Pagella Politica verdicts to facilitate
broader analysis (iii) creation of an automatic model for recognizing textual
entailment with exceptional accuracy on the FEVER dataset (iv) assessment using
GPT-4 to identify textual entailmen (v) a game to raise awareness about fake
news at national events.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Canale_L/0/1/0/all/0/1&quot;&gt;Lorenzo Canale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Messina_A/0/1/0/all/0/1&quot;&gt;Alberto Messina&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.11368">
<title>VECHR: A Dataset for Explainable and Robust Classification of Vulnerability Type in the European Court of Human Rights. (arXiv:2310.11368v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.11368</link>
<description rdf:parseType="Literal">&lt;p&gt;Recognizing vulnerability is crucial for understanding and implementing
targeted support to empower individuals in need. This is especially important
at the European Court of Human Rights (ECtHR), where the court adapts
Convention standards to meet actual individual needs and thus ensures effective
human rights protection. However, the concept of vulnerability remains elusive
at the ECtHR and no prior NLP research has dealt with it. To enable future
research in this area, we present VECHR, a novel expert-annotated multi-label
dataset comprising of vulnerability type classification and explanation
rationale. We benchmark the performance of state-of-the-art models on VECHR
from both prediction and explainability perspectives. Our results demonstrate
the challenging nature of the task with lower prediction performance and
limited agreement between models and experts. Further, we analyze the
robustness of these models in dealing with out-of-domain (OOD) data and observe
overall limited performance. Our dataset poses unique challenges offering
significant room for improvement regarding performance, explainability, and
robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shanshan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Staufer_L/0/1/0/all/0/1&quot;&gt;Leon Staufer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+S_S/0/1/0/all/0/1&quot;&gt;Santosh T.Y.S.S&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ichim_O/0/1/0/all/0/1&quot;&gt;Oana Ichim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heri_C/0/1/0/all/0/1&quot;&gt;Corina Heri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grabmair_M/0/1/0/all/0/1&quot;&gt;Matthias Grabmair&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.11670">
<title>Prototype-based HyperAdapter for Sample-Efficient Multi-task Tuning. (arXiv:2310.11670v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.11670</link>
<description rdf:parseType="Literal">&lt;p&gt;Parameter-efficient fine-tuning (PEFT) has shown its effectiveness in
adapting the pre-trained language models to downstream tasks while only
updating a small number of parameters. Despite the success, most existing
methods independently adapt to each task without considering knowledge transfer
between tasks and are limited to low-data regimes. To overcome this issue, we
propose Prototype-based HyperAdapter (PHA), a novel framework built on the
adapter-tuning and hypernetwork. It introduces an instance-dense retriever and
a prototypical hypernetwork to generate the conditional modules in a
sample-efficient manner. This leads to comparable performance improvements
against existing PEFT methods on multi-task learning and few-shot transfer
learning. More importantly, when the available data size gets smaller, our
method outperforms other strong baselines by a large margin. Based on our
extensive empirical experiments across various datasets, we demonstrate that
PHA strikes a better trade-off between trainable parameters, accuracy on stream
tasks, and sample efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zhaofeng He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.11878">
<title>From Dissonance to Insights: Dissecting Disagreements in Rationale Construction for Case Outcome Classification. (arXiv:2310.11878v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.11878</link>
<description rdf:parseType="Literal">&lt;p&gt;In legal NLP, Case Outcome Classification (COC) must not only be accurate but
also trustworthy and explainable. Existing work in explainable COC has been
limited to annotations by a single expert. However, it is well-known that
lawyers may disagree in their assessment of case facts. We hence collect a
novel dataset RAVE: Rationale Variation in ECHR1, which is obtained from two
experts in the domain of international human rights law, for whom we observe
weak agreement. We study their disagreements and build a two-level
task-independent taxonomy, supplemented with COC-specific subcategories. To our
knowledge, this is the first work in the legal NLP that focuses on human label
variation. We quantitatively assess different taxonomy categories and find that
disagreements mainly stem from underspecification of the legal context, which
poses challenges given the typically limited granularity and noise in COC
metadata. We further assess the explainablility of SOTA COC models on RAVE and
observe limited agreement between models and experts. Overall, our case study
reveals hitherto underappreciated complexities in creating benchmark datasets
in legal NLP that revolve around identifying aspects of a case&apos;s facts
supposedly relevant to its outcome.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shanshan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+S_S/0/1/0/all/0/1&quot;&gt;Santosh T.Y.S.S&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ichim_O/0/1/0/all/0/1&quot;&gt;Oana Ichim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Risini_I/0/1/0/all/0/1&quot;&gt;Isabella Risini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1&quot;&gt;Barbara Plank&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grabmair_M/0/1/0/all/0/1&quot;&gt;Matthias Grabmair&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.04934">
<title>Generative Pretrained Autoregressive Transformer Graph Neural Network applied to the Analysis and Discovery of Novel Proteins. (arXiv:2305.04934v2 [q-bio.BM] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2305.04934</link>
<description rdf:parseType="Literal">&lt;p&gt;We report a flexible language-model based deep learning strategy, applied
here to solve complex forward and inverse problems in protein modeling, based
on an attention neural network that integrates transformer and graph
convolutional architectures in a causal multi-headed graph mechanism, to
realize a generative pretrained model. The model is applied to predict
secondary structure content (per-residue level and overall content), protein
solubility, and sequencing tasks. Further trained on inverse tasks, the model
is rendered capable of designing proteins with these properties as target
features. The model is formulated as a general framework, completely
prompt-based, and can be adapted for a variety of downstream tasks. We find
that adding additional tasks yields emergent synergies that the model exploits
in improving overall performance, beyond what would be possible by training a
model on each dataset alone. Case studies are presented to validate the method,
yielding protein designs specifically focused on structural proteins, but also
exploring the applicability in the design of soluble, antimicrobial
biomaterials. While our model is trained to ultimately perform 8 distinct
tasks, with available datasets it can be extended to solve additional problems.
In a broader sense, this work illustrates a form of multiscale modeling that
relates a set of ultimate building blocks (here, byte-level utf8 characters
that define the nature of the physical system at hand) to complex output. This
materiomic scheme captures complex emergent relationships between universal
building block and resulting properties via a synergizing learning capacity to
express a set of potentialities embedded in the knowledge used in training, via
the interplay of universality and diversity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Buehler_M/0/1/0/all/0/1&quot;&gt;Markus J. Buehler&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>