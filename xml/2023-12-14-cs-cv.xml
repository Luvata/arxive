<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-12-12T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06670" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06680" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06686" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06697" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06699" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06701" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06703" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06704" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06706" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06708" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06709" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06712" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06713" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06716" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06719" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06720" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06721" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06722" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06723" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06725" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06726" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06728" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06729" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06731" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06733" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06734" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06736" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06738" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06739" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06740" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06741" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06742" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06797" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06799" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06801" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06829" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06833" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06850" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06865" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06868" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06886" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06892" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06899" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06900" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06914" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06920" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06925" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06934" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06940" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06946" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06947" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06950" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06955" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06958" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06960" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06968" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06971" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06978" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06979" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06988" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06991" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06995" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06999" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07006" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07009" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07021" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07039" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07048" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07051" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07052" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07061" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07062" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07063" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07064" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07066" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07067" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07079" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07082" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07100" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07125" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07128" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07132" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07133" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07165" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07169" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07180" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07190" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07199" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07207" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07221" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07226" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07231" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07245" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07246" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07255" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07258" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07264" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07266" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07273" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07311" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07315" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07322" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07327" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07330" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07331" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07342" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07352" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07353" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07357" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07360" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07364" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07370" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07374" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07378" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07381" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07384" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07385" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07389" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07395" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07408" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07409" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07418" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07423" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07424" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07425" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07428" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07435" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07437" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07460" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07466" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07472" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07478" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07485" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07488" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07489" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2103.09151" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.06074" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.04281" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.05575" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.01962" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.05166" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.14404" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.01819" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.05909" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.03169" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.04037" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.11093" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.00855" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.01092" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.01168" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.07072" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.14123" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.14660" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.01971" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10124" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15404" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16318" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17421" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19302" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.02099" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04356" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09124" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.12153" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.12424" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14435" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00910" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05845" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04669" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11488" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16139" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16576" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16876" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.08769" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.09431" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13747" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14585" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16808" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04816" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07730" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12474" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15646" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19797" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06000" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15497" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16500" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16516" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17116" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.18130" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00343" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02221" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03018" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03033" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03594" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03774" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04316" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04344" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04837" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05695" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05743" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05799" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05924" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06069" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06409" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06630" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06643" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04547" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2312.06670">
<title>Combating the effects of speed and delays in end-to-end self-driving. (arXiv:2312.06670v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.06670</link>
<description rdf:parseType="Literal">&lt;p&gt;In the behavioral cloning approach to end-to-end driving, a dataset of expert
driving is collected and the model learns to guess what the expert would do in
different situations. Situations are summarized in observations and the outputs
are low or mid-level commands (e.g. brake, throttle, and steering; or
trajectories). The models learn to match observations at time T to actions
recorded at T or as simultaneously as possible. However, when deploying the
models to the real world (or to an asynchronous simulation), the action
predicted based on observations at time T gets applied at T + $\Delta$ T. In a
variety of cases, $\Delta$ T can be considerable and significantly influence
performance.
&lt;/p&gt;
&lt;p&gt;We first demonstrate that driving at two different speeds is effectively two
different tasks. Delays partially cause this difference and linearly amplify
it. Even without computational delays, actuator delays and slipping due to
inertia result in the need to perform actions preemptively when driving fast.
The function mapping observations to commands becomes different compared to
slow driving. We experimentally show that models trained to drive fast cannot
perform the seemingly easier task of driving slow and vice-versa. Good driving
models may be judged to be poor due to testing them at &quot;a safe low speed&quot;, a
task they cannot perform.
&lt;/p&gt;
&lt;p&gt;Secondly, we show how to counteract the effect of delays in end-to-end
networks by changing the target labels. This is in contrast to the approaches
attempting to minimize the delays, i.e. the cause, not the effect. To exemplify
the problems and solutions in the real world, we use 1:10 scale minicars with
limited computing power, using behavioral cloning for end-to-end driving. Some
of the ideas discussed here may be transferable to the wider context of
self-driving, to vehicles with more compute power and end-to-mid or modular
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tampuu_A/0/1/0/all/0/1&quot;&gt;Ardi Tampuu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uduste_I/0/1/0/all/0/1&quot;&gt;Ilmar Uduste&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roosild_K/0/1/0/all/0/1&quot;&gt;Kristjan Roosild&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06680">
<title>Perceptual Similarity guidance and text guidance optimization for Editing Real Images using Guided Diffusion Models. (arXiv:2312.06680v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06680</link>
<description rdf:parseType="Literal">&lt;p&gt;When using a diffusion model for image editing, there are times when the
modified image can differ greatly from the source. To address this, we apply a
dual-guidance approach to maintain high fidelity to the original in areas that
are not altered. First, we employ text-guided optimization, using text
embeddings to direct latent space and classifier-free guidance. Second, we use
perceptual similarity guidance, optimizing latent vectors with posterior
sampling via Tweedie formula during the reverse process. This method ensures
the realistic rendering of both the edited elements and the preservation of the
unedited parts of the original image.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruichen Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06686">
<title>Robo360: A 3D Omnispective Multi-Material Robotic Manipulation Dataset. (arXiv:2312.06686v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06686</link>
<description rdf:parseType="Literal">&lt;p&gt;Building robots that can automate labor-intensive tasks has long been the
core motivation behind the advancements in computer vision and the robotics
community. Recent interest in leveraging 3D algorithms, particularly neural
fields, has led to advancements in robot perception and physical understanding
in manipulation scenarios. However, the real world&apos;s complexity poses
significant challenges. To tackle these challenges, we present Robo360, a
dataset that features robotic manipulation with a dense view coverage, which
enables high-quality 3D neural representation learning, and a diverse set of
objects with various physical and optical properties and facilitates research
in various object manipulation and physical world modeling tasks. We confirm
the effectiveness of our dataset using existing dynamic NeRF and evaluate its
potential in learning multi-view policies. We hope that Robo360 can open new
research directions yet to be explored at the intersection of understanding the
physical world in 3D and robot control.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1&quot;&gt;Litian Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_L/0/1/0/all/0/1&quot;&gt;Liuyu Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Caiwei Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jialin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Linghao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_I/0/1/0/all/0/1&quot;&gt;Isabella Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_F/0/1/0/all/0/1&quot;&gt;Fanbo Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhiao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hao Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06697">
<title>Performance of externally validated machine learning models based on histopathology images for the diagnosis, classification, prognosis, or treatment outcome prediction in female breast cancer: A systematic review. (arXiv:2312.06697v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06697</link>
<description rdf:parseType="Literal">&lt;p&gt;Numerous machine learning (ML) models have been developed for breast cancer
using various types of data. Successful external validation (EV) of ML models
is important evidence of their generalizability. The aim of this systematic
review was to assess the performance of externally validated ML models based on
histopathology images for diagnosis, classification, prognosis, or treatment
outcome prediction in female breast cancer. A systematic search of MEDLINE,
EMBASE, CINAHL, IEEE, MICCAI, and SPIE conferences was performed for studies
published between January 2010 and February 2022. The Prediction Model Risk of
Bias Assessment Tool (PROBAST) was employed, and the results were narratively
described. Of the 2011 non-duplicated citations, 8 journal articles and 2
conference proceedings met inclusion criteria. Three studies externally
validated ML models for diagnosis, 4 for classification, 2 for prognosis, and 1
for both classification and prognosis. Most studies used Convolutional Neural
Networks and one used logistic regression algorithms. For
diagnostic/classification models, the most common performance metrics reported
in the EV were accuracy and area under the curve, which were greater than 87%
and 90%, respectively, using pathologists&apos; annotations as ground truth. The
hazard ratios in the EV of prognostic ML models were between 1.7 (95% CI,
1.2-2.6) and 1.8 (95% CI, 1.3-2.7) to predict distant disease-free survival;
1.91 (95% CI, 1.11-3.29) for recurrence, and between 0.09 (95% CI, 0.01-0.70)
and 0.65 (95% CI, 0.43-0.98) for overall survival, using clinical data as
ground truth. Despite EV being an important step before the clinical
application of a ML model, it hasn&apos;t been performed routinely. The large
variability in the training/validation datasets, methods, performance metrics,
and reported information limited the comparison of the models and the analysis
of their results (...)
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_R/0/1/0/all/0/1&quot;&gt;Ricardo Gonzalez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nejat_P/0/1/0/all/0/1&quot;&gt;Peyman Nejat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1&quot;&gt;Ashirbani Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Campbell_C/0/1/0/all/0/1&quot;&gt;Clinton J.V. Campbell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Norgan_A/0/1/0/all/0/1&quot;&gt;Andrew P. Norgan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lokker_C/0/1/0/all/0/1&quot;&gt;Cynthia Lokker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06699">
<title>Leveraging Generative Language Models for Weakly Supervised Sentence Component Analysis in Video-Language Joint Learning. (arXiv:2312.06699v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06699</link>
<description rdf:parseType="Literal">&lt;p&gt;A thorough comprehension of textual data is a fundamental element in
multi-modal video analysis tasks. However, recent works have shown that the
current models do not achieve a comprehensive understanding of the textual data
during the training for the target downstream tasks. Orthogonal to the previous
approaches to this limitation, we postulate that understanding the significance
of the sentence components according to the target task can potentially enhance
the performance of the models. Hence, we utilize the knowledge of a pre-trained
large language model (LLM) to generate text samples from the original ones,
targeting specific sentence components. We propose a weakly supervised
importance estimation module to compute the relative importance of the
components and utilize them to improve different video-language tasks. Through
rigorous quantitative analysis, our proposed method exhibits significant
improvement across several video-language tasks. In particular, our approach
notably enhances video-text retrieval by a relative improvement of 8.3\% in
video-to-text and 1.4\% in text-to-video retrieval over the baselines, in terms
of R@1. Additionally, in video moment retrieval, average mAP shows a relative
improvement ranging from 2.0\% to 13.7 \% across different baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hakim_Z/0/1/0/all/0/1&quot;&gt;Zaber Ibn Abdul Hakim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarker_N/0/1/0/all/0/1&quot;&gt;Najibul Haque Sarker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1&quot;&gt;Rahul Pratap Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paul_B/0/1/0/all/0/1&quot;&gt;Bishmoy Paul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dabouei_A/0/1/0/all/0/1&quot;&gt;Ali Dabouei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Min Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06701">
<title>Dynamic Adversarial Attacks on Autonomous Driving Systems. (arXiv:2312.06701v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.06701</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces an attacking mechanism to challenge the resilience of
autonomous driving systems. Specifically, we manipulate the decision-making
processes of an autonomous vehicle by dynamically displaying adversarial
patches on a screen mounted on another moving vehicle. These patches are
optimized to deceive the object detection models into misclassifying targeted
objects, e.g., traffic signs. Such manipulation has significant implications
for critical multi-vehicle interactions such as intersection crossing and lane
changing, which are vital for safe and efficient autonomous driving systems.
Particularly, we make four major contributions. First, we introduce a novel
adversarial attack approach where the patch is not co-located with its target,
enabling more versatile and stealthy attacks. Moreover, our method utilizes
dynamic patches displayed on a screen, allowing for adaptive changes and
movement, enhancing the flexibility and performance of the attack. To do so, we
design a Screen Image Transformation Network (SIT-Net), which simulates
environmental effects on the displayed images, narrowing the gap between
simulated and real-world scenarios. Further, we integrate a positional loss
term into the adversarial training process to increase the success rate of the
dynamic attack. Finally, we shift the focus from merely attacking perceptual
systems to influencing the decision-making algorithms of self-driving systems.
Our experiments demonstrate the first successful implementation of such dynamic
adversarial attacks in real-world autonomous driving scenarios, paving the way
for advancements in the field of robust and secure autonomous driving.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chahe_A/0/1/0/all/0/1&quot;&gt;Amirhosein Chahe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chenan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeyapratap_A/0/1/0/all/0/1&quot;&gt;Abhishek Jeyapratap&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kaidi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Lifeng Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06703">
<title>OpenSD: Unified Open-Vocabulary Segmentation and Detection. (arXiv:2312.06703v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06703</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, a few open-vocabulary methods have been proposed by employing a
unified architecture to tackle generic segmentation and detection tasks.
However, their performance still lags behind the task-specific models due to
the conflict between different tasks, and their open-vocabulary capability is
limited due to the inadequate use of CLIP. To address these challenges, we
present a universal transformer-based framework, abbreviated as OpenSD, which
utilizes the same architecture and network parameters to handle open-vocabulary
segmentation and detection tasks. First, we introduce a decoder decoupled
learning strategy to alleviate the semantic conflict between thing and staff
categories so that each individual task can be learned more effectively under
the same framework. Second, to better leverage CLIP for end-to-end segmentation
and detection, we propose dual classifiers to handle the in-vocabulary domain
and out-of-vocabulary domain, respectively. The text encoder is further trained
to be region-aware for both thing and stuff categories through decoupled prompt
learning, enabling them to filter out duplicated and low-quality predictions,
which is important to end-to-end segmentation and detection. Extensive
experiments are conducted on multiple datasets under various circumstances. The
results demonstrate that OpenSD outperforms state-of-the-art open-vocabulary
segmentation and detection methods in both closed- and open-vocabulary
settings. Code is available at https://github.com/strongwolf/OpenSD
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Minghan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Pengfei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06704">
<title>SIFU: Side-view Conditioned Implicit Function for Real-world Usable Clothed Human Reconstruction. (arXiv:2312.06704v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06704</link>
<description rdf:parseType="Literal">&lt;p&gt;Creating high-quality 3D models of clothed humans from single images for
real-world applications is crucial. Despite recent advancements, accurately
reconstructing humans in complex poses or with loose clothing from in-the-wild
images, along with predicting textures for unseen areas, remains a significant
challenge. A key limitation of previous methods is their insufficient prior
guidance in transitioning from 2D to 3D and in texture prediction. In response,
we introduce SIFU (Side-view Conditioned Implicit Function for Real-world
Usable Clothed Human Reconstruction), a novel approach combining a Side-view
Decoupling Transformer with a 3D Consistent Texture Refinement pipeline.SIFU
employs a cross-attention mechanism within the transformer, using SMPL-X
normals as queries to effectively decouple side-view features in the process of
mapping 2D features to 3D. This method not only improves the precision of the
3D models but also their robustness, especially when SMPL-X estimates are not
perfect. Our texture refinement process leverages text-to-image diffusion-based
prior to generate realistic and consistent textures for invisible views.
Through extensive experiments, SIFU surpasses SOTA methods in both geometry and
texture reconstruction, showcasing enhanced robustness in complex scenarios and
achieving an unprecedented Chamfer and P2S measurement. Our approach extends to
practical applications such as 3D printing and scene building, demonstrating
its broad utility in real-world scenarios. Project page
https://river-zhang.github.io/SIFU-projectpage/ .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zechuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zongxin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06706">
<title>UNeR3D: Versatile and Scalable 3D RGB Point Cloud Generation from 2D Images in Unsupervised Reconstruction. (arXiv:2312.06706v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06706</link>
<description rdf:parseType="Literal">&lt;p&gt;In the realm of 3D reconstruction from 2D images, a persisting challenge is
to achieve high-precision reconstructions devoid of 3D Ground Truth data
reliance. We present UNeR3D, a pioneering unsupervised methodology that sets a
new standard for generating detailed 3D reconstructions solely from 2D views.
Our model significantly cuts down the training costs tied to supervised
approaches and introduces RGB coloration to 3D point clouds, enriching the
visual experience. Employing an inverse distance weighting technique for color
rendering, UNeR3D ensures seamless color transitions, enhancing visual
fidelity. Our model&apos;s flexible architecture supports training with any number
of views, and uniquely, it is not constrained by the number of views used
during training when performing reconstructions. It can infer with an arbitrary
count of views during inference, offering unparalleled versatility.
Additionally, the model&apos;s continuous spatial input domain allows the generation
of point clouds at any desired resolution, empowering the creation of
high-resolution 3D RGB point clouds. We solidify the reconstruction process
with a novel multi-view geometric loss and color loss, demonstrating that our
model excels with single-view inputs and beyond, thus reshaping the paradigm of
unsupervised learning in 3D vision. Our contributions signal a substantial leap
forward in 3D vision, offering new horizons for content creation across diverse
applications. Code is available at https://github.com/HongbinLin3589/UNeR3D.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1&quot;&gt;Hongbin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Juangui Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qingfeng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zhengyu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Handing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yunzhi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yongjun Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_Z/0/1/0/all/0/1&quot;&gt;Zhenguo Nie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06708">
<title>Neutral Editing Framework for Diffusion-based Video Editing. (arXiv:2312.06708v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06708</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-conditioned image editing has succeeded in various types of editing
based on a diffusion framework. Unfortunately, this success did not carry over
to a video, which continues to be challenging. Existing video editing systems
are still limited to rigid-type editing such as style transfer and object
overlay. To this end, this paper proposes Neutral Editing (NeuEdit) framework
to enable complex non-rigid editing by changing the motion of a person/object
in a video, which has never been attempted before. NeuEdit introduces a concept
of `neutralization&apos; that enhances a tuning-editing process of diffusion-based
editing systems in a model-agnostic manner by leveraging input video and text
without any other auxiliary aids (e.g., visual masks, video captions).
Extensive experiments on numerous videos demonstrate adaptability and
effectiveness of the NeuEdit framework. The website of our work is available
here: https://neuedit.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1&quot;&gt;Sunjae Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koo_G/0/1/0/all/0/1&quot;&gt;Gwanhyeong Koo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1&quot;&gt;Ji Woo Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1&quot;&gt;Chang D. Yoo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06709">
<title>AM-RADIO: Agglomerative Model -- Reduce All Domains Into One. (arXiv:2312.06709v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06709</link>
<description rdf:parseType="Literal">&lt;p&gt;A handful of visual foundation models (VFMs) have recently emerged as the
backbones for numerous downstream tasks. VFMs like CLIP, DINOv2, SAM are
trained with distinct objectives, exhibiting unique characteristics for various
downstream tasks. We find that despite their conceptual differences, these
models can be effectively merged into a unified model through multi-teacher
distillation. We name this approach AM-RADIO (Agglomerative Model -- Reduce All
Domains Into One). This integrative approach not only surpasses the performance
of individual teacher models but also amalgamates their distinctive features,
such as zero-shot vision-language comprehension, detailed pixel-level
understanding, and open vocabulary segmentation capabilities. In pursuit of the
most hardware-efficient backbone, we evaluated numerous architectures in our
multi-teacher distillation pipeline using the same training recipe. This led to
the development of a novel architecture (E-RADIO) that exceeds the performance
of its predecessors and is at least 7x faster than the teacher models. Our
comprehensive benchmarking process covers downstream tasks including ImageNet
classification, ADE20k semantic segmentation, COCO object detection and
LLaVa-1.5 framework.
&lt;/p&gt;
&lt;p&gt;Code: https://github.com/NVlabs/RADIO
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ranzinger_M/0/1/0/all/0/1&quot;&gt;Mike Ranzinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heinrich_G/0/1/0/all/0/1&quot;&gt;Greg Heinrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1&quot;&gt;Jan Kautz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molchanov_P/0/1/0/all/0/1&quot;&gt;Pavlo Molchanov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06712">
<title>Separate-and-Enhance: Compositional Finetuning for Text2Image Diffusion Models. (arXiv:2312.06712v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06712</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite recent significant strides achieved by diffusion-based Text-to-Image
(T2I) models, current systems are still less capable of ensuring decent
compositional generation aligned with text prompts, particularly for the
multi-object generation. This work illuminates the fundamental reasons for such
misalignment, pinpointing issues related to low attention activation scores and
mask overlaps. While previous research efforts have individually tackled these
issues, we assert that a holistic approach is paramount. Thus, we propose two
novel objectives, the Separate loss and the Enhance loss, that reduce object
mask overlaps and maximize attention scores, respectively. Our method diverges
from conventional test-time-adaptation techniques, focusing on finetuning
critical parameters, which enhances scalability and generalizability.
Comprehensive evaluations demonstrate the superior performance of our model in
terms of image realism, text-image alignment, and adaptability, notably
outperforming prominent baselines. Ultimately, this research paves the way for
T2I diffusion models with enhanced compositional capacities and broader
applicability. The project webpage is available at
https://zpbao.github.io/projects/SepEn/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_Z/0/1/0/all/0/1&quot;&gt;Zhipeng Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yijun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1&quot;&gt;Krishna Kumar Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Xiong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hebert_M/0/1/0/all/0/1&quot;&gt;Martial Hebert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06713">
<title>TeTriRF: Temporal Tri-Plane Radiance Fields for Efficient Free-Viewpoint Video. (arXiv:2312.06713v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06713</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Fields (NeRF) revolutionize the realm of visual media by
providing photorealistic Free-Viewpoint Video (FVV) experiences, offering
viewers unparalleled immersion and interactivity. However, the technology&apos;s
significant storage requirements and the computational complexity involved in
generation and rendering currently limit its broader application. To close this
gap, this paper presents Temporal Tri-Plane Radiance Fields (TeTriRF), a novel
technology that significantly reduces the storage size for Free-Viewpoint Video
(FVV) while maintaining low-cost generation and rendering. TeTriRF introduces a
hybrid representation with tri-planes and voxel grids to support scaling up to
long-duration sequences and scenes with complex motions or rapid changes. We
propose a group training scheme tailored to achieving high training efficiency
and yielding temporally consistent, low-entropy scene representations.
Leveraging these properties of the representations, we introduce a compression
pipeline with off-the-shelf video codecs, achieving an order of magnitude less
storage size compared to the state-of-the-art. Our experiments demonstrate that
TeTriRF can achieve competitive quality with a higher compression rate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Minye Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zehao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kouros_G/0/1/0/all/0/1&quot;&gt;Georgios Kouros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1&quot;&gt;Tinne Tuytelaars&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06716">
<title>Deciphering &apos;What&apos; and &apos;Where&apos; Visual Pathways from Spectral Clustering of Layer-Distributed Neural Representations. (arXiv:2312.06716v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06716</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an approach for analyzing grouping information contained within a
neural network&apos;s activations, permitting extraction of spatial layout and
semantic segmentation from the behavior of large pre-trained vision models.
Unlike prior work, our method conducts a wholistic analysis of a network&apos;s
activation state, leveraging features from all layers and obviating the need to
guess which part of the model contains relevant information. Motivated by
classic spectral clustering, we formulate this analysis in terms of an
optimization objective involving a set of affinity matrices, each formed by
comparing features within a different layer. Solving this optimization problem
using gradient descent allows our technique to scale from single images to
dataset-level analysis, including, in the latter, both intra- and inter-image
relationships. Analyzing a pre-trained generative transformer provides insight
into the computational strategy learned by such models. Equating affinity with
key-query similarity across attention layers yields eigenvectors encoding scene
spatial layout, whereas defining affinity by value vector similarity yields
eigenvectors encoding object identity. This result suggests that key and query
vectors coordinate attentional information flow according to spatial proximity
(a `where&apos; pathway), while value vectors refine a semantic category
representation (a `what&apos; pathway).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yunis_D/0/1/0/all/0/1&quot;&gt;David Yunis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maire_M/0/1/0/all/0/1&quot;&gt;Michael Maire&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06719">
<title>SkyScenes: A Synthetic Dataset for Aerial Scene Understanding. (arXiv:2312.06719v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06719</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-world aerial scene understanding is limited by a lack of datasets that
contain densely annotated images curated under a diverse set of conditions. Due
to inherent challenges in obtaining such images in controlled real-world
settings, we present SkyScenes, a synthetic dataset of densely annotated aerial
images captured from Unmanned Aerial Vehicle (UAV) perspectives. We carefully
curate SkyScenes images from CARLA to comprehensively capture diversity across
layout (urban and rural maps), weather conditions, times of day, pitch angles
and altitudes with corresponding semantic, instance and depth annotations.
Through our experiments using SkyScenes, we show that (1) Models trained on
SkyScenes generalize well to different real-world scenarios, (2) augmenting
training on real images with SkyScenes data can improve real-world performance,
(3) controlled variations in SkyScenes can offer insights into how models
respond to changes in viewpoint conditions, and (4) incorporating additional
sensor modalities (depth) can improve aerial scene understanding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khose_S/0/1/0/all/0/1&quot;&gt;Sahil Khose&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pal_A/0/1/0/all/0/1&quot;&gt;Anisha Pal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1&quot;&gt;Aayushi Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deepanshi/0/1/0/all/0/1&quot;&gt;Deepanshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1&quot;&gt;Judy Hoffman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chattopadhyay_P/0/1/0/all/0/1&quot;&gt;Prithvijit Chattopadhyay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06720">
<title>Audio-Visual LLM for Video Understanding. (arXiv:2312.06720v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06720</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents Audio-Visual LLM, a Multimodal Large Language Model that
takes both visual and auditory inputs for holistic video understanding. A key
design is the modality-augmented training, which involves the integration of
modality-specific tokens engineered to activate the appropriate visual and/or
auditory encoder selectively. This mechanism is pivotal in enabling end-to-end
joint training with video data at different modalities, including visual-only,
audio-only, and audio-visual formats. Moreover, we introduce a high-quality
video instruction dataset, derived from GPT-4. This dataset allows Audio-Visual
LLM to adeptly process a variety of task-oriented video instructions, ranging
from multi-turn conversations and audio-visual narratives to complex reasoning
tasks. Extensive experiments demonstrate that Audio-Visual LLM impressively
achieves strong zero-shot results across a range of video understanding tasks.
For example, Audio-Visual LLM achieves an accuracy of 53.7% on MSRVTT-QA,
outperforming non-LLM-based InterVideo by 6.6% and LLM-based Valley by 4.4%,
respectively. Additionally, our Audio-Visual LLM also achieves competitive
performance on audio tasks (e.g., AudioCaps).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_F/0/1/0/all/0/1&quot;&gt;Fangxun Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Hao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1&quot;&gt;Cihang Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06721">
<title>Counterfactual World Modeling for Physical Dynamics Understanding. (arXiv:2312.06721v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06721</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to understand physical dynamics is essential to learning agents
acting in the world. This paper presents Counterfactual World Modeling (CWM), a
candidate pure vision foundational model for physical dynamics understanding.
CWM consists of three basic concepts. First, we propose a simple and powerful
temporally-factored masking policy for masked prediction of video data, which
encourages the model to learn disentangled representations of scene appearance
and dynamics. Second, as a result of the factoring, CWM is capable of
generating counterfactual next-frame predictions by manipulating a few patch
embeddings to exert meaningful control over scene dynamics. Third, the
counterfactual modeling capability enables the design of counterfactual queries
to extract vision structures similar to keypoints, optical flows, and
segmentations, which are useful for dynamics understanding. We show that
zero-shot readouts of these structures extracted by the counterfactual queries
attain competitive performance to prior methods on real-world datasets.
Finally, we demonstrate that CWM achieves state-of-the-art performance on the
challenging Physion benchmark for evaluating physical dynamics understanding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkatesh_R/0/1/0/all/0/1&quot;&gt;Rahul Venkatesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Honglin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feigelis_K/0/1/0/all/0/1&quot;&gt;Kevin Feigelis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jedoui_K/0/1/0/all/0/1&quot;&gt;Khaled Jedoui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kotar_K/0/1/0/all/0/1&quot;&gt;Klemen Kotar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Binder_F/0/1/0/all/0/1&quot;&gt;Felix Binder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1&quot;&gt;Wanhee Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sherry Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1&quot;&gt;Kevin A. Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1&quot;&gt;Judith E. Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamins_D/0/1/0/all/0/1&quot;&gt;Daniel L. K. Yamins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06722">
<title>EgoPlan-Bench: Benchmarking Egocentric Embodied Planning with Multimodal Large Language Models. (arXiv:2312.06722v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06722</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal Large Language Models (MLLMs), building upon the powerful Large
Language Models (LLMs) with exceptional reasoning and generalization
capability, have opened up new avenues for embodied task planning. MLLMs excel
in their ability to integrate diverse environmental inputs, such as real-time
task progress, visual observations, and open-form language instructions, which
are crucial for executable task planning. In this work, we introduce a
benchmark with human annotations, EgoPlan-Bench, to quantitatively investigate
the potential of MLLMs as embodied task planners in real-world scenarios. Our
benchmark is distinguished by realistic tasks derived from real-world videos, a
diverse set of actions involving interactions with hundreds of different
objects, and complex visual observations from varied environments. We evaluate
various open-source MLLMs, revealing that these models have not yet evolved
into embodied planning generalists (even GPT-4V). We further construct an
instruction-tuning dataset EgoPlan-IT from videos of human-object interactions,
to facilitate the learning of high-level task planning in intricate real-world
situations. The experiment results demonstrate that the model tuned on
EgoPlan-IT not only significantly improves performance on our benchmark, but
also effectively acts as embodied planner in simulations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yuying Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yixiao Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1&quot;&gt;Mingyu Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bohao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Ruifeng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1&quot;&gt;Ying Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xihui Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06723">
<title>Learning to See Low-Light Images via Feature Domain Adaptation. (arXiv:2312.06723v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06723</link>
<description rdf:parseType="Literal">&lt;p&gt;Raw low light image enhancement (LLIE) has achieved much better performance
than the sRGB domain enhancement methods due to the merits of raw data.
However, the ambiguity between noisy to clean and raw to sRGB mappings may
mislead the single-stage enhancement networks. The two-stage networks avoid
ambiguity by decoupling the two mappings but usually have large computing
complexity. To solve this problem, we propose a single-stage network empowered
by Feature Domain Adaptation (FDA) to decouple the denoising and color mapping
tasks in raw LLIE. The denoising encoder is supervised by the clean raw image,
and then the denoised features are adapted for the color mapping task by an FDA
module. We propose a Lineformer to serve as the FDA, which can well explore the
global and local correlations with fewer line buffers (friendly to the
line-based imaging process). During inference, the raw supervision branch is
removed. In this way, our network combines the advantage of a two-stage
enhancement process with the efficiency of single-stage inference. Experiments
on four benchmark datasets demonstrate that our method achieves
state-of-the-art performance with fewer computing costs (60\% FLOPs of the
two-stage method DNF). \textit{Our codes will be released after the acceptance
of this work.}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qirui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+qihua_c/0/1/0/all/0/1&quot;&gt;cheng qihua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_H/0/1/0/all/0/1&quot;&gt;Huanjing Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Le Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yihao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jingyu Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06725">
<title>EpiDiff: Enhancing Multi-View Synthesis via Localized Epipolar-Constrained Diffusion. (arXiv:2312.06725v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06725</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating multiview images from a single view facilitates the rapid
generation of a 3D mesh conditioned on a single image. Recent methods that
introduce 3D global representation into diffusion models have shown the
potential to generate consistent multiviews, but they have reduced generation
speed and face challenges in maintaining generalizability and quality. To
address this issue, we propose EpiDiff, a localized interactive multiview
diffusion model. At the core of the proposed approach is to insert a
lightweight epipolar attention block into the frozen diffusion model,
leveraging epipolar constraints to enable cross-view interaction among feature
maps of neighboring views. The newly initialized 3D modeling module preserves
the original feature distribution of the diffusion model, exhibiting
compatibility with a variety of base diffusion models. Experiments show that
EpiDiff generates 16 multiview images in just 12 seconds, and it surpasses
previous methods in quality evaluation metrics, including PSNR, SSIM and LPIPS.
Additionally, EpiDiff can generate a more diverse distribution of views,
improving the reconstruction quality from generated multiviews. Please see our
project page at https://huanngzh.github.io/EpiDiff/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zehuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1&quot;&gt;Hao Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1&quot;&gt;Junting Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaohui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yangguang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yan-Pei Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1&quot;&gt;Ding Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1&quot;&gt;Bo Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_L/0/1/0/all/0/1&quot;&gt;Lu Sheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06726">
<title>Compress &amp; Align: Curating Image-Text Data with Human Knowledge. (arXiv:2312.06726v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06726</link>
<description rdf:parseType="Literal">&lt;p&gt;The massive growth of image-text data through web crawling inherently
presents the challenge of variability in data quality. This paper introduces a
novel algorithm, rooted in human knowledge, to compress this vast corpus of
web-crawled image-text datasets to a compact and high-quality form. Our method
unfolds in three major steps. First, we collect an image-text dataset, wherein
each image is associated with multiple captions sourced from diverse origins.
Then, to systemically capture human preferences regarding the best caption
paired with each image, we establish a comprehensive set of both subjective and
objective criteria for critically guiding the alignment assessment from
labelers. Lastly, we train a reward model on the annotated dataset to
internalize the nuanced human understanding of image-text alignment. The
resulting reward model thus can act as a human-like referee to filter
misaligned/low-quality image-text pairs. Extensive experiments demonstrate that
we are able to secure (or even improve) model performance by compressing the
image-text datasets up to ~90%. An impressive example is that, by aggressively
reducing the total training sample from 130M to 15.5M (e.g., ~9x smaller), our
BLIP-B/16 models still consistently show superior performance compared with the
full-size-dataset counterpart on image-text retrieval (Flickr30K, COCO) by
~2.5% in Recall@1, and on image-captioning (Nocaps, COCO) by ~10.0% in CIDEr
and ~2.7% in SPICE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_F/0/1/0/all/0/1&quot;&gt;Fangxun Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1&quot;&gt;Sucheng Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1&quot;&gt;Bingchen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Hao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1&quot;&gt;Cihang Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06728">
<title>A Multimodal Dataset and Benchmark for Radio Galaxy and Infrared Host Detection. (arXiv:2312.06728v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06728</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel multimodal dataset developed by expert astronomers to
automate the detection and localisation of multi-component extended radio
galaxies and their corresponding infrared hosts. The dataset comprises 4,155
instances of galaxies in 2,800 images with both radio and infrared modalities.
Each instance contains information on the extended radio galaxy class, its
corresponding bounding box that encompasses all of its components, pixel-level
segmentation mask, and the position of its corresponding infrared host galaxy.
Our dataset is the first publicly accessible dataset that includes images from
a highly sensitive radio telescope, infrared satellite, and instance-level
annotations for their identification. We benchmark several object detection
algorithms on the dataset and propose a novel multimodal approach to identify
radio galaxies and the positions of infrared hosts simultaneously.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1&quot;&gt;Nikhel Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayder_Z/0/1/0/all/0/1&quot;&gt;Zeeshan Hayder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Norris_R/0/1/0/all/0/1&quot;&gt;Ray P. Norris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hyunh_M/0/1/0/all/0/1&quot;&gt;Minh Hyunh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1&quot;&gt;Lars Petersson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06729">
<title>RGNet: A Unified Retrieval and Grounding Network for Long Videos. (arXiv:2312.06729v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06729</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel end-to-end method for long-form video temporal grounding
to locate specific moments described by natural language queries. Prior
long-video methods for this task typically contain two stages: proposal
selection and grounding regression. However, the proposal selection of these
methods is disjoint from the grounding network and is not trained end-to-end,
which limits the effectiveness of these methods. Moreover, these methods
operate uniformly over the entire temporal window, which is suboptimal given
redundant and irrelevant features in long videos. In contrast to these prior
approaches, we introduce RGNet, a unified network designed for jointly
selecting proposals from hour-long videos and locating moments specified by
natural language queries within them. To achieve this, we redefine proposal
selection as a video-text retrieval task, i.e., retrieving the correct
candidate videos given a text query. The core component of RGNet is a unified
cross-modal RG-Encoder that bridges the two stages with shared features and
mutual optimization. The encoder strategically focuses on relevant time frames
using a sparse sampling technique. RGNet outperforms previous methods,
demonstrating state-of-the-art performance on long video temporal grounding
datasets MAD and Ego4D. The code is released at
https://github.com/Tanveer81/RGNet
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hannan_T/0/1/0/all/0/1&quot;&gt;Tanveer Hannan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1&quot;&gt;Md Mohaiminul Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seidl_T/0/1/0/all/0/1&quot;&gt;Thomas Seidl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1&quot;&gt;Gedas Bertasius&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06731">
<title>Genixer: Empowering Multimodal Large Language Models as a Powerful Data Generator. (arXiv:2312.06731v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06731</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) excel in understanding human instructions,
driving the development of Multimodal LLMs (MLLMs) with instruction tuning.
However, acquiring high-quality multimodal instruction tuning data poses a
significant challenge. Previous approaches relying on GPT-4 for data generation
proved expensive and exhibited unsatisfactory performance for certain tasks. To
solve this, we present Genixer, an innovative data generation pipeline
producing high-quality multimodal instruction tuning data for various tasks.
Genixer collects datasets for ten prevalent multimodal tasks and designs
instruction templates to transform these datasets into instruction-tuning data.
It then trains pretrained MLLMs to generate task-specific instruction data and
proposes an effective data filtering strategy to ensure high quality. To
evaluate Genixer, a base MLLM model, Kakapo, is built and achieves SoTA
performance in image captioning and visual question answering (VQA) tasks
across multiple datasets. Experimental results show that filtered data from
Genixer continually improves Kakapo for image captioning and VQA tasks. For the
SoTA Shikra MLLM model on the image-region-related tasks, e.g., region caption
and detection, Genixer also successfully generates corresponding data and
improves its performance. Genixer opens avenues for generating high-quality
multimodal instruction data for diverse tasks, enabling innovative applications
across domains. The code and models will be released soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Henry Hengyuan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1&quot;&gt;Pan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1&quot;&gt;Mike Zheng Shou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06733">
<title>TULIP: Transformer for Upsampling of LiDAR Point Cloud. (arXiv:2312.06733v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06733</link>
<description rdf:parseType="Literal">&lt;p&gt;LiDAR Upsampling is a challenging task for the perception systems of robots
and autonomous vehicles, due to the sparse and irregular structure of
large-scale scene contexts. Recent works propose to solve this problem by
converting LiDAR data from 3D Euclidean space into an image super-resolution
problem in 2D image space. Although their methods can generate high-resolution
range images with fine-grained details, the resulting 3D point clouds often
blur out details and predict invalid points. In this paper, we propose TULIP, a
new method to reconstruct high-resolution LiDAR point clouds from
low-resolution LiDAR input. We also follow a range image-based approach but
specifically modify the patch and window geometries of a Swin-Transformer-based
network to better fit the characteristics of range images. We conducted several
experiments on three different public real-world and simulated datasets. TULIP
outperforms state-of-the-art methods in all relevant metrics and generates
robust and more realistic point clouds than prior works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfreundschuh_P/0/1/0/all/0/1&quot;&gt;Patrick Pfreundschuh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siegwart_R/0/1/0/all/0/1&quot;&gt;Roland Siegwart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hutter_M/0/1/0/all/0/1&quot;&gt;Marco Hutter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moghadam_P/0/1/0/all/0/1&quot;&gt;Peyman Moghadam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patil_V/0/1/0/all/0/1&quot;&gt;Vaishakh Patil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06734">
<title>DiffCast: A Unified Framework via Residual Diffusion for Precipitation Nowcasting. (arXiv:2312.06734v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06734</link>
<description rdf:parseType="Literal">&lt;p&gt;Precipitation nowcasting is an important spatio-temporal prediction task to
predict the radar echoes sequences based on current observations, which can
serve both meteorological science and smart city applications. Due to the
chaotic evolution nature of the precipitation systems, it is a very challenging
problem. Previous studies address the problem either from the perspectives of
deterministic modeling or probabilistic modeling. However, their predictions
suffer from the blurry, high-value echoes fading away and position inaccurate
issues. The root reason of these issues is that the chaotic evolutionary
precipitation systems are not appropriately modeled. Inspired by the nature of
the systems, we propose to decompose and model them from the perspective of
global deterministic motion and local stochastic variations with residual
mechanism. A unified and flexible framework that can equip any type of
spatio-temporal models is proposed based on residual diffusion, which
effectively tackles the shortcomings of previous methods. Extensive
experimental results on four publicly available radar datasets demonstrate the
effectiveness and superiority of the proposed framework, compared to
state-of-the-art techniques. Our code will be made publicly available soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1&quot;&gt;Demin Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xutao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1&quot;&gt;Yunming Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Baoquan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1&quot;&gt;Chuyao Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_K/0/1/0/all/0/1&quot;&gt;Kuai Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xunlai Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06736">
<title>SqueezeSAM: User friendly mobile interactive segmentation. (arXiv:2312.06736v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06736</link>
<description rdf:parseType="Literal">&lt;p&gt;Segment Anything Model (SAM) is a foundation model for interactive
segmentation, and it has catalyzed major advances in generative AI,
computational photography, and medical imaging. This model takes in an
arbitrary user input and provides segmentation masks of the corresponding
objects. It is our goal to develop a version of SAM that is appropriate for use
in a photography app. The original SAM model has a few challenges in this
setting. First, original SAM a 600 million parameter based on ViT-H, and its
high computational cost and large model size that are not suitable for todays
mobile hardware. We address this by proposing the SqueezeSAM model
architecture, which is 50x faster and 100x smaller than SAM. Next, when a user
takes a photo on their phone, it might not occur to them to click on the image
and get a mask. Our solution is to use salient object detection to generate the
first few clicks. This produces an initial segmentation mask that the user can
interactively edit. Finally, when a user clicks on an object, they typically
expect all related pieces of the object to be segmented. For instance, if a
user clicks on a person t-shirt in a photo, they expect the whole person to be
segmented, but SAM typically segments just the t-shirt. We address this with a
new data augmentation scheme, and the end result is that if the user clicks on
a person holding a basketball, the person and the basketball are all segmented
together.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varadarajan_B/0/1/0/all/0/1&quot;&gt;Balakrishnan Varadarajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soran_B/0/1/0/all/0/1&quot;&gt;Bilge Soran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iandola_F/0/1/0/all/0/1&quot;&gt;Forrest Iandola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1&quot;&gt;Yunyang Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Lemeng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Chenchen Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnamoorthi_R/0/1/0/all/0/1&quot;&gt;Raghuraman Krishnamoorthi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1&quot;&gt;Vikas Chandra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06738">
<title>InstructAny2Pix: Flexible Visual Editing via Multimodal Instruction Following. (arXiv:2312.06738v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06738</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to provide fine-grained control for generating and editing visual
imagery has profound implications for computer vision and its applications.
Previous works have explored extending controllability in two directions:
instruction tuning with text-based prompts and multi-modal conditioning.
However, these works make one or more unnatural assumptions on the number
and/or type of modality inputs used to express controllability. We propose
InstructAny2Pix, a flexible multi-modal instruction-following system that
enables users to edit an input image using instructions involving audio,
images, and text. InstructAny2Pix consists of three building blocks that
facilitate this capability: a multi-modal encoder that encodes different
modalities such as images and audio into a unified latent space, a diffusion
model that learns to decode representations in this latent space into images,
and a multi-modal LLM that can understand instructions involving multiple
images and audio pieces and generate a conditional embedding of the desired
output, which can be used by the diffusion decoder. Additionally, to facilitate
training efficiency and improve generation quality, we include an additional
refinement prior module that enhances the visual quality of LLM outputs. These
designs are critical to the performance of our system. We demonstrate that our
system can perform a series of novel instruction-guided editing tasks. The code
is available at https://github.com/jacklishufan/InstructAny2Pix.git
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shufan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_H/0/1/0/all/0/1&quot;&gt;Harkanwar Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1&quot;&gt;Aditya Grover&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06739">
<title>SmartEdit: Exploring Complex Instruction-based Image Editing with Multimodal Large Language Models. (arXiv:2312.06739v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06739</link>
<description rdf:parseType="Literal">&lt;p&gt;Current instruction-based editing methods, such as InstructPix2Pix, often
fail to produce satisfactory results in complex scenarios due to their
dependence on the simple CLIP text encoder in diffusion models. To rectify
this, this paper introduces SmartEdit, a novel approach to instruction-based
image editing that leverages Multimodal Large Language Models (MLLMs) to
enhance their understanding and reasoning capabilities. However, direct
integration of these elements still faces challenges in situations requiring
complex reasoning. To mitigate this, we propose a Bidirectional Interaction
Module that enables comprehensive bidirectional information interactions
between the input image and the MLLM output. During training, we initially
incorporate perception data to boost the perception and understanding
capabilities of diffusion models. Subsequently, we demonstrate that a small
amount of complex instruction editing data can effectively stimulate
SmartEdit&apos;s editing capabilities for more complex instructions. We further
construct a new evaluation dataset, Reason-Edit, specifically tailored for
complex instruction-based image editing. Both quantitative and qualitative
results on this evaluation dataset indicate that our SmartEdit surpasses
previous methods, paving the way for the practical application of complex
instruction-based image editing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yuzhou Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1&quot;&gt;Liangbin Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xintao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1&quot;&gt;Ziyang Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cun_X/0/1/0/all/0/1&quot;&gt;Xiaodong Cun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yixiao Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jiantao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1&quot;&gt;Chao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1&quot;&gt;Rui Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruimao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1&quot;&gt;Ying Shan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06740">
<title>MonoNPHM: Dynamic Head Reconstruction from Monocular Videos. (arXiv:2312.06740v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06740</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Monocular Neural Parametric Head Models (MonoNPHM) for dynamic 3D
head reconstructions from monocular RGB videos. To this end, we propose a
latent appearance space that parameterizes a texture field on top of a neural
parametric model. We constrain predicted color values to be correlated with the
underlying geometry such that gradients from RGB effectively influence latent
geometry codes during inverse rendering. To increase the representational
capacity of our expression space, we augment our backward deformation field
with hyper-dimensions, thus improving color and geometry representation in
topologically challenging expressions. Using MonoNPHM as a learned prior, we
approach the task of 3D head reconstruction using signed distance field based
volumetric rendering. By numerically inverting our backward deformation field,
we incorporated a landmark loss using facial anchor points that are closely
tied to our canonical geometry representation. To evaluate the task of dynamic
face reconstruction from monocular RGB videos we record 20 challenging Kinect
sequences under casual conditions. MonoNPHM outperforms all baselines with a
significant margin, and makes an important step towards easily accessible
neural parametric face models through RGB tracking.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giebenhain_S/0/1/0/all/0/1&quot;&gt;Simon Giebenhain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirschstein_T/0/1/0/all/0/1&quot;&gt;Tobias Kirschstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Georgopoulos_M/0/1/0/all/0/1&quot;&gt;Markos Georgopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Runz_M/0/1/0/all/0/1&quot;&gt;Martin R&amp;#xfc;nz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agapito_L/0/1/0/all/0/1&quot;&gt;Lourdes Agapito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1&quot;&gt;Matthias Nie&amp;#xdf;ner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06741">
<title>Gaussian Splatting SLAM. (arXiv:2312.06741v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06741</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the first application of 3D Gaussian Splatting to incremental 3D
reconstruction using a single moving monocular or RGB-D camera. Our
Simultaneous Localisation and Mapping (SLAM) method, which runs live at 3fps,
utilises Gaussians as the only 3D representation, unifying the required
representation for accurate, efficient tracking, mapping, and high-quality
rendering. Several innovations are required to continuously reconstruct 3D
scenes with high fidelity from a live camera. First, to move beyond the
original 3DGS algorithm, which requires accurate poses from an offline
Structure from Motion (SfM) system, we formulate camera tracking for 3DGS using
direct optimisation against the 3D Gaussians, and show that this enables fast
and robust tracking with a wide basin of convergence. Second, by utilising the
explicit nature of the Gaussians, we introduce geometric verification and
regularisation to handle the ambiguities occurring in incremental 3D dense
reconstruction. Finally, we introduce a full SLAM system which not only
achieves state-of-the-art results in novel view synthesis and trajectory
estimation, but also reconstruction of tiny and even transparent objects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matsuki_H/0/1/0/all/0/1&quot;&gt;Hidenobu Matsuki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murai_R/0/1/0/all/0/1&quot;&gt;Riku Murai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kelly_P/0/1/0/all/0/1&quot;&gt;Paul H.J. Kelly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davison_A/0/1/0/all/0/1&quot;&gt;Andrew J. Davison&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06742">
<title>Honeybee: Locality-enhanced Projector for Multimodal LLM. (arXiv:2312.06742v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06742</link>
<description rdf:parseType="Literal">&lt;p&gt;In Multimodal Large Language Models (MLLMs), a visual projector plays a
crucial role in bridging pre-trained vision encoders with LLMs, enabling
profound visual understanding while harnessing the LLMs&apos; robust capabilities.
Despite the importance of the visual projector, it has been relatively less
explored. In this study, we first identify two essential projector properties:
(i) flexibility in managing the number of visual tokens, crucial for MLLMs&apos;
overall efficiency, and (ii) preservation of local context from visual
features, vital for spatial understanding. Based on these findings, we propose
a novel projector design that is both flexible and locality-enhanced,
effectively satisfying the two desirable properties. Additionally, we present
comprehensive strategies to effectively utilize multiple and multifaceted
instruction datasets. Through extensive experiments, we examine the impact of
individual design choices. Finally, our proposed MLLM, Honeybee, remarkably
outperforms previous state-of-the-art methods across various benchmarks,
including MME, MMBench, SEED-Bench, and LLaVA-Bench, achieving significantly
higher efficiency. Code and models are available at
https://github.com/kakaobrain/honeybee.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cha_J/0/1/0/all/0/1&quot;&gt;Junbum Cha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1&quot;&gt;Wooyoung Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mun_J/0/1/0/all/0/1&quot;&gt;Jonghwan Mun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roh_B/0/1/0/all/0/1&quot;&gt;Byungseok Roh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06797">
<title>Improving the Robustness of 3D Human Pose Estimation: A Benchmark and Learning from Noisy Input. (arXiv:2312.06797v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06797</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the promising performance of current 3D human pose estimation
techniques, understanding and enhancing their generalization on challenging
in-the-wild videos remain an open problem. In this work, we focus on the
robustness of 2D-to-3D pose lifters. To this end, we develop two benchmark
datasets, namely Human3.6M-C and HumanEva-I-C, to examine the robustness of
video-based 3D pose lifters to a wide range of common video corruptions
including temporary occlusion, motion blur, and pixel-level noise. We observe
the poor generalization of state-of-the-art 3D pose lifters in the presence of
corruption and establish two techniques to tackle this issue. First, we
introduce Temporal Additive Gaussian Noise (TAGN) as a simple yet effective 2D
input pose data augmentation. Additionally, to incorporate the confidence
scores output by the 2D pose detectors, we design a confidence-aware
convolution (CA-Conv) block. Extensively tested on corrupted videos, the
proposed strategies consistently boost the robustness of 3D pose lifters and
serve as new baselines for future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1&quot;&gt;Trung-Hieu Hoang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zehni_M/0/1/0/all/0/1&quot;&gt;Mona Zehni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phan_H/0/1/0/all/0/1&quot;&gt;Huy Phan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Do_M/0/1/0/all/0/1&quot;&gt;Minh N. Do&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06799">
<title>Densify Your Labels: Unsupervised Clustering with Bipartite Matching for Weakly Supervised Point Cloud Segmentation. (arXiv:2312.06799v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06799</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a weakly supervised semantic segmentation method for point clouds
that predicts &quot;per-point&quot; labels from just &quot;whole-scene&quot; annotations while
achieving the performance of recent fully supervised approaches. Our core idea
is to propagate the scene-level labels to each point in the point cloud by
creating pseudo labels in a conservative way. Specifically, we over-segment
point cloud features via unsupervised clustering and associate scene-level
labels with clusters through bipartite matching, thus propagating scene labels
only to the most relevant clusters, leaving the rest to be guided solely via
unsupervised clustering. We empirically demonstrate that over-segmentation and
bipartite assignment plays a crucial role. We evaluate our method on ScanNet
and S3DIS datasets, outperforming state of the art, and demonstrate that we can
achieve results comparable to fully supervised methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1&quot;&gt;Shaobo Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_J/0/1/0/all/0/1&quot;&gt;Jun Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kania_K/0/1/0/all/0/1&quot;&gt;Kacper Kania&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1&quot;&gt;Leyuan Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1&quot;&gt;Andrea Tagliasacchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1&quot;&gt;Kwang Moo Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Weiwei Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06801">
<title>ADOD: Adaptive Domain-Aware Object Detection with Residual Attention for Underwater Environments. (arXiv:2312.06801v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06801</link>
<description rdf:parseType="Literal">&lt;p&gt;This research presents ADOD, a novel approach to address domain
generalization in underwater object detection. Our method enhances the model&apos;s
ability to generalize across diverse and unseen domains, ensuring robustness in
various underwater environments. The first key contribution is Residual
Attention YOLOv3, a novel variant of the YOLOv3 framework empowered by residual
attention modules. These modules enable the model to focus on informative
features while suppressing background noise, leading to improved detection
accuracy and adaptability to different domains. The second contribution is the
attention-based domain classification module, vital during training. This
module helps the model identify domain-specific information, facilitating the
learning of domain-invariant features. Consequently, ADOD can generalize
effectively to underwater environments with distinct visual characteristics.
Extensive experiments on diverse underwater datasets demonstrate ADOD&apos;s
superior performance compared to state-of-the-art domain generalization
methods, particularly in challenging scenarios. The proposed model achieves
exceptional detection performance in both seen and unseen domains, showcasing
its effectiveness in handling domain shifts in underwater object detection
tasks. ADOD represents a significant advancement in adaptive object detection,
providing a promising solution for real-world applications in underwater
environments. With the prevalence of domain shifts in such settings, the
model&apos;s strong generalization ability becomes a valuable asset for practical
underwater surveillance and marine research endeavors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saoud_L/0/1/0/all/0/1&quot;&gt;Lyes Saad Saoud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1&quot;&gt;Zhenwei Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sultan_A/0/1/0/all/0/1&quot;&gt;Atif Sultan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seneviratne_L/0/1/0/all/0/1&quot;&gt;Lakmal Seneviratne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hussain_I/0/1/0/all/0/1&quot;&gt;Irfan Hussain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06829">
<title>Encoding Surgical Videos as Latent Spatiotemporal Graphs for Object and Anatomy-Driven Reasoning. (arXiv:2312.06829v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06829</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, spatiotemporal graphs have emerged as a concise and elegant manner
of representing video clips in an object-centric fashion, and have shown to be
useful for downstream tasks such as action recognition. In this work, we
investigate the use of latent spatiotemporal graphs to represent a surgical
video in terms of the constituent anatomical structures and tools and their
evolving properties over time. To build the graphs, we first predict frame-wise
graphs using a pre-trained model, then add temporal edges between nodes based
on spatial coherence and visual and semantic similarity. Unlike previous
approaches, we incorporate long-term temporal edges in our graphs to better
model the evolution of the surgical scene and increase robustness to temporary
occlusions. We also introduce a novel graph-editing module that incorporates
prior knowledge and temporal coherence to correct errors in the graph, enabling
improved downstream task performance. Using our graph representations, we
evaluate two downstream tasks, critical view of safety prediction and surgical
phase recognition, obtaining strong results that demonstrate the quality and
flexibility of the learned representations. Code is available at
github.com/CAMMA-public/SurgLatentGraph.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murali_A/0/1/0/all/0/1&quot;&gt;Aditya Murali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alapatt_D/0/1/0/all/0/1&quot;&gt;Deepak Alapatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mascagni_P/0/1/0/all/0/1&quot;&gt;Pietro Mascagni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vardazaryan_A/0/1/0/all/0/1&quot;&gt;Armine Vardazaryan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_A/0/1/0/all/0/1&quot;&gt;Alain Garcia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okamoto_N/0/1/0/all/0/1&quot;&gt;Nariaki Okamoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mutter_D/0/1/0/all/0/1&quot;&gt;Didier Mutter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1&quot;&gt;Nicolas Padoy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06833">
<title>The unreasonable effectiveness of AI CADe polyp detectors to generalize to new countries. (arXiv:2312.06833v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.06833</link>
<description rdf:parseType="Literal">&lt;p&gt;$\textbf{Background and aims}$: Artificial Intelligence (AI) Computer-Aided
Detection (CADe) is commonly used for polyp detection, but data seen in
clinical settings can differ from model training. Few studies evaluate how well
CADe detectors perform on colonoscopies from countries not seen during
training, and none are able to evaluate performance without collecting
expensive and time-intensive labels.
&lt;/p&gt;
&lt;p&gt;$\textbf{Methods}$: We trained a CADe polyp detector on Israeli colonoscopy
videos (5004 videos, 1106 hours) and evaluated on Japanese videos (354 videos,
128 hours) by measuring the True Positive Rate (TPR) versus false alarms per
minute (FAPM). We introduce a colonoscopy dissimilarity measure called &quot;MAsked
mediCal Embedding Distance&quot; (MACE) to quantify differences between
colonoscopies, without labels. We evaluated CADe on all Japan videos and on
those with the highest MACE.
&lt;/p&gt;
&lt;p&gt;$\textbf{Results}$: MACE correctly quantifies that narrow-band imaging (NBI)
and chromoendoscopy (CE) frames are less similar to Israel data than Japan
whitelight (bootstrapped z-test, |z| &amp;gt; 690, p &amp;lt; $10^{-8}$ for both). Despite
differences in the data, CADe performance on Japan colonoscopies was
non-inferior to Israel ones without additional training (TPR at 0.5 FAPM: 0.957
and 0.972 for Israel and Japan; TPR at 1.0 FAPM: 0.972 and 0.989 for Israel and
Japan; superiority test t &amp;gt; 45.2, p &amp;lt; $10^{-8}$). Despite not being trained on
NBI or CE, TPR on those subsets were non-inferior to Japan overall
(non-inferiority test t &amp;gt; 47.3, p &amp;lt; $10^{-8}$, $\delta$ = 1.5% for both).
&lt;/p&gt;
&lt;p&gt;$\textbf{Conclusion}$: Differences that prevent CADe detectors from
performing well in non-medical settings do not degrade the performance of our
AI CADe polyp detector when applied to data from a new country. MACE can help
medical AI models internationalize by identifying the most &quot;dissimilar&quot; data on
which to evaluate models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shor_J/0/1/0/all/0/1&quot;&gt;Joel Shor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamano_H/0/1/0/all/0/1&quot;&gt;Hiro-o Yamano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsurumaru_D/0/1/0/all/0/1&quot;&gt;Daisuke Tsurumaru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Intrator_Y/0/1/0/all/0/1&quot;&gt;Yotami Intrator&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kayama_H/0/1/0/all/0/1&quot;&gt;Hiroki Kayama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ledsam_J/0/1/0/all/0/1&quot;&gt;Joe Ledsam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamabe_A/0/1/0/all/0/1&quot;&gt;Atsushi Hamabe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ando_K/0/1/0/all/0/1&quot;&gt;Koji Ando&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ota_M/0/1/0/all/0/1&quot;&gt;Mitsuhiko Ota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ogino_H/0/1/0/all/0/1&quot;&gt;Haruei Ogino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakase_H/0/1/0/all/0/1&quot;&gt;Hiroshi Nakase&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kobayashi_K/0/1/0/all/0/1&quot;&gt;Kaho Kobayashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oki_E/0/1/0/all/0/1&quot;&gt;Eiji Oki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldenberg_R/0/1/0/all/0/1&quot;&gt;Roman Goldenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rivlin_E/0/1/0/all/0/1&quot;&gt;Ehud Rivlin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takemasa_I/0/1/0/all/0/1&quot;&gt;Ichiro Takemasa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06850">
<title>NDELS: A Novel Approach for Nighttime Dehazing, Low-Light Enhancement, and Light Suppression. (arXiv:2312.06850v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06850</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper tackles the intricate challenge of improving the quality of
nighttime images under hazy and low-light conditions. Overcoming issues
including nonuniform illumination glows, texture blurring, glow effects, color
distortion, noise disturbance, and overall, low light have proven daunting.
Despite the inherent difficulties, this paper introduces a pioneering solution
named Nighttime Dehazing, Low-Light Enhancement, and Light Suppression (NDELS).
NDELS utilizes a unique network that combines three essential processes to
enhance visibility, brighten low-light regions, and effectively suppress glare
from bright light sources. In contrast to limited progress in nighttime
dehazing, unlike its daytime counterpart, NDELS presents a comprehensive and
innovative approach. The efficacy of NDELS is rigorously validated through
extensive comparisons with eight state-of-the-art algorithms across four
diverse datasets. Experimental results showcase the superior performance of our
method, demonstrating its outperformance in terms of overall image quality,
including color and edge enhancement. Quantitative (PSNR, SSIM) and qualitative
metrics (CLIPIQA, MANIQA, TRES), measure these results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernabel_S/0/1/0/all/0/1&quot;&gt;Silvano A. Bernabel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agaian_S/0/1/0/all/0/1&quot;&gt;Sos S. Agaian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06865">
<title>Keypoint-based Stereophotoclinometry for Characterizing and Navigating Small Bodies: A Factor Graph Approach. (arXiv:2312.06865v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06865</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes the incorporation of techniques from
stereophotoclinometry (SPC) into a keypoint-based structure-from-motion (SfM)
system to estimate the surface normal and albedo at detected landmarks to
improve autonomous surface and shape characterization of small celestial bodies
from in-situ imagery. In contrast to the current state-of-the-practice method
for small body shape reconstruction, i.e., SPC, which relies on
human-in-the-loop verification and high-fidelity a priori information to
achieve accurate results, we forego the expensive maplet estimation step and
instead leverage dense keypoint measurements and correspondences from an
autonomous keypoint detection and matching method based on deep learning to
provide the necessary photogrammetric constraints. Moreover, we develop a
factor graph-based approach allowing for simultaneous optimization of the
spacecraft&apos;s pose, landmark positions, Sun-relative direction, and surface
normals and albedos via fusion of Sun sensor measurements and image keypoint
measurements. The proposed framework is validated on real imagery of the
Cornelia crater on Asteroid 4 Vesta, along with pose estimation and mapping
comparison against an SPC reconstruction, where we demonstrate precise
alignment to the SPC solution without relying on any a priori camera pose and
topography information or humans-in-the-loop
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Driver_T/0/1/0/all/0/1&quot;&gt;Travis Driver&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaughan_A/0/1/0/all/0/1&quot;&gt;Andrew Vaughan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yang Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ansar_A/0/1/0/all/0/1&quot;&gt;Adnan Ansar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christian_J/0/1/0/all/0/1&quot;&gt;John Christian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsiotras_P/0/1/0/all/0/1&quot;&gt;Panagiotis Tsiotras&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06868">
<title>RAFIC: Retrieval-Augmented Few-shot Image Classification. (arXiv:2312.06868v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06868</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot image classification is the task of classifying unseen images to one
of N mutually exclusive classes, using only a small number of training examples
for each class. The limited availability of these examples (denoted as K)
presents a significant challenge to classification accuracy in some cases. To
address this, we have developed a method for augmenting the set of K with an
addition set of A retrieved images. We call this system Retrieval-Augmented
Few-shot Image Classification (RAFIC). Through a series of experiments, we
demonstrate that RAFIC markedly improves performance of few-shot image
classification across two challenging datasets. RAFIC consists of two main
components: (a) a retrieval component which uses CLIP, LAION-5B, and faiss, in
order to efficiently retrieve images similar to the supplied images, and (b)
retrieval meta-learning, which learns to judiciously utilize the retrieved
images. Code and data is available at github.com/amirziai/rafic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1&quot;&gt;Hangfei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_L/0/1/0/all/0/1&quot;&gt;Li Miao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziai_A/0/1/0/all/0/1&quot;&gt;Amir Ziai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06886">
<title>Relightful Harmonization: Lighting-aware Portrait Background Replacement. (arXiv:2312.06886v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06886</link>
<description rdf:parseType="Literal">&lt;p&gt;Portrait harmonization aims to composite a subject into a new background,
adjusting its lighting and color to ensure harmony with the background scene.
Existing harmonization techniques often only focus on adjusting the global
color and brightness of the foreground and ignore crucial illumination cues
from the background such as apparent lighting direction, leading to unrealistic
compositions. We introduce Relightful Harmonization, a lighting-aware diffusion
model designed to seamlessly harmonize sophisticated lighting effect for the
foreground portrait using any background image. Our approach unfolds in three
stages. First, we introduce a lighting representation module that allows our
diffusion model to encode lighting information from target image background.
Second, we introduce an alignment network that aligns lighting features learned
from image background with lighting features learned from panorama environment
maps, which is a complete representation for scene illumination. Last, to
further boost the photorealism of the proposed method, we introduce a novel
data simulation pipeline that generates synthetic training pairs from a diverse
range of natural images, which are used to refine the model. Our method
outperforms existing benchmarks in visual fidelity and lighting coherence,
showing superior generalization in real-world testing scenarios, highlighting
its versatility and practicality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1&quot;&gt;Mengwei Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1&quot;&gt;Wei Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1&quot;&gt;Jae Shin Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_Z/0/1/0/all/0/1&quot;&gt;Zhixin Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1&quot;&gt;HyunJoon Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gerig_G/0/1/0/all/0/1&quot;&gt;Guido Gerig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;He Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06892">
<title>VitalLens: Take A Vital Selfie. (arXiv:2312.06892v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06892</link>
<description rdf:parseType="Literal">&lt;p&gt;This report introduces VitalLens, an app that estimates vital signs such as
heart rate and respiration rate from selfie video in real time. VitalLens uses
a computer vision model trained on a diverse dataset of video and physiological
sensor data. We benchmark performance on several diverse datasets, including
VV-Medium, which consists of 289 unique participants. VitalLens outperforms
several existing methods including POS and MTTS-CAN on all datasets while
maintaining a fast inference speed. On VV-Medium, VitalLens achieves absolute
errors of 0.71 bpm for heart rate estimation, and 0.76 rpm for respiratory rate
estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rouast_P/0/1/0/all/0/1&quot;&gt;Philipp V. Rouast&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06899">
<title>LoRA-Enhanced Distillation on Guided Diffusion Models. (arXiv:2312.06899v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06899</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models, such as Stable Diffusion (SD), offer the ability to
generate high-resolution images with diverse features, but they come at a
significant computational and memory cost. In classifier-free guided diffusion
models, prolonged inference times are attributed to the necessity of computing
two separate diffusion models at each denoising step. Recent work has shown
promise in improving inference time through distillation techniques, teaching
the model to perform similar denoising steps with reduced computations.
However, the application of distillation introduces additional memory overhead
to these already resource-intensive diffusion models, making it less practical.
&lt;/p&gt;
&lt;p&gt;To address these challenges, our research explores a novel approach that
combines Low-Rank Adaptation (LoRA) with model distillation to efficiently
compress diffusion models. This approach not only reduces inference time but
also mitigates memory overhead, and notably decreases memory consumption even
before applying distillation. The results are remarkable, featuring a
significant reduction in inference time due to the distillation process and a
substantial 50% reduction in memory consumption. Our examination of the
generated images underscores that the incorporation of LoRA-enhanced
distillation maintains image quality and alignment with the provided prompts.
In summary, while conventional distillation tends to increase memory
consumption, LoRA-enhanced distillation offers optimization without any
trade-offs or compromises in quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golnari_P/0/1/0/all/0/1&quot;&gt;Pareesa Ameneh Golnari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06900">
<title>When Bio-Inspired Computing meets Deep Learning: Low-Latency, Accurate, &amp; Energy-Efficient Spiking Neural Networks from Artificial Neural Networks. (arXiv:2312.06900v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06900</link>
<description rdf:parseType="Literal">&lt;p&gt;Bio-inspired Spiking Neural Networks (SNN) are now demonstrating comparable
accuracy to intricate convolutional neural networks (CNN), all while delivering
remarkable energy and latency efficiency when deployed on neuromorphic
hardware. In particular, ANN-to-SNN conversion has recently gained significant
traction in developing deep SNNs with close to state-of-the-art (SOTA) test
accuracy on complex image recognition tasks. However, advanced ANN-to-SNN
conversion approaches demonstrate that for lossless conversion, the number of
SNN time steps must equal the number of quantization steps in the ANN
activation function. Reducing the number of time steps significantly increases
the conversion error. Moreover, the spiking activity of the SNN, which
dominates the compute energy in neuromorphic chips, does not reduce
proportionally with the number of time steps. To mitigate the accuracy concern,
we propose a novel ANN-to-SNN conversion framework, that incurs an
exponentially lower number of time steps compared to that required in the SOTA
conversion approaches. Our framework modifies the SNN integrate-and-fire (IF)
neuron model with identical complexity and shifts the bias term of each batch
normalization (BN) layer in the trained ANN. To mitigate the spiking activity
concern, we propose training the source ANN with a fine-grained L1 regularizer
with surrogate gradients that encourages high spike sparsity in the converted
SNN. Our proposed framework thus yields lossless SNNs with ultra-low latency,
ultra-low compute energy, thanks to the ultra-low timesteps and high spike
sparsity, and ultra-high test accuracy, for example, 73.30% with only 4 time
steps on the ImageNet dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Datta_G/0/1/0/all/0/1&quot;&gt;Gourav Datta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zeyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diffenderfer_J/0/1/0/all/0/1&quot;&gt;James Diffenderfer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1&quot;&gt;Bhavya Kailkhura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beerel_P/0/1/0/all/0/1&quot;&gt;Peter A. Beerel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06914">
<title>Exploring Novel Object Recognition and Spontaneous Location Recognition Machine Learning Analysis Techniques in Alzheimer&apos;s Mice. (arXiv:2312.06914v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.06914</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding object recognition patterns in mice is crucial for advancing
behavioral neuroscience and has significant implications for human health,
particularly in the realm of Alzheimer&apos;s research. This study is centered on
the development, application, and evaluation of a state-of-the-art
computational pipeline designed to analyze such behaviors, specifically
focusing on Novel Object Recognition (NOR) and Spontaneous Location Recognition
(SLR) tasks. The pipeline integrates three advanced computational models:
Any-Maze for initial data collection, DeepLabCut for detailed pose estimation,
and Convolutional Neural Networks (CNNs) for nuanced behavioral classification.
Employed across four distinct mouse groups, this pipeline demonstrated high
levels of accuracy and robustness. Despite certain challenges like video
quality limitations and the need for manual calculations, the results affirm
the pipeline&apos;s efficacy and potential for scalability. The study serves as a
proof of concept for a multidimensional computational approach to behavioral
neuroscience, emphasizing the pipeline&apos;s versatility and readiness for future,
more complex analyses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bafana_S/0/1/0/all/0/1&quot;&gt;Soham Bafana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raghuraman_R/0/1/0/all/0/1&quot;&gt;Radha Raghuraman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hussaini_S/0/1/0/all/0/1&quot;&gt;S. Abid Hussaini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06920">
<title>Pain Analysis using Adaptive Hierarchical Spatiotemporal Dynamic Imaging. (arXiv:2312.06920v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06920</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic pain intensity estimation plays a pivotal role in healthcare and
medical fields. While many methods have been developed to gauge human pain
using behavioral or physiological indicators, facial expressions have emerged
as a prominent tool for this purpose. Nevertheless, the dependence on labeled
data for these techniques often renders them expensive and time-consuming. To
tackle this, we introduce the Adaptive Hierarchical Spatio-temporal Dynamic
Image (AHDI) technique. AHDI encodes spatiotemporal changes in facial videos
into a singular RGB image, permitting the application of simpler 2D deep models
for video representation. Within this framework, we employ a residual network
to derive generalized facial representations. These representations are
optimized for two tasks: estimating pain intensity and differentiating between
genuine and simulated pain expressions. For the former, a regression model is
trained using the extracted representations, while for the latter, a binary
classifier identifies genuine versus feigned pain displays. Testing our method
on two widely-used pain datasets, we observed encouraging results for both
tasks. On the UNBC database, we achieved an MSE of 0.27 outperforming the SOTA
which had an MSE of 0.40. On the BioVid dataset, our model achieved an accuracy
of 89.76%, which is an improvement of 5.37% over the SOTA accuracy. Most
notably, for distinguishing genuine from simulated pain, our accuracy stands at
94.03%, marking a substantial improvement of 8.98%. Our methodology not only
minimizes the need for extensive labeled data but also augments the precision
of pain evaluations, facilitating superior pain management.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serraoui_I/0/1/0/all/0/1&quot;&gt;Issam Serraoui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1&quot;&gt;Eric Granger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hadid_A/0/1/0/all/0/1&quot;&gt;Abdenour Hadid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taleb_Ahmed_A/0/1/0/all/0/1&quot;&gt;Abdelmalik Taleb-Ahmed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06925">
<title>Facial Emotion Recognition in VR Games. (arXiv:2312.06925v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2312.06925</link>
<description rdf:parseType="Literal">&lt;p&gt;Emotion detection is a crucial component of Games User Research (GUR), as it
allows game developers to gain insights into players&apos; emotional experiences and
tailor their games accordingly. However, detecting emotions in Virtual Reality
(VR) games is challenging due to the Head-Mounted Display (HMD) that covers the
top part of the player&apos;s face, namely, their eyes and eyebrows, which provide
crucial information for recognizing the impression. To tackle this we used a
Convolutional Neural Network (CNN) to train a model to predict emotions in
full-face images where the eyes and eyebrows are covered. We used the FER2013
dataset, which we modified to cover eyes and eyebrows in images. The model in
these images can accurately recognize seven different emotions which are anger,
happiness, disgust, fear, impartiality, sadness and surprise.
&lt;/p&gt;
&lt;p&gt;We assessed the model&apos;s performance by testing it on two VR games and using
it to detect players&apos; emotions. We collected self-reported emotion data from
the players after the gameplay sessions. We analyzed the data collected from
our experiment to understand which emotions players experience during the
gameplay. We found that our approach has the potential to enhance gameplay
analysis by enabling the detection of players&apos; emotions in VR games, which can
help game developers create more engaging and immersive game experiences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dehghani_F/0/1/0/all/0/1&quot;&gt;Fatemeh Dehghani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaman_L/0/1/0/all/0/1&quot;&gt;Loutfouz Zaman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06934">
<title>Toward Real Text Manipulation Detection: New Dataset and New Solution. (arXiv:2312.06934v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06934</link>
<description rdf:parseType="Literal">&lt;p&gt;With the surge in realistic text tampering, detecting fraudulent text in
images has gained prominence for maintaining information security. However, the
high costs associated with professional text manipulation and annotation limit
the availability of real-world datasets, with most relying on synthetic
tampering, which inadequately replicates real-world tampering attributes. To
address this issue, we present the Real Text Manipulation (RTM) dataset,
encompassing 14,250 text images, which include 5,986 manually and 5,258
automatically tampered images, created using a variety of techniques, alongside
3,006 unaltered text images for evaluating solution stability. Our evaluations
indicate that existing methods falter in text forgery detection on the RTM
dataset. We propose a robust baseline solution featuring a Consistency-aware
Aggregation Hub and a Gated Cross Neighborhood-attention Fusion module for
efficient multi-modal information fusion, supplemented by a Tampered-Authentic
Contrastive Learning module during training, enriching feature representation
distinction. This framework, extendable to other dual-stream architectures,
demonstrated notable localization performance improvements of 7.33% and 6.38%
on manual and overall manipulations, respectively. Our contributions aim to
propel advancements in real-world text tampering detection. Code and dataset
will be made available at https://github.com/DrLuo/RTM
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1&quot;&gt;Dongliang Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1&quot;&gt;Rui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xianjin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1&quot;&gt;Jishen Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1&quot;&gt;Xiang Bai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06940">
<title>Benchmarking Deep Learning Classifiers for SAR Automatic Target Recognition. (arXiv:2312.06940v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06940</link>
<description rdf:parseType="Literal">&lt;p&gt;Synthetic Aperture Radar SAR Automatic Target Recognition ATR is a key
technique of remote-sensing image recognition which can be supported by deep
neural networks The existing works of SAR ATR mostly focus on improving the
accuracy of the target recognition while ignoring the systems performance in
terms of speed and storage which is critical to real-world applications of SAR
ATR For decision-makers aiming to identify a proper deep learning model to
deploy in a SAR ATR system it is important to understand the performance of
different candidate deep learning models and determine the best model
accordingly This paper comprehensively benchmarks several advanced deep
learning models for SAR ATR with multiple distinct SAR imagery datasets
Specifically we train and test five SAR image classifiers based on Residual
Neural Networks ResNet18 ResNet34 ResNet50 Graph Neural Network GNN and Vision
Transformer for Small-Sized Datasets (SS-ViT) We select three datasets MSTAR
GBSAR and SynthWakeSAR that offer heterogeneity We evaluate and compare the
five classifiers concerning their classification accuracy runtime performance
in terms of inference throughput and analytical performance in terms of number
of parameters number of layers model size and number of operations Experimental
results show that the GNN classifier outperforms with respect to throughput and
latency However it is also shown that no clear model winner emerges from all of
our chosen metrics and a one model rules all case is doubtful in the domain of
SAR ATR
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fein_Ashley_J/0/1/0/all/0/1&quot;&gt;Jacob Fein-Ashley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1&quot;&gt;Tian Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kannan_R/0/1/0/all/0/1&quot;&gt;Rajgopal Kannan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prasanna_V/0/1/0/all/0/1&quot;&gt;Viktor Prasanna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Busart_C/0/1/0/all/0/1&quot;&gt;Carl Busart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06946">
<title>WaterHE-NeRF: Water-ray Tracing Neural Radiance Fields for Underwater Scene Reconstruction. (arXiv:2312.06946v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06946</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Field (NeRF) technology demonstrates immense potential in
novel viewpoint synthesis tasks, due to its physics-based volumetric rendering
process, which is particularly promising in underwater scenes. Addressing the
limitations of existing underwater NeRF methods in handling light attenuation
caused by the water medium and the lack of real Ground Truth (GT) supervision,
this study proposes WaterHE-NeRF. We develop a new water-ray tracing field by
Retinex theory that precisely encodes color, density, and illuminance
attenuation in three-dimensional space. WaterHE-NeRF, through its illuminance
attenuation mechanism, generates both degraded and clear multi-view images and
optimizes image restoration by combining reconstruction loss with Wasserstein
distance. Additionally, the use of histogram equalization (HE) as pseudo-GT
enhances the network&apos;s accuracy in preserving original details and color
distribution. Extensive experiments on real underwater datasets and synthetic
datasets validate the effectiveness of WaterHE-NeRF. Our code will be made
publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingchun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_T/0/1/0/all/0/1&quot;&gt;Tianyu Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zongxin He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dehuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weishi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1&quot;&gt;Xianping Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chongyi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06947">
<title>MaTe3D: Mask-guided Text-based 3D-aware Portrait Editing. (arXiv:2312.06947v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06947</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, 3D-aware face editing has witnessed remarkable progress. Although
current approaches successfully perform mask-guided or text-based editing,
these properties have not been combined into a single method. To address this
limitation, we propose \textbf{MaTe3D}: mask-guided text-based 3D-aware
portrait editing. First, we propose a new SDF-based 3D generator. To better
perform masked-based editing (mainly happening in local areas), we propose SDF
and density consistency losses, aiming to effectively model both the global and
local representations jointly. Second, we introduce an inference-optimized
method. We introduce two techniques based on the SDS (Score Distillation
Sampling), including a blending SDS and a conditional SDS. The former aims to
overcome the mismatch problem between geometry and appearance, ultimately
harming fidelity. The conditional SDS contributes to further producing
satisfactory and stable results. Additionally, we create CatMask-HQ dataset, a
large-scale high-resolution cat face annotations. We perform experiments on
both the FFHQ and CatMask-HQ datasets to demonstrate the effectiveness of the
proposed method. Our method generates faithfully a edited 3D-aware face image
given a modified mask and a text prompt. Our code and models will be publicly
released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1&quot;&gt;Kangneng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1&quot;&gt;Daiheng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Peng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xusen Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Longhao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shiqi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1&quot;&gt;Liefeng Bo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaxing Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06950">
<title>READ-PVLA: Recurrent Adapter with Partial Video-Language Alignment for Parameter-Efficient Transfer Learning in Low-Resource Video-Language Modeling. (arXiv:2312.06950v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06950</link>
<description rdf:parseType="Literal">&lt;p&gt;Fully fine-tuning pretrained large-scale transformer models has become a
popular paradigm for video-language modeling tasks, such as temporal language
grounding and video-language summarization. With a growing number of tasks and
limited training data, such full fine-tuning approach leads to costly model
storage and unstable training. To overcome these shortcomings, we introduce
lightweight adapters to the pre-trained model and only update them at
fine-tuning time. However, existing adapters fail to capture intrinsic temporal
relations among video frames or textual words. Moreover, they neglect the
preservation of critical task-related information that flows from the raw
video-language input into the adapter&apos;s low-dimensional space. To address these
issues, we first propose a novel REcurrent ADapter (READ) that employs
recurrent computation to enable temporal modeling capability. Second, we
propose Partial Video-Language Alignment (PVLA) objective via the use of
partial optimal transport to maintain task-related information flowing into our
READ modules. We validate our READ-PVLA framework through extensive experiments
where READ-PVLA significantly outperforms all existing fine-tuning strategies
on multiple low-resource temporal language grounding and video-language
summarization benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Thong Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaobao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1&quot;&gt;Xinshuai Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_K/0/1/0/all/0/1&quot;&gt;Khoi Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1&quot;&gt;Cong-Duy Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_S/0/1/0/all/0/1&quot;&gt;See-Kiong Ng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuan_L/0/1/0/all/0/1&quot;&gt;Luu Anh Tuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06955">
<title>IA2U: A Transfer Plugin with Multi-Prior for In-Air Model to Underwater. (arXiv:2312.06955v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06955</link>
<description rdf:parseType="Literal">&lt;p&gt;In underwater environments, variations in suspended particle concentration
and turbidity cause severe image degradation, posing significant challenges to
image enhancement (IE) and object detection (OD) tasks. Currently, in-air image
enhancement and detection methods have made notable progress, but their
application in underwater conditions is limited due to the complexity and
variability of these environments. Fine-tuning in-air models saves high
overhead and has more optional reference work than building an underwater model
from scratch. To address these issues, we design a transfer plugin with
multiple priors for converting in-air models to underwater applications, named
IA2U. IA2U enables efficient application in underwater scenarios, thereby
improving performance in Underwater IE and OD. IA2U integrates three types of
underwater priors: the water type prior that characterizes the degree of image
degradation, such as color and visibility; the degradation prior, focusing on
differences in details and textures; and the sample prior, considering the
environmental conditions at the time of capture and the characteristics of the
photographed object. Utilizing a Transformer-like structure, IA2U employs these
priors as query conditions and a joint task loss function to achieve
hierarchical enhancement of task-level underwater image features, therefore
considering the requirements of two different tasks, IE and OD. Experimental
results show that IA2U combined with an in-air model can achieve superior
performance in underwater image enhancement and object detection tasks. The
code will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingchun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gai_Q/0/1/0/all/0/1&quot;&gt;Qilin Gai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weishi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1&quot;&gt;Kin-man Lam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1&quot;&gt;Xianping Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Ting Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chongyi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06958">
<title>PatchMorph: A Stochastic Deep Learning Approach for Unsupervised 3D Brain Image Registration with Small Patches. (arXiv:2312.06958v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06958</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce &quot;PatchMorph,&quot; an new stochastic deep learning algorithm tailored
for unsupervised 3D brain image registration. Unlike other methods, our method
uses compact patches of a constant small size to derive solutions that can
combine global transformations with local deformations. This approach minimizes
the memory footprint of the GPU during training, but also enables us to operate
on numerous amounts of randomly overlapping small patches during inference to
mitigate image and patch boundary problems. PatchMorph adeptly handles world
coordinate transformations between two input images, accommodating variances in
attributes such as spacing, array sizes, and orientations. The spatial
resolution of patches transitions from coarse to fine, addressing both global
and local attributes essential for aligning the images. Each patch offers a
unique perspective, together converging towards a comprehensive solution.
Experiments on human T1 MRI brain images and marmoset brain images from serial
2-photon tomography affirm PatchMorph&apos;s superior performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skibbe_H/0/1/0/all/0/1&quot;&gt;Henrik Skibbe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Byra_M/0/1/0/all/0/1&quot;&gt;Michal Byra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watakabe_A/0/1/0/all/0/1&quot;&gt;Akiya Watakabe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamamori_T/0/1/0/all/0/1&quot;&gt;Tetsuo Yamamori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reisert_M/0/1/0/all/0/1&quot;&gt;Marco Reisert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06960">
<title>Remote Sensing Vision-Language Foundation Models without Annotations via Ground Remote Alignment. (arXiv:2312.06960v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06960</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a method to train vision-language models for remote-sensing
images without using any textual annotations. Our key insight is to use
co-located internet imagery taken on the ground as an intermediary for
connecting remote-sensing images and language. Specifically, we train an image
encoder for remote sensing images to align with the image encoder of CLIP using
a large amount of paired internet and satellite images. Our unsupervised
approach enables the training of a first-of-its-kind large-scale vision
language model (VLM) for remote sensing images at two different resolutions. We
show that these VLMs enable zero-shot, open-vocabulary image classification,
retrieval, segmentation and visual question answering for satellite images. On
each of these tasks, our VLM trained without textual annotations outperforms
existing VLMs trained with supervision, with gains of up to 20% for
classification and 80% for segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mall_U/0/1/0/all/0/1&quot;&gt;Utkarsh Mall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phoo_C/0/1/0/all/0/1&quot;&gt;Cheng Perng Phoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Meilin Kelsey Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vondrick_C/0/1/0/all/0/1&quot;&gt;Carl Vondrick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hariharan_B/0/1/0/all/0/1&quot;&gt;Bharath Hariharan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bala_K/0/1/0/all/0/1&quot;&gt;Kavita Bala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06968">
<title>Hallucination Augmented Contrastive Learning for Multimodal Large Language Model. (arXiv:2312.06968v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06968</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-modal large language models (MLLMs) have been shown to efficiently
integrate natural language with visual information to handle multi-modal tasks.
However, MLLMs still face a fundamental limitation of hallucinations, where
they tend to generate erroneous or fabricated information. In this paper, we
address hallucinations in MLLMs from a novel perspective of representation
learning. We first analyzed the representation distribution of textual and
visual tokens in MLLM, revealing two important findings: 1) there is a
significant gap between textual and visual representations, indicating
unsatisfactory cross-modal representation alignment; 2) representations of
texts that contain and do not contain hallucinations are entangled, making it
challenging to distinguish them. These two observations inspire us with a
simple yet effective method to mitigate hallucinations. Specifically, we
introduce contrastive learning into MLLMs and use text with hallucination as
hard negative examples, naturally bringing representations of non-hallucinative
text and visual samples closer while pushing way representations of
non-hallucinating and hallucinative text. We evaluate our method quantitatively
and qualitatively, showing its effectiveness in reducing hallucination
occurrences and improving performance across multiple benchmarks. On the
MMhal-Bench benchmark, our method obtains a 34.66% /29.5% improvement over the
baseline MiniGPT-4/LLaVA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1&quot;&gt;Chaoya Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Haiyang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1&quot;&gt;Mengfan Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiaxing Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1&quot;&gt;Wei Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1&quot;&gt;Ming Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1&quot;&gt;Qinghao Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Ji Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Fei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shikun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06971">
<title>CCM: Adding Conditional Controls to Text-to-Image Consistency Models. (arXiv:2312.06971v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06971</link>
<description rdf:parseType="Literal">&lt;p&gt;Consistency Models (CMs) have showed a promise in creating visual content
efficiently and with high quality. However, the way to add new conditional
controls to the pretrained CMs has not been explored. In this technical report,
we consider alternative strategies for adding ControlNet-like conditional
control to CMs and present three significant findings. 1) ControlNet trained
for diffusion models (DMs) can be directly applied to CMs for high-level
semantic controls but struggles with low-level detail and realism control. 2)
CMs serve as an independent class of generative models, based on which
ControlNet can be trained from scratch using Consistency Training proposed by
Song et al. 3) A lightweight adapter can be jointly optimized under multiple
conditions through Consistency Training, allowing for the swift transfer of
DMs-based ControlNet to CMs. We study these three solutions across various
conditional controls, including edge, depth, human pose, low-resolution image
and masked image with text-to-image latent consistency models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1&quot;&gt;Jie Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1&quot;&gt;Kai Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Han Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhiheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yujun Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1&quot;&gt;Xueyang Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1&quot;&gt;Zheng-Jun Zha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06978">
<title>CLASSMix: Adaptive stain separation-based contrastive learning with pseudo labeling for histopathological image classification. (arXiv:2312.06978v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06978</link>
<description rdf:parseType="Literal">&lt;p&gt;Histopathological image classification is one of the critical aspects in
medical image analysis. Due to the high expense associated with the labeled
data in model training, semi-supervised learning methods have been proposed to
alleviate the need of extensively labeled datasets. In this work, we propose a
model for semi-supervised classification tasks on digital histopathological
Hematoxylin and Eosin (H&amp;amp;E) images. We call the new model Contrastive Learning
with Adaptive Stain Separation and MixUp (CLASSMix). Our model is formed by two
main parts: contrastive learning between adaptively stain separated Hematoxylin
images and Eosin images, and pseudo labeling using MixUp. We compare our model
with other state-of-the-art models on clear cell renal cell carcinoma (ccRCC)
datasets from our institution and The Cancer Genome Atlas Program (TCGA). We
demonstrate that our CLASSMix model has the best performance on both datasets.
The contributions of different parts in our model are also analyzed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bodong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manoochehri_H/0/1/0/all/0/1&quot;&gt;Hamid Manoochehri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_M/0/1/0/all/0/1&quot;&gt;Man Minh Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fooladgar_F/0/1/0/all/0/1&quot;&gt;Fahimeh Fooladgar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chong_Y/0/1/0/all/0/1&quot;&gt;Yosep Chong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knudsen_B/0/1/0/all/0/1&quot;&gt;Beatrice S. Knudsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sirohi_D/0/1/0/all/0/1&quot;&gt;Deepika Sirohi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tasdizen_T/0/1/0/all/0/1&quot;&gt;Tolga Tasdizen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06979">
<title>On the notion of Hallucinations from the lens of Bias and Validity in Synthetic CXR Images. (arXiv:2312.06979v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.06979</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical imaging has revolutionized disease diagnosis, yet the potential is
hampered by limited access to diverse and privacy-conscious datasets.
Open-source medical datasets, while valuable, suffer from data quality and
clinical information disparities. Generative models, such as diffusion models,
aim to mitigate these challenges. At Stanford, researchers explored the utility
of a fine-tuned Stable Diffusion model (RoentGen) for medical imaging data
augmentation. Our work examines specific considerations to expand the Stanford
research question, Could Stable Diffusion Solve a Gap in Medical Imaging Data?
from the lens of bias and validity of the generated outcomes. We leveraged
RoentGen to produce synthetic Chest-XRay (CXR) images and conducted assessments
on bias, validity, and hallucinations. Diagnostic accuracy was evaluated by a
disease classifier, while a COVID classifier uncovered latent hallucinations.
The bias analysis unveiled disparities in classification performance among
various subgroups, with a pronounced impact on the Female Hispanic subgroup.
Furthermore, incorporating race and gender into input prompts exacerbated
fairness issues in the generated images. The quality of synthetic images
exhibited variability, particularly in certain disease classes, where there was
more significant uncertainty compared to the original images. Additionally, we
observed latent hallucinations, with approximately 42% of the images
incorrectly indicating COVID, hinting at the presence of hallucinatory
elements. These identifications provide new research directions towards
interpretability of synthetic CXR images, for further understanding of
associated risks and patient safety in medical applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bhardwaj_G/0/1/0/all/0/1&quot;&gt;Gauri Bhardwaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Govindarajulu_Y/0/1/0/all/0/1&quot;&gt;Yuvaraj Govindarajulu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Narayanan_S/0/1/0/all/0/1&quot;&gt;Sundaraparipurnan Narayanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kulkarni_P/0/1/0/all/0/1&quot;&gt;Pavan Kulkarni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Parmar_M/0/1/0/all/0/1&quot;&gt;Manojkumar Parmar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06988">
<title>MWSIS: Multimodal Weakly Supervised Instance Segmentation with 2D Box Annotations for Autonomous Driving. (arXiv:2312.06988v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06988</link>
<description rdf:parseType="Literal">&lt;p&gt;Instance segmentation is a fundamental research in computer vision,
especially in autonomous driving. However, manual mask annotation for instance
segmentation is quite time-consuming and costly. To address this problem, some
prior works attempt to apply weakly supervised manner by exploring 2D or 3D
boxes. However, no one has ever successfully segmented 2D and 3D instances
simultaneously by only using 2D box annotations, which could further reduce the
annotation cost by an order of magnitude. Thus, we propose a novel framework
called Multimodal Weakly Supervised Instance Segmentation (MWSIS), which
incorporates various fine-grained label generation and correction modules for
both 2D and 3D modalities to improve the quality of pseudo labels, along with a
new multimodal cross-supervision approach, named Consistency Sparse Cross-modal
Supervision (CSCS), to reduce the inconsistency of multimodal predictions by
response distillation. Particularly, transferring the 3D backbone to downstream
tasks not only improves the performance of the 3D detectors, but also
outperforms fully supervised instance segmentation with only 5% fully
supervised annotations. On the Waymo dataset, the proposed framework
demonstrates significant improvements over the baseline, especially achieving
2.59% mAP and 12.75% mAP increases for 2D and 3D instance segmentation tasks,
respectively. The code is available at
https://github.com/jiangxb98/mwsis-plugin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_G/0/1/0/all/0/1&quot;&gt;Guangfeng Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yuzhi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1&quot;&gt;Wenlong Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1&quot;&gt;Tao He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_P/0/1/0/all/0/1&quot;&gt;Pai Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06991">
<title>Attacking the Loop: Adversarial Attacks on Graph-based Loop Closure Detection. (arXiv:2312.06991v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06991</link>
<description rdf:parseType="Literal">&lt;p&gt;With the advancement in robotics, it is becoming increasingly common for
large factories and warehouses to incorporate visual SLAM (vSLAM) enabled
automated robots that operate closely next to humans. This makes any
adversarial attacks on vSLAM components potentially detrimental to humans
working alongside them. Loop Closure Detection (LCD) is a crucial component in
vSLAM that minimizes the accumulation of drift in mapping, since even a small
drift can accumulate into a significant drift over time. A prior work by Kim et
al., SymbioLCD2, unified visual features and semantic objects into a single
graph structure for finding loop closure candidates. While this provided a
performance improvement over visual feature-based LCD, it also created a single
point of vulnerability for potential graph-based adversarial attacks. Unlike
previously reported visual-patch based attacks, small graph perturbations are
far more challenging to detect, making them a more significant threat. In this
paper, we present Adversarial-LCD, a novel black-box evasion attack framework
that employs an eigencentrality-based perturbation method and an SVM-RBF
surrogate model with a Weisfeiler-Lehman feature extractor for attacking
graph-based LCD. Our evaluation shows that the attack performance of
Adversarial-LCD with the SVM-RBF surrogate model was superior to that of other
machine learning surrogate algorithms, including SVM-linear, SVM-polynomial,
and Bayesian classifier, demonstrating the effectiveness of our attack
framework. Furthermore, we show that our eigencentrality-based perturbation
method outperforms other algorithms, such as Random-walk and Shortest-path,
highlighting the efficiency of Adversarial-LCD&apos;s perturbation selection method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jonathan J.Y. Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urschler_M/0/1/0/all/0/1&quot;&gt;Martin Urschler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riddle_P/0/1/0/all/0/1&quot;&gt;Patricia J. Riddle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wicker_J/0/1/0/all/0/1&quot;&gt;Jorg S. Wicker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06995">
<title>Transformer-based No-Reference Image Quality Assessment via Supervised Contrastive Learning. (arXiv:2312.06995v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06995</link>
<description rdf:parseType="Literal">&lt;p&gt;Image Quality Assessment (IQA) has long been a research hotspot in the field
of image processing, especially No-Reference Image Quality Assessment (NR-IQA).
Due to the powerful feature extraction ability, existing Convolution Neural
Network (CNN) and Transformers based NR-IQA methods have achieved considerable
progress. However, they still exhibit limited capability when facing unknown
authentic distortion datasets. To further improve NR-IQA performance, in this
paper, a novel supervised contrastive learning (SCL) and Transformer-based
NR-IQA model SaTQA is proposed. We first train a model on a large-scale
synthetic dataset by SCL (no image subjective score is required) to extract
degradation features of images with various distortion types and levels. To
further extract distortion information from images, we propose a backbone
network incorporating the Multi-Stream Block (MSB) by combining the CNN
inductive bias and Transformer long-term dependence modeling capability.
Finally, we propose the Patch Attention Block (PAB) to obtain the final
distorted image quality score by fusing the degradation features learned from
contrastive learning with the perceptual distortion information extracted by
the backbone network. Experimental results on seven standard IQA datasets show
that SaTQA outperforms the state-of-the-art methods for both synthetic and
authentic datasets. Code is available at
https://github.com/I2-Multimedia-Lab/SaTQA
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jinsong Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1&quot;&gt;Pan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1&quot;&gt;Jie Qin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06999">
<title>DGNet: Dynamic Gradient-guided Network with Noise Suppression for Underwater Image Enhancement. (arXiv:2312.06999v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06999</link>
<description rdf:parseType="Literal">&lt;p&gt;Underwater image enhancement (UIE) is a challenging task due to the complex
degradation caused by underwater environments. To solve this issue, previous
methods often idealize the degradation process, and neglect the impact of
medium noise and object motion on the distribution of image features, limiting
the generalization and adaptability of the model. Previous methods use the
reference gradient that is constructed from original images and synthetic
ground-truth images. This may cause the network performance to be influenced by
some low-quality training data. Our approach utilizes predicted images to
dynamically update pseudo-labels, adding a dynamic gradient to optimize the
network&apos;s gradient space. This process improves image quality and avoids local
optima. Moreover, we propose a Feature Restoration and Reconstruction module
(FRR) based on a Channel Combination Inference (CCI) strategy and a Frequency
Domain Smoothing module (FRS). These modules decouple other degradation
features while reducing the impact of various types of noise on network
performance. Experiments on multiple public datasets demonstrate the
superiority of our method over existing state-of-the-art approaches, especially
in achieving performance milestones: PSNR of 25.6dB and SSIM of 0.93 on the
UIEB dataset. Its efficiency in terms of parameter size and inference time
further attests to its broad practicality. The code will be made publicly
available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingchun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zongxin He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dehuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1&quot;&gt;Kin-man Lam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weishi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1&quot;&gt;Xianping Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chongyi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07006">
<title>Mixed Pseudo Labels for Semi-Supervised Object Detection. (arXiv:2312.07006v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07006</link>
<description rdf:parseType="Literal">&lt;p&gt;While the pseudo-label method has demonstrated considerable success in
semi-supervised object detection tasks, this paper uncovers notable limitations
within this approach. Specifically, the pseudo-label method tends to amplify
the inherent strengths of the detector while accentuating its weaknesses, which
is manifested in the missed detection of pseudo-labels, particularly for small
and tail category objects. To overcome these challenges, this paper proposes
Mixed Pseudo Labels (MixPL), consisting of Mixup and Mosaic for pseudo-labeled
data, to mitigate the negative impact of missed detections and balance the
model&apos;s learning across different object scales. Additionally, the model&apos;s
detection performance on tail categories is improved by resampling labeled data
with relevant instances. Notably, MixPL consistently improves the performance
of various detectors and obtains new state-of-the-art results with Faster
R-CNN, FCOS, and DINO on COCO-Standard and COCO-Full benchmarks. Furthermore,
MixPL also exhibits good scalability on large models, improving DINO Swin-L by
2.5% mAP and achieving nontrivial new records (60.2% mAP) on the COCO val2017
benchmark without extra annotations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zeming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinjiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07009">
<title>Vision-language Assisted Attribute Learning. (arXiv:2312.07009v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07009</link>
<description rdf:parseType="Literal">&lt;p&gt;Attribute labeling at large scale is typically incomplete and partial, posing
significant challenges to model optimization. Existing attribute learning
methods often treat the missing labels as negative or simply ignore them all
during training, either of which could hamper the model performance to a great
extent. To overcome these limitations, in this paper we leverage the available
vision-language knowledge to explicitly disclose the missing labels for
enhancing model learning. Given an image, we predict the likelihood of each
missing attribute label assisted by an off-the-shelf vision-language model, and
randomly select to ignore those with high scores in training. Our strategy
strikes a good balance between fully ignoring and negatifying the missing
labels, as these high scores are found to be informative on revealing label
ambiguity. Extensive experiments show that our proposed vision-language
assisted loss can achieve state-of-the-art performance on the newly cleaned VAW
dataset. Qualitative evaluation demonstrates the ability of the proposed method
in predicting more complete attributes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1&quot;&gt;Kongming Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinran Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1&quot;&gt;Donghui Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1&quot;&gt;Ling Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Weidong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiatian Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zhanyu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jun Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07021">
<title>Transferring Modality-Aware Pedestrian Attentive Learning Visible-Infrared Person Re-identification. (arXiv:2312.07021v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07021</link>
<description rdf:parseType="Literal">&lt;p&gt;Visible-infrared person re-identification (VI-ReID) aims to search the same
pedestrian of interest across visible and infrared modalities. Existing models
mainly focus on compensating for modality-specific information to reduce
modality variation. However, these methods often lead to a higher computational
overhead and may introduce interfering information when generating the
corresponding images or features. To address this issue, it is critical to
leverage pedestrian-attentive features and learn modality-complete and
-consistent representation. In this paper, a novel Transferring Modality-Aware
Pedestrian Attentive Learning (TMPA) model is proposed, focusing on the
pedestrian regions to efficiently compensate for missing modality-specific
features. Specifically, we propose a region-based data augmentation module
PedMix to enhance pedestrian region coherence by mixing the corresponding
regions from different modalities. A lightweight hybrid compensation module,
i.e., the Modality Feature Transfer (MFT), is devised to integrate cross
attention and convolution networks to fully explore the discriminative
modality-complete features with minimal computational overhead. Extensive
experiments conducted on the benchmark SYSU-MM01 and RegDB datasets
demonstrated the effectiveness of our proposed TMPA model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yuwei Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenhao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1&quot;&gt;Licheng Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07039">
<title>Diff-OP3D: Bridging 2D Diffusion for Open Pose 3D Zero-Shot Classification. (arXiv:2312.07039v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07039</link>
<description rdf:parseType="Literal">&lt;p&gt;With the explosive 3D data growth, the urgency of utilizing zero-shot
learning to facilitate data labeling becomes evident. Recently, the methods via
transferring Contrastive Language-Image Pre-training (CLIP) to 3D vision have
made great progress in the 3D zero-shot classification task. However, these
methods primarily focus on aligned pose 3D objects (ap-3os), overlooking the
recognition of 3D objects with open poses (op-3os) typically encountered in
real-world scenarios, such as an overturned chair or a lying teddy bear. To
this end, we propose a more challenging benchmark for 3D open-pose zero-shot
classification. Echoing our benchmark, we design a concise angle-refinement
mechanism that automatically optimizes one ideal pose as well as classifies
these op-3os. Furthermore, we make a first attempt to bridge 2D pre-trained
diffusion model as a classifer to 3D zero-shot classification without any
additional training. Such 2D diffusion to 3D objects proves vital in improving
zero-shot classification for both ap-3os and op-3os. Our model notably improves
by 3.5% and 15.8% on ModelNet10$^{\ddag}$ and McGill$^{\ddag}$ open pose
benchmarks, respectively, and surpasses the current state-of-the-art by 6.8% on
the aligned pose ModelNet10, affirming diffusion&apos;s efficacy in 3D zero-shot
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Weiguang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guanyu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chaolong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1&quot;&gt;Chenru Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yuyao Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kaizhu Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07048">
<title>Edge Wasserstein Distance Loss for Oriented Object Detection. (arXiv:2312.07048v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07048</link>
<description rdf:parseType="Literal">&lt;p&gt;Regression loss design is an essential topic for oriented object detection.
Due to the periodicity of the angle and the ambiguity of width and height
definition, traditional L1-distance loss and its variants have been suffered
from the metric discontinuity and the square-like problem. As a solution, the
distribution based methods show significant advantages by representing oriented
boxes as distributions. Differing from exploited the Gaussian distribution to
get analytical form of distance measure, we propose a novel oriented regression
loss, Wasserstein Distance(EWD) loss, to alleviate the square-like problem.
Specifically, for the oriented box(OBox) representation, we choose a
specially-designed distribution whose probability density function is only
nonzero over the edges. On this basis, we develop Wasserstein distance as the
measure. Besides, based on the edge representation of OBox, the EWD loss can be
generalized to quadrilateral and polynomial regression scenarios. Experiments
on multiple popular datasets and different detectors show the effectiveness of
the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yuke Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruan_Y/0/1/0/all/0/1&quot;&gt;Yumeng Ruan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1&quot;&gt;Zihua Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1&quot;&gt;Sheng Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07051">
<title>Mask as Supervision: Leveraging Unified Mask Information for Unsupervised 3D Pose Estimation. (arXiv:2312.07051v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07051</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic estimation of 3D human pose from monocular RGB images is a
challenging and unsolved problem in computer vision. In a supervised manner,
approaches heavily rely on laborious annotations and present hampered
generalization ability due to the limited diversity of 3D pose datasets. To
address these challenges, we propose a unified framework that leverages mask as
supervision for unsupervised 3D pose estimation. With general unsupervised
segmentation algorithms, the proposed model employs skeleton and physique
representations that exploit accurate pose information from coarse to fine.
Compared with previous unsupervised approaches, we organize the human skeleton
in a fully unsupervised way which enables the processing of annotation-free
data and provides ready-to-use estimation results. Comprehensive experiments
demonstrate our state-of-the-art pose estimation performance on Human3.6M and
MPI-INF-3DHP datasets. Further experiments on in-the-wild datasets also
illustrate the capability to access more data to boost our model. Code will be
available at https://github.com/Charrrrrlie/Mask-as-Supervision.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuchen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xiao Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07052">
<title>Adjustable Robust Transformer for High Myopia Screening in Optical Coherence Tomography. (arXiv:2312.07052v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07052</link>
<description rdf:parseType="Literal">&lt;p&gt;Myopia is a manifestation of visual impairment caused by an excessively
elongated eyeball. Image data is critical material for studying high myopia and
pathological myopia. Measurements of spherical equivalent and axial length are
the gold standards for identifying high myopia, but the available image data
for matching them is scarce. In addition, the criteria for defining high myopia
vary from study to study, and therefore the inclusion of samples in automated
screening efforts requires an appropriate assessment of interpretability. In
this work, we propose a model called adjustable robust transformer (ARTran) for
high myopia screening of optical coherence tomography (OCT) data. Based on
vision transformer, we propose anisotropic patch embedding (APE) to capture
more discriminative features of high myopia. To make the model effective under
variable screening conditions, we propose an adjustable class embedding (ACE)
to replace the fixed class token, which changes the output to adapt to
different conditions. Considering the confusion of the data at high myopia and
low myopia threshold, we introduce the label noise learning strategy and
propose a shifted subspace transition matrix (SST) to enhance the robustness of
the model. Besides, combining the two structures proposed above, the model can
provide evidence for uncertainty evaluation. The experimental results
demonstrate the effectiveness and reliability of the proposed method. Code is
available at: https://github.com/maxiao0234/ARTran.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zetian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1&quot;&gt;Zexuan Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_N/0/1/0/all/0/1&quot;&gt;Na Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1&quot;&gt;Songtao Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qiang Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07061">
<title>MaxQ: Multi-Axis Query for N:M Sparsity Network. (arXiv:2312.07061v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07061</link>
<description rdf:parseType="Literal">&lt;p&gt;N:M sparsity has received increasing attention due to its remarkable
performance and latency trade-off compared with structured and unstructured
sparsity. However, existing N:M sparsity methods do not differentiate the
relative importance of weights among blocks and leave important weights
underappreciated. Besides, they directly apply N:M sparsity to the whole
network, which will cause severe information loss. Thus, they are still
sub-optimal. In this paper, we propose an efficient and effective Multi-Axis
Query methodology, dubbed as MaxQ, to rectify these problems. During the
training, MaxQ employs a dynamic approach to generate soft N:M masks,
considering the weight importance across multiple axes. This method enhances
the weights with more importance and ensures more effective updates. Meanwhile,
a sparsity strategy that gradually increases the percentage of N:M weight
blocks is applied, which allows the network to heal from the pruning-induced
damage progressively. During the runtime, the N:M soft masks can be precomputed
as constants and folded into weights without causing any distortion to the
sparse pattern and incurring additional computational overhead. Comprehensive
experiments demonstrate that MaxQ achieves consistent improvements across
diverse CNN architectures in various computer vision tasks, including image
classification, object detection and instance segmentation. For ResNet50 with
1:16 sparse pattern, MaxQ can achieve 74.6\% top-1 accuracy on ImageNet and
improve by over 2.8\% over the state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_J/0/1/0/all/0/1&quot;&gt;Jingyang Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Siqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junhao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhuangzhi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tianxin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1&quot;&gt;Linpeng Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07062">
<title>ThinkBot: Embodied Instruction Following with Thought Chain Reasoning. (arXiv:2312.07062v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07062</link>
<description rdf:parseType="Literal">&lt;p&gt;Embodied Instruction Following (EIF) requires agents to complete human
instruction by interacting objects in complicated surrounding environments.
Conventional methods directly consider the sparse human instruction to generate
action plans for agents, which usually fail to achieve human goals because of
the instruction incoherence in action descriptions. On the contrary, we propose
ThinkBot that reasons the thought chain in human instruction to recover the
missing action descriptions, so that the agent can successfully complete human
goals by following the coherent instruction. Specifically, we first design an
instruction completer based on large language models to recover the missing
actions with interacted objects between consecutive human instruction, where
the perceived surrounding environments and the completed sub-goals are
considered for instruction completion. Based on the partially observed scene
semantic maps, we present an object localizer to infer the position of
interacted objects for agents to achieve complex human goals. Extensive
experiments in the simulated environment show that our ThinkBot outperforms the
state-of-the-art EIF methods by a sizable margin in both success rate and
execution efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1&quot;&gt;Guanxing Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Changliu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiwen Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yansong Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07063">
<title>Template Free Reconstruction of Human-object Interaction with Procedural Interaction Generation. (arXiv:2312.07063v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07063</link>
<description rdf:parseType="Literal">&lt;p&gt;Reconstructing human-object interaction in 3D from a single RGB image is a
challenging task and existing data driven methods do not generalize beyond the
objects present in the carefully curated 3D interaction datasets. Capturing
large-scale real data to learn strong interaction and 3D shape priors is very
expensive due to the combinatorial nature of human-object interactions. In this
paper, we propose ProciGen (Procedural interaction Generation), a method to
procedurally generate datasets with both, plausible interaction and diverse
object variation. We generate 1M+ human-object interaction pairs in 3D and
leverage this large-scale data to train our HDM (Hierarchical Diffusion Model),
a novel method to reconstruct interacting human and unseen objects, without any
templates. Our HDM is an image-conditioned diffusion model that learns both
realistic interaction and highly accurate human and object shapes. Experiments
show that our HDM trained with ProciGen significantly outperforms prior methods
that requires template meshes and that our dataset allows training methods with
strong generalization ability to unseen object instances. Our code and data
will be publicly released at:
https://virtualhumans.mpi-inf.mpg.de/procigen-hdm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xianghui Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatnagar_B/0/1/0/all/0/1&quot;&gt;Bharat Lal Bhatnagar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lenssen_J/0/1/0/all/0/1&quot;&gt;Jan Eric Lenssen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pons_Moll_G/0/1/0/all/0/1&quot;&gt;Gerard Pons-Moll&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07064">
<title>Efficient Cross-Domain Federated Learning by MixStyle Approximation. (arXiv:2312.07064v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07064</link>
<description rdf:parseType="Literal">&lt;p&gt;With the advent of interconnected and sensor-equipped edge devices, Federated
Learning (FL) has gained significant attention, enabling decentralized learning
while maintaining data privacy. However, FL faces two challenges in real-world
tasks: expensive data labeling and domain shift between source and target
samples. In this paper, we introduce a privacy-preserving, resource-efficient
FL concept for client adaptation in hardware-constrained environments. Our
approach includes server model pre-training on source data and subsequent
fine-tuning on target data via low-end clients. The local client adaptation
process is streamlined by probabilistic mixing of instance-level feature
statistics approximated from source and target domain data. The adapted
parameters are transferred back to the central server and globally aggregated.
Preliminary results indicate that our method reduces computational and
transmission costs while maintaining competitive performance on downstream
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roder_M/0/1/0/all/0/1&quot;&gt;Manuel R&amp;#xf6;der&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heller_L/0/1/0/all/0/1&quot;&gt;Leon Heller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munch_M/0/1/0/all/0/1&quot;&gt;Maximilian M&amp;#xfc;nch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schleif_F/0/1/0/all/0/1&quot;&gt;Frank-Michael Schleif&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07066">
<title>DiffuVST: Narrating Fictional Scenes with Global-History-Guided Denoising Models. (arXiv:2312.07066v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07066</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in image and video creation, especially AI-based image
synthesis, have led to the production of numerous visual scenes that exhibit a
high level of abstractness and diversity. Consequently, Visual Storytelling
(VST), a task that involves generating meaningful and coherent narratives from
a collection of images, has become even more challenging and is increasingly
desired beyond real-world imagery. While existing VST techniques, which
typically use autoregressive decoders, have made significant progress, they
suffer from low inference speed and are not well-suited for synthetic scenes.
To this end, we propose a novel diffusion-based system DiffuVST, which models
the generation of a series of visual descriptions as a single conditional
denoising process. The stochastic and non-autoregressive nature of DiffuVST at
inference time allows it to generate highly diverse narratives more
efficiently. In addition, DiffuVST features a unique design with bi-directional
text history guidance and multimodal adapter modules, which effectively improve
inter-sentence coherence and image-to-text fidelity. Extensive experiments on
the story generation task covering four fictional visual-story datasets
demonstrate the superiority of DiffuVST over traditional autoregressive models
in terms of both text quality and inference speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Shengguang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1&quot;&gt;Mei Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1&quot;&gt;Qi Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07067">
<title>Focus on Hiders: Exploring Hidden Threats for Enhancing Adversarial Training. (arXiv:2312.07067v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07067</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial training is often formulated as a min-max problem, however,
concentrating only on the worst adversarial examples causes alternating
repetitive confusion of the model, i.e., previously defended or correctly
classified samples are not defensible or accurately classifiable in subsequent
adversarial training. We characterize such non-ignorable samples as &quot;hiders&quot;,
which reveal the hidden high-risk regions within the secure area obtained
through adversarial training and prevent the model from finding the real worst
cases. We demand the model to prevent hiders when defending against adversarial
examples for improving accuracy and robustness simultaneously. By rethinking
and redefining the min-max optimization problem for adversarial training, we
propose a generalized adversarial training algorithm called Hider-Focused
Adversarial Training (HFAT). HFAT introduces the iterative evolution
optimization strategy to simplify the optimization problem and employs an
auxiliary model to reveal hiders, effectively combining the optimization
directions of standard adversarial training and prevention hiders. Furthermore,
we introduce an adaptive weighting mechanism that facilitates the model in
adaptively adjusting its focus between adversarial examples and hiders during
different training periods. We demonstrate the effectiveness of our method
based on extensive experiments, and ensure that HFAT can provide higher
robustness and accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yuxiao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yinpeng Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dongxiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuntian Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07079">
<title>Spatial-Contextual Discrepancy Information Compensation for GAN Inversion. (arXiv:2312.07079v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07079</link>
<description rdf:parseType="Literal">&lt;p&gt;Most existing GAN inversion methods either achieve accurate reconstruction
but lack editability or offer strong editability at the cost of fidelity.
Hence, how to balance the distortioneditability trade-off is a significant
challenge for GAN inversion. To address this challenge, we introduce a novel
spatial-contextual discrepancy information compensationbased GAN-inversion
method (SDIC), which consists of a discrepancy information prediction network
(DIPN) and a discrepancy information compensation network (DICN). SDIC follows
a &quot;compensate-and-edit&quot; paradigm and successfully bridges the gap in image
details between the original image and the reconstructed/edited image. On the
one hand, DIPN encodes the multi-level spatial-contextual information of the
original and initial reconstructed images and then predicts a
spatial-contextual guided discrepancy map with two hourglass modules. In this
way, a reliable discrepancy map that models the contextual relationship and
captures finegrained image details is learned. On the other hand, DICN
incorporates the predicted discrepancy information into both the latent code
and the GAN generator with different transformations, generating high-quality
reconstructed/edited images. This effectively compensates for the loss of image
details during GAN inversion. Both quantitative and qualitative experiments
demonstrate that our proposed method achieves the excellent
distortion-editability trade-off at a fast inference speed for both image
inversion and editing tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Ziqiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yan Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1&quot;&gt;Jing-Hao Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hanzi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07082">
<title>Continual Learning through Networks Splitting and Merging with Dreaming-Meta-Weighted Model Fusion. (arXiv:2312.07082v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07082</link>
<description rdf:parseType="Literal">&lt;p&gt;It&apos;s challenging to balance the networks stability and plasticity in
continual learning scenarios, considering stability suffers from the update of
model and plasticity benefits from it. Existing works usually focus more on the
stability and restrict the learning plasticity of later tasks to avoid
catastrophic forgetting of learned knowledge. Differently, we propose a
continual learning method named Split2MetaFusion which can achieve better
trade-off by employing a two-stage strategy: splitting and meta-weighted
fusion. In this strategy, a slow model with better stability, and a fast model
with better plasticity are learned sequentially at the splitting stage. Then
stability and plasticity are both kept by fusing the two models in an adaptive
manner. Towards this end, we design an optimizer named Task-Preferred Null
Space Projector(TPNSP) to the slow learning process for narrowing the fusion
gap. To achieve better model fusion, we further design a Dreaming-Meta-Weighted
fusion policy for better maintaining the old and new knowledge simultaneously,
which doesn&apos;t require to use the previous datasets. Experimental results and
analysis reported in this work demonstrate the superiority of the proposed
method for maintaining networks stability and keeping its plasticity. Our code
will be released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yi Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1&quot;&gt;Guanglei Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yifei Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1&quot;&gt;Qiang Fang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07100">
<title>Lightweight high-resolution Subject Matting in the Real World. (arXiv:2312.07100v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07100</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing saliency object detection (SOD) methods struggle to satisfy fast
inference and accurate results simultaneously in high resolution scenes. They
are limited by the quality of public datasets and efficient network modules for
high-resolution images. To alleviate these issues, we propose to construct a
saliency object matting dataset HRSOM and a lightweight network PSUNet.
Considering efficient inference of mobile depolyment framework, we design a
symmetric pixel shuffle module and a lightweight module TRSU. Compared to 13
SOD methods, the proposed PSUNet has the best objective performance on the
high-resolution benchmark dataset. Evaluation results of objective assessment
are superior compared to U$^2$Net that has 10 times of parameter amount of our
network. On Snapdragon 8 Gen 2 Mobile Platform, inference a single
640$\times$640 image only takes 113ms. And on the subjective assessment,
evaluation results are better than the industry benchmark IOS16 (Lift subject
from background).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1&quot;&gt;Peng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fanyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1&quot;&gt;Jingwen Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yanhao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1&quot;&gt;Guojun Qi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07125">
<title>Efficient Few-Shot Clinical Task Adaptation with Large Language Models. (arXiv:2312.07125v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07125</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot learning has been studied to adapt models to tasks with very few
samples. It holds profound significance, particularly in clinical tasks, due to
the high annotation cost of medical images. Several works have explored
few-shot learning on medical images, yet they still require a large number of
medical images for pre-training models to gain domain-specific priors. Vision
foundation models recently have achieved remarkable success in natural images.
Hence, adapting rapidly advancing vision foundation models from natural images
to few-shot clinical tasks holds great promise. MedFMC has recently organized a
challenge to shed more light on this topic at NeurIPS 2023. In this work, we
present our challenge solution. We observe that a simple variant of fine-tuning
with partial freezing shows remarkable performance. Empirical evidence
demonstrates that this approach could outperform various common fine-tuning
methods under limited sample sizes. Additionally, we explore enhanced
utilization of semantic supervision to boost performance. We propose a novel
approach that contextualizes labels via large language models (LLMs). Our
findings reveal that the context generated by LLMs significantly enhances the
discrimination of semantic embeddings for similar categories, resulting in a
notable performance improvement of 3%-5% in 1-shot settings compared to
commonly employed one-hot labels and other semantic supervision methods. Our
solution secures the 1st place in the MedFMC challenge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1&quot;&gt;Kaipeng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Weiran Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Lichao Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07128">
<title>MS-Twins: Multi-Scale Deep Self-Attention Networks for Medical Image Segmentation. (arXiv:2312.07128v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.07128</link>
<description rdf:parseType="Literal">&lt;p&gt;Although transformer is preferred in natural language processing, few studies
have applied it in the field of medical imaging. For its long-term dependency,
the transformer is expected to contribute to unconventional convolution neural
net conquer their inherent spatial induction bias. The lately suggested
transformer-based partition method only uses the transformer as an auxiliary
module to help encode the global context into a convolutional representation.
There is hardly any study about how to optimum bond self-attention (the kernel
of transformers) with convolution. To solve the problem, the article proposes
MS-Twins (Multi-Scale Twins), which is a powerful segmentation model on account
of the bond of self-attention and convolution. MS-Twins can better capture
semantic and fine-grained information by combining different scales and
cascading features. Compared with the existing network structure, MS-Twins has
made significant progress on the previous method based on the transformer of
two in common use data sets, Synapse and ACDC. In particular, the performance
of MS-Twins on Synapse is 8% higher than SwinUNet. Even compared with nnUNet,
the best entirely convoluted medical image segmentation network, the
performance of MS-Twins on Synapse and ACDC still has a bit advantage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jing Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07132">
<title>Image Content Generation with Causal Reasoning. (arXiv:2312.07132v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07132</link>
<description rdf:parseType="Literal">&lt;p&gt;The emergence of ChatGPT has once again sparked research in generative
artificial intelligence (GAI). While people have been amazed by the generated
results, they have also noticed the reasoning potential reflected in the
generated textual content. However, this current ability for causal reasoning
is primarily limited to the domain of language generation, such as in models
like GPT-3. In visual modality, there is currently no equivalent research.
Considering causal reasoning in visual content generation is significant. This
is because visual information contains infinite granularity. Particularly,
images can provide more intuitive and specific demonstrations for certain
reasoning tasks, especially when compared to coarse-grained text. Hence, we
propose a new image generation task called visual question answering with image
(VQAI) and establish a dataset of the same name based on the classic
\textit{Tom and Jerry} animated series. Additionally, we develop a new paradigm
for image generation to tackle the challenges of this task. Finally, we perform
extensive experiments and analyses, including visualizations of the generated
content and discussions on the potentials and limitations. The code and data
are publicly available under the license of CC BY-NC-SA 4.0 for academic and
non-commercial usage. The code and dataset are publicly available at:
https://github.com/IEIT-AGI/MIX-Shannon/blob/main/projects/VQAI/lgd_vqai.md.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaochuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_B/0/1/0/all/0/1&quot;&gt;Baoyu Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Runze Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1&quot;&gt;Liang Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Di Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zhenhua Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yaqian Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Rengang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07133">
<title>Text2AC-Zero: Consistent Synthesis of Animated Characters using 2D Diffusion. (arXiv:2312.07133v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07133</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a zero-shot approach for consistent Text-to-Animated-Characters
synthesis based on pre-trained Text-to-Image (T2I) diffusion models. Existing
Text-to-Video (T2V) methods are expensive to train and require large-scale
video datasets to produce diverse characters and motions. At the same time,
their zero-shot alternatives fail to produce temporally consistent videos. We
strive to bridge this gap, and we introduce a zero-shot approach that produces
temporally consistent videos of animated characters and requires no training or
fine-tuning. We leverage existing text-based motion diffusion models to
generate diverse motions that we utilize to guide a T2I model. To achieve
temporal consistency, we introduce the Spatial Latent Alignment module that
exploits cross-frame dense correspondences that we compute to align the latents
of the video frames. Furthermore, we propose Pixel-Wise Guidance to steer the
diffusion process in a direction that minimizes visual discrepancies. Our
proposed approach generates temporally consistent videos with diverse motions
and styles, outperforming existing zero-shot T2V approaches in terms of
pixel-wise consistency and user preference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eldesokey_A/0/1/0/all/0/1&quot;&gt;Abdelrahman Eldesokey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wonka_P/0/1/0/all/0/1&quot;&gt;Peter Wonka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07165">
<title>Language-Guided Transformer for Federated Multi-Label Classification. (arXiv:2312.07165v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07165</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) is an emerging paradigm that enables multiple users
to collaboratively train a robust model in a privacy-preserving manner without
sharing their private data. Most existing approaches of FL only consider
traditional single-label image classification, ignoring the impact when
transferring the task to multi-label image classification. Nevertheless, it is
still challenging for FL to deal with user heterogeneity in their local data
distribution in the real-world FL scenario, and this issue becomes even more
severe in multi-label image classification. Inspired by the recent success of
Transformers in centralized settings, we propose a novel FL framework for
multi-label classification. Since partial label correlation may be observed by
local clients during training, direct aggregation of locally updated models
would not produce satisfactory performances. Thus, we propose a novel FL
framework of Language-Guided Transformer (FedLGT) to tackle this challenging
task, which aims to exploit and transfer knowledge across different clients for
learning a robust global model. Through extensive experiments on various
multi-label datasets (e.g., FLAIR, MS-COCO, etc.), we show that our FedLGT is
able to achieve satisfactory performance and outperforms standard FL techniques
under multi-label FL scenarios. Code is available at
https://github.com/Jack24658735/FedLGT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_I/0/1/0/all/0/1&quot;&gt;I-Jieh Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Ci-Siang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Fu-En Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Chiang Frank Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07169">
<title>Semi-supervised Active Learning for Video Action Detection. (arXiv:2312.07169v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07169</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we focus on label efficient learning for video action
detection. We develop a novel semi-supervised active learning approach which
utilizes both labeled as well as unlabeled data along with informative sample
selection for action detection. Video action detection requires spatio-temporal
localization along with classification, which poses several challenges for both
active learning informative sample selection as well as semi-supervised
learning pseudo label generation. First, we propose NoiseAug, a simple
augmentation strategy which effectively selects informative samples for video
action detection. Next, we propose fft-attention, a novel technique based on
high-pass filtering which enables effective utilization of pseudo label for SSL
in video action detection by emphasizing on relevant activity region within a
video. We evaluate the proposed approach on three different benchmark datasets,
UCF-101-24, JHMDB-21, and Youtube-VOS. First, we demonstrate its effectiveness
on video action detection where the proposed approach outperforms prior works
in semi-supervised and weakly-supervised learning along with several baseline
approaches in both UCF101-24 and JHMDB-21. Next, we also show its effectiveness
on Youtube-VOS for video object segmentation demonstrating its generalization
capability for other dense prediction tasks in videos.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Aayush Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rana_A/0/1/0/all/0/1&quot;&gt;Aayush J Rana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Akash Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vyas_S/0/1/0/all/0/1&quot;&gt;Shruti Vyas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rawat_Y/0/1/0/all/0/1&quot;&gt;Yogesh Singh Rawat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07180">
<title>Context-Aware Iteration Policy Network for Efficient Optical Flow Estimation. (arXiv:2312.07180v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07180</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing recurrent optical flow estimation networks are computationally
expensive since they use a fixed large number of iterations to update the flow
field for each sample. An efficient network should skip iterations when the
flow improvement is limited. In this paper, we develop a Context-Aware
Iteration Policy Network for efficient optical flow estimation, which
determines the optimal number of iterations per sample. The policy network
achieves this by learning contextual information to realize whether flow
improvement is bottlenecked or minimal. On the one hand, we use iteration
embedding and historical hidden cell, which include previous iterations
information, to convey how flow has changed from previous iterations. On the
other hand, we use the incremental loss to make the policy network implicitly
perceive the magnitude of optical flow improvement in the subsequent iteration.
Furthermore, the computational complexity in our dynamic network is
controllable, allowing us to satisfy various resource preferences with a single
trained model. Our policy network can be easily integrated into
state-of-the-art optical flow networks. Extensive experiments show that our
method maintains performance while reducing FLOPs by about 40%/20% for the
Sintel/KITTI datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1&quot;&gt;Ri Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1&quot;&gt;Ruian He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xuhao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shili Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1&quot;&gt;Weimin Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1&quot;&gt;Bo Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07190">
<title>Noised Autoencoders for Point Annotation Restoration in Object Counting. (arXiv:2312.07190v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07190</link>
<description rdf:parseType="Literal">&lt;p&gt;Object counting is a field of growing importance in domains such as security
surveillance, urban planning, and biology. The annotation is usually provided
in terms of 2D points. However, the complexity of object shapes and subjective
of annotators may lead to annotation inconsistency, potentially confusing the
model during training. To alleviate this issue, we introduce the Noised
Autoencoders (NAE) methodology, which extracts general positional knowledge
from all annotations. The method involves adding random offsets to initial
point annotations, followed by a UNet to restore them to their original
positions. Similar to MAE, NAE faces challenges in restoring non-generic
points, necessitating reliance on the most common positions inferred from
general knowledge. This reliance forms the cornerstone of our method&apos;s
effectiveness. Different from existing noise-resistance methods, our approach
focus on directly improving initial point annotations. Extensive experiments
show that NAE yields more consistent annotations compared to the original ones,
steadily enhancing the performance of advanced models trained with these
revised annotations. \textbf{Remarkably, the proposed approach helps to set new
records in nine datasets}. We will make the NAE codes and refined point
annotations available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1&quot;&gt;Yuda Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1&quot;&gt;Xin Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1&quot;&gt;Peilin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhichao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1&quot;&gt;Bo Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yongchao Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07199">
<title>SeasFire as a Multivariate Earth System Datacube for Wildfire Dynamics. (arXiv:2312.07199v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07199</link>
<description rdf:parseType="Literal">&lt;p&gt;The global occurrence, scale, and frequency of wildfires pose significant
threats to ecosystem services and human livelihoods. To effectively quantify
and attribute the antecedent conditions for wildfires, a thorough understanding
of Earth system dynamics is imperative. In response, we introduce the SeasFire
datacube, a meticulously curated spatiotemporal dataset tailored for global
sub-seasonal to seasonal wildfire modeling via Earth observation. The SeasFire
datacube comprises of 59 variables encompassing climate, vegetation, oceanic
indices, and human factors, has an 8-day temporal resolution and a spatial
resolution of 0.25 degrees, and spans from 2001 to 2021. We showcase the
versatility of SeasFire for exploring the variability and seasonality of
wildfire drivers, modeling causal links between ocean-climate teleconnections
and wildfires, and predicting sub-seasonal wildfire patterns across multiple
timescales with a Deep Learning model. We publicly release the SeasFire
datacube and appeal to Earth system scientists and Machine Learning
practitioners to use it for an improved understanding and anticipation of
wildfires.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karasante_I/0/1/0/all/0/1&quot;&gt;Ilektra Karasante&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alonso_L/0/1/0/all/0/1&quot;&gt;Lazaro Alonso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prapas_I/0/1/0/all/0/1&quot;&gt;Ioannis Prapas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahuja_A/0/1/0/all/0/1&quot;&gt;Akanksha Ahuja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carvalhais_N/0/1/0/all/0/1&quot;&gt;Nuno Carvalhais&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papoutsis_I/0/1/0/all/0/1&quot;&gt;Ioannis Papoutsis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07207">
<title>MCFNet: Multi-scale Covariance Feature Fusion Network for Real-time Semantic Segmentation. (arXiv:2312.07207v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07207</link>
<description rdf:parseType="Literal">&lt;p&gt;The low-level spatial detail information and high-level semantic abstract
information are both essential to the semantic segmentation task. The features
extracted by the deep network can obtain rich semantic information, while a lot
of spatial information is lost. However, how to recover spatial detail
information effectively and fuse it with high-level semantics has not been well
addressed so far. In this paper, we propose a new architecture based on
Bilateral Segmentation Network (BiseNet) called Multi-scale Covariance Feature
Fusion Network (MCFNet). Specifically, this network introduces a new feature
refinement module and a new feature fusion module. Furthermore, a gating unit
named L-Gate is proposed to filter out invalid information and fuse multi-scale
features. We evaluate our proposed model on Cityscapes, CamVid datasets and
compare it with the state-of-the-art methods. Extensive experiments show that
our method achieves competitive success. On Cityscapes, we achieve 75.5% mIOU
with a speed of 151.3 FPS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1&quot;&gt;Xiaojie Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1&quot;&gt;Xingguo Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1&quot;&gt;Xiangyin Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1&quot;&gt;Xu Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1&quot;&gt;Sheng Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07221">
<title>Transferring CLIP&apos;s Knowledge into Zero-Shot Point Cloud Semantic Segmentation. (arXiv:2312.07221v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07221</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional 3D segmentation methods can only recognize a fixed range of
classes that appear in the training set, which limits their application in
real-world scenarios due to the lack of generalization ability. Large-scale
visual-language pre-trained models, such as CLIP, have shown their
generalization ability in the zero-shot 2D vision tasks, but are still unable
to be applied to 3D semantic segmentation directly. In this work, we focus on
zero-shot point cloud semantic segmentation and propose a simple yet effective
baseline to transfer the visual-linguistic knowledge implied in CLIP to point
cloud encoder at both feature and output levels. Both feature-level and
output-level alignments are conducted between 2D and 3D encoders for effective
knowledge transfer. Concretely, a Multi-granularity Cross-modal Feature
Alignment (MCFA) module is proposed to align 2D and 3D features from global
semantic and local position perspectives for feature-level alignment. For the
output level, per-pixel pseudo labels of unseen classes are extracted using the
pre-trained CLIP model as supervision for the 3D segmentation model to mimic
the behavior of the CLIP image encoder. Extensive experiments are conducted on
two popular benchmarks of point cloud segmentation. Our method outperforms
significantly previous state-of-the-art methods under zero-shot setting (+29.2%
mIoU on SemanticKITTI and 31.8% mIoU on nuScenes), and further achieves
promising results in the annotation-free point cloud semantic segmentation
setting, showing its great potential for label-efficient learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuanbin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shaofei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yulu Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_K/0/1/0/all/0/1&quot;&gt;Kehua Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Si Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07226">
<title>Super-Resolution on Rotationally Scanned Photoacoustic Microscopy Images Incorporating Scanning Prior. (arXiv:2312.07226v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.07226</link>
<description rdf:parseType="Literal">&lt;p&gt;Photoacoustic Microscopy (PAM) images integrating the advantages of optical
contrast and acoustic resolution have been widely used in brain studies.
However, there exists a trade-off between scanning speed and image resolution.
Compared with traditional raster scanning, rotational scanning provides good
opportunities for fast PAM imaging by optimizing the scanning mechanism.
Recently, there is a trend to incorporate deep learning into the scanning
process to further increase the scanning speed.Yet, most such attempts are
performed for raster scanning while those for rotational scanning are
relatively rare. In this study, we propose a novel and well-performing
super-resolution framework for rotational scanning-based PAM imaging. To
eliminate adjacent rows&apos; displacements due to subject motion or high-frequency
scanning distortion,we introduce a registration module across odd and even rows
in the preprocessing and incorporate displacement degradation in the training.
Besides, gradient-based patch selection is proposed to increase the probability
of blood vessel patches being selected for training. A Transformer-based
network with a global receptive field is applied for better performance.
Experimental results on both synthetic and real datasets demonstrate the
effectiveness and generalizability of our proposed framework for rotationally
scanned PAM images&apos;super-resolution, both quantitatively and qualitatively.
Code is available at https://github.&lt;a href=&quot;/abs/com/1171061&quot;&gt;com/1171061&lt;/a&gt;5/PAMSR.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pan_K/0/1/0/all/0/1&quot;&gt;Kai Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Linyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Li Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cheng_P/0/1/0/all/0/1&quot;&gt;Pujin Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lyu_J/0/1/0/all/0/1&quot;&gt;Junyan Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xi_L/0/1/0/all/0/1&quot;&gt;Lei Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tang_X/0/1/0/all/0/1&quot;&gt;Xiaoyin Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07231">
<title>Fast Training of Diffusion Transformer with Extreme Masking for 3D Point Clouds Generation. (arXiv:2312.07231v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07231</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion Transformers have recently shown remarkable effectiveness in
generating high-quality 3D point clouds. However, training voxel-based
diffusion models for high-resolution 3D voxels remains prohibitively expensive
due to the cubic complexity of attention operators, which arises from the
additional dimension of voxels. Motivated by the inherent redundancy of 3D
compared to 2D, we propose FastDiT-3D, a novel masked diffusion transformer
tailored for efficient 3D point cloud generation, which greatly reduces
training costs. Specifically, we draw inspiration from masked autoencoders to
dynamically operate the denoising process on masked voxelized point clouds. We
also propose a novel voxel-aware masking strategy to adaptively aggregate
background/foreground information from voxelized point clouds. Our method
achieves state-of-the-art performance with an extreme masking ratio of nearly
99%. Moreover, to improve multi-category 3D generation, we introduce
Mixture-of-Expert (MoE) in 3D diffusion model. Each category can learn a
distinct diffusion path with different experts, relieving gradient conflict.
Experimental results on the ShapeNet dataset demonstrate that our method
achieves state-of-the-art high-fidelity and diverse 3D point cloud generation
performance. Our FastDiT-3D improves 1-Nearest Neighbor Accuracy and Coverage
metrics when generating 128-resolution voxel point clouds, using only 6.5% of
the original training cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1&quot;&gt;Shentong Mo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1&quot;&gt;Enze Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yue Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junsong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1&quot;&gt;Matthias Nie&amp;#xdf;ner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenguo Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07245">
<title>DTA: Distribution Transform-based Attack for Query-Limited Scenario. (arXiv:2312.07245v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07245</link>
<description rdf:parseType="Literal">&lt;p&gt;In generating adversarial examples, the conventional black-box attack methods
rely on sufficient feedback from the to-be-attacked models by repeatedly
querying until the attack is successful, which usually results in thousands of
trials during an attack. This may be unacceptable in real applications since
Machine Learning as a Service Platform (MLaaS) usually only returns the final
result (i.e., hard-label) to the client and a system equipped with certain
defense mechanisms could easily detect malicious queries. By contrast, a
feasible way is a hard-label attack that simulates an attacked action being
permitted to conduct a limited number of queries. To implement this idea, in
this paper, we bypass the dependency on the to-be-attacked model and benefit
from the characteristics of the distributions of adversarial examples to
reformulate the attack problem in a distribution transform manner and propose a
distribution transform-based attack (DTA). DTA builds a statistical mapping
from the benign example to its adversarial counterparts by tackling the
conditional likelihood under the hard-label black-box settings. In this way, it
is no longer necessary to query the target model frequently. A well-trained DTA
model can directly and efficiently generate a batch of adversarial examples for
a certain input, which can be used to attack un-seen models based on the
assumed transferability. Furthermore, we surprisingly find that the
well-trained DTA model is not sensitive to the semantic spaces of the training
dataset, meaning that the model yields acceptable attack performance on other
datasets. Extensive experiments validate the effectiveness of the proposed idea
and the superiority of DTA over the state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Renyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1&quot;&gt;Xin Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Song Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuanyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruxin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07246">
<title>Unifying Correspondence, Pose and NeRF for Pose-Free Novel View Synthesis from Stereo Pairs. (arXiv:2312.07246v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07246</link>
<description rdf:parseType="Literal">&lt;p&gt;This work delves into the task of pose-free novel view synthesis from stereo
pairs, a challenging and pioneering task in 3D vision. Our innovative
framework, unlike any before, seamlessly integrates 2D correspondence matching,
camera pose estimation, and NeRF rendering, fostering a synergistic enhancement
of these tasks. We achieve this through designing an architecture that utilizes
a shared representation, which serves as a foundation for enhanced 3D geometry
understanding. Capitalizing on the inherent interplay between the tasks, our
unified framework is trained end-to-end with the proposed training strategy to
improve overall model accuracy. Through extensive evaluations across diverse
indoor and outdoor scenes from two real-world datasets, we demonstrate that our
approach achieves substantial improvement over previous methodologies,
especially in scenarios characterized by extreme viewpoint changes and the
absence of accurate camera poses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1&quot;&gt;Sunghwan Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1&quot;&gt;Jaewoo Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_H/0/1/0/all/0/1&quot;&gt;Heeseong Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jiaolong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seungryong Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1&quot;&gt;Chong Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07255">
<title>GIST: Improving Parameter Efficient Fine Tuning via Knowledge Interaction. (arXiv:2312.07255v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07255</link>
<description rdf:parseType="Literal">&lt;p&gt;The Parameter-Efficient Fine-Tuning (PEFT) method, which adjusts or
introduces fewer trainable parameters to calibrate pre-trained models on
downstream tasks, has become a recent research interest. However, existing PEFT
methods within the traditional fine-tiuning framework have two main
shortcomings: 1) They overlook the explicit association between trainable
parameters and downstream task knowledge. 2) They neglect the interaction
between the intrinsic task-agnostic knowledge of pre-trained models and the
task-specific knowledge in downstream tasks. To address this gap, we propose a
novel fine-tuning framework, named GIST, in a plug-and-play manner.
Specifically, our framework first introduces a trainable token, called the Gist
token, when applying PEFT methods on downstream tasks. This token serves as an
aggregator of the task-specific knowledge learned by the PEFT methods and forms
an explicit association with downstream knowledge. Furthermore, to facilitate
explicit interaction between task-agnostic and task-specific knowledge, we
introduce the concept of Knowledge Interaction via a Bidirectional
Kullback-Leibler Divergence objective. As a result, PEFT methods within our
framework can make the pre-trained model understand downstream tasks more
comprehensively by leveraging the knowledge interaction. Extensive experiments
demonstrate the universality and scalability of our framework. Notably, on the
VTAB-1K benchmark, we employ the Adapter (a prevalent PEFT method) within our
GIST framework and achieve a performance boost of 2.25%, with an increase of
only 0.8K parameters. The Code will be released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruan_J/0/1/0/all/0/1&quot;&gt;Jiacheng Ruan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jingsheng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1&quot;&gt;Mingye Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1&quot;&gt;Suncheng Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zefang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Ting Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yuzhuo Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07258">
<title>SSTA: Salient Spatially Transformed Attack. (arXiv:2312.07258v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07258</link>
<description rdf:parseType="Literal">&lt;p&gt;Extensive studies have demonstrated that deep neural networks (DNNs) are
vulnerable to adversarial attacks, which brings a huge security risk to the
further application of DNNs, especially for the AI models developed in the real
world. Despite the significant progress that has been made recently, existing
attack methods still suffer from the unsatisfactory performance of escaping
from being detected by naked human eyes due to the formulation of adversarial
example (AE) heavily relying on a noise-adding manner. Such mentioned
challenges will significantly increase the risk of exposure and result in an
attack to be failed. Therefore, in this paper, we propose the Salient Spatially
Transformed Attack (SSTA), a novel framework to craft imperceptible AEs, which
enhance the stealthiness of AEs by estimating a smooth spatial transform metric
on a most critical area to generate AEs instead of adding external noise to the
whole image. Compared to state-of-the-art baselines, extensive experiments
indicated that SSTA could effectively improve the imperceptibility of the AEs
while maintaining a 100\% attack success rate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Renyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Sixin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jun Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1&quot;&gt;Kwok-Yan Lam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07264">
<title>Dual Structure-Preserving Image Filterings for Semi-supervised Medical Image Segmentation. (arXiv:2312.07264v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07264</link>
<description rdf:parseType="Literal">&lt;p&gt;Semi-supervised image segmentation has attracted great attention recently.
The key is how to leverage unlabeled images in the training process. Most
methods maintain consistent predictions of the unlabeled images under
variations (e.g., adding noise/perturbations, or creating alternative versions)
in the image and/or model level. In most image-level variation, medical images
often have prior structure information, which has not been well explored. In
this paper, we propose novel dual structure-preserving image filterings (DSPIF)
as the image-level variations for semi-supervised medical image segmentation.
Motivated by connected filtering that simplifies image via filtering in
structure-aware tree-based image representation, we resort to the dual contrast
invariant Max-tree and Min-tree representation. Specifically, we propose a
novel connected filtering that removes topologically equivalent nodes (i.e.
connected components) having no siblings in the Max/Min-tree. This results in
two filtered images preserving topologically critical structure. Applying such
dual structure-preserving image filterings in mutual supervision is beneficial
for semi-supervised medical image segmentation. Extensive experimental results
on three benchmark datasets demonstrate that the proposed method
significantly/consistently outperforms some state-of-the-art methods. The
source codes will be publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yuliang Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhichao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1&quot;&gt;Xin Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1&quot;&gt;Yuda Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zelong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yongchao Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07266">
<title>ProxyDet: Synthesizing Proxy Novel Classes via Classwise Mixup for Open Vocabulary Object Detection. (arXiv:2312.07266v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07266</link>
<description rdf:parseType="Literal">&lt;p&gt;Open-vocabulary object detection (OVOD) aims to recognize novel objects whose
categories are not included in training set. In order to classify these unseen
classes during training, many OVOD frameworks leverage the zero-shot capability
of largely pretrained vision and language models, such as CLIP. To further
improve generalization on the unseen novel classes, several approaches proposed
to additionally train with pseudo region labeling on the external data sources
that contain a substantial number of novel category labels beyond the existing
training data. Albeit its simplicity, these pseudo-labeling methods still
exhibit limited improvement with regard to the genuine novel classes that were
not pseudo-labeled. In this paper, we present a novel, yet simple technique
that helps generalization on the overall distribution of novel classes.
Inspired by our observation that numerous novel classes reside within the
convex hull constructed by the base (seen) classes in the CLIP embedding space,
we propose to synthesize proxy-novel classes approximating novel classes via
linear mixup between a pair of base classes. By training our detector with
these synthetic proxy-novel classes, we effectively explore the embedding space
of novel classes. The experimental results on various OVOD benchmarks such as
LVIS and COCO demonstrate superior performance on novel classes compared to the
other state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1&quot;&gt;Joonhyun Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_G/0/1/0/all/0/1&quot;&gt;Geondo Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1&quot;&gt;Jayeon Yoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1&quot;&gt;Hyungsik Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Heesu Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07273">
<title>Benchmarking Pretrained Vision Embeddings for Near- and Duplicate Detection in Medical Images. (arXiv:2312.07273v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07273</link>
<description rdf:parseType="Literal">&lt;p&gt;Near- and duplicate image detection is a critical concern in the field of
medical imaging. Medical datasets often contain similar or duplicate images
from various sources, which can lead to significant performance issues and
evaluation biases, especially in machine learning tasks due to data leakage
between training and testing subsets. In this paper, we present an approach for
identifying near- and duplicate 3D medical images leveraging publicly available
2D computer vision embeddings. We assessed our approach by comparing embeddings
extracted from two state-of-the-art self-supervised pretrained models and two
different vector index structures for similarity retrieval. We generate an
experimental benchmark based on the publicly available Medical Segmentation
Decathlon dataset. The proposed method yields promising results for near- and
duplicate image detection achieving a mean sensitivity and specificity of
0.9645 and 0.8559, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1&quot;&gt;Tuan Truong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jush_F/0/1/0/all/0/1&quot;&gt;Farnaz Khun Jush&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lenga_M/0/1/0/all/0/1&quot;&gt;Matthias Lenga&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07311">
<title>Scalable Motion Style Transfer with Constrained Diffusion Generation. (arXiv:2312.07311v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07311</link>
<description rdf:parseType="Literal">&lt;p&gt;Current training of motion style transfer systems relies on consistency
losses across style domains to preserve contents, hindering its scalable
application to a large number of domains and private data. Recent image
transfer works show the potential of independent training on each domain by
leveraging implicit bridging between diffusion models, with the content
preservation, however, limited to simple data patterns. We address this by
imposing biased sampling in backward diffusion while maintaining the domain
independence in the training stage. We construct the bias from the source
domain keyframes and apply them as the gradient of content constraints,
yielding a framework with keyframe manifold constraint gradients (KMCGs). Our
validation demonstrates the success of training separate models to transfer
between as many as ten dance motion styles. Comprehensive experiments find a
significant improvement in preserving motion contents in comparison to baseline
and ablative diffusion-based style transfer models. In addition, we perform a
human study for a subjective assessment of the quality of generated dance
motions. The results validate the competitiveness of KMCGs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1&quot;&gt;Wenjie Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1&quot;&gt;Hang Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kragic_D/0/1/0/all/0/1&quot;&gt;Danica Kragic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bjorkman_M/0/1/0/all/0/1&quot;&gt;M&amp;#xe5;rten Bj&amp;#xf6;rkman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07315">
<title>NVS-Adapter: Plug-and-Play Novel View Synthesis from a Single Image. (arXiv:2312.07315v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07315</link>
<description rdf:parseType="Literal">&lt;p&gt;Transfer learning of large-scale Text-to-Image (T2I) models has recently
shown impressive potential for Novel View Synthesis (NVS) of diverse objects
from a single image. While previous methods typically train large models on
multi-view datasets for NVS, fine-tuning the whole parameters of T2I models not
only demands a high cost but also reduces the generalization capacity of T2I
models in generating diverse images in a new domain. In this study, we propose
an effective method, dubbed NVS-Adapter, which is a plug-and-play module for a
T2I model, to synthesize novel multi-views of visual objects while fully
exploiting the generalization capacity of T2I models. NVS-Adapter consists of
two main components; view-consistency cross-attention learns the visual
correspondences to align the local details of view features, and global
semantic conditioning aligns the semantic structure of generated views with the
reference view. Experimental results demonstrate that the NVS-Adapter can
effectively synthesize geometrically consistent multi-views and also achieve
high performance on benchmarks without full fine-tuning of T2I models. The code
and data are publicly available in
~\href{https://postech-cvlab.github.io/nvsadapter/}{https://postech-cvlab.github.io/nvsadapter/}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1&quot;&gt;Yoonwoo Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jinwoo Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1&quot;&gt;Chiheon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1&quot;&gt;Minsu Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Doyup Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07322">
<title>GenHowTo: Learning to Generate Actions and State Transformations from Instructional Videos. (arXiv:2312.07322v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07322</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the task of generating temporally consistent and physically
plausible images of actions and object state transformations. Given an input
image and a text prompt describing the targeted transformation, our generated
images preserve the environment and transform objects in the initial image. Our
contributions are threefold. First, we leverage a large body of instructional
videos and automatically mine a dataset of triplets of consecutive frames
corresponding to initial object states, actions, and resulting object
transformations. Second, equipped with this data, we develop and train a
conditioned diffusion model dubbed GenHowTo. Third, we evaluate GenHowTo on a
variety of objects and actions and show superior performance compared to
existing methods. In particular, we introduce a quantitative evaluation where
GenHowTo achieves 88% and 74% on seen and unseen interaction categories,
respectively, outperforming prior work by a large margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soucek_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;&amp;#x161; Sou&amp;#x10d;ek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damen_D/0/1/0/all/0/1&quot;&gt;Dima Damen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wray_M/0/1/0/all/0/1&quot;&gt;Michael Wray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1&quot;&gt;Ivan Laptev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1&quot;&gt;Josef Sivic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07327">
<title>Adaptive Confidence Multi-View Hashing for Multimedia Retrieval. (arXiv:2312.07327v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07327</link>
<description rdf:parseType="Literal">&lt;p&gt;The multi-view hash method converts heterogeneous data from multiple views
into binary hash codes, which is one of the critical technologies in multimedia
retrieval. However, the current methods mainly explore the complementarity
among multiple views while lacking confidence learning and fusion. Moreover, in
practical application scenarios, the single-view data contain redundant noise.
To conduct the confidence learning and eliminate unnecessary noise, we propose
a novel Adaptive Confidence Multi-View Hashing (ACMVH) method. First, a
confidence network is developed to extract useful information from various
single-view features and remove noise information. Furthermore, an adaptive
confidence multi-view network is employed to measure the confidence of each
view and then fuse multi-view features through a weighted summation. Lastly, a
dilation network is designed to further enhance the feature representation of
the fused features. To the best of our knowledge, we pioneer the application of
confidence learning into the field of multimedia retrieval. Extensive
experiments on two public datasets show that the proposed ACMVH performs better
than state-of-the-art methods (maximum increase of 3.24%). The source code is
available at https://github.com/HackerHyper/ACMVH.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jian Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1&quot;&gt;Yu Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhangmin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xingyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_L/0/1/0/all/0/1&quot;&gt;Lingfang Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_L/0/1/0/all/0/1&quot;&gt;Li-Rong Dai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07330">
<title>Learned representation-guided diffusion models for large-image generation. (arXiv:2312.07330v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07330</link>
<description rdf:parseType="Literal">&lt;p&gt;To synthesize high-fidelity samples, diffusion models typically require
auxiliary data to guide the generation process. However, it is impractical to
procure the painstaking patch-level annotation effort required in specialized
domains like histopathology and satellite imagery; it is often performed by
domain experts and involves hundreds of millions of patches. Modern-day
self-supervised learning (SSL) representations encode rich semantic and visual
information. In this paper, we posit that such representations are expressive
enough to act as proxies to fine-grained human labels. We introduce a novel
approach that trains diffusion models conditioned on embeddings from SSL. Our
diffusion models successfully project these features back to high-quality
histopathology and remote sensing images. In addition, we construct larger
images by assembling spatially consistent patches inferred from SSL embeddings,
preserving long-range dependencies. Augmenting real data by generating
variations of real images improves downstream classifier accuracy for
patch-level and larger, image-scale classification tasks. Our models are
effective even on datasets not encountered during training, demonstrating their
robustness and generalizability. Generating images from learned embeddings is
agnostic to the source of the embeddings. The SSL embeddings used to generate a
large image can either be extracted from a reference image, or sampled from an
auxiliary model conditioned on any related modality (e.g. class labels, text,
genomic data). As proof of concept, we introduce the text-to-large image
synthesis paradigm where we successfully synthesize large pathology and
satellite images out of text descriptions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Graikos_A/0/1/0/all/0/1&quot;&gt;Alexandros Graikos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yellapragada_S/0/1/0/all/0/1&quot;&gt;Srikar Yellapragada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_M/0/1/0/all/0/1&quot;&gt;Minh-Quan Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kapse_S/0/1/0/all/0/1&quot;&gt;Saarthak Kapse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prasanna_P/0/1/0/all/0/1&quot;&gt;Prateek Prasanna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saltz_J/0/1/0/all/0/1&quot;&gt;Joel Saltz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samaras_D/0/1/0/all/0/1&quot;&gt;Dimitris Samaras&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07331">
<title>Coupled Confusion Correction: Learning from Crowds with Sparse Annotations. (arXiv:2312.07331v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07331</link>
<description rdf:parseType="Literal">&lt;p&gt;As the size of the datasets getting larger, accurately annotating such
datasets is becoming more impractical due to the expensiveness on both time and
economy. Therefore, crowd-sourcing has been widely adopted to alleviate the
cost of collecting labels, which also inevitably introduces label noise and
eventually degrades the performance of the model. To learn from crowd-sourcing
annotations, modeling the expertise of each annotator is a common but
challenging paradigm, because the annotations collected by crowd-sourcing are
usually highly-sparse. To alleviate this problem, we propose Coupled Confusion
Correction (CCC), where two models are simultaneously trained to correct the
confusion matrices learned by each other. Via bi-level optimization, the
confusion matrices learned by one model can be corrected by the distilled data
from the other. Moreover, we cluster the ``annotator groups&apos;&apos; who share similar
expertise so that their confusion matrices could be corrected together. In this
way, the expertise of the annotators, especially of those who provide seldom
labels, could be better captured. Remarkably, we point out that the annotation
sparsity not only means the average number of labels is low, but also there are
always some annotators who provide very few labels, which is neglected by
previous works when constructing synthetic crowd-sourcing annotations. Based on
that, we propose to use Beta distribution to control the generation of the
crowd-sourcing labels so that the synthetic annotations could be more
consistent with the real-world ones. Extensive experiments are conducted on two
types of synthetic datasets and three real-world datasets, the results of which
demonstrate that CCC significantly outperforms state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hansong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shikun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1&quot;&gt;Dan Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1&quot;&gt;Chenggang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1&quot;&gt;Shiming Ge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07342">
<title>Expand-and-Quantize: Unsupervised Semantic Segmentation Using High-Dimensional Space and Product Quantization. (arXiv:2312.07342v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07342</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised semantic segmentation (USS) aims to discover and recognize
meaningful categories without any labels. For a successful USS, two key
abilities are required: 1) information compression and 2) clustering
capability. Previous methods have relied on feature dimension reduction for
information compression, however, this approach may hinder the process of
clustering. In this paper, we propose a novel USS framework called
Expand-and-Quantize Unsupervised Semantic Segmentation (EQUSS), which combines
the benefits of high-dimensional spaces for better clustering and product
quantization for effective information compression. Our extensive experiments
demonstrate that EQUSS achieves state-of-the-art results on three standard
benchmarks. In addition, we analyze the entropy of USS features, which is the
first step towards understanding USS from the perspective of information
theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jiyoung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shim_K/0/1/0/all/0/1&quot;&gt;Kyuhong Shim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1&quot;&gt;Insu Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shim_B/0/1/0/all/0/1&quot;&gt;Byonghyo Shim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07352">
<title>CholecTrack20: A Dataset for Multi-Class Multiple Tool Tracking in Laparoscopic Surgery. (arXiv:2312.07352v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07352</link>
<description rdf:parseType="Literal">&lt;p&gt;Tool tracking in surgical videos is vital in computer-assisted intervention
for tasks like surgeon skill assessment, safety zone estimation, and
human-machine collaboration during minimally invasive procedures. The lack of
large-scale datasets hampers Artificial Intelligence implementation in this
domain. Current datasets exhibit overly generic tracking formalization, often
lacking surgical context: a deficiency that becomes evident when tools move out
of the camera&apos;s scope, resulting in rigid trajectories that hinder realistic
surgical representation. This paper addresses the need for a more precise and
adaptable tracking formalization tailored to the intricacies of endoscopic
procedures by introducing CholecTrack20, an extensive dataset meticulously
annotated for multi-class multi-tool tracking across three perspectives
representing the various ways of considering the temporal duration of a tool
trajectory: (1) intraoperative, (2) intracorporeal, and (3) visibility within
the camera&apos;s scope. The dataset comprises 20 laparoscopic videos with over
35,000 frames and 65,000 annotated tool instances with details on spatial
location, category, identity, operator, phase, and surgical visual conditions.
This detailed dataset caters to the evolving assistive requirements within a
procedure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nwoye_C/0/1/0/all/0/1&quot;&gt;Chinedu Innocent Nwoye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elgohary_K/0/1/0/all/0/1&quot;&gt;Kareem Elgohary&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivas_A/0/1/0/all/0/1&quot;&gt;Anvita Srinivas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaid_F/0/1/0/all/0/1&quot;&gt;Fauzan Zaid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lavanchy_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xeb;l L. Lavanchy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1&quot;&gt;Nicolas Padoy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07353">
<title>CLIP in Medical Imaging: A Comprehensive Survey. (arXiv:2312.07353v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07353</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive Language-Image Pre-training (CLIP), a straightforward yet
effective pre-training paradigm, successfully introduces semantic-rich text
supervision to vision models and has demonstrated promising results in various
tasks due to its generalizability and interpretability. It has recently gained
increasing interest in the medical imaging domain, either as a powerful
pre-training paradigm for medical vision language alignment or a pre-trained
key component for various clinical tasks. With the aim of facilitating a deeper
understanding of this promising direction, this survey offers an in-depth
exploration of the CLIP paradigm within the domain of medical imaging,
regarding both refined CLIP pre-training and CLIP-driven applications. Our
survey (1) starts with a brief introduction to the fundamentals of CLIP
methodology. (2) Then, we investigate the adaptation of CLIP pre-training in
the medical domain, focusing on how to optimize CLIP given characteristics of
medical images and reports. (3) Furthermore, we explore the practical
utilization of CLIP pre-trained models in various tasks, including
classification, dense prediction, and cross-modal tasks. (4) Finally, we
discuss existing limitations of CLIP in the context of medical imaging and
propose forward-looking directions to address the demands of medical imaging
domain. We expect that this comprehensive survey will provide researchers in
the field of medical image analysis with a holistic understanding of the CLIP
paradigm and its potential implications. The project page is available at
https://github.com/zhaozh10/Awesome-CLIP-in-Medical-Imaging, which will be
regularly updated.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zihao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuxiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Han Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yonghao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teng_L/0/1/0/all/0/1&quot;&gt;Lin Teng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Disheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1&quot;&gt;Zhiming Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1&quot;&gt;Dinggang Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07357">
<title>Automatic coral reef fish identification and 3D measurement in the wild. (arXiv:2312.07357v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07357</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we present a pipeline using stereo images in order to
automatically identify, track in 3D fish, and measure fish population.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barrelet_C/0/1/0/all/0/1&quot;&gt;Cyril Barrelet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaumont_M/0/1/0/all/0/1&quot;&gt;Marc Chaumont&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subsol_G/0/1/0/all/0/1&quot;&gt;G&amp;#xe9;rard Subsol&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07360">
<title>Boosting Latent Diffusion with Flow Matching. (arXiv:2312.07360v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07360</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, there has been tremendous progress in visual synthesis and the
underlying generative models. Here, diffusion models (DMs) stand out
particularly, but lately, flow matching (FM) has also garnered considerable
interest. While DMs excel in providing diverse images, they suffer from long
training and slow generation. With latent diffusion, these issues are only
partially alleviated. Conversely, FM offers faster training and inference but
exhibits less diversity in synthesis. We demonstrate that introducing FM
between the Diffusion model and the convolutional decoder offers
high-resolution image synthesis with reduced computational cost and model size.
Diffusion can then efficiently provide the necessary generation diversity. FM
compensates for the lower resolution, mapping the small latent space to a
high-dimensional one. Subsequently, the convolutional decoder of the LDM maps
these latents to high-resolution images. By combining the diversity of DMs, the
efficiency of FMs, and the effectiveness of convolutional decoders, we achieve
state-of-the-art high-resolution image synthesis at $1024^2$ with minimal
computational cost. Importantly, our approach is orthogonal to recent
approximation and speed-up strategies for the underlying DMs, making it easily
integrable into various DM frameworks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fischer_J/0/1/0/all/0/1&quot;&gt;Johannes S. Fischer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gui_M/0/1/0/all/0/1&quot;&gt;Ming Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1&quot;&gt;Pingchuan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stracke_N/0/1/0/all/0/1&quot;&gt;Nick Stracke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baumann_S/0/1/0/all/0/1&quot;&gt;Stefan A. Baumann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1&quot;&gt;Bj&amp;#xf6;rn Ommer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07364">
<title>Collapse-Oriented Adversarial Training with Triplet Decoupling for Robust Image Retrieval. (arXiv:2312.07364v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07364</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial training has achieved substantial performance in defending image
retrieval systems against adversarial examples. However, existing studies still
suffer from two major limitations: model collapse and weak adversary. This
paper addresses these two limitations by proposing collapse-oriented (COLO)
adversarial training with triplet decoupling (TRIDE). Specifically, COLO
prevents model collapse by temporally orienting the perturbation update
direction with a new collapse metric, while TRIDE yields a strong adversary by
spatially decoupling the update targets of perturbation into the anchor and the
two candidates of a triplet. Experimental results demonstrate that our
COLO-TRIDE outperforms the current state of the art by 7% on average over 10
robustness metrics and across 3 popular datasets. In addition, we identify the
fairness limitations of commonly used robustness metrics in image retrieval and
propose a new metric for more meaningful robustness evaluation. Codes will be
made publicly available on GitHub.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1&quot;&gt;Qiwei Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chenhao Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhengyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Chao Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07370">
<title>Adversarial Semi-Supervised Domain Adaptation for Semantic Segmentation: A New Role for Labeled Target Samples. (arXiv:2312.07370v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07370</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial learning baselines for domain adaptation (DA) approaches in the
context of semantic segmentation are under explored in semi-supervised
framework. These baselines involve solely the available labeled target samples
in the supervision loss. In this work, we propose to enhance their usefulness
on both semantic segmentation and the single domain classifier neural networks.
We design new training objective losses for cases when labeled target data
behave as source samples or as real target samples. The underlying rationale is
that considering the set of labeled target samples as part of source domain
helps reducing the domain discrepancy and, hence, improves the contribution of
the adversarial loss. To support our approach, we consider a complementary
method that mixes source and labeled target data, then applies the same
adaptation process. We further propose an unsupervised selection procedure
using entropy to optimize the choice of labeled target samples for adaptation.
We illustrate our findings through extensive experiments on the benchmarks
GTA5, SYNTHIA, and Cityscapes. The empirical evaluation highlights competitive
performance of our proposed approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kechaou_M/0/1/0/all/0/1&quot;&gt;Marwa Kechaou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alaya_M/0/1/0/all/0/1&quot;&gt;Mokhtar Z. Alaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herault_R/0/1/0/all/0/1&quot;&gt;Romain H&amp;#xe9;rault&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gasso_G/0/1/0/all/0/1&quot;&gt;Gilles Gasso&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07374">
<title>Relax Image-Specific Prompt Requirement in SAM: A Single Generic Prompt for Segmenting Camouflaged Objects. (arXiv:2312.07374v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07374</link>
<description rdf:parseType="Literal">&lt;p&gt;Camouflaged object detection (COD) approaches heavily rely on pixel-level
annotated datasets. Weakly-supervised COD (WSCOD) approaches use sparse
annotations like scribbles or points to reduce annotation effort, but this can
lead to decreased accuracy. The Segment Anything Model (SAM) shows remarkable
segmentation ability with sparse prompts like points. However, manual prompt is
not always feasible, as it may not be accessible in real-world application.
Additionally, it only provides localization information instead of semantic
one, which can intrinsically cause ambiguity in interpreting the targets. In
this work, we aim to eliminate the need for manual prompt. The key idea is to
employ Cross-modal Chains of Thought Prompting (CCTP) to reason visual prompts
using the semantic information given by a generic text prompt.To that end, we
introduce a test-time adaptation per-instance mechanism called Generalizable
SAM (GenSAM) to automatically enerate and optimize visual prompts the generic
task prompt for WSCOD. In particular, CCTP maps a single generic text prompt
onto image-specific consensus foreground and background heatmaps using
vision-language models, acquiring reliable visual prompts. Moreover, to
test-time adapt the visual prompts, we further propose Progressive Mask
Generation (PMG) to iteratively reweight the input image, guiding the model to
focus on the targets in a coarse-to-fine manner. Crucially, all network
parameters are fixed, avoiding the need for additional training. Experiments
demonstrate the superiority of GenSAM. Experiments on three benchmarks
demonstrate that GenSAM outperforms point supervision approaches and achieves
comparable results to scribble supervision ones, solely relying on general task
descriptions as prompts. our codes is in: https://lwpyh.github.io/GenSAM/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jian Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jiayi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1&quot;&gt;Weitong Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1&quot;&gt;Shaogang Gong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07378">
<title>X4D-SceneFormer: Enhanced Scene Understanding on 4D Point Cloud Videos through Cross-modal Knowledge Transfer. (arXiv:2312.07378v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07378</link>
<description rdf:parseType="Literal">&lt;p&gt;The field of 4D point cloud understanding is rapidly developing with the goal
of analyzing dynamic 3D point cloud sequences. However, it remains a
challenging task due to the sparsity and lack of texture in point clouds.
Moreover, the irregularity of point cloud poses a difficulty in aligning
temporal information within video sequences. To address these issues, we
propose a novel cross-modal knowledge transfer framework, called
X4D-SceneFormer. This framework enhances 4D-Scene understanding by transferring
texture priors from RGB sequences using a Transformer architecture with
temporal relationship mining. Specifically, the framework is designed with a
dual-branch architecture, consisting of an 4D point cloud transformer and a
Gradient-aware Image Transformer (GIT). During training, we employ multiple
knowledge transfer techniques, including temporal consistency losses and masked
self-attention, to strengthen the knowledge transfer between modalities. This
leads to enhanced performance during inference using single-modal 4D point
cloud inputs. Extensive experiments demonstrate the superior performance of our
framework on various 4D point cloud video understanding tasks, including action
recognition, action segmentation and semantic segmentation. The results achieve
1st places, i.e., 85.3% (+7.9%) accuracy and 47.3% (+5.0%) mIoU for 4D action
segmentation and semantic segmentation, on the HOI4D
challenge\footnote{\url{&lt;a href=&quot;http://www.hoi4d.top/&quot;&gt;this http URL&lt;/a&gt;}.}, outperforming previous
state-of-the-art by a large margin. We release the code at
https://github.com/jinglinglingling/X4D
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1&quot;&gt;Linglin Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1&quot;&gt;Ying Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1&quot;&gt;Xu Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Chaoda Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruimao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhigang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1&quot;&gt;Hui Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1&quot;&gt;Bin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhen Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07381">
<title>ScribblePrompt: Fast and Flexible Interactive Segmentation for Any Medical Image. (arXiv:2312.07381v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07381</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic medical image segmentation is a crucial part of both scientific
research and clinical care. With enough labelled data, deep learning models can
be trained to accurately automate specific medical image segmentation tasks.
However, manually segmenting images to create training data is highly labor
intensive. In this paper, we present ScribblePrompt, an interactive
segmentation framework for medical imaging that enables human annotators to
segment unseen structures using scribbles, clicks, and bounding boxes.
Scribbles are an intuitive and effective form of user interaction for complex
tasks, however most existing methods focus on click-based interactions. We
introduce algorithms for simulating realistic scribbles that enable training
models that are amenable to multiple types of interaction. To achieve
generalization to new tasks, we train on a diverse collection of 65 open-access
biomedical datasets -- using both real and synthetic labels. We test
ScribblePrompt on multiple network architectures and unseen datasets, and
demonstrate that it can be used in real-time on a single CPU. We evaluate
ScribblePrompt using manually-collected scribbles, simulated interactions, and
a user study. ScribblePrompt outperforms existing methods in all our
evaluations. In the user study, ScribblePrompt reduced annotation time by 28%
while improving Dice by 15% compared to existing methods. We showcase
ScribblePrompt in an online demo and provide code at
https://scribbleprompt.csail.mit.edu
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_H/0/1/0/all/0/1&quot;&gt;Hallee E. Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rakic_M/0/1/0/all/0/1&quot;&gt;Marianne Rakic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guttag_J/0/1/0/all/0/1&quot;&gt;John Guttag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dalca_A/0/1/0/all/0/1&quot;&gt;Adrian V. Dalca&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07384">
<title>Unsupervised Temporal Action Localization via Self-paced Incremental Learning. (arXiv:2312.07384v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07384</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, temporal action localization (TAL) has garnered significant
interest in information retrieval community. However, existing
supervised/weakly supervised methods are heavily dependent on extensive labeled
temporal boundaries and action categories, which is labor-intensive and
time-consuming. Although some unsupervised methods have utilized the
``iteratively clustering and localization&apos;&apos; paradigm for TAL, they still suffer
from two pivotal impediments: 1) unsatisfactory video clustering confidence,
and 2) unreliable video pseudolabels for model training. To address these
limitations, we present a novel self-paced incremental learning model to
enhance clustering and localization training simultaneously, thereby
facilitating more effective unsupervised TAL. Concretely, we improve the
clustering confidence through exploring the contextual feature-robust visual
information. Thereafter, we design two (constant- and variable- speed)
incremental instance learning strategies for easy-to-hard model training, thus
ensuring the reliability of these video pseudolabels and further improving
overall localization performance. Extensive experiments on two public datasets
have substantiated the superiority of our model over several state-of-the-art
competitors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Haoyu Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Han Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Mingzhu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yupeng Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jihua Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1&quot;&gt;Liqiang Nie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07385">
<title>GSmoothFace: Generalized Smooth Talking Face Generation via Fine Grained 3D Face Guidance. (arXiv:2312.07385v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07385</link>
<description rdf:parseType="Literal">&lt;p&gt;Although existing speech-driven talking face generation methods achieve
significant progress, they are far from real-world application due to the
avatar-specific training demand and unstable lip movements. To address the
above issues, we propose the GSmoothFace, a novel two-stage generalized talking
face generation model guided by a fine-grained 3d face model, which can
synthesize smooth lip dynamics while preserving the speaker&apos;s identity. Our
proposed GSmoothFace model mainly consists of the Audio to Expression
Prediction (A2EP) module and the Target Adaptive Face Translation (TAFT)
module. Specifically, we first develop the A2EP module to predict expression
parameters synchronized with the driven speech. It uses a transformer to
capture the long-term audio context and learns the parameters from the
fine-grained 3D facial vertices, resulting in accurate and smooth
lip-synchronization performance. Afterward, the well-designed TAFT module,
empowered by Morphology Augmented Face Blending (MAFB), takes the predicted
expression parameters and target video as inputs to modify the facial region of
the target video without distorting the background content. The TAFT
effectively exploits the identity appearance and background context in the
target video, which makes it possible to generalize to different speakers
without retraining. Both quantitative and qualitative experiments confirm the
superiority of our method in terms of realism, lip synchronization, and visual
quality. See the project page for code, data, and request pre-trained models:
https://zhanghm1995.github.io/GSmoothFace.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haiming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1&quot;&gt;Zhihao Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Chaoda Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1&quot;&gt;Xu Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Baoyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guanbin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Song Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1&quot;&gt;Shuguang Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhen Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07389">
<title>Eroding Trust In Aerial Imagery: Comprehensive Analysis and Evaluation Of Adversarial Attacks In Geospatial Systems. (arXiv:2312.07389v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07389</link>
<description rdf:parseType="Literal">&lt;p&gt;In critical operations where aerial imagery plays an essential role, the
integrity and trustworthiness of data are paramount. The emergence of
adversarial attacks, particularly those that exploit control over labels or
employ physically feasible trojans, threatens to erode that trust, making the
analysis and mitigation of these attacks a matter of urgency. We demonstrate
how adversarial attacks can degrade confidence in geospatial systems,
specifically focusing on scenarios where the attacker&apos;s control over labels is
restricted and the use of realistic threat vectors. Proposing and evaluating
several innovative attack methodologies, including those tailored to overhead
images, we empirically show their threat to remote sensing systems using
high-quality SpaceNet datasets. Our experimentation reflects the unique
challenges posed by aerial imagery, and these preliminary results not only
reveal the potential risks but also highlight the non-trivial nature of the
problem compared to recent works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lanier_M/0/1/0/all/0/1&quot;&gt;Michael Lanier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhakal_A/0/1/0/all/0/1&quot;&gt;Aayush Dhakal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1&quot;&gt;Zhexiao Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1&quot;&gt;Arthur Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacobs_N/0/1/0/all/0/1&quot;&gt;Nathan Jacobs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vorobeychik_Y/0/1/0/all/0/1&quot;&gt;Yevgeniy Vorobeychik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07395">
<title>A Simple Recipe for Contrastively Pre-training Video-First Encoders Beyond 16 Frames. (arXiv:2312.07395v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07395</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding long, real-world videos requires modeling of long-range visual
dependencies. To this end, we explore video-first architectures, building on
the common paradigm of transferring large-scale, image--text models to video
via shallow temporal fusion. However, we expose two limitations to the
approach: (1) decreased spatial capabilities, likely due to poor
video--language alignment in standard video datasets, and (2) higher memory
consumption, bottlenecking the number of frames that can be processed. To
mitigate the memory bottleneck, we systematically analyze the memory/accuracy
trade-off of various efficient methods: factorized attention,
parameter-efficient image-to-video adaptation, input masking, and
multi-resolution patchification. Surprisingly, simply masking large portions of
the video (up to 75%) during contrastive pre-training proves to be one of the
most robust ways to scale encoders to videos up to 4.3 minutes at 1 FPS. Our
simple approach for training long video-to-text models, which scales to 1B
parameters, does not add new architectural complexity and is able to outperform
the popular paradigm of using much larger LLMs as an information aggregator
over segment-based information on benchmarks with long-range temporal
dependencies (YouCook2, EgoSchema).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papalampidi_P/0/1/0/all/0/1&quot;&gt;Pinelopi Papalampidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koppula_S/0/1/0/all/0/1&quot;&gt;Skanda Koppula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pathak_S/0/1/0/all/0/1&quot;&gt;Shreya Pathak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiu_J/0/1/0/all/0/1&quot;&gt;Justin Chiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heyward_J/0/1/0/all/0/1&quot;&gt;Joe Heyward&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patraucean_V/0/1/0/all/0/1&quot;&gt;Viorica Patraucean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1&quot;&gt;Jiajun Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miech_A/0/1/0/all/0/1&quot;&gt;Antoine Miech&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1&quot;&gt;Andrew Zisserman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nematzdeh_A/0/1/0/all/0/1&quot;&gt;Aida Nematzdeh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07408">
<title>Turbo: Informativity-Driven Acceleration Plug-In for Vision-Language Models. (arXiv:2312.07408v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07408</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision-Language Large Models (VLMs) have become primary backbone of AI, due
to the impressive performance. However, their expensive computation costs,
i.e., throughput and delay, impede potentials in real-world scenarios. To
achieve acceleration for VLMs, most existing methods focus on the model
perspective: pruning, distillation, quantification, but completely overlook the
data-perspective redundancy. To fill the overlook, this paper pioneers the
severity of data redundancy, and designs one plug-and-play Turbo module guided
by information degree to prune inefficient tokens from visual or textual data.
In pursuit of efficiency-performance trade-offs, information degree takes two
key factors into consideration: mutual redundancy and semantic value.
Concretely, the former evaluates the data duplication between sequential
tokens; while the latter evaluates each token by its contribution to the
overall semantics. As a result, tokens with high information degree carry less
redundancy and stronger semantics. For VLMs&apos; calculation, Turbo works as a
user-friendly plug-in that sorts data referring to information degree,
utilizing only top-level ones to save costs. Its advantages are multifaceted,
e.g., being generally compatible to various VLMs across understanding and
generation, simple use without retraining and trivial engineering efforts. On
multiple public VLMs benchmarks, we conduct extensive experiments to reveal the
gratifying acceleration of Turbo, under negligible performance drop.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ju_C/0/1/0/all/0/1&quot;&gt;Chen Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haicheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zeqian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_Z/0/1/0/all/0/1&quot;&gt;Zhonghua Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Weilin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1&quot;&gt;Shuai Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07409">
<title>DiffMorpher: Unleashing the Capability of Diffusion Models for Image Morphing. (arXiv:2312.07409v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07409</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have achieved remarkable image generation quality surpassing
previous generative models. However, a notable limitation of diffusion models,
in comparison to GANs, is their difficulty in smoothly interpolating between
two image samples, due to their highly unstructured latent space. Such a smooth
interpolation is intriguing as it naturally serves as a solution for the image
morphing task with many applications. In this work, we present DiffMorpher, the
first approach enabling smooth and natural image interpolation using diffusion
models. Our key idea is to capture the semantics of the two images by fitting
two LoRAs to them respectively, and interpolate between both the LoRA
parameters and the latent noises to ensure a smooth semantic transition, where
correspondence automatically emerges without the need for annotation. In
addition, we propose an attention interpolation and injection technique and a
new sampling schedule to further enhance the smoothness between consecutive
images. Extensive experiments demonstrate that DiffMorpher achieves starkly
better image morphing effects than previous methods across a variety of object
categories, bridging a critical functional gap that distinguished diffusion
models from GANs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kaiwen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yifan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xudong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1&quot;&gt;Xingang Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1&quot;&gt;Bo Dai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07418">
<title>Attention Based Encoder Decoder Model for Video Captioning in Nepali (2023). (arXiv:2312.07418v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07418</link>
<description rdf:parseType="Literal">&lt;p&gt;Video captioning in Nepali, a language written in the Devanagari script,
presents a unique challenge due to the lack of existing academic work in this
domain. This work develops a novel encoder-decoder paradigm for Nepali video
captioning to tackle this difficulty. LSTM and GRU sequence-to-sequence models
are used in the model to produce related textual descriptions based on features
retrieved from video frames using CNNs. Using Google Translate and manual
post-editing, a Nepali video captioning dataset is generated from the Microsoft
Research Video Description Corpus (MSVD) dataset created using Google
Translate, and manual post-editing work. The efficacy of the model for
Devanagari-scripted video captioning is demonstrated by BLEU, METOR, and ROUGE
measures, which are used to assess its performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parajuli_K/0/1/0/all/0/1&quot;&gt;Kabita Parajuli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1&quot;&gt;Shashidhar Ram Joshi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07423">
<title>Holoported Characters: Real-time Free-viewpoint Rendering of Humans from Sparse RGB Cameras. (arXiv:2312.07423v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07423</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the first approach to render highly realistic free-viewpoint
videos of a human actor in general apparel, from sparse multi-view recording to
display, in real-time at an unprecedented 4K resolution. At inference, our
method only requires four camera views of the moving actor and the respective
3D skeletal pose. It handles actors in wide clothing, and reproduces even
fine-scale dynamic detail, e.g. clothing wrinkles, face expressions, and hand
gestures. At training time, our learning-based approach expects dense
multi-view video and a rigged static surface scan of the actor. Our method
comprises three main stages. Stage 1 is a skeleton-driven neural approach for
high-quality capture of the detailed dynamic mesh geometry. Stage 2 is a novel
solution to create a view-dependent texture using four test-time camera views
as input. Finally, stage 3 comprises a new image-based refinement network
rendering the final 4K image given the output from the previous stages. Our
approach establishes a new benchmark for real-time rendering resolution and
quality using sparse input camera views, unlocking possibilities for immersive
telepresence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shetty_A/0/1/0/all/0/1&quot;&gt;Ashwath Shetty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Habermann_M/0/1/0/all/0/1&quot;&gt;Marc Habermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1&quot;&gt;Guoxing Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luvizon_D/0/1/0/all/0/1&quot;&gt;Diogo Luvizon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1&quot;&gt;Vladislav Golyanik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1&quot;&gt;Christian Theobalt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07424">
<title>How Well Does GPT-4V(ision) Adapt to Distribution Shifts? A Preliminary Investigation. (arXiv:2312.07424v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07424</link>
<description rdf:parseType="Literal">&lt;p&gt;In machine learning, generalization against distribution shifts -- where
deployment conditions diverge from the training scenarios -- is crucial,
particularly in fields like climate modeling, biomedicine, and autonomous
driving. The emergence of foundation models, distinguished by their extensive
pretraining and task versatility, has led to an increased interest in their
adaptability to distribution shifts. GPT-4V(ision) acts as the most advanced
publicly accessible multimodal foundation model, with extensive applications
across various domains, including anomaly detection, video understanding, image
generation, and medical diagnosis. However, its robustness against data
distributions remains largely underexplored. Addressing this gap, this study
rigorously evaluates GPT-4V&apos;s adaptability and generalization capabilities in
dynamic environments, benchmarking against prominent models like CLIP and
LLaVA. We delve into GPT-4V&apos;s zero-shot generalization across 13 diverse
datasets spanning natural, medical, and molecular domains. We further
investigate its adaptability to controlled data perturbations and examine the
efficacy of in-context learning as a tool to enhance its adaptation. Our
findings delineate GPT-4V&apos;s capability boundaries in distribution shifts,
shedding light on its strengths and limitations across various scenarios.
Importantly, this investigation contributes to our understanding of how AI
foundation models generalize to distribution shifts, offering pivotal insights
into their adaptability and robustness. Code is publicly available at
https://github.com/jameszhou-gl/gpt-4v-distribution-shift.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1&quot;&gt;Zhongyi Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1&quot;&gt;Guanglin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1&quot;&gt;Rundong He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jindong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xing Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tailin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1&quot;&gt;Yilong Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Salman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1&quot;&gt;Lina Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tongliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07425">
<title>Deep Internal Learning: Deep Learning from a Single Input. (arXiv:2312.07425v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07425</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning in general focuses on training a neural network from large
labeled datasets. Yet, in many cases there is value in training a network just
from the input at hand. This may involve training a network from scratch using
a single input or adapting an already trained network to a provided input
example at inference time. This survey paper aims at covering deep
internal-learning techniques that have been proposed in the past few years for
these two important directions. While our main focus will be on image
processing problems, most of the approaches that we survey are derived for
general signals (vectors with recurring patterns that can be distinguished from
noise) and are therefore applicable to other modalities. We believe that the
topic of internal-learning is very important in many signal and image
processing problems where training data is scarce and diversity is large on the
one hand, and on the other, there is a lot of structure in the data that can be
exploited.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tirer_T/0/1/0/all/0/1&quot;&gt;Tom Tirer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1&quot;&gt;Raja Giryes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1&quot;&gt;Se Young Chun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eldar_Y/0/1/0/all/0/1&quot;&gt;Yonina C. Eldar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07428">
<title>Ensemble Federated Learning: an approach for collaborative pneumonia diagnosis. (arXiv:2312.07428v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07428</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning is a very convenient approach for scenarios where (i) the
exchange of data implies privacy concerns and/or (ii) a quick reaction is
needed. In smart healthcare systems, both aspects are usually required. In this
paper, we work on the first scenario, where preserving privacy is key and,
consequently, building a unique and massive medical image data set by fusing
different data sets from different medical institutions or research centers
(computation nodes) is not an option. We propose an ensemble federated learning
(EFL) approach that is based on the following characteristics: First, each
computation node works with a different data set (but of the same type). They
work locally and apply an ensemble approach combining eight well-known CNN
models (densenet169, mobilenetv2, xception, inceptionv3, vgg16, resnet50,
densenet121, and resnet152v2) on Chest X-ray images. Second, the best two local
models are used to create a local ensemble model that is shared with a central
node. Third, the ensemble models are aggregated to obtain a global model, which
is shared with the computation nodes to continue with a new iteration. This
procedure continues until there are no changes in the best local models. We
have performed different experiments to compare our approach with centralized
ones (with or without an ensemble approach)\color{black}. The results conclude
that our proposal outperforms these ones in Chest X-ray images (achieving an
accuracy of 96.63\%) and offers very competitive results compared to other
proposals in the literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mabrouk_A/0/1/0/all/0/1&quot;&gt;Alhassan Mabrouk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Redondo_R/0/1/0/all/0/1&quot;&gt;Rebeca P. D&amp;#xed;az Redondo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elaziz_M/0/1/0/all/0/1&quot;&gt;Mohamed Abd Elaziz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kayed_M/0/1/0/all/0/1&quot;&gt;Mohammed Kayed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07435">
<title>Cross-modal Contrastive Learning with Asymmetric Co-attention Network for Video Moment Retrieval. (arXiv:2312.07435v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07435</link>
<description rdf:parseType="Literal">&lt;p&gt;Video moment retrieval is a challenging task requiring fine-grained
interactions between video and text modalities. Recent work in image-text
pretraining has demonstrated that most existing pretrained models suffer from
information asymmetry due to the difference in length between visual and
textual sequences. We question whether the same problem also exists in the
video-text domain with an auxiliary need to preserve both spatial and temporal
information. Thus, we evaluate a recently proposed solution involving the
addition of an asymmetric co-attention network for video grounding tasks.
Additionally, we incorporate momentum contrastive loss for robust,
discriminative representation learning in both modalities. We note that the
integration of these supplementary modules yields better performance compared
to state-of-the-art models on the TACoS dataset and comparable results on
ActivityNet Captions, all while utilizing significantly fewer parameters with
respect to baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panta_L/0/1/0/all/0/1&quot;&gt;Love Panta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shrestha_P/0/1/0/all/0/1&quot;&gt;Prashant Shrestha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sapkota_B/0/1/0/all/0/1&quot;&gt;Brabeem Sapkota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattarai_A/0/1/0/all/0/1&quot;&gt;Amrita Bhattarai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manandhar_S/0/1/0/all/0/1&quot;&gt;Suresh Manandhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sah_A/0/1/0/all/0/1&quot;&gt;Anand Kumar Sah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07437">
<title>Medical Image Classification Using Transfer Learning and Chaos Game Optimization on the Internet of Medical Things. (arXiv:2312.07437v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07437</link>
<description rdf:parseType="Literal">&lt;p&gt;The Internet of Medical Things (IoMT) has dramatically benefited medical
professionals that patients and physicians can access from all regions.
Although the automatic detection and prediction of diseases such as melanoma
and leukemia is still being researched and studied in IoMT, existing approaches
are not able to achieve a high degree of efficiency. Thus, with a new approach
that provides better results, patients would access the adequate treatments
earlier and the death rate would be reduced. Therefore, this paper introduces
an IoMT proposal for medical images classification that may be used anywhere,
i.e. it is an ubiquitous approach. It was design in two stages: first, we
employ a Transfer Learning (TL)-based method for feature extraction, which is
carried out using MobileNetV3; second, we use the Chaos Game Optimization (CGO)
for feature selection, with the aim of excluding unnecessary features and
improving the performance, which is key in IoMT. Our methodology was evaluated
using ISIC-2016, PH2, and Blood-Cell datasets. The experimental results
indicated that the proposed approach obtained an accuracy of 88.39% on
ISIC-2016, 97.52% on PH2, and 88.79% on Blood-cell. Moreover, our approach had
successful performances for the metrics employed compared to other existing
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mabrouk_A/0/1/0/all/0/1&quot;&gt;Alhassan Mabrouk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dahou_A/0/1/0/all/0/1&quot;&gt;Abdelghani Dahou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elaziz_M/0/1/0/all/0/1&quot;&gt;Mohamed Abd Elaziz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Redondo_R/0/1/0/all/0/1&quot;&gt;Rebeca P. D&amp;#xed;az Redondo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kayed_M/0/1/0/all/0/1&quot;&gt;Mohammed Kayed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07460">
<title>Empirical Validation of Conformal Prediction for Trustworthy Skin Lesions Classification. (arXiv:2312.07460v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.07460</link>
<description rdf:parseType="Literal">&lt;p&gt;Uncertainty quantification is a pivotal field that contributes to the
realization of reliable and robust systems. By providing complementary
information, it becomes instrumental in fortifying safe decisions, particularly
within high-risk applications. Nevertheless, a comprehensive understanding of
the advantages and limitations inherent in various methods within the medical
imaging field necessitates further research coupled with in-depth analysis. In
this paper, we explore Conformal Prediction, an emerging distribution-free
uncertainty quantification technique, along with Monte Carlo Dropout and
Evidential Deep Learning methods. Our comprehensive experiments provide a
comparative performance analysis for skin lesion classification tasks across
the three quantification methods. Furthermore, We present insights into the
effectiveness of each method in handling Out-of-Distribution samples from
domain-shifted datasets. Based on our experimental findings, our conclusion
highlights the robustness and consistent performance of conformal prediction
across diverse conditions. This positions it as the preferred choice for
decision-making in safety-critical applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fayyad_J/0/1/0/all/0/1&quot;&gt;Jamil Fayyad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Alijani_S/0/1/0/all/0/1&quot;&gt;Shadi Alijani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Najjaran_H/0/1/0/all/0/1&quot;&gt;Homayoun Najjaran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07466">
<title>Efficient Object Detection in Autonomous Driving using Spiking Neural Networks: Performance, Energy Consumption Analysis, and Insights into Open-set Object Discovery. (arXiv:2312.07466v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07466</link>
<description rdf:parseType="Literal">&lt;p&gt;Besides performance, efficiency is a key design driver of technologies
supporting vehicular perception. Indeed, a well-balanced trade-off between
performance and energy consumption is crucial for the sustainability of
autonomous vehicles. In this context, the diversity of real-world contexts in
which autonomous vehicles can operate motivates the need for empowering
perception models with the capability to detect, characterize and identify
newly appearing objects by themselves. In this manuscript we elaborate on this
threefold conundrum (performance, efficiency and open-world learning) for
object detection modeling tasks over image data collected from vehicular
scenarios. Specifically, we show that well-performing and efficient models can
be realized by virtue of Spiking Neural Networks (SNNs), reaching competitive
levels of detection performance when compared to their non-spiking counterparts
at dramatic energy consumption savings (up to 85%) and a slightly improved
robustness against image noise. Our experiments herein offered also expose
qualitatively the complexity of detecting new objects based on the preliminary
results of a simple approach to discriminate potential object proposals in the
captured image.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seras_A/0/1/0/all/0/1&quot;&gt;Aitor Martinez Seras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ser_J/0/1/0/all/0/1&quot;&gt;Javier Del Ser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_Bringas_P/0/1/0/all/0/1&quot;&gt;Pablo Garcia-Bringas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07472">
<title>MP5: A Multi-modal Open-ended Embodied System in Minecraft via Active Perception. (arXiv:2312.07472v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07472</link>
<description rdf:parseType="Literal">&lt;p&gt;It is a long-lasting goal to design an embodied system that can solve
long-horizon open-world tasks in human-like ways. However, existing approaches
usually struggle with compound difficulties caused by the logic-aware
decomposition and context-aware execution of these tasks. To this end, we
introduce MP5, an open-ended multimodal embodied system built upon the
challenging Minecraft simulator, which can decompose feasible sub-objectives,
design sophisticated situation-aware plans, and perform embodied action
control, with frequent communication with a goal-conditioned active perception
scheme. Specifically, MP5 is developed on top of recent advances in Multimodal
Large Language Models (MLLMs), and the system is modulated into functional
modules that can be scheduled and collaborated to ultimately solve pre-defined
context- and process-dependent tasks. Extensive experiments prove that MP5 can
achieve a 22% success rate on difficult process-dependent tasks and a 91%
success rate on tasks that heavily depend on the context. Moreover, MP5
exhibits a remarkable ability to address many open-ended tasks that are
entirely novel.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yiran Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1&quot;&gt;Enshen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qichang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Zhenfei Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_L/0/1/0/all/0/1&quot;&gt;Lu Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruimao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1&quot;&gt;Jing Shao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07478">
<title>Double-Flow GAN model for the reconstruction of perceived faces from brain activities. (arXiv:2312.07478v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07478</link>
<description rdf:parseType="Literal">&lt;p&gt;Face plays an important role in human&apos;s visual perception, and reconstructing
perceived faces from brain activities is challenging because of its difficulty
in extracting high-level features and maintaining consistency of multiple face
attributes, such as expression, identity, gender, etc. In this study, we
proposed a novel reconstruction framework, which we called Double-Flow GAN,
that can enhance the capability of discriminator and handle imbalances in
images from certain domains that are too easy for generators. We also designed
a pretraining process that uses features extracted from images as conditions
for making it possible to pretrain the conditional reconstruction model from
fMRI in a larger pure image dataset. Moreover, we developed a simple pretrained
model to perform fMRI alignment to alleviate the problem of cross-subject
reconstruction due to the variations of brain structure among different
subjects. We conducted experiments by using our proposed method and
state-of-the-art reconstruction models. Our results demonstrated that our
method showed significant reconstruction performance, outperformed the previous
reconstruction models, and exhibited a good generation ability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zihao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jing Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hui Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07485">
<title>MinD-3D: Reconstruct High-quality 3D objects in Human Brain. (arXiv:2312.07485v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07485</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce Recon3DMind, a groundbreaking task focused on
reconstructing 3D visuals from Functional Magnetic Resonance Imaging (fMRI)
signals. This represents a major step forward in cognitive neuroscience and
computer vision. To support this task, we present the fMRI-Shape dataset,
utilizing 360-degree view videos of 3D objects for comprehensive fMRI signal
capture. Containing 55 categories of common objects from daily life, this
dataset will bolster future research endeavors. We also propose MinD-3D, a
novel and effective three-stage framework that decodes and reconstructs the
brain&apos;s 3D visual information from fMRI signals. This method starts by
extracting and aggregating features from fMRI frames using a neuro-fusion
encoder, then employs a feature bridge diffusion model to generate
corresponding visual features, and ultimately recovers the 3D object through a
generative transformer decoder. Our experiments demonstrate that this method
effectively extracts features that are valid and highly correlated with visual
regions of interest (ROIs) in fMRI signals. Notably, it not only reconstructs
3D objects with high semantic relevance and spatial similarity but also
significantly deepens our understanding of the human brain&apos;s 3D visual
processing capabilities. Project page at: https://jianxgao.github.io/MinD-3D.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jianxiong Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yuqian Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1&quot;&gt;Xuelin Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jianfeng Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yanwei Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07488">
<title>LMDrive: Closed-Loop End-to-End Driving with Large Language Models. (arXiv:2312.07488v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07488</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite significant recent progress in the field of autonomous driving,
modern methods still struggle and can incur serious accidents when encountering
long-tail unforeseen events and challenging urban scenarios. On the one hand,
large language models (LLM) have shown impressive reasoning capabilities that
approach &quot;Artificial General Intelligence&quot;. On the other hand, previous
autonomous driving methods tend to rely on limited-format inputs (e.g. sensor
data and navigation waypoints), restricting the vehicle&apos;s ability to understand
language information and interact with humans. To this end, this paper
introduces LMDrive, a novel language-guided, end-to-end, closed-loop autonomous
driving framework. LMDrive uniquely processes and integrates multi-modal sensor
data with natural language instructions, enabling interaction with humans and
navigation software in realistic instructional settings. To facilitate further
research in language-based closed-loop autonomous driving, we also publicly
release the corresponding dataset which includes approximately 64K
instruction-following data clips, and the LangAuto benchmark that tests the
system&apos;s ability to handle complex instructions and challenging driving
scenarios. Extensive closed-loop experiments are conducted to demonstrate
LMDrive&apos;s effectiveness. To the best of our knowledge, we&apos;re the very first
work to leverage LLMs for closed-loop end-to-end autonomous driving. Codes can
be found at https://github.com/opendilab/LMDrive
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_H/0/1/0/all/0/1&quot;&gt;Hao Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Letian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waslander_S/0/1/0/all/0/1&quot;&gt;Steven L. Waslander&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongsheng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07489">
<title>NearbyPatchCL: Leveraging Nearby Patches for Self-Supervised Patch-Level Multi-Class Classification in Whole-Slide Images. (arXiv:2312.07489v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07489</link>
<description rdf:parseType="Literal">&lt;p&gt;Whole-slide image (WSI) analysis plays a crucial role in cancer diagnosis and
treatment. In addressing the demands of this critical task, self-supervised
learning (SSL) methods have emerged as a valuable resource, leveraging their
efficiency in circumventing the need for a large number of annotations, which
can be both costly and time-consuming to deploy supervised methods.
Nevertheless, patch-wise representation may exhibit instability in performance,
primarily due to class imbalances stemming from patch selection within WSIs. In
this paper, we introduce Nearby Patch Contrastive Learning (NearbyPatchCL), a
novel self-supervised learning method that leverages nearby patches as positive
samples and a decoupled contrastive loss for robust representation learning.
Our method demonstrates a tangible enhancement in performance for downstream
tasks involving patch-level multi-class classification. Additionally, we curate
a new dataset derived from WSIs sourced from the Canine Cutaneous Cancer
Histology, thus establishing a benchmark for the rigorous evaluation of
patch-level multi-class classification methodologies. Intensive experiments
show that our method significantly outperforms the supervised baseline and
state-of-the-art SSL methods with top-1 classification accuracy of 87.56%. Our
method also achieves comparable results while utilizing a mere 1% of labeled
data, a stark contrast to the 100% labeled data requirement of other
approaches. Source code: https://github.com/nvtien457/NearbyPatchCL
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_G/0/1/0/all/0/1&quot;&gt;Gia-Bao Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1&quot;&gt;Van-Tien Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1&quot;&gt;Trung-Nghia Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1&quot;&gt;Minh-Triet Tran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2103.09151">
<title>Adversarial Driving: Attacking End-to-End Autonomous Driving. (arXiv:2103.09151v8 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2103.09151</link>
<description rdf:parseType="Literal">&lt;p&gt;As research in deep neural networks advances, deep convolutional networks
become promising for autonomous driving tasks. In particular, there is an
emerging trend of employing end-to-end neural network models for autonomous
driving. However, previous research has shown that deep neural network
classifiers are vulnerable to adversarial attacks. While for regression tasks,
the effect of adversarial attacks is not as well understood. In this research,
we devise two white-box targeted attacks against end-to-end autonomous driving
models. Our attacks manipulate the behavior of the autonomous driving system by
perturbing the input image. In an average of 800 attacks with the same attack
strength (epsilon=1), the image-specific and image-agnostic attack deviates the
steering angle from the original output by 0.478 and 0.111, respectively, which
is much stronger than random noises that only perturbs the steering angle by
0.002 (The steering angle ranges from [-1, 1]). Both attacks can be initiated
in real-time on CPUs without employing GPUs. Demo video:
https://youtu.be/I0i8uN2oOP0.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Han Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yunas_S/0/1/0/all/0/1&quot;&gt;Syed Yunas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rowlands_S/0/1/0/all/0/1&quot;&gt;Sareh Rowlands&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruan_W/0/1/0/all/0/1&quot;&gt;Wenjie Ruan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wahlstrom_J/0/1/0/all/0/1&quot;&gt;Johan Wahlstrom&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.06074">
<title>Early Stopping for Deep Image Prior. (arXiv:2112.06074v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2112.06074</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep image prior (DIP) and its variants have showed remarkable potential for
solving inverse problems in computer vision, without any extra training data.
Practical DIP models are often substantially overparameterized. During the
fitting process, these models learn mostly the desired visual content first,
and then pick up the potential modeling and observational noise, i.e.,
overfitting. Thus, the practicality of DIP often depends critically on good
early stopping (ES) that captures the transition period. In this regard, the
majority of DIP works for vision tasks only demonstrates the potential of the
models -- reporting the peak performance against the ground truth, but provides
no clue about how to operationally obtain near-peak performance without access
to the groundtruth. In this paper, we set to break this practicality barrier of
DIP, and propose an efficient ES strategy, which consistently detects near-peak
performance across several vision tasks and DIP variants. Based on a simple
measure of dispersion of consecutive DIP reconstructions, our ES method not
only outpaces the existing ones -- which only work in very narrow domains, but
also remains effective when combined with a number of methods that try to
mitigate the overfitting. The code is available at
https://github.com/sun-umn/Early_Stopping_for_DIP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hengkang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Taihui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1&quot;&gt;Zhong Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tiancong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1&quot;&gt;Hengyue Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Ju Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.04281">
<title>Local Spatiotemporal Representation Learning for Longitudinally-consistent Neuroimage Analysis. (arXiv:2206.04281v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.04281</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent self-supervised advances in medical computer vision exploit global and
local anatomical self-similarity for pretraining prior to downstream tasks such
as segmentation. However, current methods assume i.i.d. image acquisition,
which is invalid in clinical study designs where follow-up longitudinal scans
track subject-specific temporal changes. Further, existing self-supervised
methods for medically-relevant image-to-image architectures exploit only
spatial or temporal self-similarity and only do so via a loss applied at a
single image-scale, with naive multi-scale spatiotemporal extensions collapsing
to degenerate solutions. To these ends, this paper makes two contributions: (1)
It presents a local and multi-scale spatiotemporal representation learning
method for image-to-image architectures trained on longitudinal images. It
exploits the spatiotemporal self-similarity of learned multi-scale
intra-subject features for pretraining and develops several feature-wise
regularizations that avoid collapsed identity representations; (2) During
finetuning, it proposes a surprisingly simple self-supervised segmentation
consistency regularization to exploit intra-subject correlation. Benchmarked in
the one-shot segmentation setting, the proposed framework outperforms both
well-tuned randomly-initialized baselines and current self-supervised
techniques designed for both i.i.d. and longitudinal datasets. These
improvements are demonstrated across both longitudinal neurodegenerative adult
MRI and developing infant brain MRI and yield both higher performance and
longitudinal consistency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1&quot;&gt;Mengwei Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dey_N/0/1/0/all/0/1&quot;&gt;Neel Dey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Styner_M/0/1/0/all/0/1&quot;&gt;Martin A. Styner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Botteron_K/0/1/0/all/0/1&quot;&gt;Kelly Botteron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gerig_G/0/1/0/all/0/1&quot;&gt;Guido Gerig&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.05575">
<title>MammoFL: Mammographic Breast Density Estimation using Federated Learning. (arXiv:2206.05575v4 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.05575</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we automate quantitative mammographic breast density
estimation with neural networks and show that this tool is a strong use case
for federated learning on multi-institutional datasets. Our dataset included
bilateral CC-view and MLO-view mammographic images from two separate
institutions. Two U-Nets were separately trained on algorithm-generated labels
to perform segmentation of the breast and dense tissue from these images and
subsequently calculate breast percent density (PD). The networks were trained
with federated learning and compared to three non-federated baselines, one
trained on each single-institution dataset and one trained on the aggregated
multi-institution dataset. We demonstrate that training on multi-institution
datasets is critical to algorithm generalizability. We further show that
federated learning on multi-institutional datasets improves model
generalization to unseen data at nearly the same level as centralized training
on multi-institutional datasets, indicating that federated learning can be
applied to our method to improve algorithm generalizability while maintaining
patient privacy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Muthukrishnan_R/0/1/0/all/0/1&quot;&gt;Ramya Muthukrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Heyler_A/0/1/0/all/0/1&quot;&gt;Angelina Heyler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Katti_K/0/1/0/all/0/1&quot;&gt;Keshava Katti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pati_S/0/1/0/all/0/1&quot;&gt;Sarthak Pati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mankowski_W/0/1/0/all/0/1&quot;&gt;Walter Mankowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Alahari_A/0/1/0/all/0/1&quot;&gt;Aprupa Alahari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sanborn_M/0/1/0/all/0/1&quot;&gt;Michael Sanborn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Conant_E/0/1/0/all/0/1&quot;&gt;Emily F. Conant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Scott_C/0/1/0/all/0/1&quot;&gt;Christopher Scott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Winham_S/0/1/0/all/0/1&quot;&gt;Stacey Winham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vachon_C/0/1/0/all/0/1&quot;&gt;Celine Vachon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chaudhari_P/0/1/0/all/0/1&quot;&gt;Pratik Chaudhari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kontos_D/0/1/0/all/0/1&quot;&gt;Despina Kontos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bakas_S/0/1/0/all/0/1&quot;&gt;Spyridon Bakas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.01962">
<title>Adversarial Detection: Attacking Object Detection in Real Time. (arXiv:2209.01962v6 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2209.01962</link>
<description rdf:parseType="Literal">&lt;p&gt;Intelligent robots rely on object detection models to perceive the
environment. Following advances in deep learning security it has been revealed
that object detection models are vulnerable to adversarial attacks. However,
prior research primarily focuses on attacking static images or offline videos.
Therefore, it is still unclear if such attacks could jeopardize real-world
robotic applications in dynamic environments. This paper bridges this gap by
presenting the first real-time online attack against object detection models.
We devise three attacks that fabricate bounding boxes for nonexistent objects
at desired locations. The attacks achieve a success rate of about 90% within
about 20 iterations. The demo video is available at
https://youtu.be/zJZ1aNlXsMU.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Han Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yunas_S/0/1/0/all/0/1&quot;&gt;Syed Yunas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rowlands_S/0/1/0/all/0/1&quot;&gt;Sareh Rowlands&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruan_W/0/1/0/all/0/1&quot;&gt;Wenjie Ruan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wahlstrom_J/0/1/0/all/0/1&quot;&gt;Johan Wahlstrom&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.05166">
<title>Exploring Domain Incremental Video Highlights Detection with the LiveFood Benchmark. (arXiv:2209.05166v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.05166</link>
<description rdf:parseType="Literal">&lt;p&gt;Video highlights detection (VHD) is an active research field in computer
vision, aiming to locate the most user-appealing clips given raw video inputs.
However, most VHD methods are based on the closed world assumption, i.e., a
fixed number of highlight categories is defined in advance and all training
data are available beforehand. Consequently, existing methods have poor
scalability with respect to increasing highlight domains and training data. To
address above issues, we propose a novel video highlights detection method
named Global Prototype Encoding (GPE) to learn incrementally for adapting to
new domains via parameterized prototypes. To facilitate this new research
direction, we collect a finely annotated dataset termed LiveFood, including
over 5,100 live gourmet videos that consist of four domains: ingredients,
cooking, presentation, and eating. To the best of our knowledge, this is the
first work to explore video highlights detection in the incremental learning
setting, opening up new land to apply VHD for practical scenarios where both
the concerned highlight domains and training data increase over time. We
demonstrate the effectiveness of GPE through extensive experiments. Notably,
GPE surpasses popular domain incremental learning methods on LiveFood,
achieving significant mAP improvements on all domains. Concerning the classic
datasets, GPE also yields comparable performance as previous arts. The code is
available at: https://github.com/ForeverPs/IncrementalVHD_GPE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_S/0/1/0/all/0/1&quot;&gt;Sen Pei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shixiong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1&quot;&gt;Xiaojie Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.14404">
<title>Adversarial Purification with the Manifold Hypothesis. (arXiv:2210.14404v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.14404</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we formulate a novel framework for adversarial robustness using
the manifold hypothesis. This framework provides sufficient conditions for
defending against adversarial examples. We develop an adversarial purification
method with this framework. Our method combines manifold learning with
variational inference to provide adversarial robustness without the need for
expensive adversarial training. Experimentally, our approach can provide
adversarial robustness even if attackers are aware of the existence of the
defense. In addition, our method can also serve as a test-time defense
mechanism for variational autoencoders.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhaoyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hartley_R/0/1/0/all/0/1&quot;&gt;Richard Hartley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_P/0/1/0/all/0/1&quot;&gt;Peter Tu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.01819">
<title>Hierarchical Terrain Attention and Multi-Scale Rainfall Guidance For Flood Image Prediction. (arXiv:2212.01819v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.01819</link>
<description rdf:parseType="Literal">&lt;p&gt;With the deterioration of climate, the phenomenon of rain-induced flooding
has become frequent. To mitigate its impact, recent works adopt convolutional
neural network or its variants to predict the floods. However, these methods
directly force the model to reconstruct the raw pixels of flood images through
a global constraint, overlooking the underlying information contained in
terrain features and rainfall patterns. To address this, we present a novel
framework for precise flood map prediction, which incorporates hierarchical
terrain spatial attention to help the model focus on spatially-salient areas of
terrain features and constructs multi-scale rainfall embedding to extensively
integrate rainfall pattern information into generation. To better adapt the
model in various rainfall conditions, we leverage a rainfall regression loss
for both the generator and the discriminator as additional supervision.
Extensive evaluations on real catchment datasets demonstrate the superior
performance of our method, which greatly surpasses the previous arts under
different rainfall conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Feifei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qidong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shaoqing Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.05909">
<title>NFResNet: Multi-scale and U-shaped Networks for Deblurring. (arXiv:2212.05909v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.05909</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-Scale and U-shaped Networks are widely used in various image
restoration problems, including deblurring. Keeping in mind the wide range of
applications, we present a comparison of these architectures and their effects
on image deblurring. We also introduce a new block called as NFResblock. It
consists of a Fast Fourier Transformation layer and a series of modified
Non-Linear Activation Free Blocks. Based on these architectures and additions,
we introduce NFResnet and NFResnet+, which are modified multi-scale and U-Net
architectures, respectively. We also use three different loss functions to
train these architectures: Charbonnier Loss, Edge Loss, and Frequency
Reconstruction Loss. Extensive experiments on the Deep Video Deblurring
dataset, along with ablation studies for each component, have been presented in
this paper. The proposed architectures achieve a considerable increase in Peak
Signal to Noise (PSNR) ratio and Structural Similarity Index (SSIM) value.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mittal_T/0/1/0/all/0/1&quot;&gt;Tanish Mittal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1&quot;&gt;Preyansh Agrawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pahwa_E/0/1/0/all/0/1&quot;&gt;Esha Pahwa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Makwana_A/0/1/0/all/0/1&quot;&gt;Aarya Makwana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.03169">
<title>A Study on the Generality of Neural Network Structures for Monocular Depth Estimation. (arXiv:2301.03169v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.03169</link>
<description rdf:parseType="Literal">&lt;p&gt;Monocular depth estimation has been widely studied, and significant
improvements in performance have been recently reported. However, most previous
works are evaluated on a few benchmark datasets, such as KITTI datasets, and
none of the works provide an in-depth analysis of the generalization
performance of monocular depth estimation. In this paper, we deeply investigate
the various backbone networks (e.g.CNN and Transformer models) toward the
generalization of monocular depth estimation. First, we evaluate
state-of-the-art models on both in-distribution and out-of-distribution
datasets, which have never been seen during network training. Then, we
investigate the internal properties of the representations from the
intermediate layers of CNN-/Transformer-based models using synthetic
texture-shifted datasets. Through extensive experiments, we observe that the
Transformers exhibit a strong shape-bias rather than CNNs, which have a strong
texture-bias. We also discover that texture-biased models exhibit worse
generalization performance for monocular depth estimation than shape-biased
models. We demonstrate that similar aspects are observed in real-world driving
datasets captured under diverse environments. Lastly, we conduct a dense
ablation study with various backbone networks which are utilized in modern
strategies. The experiments demonstrate that the intrinsic locality of the CNNs
and the self-attention of the Transformers induce texture-bias and shape-bias,
respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bae_J/0/1/0/all/0/1&quot;&gt;Jinwoo Bae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_K/0/1/0/all/0/1&quot;&gt;Kyumin Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Im_S/0/1/0/all/0/1&quot;&gt;Sunghoon Im&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.04037">
<title>ROBUSfT: Robust Real-Time Shape-from-Template, a C++ Library. (arXiv:2301.04037v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.04037</link>
<description rdf:parseType="Literal">&lt;p&gt;Tracking the 3D shape of a deforming object using only monocular 2D vision is
a challenging problem. This is because one should (i) infer the 3D shape from a
2D image, which is a severely underconstrained problem, and (ii) implement the
whole solution pipeline in real-time. The pipeline typically requires feature
detection and matching, mismatch filtering, 3D shape inference and feature
tracking algorithms. We propose ROBUSfT, a conventional pipeline based on a
template containing the object&apos;s rest shape, texturemap and deformation law.
ROBUSfT is ready-to-use, wide-baseline, capable of handling large deformations,
fast up to 30 fps, free of training, and robust against partial occlusions and
discontinuity in video frames. It outperforms the state-of-the-art methods in
challenging datasets. ROBUSfT is implemented as a publicly available C++
library and we provide a tutorial on how to use it in
https://github.com/mrshetab/ROBUSfT
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shetab_Bushehri_M/0/1/0/all/0/1&quot;&gt;Mohammadreza Shetab-Bushehri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aranda_M/0/1/0/all/0/1&quot;&gt;Miguel Aranda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mezouar_Y/0/1/0/all/0/1&quot;&gt;Youcef Mezouar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bartoli_A/0/1/0/all/0/1&quot;&gt;Adrien Bartoli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozgur_E/0/1/0/all/0/1&quot;&gt;Erol Ozgur&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.11093">
<title>Simple diffusion: End-to-end diffusion for high resolution images. (arXiv:2301.11093v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.11093</link>
<description rdf:parseType="Literal">&lt;p&gt;Currently, applying diffusion models in pixel space of high resolution images
is difficult. Instead, existing approaches focus on diffusion in lower
dimensional spaces (latent diffusion), or have multiple super-resolution levels
of generation referred to as cascades. The downside is that these approaches
add additional complexity to the diffusion framework.
&lt;/p&gt;
&lt;p&gt;This paper aims to improve denoising diffusion for high resolution images
while keeping the model as simple as possible. The paper is centered around the
research question: How can one train a standard denoising diffusion models on
high resolution images, and still obtain performance comparable to these
alternate approaches?
&lt;/p&gt;
&lt;p&gt;The four main findings are: 1) the noise schedule should be adjusted for high
resolution images, 2) It is sufficient to scale only a particular part of the
architecture, 3) dropout should be added at specific locations in the
architecture, and 4) downsampling is an effective strategy to avoid high
resolution feature maps. Combining these simple yet effective techniques, we
achieve state-of-the-art on image generation among diffusion models without
sampling modifiers on ImageNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoogeboom_E/0/1/0/all/0/1&quot;&gt;Emiel Hoogeboom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heek_J/0/1/0/all/0/1&quot;&gt;Jonathan Heek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salimans_T/0/1/0/all/0/1&quot;&gt;Tim Salimans&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.00855">
<title>Grounded Decoding: Guiding Text Generation with Grounded Models for Embodied Agents. (arXiv:2303.00855v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2303.00855</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent progress in large language models (LLMs) has demonstrated the ability
to learn and leverage Internet-scale knowledge through pre-training with
autoregressive models. Unfortunately, applying such models to settings with
embodied agents, such as robots, is challenging due to their lack of experience
with the physical world, inability to parse non-language observations, and
ignorance of rewards or safety constraints that robots may require. On the
other hand, language-conditioned robotic policies that learn from interaction
data can provide the necessary grounding that allows the agent to be correctly
situated in the real world, but such policies are limited by the lack of
high-level semantic understanding due to the limited breadth of the interaction
data available for training them. Thus, if we want to make use of the semantic
knowledge in a language model while still situating it in an embodied setting,
we must construct an action sequence that is both likely according to the
language model and also realizable according to grounded models of the
environment. We frame this as a problem similar to probabilistic filtering:
decode a sequence that both has high probability under the language model and
high probability under a set of grounded model objectives. We demonstrate how
such grounded models can be obtained across three simulation and real-world
domains, and that the proposed decoding strategy is able to solve complex,
long-horizon embodiment tasks in a robotic setting by leveraging the knowledge
of both models. The project&apos;s website can be found at
grounded-decoding.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wenlong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1&quot;&gt;Fei Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1&quot;&gt;Dhruv Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Driess_D/0/1/0/all/0/1&quot;&gt;Danny Driess&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1&quot;&gt;Andy Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Florence_P/0/1/0/all/0/1&quot;&gt;Pete Florence&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1&quot;&gt;Igor Mordatch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hausman_K/0/1/0/all/0/1&quot;&gt;Karol Hausman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ichter_B/0/1/0/all/0/1&quot;&gt;Brian Ichter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.01092">
<title>ArCL: Enhancing Contrastive Learning with Augmentation-Robust Representations. (arXiv:2303.01092v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.01092</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-Supervised Learning (SSL) is a paradigm that leverages unlabeled data
for model training. Empirical studies show that SSL can achieve promising
performance in distribution shift scenarios, where the downstream and training
distributions differ. However, the theoretical understanding of its
transferability remains limited. In this paper, we develop a theoretical
framework to analyze the transferability of self-supervised contrastive
learning, by investigating the impact of data augmentation on it. Our results
reveal that the downstream performance of contrastive learning depends largely
on the choice of data augmentation. Moreover, we show that contrastive learning
fails to learn domain-invariant features, which limits its transferability.
Based on these theoretical insights, we propose a novel method called
Augmentation-robust Contrastive Learning (ArCL), which guarantees to learn
domain-invariant features and can be easily integrated with existing
contrastive learning algorithms. We conduct experiments on several datasets and
show that ArCL significantly improves the transferability of contrastive
learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xuyang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_T/0/1/0/all/0/1&quot;&gt;Tianqi Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yisen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Jun Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Weiran Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.01168">
<title>DeepAccident: A Motion and Accident Prediction Benchmark for V2X Autonomous Driving. (arXiv:2304.01168v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.01168</link>
<description rdf:parseType="Literal">&lt;p&gt;Safety is the primary priority of autonomous driving. Nevertheless, no
published dataset currently supports the direct and explainable safety
evaluation for autonomous driving. In this work, we propose DeepAccident, a
large-scale dataset generated via a realistic simulator containing diverse
accident scenarios that frequently occur in real-world driving. The proposed
DeepAccident dataset includes 57K annotated frames and 285K annotated samples,
approximately 7 times more than the large-scale nuScenes dataset with 40k
annotated samples. In addition, we propose a new task, end-to-end motion and
accident prediction, which can be used to directly evaluate the accident
prediction ability for different autonomous driving algorithms. Furthermore,
for each scenario, we set four vehicles along with one infrastructure to record
data, thus providing diverse viewpoints for accident scenarios and enabling V2X
(vehicle-to-everything) research on perception and prediction tasks. Finally,
we present a baseline V2X model named V2XFormer that demonstrates superior
performance for motion and accident prediction and 3D object detection compared
to the single-vehicle model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sukmin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1&quot;&gt;Wenxuan Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1&quot;&gt;Enze Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_C/0/1/0/all/0/1&quot;&gt;Chongjian Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junsong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenguo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.07072">
<title>CornerFormer: Boosting Corner Representation for Fine-Grained Structured Reconstruction. (arXiv:2304.07072v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.07072</link>
<description rdf:parseType="Literal">&lt;p&gt;Structured reconstruction is a non-trivial dense prediction problem, which
extracts structural information (\eg, building corners and edges) from a raster
image, then reconstructs it to a 2D planar graph accordingly. Compared with
common segmentation or detection problems, it significantly relays on the
capability that leveraging holistic geometric information for structural
reasoning. Current transformer-based approaches tackle this challenging problem
in a two-stage manner, which detect corners in the first model and classify the
proposed edges (corner-pairs) in the second model. However, they separate
two-stage into different models and only share the backbone encoder. Unlike the
existing modeling strategies, we present an enhanced corner representation
method: 1) It fuses knowledge between the corner detection and edge prediction
by sharing feature in different granularity; 2) Corner candidates are proposed
in four heatmap channels w.r.t its direction. Both qualitative and quantitative
evaluations demonstrate that our proposed method can better reconstruct
fine-grained structures, such as adjacent corners and tiny edges. Consequently,
it outperforms the state-of-the-art model by +1.9\%@F-1 on Corner and
+3.0\%@F-1 on Edge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1&quot;&gt;Hongbo Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yulong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Linzhi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_X/0/1/0/all/0/1&quot;&gt;Xu Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yue Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jiani Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.14123">
<title>MCLFIQ: Mobile Contactless Fingerprint Image Quality. (arXiv:2304.14123v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.14123</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose MCLFIQ: Mobile Contactless Fingerprint Image Quality, the first
quality assessment algorithm for mobile contactless fingerprint samples. To
this end, we re-trained the NIST Fingerprint Image Quality (NFIQ) 2 method,
which was originally designed for contact-based fingerprints, with a synthetic
contactless fingerprint database. We evaluate the predictive performance of the
resulting MCLFIQ model in terms of Error-vs.-Discard Characteristic (EDC)
curves on three real-world contactless fingerprint databases using three
recognition algorithms. In experiments, the MCLFIQ method is compared against
the original NFIQ 2.2 method, a sharpness-based quality assessment algorithm
developed for contactless fingerprint images \rev{and the general purpose image
quality assessment method BRISQUE. Furthermore, benchmarks on four
contact-based fingerprint datasets are also conducted.}
&lt;/p&gt;
&lt;p&gt;Obtained results show that the fine-tuning of NFIQ 2 on synthetic contactless
fingerprints is a viable alternative to training on real databases. Moreover,
the evaluation shows that our MCLFIQ method works more accurate and robust
compared to all baseline methods on contactless fingerprints. We suggest
considering the proposed MCLFIQ method as a \rev{starting point for the
development of} a new standard algorithm for contactless fingerprint quality
assessment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Priesnitz_J/0/1/0/all/0/1&quot;&gt;Jannis Priesnitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weissenfeld_A/0/1/0/all/0/1&quot;&gt;Axel Wei&amp;#xdf;enfeld&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruzicka_L/0/1/0/all/0/1&quot;&gt;Laurenz Ruzicka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rathgeb_C/0/1/0/all/0/1&quot;&gt;Christian Rathgeb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strobl_B/0/1/0/all/0/1&quot;&gt;Bernhard Strobl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lessmann_R/0/1/0/all/0/1&quot;&gt;Ralph Lessmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1&quot;&gt;Christoph Busch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.14660">
<title>Segment Anything Model for Medical Images?. (arXiv:2304.14660v5 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.14660</link>
<description rdf:parseType="Literal">&lt;p&gt;The Segment Anything Model (SAM) is the first foundation model for general
image segmentation. It has achieved impressive results on various natural image
segmentation tasks. However, medical image segmentation (MIS) is more
challenging because of the complex modalities, fine anatomical structures,
uncertain and complex object boundaries, and wide-range object scales. To fully
validate SAM&apos;s performance on medical data, we collected and sorted 53
open-source datasets and built a large medical segmentation dataset with 18
modalities, 84 objects, 125 object-modality paired targets, 1050K 2D images,
and 6033K masks. We comprehensively analyzed different models and strategies on
the so-called COSMOS 1050K dataset. Our findings mainly include the following:
1) SAM showed remarkable performance in some specific objects but was unstable,
imperfect, or even totally failed in other situations. 2) SAM with the large
ViT-H showed better overall performance than that with the small ViT-B. 3) SAM
performed better with manual hints, especially box, than the Everything mode.
4) SAM could help human annotation with high labeling quality and less time. 5)
SAM was sensitive to the randomness in the center point and tight box prompts,
and may suffer from a serious performance drop. 6) SAM performed better than
interactive methods with one or a few points, but will be outpaced as the
number of points increases. 7) SAM&apos;s performance correlated to different
factors, including boundary complexity, intensity differences, etc. 8)
Finetuning the SAM on specific medical tasks could improve its average DICE
performance by 4.39% and 6.68% for ViT-B and ViT-H, respectively. We hope that
this comprehensive report can help researchers explore the potential of SAM
applications in MIS, and guide how to appropriately use and develop SAM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yuhao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Han Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chang_A/0/1/0/all/0/1&quot;&gt;Ao Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xinrui Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Rusi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Junxuan Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiongquan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chaoyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sijing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chi_H/0/1/0/all/0/1&quot;&gt;Haozhe Chi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xindi Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yue_K/0/1/0/all/0/1&quot;&gt;Kejuan Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Grau_V/0/1/0/all/0/1&quot;&gt;Vicente Grau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fan_D/0/1/0/all/0/1&quot;&gt;Deng-Ping Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dong_F/0/1/0/all/0/1&quot;&gt;Fajin Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ni_D/0/1/0/all/0/1&quot;&gt;Dong Ni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.01971">
<title>District-scale surface temperatures generated from high-resolution longitudinal thermal infrared images. (arXiv:2305.01971v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.01971</link>
<description rdf:parseType="Literal">&lt;p&gt;The paper describes a dataset that was collected by infrared thermography,
which is a non-contact, non-intrusive technique to collect data and analyze the
built environment in various aspects. While most studies focus on the city and
building scales, the rooftop observatory provides high temporal and spatial
resolution observations with dynamic interactions on the district scale. The
rooftop infrared thermography observatory with a multi-modal platform that is
capable of assessing a wide range of dynamic processes in urban systems was
deployed in Singapore. It was placed on the top of two buildings that overlook
the outdoor context of the campus of the National University of Singapore. The
platform collects remote sensing data from tropical areas on a temporal scale,
allowing users to determine the temperature trend of individual features such
as buildings, roads, and vegetation. The dataset includes 1,365,921 thermal
images collected on average at approximately 10 seconds intervals from two
locations during ten months.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1&quot;&gt;Subin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramani_V/0/1/0/all/0/1&quot;&gt;Vasantha Ramani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_M/0/1/0/all/0/1&quot;&gt;Miguel Martin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arjunan_P/0/1/0/all/0/1&quot;&gt;Pandarasamy Arjunan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chong_A/0/1/0/all/0/1&quot;&gt;Adrian Chong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biljecki_F/0/1/0/all/0/1&quot;&gt;Filip Biljecki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ignatius_M/0/1/0/all/0/1&quot;&gt;Marcel Ignatius&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poolla_K/0/1/0/all/0/1&quot;&gt;Kameshwar Poolla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miller_C/0/1/0/all/0/1&quot;&gt;Clayton Miller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10124">
<title>Principal Uncertainty Quantification with Spatial Correlation for Image Restoration Problems. (arXiv:2305.10124v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10124</link>
<description rdf:parseType="Literal">&lt;p&gt;Uncertainty quantification for inverse problems in imaging has drawn much
attention lately. Existing approaches towards this task define uncertainty
regions based on probable values per pixel, while ignoring spatial correlations
within the image, resulting in an exaggerated volume of uncertainty. In this
paper, we propose PUQ (Principal Uncertainty Quantification) -- a novel
definition and corresponding analysis of uncertainty regions that takes into
account spatial relationships within the image, thus providing reduced volume
regions. Using recent advancements in generative models, we derive uncertainty
intervals around principal components of the empirical posterior distribution,
forming an ambiguity region that guarantees the inclusion of true unseen values
with a user-defined confidence probability. To improve computational efficiency
and interpretability, we also guarantee the recovery of true unseen values
using only a few principal directions, resulting in more informative
uncertainty regions. Our approach is verified through experiments on image
colorization, super-resolution, and inpainting; its effectiveness is shown
through comparison to baseline methods, demonstrating significantly tighter
uncertainty regions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belhasin_O/0/1/0/all/0/1&quot;&gt;Omer Belhasin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romano_Y/0/1/0/all/0/1&quot;&gt;Yaniv Romano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freedman_D/0/1/0/all/0/1&quot;&gt;Daniel Freedman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rivlin_E/0/1/0/all/0/1&quot;&gt;Ehud Rivlin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elad_M/0/1/0/all/0/1&quot;&gt;Michael Elad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15404">
<title>RoMa: Robust Dense Feature Matching. (arXiv:2305.15404v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15404</link>
<description rdf:parseType="Literal">&lt;p&gt;Feature matching is an important computer vision task that involves
estimating correspondences between two images of a 3D scene, and dense methods
estimate all such correspondences. The aim is to learn a robust model, i.e., a
model able to match under challenging real-world changes. In this work, we
propose such a model, leveraging frozen pretrained features from the foundation
model DINOv2. Although these features are significantly more robust than local
features trained from scratch, they are inherently coarse. We therefore combine
them with specialized ConvNet fine features, creating a precisely localizable
feature pyramid. To further improve robustness, we propose a tailored
transformer match decoder that predicts anchor probabilities, which enables it
to express multimodality. Finally, we propose an improved loss formulation
through regression-by-classification with subsequent robust regression. We
conduct a comprehensive set of experiments that show that our method, RoMa,
achieves significant gains, setting a new state-of-the-art. In particular, we
achieve a 36% improvement on the extremely challenging WxBS benchmark. Code is
provided at https://github.com/Parskatt/RoMa
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Edstedt_J/0/1/0/all/0/1&quot;&gt;Johan Edstedt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1&quot;&gt;Qiyu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bokman_G/0/1/0/all/0/1&quot;&gt;Georg B&amp;#xf6;kman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wadenback_M/0/1/0/all/0/1&quot;&gt;M&amp;#xe5;rten Wadenb&amp;#xe4;ck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1&quot;&gt;Michael Felsberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16318">
<title>Referred by Multi-Modality: A Unified Temporal Transformer for Video Object Segmentation. (arXiv:2305.16318v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16318</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, video object segmentation (VOS) referred by multi-modal signals,
e.g., language and audio, has evoked increasing attention in both industry and
academia. It is challenging for exploring the semantic alignment within
modalities and the visual correspondence across frames. However, existing
methods adopt separate network architectures for different modalities, and
neglect the inter-frame temporal interaction with references. In this paper, we
propose MUTR, a Multi-modal Unified Temporal transformer for Referring video
object segmentation. With a unified framework for the first time, MUTR adopts a
DETR-style transformer and is capable of segmenting video objects designated by
either text or audio reference. Specifically, we introduce two strategies to
fully explore the temporal relations between videos and multi-modal signals.
Firstly, for low-level temporal aggregation before the transformer, we enable
the multi-modal references to capture multi-scale visual cues from consecutive
video frames. This effectively endows the text or audio signals with temporal
knowledge and boosts the semantic alignment between modalities. Secondly, for
high-level temporal interaction after the transformer, we conduct inter-frame
feature communication for different object embeddings, contributing to better
object-wise correspondence for tracking along the video. On Ref-YouTube-VOS and
AVSBench datasets with respective text and audio references, MUTR achieves
+4.2% and +8.7% J&amp;amp;F improvements to state-of-the-art methods, demonstrating our
significance for unified multi-modal VOS. Code is released at
https://github.com/OpenGVLab/MUTR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1&quot;&gt;Shilin Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Renrui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Ziyu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenchao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zhongjiang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1&quot;&gt;Peng Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17421">
<title>FoPro-KD: Fourier Prompted Effective Knowledge Distillation for Long-Tailed Medical Image Recognition. (arXiv:2305.17421v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17421</link>
<description rdf:parseType="Literal">&lt;p&gt;Representational transfer from publicly available models is a promising
technique for improving medical image classification, especially in long-tailed
datasets with rare diseases. However, existing methods often overlook the
frequency-dependent behavior of these models, thereby limiting their
effectiveness in transferring representations and generalizations to rare
diseases. In this paper, we propose FoPro-KD, a novel framework that leverages
the power of frequency patterns learned from frozen pre-trained models to
enhance their transferability and compression, presenting a few unique
insights: 1) We demonstrate that leveraging representations from publicly
available pre-trained models can substantially improve performance,
specifically for rare classes, even when utilizing representations from a
smaller pre-trained model. 2) We observe that pre-trained models exhibit
frequency preferences, which we explore using our proposed Fourier Prompt
Generator (FPG), allowing us to manipulate specific frequencies in the input
image, enhancing the discriminative representational transfer. 3) By amplifying
or diminishing these frequencies in the input image, we enable Effective
Knowledge Distillation (EKD). EKD facilitates the transfer of knowledge from
pre-trained models to smaller models. Through extensive experiments in
long-tailed gastrointestinal image recognition and skin lesion classification,
where rare diseases are prevalent, our FoPro-KD framework outperforms existing
methods, enabling more accessible medical models for rare disease
classification. Code is available at https://github.com/xmed-lab/FoPro-KD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Elbatel_M/0/1/0/all/0/1&quot;&gt;Marawan Elbatel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Marti_R/0/1/0/all/0/1&quot;&gt;Robert Mart&amp;#xed;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaomeng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19302">
<title>Smooth, exact rotational symmetrization for deep learning on point clouds. (arXiv:2305.19302v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19302</link>
<description rdf:parseType="Literal">&lt;p&gt;Point clouds are versatile representations of 3D objects and have found
widespread application in science and engineering. Many successful
deep-learning models have been proposed that use them as input. The domain of
chemical and materials modeling is especially challenging because exact
compliance with physical constraints is highly desirable for a model to be
usable in practice. These constraints include smoothness and invariance with
respect to translations, rotations, and permutations of identical atoms. If
these requirements are not rigorously fulfilled, atomistic simulations might
lead to absurd outcomes even if the model has excellent accuracy. Consequently,
dedicated architectures, which achieve invariance by restricting their design
space, have been developed. General-purpose point-cloud models are more varied
but often disregard rotational symmetry. We propose a general symmetrization
method that adds rotational equivariance to any given model while preserving
all the other requirements. Our approach simplifies the development of better
atomic-scale ML schemes by relaxing the constraints on the design space and
making it possible to incorporate ideas that proved effective in other domains.
We demonstrate this idea by introducing the Point Edge Transformer (PET)
architecture, which is not intrinsically equivariant but achieves
state-of-the-art performance on several benchmark datasets of molecules and
solids. A-posteriori application of our general protocol makes PET exactly
equivariant, with minimal changes to its accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pozdnyakov_S/0/1/0/all/0/1&quot;&gt;Sergey N. Pozdnyakov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ceriotti_M/0/1/0/all/0/1&quot;&gt;Michele Ceriotti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.02099">
<title>Enhancing Surface Neural Implicits with Curvature-Guided Sampling and Uncertainty-Augmented Representations. (arXiv:2306.02099v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.02099</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural implicits have become popular for representing surfaces because they
offer an adaptive resolution and support arbitrary topologies. While previous
works rely on ground truth point clouds, they often ignore the effect of input
quality and sampling methods during reconstructing process. In this paper, we
introduce a sampling method with an uncertainty-augmented surface implicit
representation that employs a sampling technique that considers the geometric
characteristics of inputs. To this end, we introduce a strategy that
efficiently computes differentiable geometric features, namely, mean
curvatures, to augment the sampling phase during the training period. The
uncertainty augmentation offers insights into the occupancy and reliability of
the output signed distance value, thereby expanding representation capabilities
into open surfaces. Finally, we demonstrate that our method leads to
state-of-the-art reconstructions on both synthetic and real-world data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sang_L/0/1/0/all/0/1&quot;&gt;Lu Sang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saroha_A/0/1/0/all/0/1&quot;&gt;Abhishek Saroha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1&quot;&gt;Maolin Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1&quot;&gt;Daniel Cremers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04356">
<title>Fine-Grained Visual Prompting. (arXiv:2306.04356v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04356</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision-Language Models (VLMs), such as CLIP, have demonstrated impressive
zero-shot transfer capabilities in image-level visual perception. However,
these models have shown limited performance in instance-level tasks that demand
precise localization and recognition. Previous works have suggested that
incorporating visual prompts, such as colorful boxes or circles, can improve
the ability of models to recognize objects of interest. Nonetheless, compared
to language prompting, visual prompting designs are rarely explored. Existing
approaches, which employ coarse visual cues such as colorful boxes or circles,
often result in sub-optimal performance due to the inclusion of irrelevant and
noisy pixels. In this paper, we carefully study the visual prompting designs by
exploring more fine-grained markings, such as segmentation masks and their
variations. In addition, we introduce a new zero-shot framework that leverages
pixel-level annotations acquired from a generalist segmentation model for
fine-grained visual prompting. Consequently, our investigation reveals that a
straightforward application of blur outside the target mask, referred to as the
Blur Reverse Mask, exhibits exceptional effectiveness. This proposed prompting
strategy leverages the precise mask annotations to reduce focus on weakly
related regions while retaining spatial coherence between the target and the
surrounding background. Our Fine-Grained Visual Prompting (FGVP) demonstrates
superior performance in zero-shot comprehension of referring expressions on the
RefCOCO, RefCOCO+, and RefCOCOg benchmarks. It outperforms prior methods by an
average margin of 3.0% to 4.6%, with a maximum improvement of 12.5% on the
RefCOCO+ testA subset. Code is available at https://github.com/ylingfeng/FGVP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lingfeng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yueze Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinlong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jian Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09124">
<title>DIFFender: Diffusion-Based Adversarial Defense against Patch Attacks. (arXiv:2306.09124v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09124</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial attacks, particularly patch attacks, pose significant threats to
the robustness and reliability of deep learning models. Developing reliable
defenses against patch attacks is crucial for real-world applications, yet
current research in this area is unsatisfactory. In this paper, we propose
DIFFender, a novel defense method that leverages a text-guided diffusion model
to defend against adversarial patches. DIFFender includes two main stages:
patch localization and patch restoration. In the localization stage, we find
and exploit an intriguing property of the diffusion model to precisely identify
the locations of adversarial patches. In the restoration stage, we employ the
diffusion model to reconstruct the adversarial regions in the images while
preserving the integrity of the visual content. Thanks to the former finding,
these two stages can be simultaneously guided by a unified diffusion model.
Thus, we can utilize the close interaction between them to improve the whole
defense performance. Moreover, we propose a few-shot prompt-tuning algorithm to
fine-tune the diffusion model, enabling the pre-trained diffusion model to
adapt to the defense task easily. We conduct extensive experiments on image
classification, face recognition, and further in the physical world,
demonstrating that our proposed method exhibits superior robustness under
strong adaptive attacks and generalizes well across various scenarios, diverse
classifiers, and multiple patch attack methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_C/0/1/0/all/0/1&quot;&gt;Caixin Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yinpeng Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhengyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruan_S/0/1/0/all/0/1&quot;&gt;Shouwei Ruan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yubo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hang Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xingxing Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.12153">
<title>DIAS: A Comprehensive Dataset and Benchmark for Intracranial Artery Segmentation in DSA sequences. (arXiv:2306.12153v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.12153</link>
<description rdf:parseType="Literal">&lt;p&gt;Digital subtraction angiography (DSA) is universally acknowledged as the gold
standard for examining lesion angioarchitecture, elucidating arterial blood
supply dynamics, and guiding endovascular interventions. The automatic
segmentation of intracranial arteries (IA) in DSA, which is pivotal for
quantifying vascular morphology, plays an essential role in computer-assisted
stroke research and clinical practices. Nevertheless, research in this specific
domain remains constrained, primarily owing to the unavailability of publicly
datasets for IA segmentation within the research community. Currently, the
predominant focus of methodologies lies in the segmentation of single-frame DSA
using in-house datasets. These methods, limited by the partial inclusion of
contrast in single-frame DSA, encounters challenges in rendering a precise
representation of vascular structures. In this paper, we introduces DIAS, a
dataset specifically developed for IA segmentation in DSA sequences. A
comprehensive benchmark has been established for evaluating DIAS, covering
fully, weakly, and semi-supervised segmentation methods. Specifically, we
propose a vessel sequence segmentation network that captures the spatiotemporal
representation of intravascular contrast for segmenting vessels in DSA
sequences. For weakly-supervised learning, we propose a novel scribble
learning-based image segmentation framework, incorporating both scribble
supervision and consistency regularization. Furthermore, we introduce a random
patch-based self-training framework that harnesses unlabeled DSA sequences to
improve segmentation performance. Our extensive experiments on the DIAS dataset
demonstrate the effectiveness of these methods as potential baselines for
future research and clinical applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wentao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tian_T/0/1/0/all/0/1&quot;&gt;Tong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lemeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Weijin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haoyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wenyi Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tian_S/0/1/0/all/0/1&quot;&gt;Siyu Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pan_X/0/1/0/all/0/1&quot;&gt;Xipeng Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Huihua Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gao_F/0/1/0/all/0/1&quot;&gt;Feng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yiming Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Su_R/0/1/0/all/0/1&quot;&gt;Ruisheng Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.12424">
<title>VisoGender: A dataset for benchmarking gender bias in image-text pronoun resolution. (arXiv:2306.12424v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.12424</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce VisoGender, a novel dataset for benchmarking gender bias in
vision-language models. We focus on occupation-related biases within a
hegemonic system of binary gender, inspired by Winograd and Winogender schemas,
where each image is associated with a caption containing a pronoun relationship
of subjects and objects in the scene. VisoGender is balanced by gender
representation in professional roles, supporting bias evaluation in two ways:
i) resolution bias, where we evaluate the difference between pronoun resolution
accuracies for image subjects with gender presentations perceived as masculine
versus feminine by human annotators and ii) retrieval bias, where we compare
ratios of professionals perceived to have masculine and feminine gender
presentations retrieved for a gender-neutral search query. We benchmark several
state-of-the-art vision-language models and find that they demonstrate bias in
resolving binary gender in complex scenes. While the direction and magnitude of
gender bias depends on the task and the model being evaluated, captioning
models are generally less biased than Vision-Language Encoders. Dataset and
code are available at https://github.com/oxai/visogender
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hall_S/0/1/0/all/0/1&quot;&gt;Siobhan Mackenzie Hall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abrantes_F/0/1/0/all/0/1&quot;&gt;Fernanda Gon&amp;#xe7;alves Abrantes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Hanwen Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sodunke_G/0/1/0/all/0/1&quot;&gt;Grace Sodunke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shtedritski_A/0/1/0/all/0/1&quot;&gt;Aleksandar Shtedritski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirk_H/0/1/0/all/0/1&quot;&gt;Hannah Rose Kirk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14435">
<title>DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing. (arXiv:2306.14435v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14435</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate and controllable image editing is a challenging task that has
attracted significant attention recently. Notably, DragGAN is an interactive
point-based image editing framework that achieves impressive editing results
with pixel-level precision. However, due to its reliance on generative
adversarial networks (GANs), its generality is limited by the capacity of
pretrained GAN models. In this work, we extend this editing framework to
diffusion models and propose a novel approach DragDiffusion. By harnessing
large-scale pretrained diffusion models, we greatly enhance the applicability
of interactive point-based editing on both real and diffusion-generated images.
Our approach involves optimizing the diffusion latents to achieve precise
spatial control. The supervision signal of this optimization process is from
the diffusion model&apos;s UNet features, which are known to contain rich semantic
and geometric information. Moreover, we introduce two additional techniques,
namely LoRA fine-tuning and latent-MasaCtrl, to further preserve the identity
of the original image. Lastly, we present a challenging benchmark dataset
called DragBench -- the first benchmark to evaluate the performance of
interactive point-based image editing methods. Experiments across a wide range
of challenging cases (e.g., images with multiple objects, diverse object
categories, various styles, etc.) demonstrate the versatility and generality of
DragDiffusion. Code: https://github.com/Yujun-Shi/DragDiffusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yujun Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_C/0/1/0/all/0/1&quot;&gt;Chuhui Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liew_J/0/1/0/all/0/1&quot;&gt;Jun Hao Liew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Jiachun Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1&quot;&gt;Hanshu Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1&quot;&gt;Vincent Y. F. Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1&quot;&gt;Song Bai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00910">
<title>CoPL: Contextual Prompt Learning for Vision-Language Understanding. (arXiv:2307.00910v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00910</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in multimodal learning has resulted in powerful
vision-language models, whose representations are generalizable across a
variety of downstream tasks. Recently, their generalization ability has been
further extended by incorporating trainable prompts, borrowed from the natural
language processing literature. While such prompt learning techniques have
shown impressive results, we identify that these prompts are trained based on
global image features which limits itself in two aspects: First, by using
global features, these prompts could be focusing less on the discriminative
foreground image, resulting in poor generalization to various
out-of-distribution test cases. Second, existing work weights all prompts
equally whereas intuitively, prompts should be reweighed according to the
semantics of the image. We address these as part of our proposed Contextual
Prompt Learning (CoPL) framework, capable of aligning the prompts to the
localized features of the image. Our key innovations over earlier works include
using local image features as part of the prompt learning process, and more
crucially, learning to weight these prompts based on local features that are
appropriate for the task at hand. This gives us dynamic prompts that are both
aligned to local image features as well as aware of local contextual
relationships. Our extensive set of experiments on a variety of standard and
few-shot datasets show that our method produces substantially improved
performance when compared to the current state of the art methods. We also
demonstrate both few-shot and out-of-distribution performance to establish the
utility of learning dynamic prompts that are aligned to local image features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goswami_K/0/1/0/all/0/1&quot;&gt;Koustava Goswami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karanam_S/0/1/0/all/0/1&quot;&gt;Srikrishna Karanam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Udhayanan_P/0/1/0/all/0/1&quot;&gt;Prateksha Udhayanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joseph_K/0/1/0/all/0/1&quot;&gt;K J Joseph&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivasan_B/0/1/0/all/0/1&quot;&gt;Balaji Vasan Srinivasan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05845">
<title>PIGEON: Predicting Image Geolocations. (arXiv:2307.05845v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05845</link>
<description rdf:parseType="Literal">&lt;p&gt;Planet-scale image geolocalization remains a challenging problem due to the
diversity of images originating from anywhere in the world. Although approaches
based on vision transformers have made significant progress in geolocalization
accuracy, success in prior literature is constrained to narrow distributions of
images of landmarks, and performance has not generalized to unseen places. We
present a new geolocalization system that combines semantic geocell creation,
multi-task contrastive pretraining, and a novel loss function. Additionally,
our work is the first to perform retrieval over location clusters for guess
refinements. We train two models for evaluations on street-level data and
general-purpose image geolocalization; the first model, PIGEON, is trained on
data from the game of Geoguessr and is capable of placing over 40% of its
guesses within 25 kilometers of the target location globally. We also develop a
bot and deploy PIGEON in a blind experiment against humans, ranking in the top
0.01% of players. We further challenge one of the world&apos;s foremost professional
Geoguessr players to a series of six matches with millions of viewers, winning
all six games. Our second model, PIGEOTTO, differs in that it is trained on a
dataset of images from Flickr and Wikipedia, achieving state-of-the-art results
on a wide range of image geolocalization benchmarks, outperforming the previous
SOTA by up to 7.7 percentage points on the city accuracy level and up to 38.8
percentage points on the country level. Our findings suggest that PIGEOTTO is
the first image geolocalization model that effectively generalizes to unseen
places and that our approach can pave the way for highly accurate, planet-scale
image geolocalization systems. Our code is available on GitHub.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haas_L/0/1/0/all/0/1&quot;&gt;Lukas Haas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skreta_M/0/1/0/all/0/1&quot;&gt;Michal Skreta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alberti_S/0/1/0/all/0/1&quot;&gt;Silas Alberti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1&quot;&gt;Chelsea Finn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04669">
<title>A General Implicit Framework for Fast NeRF Composition and Rendering. (arXiv:2308.04669v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.04669</link>
<description rdf:parseType="Literal">&lt;p&gt;A variety of Neural Radiance Fields (NeRF) methods have recently achieved
remarkable success in high render speed. However, current accelerating methods
are specialized and incompatible with various implicit methods, preventing
real-time composition over various types of NeRF works. Because NeRF relies on
sampling along rays, it is possible to provide general guidance for
acceleration. To that end, we propose a general implicit pipeline for composing
NeRF objects quickly. Our method enables the casting of dynamic shadows within
or between objects using analytical light sources while allowing multiple NeRF
objects to be seamlessly placed and rendered together with any arbitrary rigid
transformations. Mainly, our work introduces a new surface representation known
as Neural Depth Fields (NeDF) that quickly determines the spatial relationship
between objects by allowing direct intersection computation between rays and
implicit surfaces. It leverages an intersection neural network to query NeRF
for acceleration instead of depending on an explicit spatial structure.Our
proposed method is the first to enable both the progressive and interactive
composition of NeRF objects. Additionally, it also serves as a previewing
plugin for a range of existing NeRF works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xinyu Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Ziyi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yunlu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yuxiang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1&quot;&gt;Xiaogang Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_C/0/1/0/all/0/1&quot;&gt;Changqing Zou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11488">
<title>Opening the Vocabulary of Egocentric Actions. (arXiv:2308.11488v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11488</link>
<description rdf:parseType="Literal">&lt;p&gt;Human actions in egocentric videos are often hand-object interactions
composed from a verb (performed by the hand) applied to an object. Despite
their extensive scaling up, egocentric datasets still face two limitations -
sparsity of action compositions and a closed set of interacting objects. This
paper proposes a novel open vocabulary action recognition task. Given a set of
verbs and objects observed during training, the goal is to generalize the verbs
to an open vocabulary of actions with seen and novel objects. To this end, we
decouple the verb and object predictions via an object-agnostic verb encoder
and a prompt-based object encoder. The prompting leverages CLIP representations
to predict an open vocabulary of interacting objects. We create open vocabulary
benchmarks on the EPIC-KITCHENS-100 and Assembly101 datasets; whereas
closed-action methods fail to generalize, our proposed method is effective. In
addition, our object encoder significantly outperforms existing open-vocabulary
visual recognition methods in recognizing novel interacting objects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatterjee_D/0/1/0/all/0/1&quot;&gt;Dibyadip Chatterjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sener_F/0/1/0/all/0/1&quot;&gt;Fadime Sener&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Shugao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1&quot;&gt;Angela Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16139">
<title>MedShapeNet -- A Large-Scale Dataset of 3D Medical Shapes for Computer Vision. (arXiv:2308.16139v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.16139</link>
<description rdf:parseType="Literal">&lt;p&gt;Prior to the deep learning era, shape was commonly used to describe the
objects. Nowadays, state-of-the-art (SOTA) algorithms in medical imaging are
predominantly diverging from computer vision, where voxel grids, meshes, point
clouds, and implicit surface models are used. This is seen from numerous
shape-related publications in premier vision conferences as well as the growing
popularity of ShapeNet (about 51,300 models) and Princeton ModelNet (127,915
models). For the medical domain, we present a large collection of anatomical
shapes (e.g., bones, organs, vessels) and 3D models of surgical instrument,
called MedShapeNet, created to facilitate the translation of data-driven vision
algorithms to medical applications and to adapt SOTA vision algorithms to
medical problems. As a unique feature, we directly model the majority of shapes
on the imaging data of real patients. As of today, MedShapeNet includes 23
dataset with more than 100,000 shapes that are paired with annotations (ground
truth). Our data is freely accessible via a web interface and a Python
application programming interface (API) and can be used for discriminative,
reconstructive, and variational benchmarks as well as various applications in
virtual, augmented, or mixed reality, and 3D printing. Exemplary, we present
use cases in the fields of classification of brain tumors, facial and skull
reconstructions, multi-class anatomy completion, education, and 3D printing. In
future, we will extend the data and improve the interfaces. The project pages
are: https://medshapenet.ikim.nrw/ and
https://github.com/Jianningli/medshapenet-feedback
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianning Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zongwei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jiancheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pepe_A/0/1/0/all/0/1&quot;&gt;Antonio Pepe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gsaxner_C/0/1/0/all/0/1&quot;&gt;Christina Gsaxner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luijten_G/0/1/0/all/0/1&quot;&gt;Gijs Luijten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_C/0/1/0/all/0/1&quot;&gt;Chongyu Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tiezheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaoxi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenxuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wodzinski_M/0/1/0/all/0/1&quot;&gt;Marek Wodzinski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Friedrich_P/0/1/0/all/0/1&quot;&gt;Paul Friedrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_K/0/1/0/all/0/1&quot;&gt;Kangxian Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yuan Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ambigapathy_N/0/1/0/all/0/1&quot;&gt;Narmada Ambigapathy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nasca_E/0/1/0/all/0/1&quot;&gt;Enrico Nasca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solak_N/0/1/0/all/0/1&quot;&gt;Naida Solak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melito_G/0/1/0/all/0/1&quot;&gt;Gian Marco Melito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vu_V/0/1/0/all/0/1&quot;&gt;Viet Duc Vu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Memon_A/0/1/0/all/0/1&quot;&gt;Afaque R. Memon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlachta_C/0/1/0/all/0/1&quot;&gt;Christopher Schlachta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribaupierre_S/0/1/0/all/0/1&quot;&gt;Sandrine De Ribaupierre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_R/0/1/0/all/0/1&quot;&gt;Rajnikant Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eagleson_R/0/1/0/all/0/1&quot;&gt;Roy Eagleson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaojun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Machler_H/0/1/0/all/0/1&quot;&gt;Heinrich M&amp;#xe4;chler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirschke_J/0/1/0/all/0/1&quot;&gt;Jan Stefan Kirschke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosa_E/0/1/0/all/0/1&quot;&gt;Ezequiel de la Rosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christ_P/0/1/0/all/0/1&quot;&gt;Patrick Ferdinand Christ&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongwei Bran Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ellis_D/0/1/0/all/0/1&quot;&gt;David G. Ellis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aizenberg_M/0/1/0/all/0/1&quot;&gt;Michele R. Aizenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gatidis_S/0/1/0/all/0/1&quot;&gt;Sergios Gatidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kustner_T/0/1/0/all/0/1&quot;&gt;Thomas K&amp;#xfc;stner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shusharina_N/0/1/0/all/0/1&quot;&gt;Nadya Shusharina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heller_N/0/1/0/all/0/1&quot;&gt;Nicholas Heller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andrearczyk_V/0/1/0/all/0/1&quot;&gt;Vincent Andrearczyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Depeursinge_A/0/1/0/all/0/1&quot;&gt;Adrien Depeursinge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hatt_M/0/1/0/all/0/1&quot;&gt;Mathieu Hatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sekuboyina_A/0/1/0/all/0/1&quot;&gt;Anjany Sekuboyina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loffler_M/0/1/0/all/0/1&quot;&gt;Maximilian L&amp;#xf6;ffler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liebl_H/0/1/0/all/0/1&quot;&gt;Hans Liebl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dorent_R/0/1/0/all/0/1&quot;&gt;Reuben Dorent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vercauteren_T/0/1/0/all/0/1&quot;&gt;Tom Vercauteren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shapey_J/0/1/0/all/0/1&quot;&gt;Jonathan Shapey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kujawa_A/0/1/0/all/0/1&quot;&gt;Aaron Kujawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cornelissen_S/0/1/0/all/0/1&quot;&gt;Stefan Cornelissen&lt;/a&gt;, et al. (110 additional authors not shown)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16576">
<title>GHuNeRF: Generalizable Human NeRF from a Monocular Video. (arXiv:2308.16576v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.16576</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we tackle the challenging task of learning a generalizable
human NeRF model from a monocular video. Although existing generalizable human
NeRFs have achieved impressive results, they require muti-view images or videos
which might not be always available. On the other hand, some works on
free-viewpoint rendering of human from monocular videos cannot be generalized
to unseen identities. In view of these limitations, we propose GHuNeRF to learn
a generalizable human NeRF model from a monocular video of the human performer.
We first introduce a visibility-aware aggregation scheme to compute vertex-wise
features, which is used to construct a 3D feature volume. The feature volume
can only represent the overall geometry of the human performer with
insufficient accuracy due to the limited resolution. To solve this, we further
enhance the volume feature with temporally aligned point-wise features using an
attention mechanism. Finally, the enhanced feature is used for predicting
density and color for each sampled point. A surface-guided sampling strategy is
also adopted to improve the efficiency for both training and inference. We
validate our approach on the widely-used ZJU-MoCap dataset, where we achieve
comparable performance with existing multi-view video based approaches. We also
test on the monocular People-Snapshot dataset and achieve better performance
than existing works when only monocular video is used. Our code is available at
the project website.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jiahao Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1&quot;&gt;Gim Hee Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16876">
<title>SportsSloMo: A New Benchmark and Baselines for Human-centric Video Frame Interpolation. (arXiv:2308.16876v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.16876</link>
<description rdf:parseType="Literal">&lt;p&gt;Human-centric video frame interpolation has great potential for improving
people&apos;s entertainment experiences and finding commercial applications in the
sports analysis industry, e.g., synthesizing slow-motion videos. Although there
are multiple benchmark datasets available in the community, none of them is
dedicated for human-centric scenarios. To bridge this gap, we introduce
SportsSloMo, a benchmark consisting of more than 130K video clips and 1M video
frames of high-resolution ($\geq$720p) slow-motion sports videos crawled from
YouTube. We re-train several state-of-the-art methods on our benchmark, and the
results show a decrease in their accuracy compared to other datasets. It
highlights the difficulty of our benchmark and suggests that it poses
significant challenges even for the best-performing methods, as human bodies
are highly deformable and occlusions are frequent in sports videos. To improve
the accuracy, we introduce two loss terms considering the human-aware priors,
where we add auxiliary supervision to panoptic segmentation and human keypoints
detection, respectively. The loss terms are model agnostic and can be easily
plugged into any video frame interpolation approaches. Experimental results
validate the effectiveness of our proposed loss terms, leading to consistent
performance improvement over 5 existing models, which establish strong baseline
models on our benchmark. The dataset and code can be found at:
https://neu-vi.github.io/SportsSlomo/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiaben Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Huaizu Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.08769">
<title>The Use of Multi-Scale Fiducial Markers To Aid Takeoff and Landing Navigation by Rotorcraft. (arXiv:2309.08769v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.08769</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper quantifies the performance of visual SLAM that leverages
multi-scale fiducial markers (i.e., artificial landmarks that can be detected
at a wide range of distances) to show its potential for reliable takeoff and
landing navigation in rotorcraft. Prior work has shown that square markers with
a black-and-white pattern of grid cells can be used to improve the performance
of visual SLAM with color cameras. We extend this prior work to allow nested
marker layouts. We evaluate performance during semi-autonomous takeoff and
landing operations in a variety of environmental conditions by a DJI Matrice
300 RTK rotorcraft with two FLIR Blackfly color cameras, using RTK GNSS to
obtain ground truth pose estimates. Performance measures include absolute
trajectory error and the fraction of the number of estimated poses to the total
frame. We release all of our results -- our dataset and the code of the
implementation of the visual SLAM with fiducial markers -- to the public as
open-source.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jongwon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1&quot;&gt;Su Yeon Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bretl_T/0/1/0/all/0/1&quot;&gt;Timothy Bretl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.09431">
<title>FactoFormer: Factorized Hyperspectral Transformers with Self-Supervised Pre-Training. (arXiv:2309.09431v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.09431</link>
<description rdf:parseType="Literal">&lt;p&gt;Hyperspectral images (HSIs) contain rich spectral and spatial information.
Motivated by the success of transformers in the field of natural language
processing and computer vision where they have shown the ability to learn long
range dependencies within input data, recent research has focused on using
transformers for HSIs. However, current state-of-the-art hyperspectral
transformers only tokenize the input HSI sample along the spectral dimension,
resulting in the under-utilization of spatial information. Moreover,
transformers are known to be data-hungry and their performance relies heavily
on large-scale pre-training, which is challenging due to limited annotated
hyperspectral data. Therefore, the full potential of HSI transformers has not
been fully realized. To overcome these limitations, we propose a novel
factorized spectral-spatial transformer that incorporates factorized
self-supervised pre-training procedures, leading to significant improvements in
performance. The factorization of the inputs allows the spectral and spatial
transformers to better capture the interactions within the hyperspectral data
cubes. Inspired by masked image modeling pre-training, we also devise efficient
masking strategies for pre-training each of the spectral and spatial
transformers. We conduct experiments on six publicly available datasets for HSI
classification task and demonstrate that our model achieves state-of-the-art
performance in all the datasets. The code for our model will be made available
at https://github.com/csiro-robotics/factoformer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohamed_S/0/1/0/all/0/1&quot;&gt;Shaheer Mohamed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haghighat_M/0/1/0/all/0/1&quot;&gt;Maryam Haghighat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernando_T/0/1/0/all/0/1&quot;&gt;Tharindu Fernando&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1&quot;&gt;Sridha Sridharan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1&quot;&gt;Clinton Fookes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moghadam_P/0/1/0/all/0/1&quot;&gt;Peyman Moghadam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13747">
<title>Look Ma, no code: fine tuning nnU-Net for the AutoPET II challenge by only adjusting its JSON plans. (arXiv:2309.13747v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.13747</link>
<description rdf:parseType="Literal">&lt;p&gt;We participate in the AutoPET II challenge by modifying nnU-Net only through
its easy to understand and modify &apos;nnUNetPlans.json&apos; file. By switching to a
UNet with residual encoder, increasing the batch size and increasing the patch
size we obtain a configuration that substantially outperforms the automatically
configured nnU-Net baseline (5-fold cross-validation Dice score of 65.14 vs
33.28) at the expense of increased compute requirements for model training. Our
final submission ensembles the two most promising configurations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Isensee_F/0/1/0/all/0/1&quot;&gt;Fabian Isensee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Maier_Hein_K/0/1/0/all/0/1&quot;&gt;Klaus H.Maier-Hein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14585">
<title>DifAttack: Query-Efficient Black-Box Attack via Disentangled Feature Space. (arXiv:2309.14585v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14585</link>
<description rdf:parseType="Literal">&lt;p&gt;This work investigates efficient score-based black-box adversarial attacks
with a high Attack Success Rate (ASR) and good generalizability. We design a
novel attack method based on a Disentangled Feature space, called DifAttack,
which differs significantly from the existing ones operating over the entire
feature space. Specifically, DifAttack firstly disentangles an image&apos;s latent
feature into an adversarial feature and a visual feature, where the former
dominates the adversarial capability of an image, while the latter largely
determines its visual appearance. We train an autoencoder for the
disentanglement by using pairs of clean images and their Adversarial Examples
(AEs) generated from available surrogate models via white-box attack methods.
Eventually, DifAttack iteratively optimizes the adversarial feature according
to the query feedback from the victim model until a successful AE is generated,
while keeping the visual feature unaltered. In addition, due to the avoidance
of using surrogate models&apos; gradient information when optimizing AEs for
black-box models, our proposed DifAttack inherently possesses better attack
capability in the open-set scenario, where the training dataset of the victim
model is unknown. Extensive experimental results demonstrate that our method
achieves significant improvements in ASR and query efficiency simultaneously,
especially in the targeted attack and open-set scenarios. The code will be
available at https://github.com/csjunjun/DifAttack.git soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jun_L/0/1/0/all/0/1&quot;&gt;Liu Jun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiantao_Z/0/1/0/all/0/1&quot;&gt;Zhou Jiantao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiandian_Z/0/1/0/all/0/1&quot;&gt;Zeng Jiandian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1&quot;&gt;Jinyu Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16808">
<title>Granularity at Scale: Estimating Neighborhood Socioeconomic Indicators from High-Resolution Orthographic Imagery and Hybrid Learning. (arXiv:2309.16808v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.16808</link>
<description rdf:parseType="Literal">&lt;p&gt;Many areas of the world are without basic information on the socioeconomic
well-being of the residing population due to limitations in existing data
collection methods. Overhead images obtained remotely, such as from satellite
or aircraft, can help serve as windows into the state of life on the ground and
help &quot;fill in the gaps&quot; where community information is sparse, with estimates
at smaller geographic scales requiring higher resolution sensors. Concurrent
with improved sensor resolutions, recent advancements in machine learning and
computer vision have made it possible to quickly extract features from and
detect patterns in image data, in the process correlating these features with
other information. In this work, we explore how well two approaches, a
supervised convolutional neural network and semi-supervised clustering based on
bag-of-visual-words, estimate population density, median household income, and
educational attainment of individual neighborhoods from publicly available
high-resolution imagery of cities throughout the United States. Results and
analyses indicate that features extracted from the imagery can accurately
estimate the density (R$^2$ up to 0.81) of neighborhoods, with the supervised
approach able to explain about half the variation in a population&apos;s income and
education. In addition to the presented approaches serving as a basis for
further geographic generalization, the novel semi-supervised approach provides
a foundation for future work seeking to estimate fine-scale information from
aerial imagery without the need for label data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brewer_E/0/1/0/all/0/1&quot;&gt;Ethan Brewer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valdrighi_G/0/1/0/all/0/1&quot;&gt;Giovani Valdrighi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solunke_P/0/1/0/all/0/1&quot;&gt;Parikshit Solunke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rulff_J/0/1/0/all/0/1&quot;&gt;Joao Rulff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piadyk_Y/0/1/0/all/0/1&quot;&gt;Yurii Piadyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_Z/0/1/0/all/0/1&quot;&gt;Zhonghui Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poco_J/0/1/0/all/0/1&quot;&gt;Jorge Poco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_C/0/1/0/all/0/1&quot;&gt;Claudio Silva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04816">
<title>Hacking Generative Models with Differentiable Network Bending. (arXiv:2310.04816v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04816</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we propose a method to &apos;hack&apos; generative models, pushing their
outputs away from the original training distribution towards a new objective.
We inject a small-scale trainable module between the intermediate layers of the
model and train it for a low number of iterations, keeping the rest of the
network frozen. The resulting output images display an uncanny quality, given
by the tension between the original and new objectives that can be exploited
for artistic purposes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aldegheri_G/0/1/0/all/0/1&quot;&gt;Giacomo Aldegheri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rogalska_A/0/1/0/all/0/1&quot;&gt;Alina Rogalska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Youssef_A/0/1/0/all/0/1&quot;&gt;Ahmed Youssef&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iofinova_E/0/1/0/all/0/1&quot;&gt;Eugenia Iofinova&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07730">
<title>Domain-Controlled Prompt Learning. (arXiv:2310.07730v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07730</link>
<description rdf:parseType="Literal">&lt;p&gt;Large pre-trained vision-language models, such as CLIP, have shown remarkable
generalization capabilities across various tasks when appropriate text prompts
are provided. However, adapting these models to specific domains, like remote
sensing images (RSIs), medical images, etc, remains unexplored and challenging.
Existing prompt learning methods often lack domain-awareness or domain-transfer
mechanisms, leading to suboptimal performance due to the misinterpretation of
specific images in natural image patterns. To tackle this dilemma, we proposed
a \textbf{Domain-Controlled Prompt Learning} for the specific domains.
Specifically, the large-scale specific domain foundation model (LSDM) is first
introduced to provide essential specific domain knowledge. Using lightweight
neural networks, we transfer this knowledge into domain biases, which control
both the visual and language branches to obtain domain-adaptive prompts in a
directly incorporating manner. Simultaneously, to overcome the existing
overfitting challenge, we propose a novel noisy-adding strategy, without extra
trainable parameters, to help the model escape the suboptimal solution in a
global domain oscillation manner. Experimental results show our method achieves
state-of-the-art performance in specific domain image recognition datasets. Our
code is available at https://github.com/caoql98/DCPL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1&quot;&gt;Qinglong Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhengqin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuntian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaokang Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12474">
<title>Enhancing High-Resolution 3D Generation through Pixel-wise Gradient Clipping. (arXiv:2310.12474v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.12474</link>
<description rdf:parseType="Literal">&lt;p&gt;High-resolution 3D object generation remains a challenging task primarily due
to the limited availability of comprehensive annotated training data. Recent
advancements have aimed to overcome this constraint by harnessing image
generative models, pretrained on extensive curated web datasets, using
knowledge transfer techniques like Score Distillation Sampling (SDS).
Efficiently addressing the requirements of high-resolution rendering often
necessitates the adoption of latent representation-based models, such as the
Latent Diffusion Model (LDM). In this framework, a significant challenge
arises: To compute gradients for individual image pixels, it is necessary to
backpropagate gradients from the designated latent space through the frozen
components of the image model, such as the VAE encoder used within LDM.
However, this gradient propagation pathway has never been optimized, remaining
uncontrolled during training. We find that the unregulated gradients adversely
affect the 3D model&apos;s capacity in acquiring texture-related information from
the image generative model, leading to poor quality appearance synthesis. To
address this overarching challenge, we propose an innovative operation termed
Pixel-wise Gradient Clipping (PGC) designed for seamless integration into
existing 3D generative models, thereby enhancing their synthesis quality.
Specifically, we control the magnitude of stochastic gradients by clipping the
pixel-wise gradients efficiently, while preserving crucial texture-related
gradient directions. Despite this simplicity and minimal extra cost, extensive
experiments demonstrate the efficacy of our PGC in enhancing the performance of
existing 3D generative models for high-resolution object rendering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1&quot;&gt;Zijie Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiachen Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiatian Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Li Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15646">
<title>Mean Teacher DETR with Masked Feature Alignment: A Robust Domain Adaptive Detection Transformer Framework. (arXiv:2310.15646v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15646</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised domain adaptation object detection(UDAOD) research on Detection
Transformer(DETR) mainly focuses on feature alignment and existing methods can
be divided into two kinds, each of which has its unresolved issues. One-stage
feature alignment methods can easily lead to performance fluctuation and
training stagnation. Two-stage feature alignment method based on mean teacher
comprises a pretraining stage followed by a self-training stage, each facing
problems in obtaining reliable pretrained model and achieving consistent
performance gains. Methods mentioned above have not yet explore how to utilize
the third related domain such as target-like domain to assist adaptation. To
address these issues, we propose a two-stage framework named MTM, i.e. Mean
Teacher-DETR with Masked Feature Alignment. In the pretraining stage, we
utilize labeled target-like images produced by image style transfer to avoid
performance fluctuation. In the self-training stage, we leverage unlabeled
target images by pseudo labels based on mean teacher and propose a module
called Object Queries Knowledge Transfer(OQKT) to ensure consistent performance
gains of the student model. Most importantly, we propose masked feature
alignment methods including Masked Domain Query-based Feature Alignment(MDQFA)
and Masked Token-wise Feature Alignment(MTWFA) to alleviate domain shift in a
more robust way, which not only prevent training stagnation and lead to a
robust pretrained model in the pretraining stage, but also enhance the model&apos;s
target performance in the self-training stage. Experiments on three challenging
scenarios and a theoretical analysis verify the effectiveness of MTM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weng_W/0/1/0/all/0/1&quot;&gt;Weixi Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1&quot;&gt;Chun Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19797">
<title>DEFT: Dexterous Fine-Tuning for Real-World Hand Policies. (arXiv:2310.19797v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19797</link>
<description rdf:parseType="Literal">&lt;p&gt;Dexterity is often seen as a cornerstone of complex manipulation. Humans are
able to perform a host of skills with their hands, from making food to
operating tools. In this paper, we investigate these challenges, especially in
the case of soft, deformable objects as well as complex, relatively
long-horizon tasks. However, learning such behaviors from scratch can be data
inefficient. To circumvent this, we propose a novel approach, DEFT (DExterous
Fine-Tuning for Hand Policies), that leverages human-driven priors, which are
executed directly in the real world. In order to improve upon these priors,
DEFT involves an efficient online optimization procedure. With the integration
of human-based learning and online fine-tuning, coupled with a soft robotic
hand, DEFT demonstrates success across various tasks, establishing a robust,
data-efficient pathway toward general dexterous manipulation. Please see our
website at https://dexterous-finetuning.github.io for video results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kannan_A/0/1/0/all/0/1&quot;&gt;Aditya Kannan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaw_K/0/1/0/all/0/1&quot;&gt;Kenneth Shaw&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bahl_S/0/1/0/all/0/1&quot;&gt;Shikhar Bahl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mannam_P/0/1/0/all/0/1&quot;&gt;Pragna Mannam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1&quot;&gt;Deepak Pathak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06000">
<title>Keystroke Verification Challenge (KVC): Biometric and Fairness Benchmark Evaluation. (arXiv:2311.06000v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.06000</link>
<description rdf:parseType="Literal">&lt;p&gt;Analyzing keystroke dynamics (KD) for biometric verification has several
advantages: it is among the most discriminative behavioral traits; keyboards
are among the most common human-computer interfaces, being the primary means
for users to enter textual data; its acquisition does not require additional
hardware, and its processing is relatively lightweight; and it allows for
transparently recognizing subjects. However, the heterogeneity of experimental
protocols and metrics, and the limited size of the databases adopted in the
literature impede direct comparisons between different systems, thus
representing an obstacle in the advancement of keystroke biometrics. To
alleviate this aspect, we present a new experimental framework to benchmark
KD-based biometric verification performance and fairness based on tweet-long
sequences of variable transcript text from over 185,000 subjects, acquired
through desktop and mobile keyboards, extracted from the Aalto Keystroke
Databases. The framework runs on CodaLab in the form of the Keystroke
Verification Challenge (KVC). Moreover, we also introduce a novel fairness
metric, the Skewed Impostor Ratio (SIR), to capture inter- and
intra-demographic group bias patterns in the verification scores. We
demonstrate the usefulness of the proposed framework by employing two
state-of-the-art keystroke verification systems, TypeNet and TypeFormer, to
compare different sets of input features, achieving a less privacy-invasive
system, by discarding the analysis of text content (ASCII codes of the keys
pressed) in favor of extended features in the time domain. Our experiments show
that this approach allows to maintain satisfactory performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stragapede_G/0/1/0/all/0/1&quot;&gt;Giuseppe Stragapede&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vera_Rodriguez_R/0/1/0/all/0/1&quot;&gt;Ruben Vera-Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tolosana_R/0/1/0/all/0/1&quot;&gt;Ruben Tolosana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1&quot;&gt;Aythami Morales&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1&quot;&gt;Naser Damer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1&quot;&gt;Julian Fierrez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortega_Garcia_J/0/1/0/all/0/1&quot;&gt;Javier Ortega-Garcia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15497">
<title>Adaptive Image Registration: A Hybrid Approach Integrating Deep Learning and Optimization Functions for Enhanced Precision. (arXiv:2311.15497v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.15497</link>
<description rdf:parseType="Literal">&lt;p&gt;Image registration has traditionally been done using two distinct approaches:
learning based methods, relying on robust deep neural networks, and
optimization-based methods, applying complex mathematical transformations to
warp images accordingly. Of course, both paradigms offer advantages and
disadvantages, and, in this work, we seek to combine their respective strengths
into a single streamlined framework, using the outputs of the learning based
method as initial parameters for optimization while prioritizing computational
power for the image pairs that offer the greatest loss. Our investigations
showed that an improvement of 1.5% in testing when utilizing the best
performing state-of-the-art model as the backbone of the framework, while
maintaining the same inference time and a substantial 0.94% points performance
gain in deformation field smoothness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Araujo_G/0/1/0/all/0/1&quot;&gt;Gabriel De Araujo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Shanlin Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiaohui Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16500">
<title>LLMGA: Multimodal Large Language Model based Generation Assistant. (arXiv:2311.16500v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16500</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce a Multimodal Large Language Model-based
Generation Assistant (LLMGA), leveraging the vast reservoir of knowledge and
proficiency in reasoning, comprehension, and response inherent in Large
Language Models (LLMs) to assist users in image generation and editing.
Diverging from existing approaches where Multimodal Large Language Models
(MLLMs) generate fixed-size embeddings to control Stable Diffusion (SD), our
LLMGA provides a detailed language generation prompt for precise control over
SD. This not only augments LLM context understanding but also reduces noise in
generation prompts, yields images with more intricate and precise content, and
elevates the interpretability of the network. To this end, we curate a
comprehensive dataset comprising prompt refinement, similar image generation,
inpainting $\&amp;amp;$ outpainting, and visual question answering. Moreover, we
propose a two-stage training scheme. In the first stage, we train the MLLM to
grasp the properties of image generation and editing, enabling it to generate
detailed prompts. In the second stage, we optimize SD to align with the MLLM&apos;s
generation prompts. Additionally, we propose a reference-based restoration
network to alleviate texture, brightness, and contrast disparities between
generated and preserved regions during image editing. Extensive results show
that LLMGA has promising generative capabilities and can enable wider
applications in an interactive manner.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_B/0/1/0/all/0/1&quot;&gt;Bin Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shiyin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1&quot;&gt;Yingfan Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yitong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1&quot;&gt;Jiaya Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16516">
<title>Segment Every Out-of-Distribution Object. (arXiv:2311.16516v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16516</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic segmentation models, while effective for in-distribution categories,
face challenges in real-world deployment due to encountering
out-of-distribution (OoD) objects. Detecting these OoD objects is crucial for
safety-critical applications. Existing methods rely on anomaly scores, but
choosing a suitable threshold for generating masks presents difficulties and
can lead to fragmentation and inaccuracy. This paper introduces a method to
convert anomaly \underline{S}core \underline{T}o segmentation \underline{M}ask,
called S2M, a simple and effective framework for OoD detection in semantic
segmentation. Unlike assigning anomaly scores to pixels, S2M directly segments
the entire OoD object. By transforming anomaly scores into prompts for a
promptable segmentation model, S2M eliminates the need for threshold selection.
Extensive experiments demonstrate that S2M outperforms the state-of-the-art by
approximately 10% in IoU and 30% in mean F1 score, on average, across various
benchmarks including Fishyscapes, Segment-Me-If-You-Can, and RoadAnomaly
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wenjie Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1&quot;&gt;Xin Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1&quot;&gt;Yu Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yunhui Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17116">
<title>REF$^2$-NeRF: Reflection and Refraction aware Neural Radiance Field. (arXiv:2311.17116v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17116</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, significant progress has been made in the study of methods for 3D
reconstruction from multiple images using implicit neural representations,
exemplified by the neural radiance field (NeRF) method. Such methods, which are
based on volume rendering, can model various light phenomena, and various
extended methods have been proposed to accommodate different scenes and
situations. However, when handling scenes with multiple glass objects, e.g.,
objects in a glass showcase, modeling the target scene accurately has been
challenging due to the presence of multiple reflection and refraction effects.
Thus, this paper proposes a NeRF-based modeling method for scenes containing a
glass case. In the proposed method, refraction and reflection are modeled using
elements that are dependent and independent of the viewer&apos;s perspective. This
approach allows us to estimate the surfaces where refraction occurs, i.e.,
glass surfaces, and enables the separation and modeling of both direct and
reflected light components. Compared to existing methods, the proposed method
enables more accurate modeling of both glass refraction and the overall scene.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1&quot;&gt;Wooseok Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fukiage_T/0/1/0/all/0/1&quot;&gt;Taiki Fukiage&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oishi_T/0/1/0/all/0/1&quot;&gt;Takeshi Oishi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.18130">
<title>The Trifecta: Three simple techniques for training deeper Forward-Forward networks. (arXiv:2311.18130v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.18130</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern machine learning models are able to outperform humans on a variety of
non-trivial tasks. However, as the complexity of the models increases, they
consume significant amounts of power and still struggle to generalize
effectively to unseen data. Local learning, which focuses on updating subsets
of a model&apos;s parameters at a time, has emerged as a promising technique to
address these issues. Recently, a novel local learning algorithm, called
Forward-Forward, has received widespread attention due to its innovative
approach to learning. Unfortunately, its application has been limited to
smaller datasets due to scalability issues. To this end, we propose The
Trifecta, a collection of three simple techniques that synergize exceptionally
well and drastically improve the Forward-Forward algorithm on deeper networks.
Our experiments demonstrate that our models are on par with similarly
structured, backpropagation-based models in both training speed and test
accuracy on simple datasets. This is achieved by the ability to learn
representations that are informative locally, on a layer-by-layer basis, and
retain their informativeness when propagated to deeper layers in the
architecture. This leads to around 84% accuracy on CIFAR-10, a notable
improvement (25%) over the original FF algorithm. These results highlight the
potential of Forward-Forward as a genuine competitor to backpropagation and as
a promising research avenue.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dooms_T/0/1/0/all/0/1&quot;&gt;Thomas Dooms&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1&quot;&gt;Ing Jyh Tsang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oramas_J/0/1/0/all/0/1&quot;&gt;Jose Oramas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00343">
<title>OpenStereo: A Comprehensive Benchmark for Stereo Matching and Strong Baseline. (arXiv:2312.00343v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.00343</link>
<description rdf:parseType="Literal">&lt;p&gt;Stereo matching, a pivotal technique in computer vision, plays a crucial role
in robotics, autonomous navigation, and augmented reality. Despite the
development of numerous impressive methods in recent years, replicating their
results and determining the most suitable architecture for practical
application remains challenging. Addressing this gap, our paper introduces a
comprehensive benchmark focusing on practical applicability rather than solely
on performance enhancement. Specifically, we develop a flexible and efficient
stereo matching codebase, called OpenStereo. OpenStereo includes training and
inference codes of more than 12 network models, making it, to our knowledge,
the most complete stereo matching toolbox available. Based on OpenStereo, we
conducted experiments on the SceneFlow dataset and have achieved or surpassed
the performance metrics reported in the original paper. Additionally, we
conduct an in-depth revisitation of recent developments in stereo matching
through ablative experiments. These investigations inspired the creation of
StereoBase, a simple yet strong baseline model. Our extensive comparative
analyses of StereoBase against numerous contemporary stereo matching methods on
the SceneFlow dataset demonstrate its remarkably strong performance. The source
code is available at https://github.com/XiandaGuo/OpenStereo.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xianda Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Juntao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chenming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yiqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1&quot;&gt;Yiqun Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Tian Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Long Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02221">
<title>Slice3D: Multi-Slice, Occlusion-Revealing, Single View 3D Reconstruction. (arXiv:2312.02221v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02221</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce multi-slice reasoning, a new notion for single-view 3D
reconstruction which challenges the current and prevailing belief that
multi-view synthesis is the most natural conduit between single-view and 3D.
Our key observation is that object slicing is more advantageous than altering
views to reveal occluded structures. Specifically, slicing is more
occlusion-revealing since it can peel through any occluders without
obstruction. In the limit, i.e., with infinitely many slices, it is guaranteed
to unveil all hidden object parts. We realize our idea by developing Slice3D, a
novel method for single-view 3D reconstruction which first predicts multi-slice
images from a single RGB image and then integrates the slices into a 3D model
using a coordinate-based transformer network for signed distance prediction.
The slice images can be regressed or generated, both through a U-Net based
network. For the former, we inject a learnable slice indicator code to
designate each decoded image into a spatial slice location, while the slice
generator is a denoising diffusion model operating on the entirety of slice
images stacked on the input channels. We conduct extensive evaluation against
state-of-the-art alternatives to demonstrate superiority of our method,
especially in recovering complex and severely occluded shape structures, amid
ambiguities. All Slice3D results were produced by networks trained on a single
Nvidia A40 GPU, with an inference time less than 20 seconds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yizhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lira_W/0/1/0/all/0/1&quot;&gt;Wallace Lira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahdavi_Amiri_A/0/1/0/all/0/1&quot;&gt;Ali Mahdavi-Amiri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03018">
<title>DreamVideo: High-Fidelity Image-to-Video Generation with Image Retention and Text Guidance. (arXiv:2312.03018v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.03018</link>
<description rdf:parseType="Literal">&lt;p&gt;Image-to-video generation, which aims to generate a video starting from a
given reference image, has drawn great attention. Existing methods try to
extend pre-trained text-guided image diffusion models to image-guided video
generation models. Nevertheless, these methods often result in either low
fidelity or flickering over time due to their limitation to shallow image
guidance and poor temporal consistency. To tackle these problems, we propose a
high-fidelity image-to-video generation method by devising a frame retention
branch based on a pre-trained video diffusion model, named DreamVideo. Instead
of integrating the reference image into the diffusion process at a semantic
level, our DreamVideo perceives the reference image via convolution layers and
concatenates the features with the noisy latents as model input. By this means,
the details of the reference image can be preserved to the greatest extent. In
addition, by incorporating double-condition classifier-free guidance, a single
image can be directed to videos of different actions by providing varying
prompt texts. This has significant implications for controllable video
generation and holds broad application prospects. We conduct comprehensive
experiments on the public dataset, and both quantitative and qualitative
results indicate that our method outperforms the state-of-the-art method.
Especially for fidelity, our model has a powerful image retention ability and
delivers the best results in UCF101 compared to other image-to-video models to
our best knowledge. Also, precise control can be achieved by giving different
text prompts. Further details and comprehensive results of our model will be
presented in https://anonymous0769.github.io/DreamVideo/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Cong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jiaxi Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1&quot;&gt;Panwen Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Songcen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xiaodan Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03033">
<title>LiDAR-based Person Re-identification. (arXiv:2312.03033v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.03033</link>
<description rdf:parseType="Literal">&lt;p&gt;Camera-based person re-identification (ReID) systems have been widely applied
in the field of public security. However, cameras often lack the perception of
3D morphological information of human and are susceptible to various
limitations, such as inadequate illumination, complex background, and personal
privacy. In this paper, we propose a LiDAR-based ReID framework, ReID3D, that
utilizes pre-training strategy to retrieve features of 3D body shape and
introduces Graph-based Complementary Enhancement Encoder for extracting
comprehensive features. Due to the lack of LiDAR datasets, we build LReID, the
first LiDAR-based person ReID dataset, which is collected in several outdoor
scenes with variations in natural conditions. Additionally, we introduce
LReID-sync, a simulated pedestrian dataset designed for pre-training encoders
with tasks of point cloud completion and shape parameter learning. Extensive
experiments on LReID show that ReID3D achieves exceptional performance with a
rank-1 accuracy of 94.0, highlighting the significant potential of LiDAR in
addressing person ReID tasks. To the best of our knowledge, we are the first to
propose a solution for LiDAR-based ReID. The code and datasets will be released
soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1&quot;&gt;Wenxuan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1&quot;&gt;Zhiyu Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yingping Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1&quot;&gt;Ziheng Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1&quot;&gt;Zhi Chen Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jianjiang Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jie Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03594">
<title>A Task is Worth One Word: Learning with Task Prompts for High-Quality Versatile Image Inpainting. (arXiv:2312.03594v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.03594</link>
<description rdf:parseType="Literal">&lt;p&gt;Achieving high-quality versatile image inpainting, where user-specified
regions are filled with plausible content according to user intent, presents a
significant challenge. Existing methods face difficulties in simultaneously
addressing context-aware image inpainting and text-guided object inpainting due
to the distinct optimal training strategies required. To overcome this
challenge, we introduce PowerPaint, the first high-quality and versatile
inpainting model that excels in both tasks. First, we introduce learnable task
prompts along with tailored fine-tuning strategies to guide the model&apos;s focus
on different inpainting targets explicitly. This enables PowerPaint to
accomplish various inpainting tasks by utilizing different task prompts,
resulting in state-of-the-art performance. Second, we demonstrate the
versatility of the task prompt in PowerPaint by showcasing its effectiveness as
a negative prompt for object removal. Additionally, we leverage prompt
interpolation techniques to enable controllable shape-guided object inpainting.
Finally, we extensively evaluate PowerPaint on various inpainting benchmarks to
demonstrate its superior performance for versatile image inpainting. We release
our codes and models on our project page: https://powerpaint.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1&quot;&gt;Junhao Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1&quot;&gt;Yanhong Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wenran Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1&quot;&gt;Chun Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03774">
<title>OctreeOcc: Efficient and Multi-Granularity Occupancy Prediction Using Octree Queries. (arXiv:2312.03774v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.03774</link>
<description rdf:parseType="Literal">&lt;p&gt;Occupancy prediction has increasingly garnered attention in recent years for
its fine-grained understanding of 3D scenes. Traditional approaches typically
rely on dense, regular grid representations, which often leads to excessive
computational demands and a loss of spatial details for small objects. This
paper introduces OctreeOcc, an innovative 3D occupancy prediction framework
that leverages the octree representation to adaptively capture valuable
information in 3D, offering variable granularity to accommodate object shapes
and semantic regions of varying sizes and complexities. In particular, we
incorporate image semantic information to improve the accuracy of initial
octree structures and design an effective rectification mechanism to refine the
octree structure iteratively. Our extensive evaluations show that OctreeOcc not
only surpasses state-of-the-art methods in occupancy prediction, but also
achieves a 15%-24% reduction in computational overhead compared to
dense-grid-based methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yuhang Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xinge Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yuexin Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04316">
<title>Towards Knowledge-driven Autonomous Driving. (arXiv:2312.04316v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04316</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the emerging knowledge-driven autonomous driving
technologies. Our investigation highlights the limitations of current
autonomous driving systems, in particular their sensitivity to data bias,
difficulty in handling long-tail scenarios, and lack of interpretability.
Conversely, knowledge-driven methods with the abilities of cognition,
generalization and life-long learning emerge as a promising way to overcome
these challenges. This paper delves into the essence of knowledge-driven
autonomous driving and examines its core components: dataset \&amp;amp; benchmark,
environment, and driver agent. By leveraging large language models, world
models, neural rendering, and other advanced artificial intelligence
techniques, these components collectively contribute to a more holistic,
adaptive, and intelligent autonomous driving system. The paper systematically
organizes and reviews previous research efforts in this area, and provides
insights and guidance for future research and practical applications of
autonomous driving. We will continually share the latest updates on
cutting-edge developments in knowledge-driven autonomous driving along with the
relevant valuable open-source resources at:
\url{https://github.com/PJLab-ADG/awesome-knowledge-driven-AD}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yeqi Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1&quot;&gt;Pinlong Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1&quot;&gt;Licheng Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1&quot;&gt;Daocheng Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xuemeng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1&quot;&gt;Xinyu Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1&quot;&gt;Tao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jianfei Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xing Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_M/0/1/0/all/0/1&quot;&gt;Min Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Botian Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1&quot;&gt;Liang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04344">
<title>Enhancing Medical Task Performance in GPT-4V: A Comprehensive Study on Prompt Engineering Strategies. (arXiv:2312.04344v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04344</link>
<description rdf:parseType="Literal">&lt;p&gt;OpenAI&apos;s latest large vision-language model (LVLM), GPT-4V(ision), has piqued
considerable interest for its potential in medical applications. Despite its
promise, recent studies and internal reviews highlight its underperformance in
specialized medical tasks. This paper explores the boundary of GPT-4V&apos;s
capabilities in medicine, particularly in processing complex imaging data from
endoscopies, CT scans, and MRIs etc. Leveraging open-source datasets, we
assessed its foundational competencies, identifying substantial areas for
enhancement. Our research emphasizes prompt engineering, an often-underutilized
strategy for improving AI responsiveness. Through iterative testing, we refined
the model&apos;s prompts, significantly improving its interpretative accuracy and
relevance in medical imaging. From our comprehensive evaluations, we distilled
10 effective prompt engineering techniques, each fortifying GPT-4V&apos;s medical
acumen. These methodical enhancements facilitate more reliable, precise, and
clinically valuable insights from GPT-4V, advancing its operability in critical
healthcare environments. Our findings are pivotal for those employing AI in
medicine, providing clear, actionable guidance on harnessing GPT-4V&apos;s full
diagnostic potential.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pengcheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Ziyan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1&quot;&gt;Zhongying Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tianbin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yanzhou Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haoyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jin Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Junjun He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04837">
<title>Localized Symbolic Knowledge Distillation for Visual Commonsense Models. (arXiv:2312.04837v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04837</link>
<description rdf:parseType="Literal">&lt;p&gt;Instruction following vision-language (VL) models offer a flexible interface
that supports a broad range of multimodal tasks in a zero-shot fashion.
However, interfaces that operate on full images do not directly enable the user
to &quot;point to&quot; and access specific regions within images. This capability is
important not only to support reference-grounded VL benchmarks, but also, for
practical applications that require precise within-image reasoning. We build
Localized Visual Commonsense models, which allow users to specify (multiple)
regions as input. We train our model by sampling localized commonsense
knowledge from a large language model (LLM): specifically, we prompt an LLM to
collect commonsense knowledge given a global literal image description and a
local literal region description automatically generated by a set of VL models.
With a separately trained critic model that selects high-quality examples, we
find that training on the localized commonsense corpus can successfully distill
existing VL models to support a reference-as-input interface. Empirical results
and human evaluations in a zero-shot setup demonstrate that our distillation
method results in more precise VL models of reasoning compared to a baseline of
passing a generated referring expression to an LLM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Jae Sung Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1&quot;&gt;Jack Hessel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandu_K/0/1/0/all/0/1&quot;&gt;Khyathi Raghavi Chandu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Paul Pu Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Ximing Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1&quot;&gt;Peter West&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Youngjae Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qiuyuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jianfeng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1&quot;&gt;Ali Farhadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1&quot;&gt;Yejin Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05695">
<title>The Counterattack of CNNs in Self-Supervised Learning: Larger Kernel Size might be All You Need. (arXiv:2312.05695v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.05695</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformers have been rapidly uprising in computer vision thanks to
their outstanding scaling trends, and gradually replacing convolutional neural
networks (CNNs). Recent works on self-supervised learning (SSL) introduce
siamese pre-training tasks, on which Transformer backbones continue to
demonstrate ever stronger results than CNNs. People come to believe that
Transformers or self-attention modules are inherently more suitable than CNNs
in the context of SSL. However, it is noteworthy that most if not all prior
arts of SSL with CNNs chose the standard ResNets as their backbones, whose
architecture effectiveness is known to already lag behind advanced Vision
Transformers. Therefore, it remains unclear whether the self-attention
operation is crucial for the recent advances in SSL - or CNNs can deliver the
same excellence with more advanced designs, too? Can we close the SSL
performance gap between Transformers and CNNs? To answer these intriguing
questions, we apply self-supervised pre-training to the recently proposed,
stronger lager-kernel CNN architecture and conduct an apple-to-apple comparison
with Transformers, in their SSL performance. Our results show that we are able
to build pure CNN SSL architectures that perform on par with or better than the
best SSL-trained Transformers, by just scaling up convolutional kernel sizes
besides other small tweaks. Impressively, when transferring to the downstream
tasks \texttt{MS COCO} detection and segmentation, our SSL pre-trained CNN
model (trained in 100 epochs) achieves the same good performance as the
300-epoch pre-trained Transformer counterpart. We hope this work can help to
better understand what is essential (or not) for self-supervised learning
backbones.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tianjin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianlong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shiwei Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05743">
<title>Building Variable-sized Models via Learngene Pool. (arXiv:2312.05743v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.05743</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, Stitchable Neural Networks (SN-Net) is proposed to stitch some
pre-trained networks for quickly building numerous networks with different
complexity and performance trade-offs. In this way, the burdens of designing or
training the variable-sized networks, which can be used in application
scenarios with diverse resource constraints, are alleviated. However, SN-Net
still faces a few challenges. 1) Stitching from multiple independently
pre-trained anchors introduces high storage resource consumption. 2) SN-Net
faces challenges to build smaller models for low resource constraints. 3).
SN-Net uses an unlearned initialization method for stitch layers, limiting the
final performance. To overcome these challenges, motivated by the recently
proposed Learngene framework, we propose a novel method called Learngene Pool.
Briefly, Learngene distills the critical knowledge from a large pre-trained
model into a small part (termed as learngene) and then expands this small part
into a few variable-sized models. In our proposed method, we distill one
pretrained large model into multiple small models whose network blocks are used
as learngene instances to construct the learngene pool. Since only one large
model is used, we do not need to store more large models as SN-Net and after
distilling, smaller learngene instances can be created to build small models to
satisfy low resource constraints. We also insert learnable transformation
matrices between the instances to stitch them into variable-sized models to
improve the performance of these models. Exhaustive experiments have been
implemented and the results validate the effectiveness of the proposed
Learngene Pool compared with SN-Net.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Boyu Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1&quot;&gt;Shiyu Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haokun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kou_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Kou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1&quot;&gt;Xin Geng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05799">
<title>SGNet: Structure Guided Network via Gradient-Frequency Awareness for Depth Map Super-Resolution. (arXiv:2312.05799v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.05799</link>
<description rdf:parseType="Literal">&lt;p&gt;Depth super-resolution (DSR) aims to restore high-resolution (HR) depth from
low-resolution (LR) one, where RGB image is often used to promote this task.
Recent image guided DSR approaches mainly focus on spatial domain to rebuild
depth structure. However, since the structure of LR depth is usually blurry,
only considering spatial domain is not very sufficient to acquire satisfactory
results. In this paper, we propose structure guided network (SGNet), a method
that pays more attention to gradient and frequency domains, both of which have
the inherent ability to capture high-frequency structure. Specifically, we
first introduce the gradient calibration module (GCM), which employs the
accurate gradient prior of RGB to sharpen the LR depth structure. Then we
present the Frequency Awareness Module (FAM) that recursively conducts multiple
spectrum differencing blocks (SDB), each of which propagates the precise
high-frequency components of RGB into the LR depth. Extensive experimental
results on both real and synthetic datasets demonstrate the superiority of our
SGNet, reaching the state-of-the-art. Codes and pre-trained models are
available at https://github.com/yanzq95/SGNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhengxue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jian Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05924">
<title>Data-Free Hard-Label Robustness Stealing Attack. (arXiv:2312.05924v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.05924</link>
<description rdf:parseType="Literal">&lt;p&gt;The popularity of Machine Learning as a Service (MLaaS) has led to increased
concerns about Model Stealing Attacks (MSA), which aim to craft a clone model
by querying MLaaS. Currently, most research on MSA assumes that MLaaS can
provide soft labels and that the attacker has a proxy dataset with a similar
distribution. However, this fails to encapsulate the more practical scenario
where only hard labels are returned by MLaaS and the data distribution remains
elusive. Furthermore, most existing work focuses solely on stealing the model
accuracy, neglecting the model robustness, while robustness is essential in
security-sensitive scenarios, e.g., face-scan payment. Notably, improving model
robustness often necessitates the use of expensive techniques such as
adversarial training, thereby further making stealing robustness a more
lucrative prospect. In response to these identified gaps, we introduce a novel
Data-Free Hard-Label Robustness Stealing (DFHL-RS) attack in this paper, which
enables the stealing of both model accuracy and robustness by simply querying
hard labels of the target model without the help of any natural data.
Comprehensive experiments demonstrate the effectiveness of our method. The
clone model achieves a clean accuracy of 77.86% and a robust accuracy of 39.51%
against AutoAttack, which are only 4.71% and 8.40% lower than the target model
on the CIFAR-10 dataset, significantly exceeding the baselines. Our code is
available at: https://github.com/LetheSec/DFHL-RS-Attack.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xiaojian Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kejiang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wen Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weiming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1&quot;&gt;Nenghai Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06069">
<title>Mining Gaze for Contrastive Learning toward Computer-Assisted Diagnosis. (arXiv:2312.06069v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06069</link>
<description rdf:parseType="Literal">&lt;p&gt;Obtaining large-scale radiology reports can be difficult for medical images
due to various reasons, limiting the effectiveness of contrastive pre-training
in the medical image domain and underscoring the need for alternative methods.
In this paper, we propose eye-tracking as an alternative to text reports, as it
allows for the passive collection of gaze signals without disturbing
radiologist&apos;s routine diagnosis process. By tracking the gaze of radiologists
as they read and diagnose medical images, we can understand their visual
attention and clinical reasoning. When a radiologist has similar gazes for two
medical images, it may indicate semantic similarity for diagnosis, and these
images should be treated as positive pairs when pre-training a
computer-assisted diagnosis (CAD) network through contrastive learning.
Accordingly, we introduce the Medical contrastive Gaze Image Pre-training
(McGIP) as a plug-and-play module for contrastive learning frameworks. McGIP
uses radiologist&apos;s gaze to guide contrastive pre-training. We evaluate our
method using two representative types of medical images and two common types of
gaze data. The experimental results demonstrate the practicality of McGIP,
indicating its high potential for various clinical scenarios and applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zihao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1&quot;&gt;Dinggang Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06409">
<title>PointVoxel: A Simple and Effective Pipeline for Multi-View Multi-Modal 3D Human Pose Estimation. (arXiv:2312.06409v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06409</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, several methods have been proposed to estimate 3D human pose from
multi-view images and achieved impressive performance on public datasets
collected in relatively easy scenarios. However, there are limited approaches
for extracting 3D human skeletons from multimodal inputs (e.g., RGB and
pointcloud) that can enhance the accuracy of predicting 3D poses in challenging
situations. We fill this gap by introducing a pipeline called PointVoxel that
fuses multi-view RGB and pointcloud inputs to obtain 3D human poses. We
demonstrate that volumetric representation is an effective architecture for
integrating these different modalities. Moreover, in order to overcome the
challenges of annotating 3D human pose labels in difficult scenarios, we
develop a synthetic dataset generator for pretraining and design an
unsupervised domain adaptation strategy so that we can obtain a well-trained 3D
human pose estimator without using any manual annotations. We evaluate our
approach on four datasets (two public datasets, one synthetic dataset, and one
challenging dataset named BasketBall collected by ourselves), showing promising
results. The code and dataset will be released soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1&quot;&gt;Zhiyu Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1&quot;&gt;Zhicheng Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1&quot;&gt;Wenxuan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yifan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jianjiang Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jie Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06630">
<title>TMT-VIS: Taxonomy-aware Multi-dataset Joint Training for Video Instance Segmentation. (arXiv:2312.06630v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06630</link>
<description rdf:parseType="Literal">&lt;p&gt;Training on large-scale datasets can boost the performance of video instance
segmentation while the annotated datasets for VIS are hard to scale up due to
the high labor cost. What we possess are numerous isolated filed-specific
datasets, thus, it is appealing to jointly train models across the aggregation
of datasets to enhance data volume and diversity. However, due to the
heterogeneity in category space, as mask precision increases with the data
volume, simply utilizing multiple datasets will dilute the attention of models
on different taxonomies. Thus, increasing the data scale and enriching taxonomy
space while improving classification precision is important. In this work, we
analyze that providing extra taxonomy information can help models concentrate
on specific taxonomy, and propose our model named Taxonomy-aware Multi-dataset
Joint Training for Video Instance Segmentation (TMT-VIS) to address this vital
challenge. Specifically, we design a two-stage taxonomy aggregation module that
first compiles taxonomy information from input videos and then aggregates these
taxonomy priors into instance queries before the transformer decoder. We
conduct extensive experimental evaluations on four popular and challenging
benchmarks, including YouTube-VIS 2019, YouTube-VIS 2021, OVIS, and UVO. Our
model shows significant improvement over the baseline solutions, and sets new
state-of-the-art records on all benchmarks. These appealing and encouraging
results demonstrate the effectiveness and generality of our approach. The code
is available at
https://github.com/rkzheng99/TMT-VIS(https://github.com/rkzheng99/TMT-VIS)
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1&quot;&gt;Rongkun Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1&quot;&gt;Lu Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hengshuang Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06643">
<title>Gaze Detection and Analysis for Initiating Joint Activity in Industrial Human-Robot Collaboration. (arXiv:2312.06643v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06643</link>
<description rdf:parseType="Literal">&lt;p&gt;Collaborative robots (cobots) are widely used in industrial applications, yet
extensive research is still needed to enhance human-robot collaborations and
operator experience. A potential approach to improve the collaboration
experience involves adapting cobot behavior based on natural cues from the
operator. Inspired by the literature on human-human interactions, we conducted
a wizard-of-oz study to examine whether a gaze towards the cobot can serve as a
trigger for initiating joint activities in collaborative sessions. In this
study, 37 participants engaged in an assembly task while their gaze behavior
was analyzed. We employ a gaze-based attention recognition model to identify
when the participants look at the cobot. Our results indicate that in most
cases (84.88\%), the joint activity is preceded by a gaze towards the cobot.
Furthermore, during the entire assembly cycle, the participants tend to look at
the cobot around the time of the joint activity. To the best of our knowledge,
this is the first study to analyze the natural gaze behavior of participants
working on a joint activity with a robot during a collaborative assembly task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prajod_P/0/1/0/all/0/1&quot;&gt;Pooja Prajod&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nicora_M/0/1/0/all/0/1&quot;&gt;Matteo Lavit Nicora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mondellini_M/0/1/0/all/0/1&quot;&gt;Marta Mondellini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tauro_G/0/1/0/all/0/1&quot;&gt;Giovanni Tauro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vertechy_R/0/1/0/all/0/1&quot;&gt;Rocco Vertechy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malosio_M/0/1/0/all/0/1&quot;&gt;Matteo Malosio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andre_E/0/1/0/all/0/1&quot;&gt;Elisabeth Andr&amp;#xe9;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04547">
<title>Digital Life Project: Autonomous 3D Characters with Social Intelligence. (arXiv:2312.04547v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2312.04547</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we present Digital Life Project, a framework utilizing language
as the universal medium to build autonomous 3D characters, who are capable of
engaging in social interactions and expressing with articulated body motions,
thereby simulating life in a digital environment. Our framework comprises two
primary components: 1) SocioMind: a meticulously crafted digital brain that
models personalities with systematic few-shot exemplars, incorporates a
reflection process based on psychology principles, and emulates autonomy by
initiating dialogue topics; 2) MoMat-MoGen: a text-driven motion synthesis
paradigm for controlling the character&apos;s digital body. It integrates motion
matching, a proven industry technique to ensure motion quality, with
cutting-edge advancements in motion generation for diversity. Extensive
experiments demonstrate that each module achieves state-of-the-art performance
in its respective domain. Collectively, they enable virtual characters to
initiate and sustain dialogues autonomously, while evolving their
socio-psychological states. Concurrently, these characters can perform
contextually relevant bodily movements. Additionally, a motion captioning
module further allows the virtual character to recognize and appropriately
respond to human players&apos; actions. Homepage: https://digital-life-project.com/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1&quot;&gt;Zhongang Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jianping Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qing_Z/0/1/0/all/0/1&quot;&gt;Zhongfei Qing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xinying Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mingyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhengyu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1&quot;&gt;Haiyi Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1&quot;&gt;Chen Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruisi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1&quot;&gt;Wanqi Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1&quot;&gt;Xiangyu Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1&quot;&gt;Han Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1&quot;&gt;Liang Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1&quot;&gt;Peng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhitao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiaqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1&quot;&gt;Tianxiang Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yukun Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaogang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1&quot;&gt;Chen Change Loy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziwei Liu&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>