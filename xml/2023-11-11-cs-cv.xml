<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-11-09T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04932" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04938" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04940" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04942" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04950" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04986" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04991" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05006" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05021" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05029" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05032" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05041" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05043" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05071" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05077" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05100" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05109" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05122" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05143" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05146" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05147" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05152" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05168" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05190" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05192" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05197" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05198" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05199" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05221" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05230" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05237" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05269" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05276" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05289" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05298" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05310" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05323" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05332" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05336" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05348" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05371" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05383" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05400" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05410" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05425" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05432" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05437" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05452" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05461" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05463" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05464" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05477" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05478" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05479" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05494" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05521" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05524" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05538" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05539" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05548" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05556" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05559" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05565" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05567" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05579" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05591" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05602" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05604" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05607" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05609" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05613" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2012.04132" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2106.08992" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.01736" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.07145" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.09048" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.05954" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.12845" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.03081" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.08067" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.01056" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.05828" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.03251" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.13000" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.08491" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13903" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14979" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15001" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.01289" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05527" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07005" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07915" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08013" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.12929" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00562" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11123" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.05528" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07096" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12214" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16393" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07602" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.09982" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15008" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15767" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19630" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.20190" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02510" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02831" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03312" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03828" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04458" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04464" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04591" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04592" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04828" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2311.04932">
<title>GC-VTON: Predicting Globally Consistent and Occlusion Aware Local Flows with Neighborhood Integrity Preservation for Virtual Try-on. (arXiv:2311.04932v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04932</link>
<description rdf:parseType="Literal">&lt;p&gt;Flow based garment warping is an integral part of image-based virtual try-on
networks. However, optimizing a single flow predicting network for simultaneous
global boundary alignment and local texture preservation results in sub-optimal
flow fields. Moreover, dense flows are inherently not suited to handle
intricate conditions like garment occlusion by body parts or by other garments.
Forcing flows to handle the above issues results in various distortions like
texture squeezing, and stretching. In this work, we propose a novel approach
where we disentangle the global boundary alignment and local texture preserving
tasks via our GlobalNet and LocalNet modules. A consistency loss is then
employed between the two modules which harmonizes the local flows with the
global boundary alignment. Additionally, we explicitly handle occlusions by
predicting body-parts visibility mask, which is used to mask out the occluded
regions in the warped garment. The masking prevents the LocalNet from
predicting flows that distort texture to compensate for occlusions. We also
introduce a novel regularization loss (NIPR), that defines a criteria to
identify the regions in the warped garment where texture integrity is violated
(squeezed or stretched). NIPR subsequently penalizes the flow in those regions
to ensure regular and coherent warps that preserve the texture in local
neighborhoods. Evaluation on a widely used virtual try-on dataset demonstrates
strong performance of our network compared to the current SOTA methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rawal_H/0/1/0/all/0/1&quot;&gt;Hamza Rawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmad_M/0/1/0/all/0/1&quot;&gt;Muhammad Junaid Ahmad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaman_F/0/1/0/all/0/1&quot;&gt;Farooq Zaman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04938">
<title>Improved DDIM Sampling with Moment Matching Gaussian Mixtures. (arXiv:2311.04938v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04938</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose using a Gaussian Mixture Model (GMM) as reverse transition
operator (kernel) within the Denoising Diffusion Implicit Models (DDIM)
framework, which is one of the most widely used approaches for accelerated
sampling from pre-trained Denoising Diffusion Probabilistic Models (DDPM).
Specifically we match the first and second order central moments of the DDPM
forward marginals by constraining the parameters of the GMM. We see that moment
matching is sufficient to obtain samples with equal or better quality than the
original DDIM with Gaussian kernels. We provide experimental results with
unconditional models trained on CelebAHQ and FFHQ and class-conditional models
trained on ImageNet datasets respectively. Our results suggest that using the
GMM kernel leads to significant improvements in the quality of the generated
samples when the number of sampling steps is small, as measured by FID and IS
metrics. For example on ImageNet 256x256, using 10 sampling steps, we achieve a
FID of 6.94 and IS of 207.85 with a GMM kernel compared to 10.15 and 196.73
respectively with a Gaussian kernel.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gabbur_P/0/1/0/all/0/1&quot;&gt;Prasad Gabbur&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04940">
<title>Interpretable Geoscience Artificial Intelligence (XGeoS-AI): Application to Demystify Image Recognition. (arXiv:2311.04940v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04940</link>
<description rdf:parseType="Literal">&lt;p&gt;As Earth science enters the era of big data, artificial intelligence (AI) not
only offers great potential for solving geoscience problems, but also plays a
critical role in accelerating the understanding of the complex, interactive,
and multiscale processes of Earth&apos;s behavior. As geoscience AI models are
progressively utilized for significant predictions in crucial situations,
geoscience researchers are increasingly demanding their interpretability and
versatility. This study proposes an interpretable geoscience artificial
intelligence (XGeoS-AI) framework to unravel the mystery of image recognition
in the Earth sciences, and its effectiveness and versatility is demonstrated by
taking computed tomography (CT) image recognition as an example. Inspired by
the mechanism of human vision, the proposed XGeoS-AI framework generates a
threshold value from a local region within the whole image to complete the
recognition. Different kinds of artificial intelligence (AI) methods, such as
Support Vector Regression (SVR), Multilayer Perceptron (MLP), Convolutional
Neural Network (CNN), can be adopted as the AI engines of the proposed XGeoS-AI
framework to efficiently complete geoscience image recognition tasks.
Experimental results demonstrate that the effectiveness, versatility, and
heuristics of the proposed framework have great potential in solving geoscience
image recognition problems. Interpretable AI should receive more and more
attention in the field of the Earth sciences, which is the key to promoting
more rational and wider applications of AI in the field of Earth sciences. In
addition, the proposed interpretable framework may be the forerunner of
technological innovation in the Earth sciences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jin-Jian Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1&quot;&gt;Chao-Sheng Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Bin Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04942">
<title>CSAM: A 2.5D Cross-Slice Attention Module for Anisotropic Volumetric Medical Image Segmentation. (arXiv:2311.04942v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.04942</link>
<description rdf:parseType="Literal">&lt;p&gt;A large portion of volumetric medical data, especially magnetic resonance
imaging (MRI) data, is anisotropic, as the through-plane resolution is
typically much lower than the in-plane resolution. Both 3D and purely 2D deep
learning-based segmentation methods are deficient in dealing with such
volumetric data since the performance of 3D methods suffers when confronting
anisotropic data, and 2D methods disregard crucial volumetric information.
Insufficient work has been done on 2.5D methods, in which 2D convolution is
mainly used in concert with volumetric information. These models focus on
learning the relationship across slices, but typically have many parameters to
train. We offer a Cross-Slice Attention Module (CSAM) with minimal trainable
parameters, which captures information across all the slices in the volume by
applying semantic, positional, and slice attention on deep feature maps at
different scales. Our extensive experiments using different network
architectures and tasks demonstrate the usefulness and generalizability of
CSAM. Associated code is available at https://github.com/aL3x-O-o-Hung/CSAM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hung_A/0/1/0/all/0/1&quot;&gt;Alex Ling Yu Hung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Haoxin Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_K/0/1/0/all/0/1&quot;&gt;Kai Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Du_X/0/1/0/all/0/1&quot;&gt;Xiaoxi Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pang_K/0/1/0/all/0/1&quot;&gt;Kaifeng Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Miao_Q/0/1/0/all/0/1&quot;&gt;Qi Miao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Raman_S/0/1/0/all/0/1&quot;&gt;Steven S. Raman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Terzopoulos_D/0/1/0/all/0/1&quot;&gt;Demetri Terzopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sung_K/0/1/0/all/0/1&quot;&gt;Kyunghyun Sung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04950">
<title>Lightweight Diffusion Models with Distillation-Based Block Neural Architecture Search. (arXiv:2311.04950v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04950</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have recently shown remarkable generation ability, achieving
state-of-the-art performance in many tasks. However, the high computational
cost is still a troubling problem for diffusion models. To tackle this problem,
we propose to automatically remove the structural redundancy in diffusion
models with our proposed Diffusion Distillation-based Block-wise Neural
Architecture Search (DiffNAS). Specifically, given a larger pretrained teacher,
we leverage DiffNAS to search for the smallest architecture which achieves
on-par or even better performance than the teacher. Considering current
diffusion models are based on UNet which naturally has a block-wise structure,
we perform neural architecture search independently in each block, which
largely reduces the search space. Different from previous block-wise NAS
methods, DiffNAS contains a block-wise local search strategy and a retraining
strategy with a joint dynamic loss. Concretely, during the search process, we
block-wisely select the best subnet to avoid the unfairness brought by the
global search strategy used in previous works. When retraining the searched
architecture, we adopt a dynamic joint loss to maintain the consistency between
supernet training and subnet retraining, which also provides informative
objectives for each block and shortens the paths of gradient propagation. We
demonstrate this joint loss can effectively improve model performance. We also
prove the necessity of the dynamic adjustment of this loss. The experiments
show that our method can achieve significant computational reduction,
especially on latent diffusion models with about 50% MACs and Parameter
reduction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Siao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1&quot;&gt;Chaoyu Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yansong Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+zhu_W/0/1/0/all/0/1&quot;&gt;Wenwu zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04986">
<title>Exploiting Inductive Biases in Video Modeling through Neural CDEs. (arXiv:2311.04986v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04986</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a novel approach to video modeling that leverages controlled
differential equations (CDEs) to address key challenges in video tasks, notably
video interpolation and mask propagation. We apply CDEs at varying resolutions
leading to a continuous-time U-Net architecture. Unlike traditional methods,
our approach does not require explicit optical flow learning, and instead makes
use of the inherent continuous-time features of CDEs to produce a highly
expressive video model. We demonstrate competitive performance against
state-of-the-art models for video interpolation and mask propagation tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiu_J/0/1/0/all/0/1&quot;&gt;Johnathan Chiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duffield_S/0/1/0/all/0/1&quot;&gt;Samuel Duffield&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hunter_Gordon_M/0/1/0/all/0/1&quot;&gt;Max Hunter-Gordon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Donatella_K/0/1/0/all/0/1&quot;&gt;Kaelan Donatella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aifer_M/0/1/0/all/0/1&quot;&gt;Max Aifer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_A/0/1/0/all/0/1&quot;&gt;Andi Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04991">
<title>Effective Restoration of Source Knowledge in Continual Test Time Adaptation. (arXiv:2311.04991v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.04991</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional test-time adaptation (TTA) methods face significant challenges in
adapting to dynamic environments characterized by continuously changing
long-term target distributions. These challenges primarily stem from two
factors: catastrophic forgetting of previously learned valuable source
knowledge and gradual error accumulation caused by miscalibrated pseudo labels.
To address these issues, this paper introduces an unsupervised domain change
detection method that is capable of identifying domain shifts in dynamic
environments and subsequently resets the model parameters to the original
source pre-trained values. By restoring the knowledge from the source, it
effectively corrects the negative consequences arising from the gradual
deterioration of model parameters caused by ongoing shifts in the domain. Our
method involves progressive estimation of global batch-norm statistics specific
to each domain, while keeping track of changes in the statistics triggered by
domain shifts. Importantly, our method is agnostic to the specific adaptation
technique employed and thus, can be incorporated to existing TTA methods to
enhance their performance in dynamic environments. We perform extensive
experiments on benchmark datasets to demonstrate the superior performance of
our method compared to state-of-the-art adaptation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niloy_F/0/1/0/all/0/1&quot;&gt;Fahim Faisal Niloy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1&quot;&gt;Sk Miraj Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raychaudhuri_D/0/1/0/all/0/1&quot;&gt;Dripta S. Raychaudhuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oymak_S/0/1/0/all/0/1&quot;&gt;Samet Oymak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1&quot;&gt;Amit K. Roy-Chowdhury&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05006">
<title>Familiarity-Based Open-Set Recognition Under Adversarial Attacks. (arXiv:2311.05006v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05006</link>
<description rdf:parseType="Literal">&lt;p&gt;Open-set recognition (OSR), the identification of novel categories, can be a
critical component when deploying classification models in real-world
applications. Recent work has shown that familiarity-based scoring rules such
as the Maximum Softmax Probability (MSP) or the Maximum Logit Score (MLS) are
strong baselines when the closed-set accuracy is high. However, one of the
potential weaknesses of familiarity-based OSR are adversarial attacks. Here, we
present gradient-based adversarial attacks on familiarity scores for both types
of attacks, False Familiarity and False Novelty attacks, and evaluate their
effectiveness in informed and uninformed settings on TinyImageNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Enevoldsen_P/0/1/0/all/0/1&quot;&gt;Philip Enevoldsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gundersen_C/0/1/0/all/0/1&quot;&gt;Christian Gundersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lang_N/0/1/0/all/0/1&quot;&gt;Nico Lang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1&quot;&gt;Serge Belongie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Igel_C/0/1/0/all/0/1&quot;&gt;Christian Igel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05021">
<title>Leveraging a realistic synthetic database to learn Shape-from-Shading for estimating the colon depth in colonoscopy images. (arXiv:2311.05021v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05021</link>
<description rdf:parseType="Literal">&lt;p&gt;Colonoscopy is the choice procedure to diagnose colon and rectum cancer, from
early detection of small precancerous lesions (polyps), to confirmation of
malign masses. However, the high variability of the organ appearance and the
complex shape of both the colon wall and structures of interest make this
exploration difficult. Learned visuospatial and perceptual abilities mitigate
technical limitations in clinical practice by proper estimation of the
intestinal depth. This work introduces a novel methodology to estimate colon
depth maps in single frames from monocular colonoscopy videos. The generated
depth map is inferred from the shading variation of the colon wall with respect
to the light source, as learned from a realistic synthetic database. Briefly, a
classic convolutional neural network architecture is trained from scratch to
estimate the depth map, improving sharp depth estimations in haustral folds and
polyps by a custom loss function that minimizes the estimation error in edges
and curvatures. The network was trained by a custom synthetic colonoscopy
database herein constructed and released, composed of 248,400 frames (47
videos), with depth annotations at the level of pixels. This collection
comprehends 5 subsets of videos with progressively higher levels of visual
complexity. Evaluation of the depth estimation with the synthetic database
reached a threshold accuracy of 95.65%, and a mean-RMSE of 0.451 cm, while a
qualitative assessment with a real database showed consistent depth
estimations, visually evaluated by the expert gastroenterologist coauthoring
this paper. Finally, the method achieved competitive performance with respect
to another state-of-the-art method using a public synthetic database and
comparable results in a set of images with other five state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruano_J/0/1/0/all/0/1&quot;&gt;Josu&amp;#xe9; Ruano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomez_M/0/1/0/all/0/1&quot;&gt;Mart&amp;#xed;n G&amp;#xf3;mez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romero_E/0/1/0/all/0/1&quot;&gt;Eduardo Romero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manzanera_A/0/1/0/all/0/1&quot;&gt;Antoine Manzanera&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05029">
<title>S$^3$AD: Semi-supervised Small Apple Detection in Orchard Environments. (arXiv:2311.05029v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05029</link>
<description rdf:parseType="Literal">&lt;p&gt;Crop detection is integral for precision agriculture applications such as
automated yield estimation or fruit picking. However, crop detection, e.g.,
apple detection in orchard environments remains challenging due to a lack of
large-scale datasets and the small relative size of the crops in the image. In
this work, we address these challenges by reformulating the apple detection
task in a semi-supervised manner. To this end, we provide the large,
high-resolution dataset MAD comprising 105 labeled images with 14,667 annotated
apple instances and 4,440 unlabeled images. Utilizing this dataset, we also
propose a novel Semi-Supervised Small Apple Detection system S$^3$AD based on
contextual attention and selective tiling to improve the challenging detection
of small apples, while limiting the computational overhead. We conduct an
extensive evaluation on MAD and the MSU dataset, showing that S$^3$AD
substantially outperforms strong fully-supervised baselines, including several
small object detection systems, by up to $14.9\%$. Additionally, we exploit the
detailed annotations of our dataset w.r.t. apple properties to analyze the
influence of relative size or level of occlusion on the results of various
systems, quantifying current challenges.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johanson_R/0/1/0/all/0/1&quot;&gt;Robert Johanson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilms_C/0/1/0/all/0/1&quot;&gt;Christian Wilms&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johannsen_O/0/1/0/all/0/1&quot;&gt;Ole Johannsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frintrop_S/0/1/0/all/0/1&quot;&gt;Simone Frintrop&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05032">
<title>Transfer learning from a sparsely annotated dataset of 3D medical images. (arXiv:2311.05032v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.05032</link>
<description rdf:parseType="Literal">&lt;p&gt;Transfer learning leverages pre-trained model features from a large dataset
to save time and resources when training new models for various tasks,
potentially enhancing performance. Due to the lack of large datasets in the
medical imaging domain, transfer learning from one medical imaging model to
other medical imaging models has not been widely explored. This study explores
the use of transfer learning to improve the performance of deep convolutional
neural networks for organ segmentation in medical imaging. A base segmentation
model (3D U-Net) was trained on a large and sparsely annotated dataset; its
weights were used for transfer learning on four new down-stream segmentation
tasks for which a fully annotated dataset was available. We analyzed the
training set size&apos;s influence to simulate scarce data. The results showed that
transfer learning from the base model was beneficial when small datasets were
available, providing significant performance improvements; where fine-tuning
the base model is more beneficial than updating all the network weights with
vanilla transfer learning. Transfer learning with fine-tuning increased the
performance by up to 0.129 (+28\%) Dice score than experiments trained from
scratch, and on average 23 experiments increased the performance by 0.029 Dice
score in the new segmentation tasks. The study also showed that cross-modality
transfer learning using CT scans was beneficial. The findings of this study
demonstrate the potential of transfer learning to improve the efficiency of
annotation and increase the accessibility of accurate organ segmentation in
medical imaging, ultimately leading to improved patient care. We made the
network definition and weights publicly available to benefit other users and
researchers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Humpire_Mamani_G/0/1/0/all/0/1&quot;&gt;Gabriel Efrain Humpire-Mamani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jacobs_C/0/1/0/all/0/1&quot;&gt;Colin Jacobs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Prokop_M/0/1/0/all/0/1&quot;&gt;Mathias Prokop&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ginneken_B/0/1/0/all/0/1&quot;&gt;Bram van Ginneken&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lessmann_N/0/1/0/all/0/1&quot;&gt;Nikolas Lessmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05041">
<title>Active Transfer Learning for Efficient Video-Specific Human Pose Estimation. (arXiv:2311.05041v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05041</link>
<description rdf:parseType="Literal">&lt;p&gt;Human Pose (HP) estimation is actively researched because of its wide range
of applications. However, even estimators pre-trained on large datasets may not
perform satisfactorily due to a domain gap between the training and test data.
To address this issue, we present our approach combining Active Learning (AL)
and Transfer Learning (TL) to adapt HP estimators to individual video domains
efficiently. For efficient learning, our approach quantifies (i) the estimation
uncertainty based on the temporal changes in the estimated heatmaps and (ii)
the unnaturalness in the estimated full-body HPs. These quantified criteria are
then effectively combined with the state-of-the-art representativeness
criterion to select uncertain and diverse samples for efficient HP estimator
learning. Furthermore, we reconsider the existing Active Transfer Learning
(ATL) method to introduce novel ideas related to the retraining methods and
Stopping Criteria (SC). Experimental results demonstrate that our method
enhances learning efficiency and outperforms comparative methods. Our code is
publicly available at: https://github.com/ImIntheMiddle/VATL4Pose-WACV2024
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taketsugu_H/0/1/0/all/0/1&quot;&gt;Hiromu Taketsugu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ukita_N/0/1/0/all/0/1&quot;&gt;Norimichi Ukita&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05043">
<title>Zero-shot Translation of Attention Patterns in VQA Models to Natural Language. (arXiv:2311.05043v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05043</link>
<description rdf:parseType="Literal">&lt;p&gt;Converting a model&apos;s internals to text can yield human-understandable
insights about the model. Inspired by the recent success of training-free
approaches for image captioning, we propose ZS-A2T, a zero-shot framework that
translates the transformer attention of a given model into natural language
without requiring any training. We consider this in the context of Visual
Question Answering (VQA). ZS-A2T builds on a pre-trained large language model
(LLM), which receives a task prompt, question, and predicted answer, as inputs.
The LLM is guided to select tokens which describe the regions in the input
image that the VQA model attended to. Crucially, we determine this similarity
by exploiting the text-image matching capabilities of the underlying VQA model.
Our framework does not require any training and allows the drop-in replacement
of different guiding sources (e.g. attribution instead of attention maps), or
language models. We evaluate this novel task on textual explanation datasets
for VQA, giving state-of-the-art performances for the zero-shot setting on
GQA-REX and VQA-X. Our code is available at:
https://github.com/ExplainableML/ZS-A2T.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salewski_L/0/1/0/all/0/1&quot;&gt;Leonard Salewski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koepke_A/0/1/0/all/0/1&quot;&gt;A. Sophia Koepke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lensch_H/0/1/0/all/0/1&quot;&gt;Hendrik P. A. Lensch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1&quot;&gt;Zeynep Akata&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05071">
<title>On the Behavior of Audio-Visual Fusion Architectures in Identity Verification Tasks. (arXiv:2311.05071v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.05071</link>
<description rdf:parseType="Literal">&lt;p&gt;We train an identity verification architecture and evaluate modifications to
the part of the model that combines audio and visual representations, including
in scenarios where one input is missing in either of two examples to be
compared. We report results on the Voxceleb1-E test set that suggest averaging
the output embeddings improves error rate in the full-modality setting and when
a single modality is missing, and makes more complete use of the embedding
space than systems which use shared layers and discuss possible reasons for
this behavior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Claborne_D/0/1/0/all/0/1&quot;&gt;Daniel Claborne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Slyman_E/0/1/0/all/0/1&quot;&gt;Eric Slyman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pazdernik_K/0/1/0/all/0/1&quot;&gt;Karl Pazdernik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05077">
<title>POISE: Pose Guided Human Silhouette Extraction under Occlusions. (arXiv:2311.05077v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05077</link>
<description rdf:parseType="Literal">&lt;p&gt;Human silhouette extraction is a fundamental task in computer vision with
applications in various downstream tasks. However, occlusions pose a
significant challenge, leading to incomplete and distorted silhouettes. To
address this challenge, we introduce POISE: Pose Guided Human Silhouette
Extraction under Occlusions, a novel self-supervised fusion framework that
enhances accuracy and robustness in human silhouette prediction. By combining
initial silhouette estimates from a segmentation model with human joint
predictions from a 2D pose estimation model, POISE leverages the complementary
strengths of both approaches, effectively integrating precise body shape
information and spatial information to tackle occlusions. Furthermore, the
self-supervised nature of \POISE eliminates the need for costly annotations,
making it scalable and practical. Extensive experimental results demonstrate
its superiority in improving silhouette extraction under occlusions, with
promising results in downstream tasks such as gait recognition. The code for
our method is available https://github.com/take2rohit/poise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dutta_A/0/1/0/all/0/1&quot;&gt;Arindam Dutta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lal_R/0/1/0/all/0/1&quot;&gt;Rohit Lal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raychaudhuri_D/0/1/0/all/0/1&quot;&gt;Dripta S. Raychaudhuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ta_C/0/1/0/all/0/1&quot;&gt;Calvin Khang Ta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1&quot;&gt;Amit K. Roy-Chowdhury&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05100">
<title>Self-similarity Prior Distillation for Unsupervised Remote Physiological Measurement. (arXiv:2311.05100v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05100</link>
<description rdf:parseType="Literal">&lt;p&gt;Remote photoplethysmography (rPPG) is a noninvasive technique that aims to
capture subtle variations in facial pixels caused by changes in blood volume
resulting from cardiac activities. Most existing unsupervised methods for rPPG
tasks focus on the contrastive learning between samples while neglecting the
inherent self-similar prior in physiological signals. In this paper, we propose
a Self-Similarity Prior Distillation (SSPD) framework for unsupervised rPPG
estimation, which capitalizes on the intrinsic self-similarity of cardiac
activities. Specifically, we first introduce a physical-prior embedded
augmentation technique to mitigate the effect of various types of noise. Then,
we tailor a self-similarity-aware network to extract more reliable self-similar
physiological features. Finally, we develop a hierarchical self-distillation
paradigm to assist the network in disentangling self-similar physiological
patterns from facial videos. Comprehensive experiments demonstrate that the
unsupervised SSPD framework achieves comparable or even superior performance
compared to the state-of-the-art supervised methods. Meanwhile, SSPD maintains
the lowest inference time and computation cost among end-to-end models. The
source codes are available at https://github.com/LinXi1C/SSPD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Weiyu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Hao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Ying Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yun Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaolin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1&quot;&gt;Jie Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yingcong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05109">
<title>Reducing the Side-Effects of Oscillations in Training of Quantized YOLO Networks. (arXiv:2311.05109v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05109</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantized networks use less computational and memory resources and are
suitable for deployment on edge devices. While quantization-aware training QAT
is the well-studied approach to quantize the networks at low precision, most
research focuses on over-parameterized networks for classification with limited
studies on popular and edge device friendly single-shot object detection and
semantic segmentation methods like YOLO. Moreover, majority of QAT methods rely
on Straight-through Estimator (STE) approximation which suffers from an
oscillation phenomenon resulting in sub-optimal network quantization. In this
paper, we show that it is difficult to achieve extremely low precision (4-bit
and lower) for efficient YOLO models even with SOTA QAT methods due to
oscillation issue and existing methods to overcome this problem are not
effective on these models. To mitigate the effect of oscillation, we first
propose Exponentially Moving Average (EMA) based update to the QAT model.
Further, we propose a simple QAT correction method, namely QC, that takes only
a single epoch of training after standard QAT procedure to correct the error
induced by oscillating weights and activations resulting in a more accurate
quantized model. With extensive evaluation on COCO dataset using various YOLO5
and YOLO7 variants, we show that our correction method improves quantized YOLO
networks consistently on both object detection and segmentation tasks at
low-precision (4-bit and 3-bit).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1&quot;&gt;Kartik Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asthana_A/0/1/0/all/0/1&quot;&gt;Akshay Asthana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05122">
<title>ScribblePolyp: Scribble-Supervised Polyp Segmentation through Dual Consistency Alignment. (arXiv:2311.05122v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05122</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic polyp segmentation models play a pivotal role in the clinical
diagnosis of gastrointestinal diseases. In previous studies, most methods
relied on fully supervised approaches, necessitating pixel-level annotations
for model training. However, the creation of pixel-level annotations is both
expensive and time-consuming, impeding the development of model generalization.
In response to this challenge, we introduce ScribblePolyp, a novel
scribble-supervised polyp segmentation framework. Unlike fully-supervised
models, ScribblePolyp only requires the annotation of two lines (scribble
labels) for each image, significantly reducing the labeling cost. Despite the
coarse nature of scribble labels, which leave a substantial portion of pixels
unlabeled, we propose a two-branch consistency alignment approach to provide
supervision for these unlabeled pixels. The first branch employs transformation
consistency alignment to narrow the gap between predictions under different
transformations of the same input image. The second branch leverages affinity
propagation to refine predictions into a soft version, extending additional
supervision to unlabeled pixels. In summary, ScribblePolyp is an efficient
model that does not rely on teacher models or moving average pseudo labels
during training. Extensive experiments on the SUN-SEG dataset underscore the
effectiveness of ScribblePolyp, achieving a Dice score of 0.8155, with the
potential for a 1.8% improvement in the Dice score through a straightforward
self-training strategy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zixun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yuncheng Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1&quot;&gt;Jun Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1&quot;&gt;Hannah Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhen Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05143">
<title>SCAAT: Improving Neural Network Interpretability via Saliency Constrained Adaptive Adversarial Training. (arXiv:2311.05143v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05143</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Neural Networks (DNNs) are expected to provide explanation for users to
understand their black-box predictions. Saliency map is a common form of
explanation illustrating the heatmap of feature attributions, but it suffers
from noise in distinguishing important features. In this paper, we propose a
model-agnostic learning method called Saliency Constrained Adaptive Adversarial
Training (SCAAT) to improve the quality of such DNN interpretability. By
constructing adversarial samples under the guidance of saliency map, SCAAT
effectively eliminates most noise and makes saliency maps sparser and more
faithful without any modification to the model architecture. We apply SCAAT to
multiple DNNs and evaluate the quality of the generated saliency maps on
various natural and pathological image datasets. Evaluations on different
domains and metrics show that SCAAT significantly improves the interpretability
of DNNs by providing more faithful saliency maps without sacrificing their
predictive power.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Rui Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_W/0/1/0/all/0/1&quot;&gt;Wenkang Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1&quot;&gt;Peixiang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haowang/0/1/0/all/0/1&quot;&gt;Haowang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1&quot;&gt;Lin Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05146">
<title>OW-SLR: Overlapping Windows on Semi-Local Region for Image Super-Resolution. (arXiv:2311.05146v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05146</link>
<description rdf:parseType="Literal">&lt;p&gt;There has been considerable progress in implicit neural representation to
upscale an image to any arbitrary resolution. However, existing methods are
based on defining a function to predict the Red, Green and Blue (RGB) value
from just four specific loci. Relying on just four loci is insufficient as it
leads to losing fine details from the neighboring region(s). We show that by
taking into account the semi-local region leads to an improvement in
performance. In this paper, we propose applying a new technique called
Overlapping Windows on Semi-Local Region (OW-SLR) to an image to obtain any
arbitrary resolution by taking the coordinates of the semi-local region around
a point in the latent space. This extracted detail is used to predict the RGB
value of a point. We illustrate the technique by applying the algorithm to the
Optical Coherence Tomography-Angiography (OCT-A) images and show that it can
upscale them to random resolution. This technique outperforms the existing
state-of-the-art methods when applied to the OCT500 dataset. OW-SLR provides
better results for classifying healthy and diseased retinal images such as
diabetic retinopathy and normals from the given set of OCT-A images. The
project page is available at https://rishavbb.github.io/ow-slr/index.html
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhardwaj_R/0/1/0/all/0/1&quot;&gt;Rishav Bhardwaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balaji_J/0/1/0/all/0/1&quot;&gt;Janarthanam Jothi Balaji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lakshminarayanan_V/0/1/0/all/0/1&quot;&gt;Vasudevan Lakshminarayanan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05147">
<title>Dynamic Association Learning of Self-Attention and Convolution in Image Restoration. (arXiv:2311.05147v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05147</link>
<description rdf:parseType="Literal">&lt;p&gt;CNNs and Self attention have achieved great success in multimedia
applications for dynamic association learning of self-attention and convolution
in image restoration. However, CNNs have at least two shortcomings: 1) limited
receptive field; 2) static weight of sliding window at inference, unable to
cope with the content diversity.In view of the advantages and disadvantages of
CNNs and Self attention, this paper proposes an association learning method to
utilize the advantages and suppress their shortcomings, so as to achieve
high-quality and efficient inpainting. We regard rain distribution reflects the
degradation location and degree, in addition to the rain distribution
prediction. Thus, we propose to refine background textures with the predicted
degradation prior in an association learning manner. As a result, we accomplish
image deraining by associating rain streak removal and background recovery,
where an image deraining network and a background recovery network are designed
for two subtasks. The key part of association learning is a novel multi-input
attention module. It generates the degradation prior and produces the
degradation mask according to the predicted rainy distribution. Benefited from
the global correlation calculation of SA, MAM can extract the informative
complementary components from the rainy input with the degradation mask, and
then help accurate texture restoration. Meanwhile, SA tends to aggregate
feature maps with self-attention importance, but convolution diversifies them
to focus on the local textures. A hybrid fusion network involves one residual
Transformer branch and one encoder-decoder branch. The former takes a few
learnable tokens as input and stacks multi-head attention and feed-forward
networks to encode global features of the image. The latter, conversely,
leverages the multi-scale encoder-decoder to represent contexture knowledge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1&quot;&gt;Kui Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1&quot;&gt;Xuemei Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wenxin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenbin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Junjun Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05152">
<title>Cross-modal Prompts: Adapting Large Pre-trained Models for Audio-Visual Downstream Tasks. (arXiv:2311.05152v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.05152</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the deployment of large-scale pre-trained models in
audio-visual downstream tasks has yielded remarkable outcomes. However, these
models, primarily trained on single-modality unconstrained datasets, still
encounter challenges in feature extraction for multi-modal tasks, leading to
suboptimal performance. This limitation arises due to the introduction of
irrelevant modality-specific information during encoding, which adversely
affects the performance of downstream tasks. To address this challenge, this
paper proposes a novel Dual-Guided Spatial-Channel-Temporal (DG-SCT) attention
mechanism. This mechanism leverages audio and visual modalities as soft prompts
to dynamically adjust the parameters of pre-trained models based on the current
multi-modal input features. Specifically, the DG-SCT module incorporates
trainable cross-modal interaction layers into pre-trained audio-visual
encoders, allowing adaptive extraction of crucial information from the current
modality across spatial, channel, and temporal dimensions, while preserving the
frozen parameters of large-scale pre-trained models. Experimental evaluations
demonstrate that our proposed model achieves state-of-the-art results across
multiple downstream tasks, including AVE, AVVP, AVS, and AVQA. Furthermore, our
model exhibits promising performance in challenging few-shot and zero-shot
scenarios. The source code and pre-trained models are available at
https://github.com/haoyi-duan/DG-SCT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_H/0/1/0/all/0/1&quot;&gt;Haoyi Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yan Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mingze Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1&quot;&gt;Li Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jieming Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhou Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05168">
<title>FireMatch: A Semi-Supervised Video Fire Detection Network Based on Consistency and Distribution Alignment. (arXiv:2311.05168v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05168</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning techniques have greatly enhanced the performance of fire
detection in videos. However, video-based fire detection models heavily rely on
labeled data, and the process of data labeling is particularly costly and
time-consuming, especially when dealing with videos. Considering the limited
quantity of labeled video data, we propose a semi-supervised fire detection
model called FireMatch, which is based on consistency regularization and
adversarial distribution alignment. Specifically, we first combine consistency
regularization with pseudo-label. For unlabeled data, we design video data
augmentation to obtain corresponding weakly augmented and strongly augmented
samples. The proposed model predicts weakly augmented samples and retains
pseudo-label above a threshold, while training on strongly augmented samples to
predict these pseudo-labels for learning more robust feature representations.
Secondly, we generate video cross-set augmented samples by adversarial
distribution alignment to expand the training data and alleviate the decline in
classification performance caused by insufficient labeled data. Finally, we
introduce a fairness loss to help the model produce diverse predictions for
input samples, thereby addressing the issue of high confidence with the
non-fire class in fire classification scenarios. The FireMatch achieved an
accuracy of 76.92% and 91.81% on two real-world fire datasets, respectively.
The experimental results demonstrate that the proposed method outperforms the
current state-of-the-art semi-supervised classification methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1&quot;&gt;Qinghua Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zuoyong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_K/0/1/0/all/0/1&quot;&gt;Kun Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1&quot;&gt;Haoyi Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xiaoguang Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05190">
<title>Audio-visual Saliency for Omnidirectional Videos. (arXiv:2311.05190v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05190</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual saliency prediction for omnidirectional videos (ODVs) has shown great
significance and necessity for omnidirectional videos to help ODV coding, ODV
transmission, ODV rendering, etc.. However, most studies only consider visual
information for ODV saliency prediction while audio is rarely considered
despite its significant influence on the viewing behavior of ODV. This is
mainly due to the lack of large-scale audio-visual ODV datasets and
corresponding analysis. Thus, in this paper, we first establish the largest
audio-visual saliency dataset for omnidirectional videos (AVS-ODV), which
comprises the omnidirectional videos, audios, and corresponding captured
eye-tracking data for three video sound modalities including mute, mono, and
ambisonics. Then we analyze the visual attention behavior of the observers
under various omnidirectional audio modalities and visual scenes based on the
AVS-ODV dataset. Furthermore, we compare the performance of several
state-of-the-art saliency prediction models on the AVS-ODV dataset and
construct a new benchmark. Our AVS-ODV datasets and the benchmark will be
released to facilitate future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yuxin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xilei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_H/0/1/0/all/0/1&quot;&gt;Huiyu Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kaiwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yucheng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Li Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1&quot;&gt;Xiongkuo Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1&quot;&gt;Guangtao Zhai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05192">
<title>TransReg: Cross-transformer as auto-registration module for multi-view mammogram mass detection. (arXiv:2311.05192v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05192</link>
<description rdf:parseType="Literal">&lt;p&gt;Screening mammography is the most widely used method for early breast cancer
detection, significantly reducing mortality rates. The integration of
information from multi-view mammograms enhances radiologists&apos; confidence and
diminishes false-positive rates since they can examine on dual-view of the same
breast to cross-reference the existence and location of the lesion. Inspired by
this, we present TransReg, a Computer-Aided Detection (CAD) system designed to
exploit the relationship between craniocaudal (CC), and mediolateral oblique
(MLO) views. The system includes cross-transformer to model the relationship
between the region of interest (RoIs) extracted by siamese Faster RCNN network
for mass detection problems. Our work is the first time cross-transformer has
been integrated into an object detection framework to model the relation
between ipsilateral views. Our experimental evaluation on DDSM and VinDr-Mammo
datasets shows that our TransReg, equipped with SwinT as a feature extractor
achieves state-of-the-art performance. Specifically, at the false positive rate
per image at 0.5, TransReg using SwinT gets a recall at 83.3% for DDSM dataset
and 79.7% for VinDr-Mammo dataset. Furthermore, we conduct a comprehensive
analysis to demonstrate that cross-transformer can function as an
auto-registration module, aligning the masses in dual-view and utilizing this
information to inform final predictions. It is a replication diagnostic
workflow of expert radiologists
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1&quot;&gt;Hoang C. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phan_C/0/1/0/all/0/1&quot;&gt;Chi Phan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1&quot;&gt;Hieu H. Pham&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05197">
<title>Deep Learning in Computed Tomography Pulmonary Angiography Imaging: A Dual-Pronged Approach for Pulmonary Embolism Detection. (arXiv:2311.05197v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05197</link>
<description rdf:parseType="Literal">&lt;p&gt;Pulmonary Embolism (PE) is a critical medical condition characterized by
obstructions in the pulmonary arteries. Despite being a major health concern,
it often goes underdiagnosed leading to detrimental clinical outcomes. The
increasing reliance on Computed Tomography Pulmonary Angiography for diagnosis
presents challenges and a pressing need for enhanced diagnostic solutions. The
primary objective of this study is to leverage deep learning techniques to
enhance the Computer Assisted Diagnosis of PE. This study presents a
comprehensive dual-pronged approach combining classification and detection for
PE diagnosis. We introduce an Attention-Guided Convolutional Neural Network
(AG-CNN) for classification, addressing both global and local lesion region.
For detection, state-of-the-art models are employed to pinpoint potential PE
regions. Different ensembling techniques further improve detection accuracy by
combining predictions from different models. Finally, a heuristic strategy
integrates classifier outputs with detection results, ensuring robust and
accurate PE identification. Our attention-guided classification approach,
tested on the Ferdowsi University of Mashhad&apos;s Pulmonary Embolism (FUMPE)
dataset, outperformed the baseline model DenseNet-121 by achieving an 8.1%
increase in the Area Under the Receiver Operating Characteristic. By employing
ensemble techniques with detection models, the mean average precision (mAP) was
considerably enhanced by a 4.7% increase. The classifier-guided framework
further refined the mAP and F1 scores over the ensemble models. Our research
offers a comprehensive approach to PE diagnostics using deep learning,
addressing the prevalent issues of underdiagnosis and misdiagnosis. We aim to
improve PE patient care by integrating AI solutions into clinical workflows,
highlighting the potential of human-AI collaboration in medical diagnostics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bushra_F/0/1/0/all/0/1&quot;&gt;Fabiha Bushra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1&quot;&gt;Muhammad E. H. Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarmun_R/0/1/0/all/0/1&quot;&gt;Rusab Sarmun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kabir_S/0/1/0/all/0/1&quot;&gt;Saidul Kabir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Said_M/0/1/0/all/0/1&quot;&gt;Menatalla Said&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zoghoul_S/0/1/0/all/0/1&quot;&gt;Sohaib Bassam Zoghoul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mushtak_A/0/1/0/all/0/1&quot;&gt;Adam Mushtak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Hashimi_I/0/1/0/all/0/1&quot;&gt;Israa Al-Hashimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alqahtani_A/0/1/0/all/0/1&quot;&gt;Abdulrahman Alqahtani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1&quot;&gt;Anwarul Hasan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05198">
<title>Adaptive-Labeling for Enhancing Remote Sensing Cloud Understanding. (arXiv:2311.05198v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05198</link>
<description rdf:parseType="Literal">&lt;p&gt;Cloud analysis is a critical component of weather and climate science,
impacting various sectors like disaster management. However, achieving
fine-grained cloud analysis, such as cloud segmentation, in remote sensing
remains challenging due to the inherent difficulties in obtaining accurate
labels, leading to significant labeling errors in training data. Existing
methods often assume the availability of reliable segmentation annotations,
limiting their overall performance. To address this inherent limitation, we
introduce an innovative model-agnostic Cloud Adaptive-Labeling (CAL) approach,
which operates iteratively to enhance the quality of training data annotations
and consequently improve the performance of the learned model. Our methodology
commences by training a cloud segmentation model using the original
annotations. Subsequently, it introduces a trainable pixel intensity threshold
for adaptively labeling the cloud training images on the fly. The newly
generated labels are then employed to fine-tune the model. Extensive
experiments conducted on multiple standard cloud segmentation benchmarks
demonstrate the effectiveness of our approach in significantly boosting the
performance of existing segmentation models. Our CAL method establishes new
state-of-the-art results when compared to a wide array of existing
alternatives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gala_J/0/1/0/all/0/1&quot;&gt;Jay Gala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1&quot;&gt;Sauradip Nag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Huichou Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Ruirui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiatian Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05199">
<title>BrainNetDiff: Generative AI Empowers Brain Network Generation via Multimodal Diffusion Model. (arXiv:2311.05199v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05199</link>
<description rdf:parseType="Literal">&lt;p&gt;Brain network analysis has emerged as pivotal method for gaining a deeper
understanding of brain functions and disease mechanisms. Despite the existence
of various network construction approaches, shortcomings persist in the
learning of correlations between structural and functional brain imaging data.
In light of this, we introduce a novel method called BrainNetDiff, which
combines a multi-head Transformer encoder to extract relevant features from
fMRI time series and integrates a conditional latent diffusion model for brain
network generation. Leveraging a conditional prompt and a fusion attention
mechanism, this method significantly improves the accuracy and stability of
brain network generation. To the best of our knowledge, this represents the
first framework that employs diffusion for the fusion of the multimodal brain
imaging and brain network generation from images to graphs. We validate
applicability of this framework in the construction of brain network across
healthy and neurologically impaired cohorts using the authentic dataset.
Experimental results vividly demonstrate the significant effectiveness of the
proposed method across the downstream disease classification tasks. These
findings convincingly emphasize the prospective value in the field of brain
network research, particularly its key significance in neuroimaging analysis
and disease diagnosis. This research provides a valuable reference for the
processing of multimodal brain imaging data and introduces a novel, efficient
solution to the field of neuroimaging.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zong_Y/0/1/0/all/0/1&quot;&gt;Yongcheng Zong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuqiang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05221">
<title>Let&apos;s Get the FACS Straight -- Reconstructing Obstructed Facial Features. (arXiv:2311.05221v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05221</link>
<description rdf:parseType="Literal">&lt;p&gt;The human face is one of the most crucial parts in interhuman communication.
Even when parts of the face are hidden or obstructed the underlying facial
movements can be understood. Machine learning approaches often fail in that
regard due to the complexity of the facial structures. To alleviate this
problem a common approach is to fine-tune a model for such a specific
application. However, this is computational intensive and might have to be
repeated for each desired analysis task. In this paper, we propose to
reconstruct obstructed facial parts to avoid the task of repeated fine-tuning.
As a result, existing facial analysis methods can be used without further
changes with respect to the data. In our approach, the restoration of facial
features is interpreted as a style transfer task between different recording
setups. By using the CycleGAN architecture the requirement of matched pairs,
which is often hard to fullfill, can be eliminated. To proof the viability of
our approach, we compare our reconstructions with real unobstructed recordings.
We created a novel data set in which 36 test subjects were recorded both with
and without 62 surface electromyography sensors attached to their faces. In our
evaluation, we feature typical facial analysis tasks, like the computation of
Facial Action Units and the detection of emotions. To further assess the
quality of the restoration, we also compare perceptional distances. We can
show, that scores similar to the videos without obstructing sensors can be
achieved.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buchner_T/0/1/0/all/0/1&quot;&gt;Tim B&amp;#xfc;chner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sickert_S/0/1/0/all/0/1&quot;&gt;Sven Sickert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Volk_G/0/1/0/all/0/1&quot;&gt;Gerd Fabian Volk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anders_C/0/1/0/all/0/1&quot;&gt;Christoph Anders&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guntinas_Lichius_O/0/1/0/all/0/1&quot;&gt;Orlando Guntinas-Lichius&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denzler_J/0/1/0/all/0/1&quot;&gt;Joachim Denzler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05230">
<title>ConRad: Image Constrained Radiance Fields for 3D Generation from a Single Image. (arXiv:2311.05230v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05230</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel method for reconstructing 3D objects from a single RGB
image. Our method leverages the latest image generation models to infer the
hidden 3D structure while remaining faithful to the input image. While existing
methods obtain impressive results in generating 3D models from text prompts,
they do not provide an easy approach for conditioning on input RGB data.
Na\&quot;ive extensions of these methods often lead to improper alignment in
appearance between the input image and the 3D reconstructions. We address these
challenges by introducing Image Constrained Radiance Fields (ConRad), a novel
variant of neural radiance fields. ConRad is an efficient 3D representation
that explicitly captures the appearance of an input image in one viewpoint. We
propose a training algorithm that leverages the single RGB image in conjunction
with pretrained Diffusion Models to optimize the parameters of a ConRad
representation. Extensive experiments show that ConRad representations can
simplify preservation of image details while producing a realistic 3D
reconstruction. Compared to existing state-of-the-art baselines, we show that
our 3D reconstructions remain more faithful to the input and produce more
consistent 3D models while demonstrating significantly improved quantitative
performance on a ShapeNet object benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Purushwalkam_S/0/1/0/all/0/1&quot;&gt;Senthil Purushwalkam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naik_N/0/1/0/all/0/1&quot;&gt;Nikhil Naik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05237">
<title>Widely Applicable Strong Baseline for Sports Ball Detection and Tracking. (arXiv:2311.05237v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05237</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we present a novel Sports Ball Detection and Tracking (SBDT)
method that can be applied to various sports categories. Our approach is
composed of (1) high-resolution feature extraction, (2) position-aware model
training, and (3) inference considering temporal consistency, all of which are
put together as a new SBDT baseline. Besides, to validate the
wide-applicability of our approach, we compare our baseline with 6
state-of-the-art SBDT methods on 5 datasets from different sports categories.
We achieve this by newly introducing two SBDT datasets, providing new ball
annotations for two datasets, and re-implementing all the methods to ease
extensive comparison. Experimental results demonstrate that our approach is
substantially superior to existing methods on all the sports categories covered
by the datasets. We believe our proposed method can play as a Widely Applicable
Strong Baseline (WASB) of SBDT, and our datasets and codebase will promote
future SBDT research. Datasets and codes will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tarashima_S/0/1/0/all/0/1&quot;&gt;Shuhei Tarashima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haq_M/0/1/0/all/0/1&quot;&gt;Muhammad Abdul Haq&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yushan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tagawa_N/0/1/0/all/0/1&quot;&gt;Norio Tagawa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05269">
<title>Single-shot Tomography of Discrete Dynamic Objects. (arXiv:2311.05269v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.05269</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel method for the reconstruction of high-resolution
temporal images in dynamic tomographic imaging, particularly for discrete
objects with smooth boundaries that vary over time. Addressing the challenge of
limited measurements per time point, we propose a technique that
synergistically incorporates spatial and temporal information of the dynamic
objects. This is achieved through the application of the level-set method for
image segmentation and the representation of motion via a sinusoidal basis. The
result is a computationally efficient and easily optimizable variational
framework that enables the reconstruction of high-quality 2D or 3D image
sequences with a single projection per frame. Compared to current methods, our
proposed approach demonstrates superior performance on both synthetic and
pseudo-dynamic real X-ray tomography datasets. The implications of this
research extend to improved visualization and analysis of dynamic processes in
tomographic imaging, finding potential applications in diverse scientific and
industrial domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kadu_A/0/1/0/all/0/1&quot;&gt;Ajinkya Kadu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lucka_F/0/1/0/all/0/1&quot;&gt;Felix Lucka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Batenburg_K/0/1/0/all/0/1&quot;&gt;Kees Joost Batenburg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05276">
<title>SAMVG: A Multi-stage Image Vectorization Model with the Segment-Anything Model. (arXiv:2311.05276v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05276</link>
<description rdf:parseType="Literal">&lt;p&gt;Vector graphics are widely used in graphical designs and have received more
and more attention. However, unlike raster images which can be easily obtained,
acquiring high-quality vector graphics, typically through automatically
converting from raster images remains a significant challenge, especially for
more complex images such as photos or artworks. In this paper, we propose
SAMVG, a multi-stage model to vectorize raster images into SVG (Scalable Vector
Graphics). Firstly, SAMVG uses general image segmentation provided by the
Segment-Anything Model and uses a novel filtering method to identify the best
dense segmentation map for the entire image. Secondly, SAMVG then identifies
missing components and adds more detailed components to the SVG. Through a
series of extensive experiments, we demonstrate that SAMVG can produce high
quality SVGs in any domain while requiring less computation time and complexity
compared to previous state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Haokun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chong_J/0/1/0/all/0/1&quot;&gt;Juang Ian Chong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1&quot;&gt;Teng Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_R/0/1/0/all/0/1&quot;&gt;Ran Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1&quot;&gt;Yu-Kun Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosin_P/0/1/0/all/0/1&quot;&gt;Paul L. Rosin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05289">
<title>VoxNeRF: Bridging Voxel Representation and Neural Radiance Fields for Enhanced Indoor View Synthesis. (arXiv:2311.05289v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05289</link>
<description rdf:parseType="Literal">&lt;p&gt;Creating high-quality view synthesis is essential for immersive applications
but continues to be problematic, particularly in indoor environments and for
real-time deployment. Current techniques frequently require extensive
computational time for both training and rendering, and often produce
less-than-ideal 3D representations due to inadequate geometric structuring. To
overcome this, we introduce VoxNeRF, a novel approach that leverages volumetric
representations to enhance the quality and efficiency of indoor view synthesis.
Firstly, VoxNeRF constructs a structured scene geometry and converts it into a
voxel-based representation. We employ multi-resolution hash grids to adaptively
capture spatial features, effectively managing occlusions and the intricate
geometry of indoor scenes. Secondly, we propose a unique voxel-guided efficient
sampling technique. This innovation selectively focuses computational resources
on the most relevant portions of ray segments, substantially reducing
optimization time. We validate our approach against three public indoor
datasets and demonstrate that VoxNeRF outperforms state-of-the-art methods.
Remarkably, it achieves these gains while reducing both training and rendering
times, surpassing even Instant-NGP in speed and bringing the technology closer
to real-time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gasperini_S/0/1/0/all/0/1&quot;&gt;Stefano Gasperini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Shun-Cheng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1&quot;&gt;Nassir Navab&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05298">
<title>Improving Vision-and-Language Reasoning via Spatial Relations Modeling. (arXiv:2311.05298v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05298</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual commonsense reasoning (VCR) is a challenging multi-modal task, which
requires high-level cognition and commonsense reasoning ability about the real
world. In recent years, large-scale pre-training approaches have been developed
and promoted the state-of-the-art performance of VCR. However, the existing
approaches almost employ the BERT-like objectives to learn multi-modal
representations. These objectives motivated from the text-domain are
insufficient for the excavation on the complex scenario of visual modality.
Most importantly, the spatial distribution of the visual objects is basically
neglected. To address the above issue, we propose to construct the spatial
relation graph based on the given visual scenario. Further, we design two
pre-training tasks named object position regression (OPR) and spatial relation
classification (SRC) to learn to reconstruct the spatial relation graph
respectively. Quantitative analysis suggests that the proposed method can guide
the representations to maintain more spatial context and facilitate the
attention on the essential visual regions for reasoning. We achieve the
state-of-the-art results on VCR and two other vision-and-language reasoning
tasks VQA, and NLVR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Cheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Rui Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Ye Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1&quot;&gt;Peixiang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yiru Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1&quot;&gt;Wenkui Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhongyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hong Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05310">
<title>SPADES: A Realistic Spacecraft Pose Estimation Dataset using Event Sensing. (arXiv:2311.05310v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05310</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, there has been a growing demand for improved autonomy for
in-orbit operations such as rendezvous, docking, and proximity maneuvers,
leading to increased interest in employing Deep Learning-based Spacecraft Pose
Estimation techniques. However, due to limited access to real target datasets,
algorithms are often trained using synthetic data and applied in the real
domain, resulting in a performance drop due to the domain gap. State-of-the-art
approaches employ Domain Adaptation techniques to mitigate this issue. In the
search for viable solutions, event sensing has been explored in the past and
shown to reduce the domain gap between simulations and real-world scenarios.
Event sensors have made significant advancements in hardware and software in
recent years. Moreover, the characteristics of the event sensor offer several
advantages in space applications compared to RGB sensors. To facilitate further
training and evaluation of DL-based models, we introduce a novel dataset,
SPADES, comprising real event data acquired in a controlled laboratory
environment and simulated event data using the same camera intrinsics.
Furthermore, we propose an effective data filtering method to improve the
quality of training data, thus enhancing model performance. Additionally, we
introduce an image-based event representation that outperforms existing
representations. A multifaceted baseline evaluation was conducted using
different event representations, event filtering strategies, and algorithmic
frameworks, and the results are summarized. The dataset will be made available
at &lt;a href=&quot;http://cvi2.uni.lu/spades.&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rathinam_A/0/1/0/all/0/1&quot;&gt;Arunkumar Rathinam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qadadri_H/0/1/0/all/0/1&quot;&gt;Haytam Qadadri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aouada_D/0/1/0/all/0/1&quot;&gt;Djamila Aouada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05323">
<title>Spatial Attention-based Distribution Integration Network for Human Pose Estimation. (arXiv:2311.05323v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05323</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, human pose estimation has made significant progress through
the implementation of deep learning techniques. However, these techniques still
face limitations when confronted with challenging scenarios, including
occlusion, diverse appearances, variations in illumination, and overlap. To
cope with such drawbacks, we present the Spatial Attention-based Distribution
Integration Network (SADI-NET) to improve the accuracy of localization in such
situations. Our network consists of three efficient models: the receptive
fortified module (RFM), spatial fusion module (SFM), and distribution learning
module (DLM). Building upon the classic HourglassNet architecture, we replace
the basic block with our proposed RFM. The RFM incorporates a dilated residual
block and attention mechanism to expand receptive fields while enhancing
sensitivity to spatial information. In addition, the SFM incorporates
multi-scale characteristics by employing both global and local attention
mechanisms. Furthermore, the DLM, inspired by residual log-likelihood
estimation (RLE), reconfigures a predicted heatmap using a trainable
distribution weight. For the purpose of determining the efficacy of our model,
we conducted extensive experiments on the MPII and LSP benchmarks.
Particularly, our model obtained a remarkable $92.10\%$ percent accuracy on the
MPII test dataset, demonstrating significant improvements over existing models
and establishing state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Sihan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jing Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_X/0/1/0/all/0/1&quot;&gt;Xiaoxuan Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhaoyue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qijin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05332">
<title>On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving. (arXiv:2311.05332v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05332</link>
<description rdf:parseType="Literal">&lt;p&gt;The pursuit of autonomous driving technology hinges on the sophisticated
integration of perception, decision-making, and control systems. Traditional
approaches, both data-driven and rule-based, have been hindered by their
inability to grasp the nuance of complex driving environments and the
intentions of other road users. This has been a significant bottleneck,
particularly in the development of common sense reasoning and nuanced scene
understanding necessary for safe and reliable autonomous driving. The advent of
Visual Language Models (VLM) represents a novel frontier in realizing fully
autonomous vehicle driving. This report provides an exhaustive evaluation of
the latest state-of-the-art VLM, \modelnamefull, and its application in
autonomous driving scenarios. We explore the model&apos;s abilities to understand
and reason about driving scenes, make decisions, and ultimately act in the
capacity of a driver. Our comprehensive tests span from basic scene recognition
to complex causal reasoning and real-time decision-making under varying
conditions. Our findings reveal that \modelname demonstrates superior
performance in scene understanding and causal reasoning compared to existing
autonomous systems. It showcases the potential to handle out-of-distribution
scenarios, recognize intentions, and make informed decisions in real driving
contexts. However, challenges remain, particularly in direction discernment,
traffic light recognition, vision grounding, and spatial reasoning tasks. These
limitations underscore the need for further research and development. Project
is now available on GitHub for interested parties to access and utilize:
\url{https://github.com/PJLab-ADG/GPT4V-AD-Exploration}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1&quot;&gt;Licheng Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xuemeng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1&quot;&gt;Daocheng Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1&quot;&gt;Pinlong Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1&quot;&gt;Tao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yingxuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Linran Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_D/0/1/0/all/0/1&quot;&gt;Dengke Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Shaoyan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yeqi Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1&quot;&gt;Xinyu Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_M/0/1/0/all/0/1&quot;&gt;Min Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shuanglu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Botian Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05336">
<title>SynFacePAD 2023: Competition on Face Presentation Attack Detection Based on Privacy-aware Synthetic Training Data. (arXiv:2311.05336v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05336</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a summary of the Competition on Face Presentation Attack
Detection Based on Privacy-aware Synthetic Training Data (SynFacePAD 2023) held
at the 2023 International Joint Conference on Biometrics (IJCB 2023). The
competition attracted a total of 8 participating teams with valid submissions
from academia and industry. The competition aimed to motivate and attract
solutions that target detecting face presentation attacks while considering
synthetic-based training data motivated by privacy, legal and ethical concerns
associated with personal data. To achieve that, the training data used by the
participants was limited to synthetic data provided by the organizers. The
submitted solutions presented innovations and novel approaches that led to
outperforming the considered baseline in the investigated benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1&quot;&gt;Meiling Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huber_M/0/1/0/all/0/1&quot;&gt;Marco Huber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1&quot;&gt;Julian Fierrez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramachandra_R/0/1/0/all/0/1&quot;&gt;Raghavendra Ramachandra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1&quot;&gt;Naser Damer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alkhaddour_A/0/1/0/all/0/1&quot;&gt;Alhasan Alkhaddour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasantcev_M/0/1/0/all/0/1&quot;&gt;Maksim Kasantcev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pryadchenko_V/0/1/0/all/0/1&quot;&gt;Vasiliy Pryadchenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Ziyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huangfu_H/0/1/0/all/0/1&quot;&gt;Huijie Huangfu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yingyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yuchen Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Junjun Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xianming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xianyun Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Caiyong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xingyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Z/0/1/0/all/0/1&quot;&gt;Zhaohua Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1&quot;&gt;Guangzhe Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tapia_J/0/1/0/all/0/1&quot;&gt;Juan Tapia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_Soler_L/0/1/0/all/0/1&quot;&gt;Lazaro Gonzalez-Soler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aravena_C/0/1/0/all/0/1&quot;&gt;Carlos Aravena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulz_D/0/1/0/all/0/1&quot;&gt;Daniel Schulz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05348">
<title>u-LLaVA: Unifying Multi-Modal Tasks via Large Language Model. (arXiv:2311.05348v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05348</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances such as LLaVA and Mini-GPT4 have successfully integrated
visual information into LLMs, yielding inspiring outcomes and giving rise to a
new generation of multi-modal LLMs, or MLLMs. Nevertheless, these methods
struggle with hallucinations and the mutual interference between tasks. To
tackle these problems, we propose an efficient and accurate approach to adapt
to downstream tasks by utilizing LLM as a bridge to connect multiple expert
models, namely u-LLaVA. Firstly, we incorporate the modality alignment module
and multi-task modules into LLM. Then, we reorganize or rebuild multi-type
public datasets to enable efficient modality alignment and instruction
following. Finally, task-specific information is extracted from the trained LLM
and provided to different modules for solving downstream tasks. The overall
framework is simple, effective, and achieves state-of-the-art performance
across multiple benchmarks. We also release our model, the generated data, and
the code base publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jinjin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Liwu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuzhe Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yanchun Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yi-Jie Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yaqian Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05371">
<title>Training Robust Deep Physiological Measurement Models with Synthetic Video-based Data. (arXiv:2311.05371v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05371</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in supervised deep learning techniques have demonstrated the
possibility to remotely measure human physiological vital signs (e.g.,
photoplethysmograph, heart rate) just from facial videos. However, the
performance of these methods heavily relies on the availability and diversity
of real labeled data. Yet, collecting large-scale real-world data with
high-quality labels is typically challenging and resource intensive, which also
raises privacy concerns when storing personal bio-metric data. Synthetic
video-based datasets (e.g., SCAMPS~\cite{mcduff2022scamps}) with
photo-realistic synthesized avatars are introduced to alleviate the issues
while providing high-quality synthetic data. However, there exists a
significant gap between synthetic and real-world data, which hinders the
generalization of neural models trained on these synthetic datasets. In this
paper, we proposed several measures to add real-world noise to synthetic
physiological signals and corresponding facial videos. We experimented with
individual and combined augmentation methods and evaluated our framework on
three public real-world datasets. Our results show that we were able to reduce
the average MAE from 6.9 to 2.0.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ou_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Ou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuzhe Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuntang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1&quot;&gt;Shwetak Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McDuf_D/0/1/0/all/0/1&quot;&gt;Daniel McDuf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xin Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05383">
<title>Improving Hand Recognition in Uncontrolled and Uncooperative Environments using Multiple Spatial Transformers and Loss Functions. (arXiv:2311.05383v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05383</link>
<description rdf:parseType="Literal">&lt;p&gt;The prevalence of smartphone and consumer camera has led to more evidence in
the form of digital images, which are mostly taken in uncontrolled and
uncooperative environments. In these images, criminals likely hide or cover
their faces while their hands are observable in some cases, creating a
challenging use case for forensic investigation. Many existing hand-based
recognition methods perform well for hand images collected in controlled
environments with user cooperation. However, their performance deteriorates
significantly in uncontrolled and uncooperative environments. A recent work has
exposed the potential of hand recognition in these environments. However, only
the palmar regions were considered, and the recognition performance is still
far from satisfactory. To improve the recognition accuracy, an algorithm
integrating a multi-spatial transformer network (MSTN) and multiple loss
functions is proposed to fully utilize information in full hand images. MSTN is
firstly employed to localize the palms and fingers and estimate the alignment
parameters. Then, the aligned images are further fed into pretrained
convolutional neural networks, where features are extracted. Finally, a
training scheme with multiple loss functions is used to train the network
end-to-end. To demonstrate the effectiveness of the proposed algorithm, the
trained model is evaluated on NTU-PI-v1 database and six benchmark databases
from different domains. Experimental results show that the proposed algorithm
performs significantly better than the existing methods in these uncontrolled
and uncooperative environments and has good generalization capabilities to
samples from different domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matkowski_W/0/1/0/all/0/1&quot;&gt;Wojciech Michal Matkowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaojie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_A/0/1/0/all/0/1&quot;&gt;Adams Wai Kin Kong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05400">
<title>SIRE: scale-invariant, rotation-equivariant estimation of artery orientations using graph neural networks. (arXiv:2311.05400v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05400</link>
<description rdf:parseType="Literal">&lt;p&gt;Blood vessel orientation as visualized in 3D medical images is an important
descriptor of its geometry that can be used for centerline extraction and
subsequent segmentation and visualization. Arteries appear at many scales and
levels of tortuosity, and determining their exact orientation is challenging.
Recent works have used 3D convolutional neural networks (CNNs) for this
purpose, but CNNs are sensitive to varying vessel sizes and orientations. We
present SIRE: a scale-invariant, rotation-equivariant estimator for local
vessel orientation. SIRE is modular and can generalise due to symmetry
preservation.
&lt;/p&gt;
&lt;p&gt;SIRE consists of a gauge equivariant mesh CNN (GEM-CNN) operating on multiple
nested spherical meshes with different sizes in parallel. The features on each
mesh are a projection of image intensities within the corresponding sphere.
These features are intrinsic to the sphere and, in combination with the
GEM-CNN, lead to SO(3)-equivariance. Approximate scale invariance is achieved
by weight sharing and use of a symmetric maximum function to combine
multi-scale predictions. Hence, SIRE can be trained with arbitrarily oriented
vessels with varying radii to generalise to vessels with a wide range of
calibres and tortuosity.
&lt;/p&gt;
&lt;p&gt;We demonstrate the efficacy of SIRE using three datasets containing vessels
of varying scales: the vascular model repository (VMR), the ASOCA coronary
artery set, and a set of abdominal aortic aneurysms (AAAs). We embed SIRE in a
centerline tracker which accurately tracks AAAs, regardless of the data SIRE is
trained with. Moreover, SIRE can be used to track coronary arteries, even when
trained only with AAAs.
&lt;/p&gt;
&lt;p&gt;In conclusion, by incorporating SO(3) and scale symmetries, SIRE can
determine the orientations of vessels outside of the training domain, forming a
robust and data-efficient solution to geometric analysis of blood vessels in 3D
medical images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alblas_D/0/1/0/all/0/1&quot;&gt;Dieuwertje Alblas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suk_J/0/1/0/all/0/1&quot;&gt;Julian Suk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brune_C/0/1/0/all/0/1&quot;&gt;Christoph Brune&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeung_K/0/1/0/all/0/1&quot;&gt;Kak Khee Yeung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolterink_J/0/1/0/all/0/1&quot;&gt;Jelmer M. Wolterink&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05410">
<title>Linear Gaussian Bounding Box Representation and Ring-Shaped Rotated Convolution for Oriented Object Detection. (arXiv:2311.05410v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05410</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the frequent variability of object orientation, accurate prediction of
orientation information remains a challenge in oriented object detection. To
better extract orientation-related information, current methods primarily focus
on the design of reasonable representations of oriented bounding box (OBB) and
rotation-sensitive feature extraction. However, existing OBB representations
often suffer from boundary discontinuity and representation ambiguity problems.
Methods of designing continuous and unambiguous regression losses do not
essentially solve such problems. Gaussian bounding box (GBB) avoids these OBB
representation problems, but directly regressing GBB is susceptible to
numerical instability. In this paper, we propose linear GBB (LGBB), a novel OBB
representation. By linearly transforming the elements of GBB, LGBB does not
have the boundary discontinuity and representation ambiguity problems, and have
high numerical stability. On the other hand, current rotation-sensitive feature
extraction methods based on convolutions can only extract features under a
local receptive field, which is slow in aggregating rotation-sensitive
features. To address this issue, we propose ring-shaped rotated convolution
(RRC). By adaptively rotating feature maps to arbitrary orientations, RRC
extracts rotation-sensitive features under a ring-shaped receptive field,
rapidly aggregating rotation-sensitive features and contextual information. RRC
can be applied to various models in a plug-and-play manner. Experimental
results demonstrate that the proposed LGBB and RRC are effective and achieve
state-of-the-art (SOTA) performance. By integrating LGBB and RRC into various
models, the detection accuracy is effectively improved on DOTA and HRSC2016
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yunkai Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1&quot;&gt;Junfeng Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhaoyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_F/0/1/0/all/0/1&quot;&gt;Fengshui Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1&quot;&gt;Min Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05425">
<title>Active Mining Sample Pair Semantics for Image-text Matching. (arXiv:2311.05425v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05425</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, commonsense learning has been a hot topic in image-text matching.
Although it can describe more graphic correlations, commonsense learning still
has some shortcomings: 1) The existing methods are based on triplet semantic
similarity measurement loss, which cannot effectively match the intractable
negative in image-text sample pairs. 2) The weak generalization ability of the
model leads to the poor effect of image and text matching on large-scale
datasets. According to these shortcomings. This paper proposes a novel
image-text matching model, called Active Mining Sample Pair Semantics
image-text matching model (AMSPS). Compared with the single semantic learning
mode of the commonsense learning model with triplet loss function, AMSPS is an
active learning idea. Firstly, the proposed Adaptive Hierarchical Reinforcement
Loss (AHRL) has diversified learning modes. Its active learning mode enables
the model to more focus on the intractable negative samples to enhance the
discriminating ability. In addition, AMSPS can also adaptively mine more hidden
relevant semantic representations from uncommented items, which greatly
improves the performance and generalization ability of the model. Experimental
results on Flickr30K and MSCOCO universal datasets show that our proposed
method is superior to advanced comparison methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chena_Y/0/1/0/all/0/1&quot;&gt;Yongfeng Chena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liua_J/0/1/0/all/0/1&quot;&gt;Jin Liua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhijing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chena_R/0/1/0/all/0/1&quot;&gt;Ruihan Chena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1&quot;&gt;Junpeng Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05432">
<title>Dual Pipeline Style Transfer with Input Distribution Differentiation. (arXiv:2311.05432v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05432</link>
<description rdf:parseType="Literal">&lt;p&gt;The color and texture dual pipeline architecture (CTDP) suppresses texture
representation and artifacts through masked total variation loss (Mtv), and
further experiments have shown that smooth input can almost completely
eliminate texture representation. We have demonstrated through experiments that
smooth input is not the key reason for removing texture representations, but
rather the distribution differentiation of the training dataset. Based on this,
we propose an input distribution differentiation training strategy (IDD), which
forces the generation of textures to be completely dependent on the noise
distribution, while the smooth distribution will not produce textures at all.
Overall, our proposed distribution differentiation training strategy allows for
two pre-defined input distributions to be responsible for two generation tasks,
with noise distribution responsible for texture generation and smooth
distribution responsible for color smooth transfer. Finally, we choose a smooth
distribution as the input for the forward inference stage to completely
eliminate texture representations and artifacts in color transfer tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1&quot;&gt;ShiQi Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1&quot;&gt;JunJie Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;YuJian Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05437">
<title>LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents. (arXiv:2311.05437v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05437</link>
<description rdf:parseType="Literal">&lt;p&gt;LLaVA-Plus is a general-purpose multimodal assistant that expands the
capabilities of large multimodal models. It maintains a skill repository of
pre-trained vision and vision-language models and can activate relevant tools
based on users&apos; inputs to fulfill real-world tasks. LLaVA-Plus is trained on
multimodal instruction-following data to acquire the ability to use tools,
covering visual understanding, generation, external knowledge retrieval, and
compositions. Empirical results show that LLaVA-Plus outperforms LLaVA in
existing capabilities and exhibits new ones. It is distinct in that the image
query is directly grounded and actively engaged throughout the entire human-AI
interaction sessions, significantly improving tool use performance and enabling
new scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shilong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1&quot;&gt;Hao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haotian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Feng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1&quot;&gt;Tianhe Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1&quot;&gt;Xueyan Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jianwei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hang Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jianfeng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chunyuan Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05452">
<title>Transformer-based Model for Oral Epithelial Dysplasia Segmentation. (arXiv:2311.05452v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.05452</link>
<description rdf:parseType="Literal">&lt;p&gt;Oral epithelial dysplasia (OED) is a premalignant histopathological diagnosis
given to lesions of the oral cavity. OED grading is subject to large
inter/intra-rater variability, resulting in the under/over-treatment of
patients. We developed a new Transformer-based pipeline to improve detection
and segmentation of OED in haematoxylin and eosin (H&amp;amp;E) stained whole slide
images (WSIs). Our model was trained on OED cases (n = 260) and controls (n =
105) collected using three different scanners, and validated on test data from
three external centres in the United Kingdom and Brazil (n = 78). Our internal
experiments yield a mean F1-score of 0.81 for OED segmentation, which reduced
slightly to 0.71 on external testing, showing good generalisability, and
gaining state-of-the-art results. This is the first externally validated study
to use Transformers for segmentation in precancerous histology images. Our
publicly available model shows great promise to be the first step of a
fully-integrated pipeline, allowing earlier and more efficient OED diagnosis,
ultimately benefiting patient outcomes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shephard_A/0/1/0/all/0/1&quot;&gt;Adam J Shephard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mahmood_H/0/1/0/all/0/1&quot;&gt;Hanya Mahmood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Raza_S/0/1/0/all/0/1&quot;&gt;Shan E Ahmed Raza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Araujo_A/0/1/0/all/0/1&quot;&gt;Anna Luiza Damaceno Araujo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Santos_Silva_A/0/1/0/all/0/1&quot;&gt;Alan Roger Santos-Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lopes_M/0/1/0/all/0/1&quot;&gt;Marcio Ajudarte Lopes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vargas_P/0/1/0/all/0/1&quot;&gt;Pablo Agustin Vargas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+McCombe_K/0/1/0/all/0/1&quot;&gt;Kris McCombe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Craig_S/0/1/0/all/0/1&quot;&gt;Stephanie Craig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+James_J/0/1/0/all/0/1&quot;&gt;Jacqueline James&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Brooks_J/0/1/0/all/0/1&quot;&gt;Jill Brooks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nankivell_P/0/1/0/all/0/1&quot;&gt;Paul Nankivell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mehanna_H/0/1/0/all/0/1&quot;&gt;Hisham Mehanna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khurram_S/0/1/0/all/0/1&quot;&gt;Syed Ali Khurram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rajpoot_N/0/1/0/all/0/1&quot;&gt;Nasir M Rajpoot&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05461">
<title>Control3D: Towards Controllable Text-to-3D Generation. (arXiv:2311.05461v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05461</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent remarkable advances in large-scale text-to-image diffusion models have
inspired a significant breakthrough in text-to-3D generation, pursuing 3D
content creation solely from a given text prompt. However, existing text-to-3D
techniques lack a crucial ability in the creative process: interactively
control and shape the synthetic 3D contents according to users&apos; desired
specifications (e.g., sketch). To alleviate this issue, we present the first
attempt for text-to-3D generation conditioning on the additional hand-drawn
sketch, namely Control3D, which enhances controllability for users. In
particular, a 2D conditioned diffusion model (ControlNet) is remoulded to guide
the learning of 3D scene parameterized as NeRF, encouraging each view of 3D
scene aligned with the given text prompt and hand-drawn sketch. Moreover, we
exploit a pre-trained differentiable photo-to-sketch model to directly estimate
the sketch of the rendered image over synthetic 3D scene. Such estimated sketch
along with each sampled view is further enforced to be geometrically consistent
with the given sketch, pursuing better controllable text-to-3D generation.
Through extensive experiments, we demonstrate that our proposal can generate
accurate and faithful 3D scenes that align closely with the input text prompts
and sketches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yingwei Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yehao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1&quot;&gt;Ting Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1&quot;&gt;Tao Mei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05463">
<title>ControlStyle: Text-Driven Stylized Image Generation Using Diffusion Priors. (arXiv:2311.05463v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05463</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the multimedia community has witnessed the rise of diffusion models
trained on large-scale multi-modal data for visual content creation,
particularly in the field of text-to-image generation. In this paper, we
propose a new task for ``stylizing&apos;&apos; text-to-image models, namely text-driven
stylized image generation, that further enhances editability in content
creation. Given input text prompt and style image, this task aims to produce
stylized images which are both semantically relevant to input text prompt and
meanwhile aligned with the style image in style. To achieve this, we present a
new diffusion model (ControlStyle) via upgrading a pre-trained text-to-image
model with a trainable modulation network enabling more conditions of text
prompts and style images. Moreover, diffusion style and content regularizations
are simultaneously introduced to facilitate the learning of this modulation
network with these diffusion priors, pursuing high-quality stylized
text-to-image generation. Extensive experiments demonstrate the effectiveness
of our ControlStyle in producing more visually pleasing and artistic results,
surpassing a simple combination of text-to-image model and conventional style
transfer techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jingwen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yingwei Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1&quot;&gt;Ting Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1&quot;&gt;Tao Mei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05464">
<title>3DStyle-Diffusion: Pursuing Fine-grained Text-driven 3D Stylization with 2D Diffusion Models. (arXiv:2311.05464v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05464</link>
<description rdf:parseType="Literal">&lt;p&gt;3D content creation via text-driven stylization has played a fundamental
challenge to multimedia and graphics community. Recent advances of cross-modal
foundation models (e.g., CLIP) have made this problem feasible. Those
approaches commonly leverage CLIP to align the holistic semantics of stylized
mesh with the given text prompt. Nevertheless, it is not trivial to enable more
controllable stylization of fine-grained details in 3D meshes solely based on
such semantic-level cross-modal supervision. In this work, we propose a new
3DStyle-Diffusion model that triggers fine-grained stylization of 3D meshes
with additional controllable appearance and geometric guidance from 2D
Diffusion models. Technically, 3DStyle-Diffusion first parameterizes the
texture of 3D mesh into reflectance properties and scene lighting using
implicit MLP networks. Meanwhile, an accurate depth map of each sampled view is
achieved conditioned on 3D mesh. Then, 3DStyle-Diffusion leverages a
pre-trained controllable 2D Diffusion model to guide the learning of rendered
images, encouraging the synthesized image of each view semantically aligned
with text prompt and geometrically consistent with depth map. This way
elegantly integrates both image rendering via implicit MLP networks and
diffusion process of image synthesis in an end-to-end fashion, enabling a
high-quality fine-grained stylization of 3D meshes. We also build a new dataset
derived from Objaverse and the evaluation protocol for this task. Through both
qualitative and quantitative experiments, we validate the capability of our
3DStyle-Diffusion. Source code and data are available at
\url{https://github.com/yanghb22-fdu/3DStyle-Diffusion-Official}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Haibo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yingwei Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1&quot;&gt;Ting Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhineng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1&quot;&gt;Tao Mei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05477">
<title>Using ResNet to Utilize 4-class T2-FLAIR Slice Classification Based on the Cholinergic Pathways Hyperintensities Scale for Pathological Aging. (arXiv:2311.05477v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.05477</link>
<description rdf:parseType="Literal">&lt;p&gt;The Cholinergic Pathways Hyperintensities Scale (CHIPS) is a visual rating
scale used to assess the extent of cholinergic white matter hyperintensities in
T2-FLAIR images, serving as an indicator of dementia severity. However, the
manual selection of four specific slices for rating throughout the entire brain
is a time-consuming process. Our goal was to develop a deep learning-based
model capable of automatically identifying the four slices relevant to CHIPS.
To achieve this, we trained a 4-class slice classification model (BSCA) using
the ADNI T2-FLAIR dataset (N=150) with the assistance of ResNet. Subsequently,
we tested the model&apos;s performance on a local dataset (N=30). The results
demonstrated the efficacy of our model, with an accuracy of 99.82% and an
F1-score of 99.83%. This achievement highlights the potential impact of BSCA as
an automatic screening tool, streamlining the selection of four specific
T2-FLAIR slices that encompass white matter landmarks along the cholinergic
pathways. Clinicians can leverage this tool to assess the risk of clinical
dementia development efficiently.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tsai_W/0/1/0/all/0/1&quot;&gt;Wei-Chun Kevin Tsai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yi-Chien Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yu_M/0/1/0/all/0/1&quot;&gt;Ming-Chun Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chou_C/0/1/0/all/0/1&quot;&gt;Chia-Ju Chou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yan_S/0/1/0/all/0/1&quot;&gt;Sui-Hing Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yang-Teng Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yan-Hsiang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chiu_Y/0/1/0/all/0/1&quot;&gt;Yen-Ling Chiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chuang_Y/0/1/0/all/0/1&quot;&gt;Yi-Fang Chuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ran-Zan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shih_Y/0/1/0/all/0/1&quot;&gt;Yao-Chia Shih&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05478">
<title>Robust Retraining-free GAN Fingerprinting via Personalized Normalization. (arXiv:2311.05478v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05478</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, there has been significant growth in the commercial
applications of generative models, licensed and distributed by model developers
to users, who in turn use them to offer services. In this scenario, there is a
need to track and identify the responsible user in the presence of a violation
of the license agreement or any kind of malicious usage. Although there are
methods enabling Generative Adversarial Networks (GANs) to include invisible
watermarks in the images they produce, generating a model with a different
watermark, referred to as a fingerprint, for each user is time- and
resource-consuming due to the need to retrain the model to include the desired
fingerprint. In this paper, we propose a retraining-free GAN fingerprinting
method that allows model developers to easily generate model copies with the
same functionality but different fingerprints. The generator is modified by
inserting additional Personalized Normalization (PN) layers whose parameters
(scaling and bias) are generated by two dedicated shallow networks (ParamGen
Nets) taking the fingerprint as input. A watermark decoder is trained
simultaneously to extract the fingerprint from the generated images. The
proposed method can embed different fingerprints inside the GAN by just
changing the input of the ParamGen Nets and performing a feedforward pass,
without finetuning or retraining. The performance of the proposed method in
terms of robustness against both model-level and image-level attacks is also
superior to the state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_J/0/1/0/all/0/1&quot;&gt;Jianwei Fei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Z/0/1/0/all/0/1&quot;&gt;Zhihua Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tondi_B/0/1/0/all/0/1&quot;&gt;Benedetta Tondi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barni_M/0/1/0/all/0/1&quot;&gt;Mauro Barni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05479">
<title>Retinal OCT Synthesis with Denoising Diffusion Probabilistic Models for Layer Segmentation. (arXiv:2311.05479v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.05479</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern biomedical image analysis using deep learning often encounters the
challenge of limited annotated data. To overcome this issue, deep generative
models can be employed to synthesize realistic biomedical images. In this
regard, we propose an image synthesis method that utilizes denoising diffusion
probabilistic models (DDPMs) to automatically generate retinal optical
coherence tomography (OCT) images. By providing rough layer sketches, the
trained DDPMs can generate realistic circumpapillary OCT images. We further
find that more accurate pseudo labels can be obtained through knowledge
adaptation, which greatly benefits the segmentation task. Through this, we
observe a consistent improvement in layer segmentation accuracy, which is
validated using various neural networks. Furthermore, we have discovered that a
layer segmentation model trained solely with synthesized images can achieve
comparable results to a model trained exclusively with real images. These
findings demonstrate the promising potential of DDPMs in reducing the need for
manual annotations of retinal OCT images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yuli Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+He_W/0/1/0/all/0/1&quot;&gt;Weidong He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Eschweiler_D/0/1/0/all/0/1&quot;&gt;Dennis Eschweiler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dou_N/0/1/0/all/0/1&quot;&gt;Ningxin Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fan_Z/0/1/0/all/0/1&quot;&gt;Zixin Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mi_S/0/1/0/all/0/1&quot;&gt;Shengli Mi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Walter_P/0/1/0/all/0/1&quot;&gt;Peter Walter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Stegmaier_J/0/1/0/all/0/1&quot;&gt;Johannes Stegmaier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05494">
<title>Object-centric Cross-modal Feature Distillation for Event-based Object Detection. (arXiv:2311.05494v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05494</link>
<description rdf:parseType="Literal">&lt;p&gt;Event cameras are gaining popularity due to their unique properties, such as
their low latency and high dynamic range. One task where these benefits can be
crucial is real-time object detection. However, RGB detectors still outperform
event-based detectors due to the sparsity of the event data and missing visual
details. In this paper, we develop a novel knowledge distillation approach to
shrink the performance gap between these two modalities. To this end, we
propose a cross-modality object detection distillation method that by design
can focus on regions where the knowledge distillation works best. We achieve
this by using an object-centric slot attention mechanism that can iteratively
decouple features maps into object-centric features and corresponding
pixel-features used for distillation. We evaluate our novel distillation
approach on a synthetic and a real event dataset with aligned grayscale images
as a teacher modality. We show that object-centric distillation allows to
significantly improve the performance of the event-based student object
detector, nearly halving the performance gap with respect to the teacher.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liniger_A/0/1/0/all/0/1&quot;&gt;Alexander Liniger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Millhaeusler_M/0/1/0/all/0/1&quot;&gt;Mario Millhaeusler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsiminaki_V/0/1/0/all/0/1&quot;&gt;Vagia Tsiminaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanyou Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1&quot;&gt;Dengxin Dai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05521">
<title>BakedAvatar: Baking Neural Fields for Real-Time Head Avatar Synthesis. (arXiv:2311.05521v1 [cs.GR])</title>
<link>http://arxiv.org/abs/2311.05521</link>
<description rdf:parseType="Literal">&lt;p&gt;Synthesizing photorealistic 4D human head avatars from videos is essential
for VR/AR, telepresence, and video game applications. Although existing Neural
Radiance Fields (NeRF)-based methods achieve high-fidelity results, the
computational expense limits their use in real-time applications. To overcome
this limitation, we introduce BakedAvatar, a novel representation for real-time
neural head avatar synthesis, deployable in a standard polygon rasterization
pipeline. Our approach extracts deformable multi-layer meshes from learned
isosurfaces of the head and computes expression-, pose-, and view-dependent
appearances that can be baked into static textures for efficient rasterization.
We thus propose a three-stage pipeline for neural head avatar synthesis, which
includes learning continuous deformation, manifold, and radiance fields,
extracting layered meshes and textures, and fine-tuning texture details with
differential rasterization. Experimental results demonstrate that our
representation generates synthesis results of comparable quality to other
state-of-the-art methods while significantly reducing the inference time
required. We further showcase various head avatar synthesis results from
monocular videos, including view synthesis, face reenactment, expression
editing, and pose editing, all at interactive frame rates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_H/0/1/0/all/0/1&quot;&gt;Hao-Bin Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Miao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jin-Chuan Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xu-Chuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yan-Pei Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05524">
<title>SeaTurtleID2022: A long-span dataset for reliable sea turtle re-identification. (arXiv:2311.05524v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05524</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces the first public large-scale, long-span dataset with
sea turtle photographs captured in the wild -- SeaTurtleID2022
(https://www.kaggle.com/datasets/wildlifedatasets/seaturtleid2022). The dataset
contains 8729 photographs of 438 unique individuals collected within 13 years,
making it the longest-spanned dataset for animal re-identification. All
photographs include various annotations, e.g., identity, encounter timestamp,
and body parts segmentation masks. Instead of standard &quot;random&quot; splits, the
dataset allows for two realistic and ecologically motivated splits: (i) a
time-aware closed-set with training, validation, and test data from different
days/years, and (ii) a time-aware open-set with new unknown individuals in test
and validation sets. We show that time-aware splits are essential for
benchmarking re-identification methods, as random splits lead to performance
overestimation. Furthermore, a baseline instance segmentation and
re-identification performance over various body parts is provided. Finally, an
end-to-end system for sea turtle re-identification is proposed and evaluated.
The proposed system based on Hybrid Task Cascade for head instance segmentation
and ArcFace-trained feature-extractor achieved an accuracy of 86.8%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adam_L/0/1/0/all/0/1&quot;&gt;Luk&amp;#xe1;&amp;#x161; Adam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cermak_V/0/1/0/all/0/1&quot;&gt;Vojt&amp;#x11b;ch &amp;#x10c;erm&amp;#xe1;k&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papafitsoros_K/0/1/0/all/0/1&quot;&gt;Kostas Papafitsoros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Picek_L/0/1/0/all/0/1&quot;&gt;Luk&amp;#xe1;&amp;#x161; Picek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05538">
<title>Embedding Space Interpolation Beyond Mini-Batch, Beyond Pairs and Beyond Examples. (arXiv:2311.05538v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.05538</link>
<description rdf:parseType="Literal">&lt;p&gt;Mixup refers to interpolation-based data augmentation, originally motivated
as a way to go beyond empirical risk minimization (ERM). Its extensions mostly
focus on the definition of interpolation and the space (input or feature) where
it takes place, while the augmentation process itself is less studied. In most
methods, the number of generated examples is limited to the mini-batch size and
the number of examples being interpolated is limited to two (pairs), in the
input space.
&lt;/p&gt;
&lt;p&gt;We make progress in this direction by introducing MultiMix, which generates
an arbitrarily large number of interpolated examples beyond the mini-batch size
and interpolates the entire mini-batch in the embedding space. Effectively, we
sample on the entire convex hull of the mini-batch rather than along linear
segments between pairs of examples.
&lt;/p&gt;
&lt;p&gt;On sequence data, we further extend to Dense MultiMix. We densely interpolate
features and target labels at each spatial location and also apply the loss
densely. To mitigate the lack of dense labels, we inherit labels from examples
and weight interpolation factors by attention as a measure of confidence.
&lt;/p&gt;
&lt;p&gt;Overall, we increase the number of loss terms per mini-batch by orders of
magnitude at little additional cost. This is only possible because of
interpolating in the embedding space. We empirically show that our solutions
yield significant improvement over state-of-the-art mixup methods on four
different benchmarks, despite interpolation being only linear. By analyzing the
embedding space, we show that the classes are more tightly clustered and
uniformly spread over the embedding space, thereby explaining the improved
behavior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkataramanan_S/0/1/0/all/0/1&quot;&gt;Shashanka Venkataramanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kijak_E/0/1/0/all/0/1&quot;&gt;Ewa Kijak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amsaleg_L/0/1/0/all/0/1&quot;&gt;Laurent Amsaleg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Avrithis_Y/0/1/0/all/0/1&quot;&gt;Yannis Avrithis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05539">
<title>A Deep Learning Method for Simultaneous Denoising and Missing Wedge Reconstruction in Cryogenic Electron Tomography. (arXiv:2311.05539v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05539</link>
<description rdf:parseType="Literal">&lt;p&gt;Cryogenic electron tomography (cryo-ET) is a technique for imaging biological
samples such as viruses, cells, and proteins in 3D. A microscope collects a
series of 2D projections of the sample, and the goal is to reconstruct the 3D
density of the sample called the tomogram. This is difficult as the 2D
projections have a missing wedge of information and are noisy. Tomograms
reconstructed with conventional methods, such as filtered back-projection,
suffer from the noise, and from artifacts and anisotropic resolution due to the
missing wedge of information. To improve the visual quality and resolution of
such tomograms, we propose a deep-learning approach for simultaneous denoising
and missing wedge reconstruction called DeepDeWedge. DeepDeWedge is based on
fitting a neural network to the 2D projections with a self-supervised loss
inspired by noise2noise-like methods. The algorithm requires no training or
ground truth data. Experiments on synthetic and real cryo-ET data show that
DeepDeWedge achieves competitive performance for deep learning-based denoising
and missing wedge reconstruction of cryo-ET tomograms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiedemann_S/0/1/0/all/0/1&quot;&gt;Simon Wiedemann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heckel_R/0/1/0/all/0/1&quot;&gt;Reinhard Heckel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05548">
<title>L-WaveBlock: A Novel Feature Extractor Leveraging Wavelets for Generative Adversarial Networks. (arXiv:2311.05548v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05548</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Networks (GANs) have risen to prominence in the field
of deep learning, facilitating the generation of realistic data from random
noise. The effectiveness of GANs often depends on the quality of feature
extraction, a critical aspect of their architecture. This paper introduces
L-WaveBlock, a novel and robust feature extractor that leverages the
capabilities of the Discrete Wavelet Transform (DWT) with deep learning
methodologies. L-WaveBlock is catered to quicken the convergence of GAN
generators while simultaneously enhancing their performance. The paper
demonstrates the remarkable utility of L-WaveBlock across three datasets, a
road satellite imagery dataset, the CelebA dataset and the GoPro dataset,
showcasing its ability to ease feature extraction and make it more efficient.
By utilizing DWT, L-WaveBlock efficiently captures the intricate details of
both structural and textural details, and further partitions feature maps into
orthogonal subbands across multiple scales while preserving essential
information at the same time. Not only does it lead to faster convergence, but
also gives competent results on every dataset by employing the L-WaveBlock. The
proposed method achieves an Inception Score of 3.6959 and a Structural
Similarity Index of 0.4261 on the maps dataset, a Peak Signal-to-Noise Ratio of
29.05 and a Structural Similarity Index of 0.874 on the CelebA dataset. The
proposed method performs competently to the state-of-the-art for the image
denoising dataset, albeit not better, but still leads to faster convergence
than conventional methods. With this, L-WaveBlock emerges as a robust and
efficient tool for enhancing GAN-based image generation, demonstrating superior
convergence speed and competitive performance across multiple datasets for
image resolution, image generation and image denoising.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1&quot;&gt;Mirat Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1&quot;&gt;Vansh Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chokshi_A/0/1/0/all/0/1&quot;&gt;Anmol Chokshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parasnis_G/0/1/0/all/0/1&quot;&gt;Guruprasad Parasnis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bide_P/0/1/0/all/0/1&quot;&gt;Pramod Bide&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05556">
<title>LCM-LoRA: A Universal Stable-Diffusion Acceleration Module. (arXiv:2311.05556v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05556</link>
<description rdf:parseType="Literal">&lt;p&gt;Latent Consistency Models (LCMs) have achieved impressive performance in
accelerating text-to-image generative tasks, producing high-quality images with
minimal inference steps. LCMs are distilled from pre-trained latent diffusion
models (LDMs), requiring only ~32 A100 GPU training hours. This report further
extends LCMs&apos; potential in two aspects: First, by applying LoRA distillation to
Stable-Diffusion models including SD-V1.5, SSD-1B, and SDXL, we have expanded
LCM&apos;s scope to larger models with significantly less memory consumption,
achieving superior image generation quality. Second, we identify the LoRA
parameters obtained through LCM distillation as a universal Stable-Diffusion
acceleration module, named LCM-LoRA. LCM-LoRA can be directly plugged into
various Stable-Diffusion fine-tuned models or LoRAs without training, thus
representing a universally applicable accelerator for diverse image generation
tasks. Compared with previous numerical PF-ODE solvers such as DDIM,
DPM-Solver, LCM-LoRA can be viewed as a plug-in neural PF-ODE solver that
possesses strong generalization abilities. Project page:
https://github.com/luosiallen/latent-consistency-model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1&quot;&gt;Simian Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1&quot;&gt;Yiqin Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patil_S/0/1/0/all/0/1&quot;&gt;Suraj Patil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_D/0/1/0/all/0/1&quot;&gt;Daniel Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Platen_P/0/1/0/all/0/1&quot;&gt;Patrick von Platen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Passos_A/0/1/0/all/0/1&quot;&gt;Apolin&amp;#xe1;rio Passos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Longbo Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hang Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05559">
<title>Disentangling Quantum and Classical Contributions in Hybrid Quantum Machine Learning Architectures. (arXiv:2311.05559v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2311.05559</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantum computing offers the potential for superior computational
capabilities, particularly for data-intensive tasks. However, the current state
of quantum hardware puts heavy restrictions on input size. To address this,
hybrid transfer learning solutions have been developed, merging pre-trained
classical models, capable of handling extensive inputs, with variational
quantum circuits. Yet, it remains unclear how much each component - classical
and quantum - contributes to the model&apos;s results. We propose a novel hybrid
architecture: instead of utilizing a pre-trained network for compression, we
employ an autoencoder to derive a compressed version of the input data. This
compressed data is then channeled through the encoder part of the autoencoder
to the quantum component. We assess our model&apos;s classification capabilities
against two state-of-the-art hybrid transfer learning architectures, two purely
classical architectures and one quantum architecture. Their accuracy is
compared across four datasets: Banknote Authentication, Breast Cancer
Wisconsin, MNIST digits, and AudioMNIST. Our research suggests that classical
components significantly influence classification in hybrid transfer learning,
a contribution often mistakenly ascribed to the quantum element. The
performance of our model aligns with that of a variational quantum circuit
using amplitude embedding, positioning it as a feasible alternative.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Kolle_M/0/1/0/all/0/1&quot;&gt;Michael K&amp;#xf6;lle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Maurer_J/0/1/0/all/0/1&quot;&gt;Jonas Maurer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Altmann_P/0/1/0/all/0/1&quot;&gt;Philipp Altmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Sunkel_L/0/1/0/all/0/1&quot;&gt;Leo S&amp;#xfc;nkel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Stein_J/0/1/0/all/0/1&quot;&gt;Jonas Stein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Linnhoff_Popien_C/0/1/0/all/0/1&quot;&gt;Claudia Linnhoff-Popien&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05565">
<title>High-Performance Transformers for Table Structure Recognition Need Early Convolutions. (arXiv:2311.05565v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05565</link>
<description rdf:parseType="Literal">&lt;p&gt;Table structure recognition (TSR) aims to convert tabular images into a
machine-readable format, where a visual encoder extracts image features and a
textual decoder generates table-representing tokens. Existing approaches use
classic convolutional neural network (CNN) backbones for the visual encoder and
transformers for the textual decoder. However, this hybrid CNN-Transformer
architecture introduces a complex visual encoder that accounts for nearly half
of the total model parameters, markedly reduces both training and inference
speed, and hinders the potential for self-supervised learning in TSR. In this
work, we design a lightweight visual encoder for TSR without sacrificing
expressive power. We discover that a convolutional stem can match classic CNN
backbone performance, with a much simpler model. The convolutional stem strikes
an optimal balance between two crucial factors for high-performance TSR: a
higher receptive field (RF) ratio and a longer sequence length. This allows it
to &quot;see&quot; an appropriate portion of the table and &quot;store&quot; the complex table
structure within sufficient context length for the subsequent transformer. We
conducted reproducible ablation studies and open-sourced our code at
https://github.com/poloclub/tsr-convstem to enhance transparency, inspire
innovations, and facilitate fair comparisons in our domain as tables are a
promising modality for representation learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1&quot;&gt;ShengYun Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seongmin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaojing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balasubramaniyan_R/0/1/0/all/0/1&quot;&gt;Rajarajeswari Balasubramaniyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1&quot;&gt;Duen Horng Chau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05567">
<title>Exploring Emotion Expression Recognition in Older Adults Interacting with a Virtual Coach. (arXiv:2311.05567v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05567</link>
<description rdf:parseType="Literal">&lt;p&gt;The EMPATHIC project aimed to design an emotionally expressive virtual coach
capable of engaging healthy seniors to improve well-being and promote
independent aging. One of the core aspects of the system is its human sensing
capabilities, allowing for the perception of emotional states to provide a
personalized experience. This paper outlines the development of the emotion
expression recognition module of the virtual coach, encompassing data
collection, annotation design, and a first methodological approach, all
tailored to the project requirements. With the latter, we investigate the role
of various modalities, individually and combined, for discrete emotion
expression recognition in this context: speech from audio, and facial
expressions, gaze, and head dynamics from video. The collected corpus includes
users from Spain, France, and Norway, and was annotated separately for the
audio and video channels with distinct emotional labels, allowing for a
performance comparison across cultures and label types. Results confirm the
informative power of the modalities studied for the emotional categories
considered, with multimodal methods generally outperforming others (around 68%
accuracy with audio labels and 72-74% with video labels). The findings are
expected to contribute to the limited literature on emotion recognition applied
to older adults in conversational human-machine interaction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palmero_C/0/1/0/all/0/1&quot;&gt;Cristina Palmero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+deVelasco_M/0/1/0/all/0/1&quot;&gt;Mikel deVelasco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hmani_M/0/1/0/all/0/1&quot;&gt;Mohamed Amine Hmani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mtibaa_A/0/1/0/all/0/1&quot;&gt;Aymen Mtibaa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Letaifa_L/0/1/0/all/0/1&quot;&gt;Leila Ben Letaifa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buch_Cardona_P/0/1/0/all/0/1&quot;&gt;Pau Buch-Cardona&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Justo_R/0/1/0/all/0/1&quot;&gt;Raquel Justo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amorese_T/0/1/0/all/0/1&quot;&gt;Terry Amorese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_Fraile_E/0/1/0/all/0/1&quot;&gt;Eduardo Gonz&amp;#xe1;lez-Fraile&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_Ruanova_B/0/1/0/all/0/1&quot;&gt;Bego&amp;#xf1;a Fern&amp;#xe1;ndez-Ruanova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenorio_Laranga_J/0/1/0/all/0/1&quot;&gt;Jofre Tenorio-Laranga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johansen_A/0/1/0/all/0/1&quot;&gt;Anna Torp Johansen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_M/0/1/0/all/0/1&quot;&gt;Micaela Rodrigues da Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinussen_L/0/1/0/all/0/1&quot;&gt;Liva Jenny Martinussen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korsnes_M/0/1/0/all/0/1&quot;&gt;Maria Stylianou Korsnes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cordasco_G/0/1/0/all/0/1&quot;&gt;Gennaro Cordasco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Esposito_A/0/1/0/all/0/1&quot;&gt;Anna Esposito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+El_Yacoubi_M/0/1/0/all/0/1&quot;&gt;Mounim A. El-Yacoubi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petrovska_Delacretaz_D/0/1/0/all/0/1&quot;&gt;Dijana Petrovska-Delacr&amp;#xe9;taz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torres_M/0/1/0/all/0/1&quot;&gt;M. In&amp;#xe9;s Torres&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1&quot;&gt;Sergio Escalera&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05579">
<title>SigScatNet: A Siamese + Scattering based Deep Learning Approach for Signature Forgery Detection and Similarity Assessment. (arXiv:2311.05579v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05579</link>
<description rdf:parseType="Literal">&lt;p&gt;The surge in counterfeit signatures has inflicted widespread inconveniences
and formidable challenges for both individuals and organizations. This
groundbreaking research paper introduces SigScatNet, an innovative solution to
combat this issue by harnessing the potential of a Siamese deep learning
network, bolstered by Scattering wavelets, to detect signature forgery and
assess signature similarity. The Siamese Network empowers us to ascertain the
authenticity of signatures through a comprehensive similarity index, enabling
precise validation and comparison. Remarkably, the integration of Scattering
wavelets endows our model with exceptional efficiency, rendering it light
enough to operate seamlessly on cost-effective hardware systems. To validate
the efficacy of our approach, extensive experimentation was conducted on two
open-sourced datasets: the ICDAR SigComp Dutch dataset and the CEDAR dataset.
The experimental results demonstrate the practicality and resounding success of
our proposed SigScatNet, yielding an unparalleled Equal Error Rate of 3.689%
with the ICDAR SigComp Dutch dataset and an astonishing 0.0578% with the CEDAR
dataset. Through the implementation of SigScatNet, our research spearheads a
new state-of-the-art in signature analysis in terms of EER scores and
computational efficiency, offering an advanced and accessible solution for
detecting forgery and quantifying signature similarities. By employing
cutting-edge Siamese deep learning and Scattering wavelets, we provide a robust
framework that paves the way for secure and efficient signature verification
systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chokshi_A/0/1/0/all/0/1&quot;&gt;Anmol Chokshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1&quot;&gt;Vansh Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhope_R/0/1/0/all/0/1&quot;&gt;Rajas Bhope&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhage_S/0/1/0/all/0/1&quot;&gt;Sudhir Dhage&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05591">
<title>Accuracy of a Vision-Language Model on Challenging Medical Cases. (arXiv:2311.05591v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05591</link>
<description rdf:parseType="Literal">&lt;p&gt;Background: General-purpose large language models that utilize both text and
images have not been evaluated on a diverse array of challenging medical cases.
&lt;/p&gt;
&lt;p&gt;Methods: Using 934 cases from the NEJM Image Challenge published between 2005
and 2023, we evaluated the accuracy of the recently released Generative
Pre-trained Transformer 4 with Vision model (GPT-4V) compared to human
respondents overall and stratified by question difficulty, image type, and skin
tone. We further conducted a physician evaluation of GPT-4V on 69 NEJM
clinicopathological conferences (CPCs). Analyses were conducted for models
utilizing text alone, images alone, and both text and images.
&lt;/p&gt;
&lt;p&gt;Results: GPT-4V achieved an overall accuracy of 61% (95% CI, 58 to 64%)
compared to 49% (95% CI, 49 to 50%) for humans. GPT-4V outperformed humans at
all levels of difficulty and disagreement, skin tones, and image types; the
exception was radiographic images, where performance was equivalent between
GPT-4V and human respondents. Longer, more informative captions were associated
with improved performance for GPT-4V but similar performance for human
respondents. GPT-4V included the correct diagnosis in its differential for 80%
(95% CI, 68 to 88%) of CPCs when using text alone, compared to 58% (95% CI, 45
to 70%) of CPCs when using both images and text.
&lt;/p&gt;
&lt;p&gt;Conclusions: GPT-4V outperformed human respondents on challenging medical
cases and was able to synthesize information from both images and text, but
performance deteriorated when images were added to highly informative text.
Overall, our results suggest that multimodal AI models may be useful in medical
diagnostic reasoning but that their accuracy may depend heavily on context.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buckley_T/0/1/0/all/0/1&quot;&gt;Thomas Buckley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diao_J/0/1/0/all/0/1&quot;&gt;James A. Diao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodman_A/0/1/0/all/0/1&quot;&gt;Adam Rodman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manrai_A/0/1/0/all/0/1&quot;&gt;Arjun K. Manrai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05602">
<title>Reconstructing Objects in-the-wild for Realistic Sensor Simulation. (arXiv:2311.05602v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05602</link>
<description rdf:parseType="Literal">&lt;p&gt;Reconstructing objects from real world data and rendering them at novel views
is critical to bringing realism, diversity and scale to simulation for robotics
training and testing. In this work, we present NeuSim, a novel approach that
estimates accurate geometry and realistic appearance from sparse in-the-wild
data captured at distance and at limited viewpoints. Towards this goal, we
represent the object surface as a neural signed distance function and leverage
both LiDAR and camera sensor data to reconstruct smooth and accurate geometry
and normals. We model the object appearance with a robust physics-inspired
reflectance representation effective for in-the-wild data. Our experiments show
that NeuSim has strong view synthesis performance on challenging scenarios with
sparse training views. Furthermore, we showcase composing NeuSim assets into a
virtual world and generating realistic multi-sensor data for evaluating
self-driving perception models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Ze Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manivasagam_S/0/1/0/all/0/1&quot;&gt;Sivabalan Manivasagam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingkang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1&quot;&gt;Rui Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1&quot;&gt;Raquel Urtasun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05604">
<title>3D-QAE: Fully Quantum Auto-Encoding of 3D Point Clouds. (arXiv:2311.05604v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05604</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing methods for learning 3D representations are deep neural networks
trained and tested on classical hardware. Quantum machine learning
architectures, despite their theoretically predicted advantages in terms of
speed and the representational capacity, have so far not been considered for
this problem nor for tasks involving 3D data in general. This paper thus
introduces the first quantum auto-encoder for 3D point clouds. Our 3D-QAE
approach is fully quantum, i.e. all its data processing components are designed
for quantum hardware. It is trained on collections of 3D point clouds to
produce their compressed representations. Along with finding a suitable
architecture, the core challenges in designing such a fully quantum model
include 3D data normalisation and parameter optimisation, and we propose
solutions for both these tasks. Experiments on simulated gate-based quantum
hardware demonstrate that our method outperforms simple classical baselines,
paving the way for a new research direction in 3D computer vision. The source
code is available at https://4dqv.mpi-inf.mpg.de/QAE3D/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rathi_L/0/1/0/all/0/1&quot;&gt;Lakshika Rathi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tretschk_E/0/1/0/all/0/1&quot;&gt;Edith Tretschk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1&quot;&gt;Christian Theobalt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dabral_R/0/1/0/all/0/1&quot;&gt;Rishabh Dabral&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1&quot;&gt;Vladislav Golyanik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05607">
<title>Real-Time Neural Rasterization for Large Scenes. (arXiv:2311.05607v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05607</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new method for realistic real-time novel-view synthesis (NVS) of
large scenes. Existing neural rendering methods generate realistic results, but
primarily work for small scale scenes (&amp;lt;50 square meters) and have difficulty
at large scale (&amp;gt;10000 square meters). Traditional graphics-based rasterization
rendering is fast for large scenes but lacks realism and requires expensive
manually created assets. Our approach combines the best of both worlds by
taking a moderate-quality scaffold mesh as input and learning a neural texture
field and shader to model view-dependant effects to enhance realism, while
still using the standard graphics pipeline for real-time rendering. Our method
outperforms existing neural rendering methods, providing at least 30x faster
rendering with comparable or better realism for large self-driving and drone
scenes. Our work is the first to enable real-time rendering of large real-world
scenes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jeffrey Yunfan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Ze Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingkang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manivasagam_S/0/1/0/all/0/1&quot;&gt;Sivabalan Manivasagam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1&quot;&gt;Raquel Urtasun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05609">
<title>What Do I Hear? Generating Sounds for Visuals with ChatGPT. (arXiv:2311.05609v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2311.05609</link>
<description rdf:parseType="Literal">&lt;p&gt;This short paper introduces a workflow for generating realistic soundscapes
for visual media. In contrast to prior work, which primarily focus on matching
sounds for on-screen visuals, our approach extends to suggesting sounds that
may not be immediately visible but are essential to crafting a convincing and
immersive auditory environment. Our key insight is leveraging the reasoning
capabilities of language models, such as ChatGPT. In this paper, we describe
our workflow, which includes creating a scene context, brainstorming sounds,
and generating the sounds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;David Chuan-En Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martelaro_N/0/1/0/all/0/1&quot;&gt;Nikolas Martelaro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05613">
<title>Window Attention is Bugged: How not to Interpolate Position Embeddings. (arXiv:2311.05613v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.05613</link>
<description rdf:parseType="Literal">&lt;p&gt;Window attention, position embeddings, and high resolution finetuning are
core concepts in the modern transformer era of computer vision. However, we
find that naively combining these near ubiquitous components can have a
detrimental effect on performance. The issue is simple: interpolating position
embeddings while using window attention is wrong. We study two state-of-the-art
methods that have these three components, namely Hiera and ViTDet, and find
that both do indeed suffer from this bug. To fix it, we introduce a simple
absolute window position embedding strategy, which solves the bug outright in
Hiera and allows us to increase both speed and performance of the model in
ViTDet. We finally combine the two to obtain HieraDet, which achieves 61.7 box
mAP on COCO, making it state-of-the-art for models that only use ImageNet-1k
pretraining. This all stems from what is essentially a 3 line bug fix, which we
name &quot;absolute win&quot;.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bolya_D/0/1/0/all/0/1&quot;&gt;Daniel Bolya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryali_C/0/1/0/all/0/1&quot;&gt;Chaitanya Ryali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1&quot;&gt;Judy Hoffman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feichtenhofer_C/0/1/0/all/0/1&quot;&gt;Christoph Feichtenhofer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2012.04132">
<title>A Number Sense as an Emergent Property of the Manipulating Brain. (arXiv:2012.04132v3 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/2012.04132</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence (AI) systems struggle to generalize beyond their
training data and abstract general properties from the specifics of the
training examples. We propose a model that reproduces the apparent human
ability to come up with a number sense through unsupervised everyday
experience. The ability to understand and manipulate numbers and quantities
emerges during childhood, but the mechanism through which humans acquire and
develop this ability is still poorly understood. In particular, it is not known
whether acquiring such a number sense is possible without supervision from a
teacher. We explore this question through a model, assuming that the learner is
able to pick and place small objects and will spontaneously engage in
undirected manipulation. We assume that the learner&apos;s visual system will
monitor the changing arrangements of objects in the scene and will learn to
predict the effects of each action by comparing perception with the efferent
signal of the motor system. We model perception using standard deep networks
for feature extraction and classification. We find that, from learning the
unrelated task of action prediction, an unexpected image representation emerges
exhibiting regularities that foreshadow the perception and representation of
numbers. These include distinct categories for the first few natural numbers, a
strict ordering of the numbers, and a one-dimensional signal that correlates
with numerical quantity. As a result, our model acquires the ability to
estimate numerosity and subitize. Remarkably, subitization and numerosity
estimation extrapolate to scenes containing many objects, far beyond the three
objects used during training. We conclude that important aspects of a facility
with numbers and quantities may be learned without teacher supervision.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kondapaneni_N/0/1/0/all/0/1&quot;&gt;Neehar Kondapaneni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Perona_P/0/1/0/all/0/1&quot;&gt;Pietro Perona&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2106.08992">
<title>On the approximation capability of GNNs in node classification/regression tasks. (arXiv:2106.08992v6 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2106.08992</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) are a broad class of connectionist models for
graph processing. Recent studies have shown that GNNs can approximate any
function on graphs, modulo the equivalence relation on graphs defined by the
Weisfeiler--Lehman (WL) test. However, these results suffer from some
limitations, both because they were derived using the Stone--Weierstrass
theorem -- which is existential in nature, -- and because they assume that the
target function to be approximated must be continuous. Furthermore, all current
results are dedicated to graph classification/regression tasks, where the GNN
must produce a single output for the whole graph, while also node
classification/regression problems, in which an output is returned for each
node, are very common. In this paper, we propose an alternative way to
demonstrate the approximation capability of GNNs that overcomes these
limitations. Indeed, we show that GNNs are universal approximators in
probability for node classification/regression tasks, as they can approximate
any measurable function that satisfies the 1--WL equivalence on nodes. The
proposed theoretical framework allows the approximation of generic
discontinuous target functions and also suggests the GNN architecture that can
reach a desired approximation. In addition, we provide a bound on the number of
the GNN layers required to achieve the desired degree of approximation, namely
$2r-1$, where $r$ is the maximum number of nodes for the graphs in the domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DInverno_G/0/1/0/all/0/1&quot;&gt;Giuseppe Alessio D&amp;#x27;Inverno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bianchini_M/0/1/0/all/0/1&quot;&gt;Monica Bianchini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sampoli_M/0/1/0/all/0/1&quot;&gt;Maria Lucia Sampoli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scarselli_F/0/1/0/all/0/1&quot;&gt;Franco Scarselli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.01736">
<title>AdjointBackMapV2: Precise Reconstruction of Arbitrary CNN Unit&apos;s Activation via Adjoint Operators. (arXiv:2110.01736v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2110.01736</link>
<description rdf:parseType="Literal">&lt;p&gt;Adjoint operators have been found to be effective in the exploration of CNN&apos;s
inner workings [1]. However, the previous no-bias assumption restricted its
generalization. We overcome the restriction via embedding input images into an
extended normed space that includes bias in all CNN layers as part of the
extended space and propose an adjoint-operator-based algorithm that maps
high-level weights back to the extended input space for reconstructing an
effective hypersurface. Such hypersurface can be computed for an arbitrary unit
in the CNN, and we prove that this reconstructed hypersurface, when multiplied
by the original input (through an inner product), will precisely replicate the
output value of each unit. We show experimental results based on the CIFAR-10
and CIFAR-100 data sets where the proposed approach achieves near 0 activation
value reconstruction error.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_Q/0/1/0/all/0/1&quot;&gt;Qing Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheung_S/0/1/0/all/0/1&quot;&gt;Siu Wun Cheung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choe_Y/0/1/0/all/0/1&quot;&gt;Yoonsuck Choe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.07145">
<title>GAN-generated Faces Detection: A Survey and New Perspectives. (arXiv:2202.07145v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2202.07145</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Networks (GAN) have led to the generation of very
realistic face images, which have been used in fake social media accounts and
other disinformation matters that can generate profound impacts. Therefore, the
corresponding GAN-face detection techniques are under active development that
can examine and expose such fake faces. In this work, we aim to provide a
comprehensive review of recent progress in GAN-face detection. We focus on
methods that can detect face images that are generated or synthesized from GAN
models. We classify the existing detection works into four categories: (1) deep
learning-based, (2) physical-based, (3) physiological-based methods, and (4)
evaluation and comparison against human visual performance. For each category,
we summarize the key ideas and connect them with method implementations. We
also discuss open problems and suggest future research directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1&quot;&gt;Hui Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1&quot;&gt;Ming-Ching Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1&quot;&gt;Siwei Lyu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.09048">
<title>Global Contrast Masked Autoencoders Are Powerful Pathological Representation Learners. (arXiv:2205.09048v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2205.09048</link>
<description rdf:parseType="Literal">&lt;p&gt;Based on digital pathology slice scanning technology, artificial intelligence
algorithms represented by deep learning have achieved remarkable results in the
field of computational pathology. Compared to other medical images, pathology
images are more difficult to annotate, and thus, there is an extreme lack of
available datasets for conducting supervised learning to train robust deep
learning models. In this paper, we propose a self-supervised learning (SSL)
model, the global contrast-masked autoencoder (GCMAE), which can train the
encoder to have the ability to represent local-global features of pathological
images, also significantly improve the performance of transfer learning across
data sets. In this study, the ability of the GCMAE to learn migratable
representations was demonstrated through extensive experiments using a total of
three different disease-specific hematoxylin and eosin (HE)-stained pathology
datasets: Camelyon16, NCTCRC and BreakHis. In addition, this study designed an
effective automated pathology diagnosis process based on the GCMAE for clinical
applications. The source code of this paper is publicly available at
https://github.com/StarUniversus/gcmae.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Quan_H/0/1/0/all/0/1&quot;&gt;Hao Quan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xingyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weixing Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bai_Q/0/1/0/all/0/1&quot;&gt;Qun Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zou_M/0/1/0/all/0/1&quot;&gt;Mingchen Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_R/0/1/0/all/0/1&quot;&gt;Ruijie Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zheng_T/0/1/0/all/0/1&quot;&gt;Tingting Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qi_R/0/1/0/all/0/1&quot;&gt;Ruiqun Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xinghua Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cui_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Cui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.05954">
<title>Automatically Score Tissue Images Like a Pathologist by Transfer Learning. (arXiv:2209.05954v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.05954</link>
<description rdf:parseType="Literal">&lt;p&gt;Cancer is the second leading cause of death in the world. Diagnosing cancer
early on can save many lives. Pathologists have to look at tissue microarray
(TMA) images manually to identify tumors, which can be time-consuming,
inconsistent and subjective. Existing automatic algorithms either have not
achieved the accuracy level of a pathologist or require substantial human
involvements. A major challenge is that TMA images with different shapes,
sizes, and locations can have the same score. Learning staining patterns in TMA
images requires a huge number of images, which are severely limited due to
privacy and regulation concerns in medical organizations. TMA images from
different cancer types may share certain common characteristics, but combining
them directly harms the accuracy due to heterogeneity in their staining
patterns. Transfer learning is an emerging learning paradigm that allows
borrowing strength from similar problems. However, existing approaches
typically require a large sample from similar learning problems, while TMA
images of different cancer types are often available in small sample size and
further existing algorithms are limited to transfer learning from one similar
problem. We propose a new transfer learning algorithm that could learn from
multiple related problems, where each problem has a small sample and can have a
substantially different distribution from the original one. The proposed
algorithm has made it possible to break the critical accuracy barrier (the 75%
accuracy level of pathologists), with a reported accuracy of 75.9% on breast
cancer TMA images from the Stanford Tissue Microarray Database. It is supported
by recent developments in transfer learning theory and empirical evidence in
clustering technology. This will allow pathologists to confidently adopt
automatic algorithms in recognizing tumors consistently with a higher accuracy
in real time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_I/0/1/0/all/0/1&quot;&gt;Iris Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.12845">
<title>Super-resolution Reconstruction of Single Image for Latent features. (arXiv:2211.12845v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.12845</link>
<description rdf:parseType="Literal">&lt;p&gt;Single-image super-resolution (SISR) typically focuses on restoring various
degraded low-resolution (LR) images to a single high-resolution (HR) image.
However, during SISR tasks, it is often challenging for models to
simultaneously maintain high quality and rapid sampling while preserving
diversity in details and texture features. This challenge can lead to issues
such as model collapse, lack of rich details and texture features in the
reconstructed HR images, and excessive time consumption for model sampling. To
address these problems, this paper proposes a Latent Feature-oriented Diffusion
Probability Model (LDDPM). First, we designed a conditional encoder capable of
effectively encoding LR images, reducing the solution space for model image
reconstruction and thereby improving the quality of the reconstructed images.
We then employed a normalized flow and multimodal adversarial training,
learning from complex multimodal distributions, to model the denoising
distribution. Doing so boosts the generative modeling capabilities within a
minimal number of sampling steps. Experimental comparisons of our proposed
model with existing SISR methods on mainstream datasets demonstrate that our
model reconstructs more realistic HR images and achieves better performance on
multiple evaluation metrics, providing a fresh perspective for tackling SISR
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Jing-Ke Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cai_J/0/1/0/all/0/1&quot;&gt;Jing-Ye Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Deng_J/0/1/0/all/0/1&quot;&gt;Jian-Hua Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qin_Q/0/1/0/all/0/1&quot;&gt;Qin Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yao Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.03081">
<title>Automatic Diagnosis of Carotid Atherosclerosis Using a Portable Freehand 3D Ultrasound Imaging System. (arXiv:2301.03081v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.03081</link>
<description rdf:parseType="Literal">&lt;p&gt;The objective of this study is to develop a deep-learning based detection and
diagnosis technique for carotid atherosclerosis using a portable freehand 3D
ultrasound (US) imaging system. A total of 127 3D carotid artery scans were
acquired using a portable 3D US system which consisted of a handheld US scanner
and an electromagnetic tracking system. A U-Net segmentation network was
firstly applied to extract the carotid artery on 2D transverse frame, then a
novel 3D reconstruction algorithm using fast dot projection (FDP) method with
position regularization was proposed to reconstruct the carotid artery volume.
Furthermore, a convolutional neural network was used to classify healthy and
diseased cases qualitatively. 3D volume analysis methods including longitudinal
image acquisition and stenosis grade measurement were developed to obtain the
clinical metrics quantitatively. The proposed system achieved sensitivity of
0.714, specificity of 0.851 and accuracy of 0.803 respectively for diagnosis of
carotid atherosclerosis. The automatically measured stenosis grade illustrated
good correlation (r=0.762) with the experienced expert measurement. The
developed technique based on 3D US imaging can be applied to the automatic
diagnosis of carotid atherosclerosis. The proposed deep-learning based
technique was specially designed for a portable 3D freehand US system, which
can provide more convenient carotid atherosclerosis examination and decrease
the dependence on clinician&apos;s experience.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiawen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yunqian Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Sheng Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hongbo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Junni Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Duo Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haibin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Man Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zheng_R/0/1/0/all/0/1&quot;&gt;Rui Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.08067">
<title>Interpreting CNN Predictions using Conditional Generative Adversarial Networks. (arXiv:2301.08067v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.08067</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel method that trains a conditional Generative Adversarial
Network (GAN) to generate visual interpretations of a Convolutional Neural
Network (CNN). To comprehend a CNN, the GAN is trained with information on how
the CNN processes an image when making predictions. Supplying that information
has two main challenges: how to represent this information in a form that is
feedable to the GANs and how to effectively feed the representation to the GAN.
To address these issues, we developed a suitable representation of CNN
architectures by cumulatively averaging intermediate interpretation maps. We
also propose two alternative approaches to feed the representations to the GAN
and to choose an effective training strategy. Our approach learned the general
aspects of CNNs and was agnostic to datasets and CNN architectures. The study
includes both qualitative and quantitative evaluations and compares the
proposed GANs with state-of-the-art approaches. We found that the initial
layers of CNNs and final layers are equally crucial for interpreting CNNs upon
interpreting the proposed GAN. We believe training a GAN to interpret CNNs
would open doors for improved interpretations by leveraging fast-paced deep
learning advancements. The code used for experimentation is publicly available
at https://github.com/Akash-guna/Explain-CNN-With-GANS
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guna_R/0/1/0/all/0/1&quot;&gt;R T Akash Guna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benitez_R/0/1/0/all/0/1&quot;&gt;Raul Benitez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sikha_O/0/1/0/all/0/1&quot;&gt;O K Sikha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.01056">
<title>Beyond Pretrained Features: Noisy Image Modeling Provides Adversarial Defense. (arXiv:2302.01056v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.01056</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in masked image modeling (MIM) have made it a prevailing
framework for self-supervised visual representation learning. The MIM
pretrained models, like most deep neural network methods, remain vulnerable to
adversarial attacks, limiting their practical application, and this issue has
received little research attention. In this paper, we investigate how this
powerful self-supervised learning paradigm can provide adversarial robustness
to downstream classifiers. During the exploration, we find that noisy image
modeling (NIM), a simple variant of MIM that adopts denoising as the pre-text
task, reconstructs noisy images surprisingly well despite severe corruption.
Motivated by this observation, we propose an adversarial defense method,
referred to as De^3, by exploiting the pretrained decoder for denoising.
Through De^3, NIM is able to enhance adversarial robustness beyond providing
pretrained features. Furthermore, we incorporate a simple modification,
sampling the noise scale hyperparameter from random distributions, and enable
the defense to achieve a better and tunable trade-off between accuracy and
robustness. Experimental results demonstrate that, in terms of adversarial
robustness, NIM is superior to MIM thanks to its effective denoising
capability. Moreover, the defense provided by NIM achieves performance on par
with adversarial training while offering the extra tunability advantage. Source
code and models are available at https://github.com/youzunzhi/NIM-AdvDef.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_Z/0/1/0/all/0/1&quot;&gt;Zunzhi You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Daochang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1&quot;&gt;Bohyung Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chang Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.05828">
<title>Adapting Contrastive Language-Image Pretrained (CLIP) Models for Out-of-Distribution Detection. (arXiv:2303.05828v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.05828</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a comprehensive experimental study on pretrained feature
extractors for visual out-of-distribution (OOD) detection, focusing on adapting
contrastive language-image pretrained (CLIP) models. Without fine-tuning on the
training data, we are able to establish a positive correlation ($R^2\geq0.92$)
between in-distribution classification and unsupervised OOD detection for CLIP
models in $4$ benchmarks. We further propose a new simple and scalable method
called \textit{pseudo-label probing} (PLP) that adapts vision-language models
for OOD detection. Given a set of label names of the training set, PLP trains a
linear layer using the pseudo-labels derived from the text encoder of CLIP. To
test the OOD detection robustness of pretrained models, we develop a novel
feature-based adversarial OOD data manipulation approach to create adversarial
samples. Intriguingly, we show that (i) PLP outperforms the previous
state-of-the-art \citep{ming2022mcm} on all $5$ large-scale benchmarks based on
ImageNet, specifically by an average AUROC gain of 3.4\% using the largest CLIP
model (ViT-G), (ii) we show that linear probing outperforms fine-tuning by
large margins for CLIP architectures (i.e. CLIP ViT-H achieves a mean gain of
7.3\% AUROC on average on all ImageNet-based benchmarks), and (iii)
billion-parameter CLIP models still fail at detecting adversarially manipulated
OOD images. The code and adversarially created datasets will be made publicly
available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adaloglou_N/0/1/0/all/0/1&quot;&gt;Nikolas Adaloglou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michels_F/0/1/0/all/0/1&quot;&gt;Felix Michels&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaiser_T/0/1/0/all/0/1&quot;&gt;Tim Kaiser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kollmann_M/0/1/0/all/0/1&quot;&gt;Markus Kollmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.03251">
<title>SALUDA: Surface-based Automotive Lidar Unsupervised Domain Adaptation. (arXiv:2304.03251v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.03251</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning models on one labeled dataset that generalize well on another domain
is a difficult task, as several shifts might happen between the data domains.
This is notably the case for lidar data, for which models can exhibit large
performance discrepancies due for instance to different lidar patterns or
changes in acquisition conditions. This paper addresses the corresponding
Unsupervised Domain Adaptation (UDA) task for semantic segmentation. To
mitigate this problem, we introduce an unsupervised auxiliary task of learning
an implicit underlying surface representation simultaneously on source and
target data. As both domains share the same latent representation, the model is
forced to accommodate discrepancies between the two sources of data. This novel
strategy differs from classical minimization of statistical divergences or
lidar-specific domain adaptation techniques. Our experiments demonstrate that
our method achieves a better performance than the current state of the art,
both in real-to-real and synthetic-to-real scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michele_B/0/1/0/all/0/1&quot;&gt;Bjoern Michele&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boulch_A/0/1/0/all/0/1&quot;&gt;Alexandre Boulch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puy_G/0/1/0/all/0/1&quot;&gt;Gilles Puy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1&quot;&gt;Tuan-Hung Vu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marlet_R/0/1/0/all/0/1&quot;&gt;Renaud Marlet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Courty_N/0/1/0/all/0/1&quot;&gt;Nicolas Courty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.13000">
<title>Segment anything, from space?. (arXiv:2304.13000v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.13000</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the first foundation model developed specifically for image
segmentation tasks was developed, termed the &quot;Segment Anything Model&quot; (SAM).
SAM can segment objects in input imagery based on cheap input prompts, such as
one (or more) points, a bounding box, or a mask. The authors examined the
\textit{zero-shot} image segmentation accuracy of SAM on a large number of
vision benchmark tasks and found that SAM usually achieved recognition accuracy
similar to, or sometimes exceeding, vision models that had been trained on the
target tasks. The impressive generalization of SAM for segmentation has major
implications for vision researchers working on natural imagery. In this work,
we examine whether SAM&apos;s performance extends to overhead imagery problems and
help guide the community&apos;s response to its development. We examine SAM&apos;s
performance on a set of diverse and widely studied benchmark tasks. We find
that SAM does often generalize well to overhead imagery, although it fails in
some cases due to the unique characteristics of overhead imagery and its common
target objects. We report on these unique systematic failure cases for remote
sensing imagery that may comprise useful future research for the community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1&quot;&gt;Simiao Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luzi_F/0/1/0/all/0/1&quot;&gt;Francesco Luzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lahrichi_S/0/1/0/all/0/1&quot;&gt;Saad Lahrichi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kassaw_K/0/1/0/all/0/1&quot;&gt;Kaleb Kassaw&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Collins_L/0/1/0/all/0/1&quot;&gt;Leslie M. Collins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bradbury_K/0/1/0/all/0/1&quot;&gt;Kyle Bradbury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malof_J/0/1/0/all/0/1&quot;&gt;Jordan M. Malof&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.08491">
<title>Masked Collaborative Contrast for Weakly Supervised Semantic Segmentation. (arXiv:2305.08491v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.08491</link>
<description rdf:parseType="Literal">&lt;p&gt;This study introduces an efficacious approach, Masked Collaborative Contrast
(MCC), to highlight semantic regions in weakly supervised semantic
segmentation. MCC adroitly draws inspiration from masked image modeling and
contrastive learning to devise a novel framework that induces keys to contract
toward semantic regions. Unlike prevalent techniques that directly eradicate
patch regions in the input image when generating masks, we scrutinize the
neighborhood relations of patch tokens by exploring masks considering keys on
the affinity matrix. Moreover, we generate positive and negative samples in
contrastive learning by utilizing the masked local output and contrasting it
with the global output. Elaborate experiments on commonly employed datasets
evidences that the proposed MCC mechanism effectively aligns global and local
perspectives within the image, attaining impressive performance. The source
code is available at \url{https://github.com/fwu11/MCC}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Fangwen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jingxuan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1&quot;&gt;Yufei Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1&quot;&gt;Yanbin Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1&quot;&gt;Gang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1&quot;&gt;Lechao Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13903">
<title>Let&apos;s Think Frame by Frame with VIP: A Video Infilling and Prediction Dataset for Evaluating Video Chain-of-Thought. (arXiv:2305.13903v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13903</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite exciting recent results showing vision-language systems&apos; capacity to
reason about images using natural language, their capacity for video reasoning
remains under-explored. We motivate framing video reasoning as the sequential
understanding of a small number of keyframes, thereby leveraging the power and
robustness of vision-language while alleviating the computational complexities
of processing videos. To evaluate this novel application, we introduce VIP, an
inference-time challenge dataset designed to explore models&apos; reasoning
capabilities through video chain-of-thought. Inspired by visually descriptive
scene plays, we propose two formats for keyframe description: unstructured
dense captions and structured scene descriptions that identify the focus,
action, mood, objects, and setting (FAMOuS) of the keyframe. To evaluate video
reasoning, we propose two tasks: Video Infilling and Video Prediction, which
test abilities to generate multiple intermediate keyframes and predict future
keyframes, respectively. We benchmark GPT-4, GPT-3, and VICUNA on VIP,
demonstrate the performance gap in these complex video reasoning tasks, and
encourage future work to prioritize language models for efficient and
generalized video reasoning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Himakunthala_V/0/1/0/all/0/1&quot;&gt;Vaishnavi Himakunthala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_A/0/1/0/all/0/1&quot;&gt;Andy Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rose_D/0/1/0/all/0/1&quot;&gt;Daniel Rose&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1&quot;&gt;Ryan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_A/0/1/0/all/0/1&quot;&gt;Alex Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yujie Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sonar_C/0/1/0/all/0/1&quot;&gt;Chinmay Sonar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1&quot;&gt;Michael Saxon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;William Yang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14979">
<title>Assessment of the Reliablity of a Model&apos;s Decision by Generalizing Attribution to the Wavelet Domain. (arXiv:2305.14979v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14979</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks have shown remarkable performance in computer vision, but
their deployment in numerous scientific and technical fields is challenging due
to their black-box nature. Scientists and practitioners need to evaluate the
reliability of a decision, i.e., to know simultaneously if a model relies on
the relevant features and whether these features are robust to image
corruptions. Existing attribution methods aim to provide human-understandable
explanations by highlighting important regions in the image domain, but fail to
fully characterize a decision process&apos;s reliability. To bridge this gap, we
introduce the Wavelet sCale Attribution Method (WCAM), a generalization of
attribution from the pixel domain to the space-scale domain using wavelet
transforms. Attribution in the wavelet domain reveals where and on what scales
the model focuses, thus enabling us to assess whether a decision is reliable.
Our code is accessible here:
\url{https://github.com/gabrielkasmi/spectral-attribution}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasmi_G/0/1/0/all/0/1&quot;&gt;Gabriel Kasmi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubus_L/0/1/0/all/0/1&quot;&gt;Laurent Dubus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Drenan_Y/0/1/0/all/0/1&quot;&gt;Yves-Marie Saint Drenan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blanc_P/0/1/0/all/0/1&quot;&gt;Philippe Blanc&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15001">
<title>Contrastive Training of Complex-Valued Autoencoders for Object Discovery. (arXiv:2305.15001v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15001</link>
<description rdf:parseType="Literal">&lt;p&gt;Current state-of-the-art object-centric models use slots and attention-based
routing for binding. However, this class of models has several conceptual
limitations: the number of slots is hardwired; all slots have equal capacity;
training has high computational cost; there are no object-level relational
factors within slots. Synchrony-based models in principle can address these
limitations by using complex-valued activations which store binding information
in their phase components. However, working examples of such synchrony-based
models have been developed only very recently, and are still limited to toy
grayscale datasets and simultaneous storage of less than three objects in
practice. Here we introduce architectural modifications and a novel contrastive
learning method that greatly improve the state-of-the-art synchrony-based
model. For the first time, we obtain a class of synchrony-based models capable
of discovering objects in an unsupervised manner in multi-object color datasets
and simultaneously representing more than three objects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stanic_A/0/1/0/all/0/1&quot;&gt;Aleksandar Stani&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gopalakrishnan_A/0/1/0/all/0/1&quot;&gt;Anand Gopalakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Irie_K/0/1/0/all/0/1&quot;&gt;Kazuki Irie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1&quot;&gt;J&amp;#xfc;rgen Schmidhuber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.01289">
<title>nnMobileNe: Rethinking CNN for Retinopathy Research. (arXiv:2306.01289v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.01289</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the past few decades, convolutional neural networks (CNNs) have been at
the forefront of the detection and tracking of various retinal diseases (RD).
Despite their success, the emergence of vision transformers (ViT) in the 2020s
has shifted the trajectory of RD model development. The leading-edge
performance of ViT-based models in RD can be largely credited to their
scalability - their ability to improve as more parameters are added. As a
result, ViT-based models tend to outshine traditional CNNs in RD applications,
albeit at the cost of increased data and computational demands. ViTs also
differ from CNNs in their approach to processing images, working with patches
rather than local regions, which can complicate the precise identification of
small, variably presented lesions in RD. In our study, we revisited and updated
the architecture of a CNN model, specifically MobileNet, to enhance its utility
in RD diagnostics. We found that an optimized MobileNet, through selective
modifications, can surpass ViT-based models in various RD benchmarks, including
diabetic retinopathy grading, detection of multiple fundus diseases, and
classification of diabetic macular edema. Our software package is available at
https://github.com/Retinal-Research/NN-MOBILENET
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wenhui Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qiu_P/0/1/0/all/0/1&quot;&gt;Peijie Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lepore_N/0/1/0/all/0/1&quot;&gt;Natasha Lepore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dumitrascu_O/0/1/0/all/0/1&quot;&gt;Oana M. Dumitrascu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yalin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05527">
<title>Teaching AI to Teach: Leveraging Limited Human Salience Data Into Unlimited Saliency-Based Training. (arXiv:2306.05527v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05527</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning models have shown increased accuracy in classification tasks
when the training process incorporates human perceptual information. However, a
challenge in training human-guided models is the cost associated with
collecting image annotations for human salience. Collecting annotation data for
all images in a large training set can be prohibitively expensive. In this
work, we utilize &quot;teacher&quot; models (trained on a small amount of human-annotated
data) to annotate additional data by means of teacher models&apos; saliency maps.
Then, &quot;student&quot; models are trained using the larger amount of annotated
training data. This approach makes it possible to supplement a limited number
of human-supplied annotations with an arbitrarily large number of
model-generated image annotations. We compare the accuracy achieved by our
teacher-student training paradigm with (1) training using all available human
salience annotations, and (2) using all available training data without human
salience annotations. We use synthetic face detection and fake iris detection
as example challenging problems, and report results across four model
architectures (DenseNet, ResNet, Xception, and Inception), and two saliency
estimation methods (CAM and RISE). Results show that our teacher-student
training paradigm results in models that significantly exceed the performance
of both baselines, demonstrating that our approach can usefully leverage a
small amount of human annotations to generate salience maps for an arbitrary
amount of additional training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crum_C/0/1/0/all/0/1&quot;&gt;Colton R. Crum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boyd_A/0/1/0/all/0/1&quot;&gt;Aidan Boyd&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bowyer_K/0/1/0/all/0/1&quot;&gt;Kevin Bowyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Czajka_A/0/1/0/all/0/1&quot;&gt;Adam Czajka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07005">
<title>AI-Generated Image Detection using a Cross-Attention Enhanced Dual-Stream Network. (arXiv:2306.07005v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07005</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rapid evolution of AI Generated Content (AIGC), forged images
produced through this technology are inherently more deceptive and require less
human intervention compared to traditional Computer-generated Graphics (CG).
However, owing to the disparities between CG and AIGC, conventional CG
detection methods tend to be inadequate in identifying AIGC-produced images. To
address this issue, our research concentrates on the text-to-image generation
process in AIGC. Initially, we first assemble two text-to-image databases
utilizing two distinct AI systems, DALLE2 and DreamStudio. Aiming to
holistically capture the inherent anomalies produced by AIGC, we develope a
robust dual-stream network comprised of a residual stream and a content stream.
The former employs the Spatial Rich Model (SRM) to meticulously extract various
texture information from images, while the latter seeks to capture additional
forged traces in low frequency, thereby extracting complementary information
that the residual stream may overlook. To enhance the information exchange
between these two streams, we incorporate a cross multi-head attention
mechanism. Numerous comparative experiments are performed on both databases,
and the results show that our detection method consistently outperforms
traditional CG detection techniques across a range of image resolutions.
Moreover, our method exhibits superior performance through a series of
robustness tests and cross-database experiments. When applied to widely
recognized traditional CG benchmarks such as SPL2018 and DsTok, our approach
significantly exceeds the capabilities of other existing methods in the field
of CG detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1&quot;&gt;Ziyi Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wenmin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1&quot;&gt;Kangkang Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1&quot;&gt;Weiqi Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_P/0/1/0/all/0/1&quot;&gt;Peijia Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07915">
<title>Image Captioners Are Scalable Vision Learners Too. (arXiv:2306.07915v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07915</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive pretraining on image-text pairs from the web is one of the most
popular large-scale pretraining strategies for vision backbones, especially in
the context of large multimodal models. At the same time, image captioning on
this type of data is commonly considered an inferior pretraining strategy. In
this paper, we perform a fair comparison of these two pretraining strategies,
carefully matching training data, compute, and model capacity. Using a standard
encoder-decoder transformer, we find that captioning alone is surprisingly
effective: on classification tasks, captioning produces vision encoders
competitive with contrastively pretrained encoders, while surpassing them on
vision &amp;amp; language tasks. We further analyze the effect of the model
architecture and scale, as well as the pretraining data on the representation
quality, and find that captioning exhibits the same or better scaling behavior
along these axes. Overall our results show that plain image captioning is a
more powerful pretraining strategy than was previously believed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tschannen_M/0/1/0/all/0/1&quot;&gt;Michael Tschannen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1&quot;&gt;Manoj Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steiner_A/0/1/0/all/0/1&quot;&gt;Andreas Steiner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1&quot;&gt;Xiaohua Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1&quot;&gt;Neil Houlsby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1&quot;&gt;Lucas Beyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08013">
<title>TopP&amp;R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models. (arXiv:2306.08013v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08013</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a robust and reliable evaluation metric for generative models by
introducing topological and statistical treatments for rigorous support
estimation. Existing metrics, such as Inception Score (IS), Frechet Inception
Distance (FID), and the variants of Precision and Recall (P&amp;amp;R), heavily rely on
supports that are estimated from sample features. However, the reliability of
their estimation has not been seriously discussed (and overlooked) even though
the quality of the evaluation entirely depends on it. In this paper, we propose
Topological Precision and Recall (TopP&amp;amp;R, pronounced &apos;topper&apos;), which provides
a systematic approach to estimating supports, retaining only topologically and
statistically important features with a certain level of confidence. This not
only makes TopP&amp;amp;R strong for noisy features, but also provides statistical
consistency. Our theoretical and experimental results show that TopP&amp;amp;R is
robust to outliers and non-independent and identically distributed (Non-IID)
perturbations, while accurately capturing the true trend of change in samples.
To the best of our knowledge, this is the first evaluation metric focused on
the robust estimation of the support and provides its statistical consistency
under noise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_P/0/1/0/all/0/1&quot;&gt;Pum Jun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1&quot;&gt;Yoojin Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jisu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1&quot;&gt;Jaejun Yoo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.12929">
<title>Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing. (arXiv:2306.12929v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.12929</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer models have been widely adopted in various domains over the last
years, and especially large language models have advanced the field of AI
significantly. Due to their size, the capability of these networks has
increased tremendously, but this has come at the cost of a significant increase
in necessary compute. Quantization is one of the most effective ways to reduce
the computational time and memory consumption of neural networks. Many studies
have shown, however, that modern transformer models tend to learn strong
outliers in their activations, making them difficult to quantize. To retain
acceptable performance, the existence of these outliers requires activations to
be in higher bitwidth or the use of different numeric formats, extra
fine-tuning, or other workarounds. We show that strong outliers are related to
very specific behavior of attention heads that try to learn a &quot;no-op&quot; or just a
partial update of the residual. To achieve the exact zeros needed in the
attention matrix for a no-update, the input to the softmax is pushed to be
larger and larger during training, causing outliers in other parts of the
network. Based on these observations, we propose two simple (independent)
modifications to the attention mechanism - clipped softmax and gated attention.
We empirically show that models pre-trained using our methods learn
significantly smaller outliers while maintaining and sometimes even improving
the floating-point task performance. This enables us to quantize transformers
to full INT8 quantization of the activations without any additional effort. We
demonstrate the effectiveness of our methods on both language models (BERT,
OPT) and vision transformers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bondarenko_Y/0/1/0/all/0/1&quot;&gt;Yelysei Bondarenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagel_M/0/1/0/all/0/1&quot;&gt;Markus Nagel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blankevoort_T/0/1/0/all/0/1&quot;&gt;Tijmen Blankevoort&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00562">
<title>A MIL Approach for Anomaly Detection in Surveillance Videos from Multiple Camera Views. (arXiv:2307.00562v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00562</link>
<description rdf:parseType="Literal">&lt;p&gt;Occlusion and clutter are two scene states that make it difficult to detect
anomalies in surveillance video. Furthermore, anomaly events are rare and, as a
consequence, class imbalance and lack of labeled anomaly data are also key
features of this task. Therefore, weakly supervised methods are heavily
researched for this application. In this paper, we tackle these typical
problems of anomaly detection in surveillance video by combining Multiple
Instance Learning (MIL) to deal with the lack of labels and Multiple Camera
Views (MC) to reduce occlusion and clutter effects. In the resulting MC-MIL
algorithm we apply a multiple camera combined loss function to train a
regression network with Sultani&apos;s MIL ranking function. To evaluate the MC-MIL
algorithm first proposed here, the multiple camera PETS-2009 benchmark dataset
was re-labeled for the anomaly detection task from multiple camera views. The
result shows a significant performance improvement in F1 score compared to the
single-camera configuration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pereira_S/0/1/0/all/0/1&quot;&gt;Silas Santiago Lopes Pereira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maia_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Everardo Bessa Maia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11123">
<title>Hey That&apos;s Mine Imperceptible Watermarks are Preserved in Diffusion Generated Outputs. (arXiv:2308.11123v2 [cs.MM] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11123</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative models have seen an explosion in popularity with the release of
huge generative Diffusion models like Midjourney and Stable Diffusion to the
public. Because of this new ease of access, questions surrounding the automated
collection of data and issues regarding content ownership have started to
build. In this paper we present new work which aims to provide ways of
protecting content when shared to the public. We show that a generative
Diffusion model trained on data that has been imperceptibly watermarked will
generate new images with these watermarks present. We further show that if a
given watermark is correlated with a certain feature of the training data, the
generated images will also have this correlation. Using statistical tests we
show that we are able to determine whether a model has been trained on marked
data, and what data was marked. As a result our system offers a solution to
protect intellectual property when sharing content online.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ditria_L/0/1/0/all/0/1&quot;&gt;Luke Ditria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Drummond_T/0/1/0/all/0/1&quot;&gt;Tom Drummond&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.05528">
<title>On the detection of Out-Of-Distribution samples in Multiple Instance Learning. (arXiv:2309.05528v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.05528</link>
<description rdf:parseType="Literal">&lt;p&gt;The deployment of machine learning solutions in real-world scenarios often
involves addressing the challenge of out-of-distribution (OOD) detection. While
significant efforts have been devoted to OOD detection in classical supervised
settings, the context of weakly supervised learning, particularly the Multiple
Instance Learning (MIL) framework, remains under-explored. In this study, we
tackle this challenge by adapting post-hoc OOD detection methods to the MIL
setting while introducing a novel benchmark specifically designed to assess OOD
detection performance in weakly supervised scenarios. Across extensive
experiments based on diverse public datasets, KNN emerges as the
best-performing method overall. However, it exhibits significant shortcomings
on some datasets, emphasizing the complexity of this under-explored and
challenging topic. Our findings shed light on the complex nature of OOD
detection under the MIL framework, emphasizing the importance of developing
novel, robust, and reliable methods that can generalize effectively in a weakly
supervised context. The code for the paper is available here:
https://github.com/loic-lb/OOD_MIL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bescond_L/0/1/0/all/0/1&quot;&gt;Lo&amp;#xef;c Le Bescond&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vakalopoulou_M/0/1/0/all/0/1&quot;&gt;Maria Vakalopoulou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christodoulidis_S/0/1/0/all/0/1&quot;&gt;Stergios Christodoulidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andre_F/0/1/0/all/0/1&quot;&gt;Fabrice Andr&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talbot_H/0/1/0/all/0/1&quot;&gt;Hugues Talbot&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07096">
<title>The legibility of the imaged human brain. (arXiv:2309.07096v2 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/2309.07096</link>
<description rdf:parseType="Literal">&lt;p&gt;Our knowledge of the organisation of the human brain at the population-level
is yet to translate into power to predict functional differences at the
individual-level, limiting clinical applications, and casting doubt on the
generalisability of inferred mechanisms. It remains unknown whether the
difficulty arises from the absence of individuating biological patterns within
the brain, or from limited power to access them with the models and compute at
our disposal. Here we comprehensively investigate the resolvability of such
patterns with data and compute at unprecedented scale. Across 23810 unique
participants from UK Biobank, we systematically evaluate the predictability of
25 individual biological characteristics, from all available combinations of
structural and functional neuroimaging data. Over 4526 GPU*hours of
computation, we train, optimize, and evaluate out-of-sample 700 individual
predictive models, including multilayer perceptrons of demographic,
psychological, serological, chronic morbidity, and functional connectivity
characteristics, and both uni- and multi-modal 3D convolutional neural network
models of macro- and micro-structural brain imaging. We find a marked
discrepancy between the high predictability of sex (balanced accuracy 99.7%),
age (mean absolute error 2.048 years, R2 0.859), and weight (mean absolute
error 2.609Kg, R2 0.625), for which we set new state-of-the-art performance,
and the surprisingly low predictability of other characteristics. Neither
structural nor functional imaging predicted individual psychology better than
the coincidence of common chronic morbidity (p&amp;lt;0.05). Serology predicted common
morbidity (p&amp;lt;0.05) and was best predicted by it (p&amp;lt;0.001), followed by
structural neuroimaging (p&amp;lt;0.05). Our findings suggest either more informative
imaging or more powerful models will be needed to decipher individual level
characteristics from the brain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ruffle_J/0/1/0/all/0/1&quot;&gt;James K Ruffle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gray_R/0/1/0/all/0/1&quot;&gt;Robert J Gray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Mohinta_S/0/1/0/all/0/1&quot;&gt;Samia Mohinta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Pombo_G/0/1/0/all/0/1&quot;&gt;Guilherme Pombo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kaul_C/0/1/0/all/0/1&quot;&gt;Chaitanya Kaul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hyare_H/0/1/0/all/0/1&quot;&gt;Harpreet Hyare&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Rees_G/0/1/0/all/0/1&quot;&gt;Geraint Rees&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Nachev_P/0/1/0/all/0/1&quot;&gt;Parashkev Nachev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12214">
<title>Can We Reliably Improve the Robustness to Image Acquisition of Remote Sensing of PV Systems?. (arXiv:2309.12214v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.12214</link>
<description rdf:parseType="Literal">&lt;p&gt;Photovoltaic (PV) energy is crucial for the decarbonization of energy
systems. Due to the lack of centralized data, remote sensing of rooftop PV
installations is the best option to monitor the evolution of the rooftop PV
installed fleet at a regional scale. However, current techniques lack
reliability and are notably sensitive to shifts in the acquisition conditions.
To overcome this, we leverage the wavelet scale attribution method (WCAM),
which decomposes a model&apos;s prediction in the space-scale domain. The WCAM
enables us to assess on which scales the representation of a PV model rests and
provides insights to derive methods that improve the robustness to acquisition
conditions, thus increasing trust in deep learning systems to encourage their
use for the safe integration of clean energy in electric systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasmi_G/0/1/0/all/0/1&quot;&gt;Gabriel Kasmi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubus_L/0/1/0/all/0/1&quot;&gt;Laurent Dubus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saint_Drenan_Y/0/1/0/all/0/1&quot;&gt;Yves-Marie Saint-Drenan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blanc_P/0/1/0/all/0/1&quot;&gt;Philippe Blanc&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16393">
<title>HIC-YOLOv5: Improved YOLOv5 For Small Object Detection. (arXiv:2309.16393v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.16393</link>
<description rdf:parseType="Literal">&lt;p&gt;Small object detection has been a challenging problem in the field of object
detection. There has been some works that proposes improvements for this task,
such as adding several attention blocks or changing the whole structure of
feature fusion networks. However, the computation cost of these models is
large, which makes deploying a real-time object detection system unfeasible,
while leaving room for improvement. To this end, an improved YOLOv5 model:
HIC-YOLOv5 is proposed to address the aforementioned problems. Firstly, an
additional prediction head specific to small objects is added to provide a
higher-resolution feature map for better prediction. Secondly, an involution
block is adopted between the backbone and neck to increase channel information
of the feature map. Moreover, an attention mechanism named CBAM is applied at
the end of the backbone, thus not only decreasing the computation cost compared
with previous works but also emphasizing the important information in both
channel and spatial domain. Our result shows that HIC-YOLOv5 has improved
mAP@[.5:.95] by 6.42% and mAP@0.5 by 9.38% on VisDrone-2019-DET dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Shiyi Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yini Fang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07602">
<title>Dual Radar: A Multi-modal Dataset with Dual 4D Radar for Autonomous Driving. (arXiv:2310.07602v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07602</link>
<description rdf:parseType="Literal">&lt;p&gt;Radar has stronger adaptability in adverse scenarios for autonomous driving
environmental perception compared to widely adopted cameras and LiDARs.
Compared with commonly used 3D radars, the latest 4D radars have precise
vertical resolution and higher point cloud density, making it a highly
promising sensor for autonomous driving in complex environmental perception.
However, due to the much higher noise than LiDAR, manufacturers choose
different filtering strategies, resulting in an inverse ratio between noise
level and point cloud density. There is still a lack of comparative analysis on
which method is beneficial for deep learning-based perception algorithms in
autonomous driving. One of the main reasons is that current datasets only adopt
one type of 4D radar, making it difficult to compare different 4D radars in the
same scene. Therefore, in this paper, we introduce a novel large-scale
multi-modal dataset featuring, for the first time, two types of 4D radars
captured simultaneously. This dataset enables further research into effective
4D radar perception algorithms.Our dataset consists of 151 consecutive series,
most of which last 20 seconds and contain 10,007 meticulously synchronized and
annotated frames. Moreover, our dataset captures a variety of challenging
driving scenarios, including many road conditions, weather conditions,
nighttime and daytime with different lighting intensities and periods. Our
dataset annotates consecutive frames, which can be applied to 3D object
detection and tracking, and also supports the study of multi-modal tasks. We
experimentally validate our dataset, providing valuable results for studying
different types of 4D radars. This dataset is released on
https://github.com/adept-thu/Dual-Radar.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Li Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1&quot;&gt;Cheng Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Ziying Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guangqi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yichen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaofei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qingshan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhenlin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1&quot;&gt;Shuzhi Sam Ge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.09982">
<title>AP$n$P: A Less-constrained P$n$P Solver for Pose Estimation with Unknown Anisotropic Scaling or Focal Lengths. (arXiv:2310.09982v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.09982</link>
<description rdf:parseType="Literal">&lt;p&gt;Perspective-$n$-Point (P$n$P) stands as a fundamental algorithm for pose
estimation in various applications. In this paper, we present a new approach to
the P$n$P problem with relaxed constraints, eliminating the need for precise 3D
coordinates or complete calibration data. We refer to it as AP$n$P due to its
ability to handle unknown anisotropic scaling factors of 3D coordinates or
alternatively two distinct focal lengths in addition to the conventional rigid
transformation. Through algebraic manipulations and a novel parametrization,
both cases are brought into similar forms that distinguish themselves primarily
by the order of a rotation and an anisotropic scaling operation. AP$n$P then
boils down to one unique polynomial problem, which is solved by the Gr\&quot;obner
basis approach. Experimental results on both simulated and real datasets
demonstrate the effectiveness of AP$n$P as a more flexible and practical
solution to camera pose estimation. Code: https://github.com/goldoak/APnP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1&quot;&gt;Jiaxin Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leutenegger_S/0/1/0/all/0/1&quot;&gt;Stefan Leutenegger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kneip_L/0/1/0/all/0/1&quot;&gt;Laurent Kneip&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15008">
<title>Wonder3D: Single Image to 3D using Cross-Domain Diffusion. (arXiv:2310.15008v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15008</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we introduce Wonder3D, a novel method for efficiently
generating high-fidelity textured meshes from single-view images.Recent methods
based on Score Distillation Sampling (SDS) have shown the potential to recover
3D geometry from 2D diffusion priors, but they typically suffer from
time-consuming per-shape optimization and inconsistent geometry. In contrast,
certain works directly produce 3D information via fast network inferences, but
their results are often of low quality and lack geometric details. To
holistically improve the quality, consistency, and efficiency of image-to-3D
tasks, we propose a cross-domain diffusion model that generates multi-view
normal maps and the corresponding color images. To ensure consistency, we
employ a multi-view cross-domain attention mechanism that facilitates
information exchange across views and modalities. Lastly, we introduce a
geometry-aware normal fusion algorithm that extracts high-quality surfaces from
the multi-view 2D representations. Our extensive evaluations demonstrate that
our method achieves high-quality reconstruction results, robust generalization,
and reasonably good efficiency compared to prior works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yuan-Chen Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Cheng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1&quot;&gt;Zhiyang Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lingjie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yuexin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Song-Hai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Habermann_M/0/1/0/all/0/1&quot;&gt;Marc Habermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1&quot;&gt;Christian Theobalt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenping Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15767">
<title>Unpaired MRI Super Resolution with Self-Supervised Contrastive Learning. (arXiv:2310.15767v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15767</link>
<description rdf:parseType="Literal">&lt;p&gt;High-resolution (HR) magnetic resonance imaging (MRI) is crucial for
enhancing diagnostic accuracy in clinical settings. Nonetheless, the inherent
limitation of MRI resolution restricts its widespread applicability. Deep
learning-based image super-resolution (SR) methods exhibit promise in improving
MRI resolution without additional cost. However, these methods frequently
require a substantial number of HR MRI images for training, which can be
challenging to acquire. In this paper, we propose an unpaired MRI SR approach
that employs self-supervised contrastive learning to enhance SR performance
with limited training data. Our approach leverages both authentic HR images and
synthetically generated SR images to construct positive and negative sample
pairs, thus facilitating the learning of discriminative features. Empirical
results presented in this study underscore significant enhancements in the peak
signal-to-noise ratio and structural similarity index, even when a paucity of
HR images is available. These findings accentuate the potential of our approach
in addressing the challenge of limited training data, thereby contributing to
the advancement of high-resolution MRI in clinical applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Quanwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jianan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiling Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yanni Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lv_Z/0/1/0/all/0/1&quot;&gt;Zhihan Lv&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19630">
<title>Convolutional Neural Networks for Automatic Detection of Intact Adenovirus from TEM Imaging with Debris, Broken and Artefacts Particles. (arXiv:2310.19630v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19630</link>
<description rdf:parseType="Literal">&lt;p&gt;Regular monitoring of the primary particles and purity profiles of a drug
product during development and manufacturing processes is essential for
manufacturers to avoid product variability and contamination. Transmission
electron microscopy (TEM) imaging helps manufacturers predict how changes
affect particle characteristics and purity for virus-based gene therapy vector
products and intermediates. Since intact particles can characterize efficacious
products, it is beneficial to automate the detection of intact adenovirus
against a non-intact-viral background mixed with debris, broken, and artefact
particles. In the presence of such particles, detecting intact adenoviruses
becomes more challenging. To overcome the challenge, due to such a presence, we
developed a software tool for semi-automatic annotation and segmentation of
adenoviruses and a software tool for automatic segmentation and detection of
intact adenoviruses in TEM imaging systems. The developed semi-automatic tool
exploited conventional image analysis techniques while the automatic tool was
built based on convolutional neural networks and image analysis techniques. Our
quantitative and qualitative evaluations showed outstanding true positive
detection rates compared to false positive and negative rates where
adenoviruses were nicely detected without mistaking them for real debris,
broken adenoviruses, and/or staining artefacts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rukundo_O/0/1/0/all/0/1&quot;&gt;Olivier Rukundo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Behanova_A/0/1/0/all/0/1&quot;&gt;Andrea Behanova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feo_R/0/1/0/all/0/1&quot;&gt;Riccardo De Feo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ronkko_S/0/1/0/all/0/1&quot;&gt;Seppo Ronkko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oja_J/0/1/0/all/0/1&quot;&gt;Joni Oja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tohka_J/0/1/0/all/0/1&quot;&gt;Jussi Tohka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.20190">
<title>Visible to Thermal image Translation for improving visual task in low light conditions. (arXiv:2310.20190v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.20190</link>
<description rdf:parseType="Literal">&lt;p&gt;Several visual tasks, such as pedestrian detection and image-to-image
translation, are challenging to accomplish in low light using RGB images. Heat
variation of objects in thermal images can be used to overcome this. In this
work, an end-to-end framework, which consists of a generative network and a
detector network, is proposed to translate RGB image into Thermal ones and
compare generated thermal images with real data. We have collected images from
two different locations using the Parrot Anafi Thermal drone. After that, we
created a two-stream network, preprocessed, augmented, the image data, and
trained the generator and discriminator models from scratch. The findings
demonstrate that it is feasible to translate RGB training data to thermal data
using GAN. As a result, thermal data can now be produced more quickly and
affordably, which is useful for security and surveillance applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Md Azim Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02510">
<title>Anthropomorphic Grasping with Neural Object Shape Completion. (arXiv:2311.02510v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02510</link>
<description rdf:parseType="Literal">&lt;p&gt;The progressive prevalence of robots in human-suited environments has given
rise to a myriad of object manipulation techniques, in which dexterity plays a
paramount role. It is well-established that humans exhibit extraordinary
dexterity when handling objects. Such dexterity seems to derive from a robust
understanding of object properties (such as weight, size, and shape), as well
as a remarkable capacity to interact with them. Hand postures commonly
demonstrate the influence of specific regions on objects that need to be
grasped, especially when objects are partially visible. In this work, we
leverage human-like object understanding by reconstructing and completing their
full geometry from partial observations, and manipulating them using a 7-DoF
anthropomorphic robot hand. Our approach has significantly improved the
grasping success rates of baselines with only partial reconstruction by nearly
30% and achieved over 150 successful grasps with three different object
categories. This demonstrates our approach&apos;s consistent ability to predict and
execute grasping postures based on the completed object shapes from various
directions and positions in real-world scenarios. Our work opens up new
possibilities for enhancing robotic applications that require precise grasping
and manipulation skills of real-world reconstructed objects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hidalgo_Carvajal_D/0/1/0/all/0/1&quot;&gt;Diego Hidalgo-Carvajal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hanzhi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bettelani_G/0/1/0/all/0/1&quot;&gt;Gemma C. Bettelani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1&quot;&gt;Jaesug Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zavaglia_M/0/1/0/all/0/1&quot;&gt;Melissa Zavaglia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Busse_L/0/1/0/all/0/1&quot;&gt;Laura Busse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naceri_A/0/1/0/all/0/1&quot;&gt;Abdeldjallil Naceri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leutenegger_S/0/1/0/all/0/1&quot;&gt;Stefan Leutenegger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haddadin_S/0/1/0/all/0/1&quot;&gt;Sami Haddadin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02831">
<title>SemanticTopoLoop: Semantic Loop Closure With 3D Topological Graph Based on Quadric-Level Object Map. (arXiv:2311.02831v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02831</link>
<description rdf:parseType="Literal">&lt;p&gt;Loop closure, as one of the crucial components in SLAM, plays an essential
role in correcting the accumulated errors. Traditional appearance-based
methods, such as bag-of-words models, are often limited by local 2D features
and the volume of training data, making them less versatile and robust in
real-world scenarios, leading to missed detections or false positives
detections in loop closure. To address these issues, we first propose a
object-level data association method based on multi-level verification, which
can associate 2D semantic features of current frame with 3D objects landmarks
of map. Next, taking advantage of these association relations, we introduce a
semantic loop closure method based on quadric-level object map topology, which
represents scenes through the topological graph of objects and achieves
accurate loop closure at a wide field of view by comparing differences in the
topological graphs. Finally, we integrate these two methods into a complete
object-aware SLAM system. Qualitative experiments and ablation studies
demonstrate the effectiveness and robustness of the proposed object-level data
association algorithm. Quantitative experiments show that our semantic loop
closure method outperforms existing state-of-the-art methods in terms of
precision, recall and localization accuracy metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1&quot;&gt;Zhenzhong Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03312">
<title>A Single 2D Pose with Context is Worth Hundreds for 3D Human Pose Estimation. (arXiv:2311.03312v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03312</link>
<description rdf:parseType="Literal">&lt;p&gt;The dominant paradigm in 3D human pose estimation that lifts a 2D pose
sequence to 3D heavily relies on long-term temporal clues (i.e., using a
daunting number of video frames) for improved accuracy, which incurs
performance saturation, intractable computation and the non-causal problem.
This can be attributed to their inherent inability to perceive spatial context
as plain 2D joint coordinates carry no visual cues. To address this issue, we
propose a straightforward yet powerful solution: leveraging the readily
available intermediate visual representations produced by off-the-shelf
(pre-trained) 2D pose detectors -- no finetuning on the 3D task is even needed.
The key observation is that, while the pose detector learns to localize 2D
joints, such representations (e.g., feature maps) implicitly encode the
joint-centric spatial context thanks to the regional operations in backbone
networks. We design a simple baseline named Context-Aware PoseFormer to
showcase its effectiveness. Without access to any temporal information, the
proposed method significantly outperforms its context-agnostic counterpart,
PoseFormer, and other state-of-the-art methods using up to hundreds of video
frames regarding both speed and precision. Project page:
https://qitaozhao.github.io/ContextAware-PoseFormer
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qitao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Ce Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mengyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03828">
<title>Multi-view Information Integration and Propagation for Occluded Person Re-identification. (arXiv:2311.03828v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03828</link>
<description rdf:parseType="Literal">&lt;p&gt;Occluded person re-identification (re-ID) presents a challenging task due to
occlusion perturbations. Although great efforts have been made to prevent the
model from being disturbed by occlusion noise, most current solutions only
capture information from a single image, disregarding the rich complementary
information available in multiple images depicting the same pedestrian. In this
paper, we propose a novel framework called Multi-view Information Integration
and Propagation (MVI$^{2}$P). Specifically, realizing the potential of
multi-view images in effectively characterizing the occluded target pedestrian,
we integrate feature maps of which to create a comprehensive representation.
During this process, to avoid introducing occlusion noise, we develop a
CAMs-aware Localization module that selectively integrates information
contributing to the identification. Additionally, considering the divergence in
the discriminative nature of different images, we design a probability-aware
Quantification module to emphatically integrate highly reliable information.
Moreover, as multiple images with the same identity are not accessible in the
testing stage, we devise an Information Propagation (IP) mechanism to distill
knowledge from the comprehensive representation to that of a single occluded
image. Extensive experiments and analyses have unequivocally demonstrated the
effectiveness and superiority of the proposed MVI$^{2}$P. The code will be
released at \url{https://github.com/nengdong96/MVIIP}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1&quot;&gt;Neng Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1&quot;&gt;Shuanglin Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Hao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jinhui Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Liyan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04458">
<title>Retargeting video with an end-to-end framework. (arXiv:2311.04458v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.04458</link>
<description rdf:parseType="Literal">&lt;p&gt;Video holds significance in computer graphics applications. Because of the
heterogeneous of digital devices, retargeting videos becomes an essential
function to enhance user viewing experience in such applications. In the
research of video retargeting, preserving the relevant visual content in
videos, avoiding flicking, and processing time are the vital challenges.
Extending image retargeting techniques to the video domain is challenging due
to the high running time. Prior work of video retargeting mainly utilizes
time-consuming preprocessing to analyze frames. Plus, being tolerant of
different video content, avoiding important objects from shrinking, and the
ability to play with arbitrary ratios are the limitations that need to be
resolved in these systems requiring investigation. In this paper, we present an
end-to-end RETVI method to retarget videos to arbitrary aspect ratios. We
eliminate the computational bottleneck in the conventional approaches by
designing RETVI with two modules, content feature analyzer (CFA) and adaptive
deforming estimator (ADE). The extensive experiments and evaluations show that
our system outperforms previous work in quality and running time. Visit our
project website for more results at &lt;a href=&quot;http://graphics.csie.ncku.edu.tw/RETVI.&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1&quot;&gt;Thi-Ngoc-Hanh Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;HuiGuang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yi-Ru Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1&quot;&gt;Tong-Yee Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04464">
<title>Enhancing Few-shot CLIP with Semantic-Aware Fine-Tuning. (arXiv:2311.04464v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.04464</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning generalized representations from limited training samples is crucial
for applying deep neural networks in low-resource scenarios. Recently, methods
based on Contrastive Language-Image Pre-training (CLIP) have exhibited
promising performance in few-shot adaptation tasks. To avoid catastrophic
forgetting and overfitting caused by few-shot fine-tuning, existing works
usually freeze the parameters of CLIP pre-trained on large-scale datasets,
overlooking the possibility that some parameters might not be suitable for
downstream tasks. To this end, we revisit CLIP&apos;s visual encoder with a specific
focus on its distinctive attention pooling layer, which performs a spatial
weighted-sum of the dense feature maps. Given that dense feature maps contain
meaningful semantic information, and different semantics hold varying
importance for diverse downstream tasks (such as prioritizing semantics like
ears and eyes in pet classification tasks rather than side mirrors), using the
same weighted-sum operation for dense features across different few-shot tasks
might not be appropriate. Hence, we propose fine-tuning the parameters of the
attention pooling layer during the training process to encourage the model to
focus on task-specific semantics. In the inference process, we perform residual
blending between the features pooled by the fine-tuned and the original
attention pooling layers to incorporate both the few-shot knowledge and the
pre-trained CLIP&apos;s prior knowledge. We term this method as Semantic-Aware
FinE-tuning (SAFE). SAFE is effective in enhancing the conventional few-shot
CLIP and is compatible with the existing adapter approach (termed SAFE-A).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuefeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1&quot;&gt;Xiu Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhigang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+lu_W/0/1/0/all/0/1&quot;&gt;Wang lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jindong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1&quot;&gt;Xiangyang Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04591">
<title>Rethinking Human Pose Estimation for Autonomous Driving with 3D Event Representations. (arXiv:2311.04591v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.04591</link>
<description rdf:parseType="Literal">&lt;p&gt;Human pose estimation is a critical component in autonomous driving and
parking, enhancing safety by predicting human actions. Traditional frame-based
cameras and videos are commonly applied, yet, they become less reliable in
scenarios under high dynamic range or heavy motion blur. In contrast, event
cameras offer a robust solution for navigating these challenging contexts.
Predominant methodologies incorporate event cameras into learning frameworks by
accumulating events into event frames. However, such methods tend to
marginalize the intrinsic asynchronous and high temporal resolution
characteristics of events. This disregard leads to a loss in essential temporal
dimension data, crucial for safety-critical tasks associated with dynamic human
activities. To address this issue and to unlock the 3D potential of event
information, we introduce two 3D event representations: the Rasterized Event
Point Cloud (RasEPC) and the Decoupled Event Voxel (DEV). The RasEPC collates
events within concise temporal slices at identical positions, preserving 3D
attributes with statistical cues and markedly mitigating memory and
computational demands. Meanwhile, the DEV representation discretizes events
into voxels and projects them across three orthogonal planes, utilizing
decoupled event attention to retrieve 3D cues from the 2D planes. Furthermore,
we develop and release EV-3DPW, a synthetic event-based dataset crafted to
facilitate training and quantitative analysis in outdoor scenes. On the public
real-world DHP19 dataset, our event point cloud technique excels in real-time
mobile predictions, while the decoupled event voxel method achieves the highest
accuracy. Experiments reveal our proposed 3D representation methods&apos; superior
generalization capacities against traditional RGB images and event frame
techniques. Our code and dataset are available at
https://github.com/MasterHow/EventPointPose.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1&quot;&gt;Xiaoting Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Hao Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiaan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ze Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1&quot;&gt;Yaozu Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_H/0/1/0/all/0/1&quot;&gt;Huajian Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kailun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kaiwei Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04592">
<title>On Characterizing the Evolution of Embedding Space of Neural Networks using Algebraic Topology. (arXiv:2311.04592v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.04592</link>
<description rdf:parseType="Literal">&lt;p&gt;We study how the topology of feature embedding space changes as it passes
through the layers of a well-trained deep neural network (DNN) through Betti
numbers. Motivated by existing studies using simplicial complexes on shallow
fully connected networks (FCN), we present an extended analysis using Cubical
homology instead, with a variety of popular deep architectures and real image
datasets. We demonstrate that as depth increases, a topologically complicated
dataset is transformed into a simple one, resulting in Betti numbers attaining
their lowest possible value. The rate of decay in topological complexity (as a
metric) helps quantify the impact of architectural choices on the
generalization ability. Interestingly from a representation learning
perspective, we highlight several invariances such as topological invariance of
(1) an architecture on similar datasets; (2) embedding space of a dataset for
architectures of variable depth; (3) embedding space to input resolution/size,
and (4) data sub-sampling. In order to further demonstrate the link between
expressivity \&amp;amp; the generalization capability of a network, we consider the
task of ranking pre-trained models for downstream classification task (transfer
learning). Compared to existing approaches, the proposed metric has a better
correlation to the actually achievable accuracy via fine-tuning the pre-trained
model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suresh_S/0/1/0/all/0/1&quot;&gt;Suryaka Suresh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_B/0/1/0/all/0/1&quot;&gt;Bishshoy Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abrol_V/0/1/0/all/0/1&quot;&gt;Vinayak Abrol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1&quot;&gt;Sumantra Dutta Roy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04828">
<title>SODAWideNet -- Salient Object Detection with an Attention augmented Wide Encoder Decoder network without ImageNet pre-training. (arXiv:2311.04828v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.04828</link>
<description rdf:parseType="Literal">&lt;p&gt;Developing a new Salient Object Detection (SOD) model involves selecting an
ImageNet pre-trained backbone and creating novel feature refinement modules to
use backbone features. However, adding new components to a pre-trained backbone
needs retraining the whole network on the ImageNet dataset, which requires
significant time. Hence, we explore developing a neural network from scratch
directly trained on SOD without ImageNet pre-training. Such a formulation
offers full autonomy to design task-specific components. To that end, we
propose SODAWideNet, an encoder-decoder-style network for Salient Object
Detection. We deviate from the commonly practiced paradigm of narrow and deep
convolutional models to a wide and shallow architecture, resulting in a
parameter-efficient deep neural network. To achieve a shallower network, we
increase the receptive field from the beginning of the network using a
combination of dilated convolutions and self-attention. Therefore, we propose
Multi Receptive Field Feature Aggregation Module (MRFFAM) that efficiently
obtains discriminative features from farther regions at higher resolutions
using dilated convolutions. Next, we propose Multi-Scale Attention (MSA), which
creates a feature pyramid and efficiently computes attention across multiple
resolutions to extract global features from larger feature maps. Finally, we
propose two variants, SODAWideNet-S (3.03M) and SODAWideNet (9.03M), that
achieve competitive performance against state-of-the-art models on five
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dulam_R/0/1/0/all/0/1&quot;&gt;Rohit Venkata Sai Dulam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kambhamettu_C/0/1/0/all/0/1&quot;&gt;Chandra Kambhamettu&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>