<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-08-08T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03766" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03767" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03768" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03770" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03772" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03792" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03793" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03807" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03813" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03821" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03826" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03861" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03865" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03867" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03900" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03906" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03908" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03936" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03939" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03950" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03968" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03975" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03977" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03979" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03982" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03998" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03999" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04005" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04008" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04016" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04020" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04035" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04039" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04047" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04052" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04054" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04070" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04074" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04079" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04091" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04118" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04126" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04137" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04142" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04151" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04152" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04156" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04162" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04163" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04168" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04177" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04188" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04197" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04206" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04207" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04218" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04224" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04232" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04243" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04249" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04252" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04262" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04269" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04283" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04288" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04303" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04321" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04322" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04337" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04340" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04343" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04352" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04356" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04369" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04370" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04373" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04380" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04383" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04395" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04397" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04402" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04409" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04413" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04417" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04426" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04431" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2002.03729" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.01615" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.04680" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.01248" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.11041" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.04415" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.11699" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.14508" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.05785" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.14915" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.04780" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.05116" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.01635" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.05609" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.10227" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.11514" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.00290" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.00500" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.06209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.09040" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.16565" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.00501" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.07916" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.08134" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.01160" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.04170" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.04517" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.09664" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.09880" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10044" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12522" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18651" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09345" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10046" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16181" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16670" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02227" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07873" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09724" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09788" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11661" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.12344" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.12450" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.12622" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.12676" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14016" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.16074" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01006" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02494" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02552" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02632" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02781" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03177" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03276" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03286" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03463" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03529" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03698" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.00085" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2308.03766">
<title>AMaizeD: An End to End Pipeline for Automatic Maize Disease Detection. (arXiv:2308.03766v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.03766</link>
<description rdf:parseType="Literal">&lt;p&gt;This research paper presents AMaizeD: An End to End Pipeline for Automatic
Maize Disease Detection, an automated framework for early detection of diseases
in maize crops using multispectral imagery obtained from drones. A custom
hand-collected dataset focusing specifically on maize crops was meticulously
gathered by expert researchers and agronomists. The dataset encompasses a
diverse range of maize varieties, cultivation practices, and environmental
conditions, capturing various stages of maize growth and disease progression.
By leveraging multispectral imagery, the framework benefits from improved
spectral resolution and increased sensitivity to subtle changes in plant
health. The proposed framework employs a combination of convolutional neural
networks (CNNs) as feature extractors and segmentation techniques to identify
both the maize plants and their associated diseases. Experimental results
demonstrate the effectiveness of the framework in detecting a range of maize
diseases, including powdery mildew, anthracnose, and leaf blight. The framework
achieves state-of-the-art performance on the custom hand-collected dataset and
contributes to the field of automated disease detection in agriculture,
offering a practical solution for early identification of diseases in maize
crops advanced machine learning techniques and deep learning architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mall_A/0/1/0/all/0/1&quot;&gt;Anish Mall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kabra_S/0/1/0/all/0/1&quot;&gt;Sanchit Kabra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lhila_A/0/1/0/all/0/1&quot;&gt;Ankur Lhila&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ajmera_P/0/1/0/all/0/1&quot;&gt;Pawan Ajmera&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03767">
<title>Enhancing image captioning with depth information using a Transformer-based framework. (arXiv:2308.03767v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.03767</link>
<description rdf:parseType="Literal">&lt;p&gt;Captioning images is a challenging scene-understanding task that connects
computer vision and natural language processing. While image captioning models
have been successful in producing excellent descriptions, the field has
primarily focused on generating a single sentence for 2D images. This paper
investigates whether integrating depth information with RGB images can enhance
the captioning task and generate better descriptions. For this purpose, we
propose a Transformer-based encoder-decoder framework for generating a
multi-sentence description of a 3D scene. The RGB image and its corresponding
depth map are provided as inputs to our framework, which combines them to
produce a better understanding of the input scene. Depth maps could be ground
truth or estimated, which makes our framework widely applicable to any RGB
captioning dataset. We explored different fusion approaches to fuse RGB and
depth images. The experiments are performed on the NYU-v2 dataset and the
Stanford image paragraph captioning dataset. During our work with the NYU-v2
dataset, we found inconsistent labeling that prevents the benefit of using
depth information to enhance the captioning task. The results were even worse
than using RGB images only. As a result, we propose a cleaned version of the
NYU-v2 dataset that is more consistent and informative. Our results on both
datasets demonstrate that the proposed framework effectively benefits from
depth information, whether it is ground truth or estimated, and generates
better captions. Code, pre-trained models, and the cleaned version of the
NYU-v2 dataset will be made publically available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1&quot;&gt;Aya Mahmoud Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yousef_M/0/1/0/all/0/1&quot;&gt;Mohamed Yousef&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hussain_K/0/1/0/all/0/1&quot;&gt;Khaled F. Hussain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahdy_Y/0/1/0/all/0/1&quot;&gt;Yousef Bassyouni Mahdy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03768">
<title>GeoTransformer: Fast and Robust Point Cloud Registration with Geometric Transformer. (arXiv:2308.03768v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.03768</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of extracting accurate correspondences for point cloud
registration. Recent keypoint-free methods have shown great potential through
bypassing the detection of repeatable keypoints which is difficult to do
especially in low-overlap scenarios. They seek correspondences over downsampled
superpoints, which are then propagated to dense points. Superpoints are matched
based on whether their neighboring patches overlap. Such sparse and loose
matching requires contextual features capturing the geometric structure of the
point clouds. We propose Geometric Transformer, or GeoTransformer for short, to
learn geometric feature for robust superpoint matching. It encodes pair-wise
distances and triplet-wise angles, making it invariant to rigid transformation
and robust in low-overlap cases. The simplistic design attains surprisingly
high matching accuracy such that no RANSAC is required in the estimation of
alignment transformation, leading to $100$ times acceleration. Extensive
experiments on rich benchmarks encompassing indoor, outdoor, synthetic,
multiway and non-rigid demonstrate the efficacy of GeoTransformer. Notably, our
method improves the inlier ratio by $18{\sim}31$ percentage points and the
registration recall by over $7$ points on the challenging 3DLoMatch benchmark.
Our code and models are available at
\url{https://github.com/qinzheng93/GeoTransformer}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1&quot;&gt;Zheng Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Changjian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yulan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yuxing Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilic_S/0/1/0/all/0/1&quot;&gt;Slobodan Ilic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1&quot;&gt;Dewen Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kai Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03770">
<title>Visual Saliency Detection in Advanced Driver Assistance Systems. (arXiv:2308.03770v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.03770</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual Saliency refers to the innate human mechanism of focusing on and
extracting important features from the observed environment. Recently, there
has been a notable surge of interest in the field of automotive research
regarding the estimation of visual saliency. While operating a vehicle, drivers
naturally direct their attention towards specific objects, employing
brain-driven saliency mechanisms that prioritize certain elements over others.
In this investigation, we present an intelligent system that combines a
drowsiness detection system for drivers with a scene comprehension pipeline
based on saliency. To achieve this, we have implemented a specialized 3D deep
network for semantic segmentation, which has been pretrained and tailored for
processing the frames captured by an automotive-grade external camera. The
proposed pipeline was hosted on an embedded platform utilizing the STA1295
core, featuring ARM A7 dual-cores, and embeds an hardware accelerator.
Additionally, we employ an innovative biosensor embedded on the car steering
wheel to monitor the driver drowsiness, gathering the PhotoPlethysmoGraphy
(PPG) signal of the driver. A dedicated 1D temporal deep convolutional network
has been devised to classify the collected PPG time-series, enabling us to
assess the driver level of attentiveness. Ultimately, we compare the determined
attention level of the driver with the corresponding saliency-based scene
classification to evaluate the overall safety level. The efficacy of the
proposed pipeline has been validated through extensive experimental results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rundo_F/0/1/0/all/0/1&quot;&gt;Francesco Rundo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rundo_M/0/1/0/all/0/1&quot;&gt;Michael Sebastian Rundo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spampinato_C/0/1/0/all/0/1&quot;&gt;Concetto Spampinato&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03772">
<title>Improved Neural Radiance Fields Using Pseudo-depth and Fusion. (arXiv:2308.03772v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.03772</link>
<description rdf:parseType="Literal">&lt;p&gt;Since the advent of Neural Radiance Fields, novel view synthesis has received
tremendous attention. The existing approach for the generalization of radiance
field reconstruction primarily constructs an encoding volume from nearby source
images as additional inputs. However, these approaches cannot efficiently
encode the geometric information of real scenes with various scale
objects/structures. In this work, we propose constructing multi-scale encoding
volumes and providing multi-scale geometry information to NeRF models. To make
the constructed volumes as close as possible to the surfaces of objects in the
scene and the rendered depth more accurate, we propose to perform depth
prediction and radiance field reconstruction simultaneously. The predicted
depth map will be used to supervise the rendered depth, narrow the depth range,
and guide points sampling. Finally, the geometric information contained in
point volume features may be inaccurate due to occlusion, lighting, etc. To
this end, we propose enhancing the point volume feature from depth-guided
neighbor feature fusion. Experiments demonstrate the superior performance of
our method in both novel view synthesis and dense geometry modeling without
per-scene optimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jingliang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1&quot;&gt;Qiang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Chaohui Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zhengda Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1&quot;&gt;Jun Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhibin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03792">
<title>Multi-attacks: Many images $+$ the same adversarial attack $\to$ many target labels. (arXiv:2308.03792v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.03792</link>
<description rdf:parseType="Literal">&lt;p&gt;We show that we can easily design a single adversarial perturbation $P$ that
changes the class of $n$ images $X_1,X_2,\dots,X_n$ from their original,
unperturbed classes $c_1, c_2,\dots,c_n$ to desired (not necessarily all the
same) classes $c^*_1,c^*_2,\dots,c^*_n$ for up to hundreds of images and target
classes at once. We call these \textit{multi-attacks}. Characterizing the
maximum $n$ we can achieve under different conditions such as image resolution,
we estimate the number of regions of high class confidence around a particular
image in the space of pixels to be around $10^{\mathcal{O}(100)}$, posing a
significant problem for exhaustive defense strategies. We show several
immediate consequences of this: adversarial attacks that change the resulting
class based on their intensity, and scale-independent adversarial examples. To
demonstrate the redundancy and richness of class decision boundaries in the
pixel space, we look for its two-dimensional sections that trace images and
spell words using particular classes. We also show that ensembling reduces
susceptibility to multi-attacks, and that classifiers trained on random labels
are more susceptible. Our code is available on GitHub.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fort_S/0/1/0/all/0/1&quot;&gt;Stanislav Fort&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03793">
<title>ReCLIP: Refine Contrastive Language Image Pre-Training with Source Free Domain Adaptation. (arXiv:2308.03793v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.03793</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale Pre-Training Vision-Language Model such as CLIP has demonstrated
outstanding performance in zero-shot classification, e.g. achieving 76.3% top-1
accuracy on ImageNet without seeing any example, which leads to potential
benefits to many tasks that have no labeled data. However, while applying CLIP
to a downstream target domain, the presence of visual and text domain gaps and
cross-modality misalignment can greatly impact the model performance. To
address such challenges, we propose ReCLIP, the first source-free domain
adaptation method for vision-language models, which does not require any source
data or target labeled data. ReCLIP first learns a projection space to mitigate
the misaligned visual-text embeddings and learns pseudo labels, and then
deploys cross-modality self-training with the pseudo labels, to update visual
and text encoders, refine labels and reduce domain gaps and misalignments
iteratively. With extensive experiments, we demonstrate ReCLIP reduces the
average error rate of CLIP from 30.17% to 25.06% on 22 image classification
benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xuefeng_H/0/1/0/all/0/1&quot;&gt;Hu. Xuefeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ke_Z/0/1/0/all/0/1&quot;&gt;Zhang. Ke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xia. Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albert_C/0/1/0/all/0/1&quot;&gt;Chen. Albert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiajia_L/0/1/0/all/0/1&quot;&gt;Luo. Jiajia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuyin_S/0/1/0/all/0/1&quot;&gt;Sun. Yuyin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ken_W/0/1/0/all/0/1&quot;&gt;Wang. Ken&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nan_Q/0/1/0/all/0/1&quot;&gt;Qiao. Nan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1&quot;&gt;Zeng. Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1&quot;&gt;Sun. Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Hao_K/0/1/0/all/0/1&quot;&gt;Kuo. Cheng-Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ram_N/0/1/0/all/0/1&quot;&gt;Nevatia. Ram&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03807">
<title>Nest-DGIL: Nesterov-optimized Deep Geometric Incremental Learning for CS Image Reconstruction. (arXiv:2308.03807v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.03807</link>
<description rdf:parseType="Literal">&lt;p&gt;Proximal gradient-based optimization is one of the most common strategies for
solving image inverse problems as well as easy to implement. However, these
techniques often generate heavy artifacts in image reconstruction. One of the
most popular refinement methods is to fine-tune the regularization parameter to
alleviate such artifacts, but it may not always be sufficient or applicable due
to increased computational costs. In this work, we propose a deep geometric
incremental learning framework based on second Nesterov proximal gradient
optimization. The proposed end-to-end network not only has the powerful
learning ability for high/low frequency image features,but also can
theoretically guarantee that geometric texture details will be reconstructed
from preliminary linear reconstruction.Furthermore, it can avoid the risk of
intermediate reconstruction results falling outside the geometric decomposition
domains and achieve fast convergence. Our reconstruction framework is
decomposed into four modules including general linear reconstruction, cascade
geometric incremental restoration, Nesterov acceleration and post-processing.
In the image restoration step,a cascade geometric incremental learning module
is designed to compensate for the missing texture information from different
geometric spectral decomposition domains. Inspired by overlap-tile strategy, we
also develop a post-processing module to remove the block-effect in
patch-wise-based natural image reconstruction. All parameters in the proposed
model are learnable,an adaptive initialization technique of physical-parameters
is also employed to make model flexibility and ensure converging smoothly. We
compare the reconstruction performance of the proposed method with existing
state-of-the-art methods to demonstrate its superiority. Our source codes are
available at https://github.com/fanxiaohong/Nest-DGIL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fan_X/0/1/0/all/0/1&quot;&gt;Xiaohong Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Ke Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yujie Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianping Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03813">
<title>High-Resolution Cranial Defect Reconstruction by Iterative, Low-Resolution, Point Cloud Completion Transformers. (arXiv:2308.03813v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.03813</link>
<description rdf:parseType="Literal">&lt;p&gt;Each year thousands of people suffer from various types of cranial injuries
and require personalized implants whose manual design is expensive and
time-consuming. Therefore, an automatic, dedicated system to increase the
availability of personalized cranial reconstruction is highly desirable. The
problem of the automatic cranial defect reconstruction can be formulated as the
shape completion task and solved using dedicated deep networks. Currently, the
most common approach is to use the volumetric representation and apply deep
networks dedicated to image segmentation. However, this approach has several
limitations and does not scale well into high-resolution volumes, nor takes
into account the data sparsity. In our work, we reformulate the problem into a
point cloud completion task. We propose an iterative, transformer-based method
to reconstruct the cranial defect at any resolution while also being fast and
resource-efficient during training and inference. We compare the proposed
methods to the state-of-the-art volumetric approaches and show superior
performance in terms of GPU memory consumption while maintaining high-quality
of the reconstructed defects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wodzinski_M/0/1/0/all/0/1&quot;&gt;Marek Wodzinski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Daniol_M/0/1/0/all/0/1&quot;&gt;Mateusz Daniol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hemmerling_D/0/1/0/all/0/1&quot;&gt;Daria Hemmerling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Socha_M/0/1/0/all/0/1&quot;&gt;Miroslaw Socha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03821">
<title>Distributionally Robust Classification on a Data Budget. (arXiv:2308.03821v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.03821</link>
<description rdf:parseType="Literal">&lt;p&gt;Real world uses of deep learning require predictable model behavior under
distribution shifts. Models such as CLIP show emergent natural distributional
robustness comparable to humans, but may require hundreds of millions of
training samples. Can we train robust learners in a domain where data is
limited? To rigorously address this question, we introduce JANuS (Joint
Annotations and Names Set), a collection of four new training datasets with
images, labels, and corresponding captions, and perform a series of carefully
controlled investigations of factors contributing to robustness in image
classification, then compare those results to findings derived from a
large-scale meta-analysis. Using this approach, we show that standard ResNet-50
trained with the cross-entropy loss on 2.4 million image samples can attain
comparable robustness to a CLIP ResNet-50 trained on 400 million samples. To
our knowledge, this is the first result showing (near) state-of-the-art
distributional robustness on limited data budgets. Our dataset is available at
\url{https://huggingface.co/datasets/penfever/JANuS_dataset}, and the code used
to reproduce our experiments can be found at
\url{https://github.com/penfever/vlhub/}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feuer_B/0/1/0/all/0/1&quot;&gt;Benjamin Feuer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1&quot;&gt;Ameya Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_M/0/1/0/all/0/1&quot;&gt;Minh Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hegde_C/0/1/0/all/0/1&quot;&gt;Chinmay Hegde&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03826">
<title>Recurrent Multi-scale Transformer for High-Resolution Salient Object Detection. (arXiv:2308.03826v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.03826</link>
<description rdf:parseType="Literal">&lt;p&gt;Salient Object Detection (SOD) aims to identify and segment the most
conspicuous objects in an image or video. As an important pre-processing step,
it has many potential applications in multimedia and vision tasks. With the
advance of imaging devices, SOD with high-resolution images is of great demand,
recently. However, traditional SOD methods are largely limited to
low-resolution images, making them difficult to adapt to the development of
High-Resolution SOD (HRSOD). Although some HRSOD methods emerge, there are no
large enough datasets for training and evaluating. Besides, current HRSOD
methods generally produce incomplete object regions and irregular object
boundaries. To address above issues, in this work, we first propose a new
HRS10K dataset, which contains 10,500 high-quality annotated images at 2K-8K
resolution. As far as we know, it is the largest dataset for the HRSOD task,
which will significantly help future works in training and evaluating models.
Furthermore, to improve the HRSOD performance, we propose a novel Recurrent
Multi-scale Transformer (RMFormer), which recurrently utilizes shared
Transformers and multi-scale refinement architectures. Thus, high-resolution
saliency maps can be generated with the guidance of lower-resolution
predictions. Extensive experiments on both high-resolution and low-resolution
benchmarks show the effectiveness and superiority of the proposed framework.
The source code and dataset are released at:
https://github.com/DrowsyMon/RMFormer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1&quot;&gt;Xinhao Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Pingping Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Huchuan Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03861">
<title>High-Throughput and Accurate 3D Scanning of Cattle Using Time-of-Flight Sensors and Deep Learning. (arXiv:2308.03861v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.03861</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a high throughput 3D scanning solution specifically designed to
precisely measure cattle phenotypes. This scanner leverages an array of depth
sensors, i.e. time-of-flight (Tof) sensors, each governed by dedicated embedded
devices. The system excels at generating high-fidelity 3D point clouds, thus
facilitating an accurate mesh that faithfully reconstructs the cattle geometry
on the fly. In order to evaluate the performance of our system, we have
implemented a two-fold validation process. Initially, we test the scanner&apos;s
competency in determining volume and surface area measurements within a
controlled environment featuring known objects. Secondly, we explore the impact
and necessity of multi-device synchronization when operating a series of
time-of-flight sensors. Based on the experimental results, the proposed system
is capable of producing high-quality meshes of untamed cattle for livestock
studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Omotara_G/0/1/0/all/0/1&quot;&gt;Gbenga Omotara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tousi_S/0/1/0/all/0/1&quot;&gt;Seyed Mohamad Ali Tousi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Decker_J/0/1/0/all/0/1&quot;&gt;Jared Decker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brake_D/0/1/0/all/0/1&quot;&gt;Derek Brake&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DeSouza_G/0/1/0/all/0/1&quot;&gt;Guilherme N. DeSouza&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03865">
<title>DefCor-Net: Physics-Aware Ultrasound Deformation Correction. (arXiv:2308.03865v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.03865</link>
<description rdf:parseType="Literal">&lt;p&gt;The recovery of morphologically accurate anatomical images from deformed ones
is challenging in ultrasound (US) image acquisition, but crucial to accurate
and consistent diagnosis, particularly in the emerging field of
computer-assisted diagnosis. This article presents a novel anatomy-aware
deformation correction approach based on a coarse-to-fine, multi-scale deep
neural network (DefCor-Net). To achieve pixel-wise performance, DefCor-Net
incorporates biomedical knowledge by estimating pixel-wise stiffness online
using a U-shaped feature extractor. The deformation field is then computed
using polynomial regression by integrating the measured force applied by the US
probe. Based on real-time estimation of pixel-by-pixel tissue properties, the
learning-based approach enables the potential for anatomy-aware deformation
correction. To demonstrate the effectiveness of the proposed DefCor-Net, images
recorded at multiple locations on forearms and upper arms of six volunteers are
used to train and validate DefCor-Net. The results demonstrate that DefCor-Net
can significantly improve the accuracy of deformation correction to recover the
original geometry (Dice Coefficient: from $14.3\pm20.9$ to $82.6\pm12.1$ when
the force is $6N$).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhongliang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yue Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cao_D/0/1/0/all/0/1&quot;&gt;Dongliang Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Navab_N/0/1/0/all/0/1&quot;&gt;Nassir Navab&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03867">
<title>From Sky to the Ground: A Large-scale Benchmark and Simple Baseline Towards Real Rain Removal. (arXiv:2308.03867v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.03867</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning-based image deraining methods have made great progress. However, the
lack of large-scale high-quality paired training samples is the main bottleneck
to hamper the real image deraining (RID). To address this dilemma and advance
RID, we construct a Large-scale High-quality Paired real rain benchmark
(LHP-Rain), including 3000 video sequences with 1 million high-resolution
(1920*1080) frame pairs. The advantages of the proposed dataset over the
existing ones are three-fold: rain with higher-diversity and larger-scale,
image with higher-resolution and higher-quality ground-truth. Specifically, the
real rains in LHP-Rain not only contain the classical rain
streak/veiling/occlusion in the sky, but also the \textbf{splashing on the
ground} overlooked by deraining community. Moreover, we propose a novel robust
low-rank tensor recovery model to generate the GT with better separating the
static background from the dynamic rain. In addition, we design a simple
transformer-based single image deraining baseline, which simultaneously utilize
the self-attention and cross-layer attention within the image and rain layer
with discriminative feature representation. Extensive experiments verify the
superiority of the proposed dataset and deraining method over state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yun Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1&quot;&gt;Xueyao Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1&quot;&gt;Yi Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1&quot;&gt;Shumin Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1&quot;&gt;Luxin Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03900">
<title>Developability Approximation for Neural Implicits through Rank Minimization. (arXiv:2308.03900v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.03900</link>
<description rdf:parseType="Literal">&lt;p&gt;Developability refers to the process of creating a surface without any
tearing or shearing from a two-dimensional plane. It finds practical
applications in the fabrication industry. An essential characteristic of a
developable 3D surface is its zero Gaussian curvature, which means that either
one or both of the principal curvatures are zero. This paper introduces a
method for reconstructing an approximate developable surface from a neural
implicit surface. The central idea of our method involves incorporating a
regularization term that operates on the second-order derivatives of the neural
implicits, effectively promoting zero Gaussian curvature. Implicit surfaces
offer the advantage of smoother deformation with infinite resolution,
overcoming the high polygonal constraints of state-of-the-art methods using
discrete representations. We draw inspiration from the properties of surface
curvature and employ rank minimization techniques derived from compressed
sensing. Experimental results on both developable and non-developable surfaces,
including those affected by noise, validate the generalizability of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Selvaraju_P/0/1/0/all/0/1&quot;&gt;Pratheba Selvaraju&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03906">
<title>TIJO: Trigger Inversion with Joint Optimization for Defending Multimodal Backdoored Models. (arXiv:2308.03906v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.03906</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a Multimodal Backdoor Defense technique TIJO (Trigger Inversion
using Joint Optimization). Recent work &lt;a href=&quot;/abs/2112.07668&quot;&gt;arXiv:2112.07668&lt;/a&gt; has demonstrated
successful backdoor attacks on multimodal models for the Visual Question
Answering task. Their dual-key backdoor trigger is split across two modalities
(image and text), such that the backdoor is activated if and only if the
trigger is present in both modalities. We propose TIJO that defends against
dual-key attacks through a joint optimization that reverse-engineers the
trigger in both the image and text modalities. This joint optimization is
challenging in multimodal models due to the disconnected nature of the visual
pipeline which consists of an offline feature extractor, whose output is then
fused with the text using a fusion module. The key insight enabling the joint
optimization in TIJO is that the trigger inversion needs to be carried out in
the object detection box feature space as opposed to the pixel space. We
demonstrate the effectiveness of our method on the TrojVQA benchmark, where
TIJO improves upon the state-of-the-art unimodal methods from an AUC of 0.6 to
0.92 on multimodal dual-key backdoors. Furthermore, our method also improves
upon the unimodal baselines on unimodal backdoors. We present ablation studies
and qualitative results to provide insights into our algorithm such as the
critical importance of overlaying the inverted feature triggers on all visual
features during trigger inversion. The prototype implementation of TIJO is
available at https://github.com/SRI-CSL/TIJO.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sur_I/0/1/0/all/0/1&quot;&gt;Indranil Sur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sikka_K/0/1/0/all/0/1&quot;&gt;Karan Sikka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walmer_M/0/1/0/all/0/1&quot;&gt;Matthew Walmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koneripalli_K/0/1/0/all/0/1&quot;&gt;Kaushik Koneripalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1&quot;&gt;Anirban Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xiao Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Divakaran_A/0/1/0/all/0/1&quot;&gt;Ajay Divakaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1&quot;&gt;Susmit Jha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03908">
<title>ViLP: Knowledge Exploration using Vision, Language, and Pose Embeddings for Video Action Recognition. (arXiv:2308.03908v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.03908</link>
<description rdf:parseType="Literal">&lt;p&gt;Video Action Recognition (VAR) is a challenging task due to its inherent
complexities. Though different approaches have been explored in the literature,
designing a unified framework to recognize a large number of human actions is
still a challenging problem. Recently, Multi-Modal Learning (MML) has
demonstrated promising results in this domain. In literature, 2D skeleton or
pose modality has often been used for this task, either independently or in
conjunction with the visual information (RGB modality) present in videos.
However, the combination of pose, visual information, and text attributes has
not been explored yet, though text and pose attributes independently have been
proven to be effective in numerous computer vision tasks. In this paper, we
present the first pose augmented Vision-language model (VLM) for VAR. Notably,
our scheme achieves an accuracy of 92.81% and 73.02% on two popular human video
action recognition benchmark datasets, UCF-101 and HMDB-51, respectively, even
without any video data pre-training, and an accuracy of 96.11% and 75.75% after
kinetics pre-training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1&quot;&gt;Soumyabrata Chaudhuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1&quot;&gt;Saumik Bhattacharya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03936">
<title>ALFA -- Leveraging All Levels of Feature Abstraction for Enhancing the Generalization of Histopathology Image Classification Across Unseen Hospitals. (arXiv:2308.03936v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.03936</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an exhaustive methodology that leverages all levels of feature
abstraction, targeting an enhancement in the generalizability of image
classification to unobserved hospitals. Our approach incorporates
augmentation-based self-supervision with common distribution shifts in
histopathology scenarios serving as the pretext task. This enables us to derive
invariant features from training images without relying on training labels,
thereby covering different abstraction levels. Moving onto the subsequent
abstraction level, we employ a domain alignment module to facilitate further
extraction of invariant features across varying training hospitals. To
represent the highly specific features of participating hospitals, an encoder
is trained to classify hospital labels, independent of their diagnostic labels.
The features from each of these encoders are subsequently disentangled to
minimize redundancy and segregate the features. This representation, which
spans a broad spectrum of semantic information, enables the development of a
model demonstrating increased robustness to unseen images from disparate
distributions. Experimental results from the PACS dataset (a domain
generalization benchmark), a synthetic dataset created by applying
histopathology-specific jitters to the MHIST dataset (defining different
domains with varied distribution shifts), and a Renal Cell Carcinoma dataset
derived from four image repositories from TCGA, collectively indicate that our
proposed model is adept at managing varying levels of image granularity. Thus,
it shows improved generalizability when faced with new, out-of-distribution
hospital images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sikaroudi_M/0/1/0/all/0/1&quot;&gt;Milad Sikaroudi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahnamayan_S/0/1/0/all/0/1&quot;&gt;Shahryar Rahnamayan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tizhoosh_H/0/1/0/all/0/1&quot;&gt;H.R. Tizhoosh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03939">
<title>Deterministic Neural Illumination Mapping for Efficient Auto-White Balance Correction. (arXiv:2308.03939v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.03939</link>
<description rdf:parseType="Literal">&lt;p&gt;Auto-white balance (AWB) correction is a critical operation in image signal
processors for accurate and consistent color correction across various
illumination scenarios. This paper presents a novel and efficient AWB
correction method that achieves at least 35 times faster processing with
equivalent or superior performance on high-resolution images for the current
state-of-the-art methods. Inspired by deterministic color style transfer, our
approach introduces deterministic illumination color mapping, leveraging
learnable projection matrices for both canonical illumination form and
AWB-corrected output. It involves feeding high-resolution images and
corresponding latent representations into a mapping module to derive a
canonical form, followed by another mapping module that maps the pixel values
to those for the corrected version. This strategy is designed as
resolution-agnostic and also enables seamless integration of any pre-trained
AWB network as the backbone. Experimental results confirm the effectiveness of
our approach, revealing significant performance improvements and reduced time
complexity compared to state-of-the-art methods. Our method provides an
efficient deep learning-based AWB correction solution, promising real-time,
high-quality color correction for digital imaging applications. Source code is
available at https://github.com/birdortyedi/DeNIM/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kinli_F/0/1/0/all/0/1&quot;&gt;Furkan K&amp;#x131;nl&amp;#x131;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yilmaz_D/0/1/0/all/0/1&quot;&gt;Do&amp;#x11f;a Y&amp;#x131;lmaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozcan_B/0/1/0/all/0/1&quot;&gt;Bar&amp;#x131;&amp;#x15f; &amp;#xd6;zcan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirac_F/0/1/0/all/0/1&quot;&gt;Furkan K&amp;#x131;ra&amp;#xe7;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03950">
<title>Zero-shot Skeleton-based Action Recognition via Mutual Information Estimation and Maximization. (arXiv:2308.03950v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.03950</link>
<description rdf:parseType="Literal">&lt;p&gt;Zero-shot skeleton-based action recognition aims to recognize actions of
unseen categories after training on data of seen categories. The key is to
build the connection between visual and semantic space from seen to unseen
classes. Previous studies have primarily focused on encoding sequences into a
singular feature vector, with subsequent mapping the features to an identical
anchor point within the embedded space. Their performance is hindered by 1) the
ignorance of the global visual/semantic distribution alignment, which results
in a limitation to capture the true interdependence between the two spaces. 2)
the negligence of temporal information since the frame-wise features with rich
action clues are directly pooled into a single feature vector. We propose a new
zero-shot skeleton-based action recognition method via mutual information (MI)
estimation and maximization. Specifically, 1) we maximize the MI between visual
and semantic space for distribution alignment; 2) we leverage the temporal
information for estimating the MI by encouraging MI to increase as more frames
are observed. Extensive experiments on three large-scale skeleton action
datasets confirm the effectiveness of our method. Code:
https://github.com/YujieOuO/SMIE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yujie Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiang_W/0/1/0/all/0/1&quot;&gt;Wenwen Qiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1&quot;&gt;Anyi Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1&quot;&gt;Ning Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_B/0/1/0/all/0/1&quot;&gt;Bing Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiaqi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03968">
<title>CheXFusion: Effective Fusion of Multi-View Features using Transformers for Long-Tailed Chest X-Ray Classification. (arXiv:2308.03968v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.03968</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical image classification poses unique challenges due to the long-tailed
distribution of diseases, the co-occurrence of diagnostic findings, and the
multiple views available for each study or patient. This paper introduces our
solution to the ICCV CVAMD 2023 Shared Task on CXR-LT: Multi-Label Long-Tailed
Classification on Chest X-Rays. Our approach introduces CheXFusion, a
transformer-based fusion module incorporating multi-view images. The fusion
module, guided by self-attention and cross-attention mechanisms, efficiently
aggregates multi-view features while considering label co-occurrence.
Furthermore, we explore data balancing and self-training methods to optimize
the model&apos;s performance. Our solution achieves state-of-the-art results with
0.372 mAP in the MIMIC-CXR test set, securing 1st place in the competition. Our
success in the task underscores the significance of considering multi-view
settings, class imbalance, and label co-occurrence in medical image
classification. Public code is available at
https://github.com/dongkyuk/CXR-LT-public-solution
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Dongkyun Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03975">
<title>Prompted Contrast with Masked Motion Modeling: Towards Versatile 3D Action Representation Learning. (arXiv:2308.03975v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.03975</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised learning has proved effective for skeleton-based human action
understanding, which is an important yet challenging topic. Previous works
mainly rely on contrastive learning or masked motion modeling paradigm to model
the skeleton relations. However, the sequence-level and joint-level
representation learning cannot be effectively and simultaneously handled by
these methods. As a result, the learned representations fail to generalize to
different downstream tasks. Moreover, combining these two paradigms in a naive
manner leaves the synergy between them untapped and can lead to interference in
training. To address these problems, we propose Prompted Contrast with Masked
Motion Modeling, PCM$^{\rm 3}$, for versatile 3D action representation
learning. Our method integrates the contrastive learning and masked prediction
tasks in a mutually beneficial manner, which substantially boosts the
generalization capacity for various downstream tasks. Specifically, masked
prediction provides novel training views for contrastive learning, which in
turn guides the masked prediction training with high-level semantic
information. Moreover, we propose a dual-prompted multi-task pretraining
strategy, which further improves model representations by reducing the
interference caused by learning the two different pretext tasks. Extensive
experiments on five downstream tasks under three large-scale datasets are
conducted, demonstrating the superior generalization capacity of PCM$^{\rm 3}$
compared to the state-of-the-art works. Our project is publicly available at:
https://jhang2020.github.io/Projects/PCM3/PCM3.html .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiahang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Lilang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiaying Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03977">
<title>PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning. (arXiv:2308.03977v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.03977</link>
<description rdf:parseType="Literal">&lt;p&gt;Synthetic image datasets offer unmatched advantages for designing and
evaluating deep neural networks: they make it possible to (i) render as many
data samples as needed, (ii) precisely control each scene and yield granular
ground truth labels (and captions), (iii) precisely control distribution shifts
between training and testing to isolate variables of interest for sound
experimentation. Despite such promise, the use of synthetic image data is still
limited -- and often played down -- mainly due to their lack of realism. Most
works therefore rely on datasets of real images, which have often been scraped
from public images on the internet, and may have issues with regards to
privacy, bias, and copyright, while offering little control over how objects
precisely appear. In this work, we present a path to democratize the use of
photorealistic synthetic data: we develop a new generation of interactive
environments for representation learning research, that offer both
controllability and realism. We use the Unreal Engine, a powerful game engine
well known in the entertainment industry, to produce PUG (Photorealistic Unreal
Graphics) environments and datasets for representation learning. In this paper,
we demonstrate the potential of PUG to enable more rigorous evaluations of
vision models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bordes_F/0/1/0/all/0/1&quot;&gt;Florian Bordes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shekhar_S/0/1/0/all/0/1&quot;&gt;Shashank Shekhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ibrahim_M/0/1/0/all/0/1&quot;&gt;Mark Ibrahim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouchacourt_D/0/1/0/all/0/1&quot;&gt;Diane Bouchacourt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vincent_P/0/1/0/all/0/1&quot;&gt;Pascal Vincent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morcos_A/0/1/0/all/0/1&quot;&gt;Ari S. Morcos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03979">
<title>PAIF: Perception-Aware Infrared-Visible Image Fusion for Attack-Tolerant Semantic Segmentation. (arXiv:2308.03979v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.03979</link>
<description rdf:parseType="Literal">&lt;p&gt;Infrared and visible image fusion is a powerful technique that combines
complementary information from different modalities for downstream semantic
perception tasks. Existing learning-based methods show remarkable performance,
but are suffering from the inherent vulnerability of adversarial attacks,
causing a significant decrease in accuracy. In this work, a perception-aware
fusion framework is proposed to promote segmentation robustness in adversarial
scenes. We first conduct systematic analyses about the components of image
fusion, investigating the correlation with segmentation robustness under
adversarial perturbations. Based on these analyses, we propose a harmonized
architecture search with a decomposition-based structure to balance standard
accuracy and robustness. We also propose an adaptive learning strategy to
improve the parameter robustness of image fusion, which can learn effective
feature extraction under diverse adversarial perturbations. Thus, the goals of
image fusion (\textit{i.e.,} extracting complementary features from source
modalities and defending attack) can be realized from the perspectives of
architectural and learning strategies. Extensive experimental results
demonstrate that our scheme substantially enhances the robustness, with gains
of 15.3% mIOU of segmentation in the adversarial scene, compared with advanced
competitors. The source codes are available at
https://github.com/LiuZhu-CV/PAIF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jinyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Benzhuang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Long Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1&quot;&gt;Xin Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Risheng Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03982">
<title>PARTNER: Level up the Polar Representation for LiDAR 3D Object Detection. (arXiv:2308.03982v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.03982</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, polar-based representation has shown promising properties in
perceptual tasks. In addition to Cartesian-based approaches, which separate
point clouds unevenly, representing point clouds as polar grids has been
recognized as an alternative due to (1) its advantage in robust performance
under different resolutions and (2) its superiority in streaming-based
approaches. However, state-of-the-art polar-based detection methods inevitably
suffer from the feature distortion problem because of the non-uniform division
of polar representation, resulting in a non-negligible performance gap compared
to Cartesian-based approaches. To tackle this issue, we present PARTNER, a
novel 3D object detector in the polar coordinate. PARTNER alleviates the
dilemma of feature distortion with global representation re-alignment and
facilitates the regression by introducing instance-level geometric information
into the detection head. Extensive experiments show overwhelming advantages in
streaming-based detection and different resolutions. Furthermore, our method
outperforms the previous polar-based works with remarkable margins of 3.68% and
9.15% on Waymo and ONCE validation set, thus achieving competitive results over
the state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_M/0/1/0/all/0/1&quot;&gt;Ming Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1&quot;&gt;Yujing Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chunwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1&quot;&gt;Chaoqiang Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xinge Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qingqiu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mi_M/0/1/0/all/0/1&quot;&gt;Michael Bi Mi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinchao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Li Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03998">
<title>Real-time Strawberry Detection Based on Improved YOLOv5s Architecture for Robotic Harvesting in open-field environment. (arXiv:2308.03998v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.03998</link>
<description rdf:parseType="Literal">&lt;p&gt;This study proposed a YOLOv5-based custom object detection model to detect
strawberries in an outdoor environment. The original architecture of the
YOLOv5s was modified by replacing the C3 module with the C2f module in the
backbone network, which provided a better feature gradient flow. Secondly, the
Spatial Pyramid Pooling Fast in the final layer of the backbone network of
YOLOv5s was combined with Cross Stage Partial Net to improve the generalization
ability over the strawberry dataset in this study. The proposed architecture
was named YOLOv5s-Straw. The RGB images dataset of the strawberry canopy with
three maturity classes (immature, nearly mature, and mature) was collected in
open-field environment and augmented through a series of operations including
brightness reduction, brightness increase, and noise adding. To verify the
superiority of the proposed method for strawberry detection in open-field
environment, four competitive detection models (YOLOv3-tiny, YOLOv5s,
YOLOv5s-C2f, and YOLOv8s) were trained, and tested under the same computational
environment and compared with YOLOv5s-Straw. The results showed that the
highest mean average precision of 80.3% was achieved using the proposed
architecture whereas the same was achieved with YOLOv3-tiny, YOLOv5s,
YOLOv5s-C2f, and YOLOv8s were 73.4%, 77.8%, 79.8%, 79.3%, respectively.
Specifically, the average precision of YOLOv5s-Straw was 82.1% in the immature
class, 73.5% in the nearly mature class, and 86.6% in the mature class, which
were 2.3% and 3.7%, respectively, higher than that of the latest YOLOv8s. The
model included 8.6*10^6 network parameters with an inference speed of 18ms per
image while the inference speed of YOLOv8s had a slower inference speed of
21.0ms and heavy parameters of 11.1*10^6, which indicates that the proposed
model is fast enough for real time strawberry detection and localization for
the robotic picking.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zixuan He&lt;/a&gt; (1) (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khana_S/0/1/0/all/0/1&quot;&gt;Salik Ram Khana&lt;/a&gt; (1) (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xin Zhang&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karkee_M/0/1/0/all/0/1&quot;&gt;Manoj Karkee&lt;/a&gt; (1) (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qin Zhang&lt;/a&gt; (1) (2) ((1) Center for Precision and Automated Agricultural Systems, Washington State University, (2) Department of Biological Systems Engineering, Washington State University, (3) Department of Agricultural and Biological Engineering, Mississippi State University)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03999">
<title>Understanding CNN Hidden Neuron Activations using Structured Background Knowledge and Deductive Reasoning. (arXiv:2308.03999v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.03999</link>
<description rdf:parseType="Literal">&lt;p&gt;A major challenge in Explainable AI is in correctly interpreting activations
of hidden neurons: accurate interpretations would provide insights into the
question of what a deep learning system has internally detected as relevant on
the input, de-mystifying the otherwise black-box character of deep learning
systems. The state of the art indicates that hidden node activations can, in
some cases, be interpretable in a way that makes sense to humans, but
systematic automated methods that would be able to hypothesize and verify
interpretations of hidden neuron activations are underexplored. In this paper,
we provide such a method and demonstrate that it provides meaningful
interpretations. Our approach is based on using large-scale background
knowledge approximately 2 million classes curated from the Wikipedia concept
hierarchy together with a symbolic reasoning approach called Concept Induction
based on description logics, originally developed for applications in the
Semantic Web field. Our results show that we can automatically attach
meaningful labels from the background knowledge to individual neurons in the
dense layer of a Convolutional Neural Network through a hypothesis and
verification process
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dalal_A/0/1/0/all/0/1&quot;&gt;Abhilekha Dalal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarker_M/0/1/0/all/0/1&quot;&gt;Md Kamruzzaman Sarker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barua_A/0/1/0/all/0/1&quot;&gt;Adrita Barua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasserman_E/0/1/0/all/0/1&quot;&gt;Eugene Vasserman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hitzler_P/0/1/0/all/0/1&quot;&gt;Pascal Hitzler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04005">
<title>Few-shot medical image classification with simple shape and texture text descriptors using vision-language models. (arXiv:2308.04005v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04005</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we investigate the usefulness of vision-language models (VLMs)
and large language models for binary few-shot classification of medical images.
We utilize the GPT-4 model to generate text descriptors that encapsulate the
shape and texture characteristics of objects in medical images. Subsequently,
these GPT-4 generated descriptors, alongside VLMs pre-trained on natural
images, are employed to classify chest X-rays and breast ultrasound images. Our
results indicate that few-shot classification of medical images using VLMs and
GPT-4 generated descriptors is a viable approach. However, accurate
classification requires to exclude certain descriptors from the calculations of
the classification scores. Moreover, we assess the ability of VLMs to evaluate
shape features in breast mass ultrasound images. We further investigate the
degree of variability among the sets of text descriptors produced by GPT-4. Our
work provides several important insights about the application of VLMs for
medical image analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Byra_M/0/1/0/all/0/1&quot;&gt;Michal Byra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rachmadi_M/0/1/0/all/0/1&quot;&gt;Muhammad Febrian Rachmadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skibbe_H/0/1/0/all/0/1&quot;&gt;Henrik Skibbe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04008">
<title>Coarse-to-Fine: Learning Compact Discriminative Representation for Single-Stage Image Retrieval. (arXiv:2308.04008v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04008</link>
<description rdf:parseType="Literal">&lt;p&gt;Image retrieval targets to find images from a database that are visually
similar to the query image. Two-stage methods following retrieve-and-rerank
paradigm have achieved excellent performance, but their separate local and
global modules are inefficient to real-world applications. To better trade-off
retrieval efficiency and accuracy, some approaches fuse global and local
feature into a joint representation to perform single-stage image retrieval.
However, they are still challenging due to various situations to tackle,
$e.g.$, background, occlusion and viewpoint. In this work, we design a
Coarse-to-Fine framework to learn Compact Discriminative representation (CFCD)
for end-to-end single-stage image retrieval-requiring only image-level labels.
Specifically, we first design a novel adaptive softmax-based loss which
dynamically tunes its scale and margin within each mini-batch and increases
them progressively to strengthen supervision during training and intra-class
compactness. Furthermore, we propose a mechanism which attentively selects
prominent local descriptors and infuse fine-grained semantic relations into the
global representation by a hard negative sampling strategy to optimize
inter-class distinctiveness at a global scale. Extensive experimental results
have demonstrated the effectiveness of our method, which achieves
state-of-the-art single-stage image retrieval performance on benchmarks such as
Revisited Oxford and Revisited Paris. Code is available at
https://github.com/bassyess/CFCD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yunquan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xinkai Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ke_B/0/1/0/all/0/1&quot;&gt;Bo Ke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_R/0/1/0/all/0/1&quot;&gt;Ruizhi Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xing Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04016">
<title>Hierarchical Visual Primitive Experts for Compositional Zero-Shot Learning. (arXiv:2308.04016v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04016</link>
<description rdf:parseType="Literal">&lt;p&gt;Compositional zero-shot learning (CZSL) aims to recognize unseen compositions
with prior knowledge of known primitives (attribute and object). Previous works
for CZSL often suffer from grasping the contextuality between attribute and
object, as well as the discriminability of visual features, and the long-tailed
distribution of real-world compositional data. We propose a simple and scalable
framework called Composition Transformer (CoT) to address these issues. CoT
employs object and attribute experts in distinctive manners to generate
representative embeddings, using the visual network hierarchically. The object
expert extracts representative object embeddings from the final layer in a
bottom-up manner, while the attribute expert makes attribute embeddings in a
top-down manner with a proposed object-guided attention module that models
contextuality explicitly. To remedy biased prediction caused by imbalanced data
distribution, we develop a simple minority attribute augmentation (MAA) that
synthesizes virtual samples by mixing two images and oversampling minority
attribute classes. Our method achieves SoTA performance on several benchmarks,
including MIT-States, C-GQA, and VAW-CZSL. We also demonstrate the
effectiveness of CoT in improving visual discrimination and addressing the
model bias from the imbalanced data distribution. The code is available at
https://github.com/HanjaeKim98/CoT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hanjae Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jiyoung Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Seongheon Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1&quot;&gt;Kwanghoon Sohn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04020">
<title>Synthetic Augmentation with Large-scale Unconditional Pre-training. (arXiv:2308.04020v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04020</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning based medical image recognition systems often require a
substantial amount of training data with expert annotations, which can be
expensive and time-consuming to obtain. Recently, synthetic augmentation
techniques have been proposed to mitigate the issue by generating realistic
images conditioned on class labels. However, the effectiveness of these methods
heavily depends on the representation capability of the trained generative
model, which cannot be guaranteed without sufficient labeled training data. To
further reduce the dependency on annotated data, we propose a synthetic
augmentation method called HistoDiffusion, which can be pre-trained on
large-scale unlabeled datasets and later applied to a small-scale labeled
dataset for augmented training. In particular, we train a latent diffusion
model (LDM) on diverse unlabeled datasets to learn common features and generate
realistic images without conditional inputs. Then, we fine-tune the model with
classifier guidance in latent space on an unseen labeled dataset so that the
model can synthesize images of specific categories. Additionally, we adopt a
selective mechanism to only add synthetic samples with high confidence of
matching to target labels. We evaluate our proposed method by pre-training on
three histopathology datasets and testing on a histopathology dataset of
colorectal cancer (CRC) excluded from the pre-training datasets. With
HistoDiffusion augmentation, the classification accuracy of a backbone
classifier is remarkably improved by 6.4% using a small set of the original
labels. Our code is available at https://github.com/karenyyy/HistoDiffAug.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jiarong Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_H/0/1/0/all/0/1&quot;&gt;Haomiao Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1&quot;&gt;Peng Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Sharon X. Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1&quot;&gt;Yuan Xue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04035">
<title>Cross-Dataset Adaptation for Instrument Classification in Cataract Surgery Videos. (arXiv:2308.04035v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04035</link>
<description rdf:parseType="Literal">&lt;p&gt;Surgical tool presence detection is an important part of the intra-operative
and post-operative analysis of a surgery. State-of-the-art models, which
perform this task well on a particular dataset, however, perform poorly when
tested on another dataset. This occurs due to a significant domain shift
between the datasets resulting from the use of different tools, sensors, data
resolution etc. In this paper, we highlight this domain shift in the commonly
performed cataract surgery and propose a novel end-to-end Unsupervised Domain
Adaptation (UDA) method called the Barlow Adaptor that addresses the problem of
distribution shift without requiring any labels from another domain. In
addition, we introduce a novel loss called the Barlow Feature Alignment Loss
(BFAL) which aligns features across different domains while reducing redundancy
and the need for higher batch sizes, thus improving cross-dataset performance.
The use of BFAL is a novel approach to address the challenge of domain shift in
cataract surgery data. Extensive experiments are conducted on two cataract
surgery datasets and it is shown that the proposed method outperforms the
state-of-the-art UDA methods by 6%. The code can be found at
https://github.com/JayParanjape/Barlow-Adaptor
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paranjape_J/0/1/0/all/0/1&quot;&gt;Jay N. Paranjape&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sikder_S/0/1/0/all/0/1&quot;&gt;Shameema Sikder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1&quot;&gt;Vishal M. Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vedula_S/0/1/0/all/0/1&quot;&gt;S. Swaroop Vedula&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04039">
<title>Implicit neural representations for joint decomposition and registration of gene expression images in the marmoset brain. (arXiv:2308.04039v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04039</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel image registration method based on implicit neural
representations that addresses the challenging problem of registering a pair of
brain images with similar anatomical structures, but where one image contains
additional features or artifacts that are not present in the other image. To
demonstrate its effectiveness, we use 2D microscopy $\textit{in situ}$
hybridization gene expression images of the marmoset brain. Accurately
quantifying gene expression requires image registration to a brain template,
which is difficult due to the diversity of patterns causing variations in
visible anatomical brain structures. Our approach uses implicit networks in
combination with an image exclusion loss to jointly perform the registration
and decompose the image into a support and residual image. The support image
aligns well with the template, while the residual image captures individual
image characteristics that diverge from the template. In experiments, our
method provided excellent results and outperformed other registration
techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Byra_M/0/1/0/all/0/1&quot;&gt;Michal Byra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poon_C/0/1/0/all/0/1&quot;&gt;Charissa Poon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shimogori_T/0/1/0/all/0/1&quot;&gt;Tomomi Shimogori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skibbe_H/0/1/0/all/0/1&quot;&gt;Henrik Skibbe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04047">
<title>SODFormer: Streaming Object Detection with Transformer Using Events and Frames. (arXiv:2308.04047v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04047</link>
<description rdf:parseType="Literal">&lt;p&gt;DAVIS camera, streaming two complementary sensing modalities of asynchronous
events and frames, has gradually been used to address major object detection
challenges (e.g., fast motion blur and low-light). However, how to effectively
leverage rich temporal cues and fuse two heterogeneous visual streams remains a
challenging endeavor. To address this challenge, we propose a novel streaming
object detector with Transformer, namely SODFormer, which first integrates
events and frames to continuously detect objects in an asynchronous manner.
Technically, we first build a large-scale multimodal neuromorphic object
detection dataset (i.e., PKU-DAVIS-SOD) over 1080.1k manual labels. Then, we
design a spatiotemporal Transformer architecture to detect objects via an
end-to-end sequence prediction problem, where the novel temporal Transformer
module leverages rich temporal cues from two visual streams to improve the
detection performance. Finally, an asynchronous attention-based fusion module
is proposed to integrate two heterogeneous sensing modalities and take
complementary advantages from each end, which can be queried at any time to
locate objects and break through the limited output frequency from synchronized
frame-based fusion strategies. The results show that the proposed SODFormer
outperforms four state-of-the-art methods and our eight baselines by a
significant margin. We also show that our unifying framework works well even in
cases where the conventional frame-based camera fails, e.g., high-speed motion
and low-light conditions. Our dataset and code can be available at
https://github.com/dianzl/SODFormer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dianze Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yonghong Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04052">
<title>The Five-Dollar Model: Generating Game Maps and Sprites from Sentence Embeddings. (arXiv:2308.04052v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.04052</link>
<description rdf:parseType="Literal">&lt;p&gt;The five-dollar model is a lightweight text-to-image generative architecture
that generates low dimensional images from an encoded text prompt. This model
can successfully generate accurate and aesthetically pleasing content in low
dimensional domains, with limited amounts of training data. Despite the small
size of both the model and datasets, the generated images are still able to
maintain the encoded semantic meaning of the textual prompt. We apply this
model to three small datasets: pixel art video game maps, video game sprite
images, and down-scaled emoji images and apply novel augmentation strategies to
improve the performance of our model on these limited datasets. We evaluate our
models performance using cosine similarity score between text-image pairs
generated by the CLIP VIT-B/32 model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merino_T/0/1/0/all/0/1&quot;&gt;Timothy Merino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Negri_R/0/1/0/all/0/1&quot;&gt;Roman Negri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajesh_D/0/1/0/all/0/1&quot;&gt;Dipika Rajesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Charity_M/0/1/0/all/0/1&quot;&gt;M Charity&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Togelius_J/0/1/0/all/0/1&quot;&gt;Julian Togelius&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04054">
<title>An Empirical Analysis of Range for 3D Object Detection. (arXiv:2308.04054v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04054</link>
<description rdf:parseType="Literal">&lt;p&gt;LiDAR-based 3D detection plays a vital role in autonomous navigation.
Surprisingly, although autonomous vehicles (AVs) must detect both near-field
objects (for collision avoidance) and far-field objects (for longer-term
planning), contemporary benchmarks focus only on near-field 3D detection.
However, AVs must detect far-field objects for safe navigation. In this paper,
we present an empirical analysis of far-field 3D detection using the long-range
detection dataset Argoverse 2.0 to better understand the problem, and share the
following insight: near-field LiDAR measurements are dense and optimally
encoded by small voxels, while far-field measurements are sparse and are better
encoded with large voxels. We exploit this observation to build a collection of
range experts tuned for near-vs-far field detection, and propose simple
techniques to efficiently ensemble models for long-range detection that improve
efficiency by 33% and boost accuracy by 3.2% CDS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peri_N/0/1/0/all/0/1&quot;&gt;Neehar Peri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mengtian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_B/0/1/0/all/0/1&quot;&gt;Benjamin Wilson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Xiong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hays_J/0/1/0/all/0/1&quot;&gt;James Hays&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1&quot;&gt;Deva Ramanan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04070">
<title>ConDistFL: Conditional Distillation for Federated Learning from Partially Annotated Data. (arXiv:2308.04070v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04070</link>
<description rdf:parseType="Literal">&lt;p&gt;Developing a generalized segmentation model capable of simultaneously
delineating multiple organs and diseases is highly desirable. Federated
learning (FL) is a key technology enabling the collaborative development of a
model without exchanging training data. However, the limited access to fully
annotated training data poses a major challenge to training generalizable
models. We propose &quot;ConDistFL&quot;, a framework to solve this problem by combining
FL with knowledge distillation. Local models can extract the knowledge of
unlabeled organs and tumors from partially annotated data from the global model
with an adequately designed conditional probability representation. We validate
our framework on four distinct partially annotated abdominal CT datasets from
the MSD and KiTS19 challenges. The experimental results show that the proposed
framework significantly outperforms FedAvg and FedOpt baselines. Moreover, the
performance on an external test dataset demonstrates superior generalizability
compared to models trained on each dataset separately. Our ablation study
suggests that ConDistFL can perform well without frequent aggregation, reducing
the communication cost of FL. Our implementation will be available at
https://github.com/NVIDIA/NVFlare/tree/dev/research/condist-fl.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Pochuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Chen Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weichung Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oda_M/0/1/0/all/0/1&quot;&gt;Masahiro Oda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fuh_C/0/1/0/all/0/1&quot;&gt;Chiou-Shann Fuh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mori_K/0/1/0/all/0/1&quot;&gt;Kensaku Mori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roth_H/0/1/0/all/0/1&quot;&gt;Holger R. Roth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04074">
<title>Exploiting Spatial-Temporal Context for Interacting Hand Reconstruction on Monocular RGB Video. (arXiv:2308.04074v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04074</link>
<description rdf:parseType="Literal">&lt;p&gt;Reconstructing interacting hands from monocular RGB data is a challenging
task, as it involves many interfering factors, e.g. self- and mutual occlusion
and similar textures. Previous works only leverage information from a single
RGB image without modeling their physically plausible relation, which leads to
inferior reconstruction results. In this work, we are dedicated to explicitly
exploiting spatial-temporal information to achieve better interacting hand
reconstruction. On one hand, we leverage temporal context to complement
insufficient information provided by the single frame, and design a novel
temporal framework with a temporal constraint for interacting hand motion
smoothness. On the other hand, we further propose an interpenetration detection
module to produce kinetically plausible interacting hands without physical
collisions. Extensive experiments are performed to validate the effectiveness
of our proposed framework, which achieves new state-of-the-art performance on
public benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Weichao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Hezhen Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wengang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+li_L/0/1/0/all/0/1&quot;&gt;Li li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Houqiang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04079">
<title>3D Gaussian Splatting for Real-Time Radiance Field Rendering. (arXiv:2308.04079v1 [cs.GR])</title>
<link>http://arxiv.org/abs/2308.04079</link>
<description rdf:parseType="Literal">&lt;p&gt;Radiance Field methods have recently revolutionized novel-view synthesis of
scenes captured with multiple photos or videos. However, achieving high visual
quality still requires neural networks that are costly to train and render,
while recent faster methods inevitably trade off speed for quality. For
unbounded and complete scenes (rather than isolated objects) and 1080p
resolution rendering, no current method can achieve real-time display rates. We
introduce three key elements that allow us to achieve state-of-the-art visual
quality while maintaining competitive training times and importantly allow
high-quality real-time (&amp;gt;= 30 fps) novel-view synthesis at 1080p resolution.
First, starting from sparse points produced during camera calibration, we
represent the scene with 3D Gaussians that preserve desirable properties of
continuous volumetric radiance fields for scene optimization while avoiding
unnecessary computation in empty space; Second, we perform interleaved
optimization/density control of the 3D Gaussians, notably optimizing
anisotropic covariance to achieve an accurate representation of the scene;
Third, we develop a fast visibility-aware rendering algorithm that supports
anisotropic splatting and both accelerates training and allows realtime
rendering. We demonstrate state-of-the-art visual quality and real-time
rendering on several established datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kerbl_B/0/1/0/all/0/1&quot;&gt;Bernhard Kerbl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kopanas_G/0/1/0/all/0/1&quot;&gt;Georgios Kopanas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leimkuhler_T/0/1/0/all/0/1&quot;&gt;Thomas Leimk&amp;#xfc;hler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Drettakis_G/0/1/0/all/0/1&quot;&gt;George Drettakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04091">
<title>From Unimodal to Multimodal: improving the sEMG-Based Pattern Recognition via deep generative models. (arXiv:2308.04091v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04091</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal hand gesture recognition (HGR) systems can achieve higher
recognition accuracy. However, acquiring multimodal gesture recognition data
typically requires users to wear additional sensors, thereby increasing
hardware costs. This paper proposes a novel generative approach to improve
Surface Electromyography (sEMG)-based HGR accuracy via virtual Inertial
Measurement Unit (IMU) signals. Specifically, we trained a deep generative
model based on the intrinsic correlation between forearm sEMG signals and
forearm IMU signals to generate virtual forearm IMU signals from the input
forearm sEMG signals at first. Subsequently, the sEMG signals and virtual IMU
signals were fed into a multimodal Convolutional Neural Network (CNN) model for
gesture recognition. To evaluate the performance of the proposed approach, we
conducted experiments on 6 databases, including 5 publicly available databases
and our collected database comprising 28 subjects performing 38 gestures,
containing both sEMG and IMU data. The results show that our proposed approach
outperforms the sEMG-based unimodal HGR method (with increases of
2.15%-13.10%). It demonstrates that incorporating virtual IMU signals,
generated by deep generative models, can significantly enhance the accuracy of
sEMG-based HGR. The proposed approach represents a successful attempt to
transition from unimodal HGR to multimodal HGR without additional sensor
hardware.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1&quot;&gt;Wentao Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1&quot;&gt;Linyan Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04118">
<title>Multimodal Color Recommendation in Vector Graphic Documents. (arXiv:2308.04118v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04118</link>
<description rdf:parseType="Literal">&lt;p&gt;Color selection plays a critical role in graphic document design and requires
sufficient consideration of various contexts. However, recommending appropriate
colors which harmonize with the other colors and textual contexts in documents
is a challenging task, even for experienced designers. In this study, we
propose a multimodal masked color model that integrates both color and textual
contexts to provide text-aware color recommendation for graphic documents. Our
proposed model comprises self-attention networks to capture the relationships
between colors in multiple palettes, and cross-attention networks that
incorporate both color and CLIP-based text representations. Our proposed method
primarily focuses on color palette completion, which recommends colors based on
the given colors and text. Additionally, it is applicable for another color
recommendation task, full palette generation, which generates a complete color
palette corresponding to the given text. Experimental results demonstrate that
our proposed approach surpasses previous color palette completion methods on
accuracy, color distribution, and user experience, as well as full palette
generation methods concerning color diversity and similarity to the ground
truth palettes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_Q/0/1/0/all/0/1&quot;&gt;Qianru Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xueting Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Otani_M/0/1/0/all/0/1&quot;&gt;Mayu Otani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04126">
<title>OmniDataComposer: A Unified Data Structure for Multimodal Data Fusion and Infinite Data Generation. (arXiv:2308.04126v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04126</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents OmniDataComposer, an innovative approach for multimodal
data fusion and unlimited data generation with an intent to refine and
uncomplicate interplay among diverse data modalities. Coming to the core
breakthrough, it introduces a cohesive data structure proficient in processing
and merging multimodal data inputs, which include video, audio, and text. Our
crafted algorithm leverages advancements across multiple operations such as
video/image caption extraction, dense caption extraction, Automatic Speech
Recognition (ASR), Optical Character Recognition (OCR), Recognize Anything
Model(RAM), and object tracking. OmniDataComposer is capable of identifying
over 6400 categories of objects, substantially broadening the spectrum of
visual information. It amalgamates these diverse modalities, promoting
reciprocal enhancement among modalities and facilitating cross-modal data
correction. \textbf{The final output metamorphoses each video input into an
elaborate sequential document}, virtually transmuting videos into thorough
narratives, making them easier to be processed by large language models. Future
prospects include optimizing datasets for each modality to encourage unlimited
data generation. This robust base will offer priceless insights to models like
ChatGPT, enabling them to create higher quality datasets for video captioning
and easing question-answering tasks based on video content. OmniDataComposer
inaugurates a new stage in multimodal learning, imparting enormous potential
for augmenting AI&apos;s understanding and generation of complex, real-world data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1&quot;&gt;Dongyang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shihao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yuan Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_W/0/1/0/all/0/1&quot;&gt;Wangpeng An&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04137">
<title>Comprehensive Assessment of the Performance of Deep Learning Classifiers Reveals a Surprising Lack of Robustness. (arXiv:2308.04137v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.04137</link>
<description rdf:parseType="Literal">&lt;p&gt;Reliable and robust evaluation methods are a necessary first step towards
developing machine learning models that are themselves robust and reliable.
Unfortunately, current evaluation protocols typically used to assess
classifiers fail to comprehensively evaluate performance as they tend to rely
on limited types of test data, and ignore others. For example, using the
standard test data fails to evaluate the predictions made by the classifier to
samples from classes it was not trained on. On the other hand, testing with
data containing samples from unknown classes fails to evaluate how well the
classifier can predict the labels for known classes. This article advocates
bench-marking performance using a wide range of different types of data and
using a single metric that can be applied to all such data types to produce a
consistent evaluation of performance. Using such a benchmark it is found that
current deep neural networks, including those trained with methods that are
believed to produce state-of-the-art robustness, are extremely vulnerable to
making mistakes on certain types of data. This means that such models will be
unreliable in real-world scenarios where they may encounter data from many
different domains, and that they are insecure as they can easily be fooled into
making the wrong decisions. It is hoped that these results will motivate the
wider adoption of more comprehensive testing methods that will, in turn, lead
to the development of more robust machine learning methods in the future.
&lt;/p&gt;
&lt;p&gt;Code is available at:
\url{https://codeberg.org/mwspratling/RobustnessEvaluation}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spratling_M/0/1/0/all/0/1&quot;&gt;Michael W. Spratling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04142">
<title>Class-level Structural Relation Modelling and Smoothing for Visual Representation Learning. (arXiv:2308.04142v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04142</link>
<description rdf:parseType="Literal">&lt;p&gt;Representation learning for images has been advanced by recent progress in
more complex neural models such as the Vision Transformers and new learning
theories such as the structural causal models. However, these models mainly
rely on the classification loss to implicitly regularize the class-level data
distributions, and they may face difficulties when handling classes with
diverse visual patterns. We argue that the incorporation of the structural
information between data samples may improve this situation. To achieve this
goal, this paper presents a framework termed \textbf{C}lass-level Structural
Relation Modeling and Smoothing for Visual Representation Learning (CSRMS),
which includes the Class-level Relation Modelling, Class-aware Graph Sampling,
and Relational Graph-Guided Representation Learning modules to model a
relational graph of the entire dataset and perform class-aware smoothing and
regularization operations to alleviate the issue of intra-class visual
diversity and inter-class similarity. Specifically, the Class-level Relation
Modelling module uses a clustering algorithm to learn the data distributions in
the feature space and identify three types of class-level sample relations for
the training set; Class-aware Graph Sampling module extends typical training
batch construction process with three strategies to sample dataset-level
sub-graphs; and Relational Graph-Guided Representation Learning module employs
a graph convolution network with knowledge-guided smoothing operations to ease
the projection from different visual patterns to the same class. Experiments
demonstrate the effectiveness of structured knowledge modelling for enhanced
representation learning and show that CSRMS can be incorporated with any
state-of-the-art visual representation learning models for performance gains.
The source codes and demos have been released at
https://github.com/czt117/CSRMS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zitan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1&quot;&gt;Zhuang Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xiao Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiangxian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1&quot;&gt;Xiangxu Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1&quot;&gt;Lei Meng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04151">
<title>Application for White Spot Syndrome Virus (WSSV) Monitoring using Edge Machine Learning. (arXiv:2308.04151v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04151</link>
<description rdf:parseType="Literal">&lt;p&gt;The aquaculture industry, strongly reliant on shrimp exports, faces
challenges due to viral infections like the White Spot Syndrome Virus (WSSV)
that severely impact output yields. In this context, computer vision can play a
significant role in identifying features not immediately evident to skilled or
untrained eyes, potentially reducing the time required to report WSSV
infections. In this study, the challenge of limited data for WSSV recognition
was addressed. A mobile application dedicated to data collection and monitoring
was developed to facilitate the creation of an image dataset to train a WSSV
recognition model and improve country-wide disease surveillance. The study also
includes a thorough analysis of WSSV recognition to address the challenge of
imbalanced learning and on-device inference. The models explored,
MobileNetV3-Small and EfficientNetV2-B0, gained an F1-Score of 0.72 and 0.99
respectively. The saliency heatmaps of both models were also observed to
uncover the &quot;black-box&quot; nature of these models and to gain insight as to what
features in the images are most important in making a prediction. These results
highlight the effectiveness and limitations of using models designed for
resource-constrained devices and balancing their performance in accurately
recognizing WSSV, providing valuable information and direction in the use of
computer vision in this domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Querol_L/0/1/0/all/0/1&quot;&gt;Lorenzo S. Querol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cordel_M/0/1/0/all/0/1&quot;&gt;Macario O. Cordel II&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rustia_D/0/1/0/all/0/1&quot;&gt;Dan Jeric A. Rustia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_M/0/1/0/all/0/1&quot;&gt;Mary Nia M. Santos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04152">
<title>Empowering Vision-Language Models to Follow Interleaved Vision-Language Instructions. (arXiv:2308.04152v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04152</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal Large Language Models (MLLMs) have recently sparked significant
interest, which demonstrates emergent capabilities to serve as a
general-purpose model for various vision-language tasks. However, existing
methods mainly focus on limited types of instructions with a single image as
visual context, which hinders the widespread availability of MLLMs. In this
paper, we introduce the I4 benchmark to comprehensively evaluate the
instruction following ability on complicated interleaved vision-language
instructions, which involve intricate image-text sequential context, covering a
diverse range of scenarios (e.g., visually-rich webpages/textbooks, lecture
slides, embodied dialogue). Systematic evaluation on our I4 benchmark reveals a
common defect of existing methods: the Visual Prompt Generator (VPG) trained on
image-captioning alignment objective tends to attend to common foreground
information for captioning but struggles to extract specific information
required by particular tasks. To address this issue, we propose a generic and
lightweight controllable knowledge re-injection module, which utilizes the
sophisticated reasoning ability of LLMs to control the VPG to conditionally
extract instruction-specific visual information and re-inject it into the LLM.
Further, we introduce an annotation-free cross-attention guided counterfactual
image training strategy to methodically learn the proposed module by
collaborating a cascade of foundation models. Enhanced by the proposed module
and training strategy, we present Cheetah, a MLLM that can effectively handle a
wide variety of interleaved vision-language instructions and achieves
state-of-the-art zero-shot performance across all tasks of I4, without
high-quality multimodal instruction tuning data. Moreover, Cheetah also
exhibits competitive performance compared with state-of-the-art instruction
tuned models on concurrent MME benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Juncheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_K/0/1/0/all/0/1&quot;&gt;Kaihang Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1&quot;&gt;Zhiqi Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1&quot;&gt;Minghe Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hanwang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1&quot;&gt;Wei Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenqiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1&quot;&gt;Tat-Seng Chua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Siliang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1&quot;&gt;Yueting Zhuang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04156">
<title>Towards Top-Down Stereoscopic Image Quality Assessment via Stereo Attention. (arXiv:2308.04156v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04156</link>
<description rdf:parseType="Literal">&lt;p&gt;Stereoscopic image quality assessment (SIQA) plays a crucial role in
evaluating and improving the visual experience of 3D content. Existing
binocular properties and attention-based methods for SIQA have achieved
promising performance. However, these bottom-up approaches are inadequate in
exploiting the inherent characteristics of the human visual system (HVS). This
paper presents a novel network for SIQA via stereo attention, employing a
top-down perspective to guide the quality assessment process. Our proposed
method realizes the guidance from high-level binocular signals down to
low-level monocular signals, while the binocular and monocular information can
be calibrated progressively throughout the processing pipeline. We design a
generalized Stereo AttenTion (SAT) block to implement the top-down philosophy
in stereo perception. This block utilizes the fusion-generated attention map as
a high-level binocular modulator, influencing the representation of two
low-level monocular features. Additionally, we introduce an Energy Coefficient
(EC) to account for recent findings indicating that binocular responses in the
primate primary visual cortex are less than the sum of monocular responses. The
adaptive EC can tune the magnitude of binocular response flexibly, thus
enhancing the formation of robust binocular features within our framework. To
extract the most discriminative quality information from the summation and
subtraction of the two branches of monocular features, we utilize a
dual-pooling strategy that applies min-pooling and max-pooling operations to
the respective branches. Experimental results highlight the superiority of our
top-down method in simulating the property of visual perception and advancing
the state-of-the-art in the SIQA field. The code of this work is available at
https://github.com/Fanning-Zhang/SATNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huilin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sumei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1&quot;&gt;Yongli Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04162">
<title>EPCFormer: Expression Prompt Collaboration Transformer for Universal Referring Video Object Segmentation. (arXiv:2308.04162v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04162</link>
<description rdf:parseType="Literal">&lt;p&gt;Audio-guided Video Object Segmentation (A-VOS) and Referring Video Object
Segmentation (R-VOS) are two highly-related tasks, which both aim to segment
specific objects from video sequences according to user-provided expression
prompts. However, due to the challenges in modeling representations for
different modalities, contemporary methods struggle to strike a balance between
interaction flexibility and high-precision localization and segmentation. In
this paper, we address this problem from two perspectives: the alignment
representation of audio and text and the deep interaction among audio, text,
and visual features. First, we propose a universal architecture, the Expression
Prompt Collaboration Transformer, herein EPCFormer. Next, we propose an
Expression Alignment (EA) mechanism for audio and text expressions. By
introducing contrastive learning for audio and text expressions, the proposed
EPCFormer realizes comprehension of the semantic equivalence between audio and
text expressions denoting the same objects. Then, to facilitate deep
interactions among audio, text, and video features, we introduce an
Expression-Visual Attention (EVA) mechanism. The knowledge of video object
segmentation in terms of the expression prompts can seamlessly transfer between
the two tasks by deeply exploring complementary cues between text and audio.
Experiments on well-recognized benchmarks demonstrate that our universal
EPCFormer attains state-of-the-art results on both tasks. The source code of
EPCFormer will be made publicly available at
https://github.com/lab206/EPCFormer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiajun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jiacheng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Haolong Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nai_K/0/1/0/all/0/1&quot;&gt;Ke Nai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kailun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04163">
<title>Under-Display Camera Image Restoration with Scattering Effect. (arXiv:2308.04163v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04163</link>
<description rdf:parseType="Literal">&lt;p&gt;The under-display camera (UDC) provides consumers with a full-screen visual
experience without any obstruction due to notches or punched holes. However,
the semi-transparent nature of the display inevitably introduces the severe
degradation into UDC images. In this work, we address the UDC image restoration
problem with the specific consideration of the scattering effect caused by the
display. We explicitly model the scattering effect by treating the display as a
piece of homogeneous scattering medium. With the physical model of the
scattering effect, we improve the image formation pipeline for the image
synthesis to construct a realistic UDC dataset with ground truths. To suppress
the scattering effect for the eventual UDC image recovery, a two-branch
restoration network is designed. More specifically, the scattering branch
leverages global modeling capabilities of the channel-wise self-attention to
estimate parameters of the scattering effect from degraded images. While the
image branch exploits the local representation advantage of CNN to recover
clear scenes, implicitly guided by the scattering branch. Extensive experiments
are conducted on both real-world and synthesized data, demonstrating the
superiority of the proposed method over the state-of-the-art UDC restoration
techniques. The source code and dataset are available at
\url{https://github.com/NamecantbeNULL/SRUDC}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1&quot;&gt;Binbin Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiangyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shuning Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jiantao Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04168">
<title>EFaR 2023: Efficient Face Recognition Competition. (arXiv:2308.04168v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04168</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents the summary of the Efficient Face Recognition Competition
(EFaR) held at the 2023 International Joint Conference on Biometrics (IJCB
2023). The competition received 17 submissions from 6 different teams. To drive
further development of efficient face recognition models, the submitted
solutions are ranked based on a weighted score of the achieved verification
accuracies on a diverse set of benchmarks, as well as the deployability given
by the number of floating-point operations and model size. The evaluation of
submissions is extended to bias, cross-quality, and large-scale recognition
benchmarks. Overall, the paper gives an overview of the achieved performance
values of the submitted solutions as well as a diverse set of baselines. The
submitted solutions use small, efficient network architectures to reduce the
computational cost, some solutions apply model quantization. An outlook on
possible techniques that are underrepresented in current solutions is given as
well.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolf_J/0/1/0/all/0/1&quot;&gt;Jan Niklas Kolf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boutros_F/0/1/0/all/0/1&quot;&gt;Fadi Boutros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elliesen_J/0/1/0/all/0/1&quot;&gt;Jurek Elliesen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theuerkauf_M/0/1/0/all/0/1&quot;&gt;Markus Theuerkauf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1&quot;&gt;Naser Damer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alansari_M/0/1/0/all/0/1&quot;&gt;Mohamad Alansari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hay_O/0/1/0/all/0/1&quot;&gt;Oussama Abdul Hay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alansari_S/0/1/0/all/0/1&quot;&gt;Sara Alansari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Javed_S/0/1/0/all/0/1&quot;&gt;Sajid Javed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Werghi_N/0/1/0/all/0/1&quot;&gt;Naoufel Werghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grm_K/0/1/0/all/0/1&quot;&gt;Klemen Grm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Struc_V/0/1/0/all/0/1&quot;&gt;Vitomir &amp;#x160;truc&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alonso_Fernandez_F/0/1/0/all/0/1&quot;&gt;Fernando Alonso-Fernandez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diaz_K/0/1/0/all/0/1&quot;&gt;Kevin Hernandez Diaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bigun_J/0/1/0/all/0/1&quot;&gt;Josef Bigun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+George_A/0/1/0/all/0/1&quot;&gt;Anjith George&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ecabert_C/0/1/0/all/0/1&quot;&gt;Christophe Ecabert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahreza_H/0/1/0/all/0/1&quot;&gt;Hatef Otroshi Shahreza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kotwal_K/0/1/0/all/0/1&quot;&gt;Ketan Kotwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marcel_S/0/1/0/all/0/1&quot;&gt;S&amp;#xe9;bastien Marcel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Medvedev_I/0/1/0/all/0/1&quot;&gt;Iurii Medvedev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1&quot;&gt;Bo Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nunes_D/0/1/0/all/0/1&quot;&gt;Diogo Nunes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassanpour_A/0/1/0/all/0/1&quot;&gt;Ahmad Hassanpour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khatiwada_P/0/1/0/all/0/1&quot;&gt;Pankaj Khatiwada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toor_A/0/1/0/all/0/1&quot;&gt;Aafan Ahmad Toor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bian Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04177">
<title>How Generalizable are Deepfake Detectors? An Empirical Study. (arXiv:2308.04177v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04177</link>
<description rdf:parseType="Literal">&lt;p&gt;Deepfake videos and images are becoming increasingly credible, posing a
significant threat given their potential to facilitate fraud or bypass access
control systems. This has motivated the development of deepfake detection
methods, in which deep learning models are trained to distinguish between real
and synthesized footage. Unfortunately, existing detection models struggle to
generalize to deepfakes from datasets they were not trained on, but little work
has been done to examine why or how this limitation can be addressed. In this
paper, we present the first empirical study on the generalizability of deepfake
detectors, an essential goal for detectors to stay one step ahead of attackers.
Our study utilizes six deepfake datasets, five deepfake detection methods, and
two model augmentation approaches, confirming that detectors do not generalize
in zero-shot settings. Additionally, we find that detectors are learning
unwanted properties specific to synthesis methods and struggling to extract
discriminative features, limiting their ability to generalize. Finally, we find
that there are neurons universally contributing to detection across seen and
unseen datasets, illuminating a possible path forward to zero-shot
generalizability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Boquan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jun Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poskitt_C/0/1/0/all/0/1&quot;&gt;Christopher M. Poskitt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04188">
<title>Image Copy-Move Forgery Detection via Deep Cross-Scale PatchMatch. (arXiv:2308.04188v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04188</link>
<description rdf:parseType="Literal">&lt;p&gt;The recently developed deep algorithms achieve promising progress in the
field of image copy-move forgery detection (CMFD). However, they have limited
generalizability in some practical scenarios, where the copy-move objects may
not appear in the training images or cloned regions are from the background. To
address the above issues, in this work, we propose a novel end-to-end CMFD
framework by integrating merits from both conventional and deep methods.
Specifically, we design a deep cross-scale patchmatch method tailored for CMFD
to localize copy-move regions. In contrast to existing deep models, our scheme
aims to seek explicit and reliable point-to-point matching between source and
target regions using features extracted from high-resolution scales. Further,
we develop a manipulation region location branch for source/target separation.
The proposed CMFD framework is completely differentiable and can be trained in
an end-to-end manner. Extensive experimental results demonstrate the high
generalizability of our method to different copy-move contents, and the
proposed scheme achieves significantly better performance than existing
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yingjie He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanman Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Changsheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xia Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04197">
<title>D3G: Exploring Gaussian Prior for Temporal Sentence Grounding with Glance Annotation. (arXiv:2308.04197v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04197</link>
<description rdf:parseType="Literal">&lt;p&gt;Temporal sentence grounding (TSG) aims to locate a specific moment from an
untrimmed video with a given natural language query. Recently, weakly
supervised methods still have a large performance gap compared to fully
supervised ones, while the latter requires laborious timestamp annotations. In
this study, we aim to reduce the annotation cost yet keep competitive
performance for TSG task compared to fully supervised ones. To achieve this
goal, we investigate a recently proposed glance-supervised temporal sentence
grounding task, which requires only single frame annotation (referred to as
glance annotation) for each query. Under this setup, we propose a Dynamic
Gaussian prior based Grounding framework with Glance annotation (D3G), which
consists of a Semantic Alignment Group Contrastive Learning module (SA-GCL) and
a Dynamic Gaussian prior Adjustment module (DGA). Specifically, SA-GCL samples
reliable positive moments from a 2D temporal map via jointly leveraging
Gaussian prior and semantic consistency, which contributes to aligning the
positive sentence-moment pairs in the joint embedding space. Moreover, to
alleviate the annotation bias resulting from glance annotation and model
complex queries consisting of multiple events, we propose the DGA module, which
adjusts the distribution dynamically to approximate the ground truth of target
moments. Extensive experiments on three challenging benchmarks verify the
effectiveness of the proposed D3G. It outperforms the state-of-the-art weakly
supervised methods by a large margin and narrows the performance gap compared
to fully supervised methods. Code is available at
https://github.com/solicucu/D3G.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hanjun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_X/0/1/0/all/0/1&quot;&gt;Xiujun Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1&quot;&gt;Sunan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_R/0/1/0/all/0/1&quot;&gt;Ruizhi Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1&quot;&gt;Wei Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1&quot;&gt;Taian Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_B/0/1/0/all/0/1&quot;&gt;Bei Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xing Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04206">
<title>Exploring Transformers for Open-world Instance Segmentation. (arXiv:2308.04206v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04206</link>
<description rdf:parseType="Literal">&lt;p&gt;Open-world instance segmentation is a rising task, which aims to segment all
objects in the image by learning from a limited number of base-category
objects. This task is challenging, as the number of unseen categories could be
hundreds of times larger than that of seen categories. Recently, the DETR-like
models have been extensively studied in the closed world while stay unexplored
in the open world. In this paper, we utilize the Transformer for open-world
instance segmentation and present SWORD. Firstly, we introduce to attach the
stop-gradient operation before classification head and further add IoU heads
for discovering novel objects. We demonstrate that a simple stop-gradient
operation not only prevents the novel objects from being suppressed as
background, but also allows the network to enjoy the merit of heuristic label
assignment. Secondly, we propose a novel contrastive learning framework to
enlarge the representations between objects and background. Specifically, we
maintain a universal object queue to obtain the object center, and dynamically
select positive and negative samples from the object queries for contrastive
learning. While the previous works only focus on pursuing average recall and
neglect average precision, we show the prominence of SWORD by giving
consideration to both criteria. Our models achieve state-of-the-art performance
in various open-world cross-category and cross-dataset generalizations.
Particularly, in VOC to non-VOC setup, our method sets new state-of-the-art
results of 40.0% on ARb100 and 34.9% on ARm100. For COCO to UVO generalization,
SWORD significantly outperforms the previous best open-world model by 5.9% on
APm and 8.1% on ARm100.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiannan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yi Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1&quot;&gt;Bin Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Huchuan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1&quot;&gt;Zehuan Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04207">
<title>Robust retrieval of material chemical states in X-ray microspectroscopy. (arXiv:2308.04207v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04207</link>
<description rdf:parseType="Literal">&lt;p&gt;X-ray microspectroscopic techniques are essential for studying morphological
and chemical changes in materials, providing high-resolution structural and
spectroscopic information. However, its practical data analysis for reliably
retrieving the chemical states remains a major obstacle to accelerating the
fundamental understanding of materials in many research fields. In this work,
we propose a novel data formulation model for X-ray microspectroscopy and
develop a dedicated unmixing framework to solve this problem, which is robust
to noise and spectral variability. Moreover, this framework is not limited to
the analysis of two-state material chemistry, making it an effective
alternative to conventional and widely-used methods. In addition, an
alternative directional multiplier method with provable convergence is applied
to obtain the solution efficiently. Our framework can accurately identify and
characterize chemical states in complex and heterogeneous samples, even under
challenging conditions such as low signal-to-noise ratios and overlapping
spectral features. Extensive experimental results on simulated and real
datasets demonstrate its effectiveness and reliability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Ting Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaotong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jizhou Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04218">
<title>AquaSAM: Underwater Image Foreground Segmentation. (arXiv:2308.04218v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04218</link>
<description rdf:parseType="Literal">&lt;p&gt;The Segment Anything Model (SAM) has revolutionized natural image
segmentation, nevertheless, its performance on underwater images is still
restricted. This work presents AquaSAM, the first attempt to extend the success
of SAM on underwater images with the purpose of creating a versatile method for
the segmentation of various underwater targets. To achieve this, we begin by
classifying and extracting various labels automatically in SUIM dataset.
Subsequently, we develop a straightforward fine-tuning method to adapt SAM to
general foreground underwater image segmentation. Through extensive experiments
involving eight segmentation tasks like human divers, we demonstrate that
AquaSAM outperforms the default SAM model especially at hard tasks like coral
reefs. AquaSAM achieves an average Dice Similarity Coefficient (DSC) of 7.13
(%) improvement and an average of 8.27 (%) on mIoU improvement in underwater
segmentation tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Muduo Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1&quot;&gt;Jianhao Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yutao Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04224">
<title>Will your Doorbell Camera still recognize you as you grow old. (arXiv:2308.04224v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04224</link>
<description rdf:parseType="Literal">&lt;p&gt;Robust authentication for low-power consumer devices such as doorbell cameras
poses a valuable and unique challenge. This work explores the effect of age and
aging on the performance of facial authentication methods. Two public age
datasets, AgeDB and Morph-II have been used as baselines in this work. A
photo-realistic age transformation method has been employed to augment a set of
high-quality facial images with various age effects. Then the effect of these
synthetic aging data on the high-performance deep-learning-based face
recognition model is quantified by using various metrics including Receiver
Operating Characteristic (ROC) curves and match score distributions.
Experimental results demonstrate that long-term age effects are still a
significant challenge for the state-of-the-art facial authentication method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1&quot;&gt;Wang Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farooq_M/0/1/0/all/0/1&quot;&gt;Muhammad Ali Farooq&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lemley_J/0/1/0/all/0/1&quot;&gt;Joseph Lemley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corcoran_P/0/1/0/all/0/1&quot;&gt;Peter Corcoran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04232">
<title>A Comparative Study of Image-to-Image Translation Using GANs for Synthetic Child Race Data. (arXiv:2308.04232v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04232</link>
<description rdf:parseType="Literal">&lt;p&gt;The lack of ethnic diversity in data has been a limiting factor of face
recognition techniques in the literature. This is particularly the case for
children where data samples are scarce and presents a challenge when seeking to
adapt machine vision algorithms that are trained on adult data to work on
children. This work proposes the utilization of image-to-image transformation
to synthesize data of different races and thus adjust the ethnicity of
children&apos;s face data. We consider ethnicity as a style and compare three
different Image-to-Image neural network based methods, specifically pix2pix,
CycleGAN, and CUT networks to implement Caucasian child data and Asian child
data conversion. Experimental validation results on synthetic data demonstrate
the feasibility of using image-to-image transformation methods to generate
various synthetic child data samples with broader ethnic diversity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1&quot;&gt;Wang Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farooq_M/0/1/0/all/0/1&quot;&gt;Muhammad Ali Farooq&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lemley_J/0/1/0/all/0/1&quot;&gt;Joseph Lemley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corcoran_P/0/1/0/all/0/1&quot;&gt;Peter Corcoran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04243">
<title>AICSD: Adaptive Inter-Class Similarity Distillation for Semantic Segmentation. (arXiv:2308.04243v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04243</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, deep neural networks have achieved remarkable accuracy in
computer vision tasks. With inference time being a crucial factor, particularly
in dense prediction tasks such as semantic segmentation, knowledge distillation
has emerged as a successful technique for improving the accuracy of lightweight
student networks. The existing methods often neglect the information in
channels and among different classes. To overcome these limitations, this paper
proposes a novel method called Inter-Class Similarity Distillation (ICSD) for
the purpose of knowledge distillation. The proposed method transfers high-order
relations from the teacher network to the student network by independently
computing intra-class distributions for each class from network outputs. This
is followed by calculating inter-class similarity matrices for distillation
using KL divergence between distributions of each pair of classes. To further
improve the effectiveness of the proposed method, an Adaptive Loss Weighting
(ALW) training strategy is proposed. Unlike existing methods, the ALW strategy
gradually reduces the influence of the teacher network towards the end of
training process to account for errors in teacher&apos;s predictions. Extensive
experiments conducted on two well-known datasets for semantic segmentation,
Cityscapes and Pascal VOC 2012, validate the effectiveness of the proposed
method in terms of mIoU and pixel accuracy. The proposed method outperforms
most of existing knowledge distillation methods as demonstrated by both
quantitative and qualitative evaluations. Code is available at:
https://github.com/AmirMansurian/AICSD
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mansourian_A/0/1/0/all/0/1&quot;&gt;Amir M. Mansourian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmadi_R/0/1/0/all/0/1&quot;&gt;Rozhan Ahmadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasaei_S/0/1/0/all/0/1&quot;&gt;Shohreh Kasaei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04249">
<title>MindDiffuser: Controlled Image Reconstruction from Human Brain Activity with Semantic and Structural Diffusion. (arXiv:2308.04249v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04249</link>
<description rdf:parseType="Literal">&lt;p&gt;Reconstructing visual stimuli from brain recordings has been a meaningful and
challenging task. Especially, the achievement of precise and controllable image
reconstruction bears great significance in propelling the progress and
utilization of brain-computer interfaces. Despite the advancements in complex
image reconstruction techniques, the challenge persists in achieving a cohesive
alignment of both semantic (concepts and objects) and structure (position,
orientation, and size) with the image stimuli. To address the aforementioned
issue, we propose a two-stage image reconstruction model called MindDiffuser.
In Stage 1, the VQ-VAE latent representations and the CLIP text embeddings
decoded from fMRI are put into Stable Diffusion, which yields a preliminary
image that contains semantic information. In Stage 2, we utilize the CLIP
visual feature decoded from fMRI as supervisory information, and continually
adjust the two feature vectors decoded in Stage 1 through backpropagation to
align the structural information. The results of both qualitative and
quantitative analyses demonstrate that our model has surpassed the current
state-of-the-art models on Natural Scenes Dataset (NSD). The subsequent
experimental findings corroborate the neurobiological plausibility of the
model, as evidenced by the interpretability of the multimodal feature employed,
which align with the corresponding brain responses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yizhuo Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1&quot;&gt;Changde Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+zhou_Q/0/1/0/all/0/1&quot;&gt;Qiongyi zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dianpeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;Huiguang He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04252">
<title>Blur aware metric depth estimation with multi-focus plenoptic cameras. (arXiv:2308.04252v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.04252</link>
<description rdf:parseType="Literal">&lt;p&gt;While a traditional camera only captures one point of view of a scene, a
plenoptic or light-field camera, is able to capture spatial and angular
information in a single snapshot, enabling depth estimation from a single
acquisition. In this paper, we present a new metric depth estimation algorithm
using only raw images from a multi-focus plenoptic camera. The proposed
approach is especially suited for the multi-focus configuration where several
micro-lenses with different focal lengths are used. The main goal of our blur
aware depth estimation (BLADE) approach is to improve disparity estimation for
defocus stereo images by integrating both correspondence and defocus cues. We
thus leverage blur information where it was previously considered a drawback.
We explicitly derive an inverse projection model including the defocus blur
providing depth estimates up to a scale factor. A method to calibrate the
inverse model is then proposed. We thus take into account depth scaling to
achieve precise and accurate metric depth estimates. Our results show that
introducing defocus cues improves the depth estimation. We demonstrate the
effectiveness of our framework and depth scaling calibration on relative depth
estimation setups and on real-world 3D complex scenes with ground truth
acquired with a 3D lidar scanner.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Labussiere_M/0/1/0/all/0/1&quot;&gt;Mathieu Labussi&amp;#xe8;re&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Teuliere_C/0/1/0/all/0/1&quot;&gt;C&amp;#xe9;line Teuli&amp;#xe8;re&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ait_Aider_O/0/1/0/all/0/1&quot;&gt;Omar Ait-Aider&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04262">
<title>SDLFormer: A Sparse and Dense Locality-enhanced Transformer for Accelerated MR Image Reconstruction. (arXiv:2308.04262v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.04262</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformers have emerged as viable alternatives to convolutional neural
networks owing to their ability to learn non-local region relationships in the
spatial domain. The self-attention mechanism of the transformer enables
transformers to capture long-range dependencies in the images, which might be
desirable for accelerated MRI image reconstruction as the effect of
undersampling is non-local in the image domain. Despite its computational
efficiency, the window-based transformers suffer from restricted receptive
fields as the dependencies are limited to within the scope of the image
windows. We propose a window-based transformer network that integrates dilated
attention mechanism and convolution for accelerated MRI image reconstruction.
The proposed network consists of dilated and dense neighborhood attention
transformers to enhance the distant neighborhood pixel relationship and
introduce depth-wise convolutions within the transformer module to learn
low-level translation invariant features for accelerated MRI image
reconstruction. The proposed model is trained in a self-supervised manner. We
perform extensive experiments for multi-coil MRI acceleration for coronal PD,
coronal PDFS and axial T2 contrasts with 4x and 5x under-sampling in
self-supervised learning based on k-space splitting. We compare our method
against other reconstruction architectures and the parallel domain
self-supervised learning baseline. Results show that the proposed model
exhibits improvement margins of (i) around 1.40 dB in PSNR and around 0.028 in
SSIM on average over other architectures (ii) around 1.44 dB in PSNR and around
0.029 in SSIM over parallel domain self-supervised learning. The code is
available at https://github.com/rahul-gs-16/sdlformer.git
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+S%2E_R/0/1/0/all/0/1&quot;&gt;Rahul G.S.&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ramnarayanan_S/0/1/0/all/0/1&quot;&gt;Sriprabha Ramnarayanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fahim_M/0/1/0/all/0/1&quot;&gt;Mohammad Al Fahim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ram_K/0/1/0/all/0/1&quot;&gt;Keerthi Ram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+P_P/0/1/0/all/0/1&quot;&gt;Preejith S.P&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sivaprakasam_M/0/1/0/all/0/1&quot;&gt;Mohanasankar Sivaprakasam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04269">
<title>Lossy and Lossless (L$^2$) Post-training Model Size Compression. (arXiv:2308.04269v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04269</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks have delivered remarkable performance and have been
widely used in various visual tasks. However, their huge size causes
significant inconvenience for transmission and storage. Many previous studies
have explored model size compression. However, these studies often approach
various lossy and lossless compression methods in isolation, leading to
challenges in achieving high compression ratios efficiently. This work proposes
a post-training model size compression method that combines lossy and lossless
compression in a unified way. We first propose a unified parametric weight
transformation, which ensures different lossy compression methods can be
performed jointly in a post-training manner. Then, a dedicated differentiable
counter is introduced to guide the optimization of lossy compression to arrive
at a more suitable point for later lossless compression. Additionally, our
method can easily control a desired global compression ratio and allocate
adaptive ratios for different layers. Finally, our method can achieve a stable
$10\times$ compression ratio without sacrificing accuracy and a $20\times$
compression ratio with minor accuracy loss in a short time. Our code is
available at https://github.com/ModelTC/L2_Compression .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yumeng Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1&quot;&gt;Shihao Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xiuying Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1&quot;&gt;Ruihao Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jianlei Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04283">
<title>Vision-Based Autonomous Navigation for Unmanned Surface Vessel in Extreme Marine Conditions. (arXiv:2308.04283v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04283</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual perception is an important component for autonomous navigation of
unmanned surface vessels (USV), particularly for the tasks related to
autonomous inspection and tracking. These tasks involve vision-based navigation
techniques to identify the target for navigation. Reduced visibility under
extreme weather conditions in marine environments makes it difficult for
vision-based approaches to work properly. To overcome these issues, this paper
presents an autonomous vision-based navigation framework for tracking target
objects in extreme marine conditions. The proposed framework consists of an
integrated perception pipeline that uses a generative adversarial network (GAN)
to remove noise and highlight the object features before passing them to the
object detector (i.e., YOLOv5). The detected visual features are then used by
the USV to track the target. The proposed framework has been thoroughly tested
in simulation under extremely reduced visibility due to sandstorms and fog. The
results are compared with state-of-the-art de-hazing methods across the
benchmarked MBZIRC simulation dataset, on which the proposed scheme has
outperformed the existing methods across various metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_M/0/1/0/all/0/1&quot;&gt;Muhayyuddin Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bakht_A/0/1/0/all/0/1&quot;&gt;Ahsan Baidar Bakht&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassan_T/0/1/0/all/0/1&quot;&gt;Taimur Hassan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akram_W/0/1/0/all/0/1&quot;&gt;Waseem Akram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Humais_A/0/1/0/all/0/1&quot;&gt;Ahmed Humais&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seneviratne_L/0/1/0/all/0/1&quot;&gt;Lakmal Seneviratne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1&quot;&gt;Shaoming He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Defu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hussain_I/0/1/0/all/0/1&quot;&gt;Irfan Hussain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04288">
<title>Cloth2Tex: A Customized Cloth Texture Generation Pipeline for 3D Virtual Try-On. (arXiv:2308.04288v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04288</link>
<description rdf:parseType="Literal">&lt;p&gt;Fabricating and designing 3D garments has become extremely demanding with the
increasing need for synthesizing realistic dressed persons for a variety of
applications, e.g. 3D virtual try-on, digitalization of 2D clothes into 3D
apparel, and cloth animation. It thus necessitates a simple and straightforward
pipeline to obtain high-quality texture from simple input, such as 2D reference
images. Since traditional warping-based texture generation methods require a
significant number of control points to be manually selected for each type of
garment, which can be a time-consuming and tedious process. We propose a novel
method, called Cloth2Tex, which eliminates the human burden in this process.
Cloth2Tex is a self-supervised method that generates texture maps with
reasonable layout and structural consistency. Another key feature of Cloth2Tex
is that it can be used to support high-fidelity texture inpainting. This is
done by combining Cloth2Tex with a prevailing latent diffusion model. We
evaluate our approach both qualitatively and quantitatively and demonstrate
that Cloth2Tex can generate high-quality texture maps and achieve the best
visual effects in comparison to other methods. Project page:
tomguluson92.github.io/projects/cloth2tex/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1&quot;&gt;Daiheng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xindi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1&quot;&gt;Ke Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1&quot;&gt;Liefeng Bo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qixing Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04303">
<title>Vehicle Motion Forecasting using Prior Information and Semantic-assisted Occupancy Grid Maps. (arXiv:2308.04303v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04303</link>
<description rdf:parseType="Literal">&lt;p&gt;Motion prediction is a challenging task for autonomous vehicles due to
uncertainty in the sensor data, the non-deterministic nature of future, and
complex behavior of agents. In this paper, we tackle this problem by
representing the scene as dynamic occupancy grid maps (DOGMs), associating
semantic labels to the occupied cells and incorporating map information. We
propose a novel framework that combines deep-learning-based spatio-temporal and
probabilistic approaches to predict vehicle behaviors.Contrary to the
conventional OGM prediction methods, evaluation of our work is conducted
against the ground truth annotations. We experiment and validate our results on
real-world NuScenes dataset and show that our model shows superior ability to
predict both static and dynamic vehicles compared to OGM predictions.
Furthermore, we perform an ablation study and assess the role of semantic
labels and map in the architecture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asghar_R/0/1/0/all/0/1&quot;&gt;Rabbia Asghar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diaz_Zapata_M/0/1/0/all/0/1&quot;&gt;Manuel Diaz-Zapata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rummelhard_L/0/1/0/all/0/1&quot;&gt;Lukas Rummelhard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spalanzani_A/0/1/0/all/0/1&quot;&gt;Anne Spalanzani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laugier_C/0/1/0/all/0/1&quot;&gt;Christian Laugier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04321">
<title>All-pairs Consistency Learning for Weakly Supervised Semantic Segmentation. (arXiv:2308.04321v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04321</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we propose a new transformer-based regularization to better
localize objects for Weakly supervised semantic segmentation (WSSS). In
image-level WSSS, Class Activation Map (CAM) is adopted to generate object
localization as pseudo segmentation labels. To address the partial activation
issue of the CAMs, consistency regularization is employed to maintain
activation intensity invariance across various image augmentations. However,
such methods ignore pair-wise relations among regions within each CAM, which
capture context and should also be invariant across image views. To this end,
we propose a new all-pairs consistency regularization (ACR). Given a pair of
augmented views, our approach regularizes the activation intensities between a
pair of augmented views, while also ensuring that the affinity across regions
within each view remains consistent. We adopt vision transformers as the
self-attention mechanism naturally embeds pair-wise affinity. This enables us
to simply regularize the distance between the attention matrices of augmented
image pairs. Additionally, we introduce a novel class-wise localization method
that leverages the gradients of the class token. Our method can be seamlessly
integrated into existing WSSS methods using transformers without modifying the
architectures. We evaluate our method on PASCAL VOC and MS COCO datasets. Our
method produces noticeably better class localization maps (67.3% mIoU on PASCAL
VOC train), resulting in superior WSSS performances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Weixuan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yanhao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1&quot;&gt;Zhen Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zheyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1&quot;&gt;Lin Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fanyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yiran Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1&quot;&gt;Nick Barnes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04322">
<title>Domain Adaptive Person Search via GAN-based Scene Synthesis for Cross-scene Videos. (arXiv:2308.04322v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04322</link>
<description rdf:parseType="Literal">&lt;p&gt;Person search has recently been a challenging task in the computer vision
domain, which aims to search specific pedestrians from real
cameras.Nevertheless, most surveillance videos comprise only a handful of
images of each pedestrian, which often feature identical backgrounds and
clothing. Hence, it is difficult to learn more discriminative features for
person search in real scenes. To tackle this challenge, we draw on Generative
Adversarial Networks (GAN) to synthesize data from surveillance videos. GAN has
thrived in computer vision problems because it produces high-quality images
efficiently. We merely alter the popular Fast R-CNN model, which is capable of
processing videos and yielding accurate detection outcomes. In order to
appropriately relieve the pressure brought by the two-stage model, we design an
Assisted-Identity Query Module (AIDQ) to provide positive images for the behind
part. Besides, the proposed novel GAN-based Scene Synthesis model that can
synthesize high-quality cross-id person images for person search tasks. In
order to facilitate the feature learning of the GAN-based Scene Synthesis
model, we adopt an online learning strategy that collaboratively learns the
synthesized images and original images. Extensive experiments on two widely
used person search benchmarks, CUHK-SYSU and PRW, have shown that our method
has achieved great performance, and the extensive ablation study further
justifies our GAN-synthetic data can effectively increase the variability of
the datasets and be more realistic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Huibing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_T/0/1/0/all/0/1&quot;&gt;Tianxiang Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_M/0/1/0/all/0/1&quot;&gt;Mingze Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_H/0/1/0/all/0/1&quot;&gt;Huijuan Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yushan Du&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04337">
<title>Pengembangan Model untuk Mendeteksi Kerusakan pada Terumbu Karang dengan Klasifikasi Citra. (arXiv:2308.04337v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04337</link>
<description rdf:parseType="Literal">&lt;p&gt;The abundant biodiversity of coral reefs in Indonesian waters is a valuable
asset that needs to be preserved. Rapid climate change and uncontrolled human
activities have led to the degradation of coral reef ecosystems, including
coral bleaching, which is a critical indicator of coral health conditions.
Therefore, this research aims to develop an accurate classification model to
distinguish between healthy corals and corals experiencing bleaching. This
study utilizes a specialized dataset consisting of 923 images collected from
Flickr using the Flickr API. The dataset comprises two distinct classes:
healthy corals (438 images) and bleached corals (485 images). These images have
been resized to a maximum of 300 pixels in width or height, whichever is
larger, to maintain consistent sizes across the dataset.
&lt;/p&gt;
&lt;p&gt;The method employed in this research involves the use of machine learning
models, particularly convolutional neural networks (CNN), to recognize and
differentiate visual patterns associated with healthy and bleached corals. In
this context, the dataset can be used to train and test various classification
models to achieve optimal results. By leveraging the ResNet model, it was found
that a from-scratch ResNet model can outperform pretrained models in terms of
precision and accuracy. The success in developing accurate classification
models will greatly benefit researchers and marine biologists in gaining a
better understanding of coral reef health. These models can also be employed to
monitor changes in the coral reef environment, thereby making a significant
contribution to conservation and ecosystem restoration efforts that have
far-reaching impacts on life.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muhammad_F/0/1/0/all/0/1&quot;&gt;Fadhil Muhammad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elfandra_A/0/1/0/all/0/1&quot;&gt;Alif Bintang Elfandra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amin_I/0/1/0/all/0/1&quot;&gt;Iqbal Pahlevi Amin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wicaksono_A/0/1/0/all/0/1&quot;&gt;Alfan Farizki Wicaksono&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04340">
<title>A Lightweight and Accurate Face Detection Algorithm Based on Retinaface. (arXiv:2308.04340v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04340</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a lightweight and accurate face detection algorithm
LAFD (Light and accurate face detection) based on Retinaface. Backbone network
in the algorithm is a modified MobileNetV3 network which adjusts the size of
the convolution kernel, the channel expansion multiplier of the inverted
residuals block and the use of the SE attention mechanism. Deformable
convolution network(DCN) is introduced in the context module and the algorithm
uses focal loss function instead of cross-entropy loss function as the
classification loss function of the model. The test results on the WIDERFACE
dataset indicate that the average accuracy of LAFD is 94.1%, 92.2% and 82.1%
for the &quot;easy&quot;, &quot;medium&quot; and &quot;hard&quot; validation subsets respectively with an
improvement of 3.4%, 4.0% and 8.3% compared to Retinaface and 3.1%, 4.1% and
4.1% higher than the well-performing lightweight model, LFFD. If the input
image is pre-processed and scaled to 1560px in length or 1200px in width, the
model achieves an average accuracy of 86.2% on the &apos;hard&apos; validation subset.
The model is lightweight, with a size of only 10.2MB.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Baozhu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hewei Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04343">
<title>Unifying Two-Stream Encoders with Transformers for Cross-Modal Retrieval. (arXiv:2308.04343v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04343</link>
<description rdf:parseType="Literal">&lt;p&gt;Most existing cross-modal retrieval methods employ two-stream encoders with
different architectures for images and texts, \textit{e.g.}, CNN for images and
RNN/Transformer for texts. Such discrepancy in architectures may induce
different semantic distribution spaces and limit the interactions between
images and texts, and further result in inferior alignment between images and
texts. To fill this research gap, inspired by recent advances of Transformers
in vision tasks, we propose to unify the encoder architectures with
Transformers for both modalities. Specifically, we design a cross-modal
retrieval framework purely based on two-stream Transformers, dubbed
\textbf{Hierarchical Alignment Transformers (HAT)}, which consists of an image
Transformer, a text Transformer, and a hierarchical alignment module. With such
identical architectures, the encoders could produce representations with more
similar characteristics for images and texts, and make the interactions and
alignments between them much easier. Besides, to leverage the rich semantics,
we devise a hierarchical alignment scheme to explore multi-level
correspondences of different layers between images and texts. To evaluate the
effectiveness of the proposed HAT, we conduct extensive experiments on two
benchmark datasets, MSCOCO and Flickr30K. Experimental results demonstrate that
HAT outperforms SOTA baselines by a large margin. Specifically, on two key
tasks, \textit{i.e.}, image-to-text and text-to-image retrieval, HAT achieves
7.6\% and 16.7\% relative score improvement of Recall@1 on MSCOCO, and 4.4\%
and 11.6\% on Flickr30k respectively. The code is available at
\url{https://github.com/LuminosityX/HAT}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bin_Y/0/1/0/all/0/1&quot;&gt;Yi Bin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haoxuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yahui Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Heng Tao Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04352">
<title>3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment. (arXiv:2308.04352v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04352</link>
<description rdf:parseType="Literal">&lt;p&gt;3D vision-language grounding (3D-VL) is an emerging field that aims to
connect the 3D physical world with natural language, which is crucial for
achieving embodied intelligence. Current 3D-VL models rely heavily on
sophisticated modules, auxiliary losses, and optimization tricks, which calls
for a simple and unified model. In this paper, we propose 3D-VisTA, a
pre-trained Transformer for 3D Vision and Text Alignment that can be easily
adapted to various downstream tasks. 3D-VisTA simply utilizes self-attention
layers for both single-modal modeling and multi-modal fusion without any
sophisticated task-specific design. To further enhance its performance on 3D-VL
tasks, we construct ScanScribe, the first large-scale 3D scene-text pairs
dataset for 3D-VL pre-training. ScanScribe contains 2,995 RGB-D scans for 1,185
unique indoor scenes originating from ScanNet and 3R-Scan datasets, along with
paired 278K scene descriptions generated from existing 3D-VL tasks, templates,
and GPT-3. 3D-VisTA is pre-trained on ScanScribe via masked language/object
modeling and scene-text matching. It achieves state-of-the-art results on
various 3D-VL tasks, ranging from visual grounding and dense captioning to
question answering and situated reasoning. Moreover, 3D-VisTA demonstrates
superior data efficiency, obtaining strong performance even with limited
annotations during downstream task fine-tuning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Ziyu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiaojian Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yixin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1&quot;&gt;Zhidong Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Siyuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qing Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04356">
<title>Learning Unbiased Image Segmentation: A Case Study with Plain Knee Radiographs. (arXiv:2308.04356v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04356</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic segmentation of knee bony anatomy is essential in orthopedics, and
it has been around for several years in both pre-operative and post-operative
settings. While deep learning algorithms have demonstrated exceptional
performance in medical image analysis, the assessment of fairness and potential
biases within these models remains limited. This study aims to revisit deep
learning-powered knee-bony anatomy segmentation using plain radiographs to
uncover visible gender and racial biases. The current contribution offers the
potential to advance our understanding of biases, and it provides practical
insights for researchers and practitioners in medical imaging. The proposed
mitigation strategies mitigate gender and racial biases, ensuring fair and
unbiased segmentation results. Furthermore, this work promotes equal access to
accurate diagnoses and treatment outcomes for diverse patient populations,
fostering equitable and inclusive healthcare provision.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Littlefield_N/0/1/0/all/0/1&quot;&gt;Nickolas Littlefield&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plate_J/0/1/0/all/0/1&quot;&gt;Johannes F. Plate&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weiss_K/0/1/0/all/0/1&quot;&gt;Kurt R. Weiss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lohse_I/0/1/0/all/0/1&quot;&gt;Ines Lohse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chhabra_A/0/1/0/all/0/1&quot;&gt;Avani Chhabra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siddiqui_I/0/1/0/all/0/1&quot;&gt;Ismaeel A. Siddiqui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menezes_Z/0/1/0/all/0/1&quot;&gt;Zoe Menezes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mastorakos_G/0/1/0/all/0/1&quot;&gt;George Mastorakos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thakar_S/0/1/0/all/0/1&quot;&gt;Sakshi Mehul Thakar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abedian_M/0/1/0/all/0/1&quot;&gt;Mehrnaz Abedian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1&quot;&gt;Matthew F. Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carlson_L/0/1/0/all/0/1&quot;&gt;Luke A. Carlson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moradi_H/0/1/0/all/0/1&quot;&gt;Hamidreza Moradi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amirian_S/0/1/0/all/0/1&quot;&gt;Soheyla Amirian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tafti_A/0/1/0/all/0/1&quot;&gt;Ahmad P. Tafti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04369">
<title>SSTFormer: Bridging Spiking Neural Network and Memory Support Transformer for Frame-Event based Recognition. (arXiv:2308.04369v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04369</link>
<description rdf:parseType="Literal">&lt;p&gt;Event camera-based pattern recognition is a newly arising research topic in
recent years. Current researchers usually transform the event streams into
images, graphs, or voxels, and adopt deep neural networks for event-based
classification. Although good performance can be achieved on simple event
recognition datasets, however, their results may be still limited due to the
following two issues. Firstly, they adopt spatial sparse event streams for
recognition only, which may fail to capture the color and detailed texture
information well. Secondly, they adopt either Spiking Neural Networks (SNN) for
energy-efficient recognition with suboptimal results, or Artificial Neural
Networks (ANN) for energy-intensive, high-performance recognition. However,
seldom of them consider achieving a balance between these two aspects. In this
paper, we formally propose to recognize patterns by fusing RGB frames and event
streams simultaneously and propose a new RGB frame-event recognition framework
to address the aforementioned issues. The proposed method contains four main
modules, i.e., memory support Transformer network for RGB frame encoding,
spiking neural network for raw event stream encoding, multi-modal bottleneck
fusion module for RGB-Event feature aggregation, and prediction head. Due to
the scarce of RGB-Event based classification dataset, we also propose a
large-scale PokerEvent dataset which contains 114 classes, and 27102
frame-event pairs recorded using a DVS346 event camera. Extensive experiments
on two RGB-Event based classification datasets fully validated the
effectiveness of our proposed framework. We hope this work will boost the
development of pattern recognition by fusing RGB frames and event streams. Both
our dataset and source code of this work will be released at
https://github.com/Event-AHU/SSTFormer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zongzhen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1&quot;&gt;Yao Rong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Lin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1&quot;&gt;Bo Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jin Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yonghong Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04370">
<title>When Super-Resolution Meets Camouflaged Object Detection: A Comparison Study. (arXiv:2308.04370v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04370</link>
<description rdf:parseType="Literal">&lt;p&gt;Super Resolution (SR) and Camouflaged Object Detection (COD) are two hot
topics in computer vision with various joint applications. For instance,
low-resolution surveillance images can be successively processed by
super-resolution techniques and camouflaged object detection. However, in
previous work, these two areas are always studied in isolation. In this paper,
we, for the first time, conduct an integrated comparative evaluation for both.
Specifically, we benchmark different super-resolution methods on commonly used
COD datasets, and meanwhile, we evaluate the robustness of different COD models
by using COD data processed by SR methods. Our goal is to bridge these two
domains, discover novel experimental phenomena, summarize new experim.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Juan Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;Shupeng Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Peng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Bowen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1&quot;&gt;Radu Timofte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_W/0/1/0/all/0/1&quot;&gt;Weiyan Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1&quot;&gt;Luc Van Gool&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04373">
<title>Pelta: Shielding Transformers to Mitigate Evasion Attacks in Federated Learning. (arXiv:2308.04373v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.04373</link>
<description rdf:parseType="Literal">&lt;p&gt;The main premise of federated learning is that machine learning model updates
are computed locally, in particular to preserve user data privacy, as those
never leave the perimeter of their device. This mechanism supposes the general
model, once aggregated, to be broadcast to collaborating and non malicious
nodes. However, without proper defenses, compromised clients can easily probe
the model inside their local memory in search of adversarial examples. For
instance, considering image-based applications, adversarial examples consist of
imperceptibly perturbed images (to the human eye) misclassified by the local
model, which can be later presented to a victim node&apos;s counterpart model to
replicate the attack. To mitigate such malicious probing, we introduce Pelta, a
novel shielding mechanism leveraging trusted hardware. By harnessing the
capabilities of Trusted Execution Environments (TEEs), Pelta masks part of the
back-propagation chain rule, otherwise typically exploited by attackers for the
design of malicious samples. We evaluate Pelta on a state of the art ensemble
model and demonstrate its effectiveness against the Self Attention Gradient
adversarial Attack.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Queyrut_S/0/1/0/all/0/1&quot;&gt;Simon Queyrut&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bromberg_Y/0/1/0/all/0/1&quot;&gt;Y&amp;#xe9;rom-David Bromberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schiavoni_V/0/1/0/all/0/1&quot;&gt;Valerio Schiavoni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04380">
<title>Your Negative May not Be True Negative: Boosting Image-Text Matching with False Negative Elimination. (arXiv:2308.04380v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04380</link>
<description rdf:parseType="Literal">&lt;p&gt;Most existing image-text matching methods adopt triplet loss as the
optimization objective, and choosing a proper negative sample for the triplet
of &amp;lt;anchor, positive, negative&amp;gt; is important for effectively training the
model, e.g., hard negatives make the model learn efficiently and effectively.
However, we observe that existing methods mainly employ the most similar
samples as hard negatives, which may not be true negatives. In other words, the
samples with high similarity but not paired with the anchor may reserve
positive semantic associations, and we call them false negatives. Repelling
these false negatives in triplet loss would mislead the semantic representation
learning and result in inferior retrieval performance. In this paper, we
propose a novel False Negative Elimination (FNE) strategy to select negatives
via sampling, which could alleviate the problem introduced by false negatives.
Specifically, we first construct the distributions of positive and negative
samples separately via their similarities with the anchor, based on the
features extracted from image and text encoders. Then we calculate the false
negative probability of a given sample based on its similarity with the anchor
and the above distributions via the Bayes&apos; rule, which is employed as the
sampling weight during negative sampling process. Since there may not exist any
false negative in a small batch size, we design a memory module with momentum
to retain a large negative buffer and implement our negative sampling strategy
spanning over the buffer. In addition, to make the model focus on hard
negatives, we reassign the sampling weights for the simple negatives with a
cut-down strategy. The extensive experiments are conducted on Flickr30K and
MS-COCO, and the results demonstrate the superiority of our proposed false
negative elimination strategy. The code is available at
https://github.com/LuminosityX/FNE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haoxuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bin_Y/0/1/0/all/0/1&quot;&gt;Yi Bin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1&quot;&gt;Junrong Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Heng Tao Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04383">
<title>DELFlow: Dense Efficient Learning of Scene Flow for Large-Scale Point Clouds. (arXiv:2308.04383v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04383</link>
<description rdf:parseType="Literal">&lt;p&gt;Point clouds are naturally sparse, while image pixels are dense. The
inconsistency limits feature fusion from both modalities for point-wise scene
flow estimation. Previous methods rarely predict scene flow from the entire
point clouds of the scene with one-time inference due to the memory
inefficiency and heavy overhead from distance calculation and sorting involved
in commonly used farthest point sampling, KNN, and ball query algorithms for
local feature aggregation. To mitigate these issues in scene flow learning, we
regularize raw points to a dense format by storing 3D coordinates in 2D grids.
Unlike the sampling operation commonly used in existing works, the dense 2D
representation 1) preserves most points in the given scene, 2) brings in a
significant boost of efficiency, and 3) eliminates the density gap between
points and pixels, allowing us to perform effective feature fusion. We also
present a novel warping projection technique to alleviate the information loss
problem resulting from the fact that multiple points could be mapped into one
grid during projection when computing cost volume. Sufficient experiments
demonstrate the efficiency and effectiveness of our method, outperforming the
prior-arts on the FlyingThings3D and KITTI dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1&quot;&gt;Chensheng Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guangming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lo_X/0/1/0/all/0/1&quot;&gt;Xian Wan Lo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xinrui Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chenfeng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1&quot;&gt;Masayoshi Tomizuka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1&quot;&gt;Wei Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hesheng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04395">
<title>Data Augmentation-Based Unsupervised Domain Adaptation In Medical Imaging. (arXiv:2308.04395v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.04395</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning-based models in medical imaging often struggle to generalize
effectively to new scans due to data heterogeneity arising from differences in
hardware, acquisition parameters, population, and artifacts. This limitation
presents a significant challenge in adopting machine learning models for
clinical practice. We propose an unsupervised method for robust domain
adaptation in brain MRI segmentation by leveraging MRI-specific augmentation
techniques. To evaluate the effectiveness of our method, we conduct extensive
experiments across diverse datasets, modalities, and segmentation tasks,
comparing against the state-of-the-art methods. The results show that our
proposed approach achieves high accuracy, exhibits broad applicability, and
showcases remarkable robustness against domain shift in various tasks,
surpassing the state-of-the-art performance in the majority of cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Llambias_S/0/1/0/all/0/1&quot;&gt;Sebastian N&amp;#xf8;rgaard Llambias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nielsen_M/0/1/0/all/0/1&quot;&gt;Mads Nielsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ghazi_M/0/1/0/all/0/1&quot;&gt;Mostafa Mehdipour Ghazi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04397">
<title>LEFormer: A Hybrid CNN-Transformer Architecture for Accurate Lake Extraction from Remote Sensing Imagery. (arXiv:2308.04397v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04397</link>
<description rdf:parseType="Literal">&lt;p&gt;Lake extraction from remote sensing imagery is challenging due to the complex
shapes of lakes and the presence of noise. Existing methods suffer from blurred
segmentation boundaries and poor foreground modeling. In this paper, we propose
a hybrid CNN-Transformer architecture, called LEFormer, for accurate lake
extraction. LEFormer contains four main modules: CNN encoder, Transformer
encoder, cross-encoder fusion, and lightweight decoder. The CNN encoder
recovers local spatial information and improves fine-scale details.
Simultaneously, the Transformer encoder captures long-range dependencies
between sequences of any length, allowing them to obtain global features and
context information better. Finally, a lightweight decoder is employed for mask
prediction. We evaluate the performance and efficiency of LEFormer on two
datasets, the Surface Water (SW) and the Qinghai-Tibet Plateau Lake (QTPL).
Experimental results show that LEFormer consistently achieves state-of-the-art
(SOTA) performance and efficiency on these two datasets, outperforming existing
methods. Specifically, LEFormer achieves 90.86% and 97.42% mIoU on the SW and
QTPL datasets with a parameter count of 3.61M, respectively, while being 20x
minor than the previous SOTA method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Ben Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1&quot;&gt;Xuechao Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiayu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_P/0/1/0/all/0/1&quot;&gt;Pin Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04402">
<title>Person Re-Identification without Identification via Event Anonymization. (arXiv:2308.04402v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04402</link>
<description rdf:parseType="Literal">&lt;p&gt;Wide-scale use of visual surveillance in public spaces puts individual
privacy at stake while increasing resource consumption (energy, bandwidth, and
computation). Neuromorphic vision sensors (event-cameras) have been recently
considered a valid solution to the privacy issue because they do not capture
detailed RGB visual information of the subjects in the scene. However, recent
deep learning architectures have been able to reconstruct images from event
cameras with high fidelity, reintroducing a potential threat to privacy for
event-based vision applications. In this paper, we aim to anonymize
event-streams to protect the identity of human subjects against such image
reconstruction attacks. To achieve this, we propose an end-to-end network
architecture jointly optimized for the twofold objective of preserving privacy
and performing a downstream task such as person ReId. Our network learns to
scramble events, enforcing the degradation of images recovered from the privacy
attacker. In this work, we also bring to the community the first ever
event-based person ReId dataset gathered to evaluate the performance of our
approach. We validate our approach with extensive experiments and report
results on the synthetic event data simulated from the publicly available
SoftBio dataset and our proposed Event-ReId dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmad_S/0/1/0/all/0/1&quot;&gt;Shafiq Ahmad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morerio_P/0/1/0/all/0/1&quot;&gt;Pietro Morerio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bue_A/0/1/0/all/0/1&quot;&gt;Alessio Del Bue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04409">
<title>V-DETR: DETR with Vertex Relative Position Encoding for 3D Object Detection. (arXiv:2308.04409v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04409</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a highly performant 3D object detector for point clouds using
the DETR framework. The prior attempts all end up with suboptimal results
because they fail to learn accurate inductive biases from the limited scale of
training data. In particular, the queries often attend to points that are far
away from the target objects, violating the locality principle in object
detection. To address the limitation, we introduce a novel 3D Vertex Relative
Position Encoding (3DV-RPE) method which computes position encoding for each
point based on its relative position to the 3D boxes predicted by the queries
in each decoder layer, thus providing clear information to guide the model to
focus on points near the objects, in accordance with the principle of locality.
In addition, we systematically improve the pipeline from various aspects such
as data normalization based on our understanding of the task. We show
exceptional results on the challenging ScanNetV2 benchmark, achieving
significant improvements over the previous 3DETR in
$\rm{AP}_{25}$/$\rm{AP}_{50}$ from 65.0\%/47.0\% to 77.8\%/66.0\%,
respectively. In addition, our method sets a new record on ScanNetV2 and SUN
RGB-D datasets.Code will be released at &lt;a href=&quot;http://github.com/yichaoshen-MS/V-DETR.&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yichao Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_Z/0/1/0/all/0/1&quot;&gt;Zigang Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yuhui Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yutong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ze Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chunyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Han Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1&quot;&gt;Nanning Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1&quot;&gt;Baining Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04413">
<title>Digging into Depth Priors for Outdoor Neural Radiance Fields. (arXiv:2308.04413v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04413</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Fields (NeRF) have demonstrated impressive performance in
vision and graphics tasks, such as novel view synthesis and immersive reality.
However, the shape-radiance ambiguity of radiance fields remains a challenge,
especially in the sparse viewpoints setting. Recent work resorts to integrating
depth priors into outdoor NeRF training to alleviate the issue. However, the
criteria for selecting depth priors and the relative merits of different priors
have not been thoroughly investigated. Moreover, the relative merits of
selecting different approaches to use the depth priors is also an unexplored
problem. In this paper, we provide a comprehensive study and evaluation of
employing depth priors to outdoor neural radiance fields, covering common depth
sensing technologies and most application ways. Specifically, we conduct
extensive experiments with two representative NeRF methods equipped with four
commonly-used depth priors and different depth usages on two widely used
outdoor datasets. Our experimental results reveal several interesting findings
that can potentially benefit practitioners and researchers in training their
NeRF models with depth priors. Project Page:
https://cwchenwang.github.io/outdoor-nerf-depth
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiadai Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lina Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chenming Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1&quot;&gt;Zhelun Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Dayan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1&quot;&gt;Yuchao Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Liangjun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04417">
<title>DiffCR: A Fast Conditional Diffusion Framework for Cloud Removal from Optical Satellite Images. (arXiv:2308.04417v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04417</link>
<description rdf:parseType="Literal">&lt;p&gt;Optical satellite images are a critical data source; however, cloud cover
often compromises their quality, hindering image applications and analysis.
Consequently, effectively removing clouds from optical satellite images has
emerged as a prominent research direction. While recent advancements in cloud
removal primarily rely on generative adversarial networks, which may yield
suboptimal image quality, diffusion models have demonstrated remarkable success
in diverse image-generation tasks, showcasing their potential in addressing
this challenge. This paper presents a novel framework called DiffCR, which
leverages conditional guided diffusion with deep convolutional networks for
high-performance cloud removal for optical satellite imagery. Specifically, we
introduce a decoupled encoder for conditional image feature extraction,
providing a robust color representation to ensure the close similarity of
appearance information between the conditional input and the synthesized
output. Moreover, we propose a novel and efficient time and condition fusion
block within the cloud removal model to accurately simulate the correspondence
between the appearance in the conditional image and the target image at a low
computational cost. Extensive experimental evaluations on two commonly used
benchmark datasets demonstrate that DiffCR consistently achieves
state-of-the-art performance on all metrics, with parameter and computational
complexities amounting to only 5.1% and 5.4%, respectively, of those previous
best methods. The source code, pre-trained models, and all the experimental
results will be publicly available at https://github.com/XavierJiezou/DiffCR
upon the paper&apos;s acceptance of this work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1&quot;&gt;Xuechao Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1&quot;&gt;Junliang Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shiying Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1&quot;&gt;Lei Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_P/0/1/0/all/0/1&quot;&gt;Pin Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04426">
<title>A Deep-Learning Method Using Auto-encoder and Generative Adversarial Network for Anomaly Detection on Ancient Stone Stele Surfaces. (arXiv:2308.04426v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04426</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate detection of natural deterioration and man-made damage on the
surfaces of ancient stele in the first instance is essential for their
preventive conservation. Existing methods for cultural heritage preservation
are not able to achieve this goal perfectly due to the difficulty of balancing
accuracy, efficiency, timeliness, and cost. This paper presents a deep-learning
method to automatically detect above mentioned emergencies on ancient stone
stele in real time, employing autoencoder (AE) and generative adversarial
network (GAN). The proposed method overcomes the limitations of existing
methods by requiring no extensive anomaly samples while enabling comprehensive
detection of unpredictable anomalies. the method includes stages of monitoring,
data acquisition, pre-processing, model structuring, and post-processing.
Taking the Longmen Grottoes&apos; stone steles as a case study, an unsupervised
learning model based on AE and GAN architectures is proposed and validated with
a reconstruction accuracy of 99.74\%. The method&apos;s evaluation revealed the
proficient detection of seven artificially designed anomalies and demonstrated
precision and reliability without false alarms. This research provides novel
ideas and possibilities for the application of deep learning in the field of
cultural heritage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yikun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuning Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Cheng Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04431">
<title>When More is Less: Incorporating Additional Datasets Can Hurt Performance By Introducing Spurious Correlations. (arXiv:2308.04431v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.04431</link>
<description rdf:parseType="Literal">&lt;p&gt;In machine learning, incorporating more data is often seen as a reliable
strategy for improving model performance; this work challenges that notion by
demonstrating that the addition of external datasets in many cases can hurt the
resulting model&apos;s performance. In a large-scale empirical study across
combinations of four different open-source chest x-ray datasets and 9 different
labels, we demonstrate that in 43% of settings, a model trained on data from
two hospitals has poorer worst group accuracy over both hospitals than a model
trained on just a single hospital&apos;s data. This surprising result occurs even
though the added hospital makes the training distribution more similar to the
test distribution. We explain that this phenomenon arises from the spurious
correlation that emerges between the disease and hospital, due to
hospital-specific image artifacts. We highlight the trade-off one encounters
when training on multiple datasets, between the obvious benefit of additional
data and insidious cost of the introduced spurious correlation. In some cases,
balancing the dataset can remove the spurious correlation and improve
performance, but it is not always an effective strategy. We contextualize our
results within the literature on spurious correlations to help explain these
outcomes. Our experiments underscore the importance of exercising caution when
selecting training data for machine learning models, especially in settings
where there is a risk of spurious correlations such as with medical imaging.
The risks outlined highlight the need for careful data selection and model
evaluation in future research and practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Compton_R/0/1/0/all/0/1&quot;&gt;Rhys Compton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lily Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puli_A/0/1/0/all/0/1&quot;&gt;Aahlad Puli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ranganath_R/0/1/0/all/0/1&quot;&gt;Rajesh Ranganath&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2002.03729">
<title>A lightweight target detection algorithm based on Mobilenet Convolution. (arXiv:2002.03729v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2002.03729</link>
<description rdf:parseType="Literal">&lt;p&gt;Target detection algorithm based on deep learning needs high computer GPU
configuration, even need to use high performance deep learning workstation,
this not only makes the cost increase, also greatly limits the realizability of
the ground, this paper introduces a kind of lightweight algorithm for target
detection under the condition of the balance accuracy and computational
efficiency, MobileNet as Backbone performs parameter The processing speed is
30fps on the RTX2060 card for images with the CNN separator layer. The
processing speed is 30fps on the RTX2060 card for images with a resolution of
320*320.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shengquan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.01615">
<title>Lawin Transformer: Improving New-Era Vision Backbones with Multi-Scale Representations for Semantic Segmentation. (arXiv:2201.01615v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2201.01615</link>
<description rdf:parseType="Literal">&lt;p&gt;The multi-level aggregation (MLA) module has emerged as a critical component
for advancing new-era vision back-bones in semantic segmentation. In this
paper, we propose Lawin (large window) Transformer, a novel MLA architecture
that creatively utilizes multi-scale feature maps from the vision backbone. At
the core of Lawin Transformer is the Lawin attention, a newly designed window
attention mechanism capable of querying much larger context windows than local
windows. We focus on studying the efficient and simplistic application of the
large-window paradigm, allowing for flexible regulation of the ratio of large
context to query and capturing multi-scale representations. We validate the
effectiveness of Lawin Transformer on Cityscapes and ADE20K, consistently
demonstrating great superiority to widely-used MLA modules when combined with
new-era vision backbones. The code is available at
https://github.com/yan-hao-tian/lawin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1&quot;&gt;Haotian Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chuang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Ming Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.04680">
<title>Lifting-based variational multiclass segmentation: design, analysis and implementation. (arXiv:2202.04680v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2202.04680</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose, analyze and realize a variational multiclass segmentation scheme
that partitions a given image into multiple regions exhibiting specific
properties. Our method determines multiple functions that encode the
segmentation regions by minimizing an energy functional combining information
from different channels. Multichannel image data can be obtained by lifting the
image into a higher dimensional feature space using specific multichannel
filtering or may already be provided by the imaging modality under
consideration, such as an RGB image or multimodal medical data. Experimental
results show that the proposed method performs well in various scenarios. In
particular, promising results are presented for two medical applications
involving classification of brain abscess and tumor growth, respectively. As
main theoretical contributions, we prove the existence of global minimizers of
the proposed energy functional and show its stability and convergence with
respect to noisy inputs. In particular, these results also apply to the special
case of binary segmentation, and these results are also novel in this
particular situation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gruber_N/0/1/0/all/0/1&quot;&gt;Nadja Gruber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwab_J/0/1/0/all/0/1&quot;&gt;Johannes Schwab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Court_S/0/1/0/all/0/1&quot;&gt;Sebastien Court&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gizewski_E/0/1/0/all/0/1&quot;&gt;Elke Gizewski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haltmeier_M/0/1/0/all/0/1&quot;&gt;Markus Haltmeier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.01248">
<title>Differentiable Rendering for Synthetic Aperture Radar Imagery. (arXiv:2204.01248v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2204.01248</link>
<description rdf:parseType="Literal">&lt;p&gt;There is rising interest in differentiable rendering, which allows explicitly
modeling geometric priors and constraints in optimization pipelines using
first-order methods such as backpropagation. Incorporating such domain
knowledge can lead to deep neural networks that are trained more robustly and
with limited data, as well as the capability to solve ill-posed inverse
problems. Existing efforts in differentiable rendering have focused on imagery
from electro-optical sensors, particularly conventional RGB-imagery. In this
work, we propose an approach for differentiable rendering of Synthetic Aperture
Radar (SAR) imagery, which combines methods from 3D computer graphics with
neural rendering. We demonstrate the approach on the inverse graphics problem
of 3D Object Reconstruction from limited SAR imagery using high-fidelity
simulated SAR data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wilmanski_M/0/1/0/all/0/1&quot;&gt;Michael Wilmanski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tamir_J/0/1/0/all/0/1&quot;&gt;Jonathan Tamir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.11041">
<title>Learning by Erasing: Conditional Entropy based Transferable Out-Of-Distribution Detection. (arXiv:2204.11041v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2204.11041</link>
<description rdf:parseType="Literal">&lt;p&gt;Out-of-distribution (OOD) detection is essential to handle the distribution
shifts between training and test scenarios. For a new in-distribution (ID)
dataset, existing methods require retraining to capture the dataset-specific
feature representation or data distribution. In this paper, we propose a deep
generative models (DGM) based transferable OOD detection method, which is
unnecessary to retrain on a new ID dataset. We design an image erasing strategy
to equip exclusive conditional entropy distribution for each ID dataset, which
determines the discrepancy of DGM&apos;s posteriori ucertainty distribution on
different ID datasets. Owing to the powerful representation capacity of
convolutional neural networks, the proposed model trained on complex dataset
can capture the above discrepancy between ID datasets without retraining and
thus achieve transferable OOD detection. We validate the proposed method on
five datasets and verity that ours achieves comparable performance to the
state-of-the-art group based OOD detection methods that need to be retrained to
deploy on new ID datasets. Our code is available at
https://github.com/oOHCIOo/CETOOD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_M/0/1/0/all/0/1&quot;&gt;Meng Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yong Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_C/0/1/0/all/0/1&quot;&gt;Changjae Oh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.04415">
<title>SFNet: Faster and Accurate Semantic Segmentation via Semantic Flow. (arXiv:2207.04415v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.04415</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we focus on exploring effective methods for faster and
accurate semantic segmentation. A common practice to improve the performance is
to attain high-resolution feature maps with strong semantic representation. Two
strategies are widely used: atrous convolutions and feature pyramid fusion,
while both are either computationally intensive or ineffective. Inspired by the
Optical Flow for motion alignment between adjacent video frames, we propose a
Flow Alignment Module (FAM) to learn \textit{Semantic Flow} between feature
maps of adjacent levels and broadcast high-level features to high-resolution
features effectively and efficiently. Furthermore, integrating our FAM to a
standard feature pyramid structure exhibits superior performance over other
real-time methods, even on lightweight backbone networks, such as ResNet-18 and
DFNet. Then to further speed up the inference procedure, we also present a
novel Gated Dual Flow Alignment Module to directly align high-resolution
feature maps and low-resolution feature maps where we term the improved version
network as SFNet-Lite. Extensive experiments are conducted on several
challenging datasets, where results show the effectiveness of both SFNet and
SFNet-Lite. In particular, when using Cityscapes test set, the SFNet-Lite
series achieve 80.1 mIoU while running at 60 FPS using ResNet-18 backbone and
78.8 mIoU while running at 120 FPS using STDC backbone on RTX-3090. Moreover,
we unify four challenging driving datasets into one large dataset, which we
named Unified Driving Segmentation (UDS) dataset. It contains diverse domain
and style information. We benchmark several representative works on UDS. Both
SFNet and SFNet-Lite still achieve the best speed and accuracy trade-off on
UDS, which serves as a strong baseline in such a challenging setting. The code
and models are publicly available at https://github.com/lxtGH/SFSegNets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiangtai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiangning Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yibo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1&quot;&gt;Guangliang Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kuiyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1&quot;&gt;Yunhai Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.11699">
<title>Semi-supervised Deep Multi-view Stereo. (arXiv:2207.11699v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.11699</link>
<description rdf:parseType="Literal">&lt;p&gt;Significant progress has been witnessed in learning-based Multi-view Stereo
(MVS) under supervised and unsupervised settings. To combine their respective
merits in accuracy and completeness, meantime reducing the demand for expensive
labeled data, this paper explores the problem of learning-based MVS in a
semi-supervised setting that only a tiny part of the MVS data is attached with
dense depth ground truth. However, due to huge variation of scenarios and
flexible settings in views, it may break the basic assumption in classic
semi-supervised learning, that unlabeled data and labeled data share the same
label space and data distribution, named as semi-supervised distribution-gap
ambiguity in the MVS problem. To handle these issues, we propose a novel
semi-supervised distribution-augmented MVS framework, namely SDA-MVS. For the
simple case that the basic assumption works in MVS data, consistency
regularization encourages the model predictions to be consistent between
original sample and randomly augmented sample. For further troublesome case
that the basic assumption is conflicted in MVS data, we propose a novel style
consistency loss to alleviate the negative effect caused by the distribution
gap. The visual style of unlabeled sample is transferred to labeled sample to
shrink the gap, and the model prediction of generated sample is further
supervised with the label in original labeled sample. The experimental results
in semi-supervised settings of multiple MVS datasets show the superior
performance of the proposed method. With the same settings in backbone network,
our proposed SDA-MVS outperforms its fully-supervised and unsupervised
baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hongbin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhipeng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weitao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1&quot;&gt;Baigui Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1&quot;&gt;Wenxiong Kang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.14508">
<title>Swin-transformer-yolov5 For Real-time Wine Grape Bunch Detection. (arXiv:2208.14508v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2208.14508</link>
<description rdf:parseType="Literal">&lt;p&gt;In this research, an integrated detection model, Swin-transformer-YOLOv5 or
Swin-T-YOLOv5, was proposed for real-time wine grape bunch detection to inherit
the advantages from both YOLOv5 and Swin-transformer. The research was
conducted on two different grape varieties of Chardonnay (always white berry
skin) and Merlot (white or white-red mix berry skin when immature; red when
matured) from July to September in 2019. To verify the superiority of
Swin-T-YOLOv5, its performance was compared against several commonly
used/competitive object detectors, including Faster R-CNN, YOLOv3, YOLOv4, and
YOLOv5. All models were assessed under different test conditions, including two
different weather conditions (sunny and cloudy), two different berry maturity
stages (immature and mature), and three different sunlight
directions/intensities (morning, noon, and afternoon) for a comprehensive
comparison. Additionally, the predicted number of grape bunches by
Swin-T-YOLOv5 was further compared with ground truth values, including both
in-field manual counting and manual labeling during the annotation process.
Results showed that the proposed Swin-T-YOLOv5 outperformed all other studied
models for grape bunch detection, with up to 97% of mean Average Precision
(mAP) and 0.89 of F1-score when the weather was cloudy. This mAP was
approximately 44%, 18%, 14%, and 4% greater than Faster R-CNN, YOLOv3, YOLOv4,
and YOLOv5, respectively. Swin-T-YOLOv5 achieved its lowest mAP (90%) and
F1-score (0.82) when detecting immature berries, where the mAP was
approximately 40%, 5%, 3%, and 1% greater than the same. Furthermore,
Swin-T-YOLOv5 performed better on Chardonnay variety with achieved up to 0.91
of R2 and 2.36 root mean square error (RMSE) when comparing the predictions
with ground truth. However, it underperformed on Merlot variety with achieved
only up to 0.70 of R2 and 3.30 of RMSE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1&quot;&gt;Shenglian Lu&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Liu&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zixaun He&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wenbo Liu&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xin Zhang&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karkee_M/0/1/0/all/0/1&quot;&gt;Manoj Karkee&lt;/a&gt; (2) ((1) Guangxi Normal University, China, (2) Washington State University, US, (3) Mississippi State University, US)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.05785">
<title>Adversarial Coreset Selection for Efficient Robust Training. (arXiv:2209.05785v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.05785</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks are vulnerable to adversarial attacks: adding well-crafted,
imperceptible perturbations to their input can modify their output. Adversarial
training is one of the most effective approaches to training robust models
against such attacks. Unfortunately, this method is much slower than vanilla
training of neural networks since it needs to construct adversarial examples
for the entire training data at every iteration. By leveraging the theory of
coreset selection, we show how selecting a small subset of training data
provides a principled approach to reducing the time complexity of robust
training. To this end, we first provide convergence guarantees for adversarial
coreset selection. In particular, we show that the convergence bound is
directly related to how well our coresets can approximate the gradient computed
over the entire training data. Motivated by our theoretical analysis, we
propose using this gradient approximation error as our adversarial coreset
selection objective to reduce the training set size effectively. Once built, we
run adversarial training over this subset of the training data. Unlike existing
methods, our approach can be adapted to a wide variety of training objectives,
including TRADES, $\ell_p$-PGD, and Perceptual Adversarial Training. We conduct
extensive experiments to demonstrate that our approach speeds up adversarial
training by 2-3 times while experiencing a slight degradation in the clean and
robust accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dolatabadi_H/0/1/0/all/0/1&quot;&gt;Hadi M. Dolatabadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erfani_S/0/1/0/all/0/1&quot;&gt;Sarah Erfani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leckie_C/0/1/0/all/0/1&quot;&gt;Christopher Leckie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.14915">
<title>Spiking Neural Networks for event-based action recognition: A new task to understand their advantage. (arXiv:2209.14915v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.14915</link>
<description rdf:parseType="Literal">&lt;p&gt;Spiking Neural Networks (SNN) are characterised by their unique temporal
dynamics, but the properties and advantages of such computations are still not
well understood. In order to provide answers, in this work we demonstrate how
Spiking neurons can enable temporal feature extraction in feed-forward neural
networks without the need for recurrent synapses, showing how their
bio-inspired computing principles can be successfully exploited beyond energy
efficiency gains and evidencing their differences with respect to conventional
neurons. This is demonstrated by proposing a new task, DVS-Gesture-Chain
(DVS-GC), which allows, for the first time, to evaluate the perception of
temporal dependencies in a real event-based action recognition dataset. Our
study proves how the widely used DVS Gesture benchmark could be solved by
networks without temporal feature extraction, unlike the new DVS-GC which
demands an understanding of the ordering of the events. Furthermore, this setup
allowed us to unveil the role of the leakage rate in spiking neurons for
temporal processing tasks and demonstrated the benefits of &quot;hard reset&quot;
mechanisms. Additionally, we also show how time-dependent weights and
normalization can lead to understanding order by means of temporal attention.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vicente_Sola_A/0/1/0/all/0/1&quot;&gt;Alex Vicente-Sola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manna_D/0/1/0/all/0/1&quot;&gt;Davide L. Manna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirkland_P/0/1/0/all/0/1&quot;&gt;Paul Kirkland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caterina_G/0/1/0/all/0/1&quot;&gt;Gaetano Di Caterina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bihl_T/0/1/0/all/0/1&quot;&gt;Trevor Bihl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.04780">
<title>Genie: Show Me the Data for Quantization. (arXiv:2212.04780v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.04780</link>
<description rdf:parseType="Literal">&lt;p&gt;Zero-shot quantization is a promising approach for developing lightweight
deep neural networks when data is inaccessible owing to various reasons,
including cost and issues related to privacy. By exploiting the learned
parameters ($\mu$ and $\sigma$) of batch normalization layers in an
FP32-pre-trained model, zero-shot quantization schemes focus on generating
synthetic data. Subsequently, they distill knowledge from the pre-trained model
(teacher) to the quantized model (student) such that the quantized model can be
optimized with the synthetic dataset. However, thus far, zero-shot quantization
has primarily been discussed in the context of quantization-aware training
methods, which require task-specific losses and long-term optimization as much
as retraining. We thus introduce a post-training quantization scheme for
zero-shot quantization that produces high-quality quantized networks within a
few hours. Furthermore, we propose a framework called Genie~that generates data
suited for quantization. With the data synthesized by Genie, we can produce
robust quantized models without real datasets, which is comparable to few-shot
quantization. We also propose a post-training quantization algorithm to enhance
the performance of quantized models. By combining them, we can bridge the gap
between zero-shot and few-shot quantization while significantly improving the
quantization performance compared to that of existing approaches. In other
words, we can obtain a unique state-of-the-art zero-shot quantization approach.
The code is available at \url{https://github.com/SamsungLabs/Genie}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeon_Y/0/1/0/all/0/1&quot;&gt;Yongkweon Jeon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1&quot;&gt;Chungman Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Ho-young Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.05116">
<title>Leveraging Contextual Data Augmentation for Generalizable Melanoma Detection. (arXiv:2212.05116v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.05116</link>
<description rdf:parseType="Literal">&lt;p&gt;While skin cancer detection has been a valuable deep learning application for
years, its evaluation has often neglected the context in which testing images
are assessed. Traditional melanoma classifiers assume that their testing
environments are comparable to the structured images they are trained on. This
paper challenges this notion and argues that mole size, a critical attribute in
professional dermatology, can be misleading in automated melanoma detection.
While malignant melanomas tend to be larger than benign melanomas, relying
solely on size can be unreliable and even harmful when contextual scaling of
images is not possible. To address this issue, this implementation proposes a
custom model that performs various data augmentation procedures to prevent
overfitting to incorrect parameters and simulate real-world usage of melanoma
detection applications. Multiple custom models employing different forms of
data augmentation are implemented to highlight the most significant features of
mole classifiers. These implementations emphasize the importance of considering
user unpredictability when deploying such applications. The caution required
when manually modifying data is acknowledged, as it can result in data loss and
biased conclusions. Additionally, the significance of data augmentation in both
the dermatology and deep learning communities is considered.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+DiSanto_N/0/1/0/all/0/1&quot;&gt;Nick DiSanto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Harding_G/0/1/0/all/0/1&quot;&gt;Gavin Harding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Martinez_E/0/1/0/all/0/1&quot;&gt;Ethan Martinez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sanders_B/0/1/0/all/0/1&quot;&gt;Benjamin Sanders&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.01635">
<title>SPTS v2: Single-Point Scene Text Spotting. (arXiv:2301.01635v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.01635</link>
<description rdf:parseType="Literal">&lt;p&gt;End-to-end scene text spotting has made significant progress due to its
intrinsic synergy between text detection and recognition. Previous methods
commonly regard manual annotations such as horizontal rectangles, rotated
rectangles, quadrangles, and polygons as a prerequisite, which are much more
expensive than using single-point. Our new framework, SPTS v2, allows us to
train high-performing text-spotting models using a single-point annotation.
SPTS v2 reserves the advantage of the auto-regressive Transformer with an
Instance Assignment Decoder (IAD) through sequentially predicting the center
points of all text instances inside the same predicting sequence, while with a
Parallel Recognition Decoder (PRD) for text recognition in parallel. These two
decoders share the same parameters and are interactively connected with a
simple but effective information transmission process to pass the gradient and
information. Comprehensive experiments on various existing benchmark datasets
demonstrate the SPTS v2 can outperform previous state-of-the-art single-point
text spotters with fewer parameters while achieving 19$\times$ faster inference
speed. Within the context of our SPTS v2 framework, our experiments suggest a
potential preference for single-point representation in scene text spotting
when compared to other representations. Such an attempt provides a significant
opportunity for scene text spotting applications beyond the realms of existing
paradigms. Code is available at https://github.com/Yuliang-Liu/SPTSv2.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiaxin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1&quot;&gt;Dezhi Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1&quot;&gt;Mingxin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jingqun Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Can Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dahua Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Chunhua Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1&quot;&gt;Xiang Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1&quot;&gt;Lianwen Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.05609">
<title>Co-manipulation of soft-materials estimating deformation from depth images. (arXiv:2301.05609v4 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2301.05609</link>
<description rdf:parseType="Literal">&lt;p&gt;Human-robot co-manipulation of soft materials, such as fabrics, composites,
and sheets of paper/cardboard, is a challenging operation that presents several
relevant industrial applications. Estimating the deformation state of the
co-manipulated material is one of the main challenges. Viable methods provide
the indirect measure by calculating the human-robot relative distance. In this
paper, we develop a data-driven model to estimate the deformation state of the
material from a depth image through a Convolutional Neural Network (CNN).
First, we define the deformation state of the material as the relative
roto-translation from the current robot pose and a human grasping position. The
model estimates the current deformation state through a Convolutional Neural
Network, specifically a DenseNet-121 pretrained on ImageNet.The delta between
the current and the desired deformation state is fed to the robot controller
that outputs twist commands. The paper describes the developed approach to
acquire, preprocess the dataset and train the model. The model is compared with
the current state-of-the-art method based on a skeletal tracker from cameras.
Results show that our approach achieves better performances and avoids the
various drawbacks caused by using a skeletal tracker.Finally, we also studied
the model performance according to different architectures and dataset
dimensions to minimize the time required for dataset acquisition
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nicola_G/0/1/0/all/0/1&quot;&gt;Giorgio Nicola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Villagrossi_E/0/1/0/all/0/1&quot;&gt;Enrico Villagrossi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedrocchi_N/0/1/0/all/0/1&quot;&gt;Nicola Pedrocchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.10227">
<title>Denoising Diffusion Probabilistic Models for Generation of Realistic Fully-Annotated Microscopy Image Data Sets. (arXiv:2301.10227v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.10227</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in computer vision have led to significant progress in the
generation of realistic image data, with denoising diffusion probabilistic
models proving to be a particularly effective method. In this study, we
demonstrate that diffusion models can effectively generate fully-annotated
microscopy image data sets through an unsupervised and intuitive approach,
using rough sketches of desired structures as the starting point. The proposed
pipeline helps to reduce the reliance on manual annotations when training deep
learning-based segmentation approaches and enables the segmentation of diverse
datasets without the need for human annotations. This approach holds great
promise in streamlining the data generation process and enabling a more
efficient and scalable training of segmentation models, as we show in the
example of different practical experiments involving various organisms and cell
types.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Eschweiler_D/0/1/0/all/0/1&quot;&gt;Dennis Eschweiler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yilmaz_R/0/1/0/all/0/1&quot;&gt;R&amp;#xfc;veyda Yilmaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Baumann_M/0/1/0/all/0/1&quot;&gt;Matisse Baumann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Laube_I/0/1/0/all/0/1&quot;&gt;Ina Laube&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Roy_R/0/1/0/all/0/1&quot;&gt;Rijo Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jose_A/0/1/0/all/0/1&quot;&gt;Abin Jose&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bruckner_D/0/1/0/all/0/1&quot;&gt;Daniel Br&amp;#xfc;ckner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Stegmaier_J/0/1/0/all/0/1&quot;&gt;Johannes Stegmaier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.11514">
<title>Deep Industrial Image Anomaly Detection: A Survey. (arXiv:2301.11514v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.11514</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent rapid development of deep learning has laid a milestone in
industrial Image Anomaly Detection (IAD). In this paper, we provide a
comprehensive review of deep learning-based image anomaly detection techniques,
from the perspectives of neural network architectures, levels of supervision,
loss functions, metrics and datasets. In addition, we extract the new setting
from industrial manufacturing and review the current IAD approaches under our
proposed our new setting. Moreover, we highlight several opening challenges for
image anomaly detection. The merits and downsides of representative network
architectures under varying supervision are discussed. Finally, we summarize
the research findings and point out future research directions. More resources
are available at
https://github.com/M-3LAB/awesome-industrial-anomaly-detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiaqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1&quot;&gt;Guoyang Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingbao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shangnian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chengjie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1&quot;&gt;Feng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yaochu Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.00290">
<title>MS-DETR: Multispectral Pedestrian Detection Transformer with Loosely Coupled Fusion and Modality-Balanced Optimization. (arXiv:2302.00290v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.00290</link>
<description rdf:parseType="Literal">&lt;p&gt;Multispectral pedestrian detection is an important task for many
around-the-clock applications, since the visible and thermal modalities can
provide complementary information especially under low light conditions. Most
of the available multispectral pedestrian detectors are based on non-end-to-end
detectors, while in this paper, we propose MultiSpectral pedestrian DEtection
TRansformer (MS-DETR), an end-to-end multispectral pedestrian detector, which
extends DETR into the field of multi-modal detection. MS-DETR consists of two
modality-specific backbones and Transformer encoders, followed by a multi-modal
Transformer decoder, and the visible and thermal features are fused in the
multi-modal Transformer decoder. To well resist the misalignment between
multi-modal images, we design a loosely coupled fusion strategy by sparsely
sampling some keypoints from multi-modal features independently and fusing them
with adaptively learned attention weights. Moreover, based on the insight that
not only different modalities, but also different pedestrian instances tend to
have different confidence scores to final detection, we further propose an
instance-aware modality-balanced optimization strategy, which preserves visible
and thermal decoder branches and aligns their predicted slots through an
instance-wise dynamic loss. Our end-to-end MS-DETR shows superior performance
on the challenging KAIST, CVC-14 and LLVIP benchmark datasets. The source code
is available at https://github.com/YinghuiXing/MS-DETR .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1&quot;&gt;Yinghui Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Song Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shizhou Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_G/0/1/0/all/0/1&quot;&gt;Guoqiang Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiuwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yanning Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.00500">
<title>Inherently Interpretable Multi-Label Classification Using Class-Specific Counterfactuals. (arXiv:2303.00500v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.00500</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpretability is essential for machine learning algorithms in high-stakes
application fields such as medical image analysis. However, high-performing
black-box neural networks do not provide explanations for their predictions,
which can lead to mistrust and suboptimal human-ML collaboration. Post-hoc
explanation techniques, which are widely used in practice, have been shown to
suffer from severe conceptual problems. Furthermore, as we show in this paper,
current explanation techniques do not perform adequately in the multi-label
scenario, in which multiple medical findings may co-occur in a single image. We
propose Attri-Net, an inherently interpretable model for multi-label
classification. Attri-Net is a powerful classifier that provides transparent,
trustworthy, and human-understandable explanations. The model first generates
class-specific attribution maps based on counterfactuals to identify which
image regions correspond to certain medical findings. Then a simple logistic
regression classifier is used to make predictions based solely on these
attribution maps. We compare Attri-Net to five post-hoc explanation techniques
and one inherently interpretable classifier on three chest X-ray datasets. We
find that Attri-Net produces high-quality multi-label explanations consistent
with clinical knowledge and has comparable classification performance to
state-of-the-art classification models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Susu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woerner_S/0/1/0/all/0/1&quot;&gt;Stefano Woerner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1&quot;&gt;Andreas Maier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koch_L/0/1/0/all/0/1&quot;&gt;Lisa M. Koch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baumgartner_C/0/1/0/all/0/1&quot;&gt;Christian F. Baumgartner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.06209">
<title>SemARFlow: Injecting Semantics into Unsupervised Optical Flow Estimation for Autonomous Driving. (arXiv:2303.06209v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.06209</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised optical flow estimation is especially hard near occlusions and
motion boundaries and in low-texture regions. We show that additional
information such as semantics and domain knowledge can help better constrain
this problem. We introduce SemARFlow, an unsupervised optical flow network
designed for autonomous driving data that takes estimated semantic segmentation
masks as additional inputs. This additional information is injected into the
encoder and into a learned upsampler that refines the flow output. In addition,
a simple yet effective semantic augmentation module provides self-supervision
when learning flow and its boundaries for vehicles, poles, and sky. Together,
these injections of semantic information improve the KITTI-2015 optical flow
test error rate from 11.80% to 8.38%. We also show visible improvements around
object boundaries as well as a greater ability to generalize across datasets.
Code is available at
https://github.com/duke-vision/semantic-unsup-flow-release.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1&quot;&gt;Shuai Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Shuzhi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hannah Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomasi_C/0/1/0/all/0/1&quot;&gt;Carlo Tomasi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.09040">
<title>Hybrid Spectral Denoising Transformer with Guided Attention. (arXiv:2303.09040v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.09040</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a Hybrid Spectral Denoising Transformer (HSDT) for
hyperspectral image denoising. Challenges in adapting transformer for HSI arise
from the capabilities to tackle existing limitations of CNN-based methods in
capturing the global and local spatial-spectral correlations while maintaining
efficiency and flexibility. To address these issues, we introduce a hybrid
approach that combines the advantages of both models with a Spatial-Spectral
Separable Convolution (S3Conv), Guided Spectral Self-Attention (GSSA), and
Self-Modulated Feed-Forward Network (SM-FFN). Our S3Conv works as a lightweight
alternative to 3D convolution, which extracts more spatial-spectral correlated
features while keeping the flexibility to tackle HSIs with an arbitrary number
of bands. These features are then adaptively processed by GSSA which per-forms
3D self-attention across the spectral bands, guided by a set of learnable
queries that encode the spectral signatures. This not only enriches our model
with powerful capabilities for identifying global spectral correlations but
also maintains linear complexity. Moreover, our SM-FFN proposes the
self-modulation that intensifies the activations of more informative regions,
which further strengthens the aggregated features. Extensive experiments are
conducted on various datasets under both simulated and real-world noise, and it
shows that our HSDT significantly outperforms the existing state-of-the-art
methods while maintaining low computational overhead. Code is at https:
//github.com/Zeqiang-Lai/HSDT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_Z/0/1/0/all/0/1&quot;&gt;Zeqiang Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1&quot;&gt;Chenggang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Ying Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.16565">
<title>PMAA: A Progressive Multi-scale Attention Autoencoder Model for High-performance Cloud Removal from Multi-temporal Satellite Imagery. (arXiv:2303.16565v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.16565</link>
<description rdf:parseType="Literal">&lt;p&gt;Satellite imagery analysis plays a pivotal role in remote sensing; however,
information loss due to cloud cover significantly impedes its application.
Although existing deep cloud removal models have achieved notable outcomes,
they scarcely consider contextual information. This study introduces a
high-performance cloud removal architecture, termed Progressive Multi-scale
Attention Autoencoder (PMAA), which concurrently harnesses global and local
information to construct robust contextual dependencies using a novel
Multi-scale Attention Module (MAM) and a novel Local Interaction Module (LIM).
PMAA establishes long-range dependencies of multi-scale features using MAM and
modulates the reconstruction of fine-grained details utilizing LIM, enabling
simultaneous representation of fine- and coarse-grained features at the same
level. With the help of diverse and multi-scale features, PMAA consistently
outperforms the previous state-of-the-art model CTGAN on two benchmark
datasets. Moreover, PMAA boasts considerable efficiency advantages, with only
0.5% and 14.6% of the parameters and computational complexity of CTGAN,
respectively. These comprehensive results underscore PMAA&apos;s potential as a
lightweight cloud removal network suitable for deployment on edge devices to
accomplish large-scale cloud removal tasks. Our source code and pre-trained
models are available at https://github.com/XavierJiezou/PMAA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1&quot;&gt;Xuechao Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1&quot;&gt;Junliang Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_P/0/1/0/all/0/1&quot;&gt;Pin Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1&quot;&gt;Yachao Cui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.00501">
<title>A Comprehensive Review of YOLO: From YOLOv1 and Beyond. (arXiv:2304.00501v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.00501</link>
<description rdf:parseType="Literal">&lt;p&gt;YOLO has become a central real-time object detection system for robotics,
driverless cars, and video monitoring applications. We present a comprehensive
analysis of YOLO&apos;s evolution, examining the innovations and contributions in
each iteration from the original YOLO up to YOLOv8, YOLO-NAS, and YOLO with
Transformers. We start by describing the standard metrics and postprocessing;
then, we discuss the major changes in network architecture and training tricks
for each model. Finally, we summarize the essential lessons from YOLO&apos;s
development and provide a perspective on its future, highlighting potential
research directions to enhance real-time object detection systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Terven_J/0/1/0/all/0/1&quot;&gt;Juan Terven&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cordova_Esparza_D/0/1/0/all/0/1&quot;&gt;Diana Cordova-Esparza&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.07916">
<title>GaitRef: Gait Recognition with Refined Sequential Skeletons. (arXiv:2304.07916v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.07916</link>
<description rdf:parseType="Literal">&lt;p&gt;Identifying humans with their walking sequences, known as gait recognition,
is a useful biometric understanding task as it can be observed from a long
distance and does not require cooperation from the subject. Two common
modalities used for representing the walking sequence of a person are
silhouettes and joint skeletons. Silhouette sequences, which record the
boundary of the walking person in each frame, may suffer from the variant
appearances from carried-on objects and clothes of the person. Framewise joint
detections are noisy and introduce some jitters that are not consistent with
sequential detections. In this paper, we combine the silhouettes and skeletons
and refine the framewise joint predictions for gait recognition. With temporal
information from the silhouette sequences, we show that the refined skeletons
can improve gait recognition performance without extra annotations. We compare
our methods on four public datasets, CASIA-B, OUMVLP, Gait3D and GREW, and show
state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Haidong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1&quot;&gt;Wanrong Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1&quot;&gt;Zhaoheng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nevatia_R/0/1/0/all/0/1&quot;&gt;Ram Nevatia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.08134">
<title>Tackling Face Verification Edge Cases: In-Depth Analysis and Human-Machine Fusion Approach. (arXiv:2304.08134v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.08134</link>
<description rdf:parseType="Literal">&lt;p&gt;Nowadays, face recognition systems surpass human performance on several
datasets. However, there are still edge cases that the machine can&apos;t correctly
classify. This paper investigates the effect of a combination of machine and
human operators in the face verification task. First, we look closer at the
edge cases for several state-of-the-art models to discover common datasets&apos;
challenging settings. Then, we conduct a study with 60 participants on these
selected tasks with humans and provide an extensive analysis. Finally, we
demonstrate that combining machine and human decisions can further improve the
performance of state-of-the-art face verification systems on various benchmark
datasets. Code and data are publicly available on GitHub.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knoche_M/0/1/0/all/0/1&quot;&gt;Martin Knoche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rigoll_G/0/1/0/all/0/1&quot;&gt;Gerhard Rigoll&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.01160">
<title>Long-Tailed Recognition by Mutual Information Maximization between Latent Features and Ground-Truth Labels. (arXiv:2305.01160v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.01160</link>
<description rdf:parseType="Literal">&lt;p&gt;Although contrastive learning methods have shown prevailing performance on a
variety of representation learning tasks, they encounter difficulty when the
training dataset is long-tailed. Many researchers have combined contrastive
learning and a logit adjustment technique to address this problem, but the
combinations are done ad-hoc and a theoretical background has not yet been
provided. The goal of this paper is to provide the background and further
improve the performance. First, we show that the fundamental reason contrastive
learning methods struggle with long-tailed tasks is that they try to maximize
the mutual information maximization between latent features and input data. As
ground-truth labels are not considered in the maximization, they are not able
to address imbalances between class labels. Rather, we interpret the
long-tailed recognition task as a mutual information maximization between
latent features and ground-truth labels. This approach integrates contrastive
learning and logit adjustment seamlessly to derive a loss function that shows
state-of-the-art performance on long-tailed recognition benchmarks. It also
demonstrates its efficacy in image segmentation tasks, verifying its
versatility beyond image classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suh_M/0/1/0/all/0/1&quot;&gt;Min-Kook Suh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1&quot;&gt;Seung-Woo Seo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.04170">
<title>YOLOCS: Object Detection based on Dense Channel Compression for Feature Spatial Solidification. (arXiv:2305.04170v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.04170</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we examine the associations between channel features and
convolutional kernels during the processes of feature purification and gradient
backpropagation, with a focus on the forward and backward propagation within
the network. Consequently, we propose a method called Dense Channel Compression
for Feature Spatial Solidification. Drawing upon the central concept of this
method, we introduce two innovative modules for backbone and head networks: the
Dense Channel Compression for Feature Spatial Solidification Structure (DCFS)
and the Asymmetric Multi-Level Compression Decoupled Head (ADH). When
integrated into the YOLOv5 model, these two modules demonstrate exceptional
performance, resulting in a modified model referred to as YOLOCS. Evaluated on
the MSCOCO dataset, the large, medium, and small YOLOCS models yield AP of
50.1%, 47.6%, and 42.5%, respectively. Maintaining inference speeds remarkably
similar to those of the YOLOv5 model, the large, medium, and small YOLOCS
models surpass the YOLOv5 model&apos;s AP by 1.1%, 2.3%, and 5.2%, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Lin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Weisheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Linlin Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Haojie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1&quot;&gt;Xue Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1&quot;&gt;Suihan Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.04517">
<title>DiffBFR: Bootstrapping Diffusion Model Towards Blind Face Restoration. (arXiv:2305.04517v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.04517</link>
<description rdf:parseType="Literal">&lt;p&gt;Blind face restoration (BFR) is important while challenging. Prior works
prefer to exploit GAN-based frameworks to tackle this task due to the balance
of quality and efficiency. However, these methods suffer from poor stability
and adaptability to long-tail distribution, failing to simultaneously retain
source identity and restore detail. We propose DiffBFR to introduce Diffusion
Probabilistic Model (DPM) for BFR to tackle the above problem, given its
superiority over GAN in aspects of avoiding training collapse and generating
long-tail distribution. DiffBFR utilizes a two-step design, that first restores
identity information from low-quality images and then enhances texture details
according to the distribution of real faces. This design is implemented with
two key components: 1) Identity Restoration Module (IRM) for preserving the
face details in results. Instead of denoising from pure Gaussian random
distribution with LQ images as the condition during the reverse process, we
propose a novel truncated sampling method which starts from LQ images with part
noise added. We theoretically prove that this change shrinks the evidence lower
bound of DPM and then restores more original details. With theoretical proof,
two cascade conditional DPMs with different input sizes are introduced to
strengthen this sampling effect and reduce training difficulty in the
high-resolution image generated directly. 2) Texture Enhancement Module (TEM)
for polishing the texture of the image. Here an unconditional DPM, a LQ-free
model, is introduced to further force the restorations to appear realistic. We
theoretically proved that this unconditional DPM trained on pure HQ images
contributes to justifying the correct distribution of inference images output
from IRM in pixel-level space. Truncated sampling with fractional time step is
utilized to polish pixel-level textures while preserving identity information.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1&quot;&gt;Xinmin Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1&quot;&gt;Congying Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zicheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bonan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1&quot;&gt;Tiande Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_X/0/1/0/all/0/1&quot;&gt;Xuecheng Nie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.09664">
<title>Understanding 3D Object Interaction from a Single Image. (arXiv:2305.09664v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.09664</link>
<description rdf:parseType="Literal">&lt;p&gt;Humans can easily understand a single image as depicting multiple potential
objects permitting interaction. We use this skill to plan our interactions with
the world and accelerate understanding new objects without engaging in
interaction. In this paper, we would like to endow machines with the similar
ability, so that intelligent agents can better explore the 3D scene or
manipulate objects. Our approach is a transformer-based model that predicts the
3D location, physical properties and affordance of objects. To power this
model, we collect a dataset with Internet videos, egocentric videos and indoor
images to train and validate our approach. Our model yields strong performance
on our data, and generalizes well to robotics data. Project site:
https://jasonqsy.github.io/3DOI/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1&quot;&gt;Shengyi Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fouhey_D/0/1/0/all/0/1&quot;&gt;David F. Fouhey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.09880">
<title>A survey of the Vision Transformers and its CNN-Transformer based Variants. (arXiv:2305.09880v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.09880</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision transformers have become popular as a possible substitute to
convolutional neural networks (CNNs) for a variety of computer vision
applications. These transformers, with their ability to focus on global
relationships in images, offer large learning capacity. However, they may
suffer from limited generalization as they do not tend to model local
correlation in images. Recently, in vision transformers hybridization of both
the convolution operation and self-attention mechanism has emerged, to exploit
both the local and global image representations. These hybrid vision
transformers, also referred to as CNN-Transformer architectures, have
demonstrated remarkable results in vision applications. Given the rapidly
growing number of hybrid vision transformers, it has become necessary to
provide a taxonomy and explanation of these hybrid architectures. This survey
presents a taxonomy of the recent vision transformer architectures and more
specifically that of the hybrid vision transformers. Additionally, the key
features of these architectures such as the attention mechanisms, positional
embeddings, multi-scale processing, and convolution are also discussed. In
contrast to the previous survey papers that are primarily focused on individual
vision transformer architectures or CNNs, this survey uniquely emphasizes the
emerging trend of hybrid vision transformers. By showcasing the potential of
hybrid vision transformers to deliver exceptional performance across a range of
computer vision tasks, this survey sheds light on the future directions of this
rapidly evolving architecture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Asifullah Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rauf_Z/0/1/0/all/0/1&quot;&gt;Zunaira Rauf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sohail_A/0/1/0/all/0/1&quot;&gt;Anabia Sohail&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rehman_A/0/1/0/all/0/1&quot;&gt;Abdul Rehman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asif_H/0/1/0/all/0/1&quot;&gt;Hifsa Asif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asif_A/0/1/0/all/0/1&quot;&gt;Aqsa Asif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farooq_U/0/1/0/all/0/1&quot;&gt;Umair Farooq&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10044">
<title>Two-Stream Regression Network for Dental Implant Position Prediction. (arXiv:2305.10044v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10044</link>
<description rdf:parseType="Literal">&lt;p&gt;In implant prosthesis treatment, the design of the surgical guide heavily
relies on the manual location of the implant position, which is subjective and
prone to doctor&apos;s experiences. When deep learning based methods has started to
be applied to address this problem, the space between teeth are various and
some of them might present similar texture characteristic with the actual
implant region. Both problems make a big challenge for the implant position
prediction. In this paper, we develop a two-stream implant position regression
framework (TSIPR), which consists of an implant region detector (IRD) and a
multi-scale patch embedding regression network (MSPENet), to address this
issue. For the training of IRD, we extend the original annotation to provide
additional supervisory information, which contains much more rich
characteristic and do not introduce extra labeling costs. A multi-scale patch
embedding module is designed for the MSPENet to adaptively extract features
from the images with various tooth spacing. The global-local feature
interaction block is designed to build the encoder of MSPENet, which combines
the transformer and convolution for enriched feature representation. During
inference, the RoI mask extracted from the IRD is used to refine the prediction
results of the MSPENet. Extensive experiments on a dental implant dataset
through five-fold cross-validation demonstrated that the proposed TSIPR
achieves superior performance than existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xinquan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuguang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuechen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenting Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Linlin Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yongqiang Deng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12522">
<title>P-NOC: Adversarial CAM Generation for Weakly Supervised Semantic Segmentation. (arXiv:2305.12522v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12522</link>
<description rdf:parseType="Literal">&lt;p&gt;To mitigate the necessity for large amounts of supervised segmentation
annotation sets, multiple Weakly Supervised Semantic Segmentation (WSSS)
strategies have been devised. These will often rely on advanced data and model
regularization strategies to instigate the development of useful properties
(e.g., prediction completeness and fidelity to semantic boundaries) in
segmentation priors, notwithstanding the lack of annotated information. In this
work, we first create a strong baseline by analyzing complementary WSSS
techniques and regularizing strategies, considering their strengths and
limitations. We then propose a new Class-specific Adversarial Erasing strategy,
comprising two adversarial CAM generating networks being gradually refined to
produce robust semantic segmentation proposals. Empirical results suggest that
our approach induces substantial improvement in the effectiveness of the
baseline, resulting in a noticeable improvement over both Pascal VOC 2012 and
MS COCO 2014 datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+David_L/0/1/0/all/0/1&quot;&gt;Lucas David&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedrini_H/0/1/0/all/0/1&quot;&gt;Helio Pedrini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dias_Z/0/1/0/all/0/1&quot;&gt;Zanoni Dias&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18651">
<title>UMD: Unsupervised Model Detection for X2X Backdoor Attacks. (arXiv:2305.18651v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18651</link>
<description rdf:parseType="Literal">&lt;p&gt;Backdoor (Trojan) attack is a common threat to deep neural networks, where
samples from one or more source classes embedded with a backdoor trigger will
be misclassified to adversarial target classes. Existing methods for detecting
whether a classifier is backdoor attacked are mostly designed for attacks with
a single adversarial target (e.g., all-to-one attack). To the best of our
knowledge, without supervision, no existing methods can effectively address the
more general X2X attack with an arbitrary number of source classes, each paired
with an arbitrary target class. In this paper, we propose UMD, the first
Unsupervised Model Detection method that effectively detects X2X backdoor
attacks via a joint inference of the adversarial (source, target) class pairs.
In particular, we first define a novel transferability statistic to measure and
select a subset of putative backdoor class pairs based on a proposed clustering
approach. Then, these selected class pairs are jointly assessed based on an
aggregation of their reverse-engineered trigger size for detection inference,
using a robust and unsupervised anomaly detector we proposed. We conduct
comprehensive evaluations on CIFAR-10, GTSRB, and Imagenette dataset, and show
that our unsupervised UMD outperforms SOTA detectors (even with supervision) by
17%, 4%, and 8%, respectively, in terms of the detection accuracy against
diverse X2X attacks. We also show the strong detection performance of UMD
against several strong adaptive attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_Z/0/1/0/all/0/1&quot;&gt;Zhen Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1&quot;&gt;Zidi Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09345">
<title>Evaluating Data Attribution for Text-to-Image Models. (arXiv:2306.09345v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09345</link>
<description rdf:parseType="Literal">&lt;p&gt;While large text-to-image models are able to synthesize &quot;novel&quot; images, these
images are necessarily a reflection of the training data. The problem of data
attribution in such models -- which of the images in the training set are most
responsible for the appearance of a given generated image -- is a difficult yet
important one. As an initial step toward this problem, we evaluate attribution
through &quot;customization&quot; methods, which tune an existing large-scale model
toward a given exemplar object or style. Our key insight is that this allows us
to efficiently create synthetic images that are computationally influenced by
the exemplar by construction. With our new dataset of such exemplar-influenced
images, we are able to evaluate various data attribution algorithms and
different possible feature spaces. Furthermore, by training on our dataset, we
can tune standard models, such as DINO, CLIP, and ViT, toward the attribution
problem. Even though the procedure is tuned towards small exemplar sets, we
show generalization to larger sets. Finally, by taking into account the
inherent uncertainty of the problem, we can assign soft attribution scores over
a set of training images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sheng-Yu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1&quot;&gt;Alexei A. Efros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun-Yan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Richard Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10046">
<title>Document Layout Annotation: Database and Benchmark in the Domain of Public Affairs. (arXiv:2306.10046v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10046</link>
<description rdf:parseType="Literal">&lt;p&gt;Every day, thousands of digital documents are generated with useful
information for companies, public organizations, and citizens. Given the
impossibility of processing them manually, the automatic processing of these
documents is becoming increasingly necessary in certain sectors. However, this
task remains challenging, since in most cases a text-only based parsing is not
enough to fully understand the information presented through different
components of varying significance. In this regard, Document Layout Analysis
(DLA) has been an interesting research field for many years, which aims to
detect and classify the basic components of a document. In this work, we used a
procedure to semi-automatically annotate digital documents with different
layout labels, including 4 basic layout blocks and 4 text categories. We apply
this procedure to collect a novel database for DLA in the public affairs
domain, using a set of 24 data sources from the Spanish Administration. The
database comprises 37.9K documents with more than 441K document pages, and more
than 8M labels associated to 8 layout block units. The results of our
experiments validate the proposed text labeling procedure with accuracy up to
99%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pena_A/0/1/0/all/0/1&quot;&gt;Alejandro Pe&amp;#xf1;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1&quot;&gt;Aythami Morales&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1&quot;&gt;Julian Fierrez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortega_Garcia_J/0/1/0/all/0/1&quot;&gt;Javier Ortega-Garcia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grande_M/0/1/0/all/0/1&quot;&gt;Marcos Grande&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puente_I/0/1/0/all/0/1&quot;&gt;I&amp;#xf1;igo Puente&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cordova_J/0/1/0/all/0/1&quot;&gt;Jorge Cordova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cordova_G/0/1/0/all/0/1&quot;&gt;Gonzalo Cordova&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16181">
<title>Learning to Pan-sharpening with Memories of Spatial Details. (arXiv:2306.16181v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16181</link>
<description rdf:parseType="Literal">&lt;p&gt;Pan-sharpening, as one of the most commonly used techniques in remote sensing
systems, aims to inject spatial details from panchromatic images into
multispectral images (MS) to obtain high-resolution multispectral images. Since
deep learning has received widespread attention because of its powerful fitting
ability and efficient feature extraction, a variety of pan-sharpening methods
have been proposed to achieve remarkable performance. However, current
pan-sharpening methods usually require the paired panchromatic (PAN) and MS
images as input, which limits their usage in some scenarios. To address this
issue, in this paper we observe that the spatial details from PAN images are
mainly high-frequency cues, i.e., the edges reflect the contour of input PAN
images. This motivates us to develop a PAN-agnostic representation to store
some base edges, so as to compose the contour for the corresponding PAN image
via them. As a result, we can perform the pan-sharpening task with only the MS
image when inference. To this end, a memory-based network is adapted to extract
and memorize the spatial details during the training phase and is used to
replace the process of obtaining spatial information from PAN images when
inference, which is called Memory-based Spatial Details Network (MSDN).
Finally, we integrate the proposed MSDN module into the existing deep
learning-based pan-sharpening methods to achieve an end-to-end pan-sharpening
network. With extensive experiments on the Gaofen1 and WorldView-4 satellites,
we verify that our method constructs good spatial details without PAN images
and achieves the best performance. The code is available at
https://github.com/Zhao-Tian-yi/Learning-to-Pan-sharpening-with-Memories-of-Spatial-Details.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1&quot;&gt;Maoxun Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tianyi Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xingxing Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16670">
<title>End-to-End Learnable Multi-Scale Feature Compression for VCM. (arXiv:2306.16670v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16670</link>
<description rdf:parseType="Literal">&lt;p&gt;The proliferation of deep learning-based machine vision applications has
given rise to a new type of compression, so called video coding for machine
(VCM). VCM differs from traditional video coding in that it is optimized for
machine vision performance instead of human visual quality. In the feature
compression track of MPEG-VCM, multi-scale features extracted from images are
subject to compression. Recent feature compression works have demonstrated that
the versatile video coding (VVC) standard-based approach can achieve a BD-rate
reduction of up to 96% against MPEG-VCM feature anchor. However, it is still
sub-optimal as VVC was not designed for extracted features but for natural
images. Moreover, the high encoding complexity of VVC makes it difficult to
design a lightweight encoder without sacrificing performance. To address these
challenges, we propose a novel multi-scale feature compression method that
enables both the end-to-end optimization on the extracted features and the
design of lightweight encoders. The proposed model combines a learnable
compressor with a multi-scale feature fusion network so that the redundancy in
the multi-scale features is effectively removed. Instead of simply cascading
the fusion network and the compression network, we integrate the fusion and
encoding processes in an interleaved way. Our model first encodes a
larger-scale feature to obtain a latent representation and then fuses the
latent with a smaller-scale feature. This process is successively performed
until the smallest-scale feature is fused and then the encoded latent at the
final stage is entropy-coded for transmission. The results show that our model
outperforms previous approaches by at least 52% BD-rate reduction and has
$\times5$ to $\times27$ times less encoding time for object detection...
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yeongwoong Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_H/0/1/0/all/0/1&quot;&gt;Hyewon Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Janghyun Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Younhee Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jooyoung Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_S/0/1/0/all/0/1&quot;&gt;Se Yoon Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hui Yong Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02227">
<title>MAE-DFER: Efficient Masked Autoencoder for Self-supervised Dynamic Facial Expression Recognition. (arXiv:2307.02227v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02227</link>
<description rdf:parseType="Literal">&lt;p&gt;Dynamic facial expression recognition (DFER) is essential to the development
of intelligent and empathetic machines. Prior efforts in this field mainly fall
into supervised learning paradigm, which is severely restricted by the limited
labeled data in existing datasets. Inspired by recent unprecedented success of
masked autoencoders (e.g., VideoMAE), this paper proposes MAE-DFER, a novel
self-supervised method which leverages large-scale self-supervised pre-training
on abundant unlabeled data to largely advance the development of DFER. Since
the vanilla Vision Transformer (ViT) employed in VideoMAE requires substantial
computation during fine-tuning, MAE-DFER develops an efficient local-global
interaction Transformer (LGI-Former) as the encoder. Moreover, in addition to
the standalone appearance content reconstruction in VideoMAE, MAE-DFER also
introduces explicit temporal facial motion modeling to encourage LGI-Former to
excavate both static appearance and dynamic motion information. Extensive
experiments on six datasets show that MAE-DFER consistently outperforms
state-of-the-art supervised methods by significant margins (e.g., +6.30\% UAR
on DFEW and +8.34\% UAR on MAFW), verifying that it can learn powerful dynamic
facial representations via large-scale self-supervised pre-training. Besides,
it has comparable or even better performance than VideoMAE, while largely
reducing the computational cost (about 38\% FLOPs). We believe MAE-DFER has
paved a new way for the advancement of DFER and can inspire more relevant
research in this field and even other related tasks. Codes and models are
publicly available at https://github.com/sunlicai/MAE-DFER.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Licai Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1&quot;&gt;Zheng Lian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1&quot;&gt;Jianhua Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07873">
<title>Why Does Little Robustness Help? Understanding Adversarial Transferability From Surrogate Training. (arXiv:2307.07873v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07873</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial examples (AEs) for DNNs have been shown to be transferable: AEs
that successfully fool white-box surrogate models can also deceive other
black-box models with different architectures. Although a bunch of empirical
studies have provided guidance on generating highly transferable AEs, many of
these findings lack explanations and even lead to inconsistent advice. In this
paper, we take a further step towards understanding adversarial
transferability, with a particular focus on surrogate aspects. Starting from
the intriguing little robustness phenomenon, where models adversarially trained
with mildly perturbed adversarial samples can serve as better surrogates, we
attribute it to a trade-off between two predominant factors: model smoothness
and gradient similarity. Our investigations focus on their joint effects,
rather than their separate correlations with transferability. Through a series
of theoretical and empirical analyses, we conjecture that the data distribution
shift in adversarial training explains the degradation of gradient similarity.
Building on these insights, we explore the impacts of data augmentation and
gradient regularization on transferability and identify that the trade-off
generally exists in the various training mechanisms, thus building a
comprehensive blueprint for the regulation mechanism behind transferability.
Finally, we provide a general route for constructing better surrogates to boost
transferability which optimizes both model smoothness and gradient similarity
simultaneously, e.g., the combination of input gradient regularization and
sharpness-aware minimization (SAM), validated by extensive experiments. In
summary, we call for attention to the united impacts of these two factors for
launching effective transfer attacks, rather than optimizing one while ignoring
the other, and emphasize the crucial role of manipulating surrogate models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yechao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shengshan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Leo Yu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Junyu Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Minghui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaogeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_W/0/1/0/all/0/1&quot;&gt;Wei Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1&quot;&gt;Hai Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09724">
<title>AesPA-Net: Aesthetic Pattern-Aware Style Transfer Networks. (arXiv:2307.09724v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09724</link>
<description rdf:parseType="Literal">&lt;p&gt;To deliver the artistic expression of the target style, recent studies
exploit the attention mechanism owing to its ability to map the local patches
of the style image to the corresponding patches of the content image. However,
because of the low semantic correspondence between arbitrary content and
artworks, the attention module repeatedly abuses specific local patches from
the style image, resulting in disharmonious and evident repetitive artifacts.
To overcome this limitation and accomplish impeccable artistic style transfer,
we focus on enhancing the attention mechanism and capturing the rhythm of
patterns that organize the style. In this paper, we introduce a novel metric,
namely pattern repeatability, that quantifies the repetition of patterns in the
style image. Based on the pattern repeatability, we propose Aesthetic
Pattern-Aware style transfer Networks (AesPA-Net) that discover the sweet spot
of local and global style expressions. In addition, we propose a novel
self-supervisory task to encourage the attention mechanism to learn precise and
meaningful semantic correspondence. Lastly, we introduce the patch-wise style
loss to transfer the elaborate rhythm of local patterns. Through qualitative
and quantitative evaluations, we verify the reliability of the proposed pattern
repeatability that aligns with human perception, and demonstrate the
superiority of the proposed framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_K/0/1/0/all/0/1&quot;&gt;Kibeom Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeon_S/0/1/0/all/0/1&quot;&gt;Seogkyu Jeon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Junsoo Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahn_N/0/1/0/all/0/1&quot;&gt;Namhyuk Ahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kunhee Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_P/0/1/0/all/0/1&quot;&gt;Pilhyeon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Daesik Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uh_Y/0/1/0/all/0/1&quot;&gt;Youngjung Uh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Byun_H/0/1/0/all/0/1&quot;&gt;Hyeran Byun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09788">
<title>Density-invariant Features for Distant Point Cloud Registration. (arXiv:2307.09788v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09788</link>
<description rdf:parseType="Literal">&lt;p&gt;Registration of distant outdoor LiDAR point clouds is crucial to extending
the 3D vision of collaborative autonomous vehicles, and yet is challenging due
to small overlapping area and a huge disparity between observed point
densities. In this paper, we propose Group-wise Contrastive Learning (GCL)
scheme to extract density-invariant geometric features to register distant
outdoor LiDAR point clouds. We mark through theoretical analysis and
experiments that, contrastive positives should be independent and identically
distributed (i.i.d.), in order to train densityinvariant feature extractors. We
propose upon the conclusion a simple yet effective training scheme to force the
feature of multiple point clouds in the same spatial location (referred to as
positive groups) to be similar, which naturally avoids the sampling bias
introduced by a pair of point clouds to conform with the i.i.d. principle. The
resulting fully-convolutional feature extractor is more powerful and
density-invariant than state-of-the-art methods, improving the registration
recall of distant scenarios on KITTI and nuScenes benchmarks by 40.9% and
26.9%, respectively. Code is available at https://github.com/liuQuan98/GCL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Quan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Hongzi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yunsong Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1&quot;&gt;Shan Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1&quot;&gt;Minyi Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11661">
<title>Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts. (arXiv:2307.11661v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.11661</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive pretrained large Vision-Language Models (VLMs) like CLIP have
revolutionized visual representation learning by providing good performance on
downstream datasets. VLMs are 0-shot adapted to a downstream dataset by
designing prompts that are relevant to the dataset. Such prompt engineering
makes use of domain expertise and a validation dataset. Meanwhile, recent
developments in generative pretrained models like GPT-4 mean they can be used
as advanced internet search tools. They can also be manipulated to provide
visual information in any structure. In this work, we show that GPT-4 can be
used to generate text that is visually descriptive and how this can be used to
adapt CLIP to downstream tasks. We show considerable improvements in 0-shot
transfer accuracy on specialized fine-grained datasets like EuroSAT (~7%), DTD
(~7%), SUN397 (~4.6%), and CUB (~3.3%) when compared to CLIP&apos;s default prompt.
We also design a simple few-shot adapter that learns to choose the best
possible sentences to construct generalizable classifiers that outperform the
recently proposed CoCoOP by ~2% on average and by over 4% on 4 specialized
fine-grained datasets. The code, prompts, and auxiliary text dataset is
available at https://github.com/mayug/VDT-Adapter.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maniparambil_M/0/1/0/all/0/1&quot;&gt;Mayug Maniparambil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vorster_C/0/1/0/all/0/1&quot;&gt;Chris Vorster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molloy_D/0/1/0/all/0/1&quot;&gt;Derek Molloy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murphy_N/0/1/0/all/0/1&quot;&gt;Noel Murphy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McGuinness_K/0/1/0/all/0/1&quot;&gt;Kevin McGuinness&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OConnor_N/0/1/0/all/0/1&quot;&gt;Noel E. O&amp;#x27;Connor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.12344">
<title>Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?. (arXiv:2307.12344v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.12344</link>
<description rdf:parseType="Literal">&lt;p&gt;While deep neural network models offer unmatched classification performance,
they are prone to learning spurious correlations in the data. Such dependencies
on confounding information can be difficult to detect using performance metrics
if the test data comes from the same distribution as the training data.
Interpretable ML methods such as post-hoc explanations or inherently
interpretable classifiers promise to identify faulty model reasoning. However,
there is mixed evidence whether many of these techniques are actually able to
do so. In this paper, we propose a rigorous evaluation strategy to assess an
explanation technique&apos;s ability to correctly identify spurious correlations.
Using this strategy, we evaluate five post-hoc explanation techniques and one
inherently interpretable method for their ability to detect three types of
artificially added confounders in a chest x-ray diagnosis task. We find that
the post-hoc technique SHAP, as well as the inherently interpretable Attri-Net
provide the best performance and can be used to reliably identify faulty model
behavior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Susu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koch_L/0/1/0/all/0/1&quot;&gt;Lisa M. Koch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baumgartner_C/0/1/0/all/0/1&quot;&gt;Christian F. Baumgartner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.12450">
<title>ProtoFL: Unsupervised Federated Learning via Prototypical Distillation. (arXiv:2307.12450v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.12450</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) is a promising approach for enhancing data privacy
preservation, particularly for authentication systems. However, limited round
communications, scarce representation, and scalability pose significant
challenges to its deployment, hindering its full potential. In this paper, we
propose &apos;ProtoFL&apos;, Prototypical Representation Distillation based unsupervised
Federated Learning to enhance the representation power of a global model and
reduce round communication costs. Additionally, we introduce a local one-class
classifier based on normalizing flows to improve performance with limited data.
Our study represents the first investigation of using FL to improve one-class
classification performance. We conduct extensive experiments on five widely
used benchmarks, namely MNIST, CIFAR-10, CIFAR-100, ImageNet-30, and
Keystroke-Dynamics, to demonstrate the superior performance of our proposed
framework over previous methods in the literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hansol Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwak_Y/0/1/0/all/0/1&quot;&gt;Youngjun Kwak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_M/0/1/0/all/0/1&quot;&gt;Minyoung Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1&quot;&gt;Jinho Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Youngsung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1&quot;&gt;Changick Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.12622">
<title>Phase Matching for Out-of-Distribution Generalization. (arXiv:2307.12622v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.12622</link>
<description rdf:parseType="Literal">&lt;p&gt;The Fourier transform, serving as an explicit decomposition method for visual
signals, has been employed to explain the out-of-distribution generalization
behaviors of Convolutional Neural Networks (CNNs). Previous studies have
indicated that the amplitude spectrum is susceptible to the disturbance caused
by distribution shifts. On the other hand, the phase spectrum preserves
highly-structured spatial information, which is crucial for robust visual
representation learning. However, the spatial relationships of phase spectrum
remain unexplored in previous researches. In this paper, we aim to clarify the
relationships between Domain Generalization (DG) and the frequency components,
and explore the spatial relationships of the phase spectrum. Specifically, we
first introduce a Fourier-based structural causal model which interprets the
phase spectrum as semi-causal factors and the amplitude spectrum as non-causal
factors. Then, we propose Phase Matching (PhaMa) to address DG problems. Our
method introduces perturbations on the amplitude spectrum and establishes
spatial relationships to match the phase components. Through experiments on
multiple benchmarks, we demonstrate that our proposed method achieves
state-of-the-art performance in domain generalization and out-of-distribution
robustness tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1&quot;&gt;Chengming Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yeqian Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.12676">
<title>Damage Vision Mining Opportunity for Imbalanced Anomaly Detection. (arXiv:2307.12676v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.12676</link>
<description rdf:parseType="Literal">&lt;p&gt;In past decade, previous balanced datasets have been used to advance
algorithms for classification, object detection, semantic segmentation, and
anomaly detection in industrial applications. Specifically, for condition-based
maintenance, automating visual inspection is crucial to ensure high quality.
Deterioration prognostic attempts to optimize the fine decision process for
predictive maintenance and proactive repair. In civil infrastructure and living
environment, damage data mining cannot avoid the imbalanced data issue because
of rare unseen events and high quality status by improved operations. For
visual inspection, deteriorated class acquired from the surface of concrete and
steel components are occasionally imbalanced. From numerous related surveys, we
summarize that imbalanced data problems can be categorized into four types; 1)
missing range of target and label valuables, 2) majority-minority class
imbalance, 3) foreground-background of spatial imbalance, 4) long-tailed class
of pixel-wise imbalance. Since 2015, there has been many imbalanced studies
using deep learning approaches that includes regression, image classification,
object detection, semantic segmentation. However, anomaly detection for
imbalanced data is not yet well known. In the study, we highlight one-class
anomaly detection application whether anomalous class or not, and demonstrate
clear examples on imbalanced vision datasets: blood smear, lung infection,
hazardous driving, wooden, concrete deterioration, river sludge, and disaster
damage. Illustrated in Fig.1, we provide key results on damage vision mining
advantage, hypothesizing that the more effective range of positive ratio, the
higher accuracy gain of anomaly detection application. In our imbalanced
studies, compared with the balanced case of positive ratio 1/1, we find that
there is applicable positive ratio, where the accuracy are consistently high.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yasuno_T/0/1/0/all/0/1&quot;&gt;Takato Yasuno&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14016">
<title>RPG-Palm: Realistic Pseudo-data Generation for Palmprint Recognition. (arXiv:2307.14016v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.14016</link>
<description rdf:parseType="Literal">&lt;p&gt;Palmprint recently shows great potential in recognition applications as it is
a privacy-friendly and stable biometric. However, the lack of large-scale
public palmprint datasets limits further research and development of palmprint
recognition. In this paper, we propose a novel realistic pseudo-palmprint
generation (RPG) model to synthesize palmprints with massive identities. We
first introduce a conditional modulation generator to improve the intra-class
diversity. Then an identity-aware loss is proposed to ensure identity
consistency against unpaired training. We further improve the B\&apos;ezier palm
creases generation strategy to guarantee identity independence. Extensive
experimental results demonstrate that synthetic pretraining significantly
boosts the recognition model performance. For example, our model improves the
state-of-the-art B\&apos;ezierPalm by more than $5\%$ and $14\%$ in terms of
TAR@FAR=1e-6 under the $1:1$ and $1:3$ Open-set protocol. When accessing only
$10\%$ of the real training data, our method still outperforms ArcFace with
$100\%$ real training data, indicating that we are closer to real-data-free
palmprint recognition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Lei Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1&quot;&gt;Jianlong Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruixin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huaen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1&quot;&gt;Kai Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yingyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jingyun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1&quot;&gt;Shouhong Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1&quot;&gt;Wei Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.16074">
<title>Iterative Graph Filtering Network for 3D Human Pose Estimation. (arXiv:2307.16074v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.16074</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph convolutional networks (GCNs) have proven to be an effective approach
for 3D human pose estimation. By naturally modeling the skeleton structure of
the human body as a graph, GCNs are able to capture the spatial relationships
between joints and learn an efficient representation of the underlying pose.
However, most GCN-based methods use a shared weight matrix, making it
challenging to accurately capture the different and complex relationships
between joints. In this paper, we introduce an iterative graph filtering
framework for 3D human pose estimation, which aims to predict the 3D joint
positions given a set of 2D joint locations in images. Our approach builds upon
the idea of iteratively solving graph filtering with Laplacian regularization
via the Gauss-Seidel iterative method. Motivated by this iterative solution, we
design a Gauss-Seidel network (GS-Net) architecture, which makes use of weight
and adjacency modulation, skip connection, and a pure convolutional block with
layer normalization. Adjacency modulation facilitates the learning of edges
that go beyond the inherent connections of body joints, resulting in an
adjusted graph structure that reflects the human skeleton, while skip
connections help maintain crucial information from the input layer&apos;s initial
features as the network depth increases. We evaluate our proposed model on two
standard benchmark datasets, and compare it with a comprehensive set of strong
baseline methods for 3D human pose estimation. Our experimental results
demonstrate that our approach outperforms the baseline methods on both
datasets, achieving state-of-the-art performance. Furthermore, we conduct
ablation studies to analyze the contributions of different components of our
model architecture and show that the skip connection and adjacency modulation
help improve the model performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_Z/0/1/0/all/0/1&quot;&gt;Zaedul Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamza_A/0/1/0/all/0/1&quot;&gt;A. Ben Hamza&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01006">
<title>FusionAD: Multi-modality Fusion for Prediction and Planning Tasks of Autonomous Driving. (arXiv:2308.01006v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.01006</link>
<description rdf:parseType="Literal">&lt;p&gt;Building a multi-modality multi-task neural network toward accurate and
robust performance is a de-facto standard in perception task of autonomous
driving. However, leveraging such data from multiple sensors to jointly
optimize the prediction and planning tasks remains largely unexplored. In this
paper, we present FusionAD, to the best of our knowledge, the first unified
framework that fuse the information from two most critical sensors, camera and
LiDAR, goes beyond perception task. Concretely, we first build a transformer
based multi-modality fusion network to effectively produce fusion based
features. In constrast to camera-based end-to-end method UniAD, we then
establish a fusion aided modality-aware prediction and status-aware planning
modules, dubbed FMSPnP that take advantages of multi-modality features. We
conduct extensive experiments on commonly used benchmark nuScenes dataset, our
FusionAD achieves state-of-the-art performance and surpassing baselines on
average 15% on perception tasks like detection and tracking, 10% on occupancy
prediction accuracy, reducing prediction error from 0.708 to 0.389 in ADE score
and reduces the collision rate from 0.31% to only 0.12%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1&quot;&gt;Tengju Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_W/0/1/0/all/0/1&quot;&gt;Wei Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1&quot;&gt;Chunyong Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shikun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1&quot;&gt;Lingping Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Fangzhen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingke Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1&quot;&gt;Ke Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1&quot;&gt;Wencong Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1&quot;&gt;Weibo Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Hang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junbo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1&quot;&gt;Kaicheng Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02494">
<title>Adaptively Placed Multi-Grid Scene Representation Networks for Large-Scale Data Visualization. (arXiv:2308.02494v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.02494</link>
<description rdf:parseType="Literal">&lt;p&gt;Scene representation networks (SRNs) have been recently proposed for
compression and visualization of scientific data. However, state-of-the-art
SRNs do not adapt the allocation of available network parameters to the complex
features found in scientific data, leading to a loss in reconstruction quality.
We address this shortcoming with an adaptively placed multi-grid SRN (APMGSRN)
and propose a domain decomposition training and inference technique for
accelerated parallel training on multi-GPU systems. We also release an
open-source neural volume rendering application that allows plug-and-play
rendering with any PyTorch-based SRN. Our proposed APMGSRN architecture uses
multiple spatially adaptive feature grids that learn where to be placed within
the domain to dynamically allocate more neural network resources where error is
high in the volume, improving state-of-the-art reconstruction accuracy of SRNs
for scientific data without requiring expensive octree refining, pruning, and
traversal like previous adaptive models. In our domain decomposition approach
for representing large-scale data, we train an set of APMGSRNs in parallel on
separate bricks of the volume to reduce training time while avoiding overhead
necessary for an out-of-core solution for volumes too large to fit in GPU
memory. After training, the lightweight SRNs are used for realtime neural
volume rendering in our open-source renderer, where arbitrary view angles and
transfer functions can be explored. A copy of this paper, all code, all models
used in our experiments, and all supplemental materials and videos are
available at https://github.com/skywolf829/APMGSRN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wurster_S/0/1/0/all/0/1&quot;&gt;Skylar Wolfgang Wurster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xiong_T/0/1/0/all/0/1&quot;&gt;Tianyu Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Han-Wei Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guo_H/0/1/0/all/0/1&quot;&gt;Hanqi Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Peterka_T/0/1/0/all/0/1&quot;&gt;Tom Peterka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02552">
<title>Degeneration-Tuning: Using Scrambled Grid shield Unwanted Concepts from Stable Diffusion. (arXiv:2308.02552v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.02552</link>
<description rdf:parseType="Literal">&lt;p&gt;Owing to the unrestricted nature of the content in the training data, large
text-to-image diffusion models, such as Stable Diffusion (SD), are capable of
generating images with potentially copyrighted or dangerous content based on
corresponding textual concepts information. This includes specific intellectual
property (IP), human faces, and various artistic styles. However, Negative
Prompt, a widely used method for content removal, frequently fails to conceal
this content due to inherent limitations in its inference logic. In this work,
we propose a novel strategy named \textbf{Degeneration-Tuning (DT)} to shield
contents of unwanted concepts from SD weights. By utilizing Scrambled Grid to
reconstruct the correlation between undesired concepts and their corresponding
image domain, we guide SD to generate meaningless content when such textual
concepts are provided as input. As this adaptation occurs at the level of the
model&apos;s weights, the SD, after DT, can be grafted onto other conditional
diffusion frameworks like ControlNet to shield unwanted concepts. In addition
to qualitatively showcasing the effectiveness of our DT method in protecting
various types of concepts, a quantitative comparison of the SD before and after
DT indicates that the DT method does not significantly impact the generative
quality of other contents. The FID and IS scores of the model on COCO-30K
exhibit only minor changes after DT, shifting from 12.61 and 39.20 to 13.04 and
38.25, respectively, which clearly outperforms the previous methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_Z/0/1/0/all/0/1&quot;&gt;Zixuan Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1&quot;&gt;Longhui Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiacheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Siliang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1&quot;&gt;Yueting Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1&quot;&gt;Qi Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02632">
<title>Generation of Realistic Synthetic Raw Radar Data for Automated Driving Applications using Generative Adversarial Networks. (arXiv:2308.02632v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.02632</link>
<description rdf:parseType="Literal">&lt;p&gt;The main approaches for simulating FMCW radar are based on ray tracing, which
is usually computationally intensive and do not account for background noise.
This work proposes a faster method for FMCW radar simulation capable of
generating synthetic raw radar data using generative adversarial networks
(GAN). The code and pre-trained weights are open-source and available on
GitHub. This method generates 16 simultaneous chirps, which allows the
generated data to be used for the further development of algorithms for
processing radar data (filtering and clustering). This can increase the
potential for data augmentation, e.g., by generating data in non-existent or
safety-critical scenarios that are not reproducible in real life. In this work,
the GAN was trained with radar measurements of a motorcycle and used to
generate synthetic raw radar data of a motorcycle traveling in a straight line.
For generating this data, the distance of the motorcycle and Gaussian noise are
used as input to the neural network. The synthetic generated radar chirps were
evaluated using the Frechet Inception Distance (FID). Then, the Range-Azimuth
(RA) map is calculated twice: first, based on synthetic data using this GAN
and, second, based on real data. Based on these RA maps, an algorithm with
adaptive threshold and edge detection is used for object detection. The results
have shown that the data is realistic in terms of coherent radar reflections of
the motorcycle and background noise based on the comparison of chirps, the RA
maps and the object detection results. Thus, the proposed method in this work
has shown to minimize the simulation-to-reality gap for the generation of radar
data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fidelis_E/0/1/0/all/0/1&quot;&gt;Eduardo C. Fidelis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reway_F/0/1/0/all/0/1&quot;&gt;Fabio Reway&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribeiro_H/0/1/0/all/0/1&quot;&gt;Herick Y. S. Ribeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Campos_P/0/1/0/all/0/1&quot;&gt;Pietro L. Campos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huber_W/0/1/0/all/0/1&quot;&gt;Werner Huber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Icking_C/0/1/0/all/0/1&quot;&gt;Christian Icking&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faria_L/0/1/0/all/0/1&quot;&gt;Lester A. Faria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schon_T/0/1/0/all/0/1&quot;&gt;Torsten Sch&amp;#xf6;n&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02781">
<title>A Voting-Stacking Ensemble of Inception Networks for Cervical Cytology Classification. (arXiv:2308.02781v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.02781</link>
<description rdf:parseType="Literal">&lt;p&gt;Cervical cancer is one of the most severe diseases threatening women&apos;s
health. Early detection and diagnosis can significantly reduce cancer risk, in
which cervical cytology classification is indispensable. Researchers have
recently designed many networks for automated cervical cancer diagnosis, but
the limited accuracy and bulky size of these individual models cannot meet
practical application needs. To address this issue, we propose a
Voting-Stacking ensemble strategy, which employs three Inception networks as
base learners and integrates their outputs through a voting ensemble. The
samples misclassified by the ensemble model generate a new training set on
which a linear classification model is trained as the meta-learner and performs
the final predictions. In addition, a multi-level Stacking ensemble framework
is designed to improve performance further. The method is evaluated on the
SIPakMed, Herlev, and Mendeley datasets, achieving accuracies of 100%, 100%,
and 100%, respectively. The experimental results outperform the current
state-of-the-art (SOTA) methods, demonstrating its potential for reducing
screening workload and helping pathologists detect cervical cancer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_L/0/1/0/all/0/1&quot;&gt;Linyi Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qian Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yulin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junzhou Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03177">
<title>Boosting Few-shot 3D Point Cloud Segmentation via Query-Guided Enhancement. (arXiv:2308.03177v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.03177</link>
<description rdf:parseType="Literal">&lt;p&gt;Although extensive research has been conducted on 3D point cloud
segmentation, effectively adapting generic models to novel categories remains a
formidable challenge. This paper proposes a novel approach to improve point
cloud few-shot segmentation (PC-FSS) models. Unlike existing PC-FSS methods
that directly utilize categorical information from support prototypes to
recognize novel classes in query samples, our method identifies two critical
aspects that substantially enhance model performance by reducing contextual
gaps between support prototypes and query features. Specifically, we (1) adapt
support background prototypes to match query context while removing extraneous
cues that may obscure foreground and background in query samples, and (2)
holistically rectify support prototypes under the guidance of query features to
emulate the latter having no semantic gap to the query targets. Our proposed
designs are agnostic to the feature extractor, rendering them readily
applicable to any prototype-based methods. The experimental results on S3DIS
and ScanNet demonstrate notable practical benefits, as our approach achieves
significant improvements while still maintaining high efficiency. The code for
our approach is available at
https://github.com/AaronNZH/Boosting-Few-shot-3D-Point-Cloud-Segmentation-via-Query-Guided-Enhancement
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_Z/0/1/0/all/0/1&quot;&gt;Zhenhua Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1&quot;&gt;Zhuotao Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1&quot;&gt;Guangming Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_W/0/1/0/all/0/1&quot;&gt;Wenjie Pei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03276">
<title>Spatialyze: A Geospatial Video Analytics System with Spatial-Aware Optimizations. (arXiv:2308.03276v2 [cs.DB] UPDATED)</title>
<link>http://arxiv.org/abs/2308.03276</link>
<description rdf:parseType="Literal">&lt;p&gt;Videos that are shot using commodity hardware such as phones and surveillance
cameras record various metadata such as time and location. We encounter such
geospatial videos on a daily basis and such videos have been growing in volume
significantly. Yet, we do not have data management systems that allow users to
interact with such data effectively.
&lt;/p&gt;
&lt;p&gt;In this paper, we describe Spatialyze, a new framework for end-to-end
querying of geospatial videos. Spatialyze comes with a domain-specific language
where users can construct geospatial video analytic workflows using a 3-step,
declarative, build-filter-observe paradigm. Internally, Spatialyze leverages
the declarative nature of such workflows, the temporal-spatial metadata stored
with videos, and physical behavior of real-world objects to optimize the
execution of workflows. Our results using real-world videos and workflows show
that Spatialyze can reduce execution time by up to 5.3x, while maintaining up
to 97.1% accuracy compared to unoptimized execution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kittivorawong_C/0/1/0/all/0/1&quot;&gt;Chanwut Kittivorawong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yongming Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Helal_Y/0/1/0/all/0/1&quot;&gt;Yousef Helal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheung_A/0/1/0/all/0/1&quot;&gt;Alvin Cheung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03286">
<title>Multi-Label Self-Supervised Learning with Scene Images. (arXiv:2308.03286v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.03286</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised learning (SSL) methods targeting scene images have seen a
rapid growth recently, and they mostly rely on either a dedicated dense
matching mechanism or a costly unsupervised object discovery module. This paper
shows that instead of hinging on these strenuous operations, quality image
representations can be learned by treating scene/multi-label image SSL simply
as a multi-label classification problem, which greatly simplifies the learning
framework. Specifically, multiple binary pseudo-labels are assigned for each
input image by comparing its embeddings with those in two dictionaries, and the
network is optimized using the binary cross entropy loss. The proposed method
is named Multi-Label Self-supervised learning (MLS). Visualizations
qualitatively show that clearly the pseudo-labels by MLS can automatically find
semantically similar pseudo-positive pairs across different images to
facilitate contrastive learning. MLS learns high quality representations on
MS-COCO and achieves state-of-the-art results on classification, detection and
segmentation benchmarks. At the same time, MLS is much simpler than existing
methods, making it easier to deploy and for further exploration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1&quot;&gt;Ke Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_M/0/1/0/all/0/1&quot;&gt;Minghao Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jianxin Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03463">
<title>DiffSynth: Latent In-Iteration Deflickering for Realistic Video Synthesis. (arXiv:2308.03463v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.03463</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, diffusion models have emerged as the most powerful approach
in image synthesis. However, applying these models directly to video synthesis
presents challenges, as it often leads to noticeable flickering contents.
Although recently proposed zero-shot methods can alleviate flicker to some
extent, we still struggle to generate coherent videos. In this paper, we
propose DiffSynth, a novel approach that aims to convert image synthesis
pipelines to video synthesis pipelines. DiffSynth consists of two key
components: a latent in-iteration deflickering framework and a video
deflickering algorithm. The latent in-iteration deflickering framework applies
video deflickering to the latent space of diffusion models, effectively
preventing flicker accumulation in intermediate steps. Additionally, we propose
a video deflickering algorithm, named patch blending algorithm, that remaps
objects in different frames and blends them together to enhance video
consistency. One of the notable advantages of DiffSynth is its general
applicability to various video synthesis tasks, including text-guided video
stylization, fashion video synthesis, image-guided video stylization, video
restoring, and 3D rendering. In the task of text-guided video stylization, we
make it possible to synthesize high-quality videos without cherry-picking. The
experimental results demonstrate the effectiveness of DiffSynth. All videos can
be viewed on our project page. Source codes will also be released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_Z/0/1/0/all/0/1&quot;&gt;Zhongjie Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_L/0/1/0/all/0/1&quot;&gt;Lizhou You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chengyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Cen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Ziheng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_W/0/1/0/all/0/1&quot;&gt;Weining Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1&quot;&gt;Fei Chao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03529">
<title>Feature Decoupling-Recycling Network for Fast Interactive Segmentation. (arXiv:2308.03529v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.03529</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent interactive segmentation methods iteratively take source image, user
guidance and previously predicted mask as the input without considering the
invariant nature of the source image. As a result, extracting features from the
source image is repeated in each interaction, resulting in substantial
computational redundancy. In this work, we propose the Feature
Decoupling-Recycling Network (FDRN), which decouples the modeling components
based on their intrinsic discrepancies and then recycles components for each
user interaction. Thus, the efficiency of the whole interactive process can be
significantly improved. To be specific, we apply the Decoupling-Recycling
strategy from three perspectives to address three types of discrepancies,
respectively. First, our model decouples the learning of source image semantics
from the encoding of user guidance to process two types of input domains
separately. Second, FDRN decouples high-level and low-level features from
stratified semantic representations to enhance feature learning. Third, during
the encoding of user guidance, current user guidance is decoupled from
historical guidance to highlight the effect of current user guidance. We
conduct extensive experiments on 6 datasets from different domains and
modalities, which demonstrate the following merits of our model: 1) superior
efficiency than other methods, particularly advantageous in challenging
scenarios requiring long-term interactions (up to 4.25x faster), while
achieving favorable segmentation performance; 2) strong applicability to
various methods serving as a universal enhancement technique; 3) well
cross-task generalizability, e.g., to medical image segmentation, and
robustness against misleading user guidance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1&quot;&gt;Huimin Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weinong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1&quot;&gt;Xin Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1&quot;&gt;Yu-Wing Tai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_W/0/1/0/all/0/1&quot;&gt;Wenjie Pei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03698">
<title>Screen-based 3D Subjective Experiment Software. (arXiv:2308.03698v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2308.03698</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, widespread 3D graphics (e.g., point clouds and meshes) have drawn
considerable efforts from academia and industry to assess their perceptual
quality by conducting subjective experiments. However, lacking a handy software
for 3D subjective experiments complicates the construction of 3D graphics
quality assessment datasets, thus hindering the prosperity of relevant fields.
In this paper, we develop a powerful platform with which users can flexibly
design their 3D subjective methodologies and build high-quality datasets,
easing a broad spectrum of 3D graphics subjective quality study. To accurately
illustrate the perceptual quality differences of 3D stimuli, our software can
simultaneously render the source stimulus and impaired stimulus and allows both
stimuli to respond synchronously to viewer interactions. Compared with amateur
3D visualization tool-based or image/video rendering-based schemes, our
approach embodies typical 3D applications while minimizing cognitive overload
during subjective experiments. We organized a subjective experiment involving
40 participants to verify the validity of the proposed software. Experimental
analyses demonstrate that subjective tests on our software can produce
reasonable subjective quality scores of 3D models. All resources in this paper
can be found at https://openi.pcl.ac.cn/OpenDatasets/3DQA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1&quot;&gt;Songlin Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1&quot;&gt;Wei Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.00085">
<title>Machine Learning and Computer Vision Techniques in Bee Monitoring Applications. (arXiv:2208.00085v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2208.00085</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning and computer vision are dynamically growing fields, which
have proven to be able to solve very complex tasks. They could also be used for
the monitoring of the honeybee colonies and for the inspection of their health
state, which could identify potentially dangerous states before the situation
is critical, or to better plan periodic bee colony inspections and therefore
save significant costs. In this paper, we present an overview of the
state-of-the-art computer vision and machine learning applications used for bee
monitoring. We also demonstrate the potential of those methods as an example of
an automated bee counter algorithm. The paper is aimed at veterinary and
apidology professionals and experts, who might not be familiar with machine
learning to introduce to them its possibilities, therefore each family of
applications is opened by a brief theoretical introduction and motivation
related to its base method. We hope that this paper will inspire other
scientists to use the machine learning techniques for other applications in bee
monitoring.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bilik_S/0/1/0/all/0/1&quot;&gt;Simon Bilik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bostik_O/0/1/0/all/0/1&quot;&gt;Ondrej Bostik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kratochvila_L/0/1/0/all/0/1&quot;&gt;Lukas Kratochvila&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ligocki_A/0/1/0/all/0/1&quot;&gt;Adam Ligocki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poncak_M/0/1/0/all/0/1&quot;&gt;Matej Poncak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zemcik_T/0/1/0/all/0/1&quot;&gt;Tomas Zemcik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richter_M/0/1/0/all/0/1&quot;&gt;Milos Richter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Janakova_I/0/1/0/all/0/1&quot;&gt;Ilona Janakova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Honec_P/0/1/0/all/0/1&quot;&gt;Petr Honec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horak_K/0/1/0/all/0/1&quot;&gt;Karel Horak&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>