<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-12-04T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00794" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00795" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00803" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00823" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00824" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00825" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00826" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00827" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00833" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00834" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00836" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00837" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00840" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00844" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00845" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00846" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00849" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00851" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00852" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00853" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00856" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00858" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00860" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00863" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00869" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00870" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00878" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00923" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00937" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00944" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00947" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00950" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00968" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00971" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00987" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01001" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01003" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01017" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01026" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01027" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2003.03923" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2004.11968" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2104.13742" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2107.07468" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.01592" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.02149" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.03408" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.02598" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.13381" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.13219" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.03320" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.15451" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.00094" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.07675" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.11388" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.13452" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.11530" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.04517" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.07475" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.09602" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.03750" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.04440" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.03307" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.06908" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.08240" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.09051" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.12484" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.13397" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.13514" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.13843" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.17580" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.17783" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.00306" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.06433" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13622" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15712" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16746" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19518" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00783" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.02956" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04527" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04633" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04848" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06082" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07581" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08736" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08939" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11300" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01197" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02129" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03166" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06614" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08596" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10135" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10205" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.16449" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03982" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04774" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.06762" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11239" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11909" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.14831" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.15984" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.05073" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.09039" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13132" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13550" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13925" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16849" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00119" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00826" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01596" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01779" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02048" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03513" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.09718" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.09909" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.13135" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15020" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16267" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18116" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18961" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19112" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00690" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01619" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08239" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08843" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11284" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11629" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12553" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12589" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12775" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12919" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13398" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14750" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16450" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16552" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16565" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17005" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17009" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17135" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17245" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17280" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17465" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17629" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17971" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.18068" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.18605" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.18803" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.18828" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00311" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00360" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00761" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2312.00794">
<title>Informative Priors Improve the Reliability of Multimodal Clinical Data Classification. (arXiv:2312.00794v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00794</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning-aided clinical decision support has the potential to
significantly improve patient care. However, existing efforts in this domain
for principled quantification of uncertainty have largely been limited to
applications of ad-hoc solutions that do not consistently improve reliability.
In this work, we consider stochastic neural networks and design a tailor-made
multimodal data-driven (M2D2) prior distribution over network parameters. We
use simple and scalable Gaussian mean-field variational inference to train a
Bayesian neural network using the M2D2 prior. We train and evaluate the
proposed approach using clinical time-series data in MIMIC-IV and corresponding
chest X-ray images in MIMIC-CXR for the classification of acute care
conditions. Our empirical results show that the proposed method produces a more
reliable predictive model compared to deterministic and Bayesian neural network
baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopez_L/0/1/0/all/0/1&quot;&gt;L. Julian Lechuga Lopez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rudner_T/0/1/0/all/0/1&quot;&gt;Tim G. J. Rudner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shamout_F/0/1/0/all/0/1&quot;&gt;Farah E. Shamout&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00795">
<title>Talent-Interview: Web-Client Cheating Detection for Online Exams. (arXiv:2312.00795v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00795</link>
<description rdf:parseType="Literal">&lt;p&gt;Online exams are more attractive after the Covid-19 pandemic. Furthermore,
during recruitment, online exams are used. However, there are more cheating
possibilities for online exams. Assigning a proctor for each exam increases
cost. At this point, automatic proctor systems detect possible cheating status.
This article proposes an end-to-end system and submodules to get better results
for online proctoring. Object detection, face recognition, human voice
detection, and segmentation are used in our system. Furthermore, our proposed
model works on the PCs of users, meaning a client-based system. So, server cost
is eliminated. As far as we know, it is the first time the client-based online
proctoring system has been used for recruitment. Online exams are more
attractive after the Covid-19 pandemic. Furthermore, during recruitment, online
exams are used. However, there are more cheating possibilities for online
exams. Assigning a proctor for each exam increases cost. At this point,
automatic proctor systems detect possible cheating status. This article
proposes an end-to-end system and submodules to get better results for online
proctoring. Object detection, face recognition, human voice detection, and
segmentation are used in our system. Furthermore, our proposed model works on
the PCs of users, meaning a client-based system. So, server cost is eliminated.
As far as we know, it is the first time the client-based online proctoring
system has been used for recruitment. Furthermore, this cheating system works
at https://www.talent-interview.com/tr/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ege_M/0/1/0/all/0/1&quot;&gt;Mert Ege&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ceyhan_M/0/1/0/all/0/1&quot;&gt;Mustafa Ceyhan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00803">
<title>InceptionCaps: A Performant Glaucoma Classification Model for Data-scarce Environment. (arXiv:2312.00803v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00803</link>
<description rdf:parseType="Literal">&lt;p&gt;Glaucoma is an irreversible ocular disease and is the second leading cause of
visual disability worldwide. Slow vision loss and the asymptomatic nature of
the disease make its diagnosis challenging. Early detection is crucial for
preventing irreversible blindness. Ophthalmologists primarily use retinal
fundus images as a non-invasive screening method. Convolutional neural networks
(CNN) have demonstrated high accuracy in the classification of medical images.
Nevertheless, CNN&apos;s translation-invariant nature and inability to handle the
part-whole relationship between objects make its direct application unsuitable
for glaucomatous fundus image classification, as it requires a large number of
labelled images for training. This work reviews existing state of the art
models and proposes InceptionCaps, a novel capsule network (CapsNet) based deep
learning model having pre-trained InceptionV3 as its convolution base, for
automatic glaucoma classification. InceptionCaps achieved an accuracy of 0.956,
specificity of 0.96, and AUC of 0.9556, which surpasses several
state-of-the-art deep learning model performances on the RIM-ONE v2 dataset.
The obtained result demonstrates the robustness of the proposed deep learning
model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manohar_G/0/1/0/all/0/1&quot;&gt;Gyanendar Manohar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OReilly_R/0/1/0/all/0/1&quot;&gt;Ruairi O&amp;#x27;Reilly&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00823">
<title>Adaptive Multi-Modality Prompt Learning. (arXiv:2312.00823v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.00823</link>
<description rdf:parseType="Literal">&lt;p&gt;Although current prompt learning methods have successfully been designed to
effectively reuse the large pre-trained models without fine-tuning their large
number of parameters, they still have limitations to be addressed, i.e.,
without considering the adverse impact of meaningless patches in every image
and without simultaneously considering in-sample generalization and
out-of-sample generalization. In this paper, we propose an adaptive
multi-modality prompt learning to address the above issues. To do this, we
employ previous text prompt learning and propose a new image prompt learning.
The image prompt learning achieves in-sample and out-of-sample generalization,
by first masking meaningless patches and then padding them with the learnable
parameters and the information from texts. Moreover, each of the prompts
provides auxiliary information to each other, further strengthening these two
kinds of generalization. Experimental results on real datasets demonstrate that
our method outperforms SOTA methods, in terms of different downstream tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zongqian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yujing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_M/0/1/0/all/0/1&quot;&gt;Mengmeng Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1&quot;&gt;Jialie Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1&quot;&gt;Ping Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00824">
<title>Variational Self-Supervised Contrastive Learning Using Beta Divergence. (arXiv:2312.00824v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00824</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning a discriminative semantic space using unlabelled and noisy data
remains unaddressed in a multi-label setting. We present a contrastive
self-supervised learning method which is robust to data noise, grounded in the
domain of variational methods. The method (VCL) utilizes variational
contrastive learning with beta-divergence to learn robustly from unlabelled
datasets, including uncurated and noisy datasets. We demonstrate the
effectiveness of the proposed method through rigorous experiments including
linear evaluation and fine-tuning scenarios with multi-label datasets in the
face understanding domain. In almost all tested scenarios, VCL surpasses the
performance of state-of-the-art self-supervised methods, achieving a noteworthy
increase in accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yavuz_M/0/1/0/all/0/1&quot;&gt;Mehmet Can Yavuz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yanikoglu_B/0/1/0/all/0/1&quot;&gt;Berrin Yanikoglu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00825">
<title>Probing and Mitigating Intersectional Social Biases in Vision-Language Models with Counterfactual Examples. (arXiv:2312.00825v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00825</link>
<description rdf:parseType="Literal">&lt;p&gt;While vision-language models (VLMs) have achieved remarkable performance
improvements recently, there is growing evidence that these models also posses
harmful biases with respect to social attributes such as gender and race. Prior
studies have primarily focused on probing such bias attributes individually
while ignoring biases associated with intersections between social attributes.
This could be due to the difficulty of collecting an exhaustive set of
image-text pairs for various combinations of social attributes. To address this
challenge, we employ text-to-image diffusion models to produce counterfactual
examples for probing intserctional social biases at scale. Our approach
utilizes Stable Diffusion with cross attention control to produce sets of
counterfactual image-text pairs that are highly similar in their depiction of a
subject (e.g., a given occupation) while differing only in their depiction of
intersectional social attributes (e.g., race &amp;amp; gender). Through our
over-generate-then-filter methodology, we produce SocialCounterfactuals, a
high-quality dataset containing over 171k image-text pairs for probing
intersectional biases related to gender, race, and physical characteristics. We
conduct extensive experiments to demonstrate the usefulness of our generated
dataset for probing and mitigating intersectional social biases in
state-of-the-art VLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Howard_P/0/1/0/all/0/1&quot;&gt;Phillip Howard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madasu_A/0/1/0/all/0/1&quot;&gt;Avinash Madasu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1&quot;&gt;Tiep Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moreno_G/0/1/0/all/0/1&quot;&gt;Gustavo Lujan Moreno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhiwandiwalla_A/0/1/0/all/0/1&quot;&gt;Anahita Bhiwandiwalla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lal_V/0/1/0/all/0/1&quot;&gt;Vasudev Lal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00826">
<title>DEVIAS: Learning Disentangled Video Representations of Action and Scene for Holistic Video Understanding. (arXiv:2312.00826v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00826</link>
<description rdf:parseType="Literal">&lt;p&gt;When watching a video, humans can naturally extract human actions from the
surrounding scene context, even when action-scene combinations are unusual.
However, unlike humans, video action recognition models often learn
scene-biased action representations from the spurious correlation in training
data, leading to poor performance in out-of-context scenarios. While
scene-debiased models achieve improved performance in out-of-context scenarios,
they often overlook valuable scene information in the data. Addressing this
challenge, we propose Disentangled VIdeo representations of Action and Scene
(DEVIAS), which aims to achieve holistic video understanding. Disentangled
action and scene representations with our method could provide flexibility to
adjust the emphasis on action or scene information depending on downstream task
and dataset characteristics. Disentangled action and scene representations
could be beneficial for both in-context and out-of-context video understanding.
To this end, we employ slot attention to learn disentangled action and scene
representations with a single model, along with auxiliary tasks that further
guide slot attention. We validate the proposed method on both in-context
datasets: UCF-101 and Kinetics-400, and out-of-context datasets: SCUBA and HAT.
Our proposed method shows favorable performance across different datasets
compared to the baselines, demonstrating its effectiveness in diverse video
understanding scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bae_K/0/1/0/all/0/1&quot;&gt;Kyungho Bae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahn_G/0/1/0/all/0/1&quot;&gt;Geo Ahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Youngrae Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jinwoo Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00827">
<title>A Unified Framework for Connecting Noise Modeling to Boost Noise Detection. (arXiv:2312.00827v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00827</link>
<description rdf:parseType="Literal">&lt;p&gt;Noisy labels can impair model performance, making the study of learning with
noisy labels an important topic. Two conventional approaches are noise modeling
and noise detection. However, these two methods are typically studied
independently, and there has been limited work on their collaboration. In this
work, we explore the integration of these two approaches, proposing an
interconnected structure with three crucial blocks: noise modeling, source
knowledge identification, and enhanced noise detection using noise
source-knowledge-integration methods. This collaboration structure offers
advantages such as discriminating hard negatives and preserving genuinely clean
labels that might be suspiciously noisy. Our experiments on four datasets,
featuring three types of noise and different combinations of each block,
demonstrate the efficacy of these components&apos; collaboration. Our collaborative
structure methods achieve up to a 10% increase in top-1 classification accuracy
in synthesized noise datasets and 3-5% in real-world noisy datasets. The
results also suggest that these components make distinct contributions to
overall performance across various noise scenarios. These findings provide
valuable insights for designing noisy label learning methods customized for
specific noise scenarios in the future. Our code is accessible to the public.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Siqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_C/0/1/0/all/0/1&quot;&gt;Chau Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plummer_B/0/1/0/all/0/1&quot;&gt;Bryan A. Plummer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00833">
<title>Lasagna: Layered Score Distillation for Disentangled Object Relighting. (arXiv:2312.00833v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00833</link>
<description rdf:parseType="Literal">&lt;p&gt;Professional artists, photographers, and other visual content creators use
object relighting to establish their photo&apos;s desired effect. Unfortunately,
manual tools that allow relighting have a steep learning curve and are
difficult to master. Although generative editing methods now enable some forms
of image editing, relighting is still beyond today&apos;s capabilities; existing
methods struggle to keep other aspects of the image -- colors, shapes, and
textures -- consistent after the edit. We propose Lasagna, a method that
enables intuitive text-guided relighting control. Lasagna learns a lighting
prior by using score distillation sampling to distill the prior of a diffusion
model, which has been finetuned on synthetic relighting data. To train Lasagna,
we curate a new synthetic dataset ReLiT, which contains 3D object assets re-lit
from multiple light source locations. Despite training on synthetic images,
quantitative results show that Lasagna relights real-world images while
preserving other aspects of the input image, outperforming state-of-the-art
text-guided image editing methods. Lasagna enables realistic and controlled
results on natural images and digital art pieces and is preferred by humans
over other methods in over 91% of cases. Finally, we demonstrate the
versatility of our learning objective by extending it to allow colorization,
another form of image editing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bashkirova_D/0/1/0/all/0/1&quot;&gt;Dina Bashkirova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1&quot;&gt;Arijit Ray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mallick_R/0/1/0/all/0/1&quot;&gt;Rupayan Mallick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bargal_S/0/1/0/all/0/1&quot;&gt;Sarah Adel Bargal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1&quot;&gt;Ranjay Krishna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1&quot;&gt;Kate Saenko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00834">
<title>AV-RIR: Audio-Visual Room Impulse Response Estimation. (arXiv:2312.00834v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2312.00834</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate estimation of Room Impulse Response (RIR), which captures an
environment&apos;s acoustic properties, is important for speech processing and AR/VR
applications. We propose AV-RIR, a novel multi-modal multi-task learning
approach to accurately estimate the RIR from a given reverberant speech signal
and the visual cues of its corresponding environment. AV-RIR builds on a novel
neural codec-based architecture that effectively captures environment geometry
and materials properties and solves speech dereverberation as an auxiliary task
by using multi-task learning. We also propose Geo-Mat features that augment
material information into visual cues and CRIP that improves late reverberation
components in the estimated RIR via image-to-RIR retrieval by 86%. Empirical
results show that AV-RIR quantitatively outperforms previous audio-only and
visual-only approaches by achieving 36% - 63% improvement across various
acoustic metrics in RIR estimation. Additionally, it also achieves higher
preference scores in human evaluation. As an auxiliary benefit, dereverbed
speech from AV-RIR shows competitive performance with the state-of-the-art in
various spoken language processing tasks and outperforms reverberation time
error score in the real-world AVSpeech dataset. Qualitative examples of both
synthesized reverberant speech and enhanced speech can be found at
https://www.youtube.com/watch?v=tTsKhviukAE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ratnarajah_A/0/1/0/all/0/1&quot;&gt;Anton Ratnarajah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Sreyan Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Sonal Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiniya_P/0/1/0/all/0/1&quot;&gt;Purva Chiniya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1&quot;&gt;Dinesh Manocha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00836">
<title>Heteroscedastic Uncertainty Estimation for Probabilistic Unsupervised Registration of Noisy Medical Images. (arXiv:2312.00836v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.00836</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a heteroscedastic uncertainty estimation framework for
unsupervised medical image registration. Existing methods rely on objectives
(e.g. mean-squared error) that assume a uniform noise level across the image,
disregarding the heteroscedastic and input-dependent characteristics of noise
distribution in real-world medical images. This further introduces noisy
gradients due to undesired penalization on outliers, causing unnatural
deformation and performance degradation. To mitigate this, we propose an
adaptive weighting scheme with a relative $\gamma$-exponentiated
signal-to-noise ratio (SNR) for the displacement estimator after modeling the
heteroscedastic noise using a separate variance estimator to prevent the model
from being driven away by spurious gradients from error residuals, leading to
more accurate displacement estimation. To illustrate the versatility and
effectiveness of the proposed method, we tested our framework on two
representative registration architectures across three medical image datasets.
Our proposed framework consistently outperforms other baselines both
quantitatively and qualitatively while also providing accurate and sensible
uncertainty measures. Paired t-tests show that our improvements in registration
accuracy are statistically significant. The code will be publicly available at
\url{https://voldemort108x.github.io/hetero_uncertainty/}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoran Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pak_D/0/1/0/all/0/1&quot;&gt;Daniel H. Pak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ahn_S/0/1/0/all/0/1&quot;&gt;Shawn S. Ahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+You_C/0/1/0/all/0/1&quot;&gt;Chenyu You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Staib_L/0/1/0/all/0/1&quot;&gt;Lawrence Staib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sinusas_A/0/1/0/all/0/1&quot;&gt;Albert J. Sinusas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wong_A/0/1/0/all/0/1&quot;&gt;Alex Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Duncan_J/0/1/0/all/0/1&quot;&gt;James S. Duncan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00837">
<title>An Adaptive Correspondence Scoring Framework for Unsupervised Image Registration of Medical Images. (arXiv:2312.00837v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.00837</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an adaptive training scheme for unsupervised medical image
registration. Existing methods rely on image reconstruction as the primary
supervision signal. However, nuisance variables (e.g. noise and covisibility)
often cause the loss of correspondence between medical images, violating the
Lambertian assumption in physical waves (e.g. ultrasound) and consistent
imaging acquisition. As the unsupervised learning scheme relies on intensity
constancy to establish correspondence between images for reconstruction, this
introduces spurious error residuals that are not modeled by the typical
training objective. To mitigate this, we propose an adaptive framework that
re-weights the error residuals with a correspondence scoring map during
training, preventing the parametric displacement estimator from drifting away
due to noisy gradients, which leads to performance degradations. To illustrate
the versatility and effectiveness of our method, we tested our framework on
three representative registration architectures across three medical image
datasets along with other baselines. Our proposed adaptive framework
consistently outperforms other methods both quantitatively and qualitatively.
Paired t-tests show that our improvements are statistically significant. The
code will be publicly available at
\url{https://voldemort108x.github.io/AdaCS/}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoran Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Stendahl_J/0/1/0/all/0/1&quot;&gt;John C. Stendahl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Staib_L/0/1/0/all/0/1&quot;&gt;Lawrence Staib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sinusas_A/0/1/0/all/0/1&quot;&gt;Albert J. Sinusas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wong_A/0/1/0/all/0/1&quot;&gt;Alex Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Duncan_J/0/1/0/all/0/1&quot;&gt;James S. Duncan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00840">
<title>Towards Redundancy-Free Sub-networks in Continual Learning. (arXiv:2312.00840v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.00840</link>
<description rdf:parseType="Literal">&lt;p&gt;Catastrophic Forgetting (CF) is a prominent issue in continual learning.
Parameter isolation addresses this challenge by masking a sub-network for each
task to mitigate interference with old tasks. However, these sub-networks are
constructed relying on weight magnitude, which does not necessarily correspond
to the importance of weights, resulting in maintaining unimportant weights and
constructing redundant sub-networks. To overcome this limitation, inspired by
information bottleneck, which removes redundancy between adjacent network
layers, we propose \textbf{\underline{I}nformation \underline{B}ottleneck
\underline{M}asked sub-network (IBM)} to eliminate redundancy within
sub-networks. Specifically, IBM accumulates valuable information into essential
weights to construct redundancy-free sub-networks, not only effectively
mitigating CF by freezing the sub-networks but also facilitating new tasks
training through the transfer of valuable knowledge. Additionally, IBM
decomposes hidden representations to automate the construction process and make
it flexible. Extensive experiments demonstrate that IBM consistently
outperforms state-of-the-art methods. Notably, IBM surpasses the
state-of-the-art parameter isolation method with a 70\% reduction in the number
of parameters within sub-networks and an 80\% decrease in training time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Cheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jingkuan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1&quot;&gt;LianLi Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Heng Tao Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00844">
<title>Sparse Beats Dense: Rethinking Supervision in Radar-Camera Depth Completion. (arXiv:2312.00844v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00844</link>
<description rdf:parseType="Literal">&lt;p&gt;It is widely believed that the dense supervision is better than the sparse
supervision in the field of depth completion, but the underlying reasons for
this are rarely discussed. In this paper, we find that the challenge of using
sparse supervision for training Radar-Camera depth prediction models is the
Projection Transformation Collapse (PTC). The PTC implies that sparse
supervision leads the model to learn unexpected collapsed projection
transformations between Image/Radar/LiDAR spaces. Building on this insight, we
propose a novel ``Disruption-Compensation&quot; framework to handle the PTC, thereby
relighting the use of sparse supervision in depth completion tasks. The
disruption part deliberately discards position correspondences among
Image/Radar/LiDAR, while the compensation part leverages 3D spatial and 2D
semantic information to compensate for the discarded beneficial position
correspondence. Extensive experimental results demonstrate that our framework
(sparse supervision) outperforms the state-of-the-art (dense supervision) with
11.6$\%$ improvement in mean absolute error and $1.6 \times$ speedup. The code
is available at ...
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huadong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_M/0/1/0/all/0/1&quot;&gt;Minhao Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Jiajun Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1&quot;&gt;Haoqiang Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1&quot;&gt;Renhe Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00845">
<title>VMC: Video Motion Customization using Temporal Attention Adaption for Text-to-Video Diffusion Models. (arXiv:2312.00845v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00845</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-video diffusion models have advanced video generation significantly.
However, customizing these models to generate videos with tailored motions
presents a substantial challenge. In specific, they encounter hurdles in (a)
accurately reproducing motion from a target video, and (b) creating diverse
visual variations. For example, straightforward extensions of static image
customization methods to video often lead to intricate entanglements of
appearance and motion data. To tackle this, here we present the Video Motion
Customization (VMC) framework, a novel one-shot tuning approach crafted to
adapt temporal attention layers within video diffusion models. Our approach
introduces a novel motion distillation objective using residual vectors between
consecutive frames as a motion reference. The diffusion process then preserves
low-frequency motion trajectories while mitigating high-frequency
motion-unrelated noise in image space. We validate our method against
state-of-the-art video generative models across diverse real-world motions and
contexts. Our codes, data and the project demo can be found at
https://video-motion-customization.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_H/0/1/0/all/0/1&quot;&gt;Hyeonho Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_G/0/1/0/all/0/1&quot;&gt;Geon Yeong Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jong Chul Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00846">
<title>NeuSG: Neural Implicit Surface Reconstruction with 3D Gaussian Splatting Guidance. (arXiv:2312.00846v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00846</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing neural implicit surface reconstruction methods have achieved
impressive performance in multi-view 3D reconstruction by leveraging explicit
geometry priors such as depth maps or point clouds as regularization. However,
the reconstruction results still lack fine details because of the over-smoothed
depth map or sparse point cloud. In this work, we propose a neural implicit
surface reconstruction pipeline with guidance from 3D Gaussian Splatting to
recover highly detailed surfaces. The advantage of 3D Gaussian Splatting is
that it can generate dense point clouds with detailed structure. Nonetheless, a
naive adoption of 3D Gaussian Splatting can fail since the generated points are
the centers of 3D Gaussians that do not necessarily lie on the surface. We thus
introduce a scale regularizer to pull the centers close to the surface by
enforcing the 3D Gaussians to be extremely thin. Moreover, we propose to refine
the point cloud from 3D Gaussians Splatting with the normal priors from the
surface predicted by neural implicit models instead of using a fixed set of
points as guidance. Consequently, the quality of surface reconstruction
improves from the guidance of the more accurate 3D Gaussian splatting. By
jointly optimizing the 3D Gaussian Splatting and the neural implicit model, our
approach benefits from both representations and generates complete surfaces
with intricate details. Experiments on Tanks and Temples verify the
effectiveness of our proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hanlin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1&quot;&gt;Gim Hee Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00849">
<title>RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback. (arXiv:2312.00849v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.00849</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal Large Language Models (MLLMs) have recently demonstrated
impressive capabilities in multimodal understanding, reasoning, and
interaction. However, existing MLLMs prevalently suffer from serious
hallucination problems, generating text that is not factually grounded in
associated images. The problem makes existing MLLMs untrustworthy and thus
impractical in real-world (especially high-stakes) applications. To address the
challenge, we present RLHF-V, which enhances MLLM trustworthiness via behavior
alignment from fine-grained correctional human feedback. Specifically, RLHF-V
collects human preference in the form of segment-level corrections on
hallucinations, and performs dense direct preference optimization over the
human feedback. Comprehensive experiments on five benchmarks in both automatic
and human evaluation show that, RLHF-V can enable substantially more
trustworthy MLLM behaviors with promising data and computation efficiency.
Remarkably, using 1.4k annotated data samples, RLHF-V significantly reduces the
hallucination rate of the base MLLM by 34.8%, outperforming the concurrent
LLaVA-RLHF trained on 10k annotated data. The final model achieves
state-of-the-art performance in trustworthiness among open-source MLLMs, and
shows better robustness than GPT-4V in preventing hallucinations aroused from
over-generalization. We open-source our code, model, and data at
https://github.com/RLHF-V/RLHF-V.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1&quot;&gt;Tianyu Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yuan Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haoye Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1&quot;&gt;Taiwen He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yifeng Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_G/0/1/0/all/0/1&quot;&gt;Ganqu Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jinyi Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Hai-Tao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;Maosong Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1&quot;&gt;Tat-Seng Chua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00851">
<title>Physics Inspired Criterion for Pruning-Quantization Joint Learning. (arXiv:2312.00851v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.00851</link>
<description rdf:parseType="Literal">&lt;p&gt;Pruning-quantization joint learning always facilitates the deployment of deep
neural networks (DNNs) on resource-constrained edge devices. However, most
existing methods do not jointly learn a global criterion for pruning and
quantization in an interpretable way. In this paper, we propose a novel physics
inspired criterion for pruning-quantization joint learning (PIC-PQ), which is
explored from an analogy we first draw between elasticity dynamics (ED) and
model compression (MC). Specifically, derived from Hooke&apos;s law in ED, we
establish a linear relationship between the filters&apos; importance distribution
and the filter property (FP) by a learnable deformation scale in the physics
inspired criterion (PIC). Furthermore, we extend PIC with a relative shift
variable for a global view. To ensure feasibility and flexibility, available
maximum bitwidth and penalty factor are introduced in quantization bitwidth
assignment. Experiments on benchmarks of image classification demonstrate that
PIC-PQ yields a good trade-off between accuracy and bit-operations (BOPs)
compression ratio e.g., 54.96X BOPs compression ratio in ResNet56 on CIFAR10
with 0.10% accuracy drop and 53.24X in ResNet18 on ImageNet with 0.61% accuracy
drop). The code will be available at https://github.com/fanxxxxyi/PIC-PQ.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1&quot;&gt;Weiying Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1&quot;&gt;Xiaoyi Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunsong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1&quot;&gt;Jie Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1&quot;&gt;Leyuan Fang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00852">
<title>Beyond First-Order Tweedie: Solving Inverse Problems using Latent Diffusion. (arXiv:2312.00852v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.00852</link>
<description rdf:parseType="Literal">&lt;p&gt;Sampling from the posterior distribution poses a major computational
challenge in solving inverse problems using latent diffusion models. Common
methods rely on Tweedie&apos;s first-order moments, which are known to induce a
quality-limiting bias. Existing second-order approximations are impractical due
to prohibitive computational costs, making standard reverse diffusion processes
intractable for posterior sampling. This paper introduces Second-order Tweedie
sampler from Surrogate Loss (STSL), a novel sampler that offers efficiency
comparable to first-order Tweedie with a tractable reverse process using
second-order approximation. Our theoretical results reveal that the
second-order approximation is lower bounded by our surrogate loss that only
requires $O(1)$ compute using the trace of the Hessian, and by the lower bound
we derive a new drift term to make the reverse process tractable. Our method
surpasses SoTA solvers PSLD and P2L, achieving 4X and 8X reduction in neural
function evaluations, respectively, while notably enhancing sampling quality on
FFHQ, ImageNet, and COCO benchmarks. In addition, we show STSL extends to
text-guided image editing and addresses residual distortions present from
corrupted images in leading text-guided image editing methods. To our best
knowledge, this is the first work to offer an efficient second-order
approximation in solving inverse problems using latent diffusion and editing
real-world images with corruptions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rout_L/0/1/0/all/0/1&quot;&gt;Litu Rout&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yujia Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Abhishek Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caramanis_C/0/1/0/all/0/1&quot;&gt;Constantine Caramanis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1&quot;&gt;Sanjay Shakkottai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1&quot;&gt;Wen-Sheng Chu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00853">
<title>Motion-Guided Latent Diffusion for Temporally Consistent Real-world Video Super-resolution. (arXiv:2312.00853v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00853</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-world low-resolution (LR) videos have diverse and complex degradations,
imposing great challenges on video super-resolution (VSR) algorithms to
reproduce their high-resolution (HR) counterparts with high quality. Recently,
the diffusion models have shown compelling performance in generating realistic
details for image restoration tasks. However, the diffusion process has
randomness, making it hard to control the contents of restored images. This
issue becomes more serious when applying diffusion models to VSR tasks because
temporal consistency is crucial to the perceptual quality of videos. In this
paper, we propose an effective real-world VSR algorithm by leveraging the
strength of pre-trained latent diffusion models. To ensure the content
consistency among adjacent frames, we exploit the temporal dynamics in LR
videos to guide the diffusion process by optimizing the latent sampling path
with a motion-guided loss, ensuring that the generated HR video maintains a
coherent and continuous visual flow. To further mitigate the discontinuity of
generated details, we insert temporal module to the decoder and fine-tune it
with an innovative sequence-oriented loss. The proposed motion-guided latent
diffusion (MGLD) based VSR algorithm achieves significantly better perceptual
quality than state-of-the-arts on real-world VSR benchmark datasets, validating
the effectiveness of the proposed model design and training strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1&quot;&gt;Chenhang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jianqi Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00856">
<title>QAFE-Net: Quality Assessment of Facial Expressions with Landmark Heatmaps. (arXiv:2312.00856v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00856</link>
<description rdf:parseType="Literal">&lt;p&gt;Facial expression recognition (FER) methods have made great inroads in
categorising moods and feelings in humans. Beyond FER, pain estimation methods
assess levels of intensity in pain expressions, however assessing the quality
of all facial expressions is of critical value in health-related applications.
In this work, we address the quality of five different facial expressions in
patients affected by Parkinson&apos;s disease. We propose a novel landmark-guided
approach, QAFE-Net, that combines temporal landmark heatmaps with RGB data to
capture small facial muscle movements that are encoded and mapped to severity
scores. The proposed approach is evaluated on a new Parkinson&apos;s Disease Facial
Expression dataset (PFED5), as well as on the pain estimation benchmark, the
UNBC-McMaster Shoulder Pain Expression Archive Database. Our comparative
experiments demonstrate that the proposed method outperforms SOTA action
quality assessment works on PFED5 and achieves lower mean absolute error than
the SOTA pain estimation methods on UNBC-McMaster. Our code and the new PFED5
dataset are available at https://github.com/shuchaoduan/QAFE-Net.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_S/0/1/0/all/0/1&quot;&gt;Shuchao Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dadashzadeh_A/0/1/0/all/0/1&quot;&gt;Amirhossein Dadashzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whone_A/0/1/0/all/0/1&quot;&gt;Alan Whone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirmehdi_M/0/1/0/all/0/1&quot;&gt;Majid Mirmehdi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00858">
<title>DeepCache: Accelerating Diffusion Models for Free. (arXiv:2312.00858v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00858</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have recently gained unprecedented attention in the field of
image synthesis due to their remarkable generative capabilities.
Notwithstanding their prowess, these models often incur substantial
computational costs, primarily attributed to the sequential denoising process
and cumbersome model size. Traditional methods for compressing diffusion models
typically involve extensive retraining, presenting cost and feasibility
challenges. In this paper, we introduce DeepCache, a novel training-free
paradigm that accelerates diffusion models from the perspective of model
architecture. DeepCache capitalizes on the inherent temporal redundancy
observed in the sequential denoising steps of diffusion models, which caches
and retrieves features across adjacent denoising stages, thereby curtailing
redundant computations. Utilizing the property of the U-Net, we reuse the
high-level features while updating the low-level features in a very cheap way.
This innovative strategy, in turn, enables a speedup factor of 2.3$\times$ for
Stable Diffusion v1.5 with only a 0.05 decline in CLIP Score, and 4.1$\times$
for LDM-4-G with a slight decrease of 0.22 in FID on ImageNet. Our experiments
also demonstrate DeepCache&apos;s superiority over existing pruning and distillation
methods that necessitate retraining and its compatibility with current sampling
techniques. Furthermore, we find that under the same throughput, DeepCache
effectively achieves comparable or even marginally improved results with DDIM
or PLMS. The code is available at https://github.com/horseee/DeepCache
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xinyin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_G/0/1/0/all/0/1&quot;&gt;Gongfan Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinchao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00860">
<title>Segment Any 3D Gaussians. (arXiv:2312.00860v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00860</link>
<description rdf:parseType="Literal">&lt;p&gt;Interactive 3D segmentation in radiance fields is an appealing task since its
importance in 3D scene understanding and manipulation. However, existing
methods face challenges in either achieving fine-grained, multi-granularity
segmentation or contending with substantial computational overhead, inhibiting
real-time interaction. In this paper, we introduce Segment Any 3D GAussians
(SAGA), a novel 3D interactive segmentation approach that seamlessly blends a
2D segmentation foundation model with 3D Gaussian Splatting (3DGS), a recent
breakthrough of radiance fields. SAGA efficiently embeds multi-granularity 2D
segmentation results generated by the segmentation foundation model into 3D
Gaussian point features through well-designed contrastive training. Evaluation
on existing benchmarks demonstrates that SAGA can achieve competitive
performance with state-of-the-art methods. Moreover, SAGA achieves
multi-granularity segmentation and accommodates various prompts, including
points, scribbles, and 2D masks. Notably, SAGA can finish the 3D segmentation
within milliseconds, achieving nearly 1000x acceleration compared to previous
SOTA. The project page is at https://jumpat.github.io/SAGA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cen_J/0/1/0/all/0/1&quot;&gt;Jiazhong Cen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1&quot;&gt;Jiemin Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1&quot;&gt;Lingxi Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaopeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1&quot;&gt;Wei Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1&quot;&gt;Qi Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00863">
<title>EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything. (arXiv:2312.00863v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00863</link>
<description rdf:parseType="Literal">&lt;p&gt;Segment Anything Model (SAM) has emerged as a powerful tool for numerous
vision applications. A key component that drives the impressive performance for
zero-shot transfer and high versatility is a super large Transformer model
trained on the extensive high-quality SA-1B dataset. While beneficial, the huge
computation cost of SAM model has limited its applications to wider real-world
applications. To address this limitation, we propose EfficientSAMs,
light-weight SAM models that exhibits decent performance with largely reduced
complexity. Our idea is based on leveraging masked image pretraining, SAMI,
which learns to reconstruct features from SAM image encoder for effective
visual representation learning. Further, we take SAMI-pretrained light-weight
image encoders and mask decoder to build EfficientSAMs, and finetune the models
on SA-1B for segment anything task. We perform evaluations on multiple vision
tasks including image classification, object detection, instance segmentation,
and semantic object detection, and find that our proposed pretraining method,
SAMI, consistently outperforms other masked image pretraining methods. On
segment anything task such as zero-shot instance segmentation, our
EfficientSAMs with SAMI-pretrained lightweight image encoders perform favorably
with a significant gain (e.g., ~4 AP on COCO/LVIS) over other fast SAM models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1&quot;&gt;Yunyang Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varadarajan_B/0/1/0/all/0/1&quot;&gt;Bala Varadarajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Lemeng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_F/0/1/0/all/0/1&quot;&gt;Fanyi Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Chenchen Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1&quot;&gt;Xiaoliang Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dilin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1&quot;&gt;Fei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iandola_F/0/1/0/all/0/1&quot;&gt;Forrest Iandola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnamoorthi_R/0/1/0/all/0/1&quot;&gt;Raghuraman Krishnamoorthi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1&quot;&gt;Vikas Chandra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00869">
<title>Segment and Caption Anything. (arXiv:2312.00869v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00869</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a method to efficiently equip the Segment Anything Model (SAM)
with the ability to generate regional captions. SAM presents strong
generalizability to segment anything while is short for semantic understanding.
By introducing a lightweight query-based feature mixer, we align the
region-specific features with the embedding space of language models for later
caption generation. As the number of trainable parameters is small (typically
in the order of tens of millions), it costs less computation, less memory
usage, and less communication bandwidth, resulting in both fast and scalable
training. To address the scarcity problem of regional caption data, we propose
to first pre-train our model on objection detection and segmentation tasks. We
call this step weak supervision pretraining since the pre-training data only
contains category names instead of full-sentence descriptions. The weak
supervision pretraining allows us to leverage many publicly available object
detection and segmentation datasets. We conduct extensive experiments to
demonstrate the superiority of our method and validate each design choice. This
work serves as a stepping stone towards scaling up regional captioning data and
sheds light on exploring efficient ways to augment SAM with regional semantics.
The project page, along with the associated code, can be accessed via the
following https://xk-huang.github.io/segment-caption-anything/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaoke Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianfeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yansong Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Han Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiwen Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lijuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zicheng Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00870">
<title>3DiFACE: Diffusion-based Speech-driven 3D Facial Animation and Editing. (arXiv:2312.00870v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00870</link>
<description rdf:parseType="Literal">&lt;p&gt;We present 3DiFACE, a novel method for personalized speech-driven 3D facial
animation and editing. While existing methods deterministically predict facial
animations from speech, they overlook the inherent one-to-many relationship
between speech and facial expressions, i.e., there are multiple reasonable
facial expression animations matching an audio input. It is especially
important in content creation to be able to modify generated motion or to
specify keyframes. To enable stochasticity as well as motion editing, we
propose a lightweight audio-conditioned diffusion model for 3D facial motion.
This diffusion model can be trained on a small 3D motion dataset, maintaining
expressive lip motion output. In addition, it can be finetuned for specific
subjects, requiring only a short video of the person. Through quantitative and
qualitative evaluations, we show that our method outperforms existing
state-of-the-art techniques and yields speech-driven animations with greater
fidelity and diversity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thambiraja_B/0/1/0/all/0/1&quot;&gt;Balamurugan Thambiraja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aliakbarian_S/0/1/0/all/0/1&quot;&gt;Sadegh Aliakbarian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cosker_D/0/1/0/all/0/1&quot;&gt;Darren Cosker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thies_J/0/1/0/all/0/1&quot;&gt;Justus Thies&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00878">
<title>Grounding Everything: Emerging Localization Properties in Vision-Language Transformers. (arXiv:2312.00878v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00878</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision-language foundation models have shown remarkable performance in
various zero-shot settings such as image retrieval, classification, or
captioning. But so far, those models seem to fall behind when it comes to
zero-shot localization of referential expressions and objects in images. As a
result, they need to be fine-tuned for this task. In this paper, we show that
pretrained vision-language (VL) models allow for zero-shot open-vocabulary
object localization without any fine-tuning. To leverage those capabilities, we
propose a Grounding Everything Module (GEM) that generalizes the idea of
value-value attention introduced by CLIPSurgery to a self-self attention path.
We show that the concept of self-self attention corresponds to clustering, thus
enforcing groups of tokens arising from the same object to be similar while
preserving the alignment with the language space. To further guide the group
formation, we propose a set of regularizations that allows the model to finally
generalize across datasets and backbones. We evaluate the proposed GEM
framework on various benchmark tasks and datasets for semantic segmentation. It
shows that GEM not only outperforms other training-free open-vocabulary
localization methods, but also achieves state-of-the-art results on the
recently proposed OpenImagesV7 large-scale segmentation benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bousselham_W/0/1/0/all/0/1&quot;&gt;Walid Bousselham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petersen_F/0/1/0/all/0/1&quot;&gt;Felix Petersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1&quot;&gt;Vittorio Ferrari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1&quot;&gt;Hilde Kuehne&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00923">
<title>Label Delay in Continual Learning. (arXiv:2312.00923v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.00923</link>
<description rdf:parseType="Literal">&lt;p&gt;Online continual learning, the process of training models on streaming data,
has gained increasing attention in recent years. However, a critical aspect
often overlooked is the label delay, where new data may not be labeled due to
slow and costly annotation processes. We introduce a new continual learning
framework with explicit modeling of the label delay between data and label
streams over time steps. In each step, the framework reveals both unlabeled
data from the current time step $t$ and labels delayed with $d$ steps, from the
time step $t-d$. In our extensive experiments amounting to 1060 GPU days, we
show that merely augmenting the computational resources is insufficient to
tackle this challenge. Our findings underline a notable performance decline
when solely relying on labeled data when the label delay becomes significant.
More surprisingly, when using state-of-the-art SSL and TTA techniques to
utilize the newer, unlabeled data, they fail to surpass the performance of a
na\&quot;ive method that simply trains on the delayed supervised stream. To this
end, we introduce a simple, efficient baseline that rehearses from the labeled
memory samples that are most similar to the new unlabeled samples. This method
bridges the accuracy gap caused by label delay without significantly increasing
computational complexity. We show experimentally that our method is the least
affected by the label delay factor and in some cases successfully recovers the
accuracy of the non-delayed counterpart. We conduct various ablations and
sensitivity experiments, demonstrating the effectiveness of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Csaba_B/0/1/0/all/0/1&quot;&gt;Botos Csaba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenxuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1&quot;&gt;Matthias M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1&quot;&gt;Ser-Nam Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1&quot;&gt;Mohamed Elhoseiny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1&quot;&gt;Philip Torr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bibi_A/0/1/0/all/0/1&quot;&gt;Adel Bibi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00937">
<title>Zero-Shot Video Question Answering with Procedural Programs. (arXiv:2312.00937v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00937</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose to answer zero-shot questions about videos by generating short
procedural programs that derive a final answer from solving a sequence of
visual subtasks. We present Procedural Video Querying (ProViQ), which uses a
large language model to generate such programs from an input question and an
API of visual modules in the prompt, then executes them to obtain the output.
Recent similar procedural approaches have proven successful for image question
answering, but videos remain challenging: we provide ProViQ with modules
intended for video understanding, allowing it to generalize to a wide variety
of videos. This code generation framework additionally enables ProViQ to
perform other video tasks in addition to question answering, such as
multi-object tracking or basic video editing. ProViQ achieves state-of-the-art
results on a diverse range of benchmarks, with improvements of up to 25% on
short, long, open-ended, and multimodal video question-answering datasets. Our
project page is at https://rccchoudhury.github.io/proviq2023.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choudhury_R/0/1/0/all/0/1&quot;&gt;Rohan Choudhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niinuma_K/0/1/0/all/0/1&quot;&gt;Koichiro Niinuma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1&quot;&gt;Kris M. Kitani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeni_L/0/1/0/all/0/1&quot;&gt;L&amp;#xe1;szl&amp;#xf3; A. Jeni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00944">
<title>Enhancing Diffusion Models with 3D Perspective Geometry Constraints. (arXiv:2312.00944v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00944</link>
<description rdf:parseType="Literal">&lt;p&gt;While perspective is a well-studied topic in art, it is generally taken for
granted in images. However, for the recent wave of high-quality image synthesis
methods such as latent diffusion models, perspective accuracy is not an
explicit requirement. Since these methods are capable of outputting a wide
gamut of possible images, it is difficult for these synthesized images to
adhere to the principles of linear perspective. We introduce a novel geometric
constraint in the training process of generative models to enforce perspective
accuracy. We show that outputs of models trained with this constraint both
appear more realistic and improve performance of downstream models trained on
generated images. Subjective human trials show that images generated with
latent diffusion models trained with our constraint are preferred over images
from the Stable Diffusion V2 model 70% of the time. SOTA monocular depth
estimation models such as DPT and PixelFormer, fine-tuned on our images,
outperform the original models trained on real images by up to 7.03% in RMSE
and 19.3% in SqRel on the KITTI test set for zero-shot transfer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Upadhyay_R/0/1/0/all/0/1&quot;&gt;Rishi Upadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Howard Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ba_Y/0/1/0/all/0/1&quot;&gt;Yunhao Ba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1&quot;&gt;Ethan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gella_B/0/1/0/all/0/1&quot;&gt;Blake Gella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1&quot;&gt;Sicheng Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1&quot;&gt;Alex Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kadambi_A/0/1/0/all/0/1&quot;&gt;Achuta Kadambi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00947">
<title>Object 6D pose estimation meets zero-shot learning. (arXiv:2312.00947v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00947</link>
<description rdf:parseType="Literal">&lt;p&gt;Object 6D pose estimation methods can achieve high accuracy when trained and
tested on the same objects. However, estimating the pose of objects that are
absent at training time is still a challenge. In this work, we advance the
state-of-the-art in zero-shot object 6D pose estimation by proposing the first
method that fuses the contribution of pre-trained geometric and vision
foundation models. Unlike state-of-the-art approaches that train their pipeline
on data specifically crafted for the 6D pose estimation task, our method does
not require task-specific finetuning. Instead, our method, which we name PoMZ,
combines geometric descriptors learned from point cloud data with visual
features learned from large-scale web images to produce distinctive 3D
point-level descriptors. By applying an off-the-shelf registration algorithm,
like RANSAC, PoMZ outperforms all state-of-the-art zero-shot object 6D pose
estimation approaches. We extensively evaluate PoMZ across the seven core
datasets of the BOP Benchmark, encompassing over a hundred objects and 20
thousand images captured in diverse scenarios. PoMZ ranks first in the BOP
Benchmark under the category Task 4: 6D localization of unseen objects. We will
release the source code publicly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caraffa_A/0/1/0/all/0/1&quot;&gt;Andrea Caraffa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boscaini_D/0/1/0/all/0/1&quot;&gt;Davide Boscaini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamza_A/0/1/0/all/0/1&quot;&gt;Amir Hamza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poiesi_F/0/1/0/all/0/1&quot;&gt;Fabio Poiesi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00950">
<title>Improve Supervised Representation Learning with Masked Image Modeling. (arXiv:2312.00950v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00950</link>
<description rdf:parseType="Literal">&lt;p&gt;Training visual embeddings with labeled data supervision has been the de
facto setup for representation learning in computer vision. Inspired by recent
success of adopting masked image modeling (MIM) in self-supervised
representation learning, we propose a simple yet effective setup that can
easily integrate MIM into existing supervised training paradigms. In our
design, in addition to the original classification task applied to a vision
transformer image encoder, we add a shallow transformer-based decoder on top of
the encoder and introduce an MIM task which tries to reconstruct image tokens
based on masked image inputs. We show with minimal change in architecture and
no overhead in inference that this setup is able to improve the quality of the
learned representations for downstream tasks such as classification, image
retrieval, and semantic segmentation. We conduct a comprehensive study and
evaluation of our setup on public benchmarks. On ImageNet-1k, our ViT-B/14
model achieves 81.72% validation accuracy, 2.01% higher than the baseline
model. On K-Nearest-Neighbor image retrieval evaluation with ImageNet-1k, the
same model outperforms the baseline by 1.32%. We also show that this setup can
be easily scaled to larger models and datasets. Code and checkpoints will be
released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kaifeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salz_D/0/1/0/all/0/1&quot;&gt;Daniel Salz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1&quot;&gt;Huiwen Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1&quot;&gt;Kihyuk Sohn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnan_D/0/1/0/all/0/1&quot;&gt;Dilip Krishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seyedhosseini_M/0/1/0/all/0/1&quot;&gt;Mojtaba Seyedhosseini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00968">
<title>Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts. (arXiv:2312.00968v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00968</link>
<description rdf:parseType="Literal">&lt;p&gt;Large multi-modal models (LMMs) exhibit remarkable performance across
numerous tasks. However, generalist LMMs often suffer from performance
degradation when tuned over a large collection of tasks. Recent research
suggests that Mixture of Experts (MoE) architectures are useful for instruction
tuning, but for LMMs of parameter size around O(50-100B), the prohibitive cost
of replicating and storing the expert models severely limits the number of
experts we can use. We propose Omni-SMoLA, an architecture that uses the Soft
MoE approach to (softly) mix many multimodal low rank experts, and avoids
introducing a significant number of new parameters compared to conventional MoE
models. The core intuition here is that the large model provides a foundational
backbone, while different lightweight experts residually learn specialized
knowledge, either per-modality or multimodally. Extensive experiments
demonstrate that the SMoLA approach helps improve the generalist performance
across a broad range of generative vision-and-language tasks, achieving new
SoTA generalist performance that often matches or outperforms single
specialized LMM baselines, as well as new SoTA specialist performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jialin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xia Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaqing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1&quot;&gt;Bo Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1&quot;&gt;Radu Soricut&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00971">
<title>Consistent Mesh Diffusion. (arXiv:2312.00971v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00971</link>
<description rdf:parseType="Literal">&lt;p&gt;Given a 3D mesh with a UV parameterization, we introduce a novel approach to
generating textures from text prompts. While prior work uses optimization from
Text-to-Image Diffusion models to generate textures and geometry, this is slow
and requires significant compute resources. Alternatively, there are projection
based approaches that use the same Text-to-Image models that paint images onto
a mesh, but lack consistency at different viewing angles, we propose a method
that uses a single Depth-to-Image diffusion network, and generates a single
consistent texture when rendered on the 3D surface by first unifying multiple
2D image&apos;s diffusion paths, and hoisting that to 3D with
MultiDiffusion~\cite{multidiffusion}. We demonstrate our approach on a dataset
containing 30 meshes, taking approximately 5 minutes per mesh. To evaluate the
quality of our approach, we use CLIP-score~\cite{clipscore} and Frechet
Inception Distance (FID)~\cite{frechet} to evaluate the quality of the
rendering, and show our improvement over prior work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knodt_J/0/1/0/all/0/1&quot;&gt;Julian Knodt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xifeng Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00987">
<title>Deep Generative Attacks and Countermeasures for Data-Driven Offline Signature Verification. (arXiv:2312.00987v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00987</link>
<description rdf:parseType="Literal">&lt;p&gt;While previous studies have explored attacks via random, simple, and skilled
forgeries, generative attacks have received limited attention in the
data-driven signature verification (DASV) process. Thus, this paper explores
the impact of generative attacks on DASV and proposes practical and
interpretable countermeasures. We investigate the power of two prominent Deep
Generative Models (DGMs), Variational Auto-encoders (VAE) and Conditional
Generative Adversarial Networks (CGAN), on their ability to generate signatures
that would successfully deceive DASV. Additionally, we evaluate the quality of
generated images using the Structural Similarity Index measure (SSIM) and use
the same to explain the attack&apos;s success. Finally, we propose countermeasures
that effectively reduce the impact of deep generative attacks on DASV.
&lt;/p&gt;
&lt;p&gt;We first generated six synthetic datasets from three benchmark
offline-signature datasets viz. CEDAR, BHSig260- Bengali, and BHSig260-Hindi
using VAE and CGAN. Then, we built baseline DASVs using Xception, ResNet152V2,
and DenseNet201. These DASVs achieved average (over the three datasets) False
Accept Rates (FARs) of 2.55%, 3.17%, and 1.06%, respectively. Then, we attacked
these baselines using the synthetic datasets. The VAE-generated signatures
increased average FARs to 10.4%, 10.1%, and 7.5%, while CGAN-generated
signatures to 32.5%, 30%, and 26.1%. The variation in the effectiveness of
attack for VAE and CGAN was investigated further and explained by a strong (rho
= -0.86) negative correlation between FARs and SSIMs. We created another set of
synthetic datasets and used the same to retrain the DASVs. The retained
baseline showed significant robustness to random, skilled, and generative
attacks as the FARs shrank to less than 1% on average. The findings underscore
the importance of studying generative attacks and potential countermeasures for
DASV.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ngo_A/0/1/0/all/0/1&quot;&gt;An Ngo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1&quot;&gt;MinhPhuong Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1&quot;&gt;Rajesh Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01001">
<title>Learning county from pixels: Corn yield prediction with attention-weighted multiple instance learning. (arXiv:2312.01001v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.01001</link>
<description rdf:parseType="Literal">&lt;p&gt;Remote sensing technology has become a promising tool in yield prediction.
Most prior work employs satellite imagery for county-level corn yield
prediction by spatially aggregating all pixels within a county into a single
value, potentially overlooking the detailed information and valuable insights
offered by more granular data. To this end, this research examines each county
at the pixel level and applies multiple instance learning to leverage detailed
information within a county. In addition, our method addresses the &quot;mixed
pixel&quot; issue caused by the inconsistent resolution between feature datasets and
crop mask, which may introduce noise into the model and therefore hinder
accurate yield prediction. Specifically, the attention mechanism is employed to
automatically assign weights to different pixels, which can mitigate the
influence of mixed pixels. The experimental results show that the developed
model outperforms four other machine learning models over the past five years
in the U.S. corn belt and demonstrates its best performance in 2022, achieving
a coefficient of determination (R2) value of 0.84 and a root mean square error
(RMSE) of 0.83. This paper demonstrates the advantages of our approach from
both spatial and temporal perspectives. Furthermore, through an in-depth study
of the relationship between mixed pixels and attention, it is verified that our
approach can capture critical feature information while filtering out noise
from mixed pixels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yuchi Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qunying Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhengwei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhou Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01003">
<title>Self-Evolving Neural Radiance Fields. (arXiv:2312.01003v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.01003</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, neural radiance field (NeRF) has shown remarkable performance in
novel view synthesis and 3D reconstruction. However, it still requires abundant
high-quality images, limiting its applicability in real-world scenarios. To
overcome this limitation, recent works have focused on training NeRF only with
sparse viewpoints by giving additional regularizations, often called few-shot
NeRF. We observe that due to the under-constrained nature of the task, solely
using additional regularization is not enough to prevent the model from
overfitting to sparse viewpoints. In this paper, we propose a novel framework,
dubbed Self-Evolving Neural Radiance Fields (SE-NeRF), that applies a
self-training framework to NeRF to address these problems. We formulate
few-shot NeRF into a teacher-student framework to guide the network to learn a
more robust representation of the scene by training the student with additional
pseudo labels generated from the teacher. By distilling ray-level pseudo labels
using distinct distillation schemes for reliable and unreliable rays obtained
with our novel reliability estimation method, we enable NeRF to learn a more
accurate and robust geometry of the 3D scene. We show and evaluate that
applying our self-training framework to existing models improves the quality of
the rendered images and achieves state-of-the-art performance in multiple
settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1&quot;&gt;Jaewoo Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jisang Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1&quot;&gt;Jiwon Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seongchan Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwak_M/0/1/0/all/0/1&quot;&gt;Min-Seop Kwak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seungryong Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01017">
<title>Unveiling the Power of Audio-Visual Early Fusion Transformers with Dense Interactions through Masked Modeling. (arXiv:2312.01017v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.01017</link>
<description rdf:parseType="Literal">&lt;p&gt;Humans possess a remarkable ability to integrate auditory and visual
information, enabling a deeper understanding of the surrounding environment.
This early fusion of audio and visual cues, demonstrated through cognitive
psychology and neuroscience research, offers promising potential for developing
multimodal perception models. However, training early fusion architectures
poses significant challenges, as the increased model expressivity requires
robust learning frameworks to harness their enhanced capabilities. In this
paper, we address this challenge by leveraging the masked reconstruction
framework, previously successful in unimodal settings, to train audio-visual
encoders with early fusion. Additionally, we propose an attention-based fusion
module that captures interactions between local audio and visual
representations, enhancing the model&apos;s ability to capture fine-grained
interactions. While effective, this procedure can become computationally
intractable, as the number of local representations increases. Thus, to address
the computational complexity, we propose an alternative procedure that
factorizes the local representations before representing audio-visual
interactions. Extensive evaluations on a variety of datasets demonstrate the
superiority of our approach in audio-event classification, visual sound
localization, sound separation, and audio-visual segmentation. These
contributions enable the efficient training of deeply integrated audio-visual
models and significantly advance the usefulness of early fusion architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1&quot;&gt;Shentong Mo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morgado_P/0/1/0/all/0/1&quot;&gt;Pedro Morgado&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01026">
<title>Token Fusion: Bridging the Gap between Token Pruning and Token Merging. (arXiv:2312.01026v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.01026</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformers (ViTs) have emerged as powerful backbones in computer
vision, outperforming many traditional CNNs. However, their computational
overhead, largely attributed to the self-attention mechanism, makes deployment
on resource-constrained edge devices challenging. Multiple solutions rely on
token pruning or token merging. In this paper, we introduce &quot;Token Fusion&quot;
(ToFu), a method that amalgamates the benefits of both token pruning and token
merging. Token pruning proves advantageous when the model exhibits sensitivity
to input interpolations, while token merging is effective when the model
manifests close to linear responses to inputs. We combine this to propose a new
scheme called Token Fusion. Moreover, we tackle the limitations of average
merging, which doesn&apos;t preserve the intrinsic feature norm, resulting in
distributional shifts. To mitigate this, we introduce MLERP merging, a variant
of the SLERP technique, tailored to merge multiple tokens while maintaining the
norm distribution. ToFu is versatile, applicable to ViTs with or without
additional training. Our empirical evaluations indicate that ToFu establishes
new benchmarks in both classification and image generation tasks concerning
computational efficiency and model accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Minchul Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Shangqian Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1&quot;&gt;Yen-Chang Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yilin Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1&quot;&gt;Hongxia Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01027">
<title>Taming Latent Diffusion Models to See in the Dark. (arXiv:2312.01027v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.01027</link>
<description rdf:parseType="Literal">&lt;p&gt;Enhancing a low-light noisy RAW image into a well-exposed and clean sRGB
image is a significant challenge in computational photography. Due to the
limitation of large-scale paired data, prior approaches have difficulty in
recovering fine details and true colors in extremely low-light regions.
Meanwhile, recent advancements in generative diffusion models have shown
promising generating capabilities, which inspires this work to explore
generative priors from a diffusion model trained on a large-scale open-domain
dataset to benefit the low-light image enhancement (LLIE) task. Based on this
intention, we propose a novel diffusion-model-based LLIE method, dubbed
LDM-SID. LDM-SID aims at inserting a set of proposed taming modules into a
frozen pre-trained diffusion model to steer its generating process.
Specifically, the taming module fed with low-light information serves to output
a pair of affine transformation parameters to modulate the intermediate feature
in the diffusion model. Additionally, based on the observation of dedicated
generative priors across different portions of the diffusion model, we propose
to apply 2D discrete wavelet transforms on the input RAW image, resulting in
dividing the LLIE task into two essential parts: low-frequency content
generation and high-frequency detail maintenance. This enables us to skillfully
tame the diffusion model for optimized structural generation and detail
enhancement. Extensive experiments demonstrate the proposed method not only
achieves state-of-the-art performance in quantitative evaluations but also
shows significant superiority in visual comparisons. These findings highlight
the effectiveness of leveraging a pre-trained diffusion model as a generative
prior to the LLIE task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Q/0/1/0/all/0/1&quot;&gt;Qiang Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1&quot;&gt;Yazhou Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qifeng Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2003.03923">
<title>Deconfounded Image Captioning: A Causal Retrospect. (arXiv:2003.03923v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2003.03923</link>
<description rdf:parseType="Literal">&lt;p&gt;Dataset bias in vision-language tasks is becoming one of the main problems
which hinders the progress of our community. Existing solutions lack a
principled analysis about why modern image captioners easily collapse into
dataset bias. In this paper, we present a novel perspective: Deconfounded Image
Captioning (DIC), to find out the answer of this question, then retrospect
modern neural image captioners, and finally propose a DIC framework: DICv1.0 to
alleviate the negative effects brought by dataset bias. DIC is based on causal
inference, whose two principles: the backdoor and front-door adjustments, help
us review previous studies and design new effective models. In particular, we
showcase that DICv1.0 can strengthen two prevailing captioning models and can
achieve a single-model 131.1 CIDEr-D and 128.4 c40 CIDEr-D on Karpathy split
and online split of the challenging MS COCO dataset, respectively.
Interestingly, DICv1.0 is a natural derivation from our causal retrospect,
which opens promising directions for image captioning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hanwang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1&quot;&gt;Jianfei Cai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2004.11968">
<title>Visualizing key features in X-ray images of epoxy resins for improved material classification using singular value decomposition of deep learning features. (arXiv:2004.11968v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2004.11968</link>
<description rdf:parseType="Literal">&lt;p&gt;Although the process variables of epoxy resins alter their mechanical
properties, the visual identification of the characteristic features of X-ray
images of samples of these materials is challenging. To facilitate the
identification, we approximate the magnitude of the gradient of the intensity
field of the X-ray images of different kinds of epoxy resins and then we use
deep learning to discover the most representative features of the transformed
images. In this solution of the inverse problem to finding characteristic
features to discriminate samples of heterogeneous materials, we use the
eigenvectors obtained from the singular value decomposition of all the channels
of the feature maps of the early layers in a convolutional neural network.
While the strongest activated channel gives a visual representation of the
characteristic features, often these are not robust enough in some practical
settings. On the other hand, the left singular vectors of the matrix
decomposition of the feature maps, barely change when variables such as the
capacity of the network or network architecture change. High classification
accuracy and robustness of characteristic features are presented in this work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Avalos_E/0/1/0/all/0/1&quot;&gt;Edgar Avalos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akagi_K/0/1/0/all/0/1&quot;&gt;Kazuto Akagi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nishiura_Y/0/1/0/all/0/1&quot;&gt;Yasumasa Nishiura&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2104.13742">
<title>MineGAN++: Mining Generative Models for Efficient Knowledge Transfer to Limited Data Domains. (arXiv:2104.13742v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2104.13742</link>
<description rdf:parseType="Literal">&lt;p&gt;GANs largely increases the potential impact of generative models. Therefore,
we propose a novel knowledge transfer method for generative models based on
mining the knowledge that is most beneficial to a specific target domain,
either from a single or multiple pretrained GANs. This is done using a miner
network that identifies which part of the generative distribution of each
pretrained GAN outputs samples closest to the target domain. Mining effectively
steers GAN sampling towards suitable regions of the latent space, which
facilitates the posterior finetuning and avoids pathologies of other methods,
such as mode collapse and lack of flexibility. Furthermore, to prevent
overfitting on small target domains, we introduce sparse subnetwork selection,
that restricts the set of trainable neurons to those that are relevant for the
target dataset. We perform comprehensive experiments on several challenging
datasets using various GAN architectures (BigGAN, Progressive GAN, and
StyleGAN) and show that the proposed method, called MineGAN, effectively
transfers knowledge to domains with few target images, outperforming existing
methods. In addition, MineGAN can successfully transfer knowledge from multiple
pretrained GANs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaxing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_Garcia_A/0/1/0/all/0/1&quot;&gt;Abel Gonzalez-Garcia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chenshen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herranz_L/0/1/0/all/0/1&quot;&gt;Luis Herranz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1&quot;&gt;Fahad Shahbaz Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jui_S/0/1/0/all/0/1&quot;&gt;Shangling Jui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1&quot;&gt;Joost van de Weijer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2107.07468">
<title>A modular U-Net for automated segmentation of X-ray tomography images in composite materials. (arXiv:2107.07468v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2107.07468</link>
<description rdf:parseType="Literal">&lt;p&gt;X-ray Computed Tomography (XCT) techniques have evolved to a point that
high-resolution data can be acquired so fast that classic segmentation methods
are prohibitively cumbersome, demanding automated data pipelines capable of
dealing with non-trivial 3D images. Deep learning has demonstrated success in
many image processing tasks, including material science applications, showing a
promising alternative for a humanfree segmentation pipeline. In this paper a
modular interpretation of UNet (Modular U-Net) is proposed and trained to
segment 3D tomography images of a three-phased glass fiber-reinforced Polyamide
66. We compare 2D and 3D versions of our model, finding that the former is
slightly better than the latter. We observe that human-comparable results can
be achievied even with only 10 annotated layers and using a shallow U-Net
yields better results than a deeper one. As a consequence, Neural Network (NN)
show indeed a promising venue to automate XCT data processing pipelines needing
no human, adhoc intervention.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bertoldo_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o P C Bertoldo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Decenciere_E/0/1/0/all/0/1&quot;&gt;Etienne Decenci&amp;#xe8;re&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ryckelynck_D/0/1/0/all/0/1&quot;&gt;David Ryckelynck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Proudhon_H/0/1/0/all/0/1&quot;&gt;Henry Proudhon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.01592">
<title>Biphasic Face Photo-Sketch Synthesis via Semantic-Driven Generative Adversarial Network with Graph Representation Learning. (arXiv:2201.01592v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2201.01592</link>
<description rdf:parseType="Literal">&lt;p&gt;Biphasic face photo-sketch synthesis has significant practical value in
wide-ranging fields such as digital entertainment and law enforcement. Previous
approaches directly generate the photo-sketch in a global view, they always
suffer from the low quality of sketches and complex photo variations, leading
to unnatural and low-fidelity results. In this paper, we propose a novel
Semantic-Driven Generative Adversarial Network to address the above issues,
cooperating with Graph Representation Learning. Considering that human faces
have distinct spatial structures, we first inject class-wise semantic layouts
into the generator to provide style-based spatial information for synthesized
face photos and sketches. Additionally, to enhance the authenticity of details
in generated faces, we construct two types of representational graphs via
semantic parsing maps upon input faces, dubbed the IntrA-class Semantic Graph
(IASG) and the InteR-class Structure Graph (IRSG). Specifically, the IASG
effectively models the intra-class semantic correlations of each facial
semantic component, thus producing realistic facial details. To preserve the
generated faces being more structure-coordinated, the IRSG models inter-class
structural relations among every facial component by graph representation
learning. To further enhance the perceptual quality of synthesized images, we
present a biphasic interactive cycle training strategy by fully taking
advantage of the multi-level feature consistency between the photo and sketch.
Extensive experiments demonstrate that our method outperforms the
state-of-the-art competitors on the CUFS and CUFSF datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1&quot;&gt;Xingqun Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;Muyi Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zijian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiaming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1&quot;&gt;Fang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shanghang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_C/0/1/0/all/0/1&quot;&gt;Caifeng Shan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.02149">
<title>3D Point Cloud Registration with Learning-based Matching Algorithm. (arXiv:2202.02149v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2202.02149</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel differential matching algorithm for 3D point cloud
registration. Instead of only optimizing the feature extractor for a matching
algorithm, we propose a learning-based matching module optimized to the
jointly-trained feature extractor. We focused on edge-wise feature-forwarding
architectures, which are memory-consuming but can avoid the over-smoothing
effect that GNNs suffer. We improve its memory efficiency to scale it for point
cloud registration while investigating the best way of connecting it to the
feature extractor. Experimental results show our matching module&apos;s significant
impact on performance improvement in rigid/non-rigid and whole/partial point
cloud registration datasets with multiple contemporary feature extractors. For
example, our module boosted the current SOTA method, RoITr, by +5.4%, and +7.2%
in the NFMR metric and +6.1% and +8.5% in the IR metric on the 4DMatch and
4DLoMatch datasets, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yanagi_R/0/1/0/all/0/1&quot;&gt;Rintaro Yanagi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hashimoto_A/0/1/0/all/0/1&quot;&gt;Atsushi Hashimoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sone_S/0/1/0/all/0/1&quot;&gt;Shusaku Sone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiba_N/0/1/0/all/0/1&quot;&gt;Naoya Chiba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jiaxin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ushiku_Y/0/1/0/all/0/1&quot;&gt;Yoshitaka Ushiku&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.03408">
<title>A High-Resolution Chest CT-Scan Image Dataset for COVID-19 Diagnosis and Differentiation. (arXiv:2205.03408v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2205.03408</link>
<description rdf:parseType="Literal">&lt;p&gt;During the COVID-19 pandemic, computed tomography (CT) is a good way to
diagnose COVID-19 patients. HRCT (High-Resolution Computed Tomography) is a
form of computed tomography that uses advanced methods to improve image
resolution. Publicly accessible COVID-19 CT image datasets are very difficult
to come by due to privacy concerns, which impedes the study and development of
AI-powered COVID-19 diagnostic algorithms based on CT images. To address this
problem, we have introduced HRCTv1-COVID-19, a new COVID-19 high resolution
chest CT Scan image dataset that includes not only COVID-19 cases of Ground
Glass Opacity (GGO), Crazy Paving, and Air Space Consolidation, but also CT
images of cases with negative COVID-19. The HRCTv1-COVID-19 dataset, which
includes slice-level, and patient-level labels, has the potential to aid
COVID-19 research, especially for diagnosis and differentiation using
artificial intelligence algorithms, machine learning and deep learning methods.
This dataset is accessible through web at: &lt;a href=&quot;http://databiox.com&quot;&gt;this http URL&lt;/a&gt; and includes
181,106 chest HRCT images from 395 patients with four labels: GGO, Crazy
Paving, Air Space Consolidation and Negative.
&lt;/p&gt;
&lt;p&gt;Keywords- Dataset, COVID-19, CT-Scan, Computed Tomography, Medical Imaging,
Chest Image.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Abedi_I/0/1/0/all/0/1&quot;&gt;Iraj Abedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vali_M/0/1/0/all/0/1&quot;&gt;Mahsa Vali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shahreza_B/0/1/0/all/0/1&quot;&gt;Bentolhoda Otroshi Shahreza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bolhasani_H/0/1/0/all/0/1&quot;&gt;Hamidreza Bolhasani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.02598">
<title>[Reproducibility Report] Explainable Deep One-Class Classification. (arXiv:2206.02598v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.02598</link>
<description rdf:parseType="Literal">&lt;p&gt;Fully Convolutional Data Description (FCDD), an explainable version of the
Hypersphere Classifier (HSC), directly addresses image anomaly detection (AD)
and pixel-wise AD without any post-hoc explainer methods. The authors claim
that FCDD achieves results comparable with the state-of-the-art in sample-wise
AD on Fashion-MNIST and CIFAR-10 and exceeds the state-of-the-art on the
pixel-wise task on MVTec-AD. We reproduced the main results of the paper using
the author&apos;s code with minor changes and provide runtime requirements to
achieve if (CPU memory, GPU memory, and training time). We propose another
analysis methodology using a critical difference diagram, and further
investigate the test performance of the model during the training phase.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertoldo_J/0/1/0/all/0/1&quot;&gt;Joao P. C. Bertoldo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Decenciere_E/0/1/0/all/0/1&quot;&gt;Etienne Decenci&amp;#xe8;re&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.13381">
<title>Look Closer to Your Enemy: Learning to Attack via Teacher-Student Mimicking. (arXiv:2207.13381v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.13381</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks have significantly advanced person re-identification
(ReID) applications in the realm of the industrial internet, yet they remain
vulnerable. Thus, it is crucial to study the robustness of ReID systems, as
there are risks of adversaries using these vulnerabilities to compromise
industrial surveillance systems. Current adversarial methods focus on
generating attack samples using misclassification feedback from victim models
(VMs), neglecting VM&apos;s cognitive processes. We seek to address this by
producing authentic ReID attack instances through VM cognition decryption. This
approach boasts advantages like better transferability to open-set ReID tests,
easier VM misdirection, and enhanced creation of realistic and undetectable
assault images. However, the task of deciphering the cognitive mechanism in VM
is widely considered to be a formidable challenge. In this paper, we propose a
novel inconspicuous and controllable ReID attack baseline, LCYE (Look Closer to
Your Enemy), to generate adversarial query images. Specifically, LCYE first
distills VM&apos;s knowledge via teacher-student memory mimicking the proxy task.
This knowledge prior serves as an unambiguous cryptographic token,
encapsulating elements deemed indispensable and plausible by the VM, with the
intent of facilitating precise adversarial misdirection. Further, benefiting
from the multiple opposing task framework of LCYE, we investigate the
interpretability and generalization of ReID models from the view of the
adversarial attack, including cross-domain adaption, cross-model consensus, and
online learning process. Extensive experiments on four ReID benchmarks show
that our method outperforms other state-of-the-art attackers with a large
margin in white-box, black-box, and target attacks. The source code can be
found at https://github.com/MingjieWang0606/LCYE-attack_reid.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mingjie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jianxiong Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sirui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_D/0/1/0/all/0/1&quot;&gt;Dingwen Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1&quot;&gt;Zhiqing Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.13219">
<title>Visualizing high-dimensional loss landscapes with Hessian directions. (arXiv:2208.13219v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2208.13219</link>
<description rdf:parseType="Literal">&lt;p&gt;Analyzing geometric properties of high-dimensional loss functions, such as
local curvature and the existence of other optima around a certain point in
loss space, can help provide a better understanding of the interplay between
neural network structure, implementation attributes, and learning performance.
In this work, we combine concepts from high-dimensional probability and
differential geometry to study how curvature properties in lower-dimensional
loss representations depend on those in the original loss space. We show that
saddle points in the original space are rarely correctly identified as such in
expected lower-dimensional representations if random projections are used. The
principal curvature in the expected lower-dimensional representation is
proportional to the mean curvature in the original loss space. Hence, the mean
curvature in the original loss space determines if saddle points appear, on
average, as either minima, maxima, or almost flat regions. We use the
connection between expected curvature in random projections and mean curvature
in the original space (i.e., the normalized Hessian trace) to compute
Hutchinson-type trace estimates without calculating Hessian-vector products as
in the original Hutchinson method. Because random projections are not suitable
to correctly identify saddle information, we propose to study projections along
dominant Hessian directions that are associated with the largest and smallest
principal curvatures. We connect our findings to the ongoing debate on loss
landscape flatness and generalizability. Finally, for different common image
classifiers and a function approximator, we show and compare random and Hessian
projections of loss landscapes with up to about $7\times 10^6$ parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bottcher_L/0/1/0/all/0/1&quot;&gt;Lucas B&amp;#xf6;ttcher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wheeler_G/0/1/0/all/0/1&quot;&gt;Gregory Wheeler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.03320">
<title>What does a platypus look like? Generating customized prompts for zero-shot image classification. (arXiv:2209.03320v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.03320</link>
<description rdf:parseType="Literal">&lt;p&gt;Open-vocabulary models are a promising new paradigm for image classification.
Unlike traditional classification models, open-vocabulary models classify among
any arbitrary set of categories specified with natural language during
inference. This natural language, called &quot;prompts&quot;, typically consists of a set
of hand-written templates (e.g., &quot;a photo of a {}&quot;) which are completed with
each of the category names. This work introduces a simple method to generate
higher accuracy prompts, without relying on any explicit knowledge of the task
domain and with far fewer hand-constructed sentences. To achieve this, we
combine open-vocabulary models with large language models (LLMs) to create
Customized Prompts via Language models (CuPL, pronounced &quot;couple&quot;). In
particular, we leverage the knowledge contained in LLMs in order to generate
many descriptive sentences that contain important discriminating
characteristics of the image categories. This allows the model to place a
greater importance on these regions in the image when making predictions. We
find that this straightforward and general approach improves accuracy on a
range of zero-shot image classification benchmarks, including over one
percentage point gain on ImageNet. Finally, this simple baseline requires no
additional training and remains completely zero-shot. Code available at
https://github.com/sarahpratt/CuPL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pratt_S/0/1/0/all/0/1&quot;&gt;Sarah Pratt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Covert_I/0/1/0/all/0/1&quot;&gt;Ian Covert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Rosanne Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1&quot;&gt;Ali Farhadi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.15451">
<title>Semi-Supervised Domain Generalization for Cardiac Magnetic Resonance Image Segmentation with High Quality Pseudo Labels. (arXiv:2209.15451v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.15451</link>
<description rdf:parseType="Literal">&lt;p&gt;Developing a deep learning method for medical segmentation tasks heavily
relies on a large amount of labeled data. However, the annotations require
professional knowledge and are limited in number. Recently, semi-supervised
learning has demonstrated great potential in medical segmentation tasks. Most
existing methods related to cardiac magnetic resonance images only focus on
regular images with similar domains and high image quality. A semi-supervised
domain generalization method was developed in [2], which enhances the quality
of pseudo labels on varied datasets. In this paper, we follow the strategy in
[2] and present a domain generalization method for semi-supervised medical
segmentation. Our main goal is to improve the quality of pseudo labels under
extreme MRI Analysis with various domains. We perform Fourier transformation on
input images to learn low-level statistics and cross-domain information. Then
we feed the augmented images as input to the double cross pseudo supervision
networks to calculate the variance among pseudo labels. We evaluate our method
on the CMRxMotion dataset [1]. With only partially labeled data and without
domain labels, our approach consistently generates accurate segmentation
results of cardiac magnetic resonance images with different respiratory
motions. Code is available at: https://github.com/MAWanqin2002/STACOM2022Ma
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ma_W/0/1/0/all/0/1&quot;&gt;Wanqin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yao_H/0/1/0/all/0/1&quot;&gt;Huifeng Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yiqun Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jiarong Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaomeng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.00094">
<title>Improving Robustness with Adaptive Weight Decay. (arXiv:2210.00094v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.00094</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose adaptive weight decay, which automatically tunes the
hyper-parameter for weight decay during each training iteration. For
classification problems, we propose changing the value of the weight decay
hyper-parameter on the fly based on the strength of updates from the
classification loss (i.e., gradient of cross-entropy), and the regularization
loss (i.e., $\ell_2$-norm of the weights). We show that this simple
modification can result in large improvements in adversarial robustness -- an
area which suffers from robust overfitting -- without requiring extra data
across various datasets and architecture choices. For example, our
reformulation results in $20\%$ relative robustness improvement for CIFAR-100,
and $10\%$ relative robustness improvement on CIFAR-10 comparing to the best
tuned hyper-parameters of traditional weight decay resulting in models that
have comparable performance to SOTA robustness methods. In addition, this
method has other desirable properties, such as less sensitivity to learning
rate, and smaller weight norms, which the latter contributes to robustness to
overfitting to label noise, and pruning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghiasi_A/0/1/0/all/0/1&quot;&gt;Amin Ghiasi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shafahi_A/0/1/0/all/0/1&quot;&gt;Ali Shafahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ardekani_R/0/1/0/all/0/1&quot;&gt;Reza Ardekani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.07675">
<title>Learning image representations for anomaly detection: application to discovery of histological alterations in drug development. (arXiv:2210.07675v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.07675</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a system for anomaly detection in histopathological images. In
histology, normal samples are usually abundant, whereas anomalous
(pathological) cases are scarce or not available. Under such settings,
one-class classifiers trained on healthy data can detect out-of-distribution
anomalous samples. Such approaches combined with pre-trained Convolutional
Neural Network (CNN) representations of images were previously employed for
anomaly detection (AD). However, pre-trained off-the-shelf CNN representations
may not be sensitive to abnormal conditions in tissues, while natural
variations of healthy tissue may result in distant representations. To adapt
representations to relevant details in healthy tissue we propose training a CNN
on an auxiliary task that discriminates healthy tissue of different species,
organs, and staining reagents. Almost no additional labeling workload is
required, since healthy samples come automatically with aforementioned labels.
During training we enforce compact image representations with a center-loss
term, which further improves representations for AD. The proposed system
outperforms established AD methods on a published dataset of liver anomalies.
Moreover, it provided comparable results to conventional methods specifically
tailored for quantification of liver anomalies. We show that our approach can
be used for toxicity assessment of candidate drugs at early development stages
and thereby may reduce expensive late-stage drug attrition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zingman_I/0/1/0/all/0/1&quot;&gt;Igor Zingman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stierstorfer_B/0/1/0/all/0/1&quot;&gt;Birgit Stierstorfer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lempp_C/0/1/0/all/0/1&quot;&gt;Charlotte Lempp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heinemann_F/0/1/0/all/0/1&quot;&gt;Fabian Heinemann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.11388">
<title>Physics-informed Deep Diffusion MRI Reconstruction with Synthetic Data: Break Training Data Bottleneck in Artificial Intelligence. (arXiv:2210.11388v4 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.11388</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion magnetic resonance imaging (MRI) is the only imaging modality for
non-invasive movement detection of in vivo water molecules, with significant
clinical and research applications. Diffusion MRI (DWI) acquired by multi-shot
techniques can achieve higher resolution, better signal-to-noise ratio, and
lower geometric distortion than single-shot, but suffers from inter-shot
motion-induced artifacts. These artifacts cannot be removed prospectively,
leading to the absence of artifact-free training labels. Thus, the potential of
deep learning in multi-shot DWI reconstruction remains largely untapped. To
break the training data bottleneck, here, we propose a Physics-Informed Deep
DWI reconstruction method (PIDD) to synthesize high-quality paired training
data by leveraging the physical diffusion model (magnitude synthesis) and
inter-shot motion-induced phase model (motion phase synthesis). The network is
trained only once with 100,000 synthetic samples, achieving encouraging results
on multiple realistic in vivo data reconstructions. Advantages over
conventional methods include: (a) Better motion artifact suppression and
reconstruction stability; (b) Outstanding generalization to multi-scenario
reconstructions, including multi-resolution, multi-b-value,
multi-undersampling, multi-vendor, and multi-center; (c) Excellent clinical
adaptability to patients with verifications by seven experienced doctors
(p&amp;lt;0.001). In conclusion, PIDD presents a novel deep learning framework by
exploiting the power of MRI physics, providing a cost-effective and explainable
way to break the data bottleneck in deep learning medical imaging.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qian_C/0/1/0/all/0/1&quot;&gt;Chen Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yuncheng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Han_M/0/1/0/all/0/1&quot;&gt;Mingyang Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ruan_D/0/1/0/all/0/1&quot;&gt;Dan Ruan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yu Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yiping Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yirong Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chengyan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jiang_B/0/1/0/all/0/1&quot;&gt;Boyu Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tao_R/0/1/0/all/0/1&quot;&gt;Ran Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhigang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiazheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Liuhong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yi Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kang_T/0/1/0/all/0/1&quot;&gt;Taishan Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jianzhong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gong_T/0/1/0/all/0/1&quot;&gt;Tao Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fei_G/0/1/0/all/0/1&quot;&gt;Guoqiang Fei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lin_M/0/1/0/all/0/1&quot;&gt;Meijin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guo_D/0/1/0/all/0/1&quot;&gt;Di Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jianjun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Meiyun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qu_X/0/1/0/all/0/1&quot;&gt;Xiaobo Qu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.13452">
<title>MetaFormer Baselines for Vision. (arXiv:2210.13452v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.13452</link>
<description rdf:parseType="Literal">&lt;p&gt;MetaFormer, the abstracted architecture of Transformer, has been found to
play a significant role in achieving competitive performance. In this paper, we
further explore the capacity of MetaFormer, again, without focusing on token
mixer design: we introduce several baseline models under MetaFormer using the
most basic or common mixers, and summarize our observations as follows: (1)
MetaFormer ensures solid lower bound of performance. By merely adopting
identity mapping as the token mixer, the MetaFormer model, termed
IdentityFormer, achieves &amp;gt;80% accuracy on ImageNet-1K. (2) MetaFormer works
well with arbitrary token mixers. When specifying the token mixer as even a
random matrix to mix tokens, the resulting model RandFormer yields an accuracy
of &amp;gt;81%, outperforming IdentityFormer. Rest assured of MetaFormer&apos;s results
when new token mixers are adopted. (3) MetaFormer effortlessly offers
state-of-the-art results. With just conventional token mixers dated back five
years ago, the models instantiated from MetaFormer already beat state of the
art. (a) ConvFormer outperforms ConvNeXt. Taking the common depthwise separable
convolutions as the token mixer, the model termed ConvFormer, which can be
regarded as pure CNNs, outperforms the strong CNN model ConvNeXt. (b) CAFormer
sets new record on ImageNet-1K. By simply applying depthwise separable
convolutions as token mixer in the bottom stages and vanilla self-attention in
the top stages, the resulting model CAFormer sets a new record on ImageNet-1K:
it achieves an accuracy of 85.5% at 224x224 resolution, under normal supervised
training without external data or distillation. In our expedition to probe
MetaFormer, we also find that a new activation, StarReLU, reduces 71% FLOPs of
activation compared with GELU yet achieves better performance. We expect
StarReLU to find great potential in MetaFormer-like models alongside other
neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1&quot;&gt;Weihao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1&quot;&gt;Chenyang Si&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1&quot;&gt;Pan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1&quot;&gt;Mi Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yichen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jiashi Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1&quot;&gt;Shuicheng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinchao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.11530">
<title>Open-Set Object Detection Using Classification-free Object Proposal and Instance-level Contrastive Learning. (arXiv:2211.11530v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.11530</link>
<description rdf:parseType="Literal">&lt;p&gt;Detecting both known and unknown objects is a fundamental skill for robot
manipulation in unstructured environments. Open-set object detection (OSOD) is
a promising direction to handle the problem consisting of two subtasks: objects
and background separation, and open-set object classification. In this paper,
we present Openset RCNN to address the challenging OSOD. To disambiguate
unknown objects and background in the first subtask, we propose to use
classification-free region proposal network (CF-RPN) which estimates the
objectness score of each region purely using cues from object&apos;s location and
shape preventing overfitting to the training categories. To identify unknown
objects in the second subtask, we propose to represent them using the
complementary region of known categories in a latent space which is
accomplished by a prototype learning network (PLN). PLN performs instance-level
contrastive learning to encode proposals to a latent space and builds a compact
region centering with a prototype for each known category. Further, we note
that the detection performance of unknown objects can not be unbiasedly
evaluated on the situation that commonly used object detection datasets are not
fully annotated. Thus, a new benchmark is introduced by reorganizing
GraspNet-1billion, a robotic grasp pose detection dataset with complete
annotation. Extensive experiments demonstrate the merits of our method. We
finally show that our Openset RCNN can endow the robot with an open-set
perception ability to support robotic rearrangement tasks in cluttered
environments. More details can be found in
https://sites.google.com/view/openset-rcnn/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhongxiang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yifei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1&quot;&gt;Rong Xiong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.04517">
<title>A new sampling methodology for defining heterogeneous subsets of samples for training image segmentation algorithms. (arXiv:2301.04517v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.04517</link>
<description rdf:parseType="Literal">&lt;p&gt;Creating a dataset for training supervised machine learning algorithms can be
a demanding task. This is especially true for medical image segmentation since
one or more specialists are usually required for image annotation, and creating
ground truth labels for just a single image can take up to several hours. In
addition, it is paramount that the annotated samples represent well the
different conditions that might affect the imaged tissues as well as possible
changes in the image acquisition process. This can only be achieved by
considering samples that are typical in the dataset as well as atypical, or
even outlier, samples. We introduce a new sampling methodology for selecting
relevant images from a large dataset in a way that evenly considers both
prototypical as well as atypical samples. The methodology involves the
generation of a uniform grid from a feature space representing the samples,
which is then used for randomly drawing relevant images. The selected images
provide a uniform covering of the original dataset, and thus define a
heterogeneous set of images that can be annotated and used for training
supervised segmentation algorithms. We provide a case example by creating a
dataset containing a representative set of blood vessel microscopy images
selected from a larger dataset containing thousands of images. The dataset,
which we call VessMAP, is being made available online to aid the development of
new blood vessel segmentation algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_M/0/1/0/all/0/1&quot;&gt;Matheus Viana da Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_N/0/1/0/all/0/1&quot;&gt;Nat&amp;#xe1;lia de Carvalho Santos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouellette_J/0/1/0/all/0/1&quot;&gt;Julie Ouellette&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lacoste_B/0/1/0/all/0/1&quot;&gt;Baptiste Lacoste&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Comin_C/0/1/0/all/0/1&quot;&gt;Cesar Henrique Comin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.07475">
<title>Curvilinear object segmentation in medical images based on ODoS filter and deep learning network. (arXiv:2301.07475v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.07475</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic segmentation of curvilinear objects in medical images plays an
important role in the diagnosis and evaluation of human diseases, yet it is a
challenging uncertainty in the complex segmentation tasks due to different
issues such as various image appearances, low contrast between curvilinear
objects and their surrounding backgrounds, thin and uneven curvilinear
structures, and improper background illumination conditions. To overcome these
challenges, we present a unique curvilinear structure segmentation framework
based on an oriented derivative of stick (ODoS) filter and a deep learning
network for curvilinear object segmentation in medical images. Currently, a
large number of deep learning models emphasize developing deep architectures
and ignore capturing the structural features of curvilinear objects, which may
lead to unsatisfactory results. Consequently, a new approach that incorporates
an ODoS filter as part of a deep learning network is presented to improve the
spatial attention of curvilinear objects. Specifically, the input image is
transfered into four-channel image constructed by the ODoS filter. In which,
the original image is considered the principal part to describe various image
appearance and complex background illumination conditions, a multi-step
strategy is used to enhance the contrast between curvilinear objects and their
surrounding backgrounds, and a vector field is applied to discriminate thin and
uneven curvilinear structures. Subsequently, a deep learning framework is
employed to extract various structural features for curvilinear object
segmentation in medical images. The performance of the computational model is
validated in experiments conducted on the publicly available DRIVE, STARE and
CHASEDB1 datasets. The experimental results indicate that the presented model
yields surprising results compared with those of some state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yuanyuan Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pan_L/0/1/0/all/0/1&quot;&gt;Lin Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Luan_P/0/1/0/all/0/1&quot;&gt;Pengpeng Luan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tu_H/0/1/0/all/0/1&quot;&gt;Hongbin Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.09602">
<title>Adapting the Hypersphere Loss Function from Anomaly Detection to Anomaly Segmentation. (arXiv:2301.09602v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.09602</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an incremental improvement to Fully Convolutional Data Description
(FCDD), an adaptation of the one-class classification approach from anomaly
detection to image anomaly segmentation (a.k.a. anomaly localization). We
analyze its original loss function and propose a substitute that better
resembles its predecessor, the Hypersphere Classifier (HSC). Both are compared
on the MVTec Anomaly Detection Dataset (MVTec-AD) -- training images are
flawless objects/textures and the goal is to segment unseen defects -- showing
that consistent improvement is achieved by better designing the pixel-wise
supervision.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertoldo_J/0/1/0/all/0/1&quot;&gt;Joao P. C. Bertoldo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Velasco_Forero_S/0/1/0/all/0/1&quot;&gt;Santiago Velasco-Forero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Angulo_J/0/1/0/all/0/1&quot;&gt;Jesus Angulo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Decenciere_E/0/1/0/all/0/1&quot;&gt;Etienne Decenci&amp;#xe8;re&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.03750">
<title>Linking convolutional kernel size to generalization bias in face analysis CNNs. (arXiv:2302.03750v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.03750</link>
<description rdf:parseType="Literal">&lt;p&gt;Training dataset biases are by far the most scrutinized factors when
explaining algorithmic biases of neural networks. In contrast, hyperparameters
related to the neural network architecture have largely been ignored even
though different network parameterizations are known to induce different
implicit biases over learned features. For example, convolutional kernel size
is known to affect the frequency content of features learned in CNNs. In this
work, we present a causal framework for linking an architectural hyperparameter
to out-of-distribution algorithmic bias. Our framework is experimental, in that
we train several versions of a network with an intervention to a specific
hyperparameter, and measure the resulting causal effect of this choice on
performance bias when a particular out-of-distribution image perturbation is
applied. In our experiments, we focused on measuring the causal relationship
between convolutional kernel size and face analysis classification bias across
different subpopulations (race/gender), with respect to high-frequency image
details. We show that modifying kernel size, even in one layer of a CNN,
changes the frequency content of learned features significantly across data
subgroups leading to biased generalization performance even in the presence of
a balanced dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1&quot;&gt;Hao Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caro_J/0/1/0/all/0/1&quot;&gt;Josue Ortega Caro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maheshri_V/0/1/0/all/0/1&quot;&gt;Vikram Maheshri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1&quot;&gt;Ankit B. Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balakrishnan_G/0/1/0/all/0/1&quot;&gt;Guha Balakrishnan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.04440">
<title>Feature Likelihood Score: Evaluating the Generalization of Generative Models Using Samples. (arXiv:2302.04440v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.04440</link>
<description rdf:parseType="Literal">&lt;p&gt;The past few years have seen impressive progress in the development of deep
generative models capable of producing high-dimensional, complex, and
photo-realistic data. However, current methods for evaluating such models
remain incomplete: standard likelihood-based metrics do not always apply and
rarely correlate with perceptual fidelity, while sample-based metrics, such as
FID, are insensitive to overfitting, i.e., inability to generalize beyond the
training set. To address these limitations, we propose a new metric called the
Feature Likelihood Score (FLS), a parametric sample-based score that uses
density estimation to provide a comprehensive trichotomic evaluation accounting
for novelty (i.e., different from the training samples), fidelity, and
diversity of generated samples. We empirically demonstrate the ability of FLS
to identify specific overfitting problem cases, where previously proposed
metrics fail. We also extensively evaluate FLS on various image datasets and
model classes, demonstrating its ability to match intuitions of previous
metrics like FID while offering a more comprehensive evaluation of generative
models. Code is available at https://github.com/marcojira/fls.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiralerspong_M/0/1/0/all/0/1&quot;&gt;Marco Jiralerspong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bose_A/0/1/0/all/0/1&quot;&gt;Avishek Joey Bose&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gemp_I/0/1/0/all/0/1&quot;&gt;Ian Gemp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1&quot;&gt;Chongli Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bachrach_Y/0/1/0/all/0/1&quot;&gt;Yoram Bachrach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gidel_G/0/1/0/all/0/1&quot;&gt;Gauthier Gidel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.03307">
<title>Learning Efficient Coding of Natural Images with Maximum Manifold Capacity Representations. (arXiv:2303.03307v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.03307</link>
<description rdf:parseType="Literal">&lt;p&gt;The efficient coding hypothesis proposes that the response properties of
sensory systems are adapted to the statistics of their inputs such that they
capture maximal information about the environment, subject to biological
constraints. While elegant, information theoretic properties are notoriously
difficult to measure in practical settings or to employ as objective functions
in optimization. This difficulty has necessitated that computational models
designed to test the hypothesis employ several different information metrics
ranging from approximations and lower bounds to proxy measures like
reconstruction error. Recent theoretical advances have characterized a novel
and ecologically relevant efficiency metric, the manifold capacity, which is
the number of object categories that may be represented in a linearly separable
fashion. However, calculating manifold capacity is a computationally intensive
iterative procedure that until now has precluded its use as an objective. Here
we outline the simplifying assumptions that allow manifold capacity to be
optimized directly, yielding Maximum Manifold Capacity Representations (MMCR).
The resulting method is closely related to and inspired by advances in the
field of self supervised learning (SSL), and we demonstrate that MMCRs are
competitive with state of the art results on standard SSL benchmarks. Empirical
analyses reveal differences between MMCRs and representations learned by other
SSL frameworks, and suggest a mechanism by which manifold compression gives
rise to class separability. Finally we evaluate a set of SSL methods on a suite
of neural predictivity benchmarks, and find MMCRs are higly competitive as
models of the ventral stream.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yerxa_T/0/1/0/all/0/1&quot;&gt;Thomas Yerxa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuang_Y/0/1/0/all/0/1&quot;&gt;Yilun Kuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simoncelli_E/0/1/0/all/0/1&quot;&gt;Eero Simoncelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1&quot;&gt;SueYeon Chung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.06908">
<title>CrossFormer++: A Versatile Vision Transformer Hinging on Cross-scale Attention. (arXiv:2303.06908v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.06908</link>
<description rdf:parseType="Literal">&lt;p&gt;While features of different scales are perceptually important to visual
inputs, existing vision transformers do not yet take advantage of them
explicitly. To this end, we first propose a cross-scale vision transformer,
CrossFormer. It introduces a cross-scale embedding layer (CEL) and a long-short
distance attention (LSDA). On the one hand, CEL blends each token with multiple
patches of different scales, providing the self-attention module itself with
cross-scale features. On the other hand, LSDA splits the self-attention module
into a short-distance one and a long-distance counterpart, which not only
reduces the computational burden but also keeps both small-scale and
large-scale features in the tokens. Moreover, through experiments on
CrossFormer, we observe another two issues that affect vision transformers&apos;
performance, i.e., the enlarging self-attention maps and amplitude explosion.
Thus, we further propose a progressive group size (PGS) paradigm and an
amplitude cooling layer (ACL) to alleviate the two issues, respectively. The
CrossFormer incorporating with PGS and ACL is called CrossFormer++. Extensive
experiments show that CrossFormer++ outperforms the other vision transformers
on image classification, object detection, instance segmentation, and semantic
segmentation tasks. The code will be available at:
https://github.com/cheerss/CrossFormer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenxiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_Q/0/1/0/all/0/1&quot;&gt;Qibo Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Long Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Boxi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Binbin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xiaofei He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.08240">
<title>Parametric Surface Constrained Upsampler Network for Point Cloud. (arXiv:2303.08240v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.08240</link>
<description rdf:parseType="Literal">&lt;p&gt;Designing a point cloud upsampler, which aims to generate a clean and dense
point cloud given a sparse point representation, is a fundamental and
challenging problem in computer vision. A line of attempts achieves this goal
by establishing a point-to-point mapping function via deep neural networks.
However, these approaches are prone to produce outlier points due to the lack
of explicit surface-level constraints. To solve this problem, we introduce a
novel surface regularizer into the upsampler network by forcing the neural
network to learn the underlying parametric surface represented by bicubic
functions and rotation functions, where the new generated points are then
constrained on the underlying surface. These designs are integrated into two
different networks for two tasks that take advantages of upsampling layers -
point cloud upsampling and point cloud completion for evaluation. The
state-of-the-art experimental results on both tasks demonstrate the
effectiveness of the proposed method. The code is available at
https://github.com/corecai163/PSCU.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1&quot;&gt;Pingping Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhenyao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xinyi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Song Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.09051">
<title>Robust Evaluation of Diffusion-Based Adversarial Purification. (arXiv:2303.09051v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.09051</link>
<description rdf:parseType="Literal">&lt;p&gt;We question the current evaluation practice on diffusion-based purification
methods. Diffusion-based purification methods aim to remove adversarial effects
from an input data point at test time. The approach gains increasing attention
as an alternative to adversarial training due to the disentangling between
training and testing. Well-known white-box attacks are often employed to
measure the robustness of the purification. However, it is unknown whether
these attacks are the most effective for the diffusion-based purification since
the attacks are often tailored for adversarial training. We analyze the current
practices and provide a new guideline for measuring the robustness of
purification methods against adversarial attacks. Based on our analysis, we
further propose a new purification strategy improving robustness compared to
the current diffusion-based purification methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1&quot;&gt;Minjong Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Dongwoo Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.12484">
<title>Label-Efficient Deep Learning in Medical Image Analysis: Challenges and Future Directions. (arXiv:2303.12484v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.12484</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has seen rapid growth in recent years and achieved
state-of-the-art performance in a wide range of applications. However, training
models typically requires expensive and time-consuming collection of large
quantities of labeled data. This is particularly true within the scope of
medical imaging analysis (MIA), where data are limited and labels are expensive
to be acquired. Thus, label-efficient deep learning methods are developed to
make comprehensive use of the labeled data as well as the abundance of
unlabeled and weak-labeled data. In this survey, we extensively investigated
over 300 recent papers to provide a comprehensive overview of recent progress
on label-efficient learning strategies in MIA. We first present the background
of label-efficient learning and categorize the approaches into different
schemes. Next, we examine the current state-of-the-art methods in detail
through each scheme. Specifically, we provide an in-depth investigation,
covering not only canonical semi-supervised, self-supervised, and
multi-instance learning schemes, but also recently emerged active and
annotation-efficient learning strategies. Moreover, as a comprehensive
contribution to the field, this survey not only elucidates the commonalities
and unique features of the surveyed methods but also presents a detailed
analysis of the current challenges in the field and suggests potential avenues
for future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1&quot;&gt;Cheng Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zhengrui Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1&quot;&gt;Luyang Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.13397">
<title>DiffMesh: A Motion-aware Diffusion-like Framework for Human Mesh Recovery from Videos. (arXiv:2303.13397v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.13397</link>
<description rdf:parseType="Literal">&lt;p&gt;Human mesh recovery (HMR) provides rich human body information for various
real-world applications. While image-based HMR methods have achieved impressive
results, they often struggle to recover humans in dynamic scenarios, leading to
temporal inconsistencies and non-smooth 3D motion predictions due to the
absence of human motion. In contrast, video-based approaches leverage temporal
information to mitigate this issue. In this paper, we present DiffMesh, an
innovative motion-aware Diffusion-like framework for video-based HMR. DiffMesh
establishes a bridge between diffusion models and human motion, efficiently
generating accurate and smooth output mesh sequences by incorporating human
motion within the forward process and reverse process in the diffusion model.
Extensive experiments are conducted on the widely used datasets (Human3.6M
\cite{h36m_pami} and 3DPW \cite{pw3d2018}), which demonstrate the effectiveness
and efficiency of our DiffMesh. Visual comparisons in real-world scenarios
further highlight DiffMesh&apos;s suitability for practical applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Ce Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xianpeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mengyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tianfu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1&quot;&gt;Guo-Jun Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.13514">
<title>SAOR: Single-View Articulated Object Reconstruction. (arXiv:2303.13514v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.13514</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce SAOR, a novel approach for estimating the 3D shape, texture, and
viewpoint of an articulated object from a single image captured in the wild.
Unlike prior approaches that rely on pre-defined category-specific 3D templates
or tailored 3D skeletons, SAOR learns to articulate shapes from single-view
image collections with a skeleton-free part-based model without requiring any
3D object shape priors. To prevent ill-posed solutions, we propose a
cross-instance consistency loss that exploits disentangled object shape
deformation and articulation. This is helped by a new silhouette-based sampling
mechanism to enhance viewpoint diversity during training. Our method only
requires estimated object silhouettes and relative depth maps from
off-the-shelf pre-trained networks during training. At inference time, given a
single-view image, it efficiently outputs an explicit mesh representation. We
obtain improved qualitative and quantitative results on challenging quadruped
animals compared to relevant existing work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aygun_M/0/1/0/all/0/1&quot;&gt;Mehmet Ayg&amp;#xfc;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aodha_O/0/1/0/all/0/1&quot;&gt;Oisin Mac Aodha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.13843">
<title>CompoNeRF: Text-guided Multi-object Compositional NeRF with Editable 3D Scene Layout. (arXiv:2303.13843v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.13843</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances have shown promise in merging neural radiance fields (NeRFs)
with pre-trained diffusion models for text-to-3D object generation. However,
one enduring challenge is their inadequate capability to accurately parse and
regenerate consistent multi-object environments. Specifically, these models
encounter difficulties in accurately representing quantity and style prompted
by multi-object texts, often resulting in a collapse of the rendering fidelity
that fails to match the semantic intricacies. Moreover, amalgamating these
elements into a coherent 3D scene is a substantial challenge, stemming from
generic distribution inherent in diffusion models. To tackle the issue of
&apos;guidance collapse&apos; and enhance consistency, we propose a novel framework,
dubbed CompoNeRF, by integrating an editable 3D scene layout with object
specific and scene-wide guidance mechanisms. It initiates by interpreting a
complex text into an editable 3D layout populated with multiple NeRFs, each
paired with a corresponding subtext prompt for precise object depiction. Next,
a tailored composition module seamlessly blends these NeRFs, promoting
consistency, while the dual-level text guidance reduces ambiguity and boosts
accuracy. Noticeably, the unique modularity of CompoNeRF permits NeRF
decomposition. This enables flexible scene editing and recomposition into new
scenes based on the edited layout or text prompts. Utilizing the open source
Stable Diffusion model, CompoNeRF not only generates scenes with high fidelity
but also paves the way for innovative multi-object composition using editable
3D layouts. Remarkably, our framework achieves up to a 54\% improvement in
performance, as measured by the multi-view CLIP score metric. Code is available
at https://github.com/hbai98/Componerf.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1&quot;&gt;Haotian Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1&quot;&gt;Yuanhuiyi Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1&quot;&gt;Lutao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sijia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Haonan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xiaodong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.17580">
<title>HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face. (arXiv:2303.17580v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2303.17580</link>
<description rdf:parseType="Literal">&lt;p&gt;Solving complicated AI tasks with different domains and modalities is a key
step toward artificial general intelligence. While there are numerous AI models
available for various domains and modalities, they cannot handle complicated AI
tasks autonomously. Considering large language models (LLMs) have exhibited
exceptional abilities in language understanding, generation, interaction, and
reasoning, we advocate that LLMs could act as a controller to manage existing
AI models to solve complicated AI tasks, with language serving as a generic
interface to empower this. Based on this philosophy, we present HuggingGPT, an
LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI
models in machine learning communities (e.g., Hugging Face) to solve AI tasks.
Specifically, we use ChatGPT to conduct task planning when receiving a user
request, select models according to their function descriptions available in
Hugging Face, execute each subtask with the selected AI model, and summarize
the response according to the execution results. By leveraging the strong
language capability of ChatGPT and abundant AI models in Hugging Face,
HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different
modalities and domains and achieve impressive results in language, vision,
speech, and other challenging tasks, which paves a new way towards the
realization of artificial general intelligence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yongliang Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1&quot;&gt;Kaitao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1&quot;&gt;Xu Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dongsheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1&quot;&gt;Weiming Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1&quot;&gt;Yueting Zhuang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.17783">
<title>Uncertainty-Aware Source-Free Adaptive Image Super-Resolution with Wavelet Augmentation Transformer. (arXiv:2303.17783v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.17783</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised Domain Adaptation (UDA) can effectively address domain gap
issues in real-world image Super-Resolution (SR) by accessing both the source
and target data. Considering privacy policies or transmission restrictions of
source data in practical scenarios, we propose a SOurce-free Domain Adaptation
framework for image SR (SODA-SR) to address this issue, i.e., adapt a
source-trained model to a target domain with only unlabeled target data.
SODA-SR leverages the source-trained model to generate refined pseudo-labels
for teacher-student learning. To better utilize pseudo-labels, we propose a
novel wavelet-based augmentation method, named Wavelet Augmentation Transformer
(WAT), which can be flexibly incorporated with existing networks, to implicitly
produce useful augmented data. WAT learns low-frequency information of varying
levels across diverse samples, which is aggregated efficiently via deformable
attention. Furthermore, an uncertainty-aware self-training mechanism is
proposed to improve the accuracy of pseudo-labels, with inaccurate predictions
being rectified by uncertainty estimation. To acquire better SR results and
avoid overfitting pseudo-labels, several regularization losses are proposed to
constrain target LR and SR images in the frequency domain. Experiments show
that without accessing source data, SODA-SR outperforms state-of-the-art UDA
methods in both synthetic$\rightarrow$real and real$\rightarrow$real adaptation
settings, and is not constrained by specific network architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ai_Y/0/1/0/all/0/1&quot;&gt;Yuang Ai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xiaoqiang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Huaibo Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1&quot;&gt;Ran He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.00306">
<title>CapsFlow: Optical Flow Estimation with Capsule Networks. (arXiv:2304.00306v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.00306</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a framework to use recently introduced Capsule Networks for
solving the problem of Optical Flow, one of the fundamental computer vision
tasks. Most of the existing state of the art deep architectures either uses a
correlation oepration to match features from them. While correlation layer is
sensitive to the choice of hyperparameters and does not put a prior on the
underlying structure of the object, spatio temporal features will be limited by
the network&apos;s receptive field. Also, we as humans look at moving objects as
whole, something which cannot be encoded by correlation or spatio temporal
features. Capsules, on the other hand, are specialized to model seperate
entities and their pose as a continuous matrix. Thus, we show that a simpler
linear operation over poses of the objects detected by the capsules in enough
to model flow. We show reslts on a small toy dataset where we outperform
FlowNetC and PWC-Net models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chand_R/0/1/0/all/0/1&quot;&gt;Rahul Chand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_R/0/1/0/all/0/1&quot;&gt;Rajat Arora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabhakar_K/0/1/0/all/0/1&quot;&gt;K Ram Prabhakar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babu_R/0/1/0/all/0/1&quot;&gt;R Venkatesh Babu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.06433">
<title>High-Fidelity Zero-Shot Texture Anomaly Localization Using Feature Correspondence Analysis. (arXiv:2304.06433v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.06433</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel method for Zero-Shot Anomaly Localization on textures. The
task refers to identifying abnormal regions in an otherwise homogeneous image.
To obtain a high-fidelity localization, we leverage a bijective mapping derived
from the 1-dimensional Wasserstein Distance. As opposed to using holistic
distances between distributions, the proposed approach allows pinpointing the
non-conformity of a pixel in a local context with increased precision. By
aggregating the contribution of the pixel to the errors of all nearby patches
we obtain a reliable anomaly score estimate. We validate our solution on
several datasets and obtain more than a 40% reduction in error over the
previous state of the art on the MVTec AD dataset in a zero-shot setting. Also
see https://reality.tf.fau.de/pub/ardelean2024highfidelity.html.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ardelean_A/0/1/0/all/0/1&quot;&gt;Andrei-Timotei Ardelean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weyrich_T/0/1/0/all/0/1&quot;&gt;Tim Weyrich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13622">
<title>Continual Learning with Strong Experience Replay. (arXiv:2305.13622v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13622</link>
<description rdf:parseType="Literal">&lt;p&gt;Continual Learning (CL) aims at incrementally learning new tasks without
forgetting the knowledge acquired from old ones. Experience Replay (ER) is a
simple and effective rehearsal-based strategy, which optimizes the model with
current training data and a subset of old samples stored in a memory buffer. To
further reduce forgetting, recent approaches extend ER with various techniques,
such as model regularization and memory sampling. However, the prediction
consistency between the new model and the old one on current training data has
been seldom explored, resulting in less knowledge preserved when few previous
samples are available. To address this issue, we propose a CL method with
Strong Experience Replay (SER), which additionally utilizes future experiences
mimicked on the current training data, besides distilling past experience from
the memory buffer. In our method, the updated model will produce approximate
outputs as its original ones, which can effectively preserve the acquired
knowledge. Experimental results on multiple image classification datasets show
that our SER method surpasses the state-of-the-art methods by a noticeable
margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuo_T/0/1/0/all/0/1&quot;&gt;Tao Zhuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1&quot;&gt;Zan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1&quot;&gt;Hehe Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1&quot;&gt;Mohan Kankanhalli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15712">
<title>Knowledge Diffusion for Distillation. (arXiv:2305.15712v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15712</link>
<description rdf:parseType="Literal">&lt;p&gt;The representation gap between teacher and student is an emerging topic in
knowledge distillation (KD). To reduce the gap and improve the performance,
current methods often resort to complicated training schemes, loss functions,
and feature alignments, which are task-specific and feature-specific. In this
paper, we state that the essence of these methods is to discard the noisy
information and distill the valuable information in the feature, and propose a
novel KD method dubbed DiffKD, to explicitly denoise and match features using
diffusion models. Our approach is based on the observation that student
features typically contain more noises than teacher features due to the smaller
capacity of student model. To address this, we propose to denoise student
features using a diffusion model trained by teacher features. This allows us to
perform better distillation between the refined clean feature and teacher
feature. Additionally, we introduce a light-weight diffusion model with a
linear autoencoder to reduce the computation cost and an adaptive noise
matching module to improve the denoising performance. Extensive experiments
demonstrate that DiffKD is effective across various types of features and
achieves state-of-the-art performance consistently on image classification,
object detection, and semantic segmentation tasks. Code is available at
https://github.com/hunto/DiffKD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1&quot;&gt;Mingkai Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1&quot;&gt;Shan You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1&quot;&gt;Chen Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chang Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16746">
<title>CNN Feature Map Augmentation for Single-Source Domain Generalization. (arXiv:2305.16746v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16746</link>
<description rdf:parseType="Literal">&lt;p&gt;In search of robust and generalizable machine learning models, Domain
Generalization (DG) has gained significant traction during the past few years.
The goal in DG is to produce models which continue to perform well when
presented with data distributions different from the ones available during
training. While deep convolutional neural networks (CNN) have been able to
achieve outstanding performance on downstream computer vision tasks, they still
often fail to generalize on previously unseen data Domains. Therefore, in this
work we focus on producing a model which is able to remain robust under data
distribution shift and propose an alternative regularization technique for
convolutional neural network architectures in the single-source DG image
classification setting. To mitigate the problem caused by domain shift between
source and target data, we propose augmenting intermediate feature maps of
CNNs. Specifically, we pass them through a novel Augmentation Layer} to prevent
models from overfitting on the training set and improve their cross-domain
generalization. To the best of our knowledge, this is the first paper proposing
such a setup for the DG image classification setting. Experiments on the DG
benchmark datasets of PACS, VLCS, Office-Home and TerraIncognita validate the
effectiveness of our method, in which our model surpasses state-of-the-art
algorithms in most cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ballas_A/0/1/0/all/0/1&quot;&gt;Aristotelis Ballas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diou_C/0/1/0/all/0/1&quot;&gt;Christos Diou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19518">
<title>Label-Retrieval-Augmented Diffusion Models for Learning from Noisy Labels. (arXiv:2305.19518v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19518</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning from noisy labels is an important and long-standing problem in
machine learning for real applications. One of the main research lines focuses
on learning a label corrector to purify potential noisy labels. However, these
methods typically rely on strict assumptions and are limited to certain types
of label noise. In this paper, we reformulate the label-noise problem from a
generative-model perspective, $\textit{i.e.}$, labels are generated by
gradually refining an initial random guess. This new perspective immediately
enables existing powerful diffusion models to seamlessly learn the stochastic
generative process. Once the generative uncertainty is modeled, we can perform
classification inference using maximum likelihood estimation of labels. To
mitigate the impact of noisy labels, we propose the
$\textbf{L}$abel-$\textbf{R}$etrieval-$\textbf{A}$ugmented (LRA) diffusion
model, which leverages neighbor consistency to effectively construct
pseudo-clean labels for diffusion training. Our model is flexible and general,
allowing easy incorporation of different types of conditional information,
$\textit{e.g.}$, use of pre-trained models, to further boost model performance.
Extensive experiments are conducted for evaluation. Our model achieves new
state-of-the-art (SOTA) results on all the standard real-world benchmark
datasets. Remarkably, by incorporating conditional information from the
powerful CLIP model, our method can boost the current SOTA accuracy by 10-20
absolute points in many cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruiyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1&quot;&gt;Tong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1&quot;&gt;Rohan Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1&quot;&gt;Tong Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Changyou Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00783">
<title>FaceDNeRF: Semantics-Driven Face Reconstruction, Prompt Editing and Relighting with Diffusion Models. (arXiv:2306.00783v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00783</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to create high-quality 3D faces from a single image has become
increasingly important with wide applications in video conferencing, AR/VR, and
advanced video editing in movie industries. In this paper, we propose Face
Diffusion NeRF (FaceDNeRF), a new generative method to reconstruct high-quality
Face NeRFs from single images, complete with semantic editing and relighting
capabilities. FaceDNeRF utilizes high-resolution 3D GAN inversion and expertly
trained 2D latent-diffusion model, allowing users to manipulate and construct
Face NeRFs in zero-shot learning without the need for explicit 3D data. With
carefully designed illumination and identity preserving loss, as well as
multi-modal pre-training, FaceDNeRF offers users unparalleled control over the
editing process enabling them to create and edit face NeRFs using just
single-view images, text prompts, and explicit target lighting. The advanced
features of FaceDNeRF have been designed to produce more impressive results
than existing 2D editing approaches that rely on 2D segmentation maps for
editable attributes. Experiments show that our FaceDNeRF achieves exceptionally
realistic results and unprecedented flexibility in editing compared with
state-of-the-art 3D face reconstruction and editing methods. Our code will be
available at https://github.com/BillyXYB/FaceDNeRF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yanbo Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1&quot;&gt;Tianyuan Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1&quot;&gt;Yu-Wing Tai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1&quot;&gt;Chi-Keung Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.02956">
<title>Explicit Neural Surfaces: Learning Continuous Geometry With Deformation Fields. (arXiv:2306.02956v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.02956</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Explicit Neural Surfaces (ENS), an efficient surface
reconstruction method that learns an explicitly defined continuous surface from
multiple views. We use a series of neural deformation fields to progressively
transform a continuous input surface to a target shape. By sampling meshes as
discrete surface proxies, we train the deformation fields through efficient
differentiable rasterization, and attain a mesh-independent and smooth surface
representation. By using Laplace-Beltrami eigenfunctions as an intrinsic
positional encoding alongside standard extrinsic Fourier features, our approach
can capture fine surface details. ENS trains 1 to 2 orders of magnitude faster
and can extract meshes of higher quality compared to implicit representations,
whilst maintaining competitive surface reconstruction performance and real-time
capabilities. Finally, we apply our approach to learn a collection of objects
in a single model, and achieve disentangled interpolations between different
shapes, their surface details, and textures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walker_T/0/1/0/all/0/1&quot;&gt;Thomas Walker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mariotti_O/0/1/0/all/0/1&quot;&gt;Octave Mariotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaxman_A/0/1/0/all/0/1&quot;&gt;Amir Vaxman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bilen_H/0/1/0/all/0/1&quot;&gt;Hakan Bilen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04527">
<title>ContriMix: Unsupervised disentanglement of content and attribute for domain generalization in microscopy image analysis. (arXiv:2306.04527v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04527</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain generalization is critical for real-world applications of machine
learning to microscopy images, including histopathology and fluorescence
imaging. Artifacts in these modalities arise through a complex combination of
factors relating to tissue collection and laboratory processing, as well as
factors intrinsic to patient samples. In fluorescence imaging, these artifacts
stem from variations across experimental batches. The complexity and subtlety
of these artifacts make the enumeration of data domains intractable. Therefore,
augmentation-based methods of domain generalization that require domain
identifiers and manual fine-tuning are inadequate in this setting. To overcome
this challenge, we introduce ContriMix, a domain generalization technique that
learns to generate synthetic images by disentangling and permuting the
biological content (&quot;content&quot;) and technical variations (&quot;attributes&quot;) in
microscopy images. ContriMix does not rely on domain identifiers or handcrafted
augmentations and makes no assumptions about the input characteristics of
images. We assess the performance of ContriMix on two pathology datasets
dealing with patch classification and Whole Slide Image label prediction tasks
respectively (Camelyon17-WILDS and RCC subtyping), and one fluorescence
microscopy dataset (RxRx1-WILDS). Without any access to domain identifiers at
train or test time, ContriMix performs similar or better than current
state-of-the-art methods in all these datasets, motivating its usage for
microscopy image analysis in real-world settings where domain information is
hard to come by. The code for ContriMix can be found at
https://gitlab.com/huutan86/contrimix
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tan H. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Juyal_D/0/1/0/all/0/1&quot;&gt;Dinkar Juyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Prakash_A/0/1/0/all/0/1&quot;&gt;Aaditya Prakash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nofallah_S/0/1/0/all/0/1&quot;&gt;Shima Nofallah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shah_C/0/1/0/all/0/1&quot;&gt;Chintan Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gullapally_S/0/1/0/all/0/1&quot;&gt;Sai Chowdary Gullapally&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Limin Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Griffin_M/0/1/0/all/0/1&quot;&gt;Michael Griffin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sampat_A/0/1/0/all/0/1&quot;&gt;Anand Sampat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Abel_J/0/1/0/all/0/1&quot;&gt;John Abel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Justin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Taylor_Weiner_A/0/1/0/all/0/1&quot;&gt;Amaro Taylor-Weiner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04633">
<title>Contrastive Lift: 3D Object Instance Segmentation by Slow-Fast Contrastive Fusion. (arXiv:2306.04633v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04633</link>
<description rdf:parseType="Literal">&lt;p&gt;Instance segmentation in 3D is a challenging task due to the lack of
large-scale annotated datasets. In this paper, we show that this task can be
addressed effectively by leveraging instead 2D pre-trained models for instance
segmentation. We propose a novel approach to lift 2D segments to 3D and fuse
them by means of a neural field representation, which encourages multi-view
consistency across frames. The core of our approach is a slow-fast clustering
objective function, which is scalable and well-suited for scenes with a large
number of objects. Unlike previous approaches, our method does not require an
upper bound on the number of objects or object tracking across frames. To
demonstrate the scalability of the slow-fast clustering, we create a new
semi-realistic dataset called the Messy Rooms dataset, which features scenes
with up to 500 objects per scene. Our approach outperforms the state-of-the-art
on challenging scenes from the ScanNet, Hypersim, and Replica datasets, as well
as on our newly created Messy Rooms dataset, demonstrating the effectiveness
and scalability of our slow-fast clustering method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhalgat_Y/0/1/0/all/0/1&quot;&gt;Yash Bhalgat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laina_I/0/1/0/all/0/1&quot;&gt;Iro Laina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henriques_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o F. Henriques&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1&quot;&gt;Andrew Zisserman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1&quot;&gt;Andrea Vedaldi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04848">
<title>Interpreting and Improving Diffusion Models Using the Euclidean Distance Function. (arXiv:2306.04848v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04848</link>
<description rdf:parseType="Literal">&lt;p&gt;Denoising is intuitively related to projection. Indeed, under the manifold
hypothesis, adding random noise is approximately equivalent to orthogonal
perturbation. Hence, learning to denoise is approximately learning to project.
In this paper, we use this observation to reinterpret denoising diffusion
models as approximate gradient descent applied to the Euclidean distance
function. We then provide straight-forward convergence analysis of the DDIM
sampler under simple assumptions on the projection-error of the denoiser.
Finally, we propose a new sampler based on two simple modifications to DDIM
using insights from our theoretical results. In as few as 5-10 function
evaluations, our sampler achieves state-of-the-art FID scores on pretrained
CIFAR-10 and CelebA models and can generate high quality samples on latent
diffusion models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Permenter_F/0/1/0/all/0/1&quot;&gt;Frank Permenter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1&quot;&gt;Chenyang Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06082">
<title>Augmentation-aware Self-supervised Learning with Conditioned Projector. (arXiv:2306.06082v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06082</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised learning (SSL) is a powerful technique for learning robust
representations from unlabeled data. By learning to remain invariant to applied
data augmentations, methods such as SimCLR and MoCo are able to reach quality
on par with supervised approaches. However, this invariance may be harmful to
solving some downstream tasks which depend on traits affected by augmentations
used during pretraining, such as color. In this paper, we propose to foster
sensitivity to such characteristics in the representation space by modifying
the projector network, a common component of self-supervised architectures.
Specifically, we supplement the projector with information about augmentations
applied to images. In order for the projector to take advantage of this
auxiliary conditioning when solving the SSL task, the feature extractor learns
to preserve the augmentation information in its representations. Our approach,
coined Conditional Augmentation-aware Self-supervised Learning (CASSLE), is
directly applicable to typical joint-embedding SSL methods regardless of their
objective functions. Moreover, it does not require major changes in the network
architecture or prior knowledge of downstream tasks. In addition to an analysis
of sensitivity towards different data augmentations, we conduct a series of
experiments, which show that CASSLE improves over various SSL methods, reaching
state-of-the-art performance in multiple downstream tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Przewiezlikowski_M/0/1/0/all/0/1&quot;&gt;Marcin Przewi&amp;#x119;&amp;#x17a;likowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pyla_M/0/1/0/all/0/1&quot;&gt;Mateusz Pyla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zielinski_B/0/1/0/all/0/1&quot;&gt;Bartosz Zieli&amp;#x144;ski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Twardowski_B/0/1/0/all/0/1&quot;&gt;Bart&amp;#x142;omiej Twardowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tabor_J/0/1/0/all/0/1&quot;&gt;Jacek Tabor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smieja_M/0/1/0/all/0/1&quot;&gt;Marek &amp;#x15a;mieja&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07581">
<title>Binary Radiance Fields. (arXiv:2306.07581v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07581</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose \textit{binary radiance fields} (BiRF), a
storage-efficient radiance field representation employing binary feature
encoding that encodes local features using binary encoding parameters in a
format of either $+1$ or $-1$. This binarization strategy lets us represent the
feature grid with highly compact feature encoding and a dramatic reduction in
storage size. Furthermore, our 2D-3D hybrid feature grid design enhances the
compactness of feature encoding as the 3D grid includes main components while
2D grids capture details. In our experiments, binary radiance field
representation successfully outperforms the reconstruction performance of
state-of-the-art (SOTA) efficient radiance field models with lower storage
allocation. In particular, our model achieves impressive results in static
scene reconstruction, with a PSNR of 32.03 dB for Synthetic-NeRF scenes, 34.48
dB for Synthetic-NSVF scenes, 28.20 dB for Tanks and Temples scenes while only
utilizing 0.5 MB of storage space, respectively. We hope the proposed binary
radiance field representation will make radiance fields more accessible without
a storage bottleneck.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1&quot;&gt;Seungjoo Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Jaesik Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08736">
<title>LoSh: Long-Short Text Joint Prediction Network for Referring Video Object Segmentation. (arXiv:2306.08736v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08736</link>
<description rdf:parseType="Literal">&lt;p&gt;Referring video object segmentation (RVOS) aims to segment the target
instance referred by a given text expression in a video clip. The text
expression normally contains sophisticated description of the instance&apos;s
appearance, action, and relation with others. It is therefore rather difficult
for a RVOS model to capture all these attributes correspondingly in the video;
in fact, the model often favours more on the action- and relation-related
visual attributes of the instance. This can end up with partial or even
incorrect mask prediction of the target instance. We tackle this problem by
taking a subject-centric short text expression from the original long text
expression. The short one retains only the appearance-related information of
the target instance so that we can use it to focus the model&apos;s attention on the
instance&apos;s appearance. We let the model make joint predictions using both long
and short text expressions; and insert a long-short cross-attention module to
interact the joint features and a long-short predictions intersection loss to
regulate the joint predictions. Besides the improvement on the linguistic part,
we also introduce a forward-backward visual consistency loss, which utilizes
optical flows to warp visual features between the annotated frames and their
temporal neighbors for consistency. We build our method on top of two state of
the art pipelines. Extensive experiments on A2D-Sentences, Refer-YouTube-VOS,
JHMDB-Sentences and Refer-DAVIS17 show impressive improvements of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1&quot;&gt;Linfeng Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1&quot;&gt;Miaojing Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1&quot;&gt;Zijie Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qijun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08939">
<title>Revisiting Stereo Triangulation in UAV Distance Estimation. (arXiv:2306.08939v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08939</link>
<description rdf:parseType="Literal">&lt;p&gt;Distance estimation plays an important role for path planning and collision
avoidance of swarm UAVs. However, the lack of annotated data seriously hinders
the related studies. In this work, we build and present a UAVDE dataset for UAV
distance estimation, in which distance between two UAVs is obtained by UWB
sensors. During experiments, we surprisingly observe that the stereo
triangulation cannot stand for UAV scenes. The core reason is the position
deviation issue due to long shooting distance and camera vibration, which is
common in UAV scenes. To tackle this issue, we propose a novel position
correction module, which can directly predict the offset between the observed
positions and the actual ones and then perform compensation in stereo
triangulation calculation. Besides, to further boost performance on hard
samples, we propose a dynamic iterative correction mechanism, which is composed
of multiple stacked PCMs and a gating mechanism to adaptively determine whether
further correction is required according to the difficulty of data samples. We
conduct extensive experiments on UAVDE, and our method can achieve a
significant performance improvement over a strong baseline (by reducing the
relative difference from 49.4% to 9.8%), which demonstrates its effectiveness
and superiority. The code and dataset are available at
https://github.com/duanyuan13/PCM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1&quot;&gt;Jiafan Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_D/0/1/0/all/0/1&quot;&gt;Duan Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1&quot;&gt;Rihong Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Weixin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenji Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1&quot;&gt;Zhun Fan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11300">
<title>RS5M: A Large Scale Vision-Language Dataset for Remote Sensing Vision-Language Foundation Model. (arXiv:2306.11300v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11300</link>
<description rdf:parseType="Literal">&lt;p&gt;Pre-trained Vision-Language Models (VLMs) utilizing extensive image-text
paired data have demonstrated unprecedented image-text association
capabilities, achieving remarkable results across various downstream tasks. A
critical challenge is how to make use of existing large-scale pre-trained VLMs,
which are trained on common objects, to perform the domain-specific transfer
for accomplishing domain-related downstream tasks. A critical challenge is how
to make use of existing large-scale pre-trained VLMs, which are trained on
common objects, to perform the domain-specific transfer for accomplishing
domain-related downstream tasks. In this paper, we propose a new framework that
includes the Domain pre-trained Vision-Language Model (DVLM), bridging the gap
between the General Vision-Language Model (GVLM) and domain-specific downstream
tasks. Moreover, we present an image-text paired dataset in the field of remote
sensing (RS), RS5M, which has 5 million RS images with English descriptions.
The dataset is obtained from filtering publicly available image-text paired
datasets and captioning label-only RS datasets with pre-trained VLM. These
constitute the first large-scale RS image-text paired dataset. Additionally, we
fine-tuned the CLIP model and tried several Parameter-Efficient Fine-Tuning
methods on RS5M to implement the DVLM. Experimental results show that our
proposed dataset is highly effective for various tasks, and our model GeoRSCLIP
improves upon the baseline or previous state-of-the-art model by $3\%\sim20\%$
in Zero-shot Classification (ZSC), $3\%\sim6\%$ in Remote Sensing Cross-Modal
Text-Image Retrieval (RSCTIR) and $4\%\sim5\%$ in Semantic Localization (SeLo)
tasks. Dataset and models have been released in:
\url{https://github.com/om-ai-lab/RS5M}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zilun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tiancheng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yulong Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1&quot;&gt;Jianwei Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01197">
<title>Segment Anything Meets Point Tracking. (arXiv:2307.01197v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01197</link>
<description rdf:parseType="Literal">&lt;p&gt;The Segment Anything Model (SAM) has established itself as a powerful
zero-shot image segmentation model, enabled by efficient point-centric
annotation and prompt-based models. While click and brush interactions are both
well explored in interactive image segmentation, the existing methods on videos
focus on mask annotation and propagation. This paper presents SAM-PT, a novel
method for point-centric interactive video segmentation, empowered by SAM and
long-term point tracking. SAM-PT leverages robust and sparse point selection
and propagation techniques for mask generation. Compared to traditional
object-centric mask propagation strategies, we uniquely use point propagation
to exploit local structure information agnostic to object semantics. We
highlight the merits of point-based tracking through direct evaluation on the
zero-shot open-world Unidentified Video Objects (UVO) benchmark. Our
experiments on popular video object segmentation and multi-object segmentation
tracking benchmarks, including DAVIS, YouTube-VOS, and BDD100K, suggest that a
point-based segmentation tracker yields better zero-shot performance and
efficient interactions. We release our code that integrates different point
trackers and video segmentation benchmarks at https://github.com/SysCV/sam-pt.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajic_F/0/1/0/all/0/1&quot;&gt;Frano Raji&amp;#x10d;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ke_L/0/1/0/all/0/1&quot;&gt;Lei Ke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1&quot;&gt;Yu-Wing Tai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1&quot;&gt;Chi-Keung Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1&quot;&gt;Martin Danelljan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1&quot;&gt;Fisher Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02129">
<title>How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model. (arXiv:2307.02129v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02129</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning algorithms demonstrate a surprising ability to learn
high-dimensional tasks from limited examples. This is commonly attributed to
the depth of neural networks, enabling them to build a hierarchy of abstract,
low-dimensional data representations. However, how many training examples are
required to learn such representations remains unknown. To quantitatively study
this question, we introduce the Random Hierarchy Model: a family of synthetic
tasks inspired by the hierarchical structure of language and images. The model
is a classification task where each class corresponds to a group of high-level
features, chosen among several equivalent groups associated with the same
class. In turn, each feature corresponds to a group of sub-features chosen
among several equivalent ones and so on, following a hierarchy of composition
rules. We find that deep networks learn the task by developing internal
representations invariant to exchanging equivalent groups. Moreover, the number
of data required corresponds to the point where correlations between low-level
features and classes become detectable. Overall, our results indicate how deep
networks overcome the curse of dimensionality by building invariant
representations, and provide an estimate of the number of data required to
learn a hierarchical task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cagnetta_F/0/1/0/all/0/1&quot;&gt;Francesco Cagnetta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petrini_L/0/1/0/all/0/1&quot;&gt;Leonardo Petrini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomasini_U/0/1/0/all/0/1&quot;&gt;Umberto M. Tomasini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Favero_A/0/1/0/all/0/1&quot;&gt;Alessandro Favero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wyart_M/0/1/0/all/0/1&quot;&gt;Matthieu Wyart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03166">
<title>VideoGLUE: Video General Understanding Evaluation of Foundation Models. (arXiv:2307.03166v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03166</link>
<description rdf:parseType="Literal">&lt;p&gt;We evaluate existing foundation models video understanding capabilities using
a carefully designed experiment protocol consisting of three hallmark tasks
(action recognition, temporal localization, and spatiotemporal localization),
eight datasets well received by the community, and four adaptation methods
tailoring a foundation model (FM) for a downstream task. Moreover, we propose a
scalar VideoGLUE score (VGS) to measure an FMs efficacy and efficiency when
adapting to general video understanding tasks. Our main findings are as
follows. First, task-specialized models significantly outperform the six FMs
studied in this work, in sharp contrast to what FMs have achieved in natural
language and image understanding. Second,video-native FMs, whose pretraining
data contains the video modality, are generally better than image-native FMs in
classifying motion-rich videos, localizing actions in time, and understanding a
video of more than one action. Third, the video-native FMs can perform well on
video tasks under light adaptations to downstream tasks(e.g., freezing the FM
backbones), while image-native FMs win in full end-to-end finetuning. The first
two observations reveal the need and tremendous opportunities to conduct
research on video-focused FMs, and the last confirms that both tasks and
adaptation methods matter when it comes to the evaluation of FMs. Our code is
released under:
https://github.com/tensorflow/models/tree/master/official/projects/videoglue.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1&quot;&gt;Liangzhe Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gundavarapu_N/0/1/0/all/0/1&quot;&gt;Nitesh Bharadwaj Gundavarapu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Long Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1&quot;&gt;Yin Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1&quot;&gt;Lu Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_M/0/1/0/all/0/1&quot;&gt;Menglin Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weyand_T/0/1/0/all/0/1&quot;&gt;Tobias Weyand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Friedman_L/0/1/0/all/0/1&quot;&gt;Luke Friedman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sirotenko_M/0/1/0/all/0/1&quot;&gt;Mikhail Sirotenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Huisheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schroff_F/0/1/0/all/0/1&quot;&gt;Florian Schroff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adam_H/0/1/0/all/0/1&quot;&gt;Hartwig Adam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Ming-Hsuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Ting Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1&quot;&gt;Boqing Gong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06614">
<title>Interpretable 2D Vision Models for 3D Medical Images. (arXiv:2307.06614v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06614</link>
<description rdf:parseType="Literal">&lt;p&gt;Training Artificial Intelligence (AI) models on 3D images presents unique
challenges compared to the 2D case: Firstly, the demand for computational
resources is significantly higher, and secondly, the availability of large
datasets for pre-training is often limited, impeding training success. This
study proposes a simple approach of adapting 2D networks with an intermediate
feature representation for processing 3D images. Our method employs attention
pooling to learn to assign each slice an importance weight and, by that, obtain
a weighted average of all 2D slices. These weights directly quantify the
contribution of each slice to the contribution and thus make the model
prediction inspectable. We show on all 3D MedMNIST datasets as benchmark and
two real-world datasets consisting of several hundred high-resolution CT or MRI
scans that our approach performs on par with existing methods. Furthermore, we
compare the in-built interpretability of our approach to HiResCam, a
state-of-the-art retrospective interpretability approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ziller_A/0/1/0/all/0/1&quot;&gt;Alexander Ziller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Erdur_A/0/1/0/all/0/1&quot;&gt;Ayhan Can Erdur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Trigui_M/0/1/0/all/0/1&quot;&gt;Marwa Trigui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guvenir_A/0/1/0/all/0/1&quot;&gt;Alp G&amp;#xfc;venir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mueller_T/0/1/0/all/0/1&quot;&gt;Tamara T. Mueller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Muller_P/0/1/0/all/0/1&quot;&gt;Philip M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jungmann_F/0/1/0/all/0/1&quot;&gt;Friederike Jungmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Brandt_J/0/1/0/all/0/1&quot;&gt;Johannes Brandt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Peeken_J/0/1/0/all/0/1&quot;&gt;Jan Peeken&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Braren_R/0/1/0/all/0/1&quot;&gt;Rickmer Braren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1&quot;&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kaissis_G/0/1/0/all/0/1&quot;&gt;Georgios Kaissis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08596">
<title>Omnipotent Adversarial Training in the Wild. (arXiv:2307.08596v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08596</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial training is an important topic in robust deep learning, but the
community lacks attention to its practical usage. In this paper, we aim to
resolve a real-world challenge, i.e., training a model on an imbalanced and
noisy dataset to achieve high clean accuracy and adversarial robustness, with
our proposed Omnipotent Adversarial Training (OAT) strategy. OAT consists of
two innovative methodologies to address the imperfection in the training set.
We first introduce an oracle into the adversarial training process to help the
model learn a correct data-label conditional distribution. This
carefully-designed oracle can provide correct label annotations for adversarial
training. We further propose logits adjustment adversarial training to overcome
the data imbalance issue, which can help the model learn a Bayes-optimal
distribution. Our comprehensive evaluation results show that OAT outperforms
other baselines by more than 20% clean accuracy improvement and 10% robust
accuracy improvement under complex combinations of data imbalance and label
noise scenarios. The code can be found in https://github.com/GuanlinLee/OAT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guanlin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kangjie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1&quot;&gt;Han Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianwei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10135">
<title>A Hierarchical Architecture for Neural Materials. (arXiv:2307.10135v2 [cs.GR] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10135</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural reflectance models are capable of reproducing the spatially-varying
appearance of many real-world materials at different scales. Unfortunately,
existing techniques such as NeuMIP have difficulties handling materials with
strong shadowing effects or detailed specular highlights. In this paper, we
introduce a neural appearance model that offers a new level of accuracy.
Central to our model is an inception-based core network structure that captures
material appearances at multiple scales using parallel-operating kernels and
ensures multi-stage features through specialized convolution layers.
Furthermore, we encode the inputs into frequency space, introduce a
gradient-based loss, and employ it adaptive to the progress of the learning
phase. We demonstrate the effectiveness of our method using a variety of
synthetic and real examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_B/0/1/0/all/0/1&quot;&gt;Bowen Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shuang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jensen_H/0/1/0/all/0/1&quot;&gt;Henrik Wann Jensen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montazeri_Z/0/1/0/all/0/1&quot;&gt;Zahra Montazeri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10205">
<title>Alleviating the Effect of Data Imbalance on Adversarial Training. (arXiv:2307.10205v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10205</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study adversarial training on datasets that obey the
long-tailed distribution, which is practical but rarely explored in previous
works. Compared with conventional adversarial training on balanced datasets,
this process falls into the dilemma of generating uneven adversarial examples
(AEs) and an unbalanced feature embedding space, causing the resulting model to
exhibit low robustness and accuracy on tail data. To combat that, we
theoretically analyze the lower bound of the robust risk to train a model on a
long-tailed dataset to obtain the key challenges in addressing the
aforementioned dilemmas. Based on it, we propose a new adversarial training
framework -- Re-balancing Adversarial Training (REAT). This framework consists
of two components: (1) a new training strategy inspired by the effective number
to guide the model to generate more balanced and informative AEs; (2) a
carefully constructed penalty function to force a satisfactory feature space.
Evaluation results on different datasets and model structures prove that REAT
can effectively enhance the model&apos;s robustness and preserve the model&apos;s clean
accuracy. The code can be found in https://github.com/GuanlinLee/REAT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guanlin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1&quot;&gt;Guowen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianwei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.16449">
<title>MovieChat: From Dense Token to Sparse Memory for Long Video Understanding. (arXiv:2307.16449v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.16449</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, integrating video foundation models and large language models to
build a video understanding system can overcome the limitations of specific
pre-defined vision tasks. Yet, existing systems can only handle videos with
very few frames. For long videos, the computation complexity, memory cost, and
long-term temporal connection impose additional challenges. Taking advantage of
the Atkinson-Shiffrin memory model, with tokens in Transformers being employed
as the carriers of memory in combination with our specially designed memory
mechanism, we propose the MovieChat to overcome these challenges. MovieChat
achieves state-of-the-art performance in long video understanding, along with
the released MovieChat-1K benchmark with 1K long video and 14K manual
annotations for validation of the effectiveness of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_E/0/1/0/all/0/1&quot;&gt;Enxin Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chai_W/0/1/0/all/0/1&quot;&gt;Wenhao Chai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guanhong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yucheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Haoyang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Feiyang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chi_H/0/1/0/all/0/1&quot;&gt;Haozhe Chi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xun Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1&quot;&gt;Tian Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yanting Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1&quot;&gt;Jenq-Neng Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Gaoang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03982">
<title>PARTNER: Level up the Polar Representation for LiDAR 3D Object Detection. (arXiv:2308.03982v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.03982</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, polar-based representation has shown promising properties in
perceptual tasks. In addition to Cartesian-based approaches, which separate
point clouds unevenly, representing point clouds as polar grids has been
recognized as an alternative due to (1) its advantage in robust performance
under different resolutions and (2) its superiority in streaming-based
approaches. However, state-of-the-art polar-based detection methods inevitably
suffer from the feature distortion problem because of the non-uniform division
of polar representation, resulting in a non-negligible performance gap compared
to Cartesian-based approaches. To tackle this issue, we present PARTNER, a
novel 3D object detector in the polar coordinate. PARTNER alleviates the
dilemma of feature distortion with global representation re-alignment and
facilitates the regression by introducing instance-level geometric information
into the detection head. Extensive experiments show overwhelming advantages in
streaming-based detection and different resolutions. Furthermore, our method
outperforms the previous polar-based works with remarkable margins of 3.68% and
9.15% on Waymo and ONCE validation set, thus achieving competitive results over
the state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_M/0/1/0/all/0/1&quot;&gt;Ming Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1&quot;&gt;Yujing Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chunwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1&quot;&gt;Chaoqiang Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xinge Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qingqiu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mi_M/0/1/0/all/0/1&quot;&gt;Michael Bi Mi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinchao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Li Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04774">
<title>E$^3$-UAV: An Edge-based Energy-Efficient Object Detection System for Unmanned Aerial Vehicles. (arXiv:2308.04774v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2308.04774</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivated by the advances in deep learning techniques, the application of
Unmanned Aerial Vehicle (UAV)-based object detection has proliferated across a
range of fields, including vehicle counting, fire detection, and city
monitoring. While most existing research studies only a subset of the
challenges inherent to UAV-based object detection, there are few studies that
balance various aspects to design a practical system for energy consumption
reduction. In response, we present the E$^3$-UAV, an edge-based
energy-efficient object detection system for UAVs. The system is designed to
dynamically support various UAV devices, edge devices, and detection
algorithms, with the aim of minimizing energy consumption by deciding the most
energy-efficient flight parameters (including flight altitude, flight speed,
detection algorithm, and sampling rate) required to fulfill the detection
requirements of the task. We first present an effective evaluation metric for
actual tasks and construct a transparent energy consumption model based on
hundreds of actual flight data to formalize the relationship between energy
consumption and flight parameters. Then we present a lightweight
energy-efficient priority decision algorithm based on a large quantity of
actual flight data to assist the system in deciding flight parameters. Finally,
we evaluate the performance of the system, and our experimental results
demonstrate that it can significantly decrease energy consumption in real-world
scenarios. Additionally, we provide four insights that can assist researchers
and engineers in their efforts to study UAV-based object detection further.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suo_J/0/1/0/all/0/1&quot;&gt;Jiashun Suo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xingzhou Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1&quot;&gt;Weisong Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wei Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.06762">
<title>Tissue Segmentation of Thick-Slice Fetal Brain MR Scans with Guidance from High-Quality Isotropic Volumes. (arXiv:2308.06762v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.06762</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate tissue segmentation of thick-slice fetal brain magnetic resonance
(MR) scans is crucial for both reconstruction of isotropic brain MR volumes and
the quantification of fetal brain development. However, this task is
challenging due to the use of thick-slice scans in clinically-acquired fetal
brain data. To address this issue, we propose to leverage high-quality
isotropic fetal brain MR volumes (and also their corresponding annotations) as
guidance for segmentation of thick-slice scans. Due to existence of significant
domain gap between high-quality isotropic volume (i.e., source data) and
thick-slice scans (i.e., target data), we employ a domain adaptation technique
to achieve the associated knowledge transfer (from high-quality &amp;lt;source&amp;gt;
volumes to thick-slice &amp;lt;target&amp;gt; scans). Specifically, we first register the
available high-quality isotropic fetal brain MR volumes across different
gestational weeks to construct longitudinally-complete source data. To capture
domain-invariant information, we then perform Fourier decomposition to extract
image content and style codes. Finally, we propose a novel Cycle-Consistent
Domain Adaptation Network (C2DA-Net) to efficiently transfer the knowledge
learned from high-quality isotropic volumes for accurate tissue segmentation of
thick-slice scans. Our C2DA-Net can fully utilize a small set of annotated
isotropic volumes to guide tissue segmentation on unannotated thick-slice
scans. Extensive experiments on a large-scale dataset of 372 clinically
acquired thick-slice MR scans demonstrate that our C2DA-Net achieves much
better performance than cutting-edge methods quantitatively and qualitatively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shijie Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xukun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cui_Z/0/1/0/all/0/1&quot;&gt;Zhiming Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;He Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Geng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shen_D/0/1/0/all/0/1&quot;&gt;Dinggang Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11239">
<title>LOCATE: Self-supervised Object Discovery via Flow-guided Graph-cut and Bootstrapped Self-training. (arXiv:2308.11239v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11239</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning object segmentation in image and video datasets without human
supervision is a challenging problem. Humans easily identify moving salient
objects in videos using the gestalt principle of common fate, which suggests
that what moves together belongs together. Building upon this idea, we propose
a self-supervised object discovery approach that leverages motion and
appearance information to produce high-quality object segmentation masks.
Specifically, we redesign the traditional graph cut on images to include motion
information in a linear combination with appearance information to produce edge
weights. Remarkably, this step produces object segmentation masks comparable to
the current state-of-the-art on multiple benchmarks. To further improve
performance, we bootstrap a segmentation network trained on these preliminary
masks as pseudo-ground truths to learn from its own outputs via self-training.
We demonstrate the effectiveness of our approach, named LOCATE, on multiple
standard video object segmentation, image saliency detection, and object
segmentation benchmarks, achieving results on par with and, in many cases
surpassing state-of-the-art methods. We also demonstrate the transferability of
our approach to novel domains through a qualitative study on in-the-wild
images. Additionally, we present extensive ablation analysis to support our
design choices and highlight the contribution of each component of our proposed
method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Silky Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deshmukh_S/0/1/0/all/0/1&quot;&gt;Shripad Deshmukh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkar_M/0/1/0/all/0/1&quot;&gt;Mausoom Sarkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnamurthy_B/0/1/0/all/0/1&quot;&gt;Balaji Krishnamurthy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11909">
<title>Edge-aware Hard Clustering Graph Pooling for Brain Imaging. (arXiv:2308.11909v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11909</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Convolutional Networks (GCNs) can capture non-Euclidean spatial
dependence between different brain regions. The graph pooling operator, a
crucial element of GCNs, enhances the representation learning capability and
facilitates the acquisition of abnormal brain maps. However, most existing
research designs graph pooling operators solely from the perspective of nodes
while disregarding the original edge features. This confines graph pooling
application scenarios and diminishes its ability to capture critical
substructures. In this paper, we propose a novel edge-aware hard clustering
graph pool (EHCPool), which is tailored to dominant edge features and redefines
the clustering process. EHCPool initially introduced the &apos;Edge-to-Node&apos; score
criterion which utilized edge information to evaluate the significance of
nodes. An innovative Iteration n-top strategy was then developed, guided by
edge scores, to adaptively learn sparse hard clustering assignments for graphs.
Additionally, a N-E Aggregation strategy is designed to aggregate node and edge
features in each independent subgraph. Extensive experiments on the multi-site
public datasets demonstrate the superiority and robustness of the proposed
model. More notably, EHCPool has the potential to probe different types of
dysfunctional brain networks from a data-driven perspective. Method code:
https://github.com/swfen/EHCPool
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Cheng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jiayi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lijuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shuqi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Ping Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Honghan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1&quot;&gt;Ying Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.14831">
<title>Continual Learning with Dynamic Sparse Training: Exploring Algorithms for Effective Model Updates. (arXiv:2308.14831v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.14831</link>
<description rdf:parseType="Literal">&lt;p&gt;Continual learning (CL) refers to the ability of an intelligent system to
sequentially acquire and retain knowledge from a stream of data with as little
computational overhead as possible. To this end; regularization, replay,
architecture, and parameter isolation approaches were introduced to the
literature. Parameter isolation using a sparse network which enables to
allocate distinct parts of the neural network to different tasks and also
allows to share of parameters between tasks if they are similar. Dynamic Sparse
Training (DST) is a prominent way to find these sparse networks and isolate
them for each task. This paper is the first empirical study investigating the
effect of different DST components under the CL paradigm to fill a critical
research gap and shed light on the optimal configuration of DST for CL if it
exists. Therefore, we perform a comprehensive study in which we investigate
various DST components to find the best topology per task on well-known
CIFAR100 and miniImageNet benchmarks in a task-incremental CL setup since our
primary focus is to evaluate the performance of various DST criteria, rather
than the process of mask selection. We found that, at a low sparsity level,
Erdos-R\&apos;enyi Kernel (ERK) initialization utilizes the backbone more
efficiently and allows to effectively learn increments of tasks. At a high
sparsity level, unless it is extreme, uniform initialization demonstrates a
more reliable and robust performance. In terms of growth strategy; performance
is dependent on the defined initialization strategy and the extent of sparsity.
Finally, adaptivity within DST components is a promising way for better
continual learners.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yildirim_M/0/1/0/all/0/1&quot;&gt;Murat Onur Yildirim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yildirim_E/0/1/0/all/0/1&quot;&gt;Elif Ceren Gok Yildirim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sokar_G/0/1/0/all/0/1&quot;&gt;Ghada Sokar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mocanu_D/0/1/0/all/0/1&quot;&gt;Decebal Constantin Mocanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vanschoren_J/0/1/0/all/0/1&quot;&gt;Joaquin Vanschoren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.15984">
<title>Learning Structure-from-Motion with Graph Attention Networks. (arXiv:2308.15984v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.15984</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we tackle the problem of learning Structure-from-Motion (SfM)
through the use of graph attention networks. SfM is a classic computer vision
problem that is solved though iterative minimization of reprojection errors,
referred to as Bundle Adjustment (BA), starting from a good initialization. In
order to obtain a good enough initialization to BA, conventional methods rely
on a sequence of sub-problems (such as pairwise pose estimation, pose averaging
or triangulation) which provides an initial solution that can then be refined
using BA. In this work we replace these sub-problems by learning a model that
takes as input the 2D keypoints detected across multiple views, and outputs the
corresponding camera poses and 3D keypoint coordinates. Our model takes
advantage of graph neural networks to learn SfM-specific primitives, and we
show that it can be used for fast inference of the reconstruction for new and
unseen sequences. The experimental results show that the proposed model
outperforms competing learning-based methods, and challenges COLMAP while
having lower runtime.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brynte_L/0/1/0/all/0/1&quot;&gt;Lucas Brynte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iglesias_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Pedro Iglesias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olsson_C/0/1/0/all/0/1&quot;&gt;Carl Olsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kahl_F/0/1/0/all/0/1&quot;&gt;Fredrik Kahl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.05073">
<title>FreeMan: Towards Benchmarking 3D Human Pose Estimation under Real-World Conditions. (arXiv:2309.05073v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.05073</link>
<description rdf:parseType="Literal">&lt;p&gt;Estimating the 3D structure of the human body from natural scenes is a
fundamental aspect of visual perception. 3D human pose estimation is a vital
step in advancing fields like AIGC and human-robot interaction, serving as a
crucial technique for understanding and interacting with human actions in
real-world settings. However, the current datasets, often collected under
single laboratory conditions using complex motion capture equipment and
unvarying backgrounds, are insufficient. The absence of datasets on variable
conditions is stalling the progress of this crucial task. To facilitate the
development of 3D pose estimation, we present FreeMan, the first large-scale,
multi-view dataset collected under the real-world conditions. FreeMan was
captured by synchronizing 8 smartphones across diverse scenarios. It comprises
11M frames from 8000 sequences, viewed from different perspectives. These
sequences cover 40 subjects across 10 different scenarios, each with varying
lighting conditions. We have also established an semi-automated pipeline
containing error detection to reduce the workload of manual check and ensure
precise annotation. We provide comprehensive evaluation baselines for a range
of tasks, underlining the significant challenges posed by FreeMan. Further
evaluations of standard indoor/outdoor human sensing datasets reveal that
FreeMan offers robust representation transferability in real and complex
scenes. Code and data will be available at
https://wangjiongw.github.io/freeman.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Fengyu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gou_W/0/1/0/all/0/1&quot;&gt;Wenbo Gou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bingliang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_D/0/1/0/all/0/1&quot;&gt;Danqi Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1&quot;&gt;Ailing Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yijun Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junle Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_Y/0/1/0/all/0/1&quot;&gt;Yanqing Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruimao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.09039">
<title>Microscale 3-D Capacitance Tomography with a CMOS Sensor Array. (arXiv:2309.09039v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.09039</link>
<description rdf:parseType="Literal">&lt;p&gt;Electrical capacitance tomography (ECT) is a nonoptical imaging technique in
which a map of the interior permittivity of a volume is estimated by making
capacitance measurements at its boundary and solving an inverse problem. While
previous ECT demonstrations have often been at centimeter scales, ECT is not
limited to macroscopic systems. In this paper, we demonstrate ECT imaging of
polymer microspheres and bacterial biofilms using a CMOS microelectrode array,
achieving spatial resolution of 10 microns. Additionally, we propose a deep
learning architecture and an improved multi-objective training scheme for
reconstructing out-of-plane permittivity maps from the sensor measurements.
Experimental results show that the proposed approach is able to resolve
microscopic 3-D structures, achieving 91.5% prediction accuracy on the
microsphere dataset and 82.7% on the biofilm dataset, including an average of
4.6% improvement over baseline computational methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdelatty_M/0/1/0/all/0/1&quot;&gt;Manar Abdelatty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Incandela_J/0/1/0/all/0/1&quot;&gt;Joseph Incandela&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1&quot;&gt;Kangping Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Larkin_J/0/1/0/all/0/1&quot;&gt;Joseph W. Larkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reda_S/0/1/0/all/0/1&quot;&gt;Sherief Reda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosenstein_J/0/1/0/all/0/1&quot;&gt;Jacob K. Rosenstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13132">
<title>Understanding Calibration of Deep Neural Networks for Medical Image Classification. (arXiv:2309.13132v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.13132</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of medical image analysis, achieving high accuracy is not
enough; ensuring well-calibrated predictions is also crucial. Confidence scores
of a deep neural network play a pivotal role in explainability by providing
insights into the model&apos;s certainty, identifying cases that require attention,
and establishing trust in its predictions. Consequently, the significance of a
well-calibrated model becomes paramount in the medical imaging domain, where
accurate and reliable predictions are of utmost importance. While there has
been a significant effort towards training modern deep neural networks to
achieve high accuracy on medical imaging tasks, model calibration and factors
that affect it remain under-explored. To address this, we conducted a
comprehensive empirical study that explores model performance and calibration
under different training regimes. We considered fully supervised training,
which is the prevailing approach in the community, as well as rotation-based
self-supervised method with and without transfer learning, across various
datasets and architecture sizes. Multiple calibration metrics were employed to
gain a holistic understanding of model calibration. Our study reveals that
factors such as weight distributions and the similarity of learned
representations correlate with the calibration trends observed in the models.
Notably, models trained using rotation-based self-supervised pretrained regime
exhibit significantly better calibration while achieving comparable or even
superior performance compared to fully supervised models across different
medical imaging datasets. These findings shed light on the importance of model
calibration in medical image analysis and highlight the benefits of
incorporating self-supervised learning approach to improve both performance and
calibration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sambyal_A/0/1/0/all/0/1&quot;&gt;Abhishek Singh Sambyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niyaz_U/0/1/0/all/0/1&quot;&gt;Usma Niyaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnan_N/0/1/0/all/0/1&quot;&gt;Narayanan C. Krishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bathula_D/0/1/0/all/0/1&quot;&gt;Deepti R. Bathula&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13550">
<title>I-AI: A Controllable &amp; Interpretable AI System for Decoding Radiologists&apos; Intense Focus for Accurate CXR Diagnoses. (arXiv:2309.13550v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.13550</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of chest X-ray (CXR) diagnosis, existing works often focus
solely on determining where a radiologist looks, typically through tasks such
as detection, segmentation, or classification. However, these approaches are
often designed as black-box models, lacking interpretability. In this paper, we
introduce Interpretable Artificial Intelligence (I-AI) a novel and unified
controllable interpretable pipeline for decoding the intense focus of
radiologists in CXR diagnosis. Our I-AI addresses three key questions: where a
radiologist looks, how long they focus on specific areas, and what findings
they diagnose. By capturing the intensity of the radiologist&apos;s gaze, we provide
a unified solution that offers insights into the cognitive process underlying
radiological interpretation. Unlike current methods that rely on black-box
machine learning models, which can be prone to extracting erroneous information
from the entire input image during the diagnosis process, we tackle this issue
by effectively masking out irrelevant information. Our proposed I-AI leverages
a vision-language model, allowing for precise control over the interpretation
process while ensuring the exclusion of irrelevant features. To train our I-AI
model, we utilize an eye gaze dataset to extract anatomical gaze information
and generate ground truth heatmaps. Through extensive experimentation, we
demonstrate the efficacy of our method. We showcase that the attention
heatmaps, designed to mimic radiologists&apos; focus, encode sufficient and relevant
information, enabling accurate classification tasks using only a portion of
CXR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1&quot;&gt;Trong Thang Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brecheisen_J/0/1/0/all/0/1&quot;&gt;Jacob Brecheisen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1&quot;&gt;Anh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1&quot;&gt;Hien Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1&quot;&gt;Ngan Le&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13925">
<title>Towards Surveillance Video-and-Language Understanding: New Dataset, Baselines, and Challenges. (arXiv:2309.13925v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.13925</link>
<description rdf:parseType="Literal">&lt;p&gt;Surveillance videos are an essential component of daily life with various
critical applications, particularly in public security. However, current
surveillance video tasks mainly focus on classifying and localizing anomalous
events. Existing methods are limited to detecting and classifying the
predefined events with unsatisfactory semantic understanding, although they
have obtained considerable performance. To address this issue, we propose a new
research direction of surveillance video-and-language understanding, and
construct the first multimodal surveillance video dataset. We manually annotate
the real-world surveillance dataset UCF-Crime with fine-grained event content
and timing. Our newly annotated dataset, UCA (UCF-Crime Annotation), contains
23,542 sentences, with an average length of 20 words, and its annotated videos
are as long as 110.7 hours. Furthermore, we benchmark SOTA models for four
multimodal tasks on this newly created dataset, which serve as new baselines
for surveillance video-and-language understanding. Through our experiments, we
find that mainstream models used in previously publicly available datasets
perform poorly on surveillance video, which demonstrates the new challenges in
surveillance video-and-language understanding. To validate the effectiveness of
our UCA, we conducted experiments on multimodal anomaly detection. The results
demonstrate that our multimodal surveillance learning can improve the
performance of conventional anomaly detection tasks. All the experiments
highlight the necessity of constructing this dataset to advance surveillance
AI. The link to our dataset is provided at:
https://xuange923.github.io/Surveillance-Video-Understanding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_T/0/1/0/all/0/1&quot;&gt;Tongtong Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xuange Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1&quot;&gt;Jian Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_Z/0/1/0/all/0/1&quot;&gt;Zhenzhen Jiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16849">
<title>Space-Time Attention with Shifted Non-Local Search. (arXiv:2309.16849v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.16849</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficiently computing attention maps for videos is challenging due to the
motion of objects between frames. While a standard non-local search is
high-quality for a window surrounding each query point, the window&apos;s small size
cannot accommodate motion. Methods for long-range motion use an auxiliary
network to predict the most similar key coordinates as offsets from each query
location. However, accurately predicting this flow field of offsets remains
challenging, even for large-scale networks. Small spatial inaccuracies
significantly impact the attention module&apos;s quality. This paper proposes a
search strategy that combines the quality of a non-local search with the range
of predicted offsets. The method, named Shifted Non-Local Search, executes a
small grid search surrounding the predicted offsets to correct small spatial
errors. Our method&apos;s in-place computation consumes 10 times less memory and is
over 3 times faster than previous work. Experimentally, correcting the small
spatial errors improves the video frame alignment quality by over 3 dB PSNR.
Our search upgrades existing space-time attention modules, which improves video
denoising results by 0.30 dB PSNR for a 7.5% increase in overall runtime. We
integrate our space-time attention module into a UNet-like architecture to
achieve state-of-the-art results on video denoising.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gauen_K/0/1/0/all/0/1&quot;&gt;Kent Gauen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1&quot;&gt;Stanley Chan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00119">
<title>Fewshot learning on global multimodal embeddings for earth observation tasks. (arXiv:2310.00119v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00119</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we pretrain a CLIP/ViT based model using three different
modalities of satellite imagery across five AOIs covering over ~10\% of Earth&apos;s
total landmass, namely Sentinel 2 RGB optical imagery, Sentinel 1 SAR radar
amplitude and interferometric coherence. This model uses $\sim 250$ M
parameters. Then, we use the embeddings produced for each modality with a
classical machine learning method to attempt different downstream tasks for
earth observation related to vegetation, built up surface, croplands and
permanent water. We consistently show how we reduce the need for labeled data
by 99\%, so that with ~200-500 randomly selected labeled examples (around
4K-10K km$^2$) we reach performance levels analogous to those achieved with the
full labeled datasets (about 150K image chips or 3M km$^2$ in each area of
interest - AOI) on all modalities, AOIs and downstream tasks. This leads us to
think that the model has captured significant earth features useful in a wide
variety of scenarios. To enhance our model&apos;s usability in practice, its
architecture allows inference in contexts with missing modalities and even
missing channels within each modality. Additionally, we visually show that this
embedding space, obtained with no labels, is sensible to the different earth
features represented by the labelled datasets we selected.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allen_M/0/1/0/all/0/1&quot;&gt;Matt Allen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dorr_F/0/1/0/all/0/1&quot;&gt;Francisco Dorr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gallego_Mejia_J/0/1/0/all/0/1&quot;&gt;Joseph A. Gallego-Mejia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_Ferrer_L/0/1/0/all/0/1&quot;&gt;Laura Mart&amp;#xed;nez-Ferrer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jungbluth_A/0/1/0/all/0/1&quot;&gt;Anna Jungbluth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalaitzis_F/0/1/0/all/0/1&quot;&gt;Freddie Kalaitzis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramos_Pollan_R/0/1/0/all/0/1&quot;&gt;Ra&amp;#xfa;l Ramos-Poll&amp;#xe1;n&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00826">
<title>Large Scale Masked Autoencoding for Reducing Label Requirements on SAR Data. (arXiv:2310.00826v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00826</link>
<description rdf:parseType="Literal">&lt;p&gt;Satellite-based remote sensing is instrumental in the monitoring and
mitigation of the effects of anthropogenic climate change. Large scale, high
resolution data derived from these sensors can be used to inform intervention
and policy decision making, but the timeliness and accuracy of these
interventions is limited by use of optical data, which cannot operate at night
and is affected by adverse weather conditions. Synthetic Aperture Radar (SAR)
offers a robust alternative to optical data, but its associated complexities
limit the scope of labelled data generation for traditional deep learning. In
this work, we apply a self-supervised pretraining scheme, masked autoencoding,
to SAR amplitude data covering 8.7\% of the Earth&apos;s land surface area, and tune
the pretrained weights on two downstream tasks crucial to monitoring climate
change - vegetation cover prediction and land cover classification. We show
that the use of this pretraining scheme reduces labelling requirements for the
downstream tasks by more than an order of magnitude, and that this pretraining
generalises geographically, with the performance gain increasing when tuned
downstream on regions outside the pretraining set. Our findings significantly
advance climate change mitigation by facilitating the development of task and
region-specific SAR models, allowing local communities and organizations to
deploy tailored solutions for rapid, accurate monitoring of climate change
effects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allen_M/0/1/0/all/0/1&quot;&gt;Matt Allen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dorr_F/0/1/0/all/0/1&quot;&gt;Francisco Dorr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gallego_Mejia_J/0/1/0/all/0/1&quot;&gt;Joseph A. Gallego-Mejia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_Ferrer_L/0/1/0/all/0/1&quot;&gt;Laura Mart&amp;#xed;nez-Ferrer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jungbluth_A/0/1/0/all/0/1&quot;&gt;Anna Jungbluth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalaitzis_F/0/1/0/all/0/1&quot;&gt;Freddie Kalaitzis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramos_Pollan_R/0/1/0/all/0/1&quot;&gt;Ra&amp;#xfa;l Ramos-Poll&amp;#xe1;n&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01596">
<title>ImagenHub: Standardizing the evaluation of conditional image generation models. (arXiv:2310.01596v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01596</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, a myriad of conditional image generation and editing models have
been developed to serve different downstream tasks, including text-to-image
generation, text-guided image editing, subject-driven image generation,
control-guided image generation, etc. However, we observe huge inconsistencies
in experimental conditions: datasets, inference, and evaluation metrics -
render fair comparisons difficult. This paper proposes ImagenHub, which is a
one-stop library to standardize the inference and evaluation of all the
conditional image generation models. Firstly, we define seven prominent tasks
and curate high-quality evaluation datasets for them. Secondly, we built a
unified inference pipeline to ensure fair comparison. Thirdly, we design two
human evaluation scores, i.e. Semantic Consistency and Perceptual Quality,
along with comprehensive guidelines to evaluate generated images. We train
expert raters to evaluate the model outputs based on the proposed metrics. Our
human evaluation achieves a high inter-worker agreement of Krippendorff&apos;s alpha
on 76% models with a value higher than 0.4. We comprehensively evaluated a
total of around 30 models and observed three key takeaways: (1) the existing
models&apos; performance is generally unsatisfying except for Text-guided Image
Generation and Subject-driven Image Generation, with 74% models achieving an
overall score lower than 0.5. (2) we examined the claims from published papers
and found 83% of them hold with a few exceptions. (3) None of the existing
automatic metrics has a Spearman&apos;s correlation higher than 0.2 except
subject-driven image generation. Moving forward, we will continue our efforts
to evaluate newly published models and update our leaderboard to keep track of
the progress in conditional image generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ku_M/0/1/0/all/0/1&quot;&gt;Max Ku&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tianle Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yujie Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1&quot;&gt;Xingyu Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_W/0/1/0/all/0/1&quot;&gt;Wenwen Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenhu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01779">
<title>HallE-Switch: Controlling Object Hallucination in Large Vision Language Models. (arXiv:2310.01779v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01779</link>
<description rdf:parseType="Literal">&lt;p&gt;Current large vision-language models (LVLMs) achieve remarkable progress, yet
there remains significant uncertainty regarding their ability to accurately
apprehend visual details, that is, in performing detailed captioning. To
address this, we introduce $\textit{CCEval}$, a GPT-4 assisted evaluation
method tailored for detailed captioning. Interestingly, while LVLMs demonstrate
minimal object existence hallucination in existing VQA benchmarks, our proposed
evaluation reveals continued susceptibility to such hallucinations. In this
paper, we make the first attempt to investigate such hallucination from
different aspects, including image resolution, the language decoder size, and
instruction data amount, quality, granularity. Our findings underscore the
unwarranted inference when the language description includes details at a finer
object granularity than what the vision module can ground or verify, thus
inducing hallucination. To control such hallucinations, we further attribute
the reliability of captioning to contextual knowledge (involving only
contextually grounded objects) and parametric knowledge (containing inferred
objects by the model). Thus, we introduce $\textit{HallE-Switch}$, a
controllable LVLM in terms of $\textbf{Hall}$ucination in object
$\textbf{E}$xistence. HallE-Switch can condition the captioning to shift
between (i) exclusively depicting contextual knowledge for grounded objects and
(ii) blending it with parametric knowledge to imagine inferred objects. Our
method reduces hallucination by 44% compared to LLaVA$_{7B}$ and maintains the
same object coverage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_B/0/1/0/all/0/1&quot;&gt;Bohan Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shijia Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chenfeng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1&quot;&gt;Sheng Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1&quot;&gt;Kurt Keutzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Manling Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02048">
<title>Exploring Generalisability of Self-Distillation with No Labels for SAR-Based Vegetation Prediction. (arXiv:2310.02048v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02048</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we pre-train a DINO-ViT based model using two Synthetic Aperture
Radar datasets (S1GRD or GSSIC) across three regions (China, Conus, Europe). We
fine-tune the models on smaller labeled datasets to predict vegetation
percentage, and empirically study the connection between the embedding space of
the models and their ability to generalize across diverse geographic regions
and to unseen data. For S1GRD, embedding spaces of different regions are
clearly separated, while GSSIC&apos;s overlaps. Positional patterns remain during
fine-tuning, and greater distances in embeddings often result in higher errors
for unfamiliar regions. With this, our work increases our understanding of
generalizability for self-supervised models applied to remote sensing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_Ferrer_L/0/1/0/all/0/1&quot;&gt;Laura Mart&amp;#xed;nez-Ferrer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jungbluth_A/0/1/0/all/0/1&quot;&gt;Anna Jungbluth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gallego_Mejia_J/0/1/0/all/0/1&quot;&gt;Joseph A. Gallego-Mejia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allen_M/0/1/0/all/0/1&quot;&gt;Matt Allen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dorr_F/0/1/0/all/0/1&quot;&gt;Francisco Dorr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalaitzis_F/0/1/0/all/0/1&quot;&gt;Freddie Kalaitzis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramos_Pollan_R/0/1/0/all/0/1&quot;&gt;Ra&amp;#xfa;l Ramos-Poll&amp;#xe1;n&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03513">
<title>Exploring DINO: Emergent Properties and Limitations for Synthetic Aperture Radar Imagery. (arXiv:2310.03513v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03513</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised learning (SSL) models have recently demonstrated remarkable
performance across various tasks, including image segmentation. This study
delves into the emergent characteristics of the Self-Distillation with No
Labels (DINO) algorithm and its application to Synthetic Aperture Radar (SAR)
imagery. We pre-train a vision transformer (ViT)-based DINO model using
unlabeled SAR data, and later fine-tune the model to predict high-resolution
land cover maps. We rigorously evaluate the utility of attention maps generated
by the ViT backbone and compare them with the model&apos;s token embedding space. We
observe a small improvement in model performance with pre-training compared to
training from scratch and discuss the limitations and opportunities of SSL for
remote sensing and land cover segmentation. Beyond small performance increases,
we show that ViT attention maps hold great intrinsic value for remote sensing,
and could provide useful inputs to other algorithms. With this, our work lays
the groundwork for bigger and better SSL models for Earth Observation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gallego_Mejia_J/0/1/0/all/0/1&quot;&gt;Joseph A. Gallego-Mejia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jungbluth_A/0/1/0/all/0/1&quot;&gt;Anna Jungbluth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_Ferrer_L/0/1/0/all/0/1&quot;&gt;Laura Mart&amp;#xed;nez-Ferrer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allen_M/0/1/0/all/0/1&quot;&gt;Matt Allen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dorr_F/0/1/0/all/0/1&quot;&gt;Francisco Dorr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalaitzis_F/0/1/0/all/0/1&quot;&gt;Freddie Kalaitzis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramos_Pollan_R/0/1/0/all/0/1&quot;&gt;Ra&amp;#xfa;l Ramos-Poll&amp;#xe1;n&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.09718">
<title>Efficient and Effective Deep Multi-view Subspace Clustering. (arXiv:2310.09718v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.09718</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent multi-view subspace clustering achieves impressive results utilizing
deep networks, where the self-expressive correlation is typically modeled by a
fully connected (FC) layer. However, they still suffer from two limitations. i)
The parameter scale of the FC layer is quadratic to sample numbers, resulting
in high time and memory costs that significantly degrade their feasibility in
large-scale datasets. ii) It is under-explored to extract a unified
representation that simultaneously satisfies minimal sufficiency and
discriminability. To this end, we propose a novel deep framework, termed
Efficient and Effective deep Multi-View Subspace Clustering (E$^2$MVSC).
Instead of a parameterized FC layer, we design a Relation-Metric Net that
decouples network parameter scale from sample numbers for greater computational
efficiency. Most importantly, the proposed method devises a multi-type
auto-encoder to explicitly decouple consistent, complementary, and superfluous
information from every view, which is supervised by a soft clustering
assignment similarity constraint. Following information bottleneck theory and
the maximal coding rate reduction principle, a sufficient yet minimal unified
representation can be obtained, as well as pursuing intra-cluster aggregation
and inter-cluster separability within it. Extensive experiments show that
E$^2$MVSC yields comparable results to existing methods and achieves
state-of-the-art performance in various types of multi-view datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yuxiu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ren Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qiang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Caiming Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.09909">
<title>Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for Multimodal Medical Diagnosis. (arXiv:2310.09909v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.09909</link>
<description rdf:parseType="Literal">&lt;p&gt;Driven by the large foundation models, the development of artificial
intelligence has witnessed tremendous progress lately, leading to a surge of
general interest from the public. In this study, we aim to assess the
performance of OpenAI&apos;s newest model, GPT-4V(ision), specifically in the realm
of multimodal medical diagnosis. Our evaluation encompasses 17 human body
systems, including Central Nervous System, Head and Neck, Cardiac, Chest,
Hematology, Hepatobiliary, Gastrointestinal, Urogenital, Gynecology,
Obstetrics, Breast, Musculoskeletal, Spine, Vascular, Oncology, Trauma,
Pediatrics, with images taken from 8 modalities used in daily clinic routine,
e.g., X-ray, Computed Tomography (CT), Magnetic Resonance Imaging (MRI),
Positron Emission Tomography (PET), Digital Subtraction Angiography (DSA),
Mammography, Ultrasound, and Pathology. We probe the GPT-4V&apos;s ability on
multiple clinical tasks with or without patent history provided, including
imaging modality and anatomy recognition, disease diagnosis, report generation,
disease localisation.
&lt;/p&gt;
&lt;p&gt;Our observation shows that, while GPT-4V demonstrates proficiency in
distinguishing between medical image modalities and anatomy, it faces
significant challenges in disease diagnosis and generating comprehensive
reports. These findings underscore that while large multimodal models have made
significant advancements in computer vision and natural language processing, it
remains far from being used to effectively support real-world medical
applications and clinical decision-making.
&lt;/p&gt;
&lt;p&gt;All images used in this report can be found in
https://github.com/chaoyi-wu/GPT-4V_Medical_Evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chaoyi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1&quot;&gt;Jiayu Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1&quot;&gt;Qiaoyu Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Weike Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Weixiong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoman Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xiao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Ziheng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Ya Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanfeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1&quot;&gt;Weidi Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.13135">
<title>LeTFuser: Light-weight End-to-end Transformer-Based Sensor Fusion for Autonomous Driving with Multi-Task Learning. (arXiv:2310.13135v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.13135</link>
<description rdf:parseType="Literal">&lt;p&gt;In end-to-end autonomous driving, the utilization of existing sensor fusion
techniques and navigational control methods for imitation learning proves
inadequate in challenging situations that involve numerous dynamic agents. To
address this issue, we introduce LeTFuser, a lightweight transformer-based
algorithm for fusing multiple RGB-D camera representations. To perform
perception and control tasks simultaneously, we utilize multi-task learning.
Our model comprises of two modules, the first being the perception module that
is responsible for encoding the observation data obtained from the RGB-D
cameras. Our approach employs the Convolutional vision Transformer (CvT)
\cite{wu2021cvt} to better extract and fuse features from multiple RGB cameras
due to local and global feature extraction capability of convolution and
transformer modules, respectively. Encoded features combined with static and
dynamic environments are later employed by our control module to predict
waypoints and vehicular controls (e.g. steering, throttle, and brake). We use
two methods to generate the vehicular controls levels. The first method uses a
PID algorithm to follow the waypoints on the fly, whereas the second one
directly predicts the control policy using the measurement features and
environmental state. We evaluate the model and conduct a comparative analysis
with recent models on the CARLA simulator using various scenarios, ranging from
normal to adversarial conditions, to simulate real-world scenarios. Our method
demonstrated better or comparable results with respect to our baselines in term
of driving abilities. The code is available at
\url{https://github.com/pagand/e2etransfuser/tree/cvpr-w} to facilitate future
studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agand_P/0/1/0/all/0/1&quot;&gt;Pedram Agand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahdavian_M/0/1/0/all/0/1&quot;&gt;Mohammad Mahdavian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savva_M/0/1/0/all/0/1&quot;&gt;Manolis Savva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mo Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15020">
<title>Invariance is Key to Generalization: Examining the Role of Representation in Sim-to-Real Transfer for Visual Navigation. (arXiv:2310.15020v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15020</link>
<description rdf:parseType="Literal">&lt;p&gt;The data-driven approach to robot control has been gathering pace rapidly,
yet generalization to unseen task domains remains a critical challenge. We
argue that the key to generalization is representations that are (i) rich
enough to capture all task-relevant information and (ii) invariant to
superfluous variability between the training and the test domains. We
experimentally study such a representation -- containing both depth and
semantic information -- for visual navigation and show that it enables a
control policy trained entirely in simulated indoor scenes to generalize to
diverse real-world environments, both indoors and outdoors. Further, we show
that our representation reduces the A-distance between the training and test
domains, improving the generalization error bound as a result. Our proposed
approach is scalable: the learned policy improves continuously, as the
foundation models that it exploits absorb more diverse data during
pre-training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ai_B/0/1/0/all/0/1&quot;&gt;Bo Ai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhanxin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_D/0/1/0/all/0/1&quot;&gt;David Hsu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16267">
<title>Student Classroom Behavior Detection based on Spatio-Temporal Network and Multi-Model Fusion. (arXiv:2310.16267v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.16267</link>
<description rdf:parseType="Literal">&lt;p&gt;Using deep learning methods to detect students&apos; classroom behavior
automatically is a promising approach for analyzing their class performance and
improving teaching effectiveness. However, the lack of publicly available
spatio-temporal datasets on student behavior, as well as the high cost of
manually labeling such datasets, pose significant challenges for researchers in
this field. To address this issue, we proposed a method for extending the
spatio-temporal behavior dataset in Student Classroom Scenarios
(SCB-ST-Dataset4) through image dataset. Our SCB-ST-Dataset4 comprises 757265
images with 25810 labels, focusing on 3 behaviors: hand-raising, reading,
writing. Our proposed method can rapidly generate spatio-temporal behavior
datasets without requiring extra manual labeling. Furthermore, we proposed a
Behavior Similarity Index (BSI) to explore the similarity of behaviors. We
evaluated the dataset using the YOLOv5, YOLOv7, YOLOv8, and SlowFast
algorithms, achieving a mean average precision (map) of up to 82.3%. Last, we
fused multiple models to generate student behavior-related data from various
perspectives. The experiment further demonstrates the effectiveness of our
method. And SCB-ST-Dataset4 provides a robust foundation for future research in
student behavior detection, potentially contributing to advancements in this
field. The SCB-ST-Dataset4 is available for download at:
https://github.com/Whiffe/SCB-dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Fan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaofei Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18116">
<title>Direct Unsupervised Denoising. (arXiv:2310.18116v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18116</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional supervised denoisers are trained using pairs of noisy input and
clean target images. They learn to predict a central tendency of the posterior
distribution over possible clean images. When, e.g., trained with the popular
quadratic loss function, the network&apos;s output will correspond to the minimum
mean square error (MMSE) estimate. Unsupervised denoisers based on Variational
AutoEncoders (VAEs) have succeeded in achieving state-of-the-art results while
requiring only unpaired noisy data as training input. In contrast to the
traditional supervised approach, unsupervised denoisers do not directly produce
a single prediction, such as the MMSE estimate, but allow us to draw samples
from the posterior distribution of clean solutions corresponding to the noisy
input. To approximate the MMSE estimate during inference, unsupervised methods
have to create and draw a large number of samples - a computationally expensive
process - rendering the approach inapplicable in many situations. Here, we
present an alternative approach that trains a deterministic network alongside
the VAE to directly predict a central tendency. Our method achieves results
that surpass the results achieved by the unsupervised method at a fraction of
the computational cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salmon_B/0/1/0/all/0/1&quot;&gt;Benjamin Salmon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krull_A/0/1/0/all/0/1&quot;&gt;Alexander Krull&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18961">
<title>AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection. (arXiv:2310.18961v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18961</link>
<description rdf:parseType="Literal">&lt;p&gt;Zero-shot anomaly detection (ZSAD) requires detection models trained using
auxiliary data to detect anomalies without any training sample in a target
dataset. It is a crucial task when training data is not accessible due to
various concerns, \eg, data privacy, yet it is challenging since the models
need to generalize to anomalies across different domains where the appearance
of foreground objects, abnormal regions, and background features, such as
defects/tumors on different products/organs, can vary significantly. Recently
large pre-trained vision-language models (VLMs), such as CLIP, have
demonstrated strong zero-shot recognition ability in various vision tasks,
including anomaly detection. However, their ZSAD performance is weak since the
VLMs focus more on modeling the class semantics of the foreground objects
rather than the abnormality/normality in the images. In this paper we introduce
a novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD across
different domains. The key insight of AnomalyCLIP is to learn object-agnostic
text prompts that capture generic normality and abnormality in an image
regardless of its foreground objects. This allows our model to focus on the
abnormal image regions rather than the object semantics, enabling generalized
normality and abnormality recognition on diverse types of objects. Large-scale
experiments on 17 real-world anomaly detection datasets show that AnomalyCLIP
achieves superior zero-shot performance of detecting and segmenting anomalies
in datasets of highly diverse class semantics from various defect inspection
and medical imaging domains. Code will be made available at
https://github.com/zqhang/AnomalyCLIP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1&quot;&gt;Qihang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1&quot;&gt;Guansong Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yu Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1&quot;&gt;Shibo He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiming Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19112">
<title>Efficient IoT Inference via Context-Awareness. (arXiv:2310.19112v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19112</link>
<description rdf:parseType="Literal">&lt;p&gt;While existing strategies to execute deep learning-based classification on
low-power platforms assume the models are trained on all classes of interest,
this paper posits that adopting context-awareness i.e. narrowing down a
classification task to the current deployment context consisting of only recent
inference queries can substantially enhance performance in resource-constrained
environments. We propose a new paradigm, CACTUS, for scalable and efficient
context-aware classification where a micro-classifier recognizes a small set of
classes relevant to the current context and, when context change happens (e.g.,
a new class comes into the scene), rapidly switches to another suitable
micro-classifier. CACTUS features several innovations, including optimizing the
training cost of context-aware classifiers, enabling on-the-fly context-aware
switching between classifiers, and balancing context switching costs and
performance gains via simple yet effective switching policies. We show that
CACTUS achieves significant benefits in accuracy, latency, and compute budget
across a range of datasets and IoT platforms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rastikerdar_M/0/1/0/all/0/1&quot;&gt;Mohammad Mehdi Rastikerdar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1&quot;&gt;Shiwei Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_H/0/1/0/all/0/1&quot;&gt;Hui Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganesan_D/0/1/0/all/0/1&quot;&gt;Deepak Ganesan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00690">
<title>What User Behaviors Make the Differences During the Process of Visual Analytics?. (arXiv:2311.00690v3 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00690</link>
<description rdf:parseType="Literal">&lt;p&gt;The understanding of visual analytics process can benefit visualization
researchers from multiple aspects, including improving visual designs and
developing advanced interaction functions. However, the log files of user
behaviors are still hard to analyze due to the complexity of sensemaking and
our lack of knowledge on the related user behaviors. This work presents a study
on a comprehensive data collection of user behaviors, and our analysis approach
with time-series classification methods. We have chosen a classical
visualization application, Covid-19 data analysis, with common analysis tasks
covering geo-spatial, time-series and multi-attributes. Our user study collects
user behaviors on a diverse set of visualization tasks with two comparable
systems, desktop and immersive visualizations. We summarize the classification
results with three time-series machine learning algorithms at two scales, and
explore the influences of behavior features. Our results reveal that user
behaviors can be distinguished during the process of visual analytics and there
is a potentially strong association between the physical behaviors of users and
the visualization tasks they perform. We also demonstrate the usage of our
models by interpreting open sessions of visual analytics, which provides an
automatic way to study sensemaking without tedious manual annotations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zekun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doroudian_S/0/1/0/all/0/1&quot;&gt;Shahin Doroudian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_A/0/1/0/all/0/1&quot;&gt;Aidong Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01619">
<title>InsPLAD: A Dataset and Benchmark for Power Line Asset Inspection in UAV Images. (arXiv:2311.01619v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01619</link>
<description rdf:parseType="Literal">&lt;p&gt;Power line maintenance and inspection are essential to avoid power supply
interruptions, reducing its high social and financial impacts yearly.
Automating power line visual inspections remains a relevant open problem for
the industry due to the lack of public real-world datasets of power line
components and their various defects to foster new research. This paper
introduces InsPLAD, a Power Line Asset Inspection Dataset and Benchmark
containing 10,607 high-resolution Unmanned Aerial Vehicles colour images. The
dataset contains seventeen unique power line assets captured from real-world
operating power lines. Additionally, five of those assets present six defects:
four of which are corrosion, one is a broken component, and one is a bird&apos;s
nest presence. All assets were labelled according to their condition, whether
normal or the defect name found on an image level. We thoroughly evaluate
state-of-the-art and popular methods for three image-level computer vision
tasks covered by InsPLAD: object detection, through the AP metric; defect
classification, through Balanced Accuracy; and anomaly detection, through the
AUROC metric. InsPLAD offers various vision challenges from uncontrolled
environments, such as multi-scale objects, multi-size class instances, multiple
objects per image, intra-class variation, cluttered background, distinct
point-of-views, perspective distortion, occlusion, and varied lighting
conditions. To the best of our knowledge, InsPLAD is the first large real-world
dataset and benchmark for power line asset inspection with multiple components
and defects for various computer vision tasks, with a potential impact to
improve state-of-the-art methods in the field. It will be publicly available in
its integrity on a repository with a thorough description. It can be found at
https://github.com/andreluizbvs/InsPLAD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Luiz Buarque Vieira e Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Felix_H/0/1/0/all/0/1&quot;&gt;Heitor de Castro Felix&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simoes_F/0/1/0/all/0/1&quot;&gt;Franscisco Paulo Magalh&amp;#xe3;es Sim&amp;#xf5;es&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teichrieb_V/0/1/0/all/0/1&quot;&gt;Veronica Teichrieb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_M/0/1/0/all/0/1&quot;&gt;Michel Mozinho dos Santos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santiago_H/0/1/0/all/0/1&quot;&gt;Hemir Santiago&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sgotti_V/0/1/0/all/0/1&quot;&gt;Virginia Sgotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neto_H/0/1/0/all/0/1&quot;&gt;Henrique Lott Neto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08239">
<title>Learning Physics-Inspired Regularization for Medical Image Registration with Hypernetworks. (arXiv:2311.08239v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.08239</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical image registration aims at identifying the spatial deformation
between images of the same anatomical region and is fundamental to image-based
diagnostics and therapy. To date, the majority of the deep learning-based
registration methods employ regularizers that enforce global spatial
smoothness, e.g., the diffusion regularizer. However, such regularizers are not
tailored to the data and might not be capable of reflecting the complex
underlying deformation. In contrast, physics-inspired regularizers promote
physically plausible deformations. One such regularizer is the linear elastic
regularizer which models the deformation of elastic material. These
regularizers are driven by parameters that define the material&apos;s physical
properties. For biological tissue, a wide range of estimations of such
parameters can be found in the literature and it remains an open challenge to
identify suitable parameter values for successful registration. To overcome
this problem and to incorporate physical properties into learning-based
registration, we propose to use a hypernetwork that learns the effect of the
physical parameters of a physics-inspired regularizer on the resulting spatial
deformation field. In particular, we adapt the HyperMorph framework to learn
the effect of the two elasticity parameters of the linear elastic regularizer.
Our approach enables the efficient discovery of suitable, data-specific
physical parameters at test time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Reithmeir_A/0/1/0/all/0/1&quot;&gt;Anna Reithmeir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schnabel_J/0/1/0/all/0/1&quot;&gt;Julia A. Schnabel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zimmer_V/0/1/0/all/0/1&quot;&gt;Veronika A. Zimmer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08843">
<title>Personalized Video Relighting Using Casual Light Stage. (arXiv:2311.08843v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.08843</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we develop a personalized video relighting algorithm that
produces high-quality and temporally consistent relit video under any pose,
expression, and lighting conditions in real-time. Existing relighting
algorithms typically rely either on publicly available synthetic data, which
yields poor relighting results, or instead on Light Stage data which is
inaccessible and is not publicly available. We show that by casually capturing
video of a user watching YouTube videos on a monitor we can train a
personalized algorithm capable of producing high-quality relighting under any
condition. Our key contribution is a novel neural relighting architecture that
effectively separates the intrinsic appearance features, geometry and
reflectance, from the source lighting and then combines it with the target
lighting to generate a relit image. This neural architecture enables smoothing
of intrinsic appearance features leading to temporally stable video relighting.
Both qualitative and quantitative evaluations show that our relighting
architecture improves portrait image relighting quality and temporal
consistency over state-of-the-art approaches on both casually captured Light
Stage at Your Desk (LSYD) data and Light Stage captured One Light At a Time
(OLAT) datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jun Myeong Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christman_M/0/1/0/all/0/1&quot;&gt;Max Christman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sengupta_R/0/1/0/all/0/1&quot;&gt;Roni Sengupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11284">
<title>LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching. (arXiv:2311.11284v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11284</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent advancements in text-to-3D generation mark a significant milestone
in generative models, unlocking new possibilities for creating imaginative 3D
assets across various real-world scenarios. While recent advancements in
text-to-3D generation have shown promise, they often fall short in rendering
detailed and high-quality 3D models. This problem is especially prevalent as
many methods base themselves on Score Distillation Sampling (SDS). This paper
identifies a notable deficiency in SDS, that it brings inconsistent and
low-quality updating direction for the 3D model, causing the over-smoothing
effect. To address this, we propose a novel approach called Interval Score
Matching (ISM). ISM employs deterministic diffusing trajectories and utilizes
interval-based score matching to counteract over-smoothing. Furthermore, we
incorporate 3D Gaussian Splatting into our text-to-3D generation pipeline.
Extensive experiments show that our model largely outperforms the
state-of-the-art in quality and training efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yixun Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jiantao Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haodong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaogang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yingcong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11629">
<title>Generating Realistic Counterfactuals for Retinal Fundus and OCT Images using Diffusion Models. (arXiv:2311.11629v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11629</link>
<description rdf:parseType="Literal">&lt;p&gt;Counterfactual reasoning is often used in clinical settings to explain
decisions or weigh alternatives. Therefore, for imaging based specialties such
as ophthalmology, it would be beneficial to be able to create counterfactual
images, illustrating answers to questions like &quot;If the subject had had diabetic
retinopathy, how would the fundus image have looked?&quot;. Here, we demonstrate
that using a diffusion model in combination with an adversarially robust
classifier trained on retinal disease classification tasks enables the
generation of highly realistic counterfactuals of retinal fundus images and
optical coherence tomography (OCT) B-scans. The key to the realism of
counterfactuals is that these classifiers encode salient features indicative
for each disease class and can steer the diffusion model to depict disease
signs or remove disease-related lesions in a realistic way. In a user study,
domain experts also found the counterfactuals generated using our method
significantly more realistic than counterfactuals generated from a previous
method, and even indistinguishable from real images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilanchezian_I/0/1/0/all/0/1&quot;&gt;Indu Ilanchezian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boreiko_V/0/1/0/all/0/1&quot;&gt;Valentyn Boreiko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuhlewein_L/0/1/0/all/0/1&quot;&gt;Laura K&amp;#xfc;hlewein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Ziwei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayhan_M/0/1/0/all/0/1&quot;&gt;Murat Se&amp;#xe7;kin Ayhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1&quot;&gt;Matthias Hein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koch_L/0/1/0/all/0/1&quot;&gt;Lisa Koch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berens_P/0/1/0/all/0/1&quot;&gt;Philipp Berens&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12553">
<title>HoVer-UNet: Accelerating HoVerNet with UNet-based multi-class nuclei segmentation via knowledge distillation. (arXiv:2311.12553v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12553</link>
<description rdf:parseType="Literal">&lt;p&gt;We present HoVer-UNet, an approach to distill the knowledge of the
multi-branch HoVerNet framework for nuclei instance segmentation and
classification in histopathology. We propose a compact, streamlined single UNet
network with a Mix Vision Transformer backbone, and equip it with a custom loss
function to optimally encode the distilled knowledge of HoVerNet, reducing
computational requirements without compromising performances. We show that our
model achieved results comparable to HoVerNet on the public PanNuke and Consep
datasets with a three-fold reduction in inference time. We make the code of our
model publicly available at https://github.com/DIAGNijmegen/HoVer-UNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tommasino_C/0/1/0/all/0/1&quot;&gt;Cristian Tommasino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Russo_C/0/1/0/all/0/1&quot;&gt;Cristiano Russo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rinaldi_A/0/1/0/all/0/1&quot;&gt;Antonio Maria Rinaldi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ciompi_F/0/1/0/all/0/1&quot;&gt;Francesco Ciompi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12589">
<title>Improving Source-Free Target Adaptation with Vision Transformers Leveraging Domain Representation Images. (arXiv:2311.12589v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12589</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised Domain Adaptation (UDA) methods facilitate knowledge transfer
from a labeled source domain to an unlabeled target domain, navigating the
obstacle of domain shift. While Convolutional Neural Networks (CNNs) are a
staple in UDA, the rise of Vision Transformers (ViTs) provides new avenues for
domain generalization. This paper presents an innovative method to bolster ViT
performance in source-free target adaptation, beginning with an evaluation of
how key, query, and value elements affect ViT outcomes. Experiments indicate
that altering the key component has negligible effects on Transformer
performance. Leveraging this discovery, we introduce Domain Representation
Images (DRIs), feeding embeddings through the key element. DRIs act as
domain-specific markers, effortlessly merging with the training regimen. To
assess our method, we perform target adaptation tests on the Cross Instance DRI
source-only (SO) control. We measure the efficacy of target adaptation with and
without DRIs, against existing benchmarks like SHOT-B* and adaptations via
CDTrans. Findings demonstrate that excluding DRIs offers limited gains over
SHOT-B*, while their inclusion in the key segment boosts average precision
promoting superior domain generalization. This research underscores the vital
role of DRIs in enhancing ViT efficiency in UDA scenarios, setting a precedent
for further domain adaptation explorations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sawhney_G/0/1/0/all/0/1&quot;&gt;Gauransh Sawhney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dave_D/0/1/0/all/0/1&quot;&gt;Daksh Dave&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1&quot;&gt;Adeel Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jiechao Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saleem_K/0/1/0/all/0/1&quot;&gt;Khalid Saleem&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12775">
<title>SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering. (arXiv:2311.12775v3 [cs.GR] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12775</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a method to allow precise and extremely fast mesh extraction from
3D Gaussian Splatting. Gaussian Splatting has recently become very popular as
it yields realistic rendering while being significantly faster to train than
NeRFs. It is however challenging to extract a mesh from the millions of tiny 3D
gaussians as these gaussians tend to be unorganized after optimization and no
method has been proposed so far. Our first key contribution is a regularization
term that encourages the gaussians to align well with the surface of the scene.
We then introduce a method that exploits this alignment to extract a mesh from
the Gaussians using Poisson reconstruction, which is fast, scalable, and
preserves details, in contrast to the Marching Cubes algorithm usually applied
to extract meshes from Neural SDFs. Finally, we introduce an optional
refinement strategy that binds gaussians to the surface of the mesh, and
jointly optimizes these Gaussians and the mesh through Gaussian splatting
rendering. This enables easy editing, sculpting, rigging, animating,
compositing and relighting of the Gaussians using traditional softwares by
manipulating the mesh instead of the gaussians themselves. Retrieving such an
editable mesh for realistic rendering is done within minutes with our method,
compared to hours with the state-of-the-art methods on neural SDFs, while
providing a better rendering quality. Our project page is the following:
https://anttwo.github.io/sugar/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guedon_A/0/1/0/all/0/1&quot;&gt;Antoine Gu&amp;#xe9;don&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lepetit_V/0/1/0/all/0/1&quot;&gt;Vincent Lepetit&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12919">
<title>SPOT! Revisiting Video-Language Models for Event Understanding. (arXiv:2311.12919v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12919</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding videos is an important research topic for multimodal learning.
Leveraging large-scale datasets of web-crawled video-text pairs as weak
supervision has become a pre-training paradigm for learning joint
representations and showcased remarkable potential in video understanding
tasks. However, videos can be multi-event and multi-grained, while these
video-text pairs usually contain only broad-level video captions. This raises a
question: with such weak supervision, can video representation in
video-language models gain the ability to distinguish even factual
discrepancies in textual description and understand fine-grained events? To
address this, we introduce SPOT Prober, to benchmark existing video-language
models&apos;s capacities of distinguishing event-level discrepancies as an indicator
of models&apos; event understanding ability. Our approach involves extracting events
as tuples (&amp;lt;Subject, Predicate, Object, Attribute, Timestamps&amp;gt;) from videos and
generating false event tuples by manipulating tuple components systematically.
We reevaluate the existing video-language models with these positive and
negative captions and find they fail to distinguish most of the manipulated
events. Based on our findings, we propose to plug in these manipulated event
captions as hard negative samples and find them effective in enhancing models
for event understanding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Gengyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1&quot;&gt;Jinhe Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jindong Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yanyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1&quot;&gt;Volker Tresp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13398">
<title>Depth-Regularized Optimization for 3D Gaussian Splatting in Few-Shot Images. (arXiv:2311.13398v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13398</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a method to optimize Gaussian splatting with a
limited number of images while avoiding overfitting. Representing a 3D scene by
combining numerous Gaussian splats has yielded outstanding visual quality.
However, it tends to overfit the training views when only a small number of
images are available. To address this issue, we introduce a dense depth map as
a geometry guide to mitigate overfitting. We obtained the depth map using a
pre-trained monocular depth estimation model and aligning the scale and offset
using sparse COLMAP feature points. The adjusted depth aids in the color-based
optimization of 3D Gaussian splatting, mitigating floating artifacts, and
ensuring adherence to geometric constraints. We verify the proposed method on
the NeRF-LLFF dataset with varying numbers of few images. Our approach
demonstrates robust geometry compared to the original method that relies solely
on images. Project page: robot0321.github.io/DepthRegGS
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1&quot;&gt;Jaeyoung Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1&quot;&gt;Jeongtaek Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kyoung Mu Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14750">
<title>Attribute-Aware Representation Rectification for Generalized Zero-Shot Learning. (arXiv:2311.14750v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.14750</link>
<description rdf:parseType="Literal">&lt;p&gt;Generalized Zero-shot Learning (GZSL) has yielded remarkable performance by
designing a series of unbiased visual-semantics mappings, wherein, the
precision relies heavily on the completeness of extracted visual features from
both seen and unseen classes. However, as a common practice in GZSL, the
pre-trained feature extractor may easily exhibit difficulty in capturing
domain-specific traits of the downstream tasks/datasets to provide fine-grained
discriminative features, i.e., domain bias, which hinders the overall
recognition performance, especially for unseen classes. Recent studies
partially address this issue by fine-tuning feature extractors, while may
inevitably incur catastrophic forgetting and overfitting issues. In this paper,
we propose a simple yet effective Attribute-Aware Representation Rectification
framework for GZSL, dubbed $\mathbf{(AR)^{2}}$, to adaptively rectify the
feature extractor to learn novel features while keeping original valuable
features. Specifically, our method consists of two key components, i.e.,
Unseen-Aware Distillation (UAD) and Attribute-Guided Learning (AGL). During
training, UAD exploits the prior knowledge of attribute texts that are shared
by both seen/unseen classes with attention mechanisms to detect and maintain
unseen class-sensitive visual features in a targeted manner, and meanwhile, AGL
aims to steer the model to focus on valuable features and suppress them to fit
noisy elements in the seen classes by attribute-guided representation learning.
Extensive experiments on various benchmark datasets demonstrate the
effectiveness of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_Z/0/1/0/all/0/1&quot;&gt;Zhijie Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jingcai Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xiaocheng Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1&quot;&gt;Qihua Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1&quot;&gt;Kang Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenxin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1&quot;&gt;Song Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16450">
<title>Typhoon Intensity Prediction with Vision Transformer. (arXiv:2311.16450v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16450</link>
<description rdf:parseType="Literal">&lt;p&gt;Predicting typhoon intensity accurately across space and time is crucial for
issuing timely disaster warnings and facilitating emergency response. This has
vast potential for minimizing life losses and property damages as well as
reducing economic and environmental impacts. Leveraging satellite imagery for
scenario analysis is effective but also introduces additional challenges due to
the complex relations among clouds and the highly dynamic context. Existing
deep learning methods in this domain rely on convolutional neural networks
(CNNs), which suffer from limited per-layer receptive fields. This limitation
hinders their ability to capture long-range dependencies and global contextual
knowledge during inference. In response, we introduce a novel approach, namely
&quot;Typhoon Intensity Transformer&quot; (Tint), which leverages self-attention
mechanisms with global receptive fields per layer. Tint adopts a
sequence-to-sequence feature representation learning perspective. It begins by
cutting a given satellite image into a sequence of patches and recursively
employs self-attention operations to extract both local and global contextual
relations between all patch pairs simultaneously, thereby enhancing per-patch
feature representation learning. Extensive experiments on a publicly available
typhoon benchmark validate the efficacy of Tint in comparison with both
state-of-the-art deep learning and conventional meteorological methods. Our
code is available at https://github.com/chen-huanxin/Tint.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huanxin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1&quot;&gt;Pengshuai Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Huichou Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qingyao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Ruirui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiatian Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16552">
<title>HandyPriors: Physically Consistent Perception of Hand-Object Interactions with Differentiable Priors. (arXiv:2311.16552v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16552</link>
<description rdf:parseType="Literal">&lt;p&gt;Various heuristic objectives for modeling hand-object interaction have been
proposed in past work. However, due to the lack of a cohesive framework, these
objectives often possess a narrow scope of applicability and are limited by
their efficiency or accuracy. In this paper, we propose HandyPriors, a unified
and general pipeline for pose estimation in human-object interaction scenes by
leveraging recent advances in differentiable physics and rendering. Our
approach employs rendering priors to align with input images and segmentation
masks along with physics priors to mitigate penetration and relative-sliding
across frames. Furthermore, we present two alternatives for hand and object
pose estimation. The optimization-based pose estimation achieves higher
accuracy, while the filtering-based tracking, which utilizes the differentiable
priors as dynamics and observation models, executes faster. We demonstrate that
HandyPriors attains comparable or superior results in the pose estimation task,
and that the differentiable physics module can predict contact information for
pose refinement. We also show that our approach generalizes to perception
tasks, including robotic hand manipulation and human-object pose estimation in
the wild.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shutong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yi-Ling Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1&quot;&gt;Guanglei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heiden_E/0/1/0/all/0/1&quot;&gt;Eric Heiden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turpin_D/0/1/0/all/0/1&quot;&gt;Dylan Turpin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jingzhou Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1&quot;&gt;Ming Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Macklin_M/0/1/0/all/0/1&quot;&gt;Miles Macklin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1&quot;&gt;Animesh Garg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16565">
<title>DiffusionTalker: Personalization and Acceleration for Speech-Driven 3D Face Diffuser. (arXiv:2311.16565v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16565</link>
<description rdf:parseType="Literal">&lt;p&gt;Speech-driven 3D facial animation has been an attractive task in both
academia and industry. Traditional methods mostly focus on learning a
deterministic mapping from speech to animation. Recent approaches start to
consider the non-deterministic fact of speech-driven 3D face animation and
employ the diffusion model for the task. However, personalizing facial
animation and accelerating animation generation are still two major limitations
of existing diffusion-based methods. To address the above limitations, we
propose DiffusionTalker, a diffusion-based method that utilizes contrastive
learning to personalize 3D facial animation and knowledge distillation to
accelerate 3D animation generation. Specifically, to enable personalization, we
introduce a learnable talking identity to aggregate knowledge in audio
sequences. The proposed identity embeddings extract customized facial cues
across different people in a contrastive learning manner. During inference,
users can obtain personalized facial animation based on input audio, reflecting
a specific talking style. With a trained diffusion model with hundreds of
steps, we distill it into a lightweight model with 8 steps for acceleration.
Extensive experiments are conducted to demonstrate that our method outperforms
state-of-the-art methods. The code will be released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Peng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xiaobao Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1&quot;&gt;Ming Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yitong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_N/0/1/0/all/0/1&quot;&gt;Naiming Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1&quot;&gt;Xingyu Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hui Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17005">
<title>MVBench: A Comprehensive Multi-modal Video Understanding Benchmark. (arXiv:2311.17005v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17005</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rapid development of Multi-modal Large Language Models (MLLMs), a
number of diagnostic benchmarks have recently emerged to evaluate the
comprehension capabilities of these models. However, most benchmarks
predominantly assess spatial understanding in the static image tasks, while
overlooking temporal understanding in the dynamic video tasks. To alleviate
this issue, we introduce a comprehensive Multi-modal Video understanding
Benchmark, namely MVBench, which covers 20 challenging video tasks that cannot
be effectively solved with a single frame. Specifically, we first introduce a
novel static-to-dynamic method to define these temporal-related tasks. By
transforming various static tasks into dynamic ones, we enable the systematic
generation of video tasks that require a broad spectrum of temporal skills,
ranging from perception to cognition. Then, guided by the task definition, we
automatically convert public video annotations into multiple-choice QA to
evaluate each task. On one hand, such a distinct paradigm allows us to build
MVBench efficiently, without much manual intervention. On the other hand, it
guarantees evaluation fairness with ground-truth video annotations, avoiding
the biased scoring of LLMs. Moreover, we further develop a robust video MLLM
baseline, i.e., VideoChat2, by progressive multi-modal training with diverse
instruction-tuning data. The extensive results on our MVBench reveal that, the
existing MLLMs are far from satisfactory in temporal understanding, while our
VideoChat2 largely surpasses these leading models by over 15% on MVBench. All
models and data are available at https://github.com/OpenGVLab/Ask-Anything.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kunchang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yali Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yinan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yizhuo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jilan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Limin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17009">
<title>Space-Time Diffusion Features for Zero-Shot Text-Driven Motion Transfer. (arXiv:2311.17009v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17009</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a new method for text-driven motion transfer - synthesizing a
video that complies with an input text prompt describing the target objects and
scene while maintaining an input video&apos;s motion and scene layout. Prior methods
are confined to transferring motion across two subjects within the same or
closely related object categories and are applicable for limited domains (e.g.,
humans). In this work, we consider a significantly more challenging setting in
which the target and source objects differ drastically in shape and
fine-grained motion characteristics (e.g., translating a jumping dog into a
dolphin). To this end, we leverage a pre-trained and fixed text-to-video
diffusion model, which provides us with generative and motion priors. The
pillar of our method is a new space-time feature loss derived directly from the
model. This loss guides the generation process to preserve the overall motion
of the input video while complying with the target object in terms of shape and
fine-grained motion traits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yatim_D/0/1/0/all/0/1&quot;&gt;Danah Yatim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fridman_R/0/1/0/all/0/1&quot;&gt;Rafail Fridman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bar_Tal_O/0/1/0/all/0/1&quot;&gt;Omer Bar-Tal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasten_Y/0/1/0/all/0/1&quot;&gt;Yoni Kasten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dekel_T/0/1/0/all/0/1&quot;&gt;Tali Dekel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17135">
<title>TLControl: Trajectory and Language Control for Human Motion Synthesis. (arXiv:2311.17135v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17135</link>
<description rdf:parseType="Literal">&lt;p&gt;Controllable human motion synthesis is essential for applications in AR/VR,
gaming, movies, and embodied AI. Existing methods often focus solely on either
language or full trajectory control, lacking precision in synthesizing motions
aligned with user-specified trajectories, especially for multi-joint control.
To address these issues, we present TLControl, a new method for realistic human
motion synthesis, incorporating both low-level trajectory and high-level
language semantics controls. Specifically, we first train a VQ-VAE to learn a
compact latent motion space organized by body parts. We then propose a Masked
Trajectories Transformer to make coarse initial predictions of full
trajectories of joints based on the learned latent motion space, with
user-specified partial trajectories and text descriptions as conditioning.
Finally, we introduce an efficient test-time optimization to refine these
coarse predictions for accurate trajectory control. Experiments demonstrate
that TLControl outperforms the state-of-the-art in trajectory accuracy and time
efficiency, making it practical for interactive and high-quality animation
generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_W/0/1/0/all/0/1&quot;&gt;Weilin Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1&quot;&gt;Zhiyang Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Komura_T/0/1/0/all/0/1&quot;&gt;Taku Komura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenping Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayaraman_D/0/1/0/all/0/1&quot;&gt;Dinesh Jayaraman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lingjie Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17245">
<title>LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS. (arXiv:2311.17245v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17245</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in real-time neural rendering using point-based
techniques have paved the way for the widespread adoption of 3D
representations. However, foundational approaches like 3D Gaussian Splatting
come with a substantial storage overhead caused by growing the SfM points to
millions, often demanding gigabyte-level disk space for a single unbounded
scene, posing significant scalability challenges and hindering the splatting
efficiency.
&lt;/p&gt;
&lt;p&gt;To address this challenge, we introduce LightGaussian, a novel method
designed to transform 3D Gaussians into a more efficient and compact format.
Drawing inspiration from the concept of Network Pruning, LightGaussian
identifies Gaussians that are insignificant in contributing to the scene
reconstruction and adopts a pruning and recovery process, effectively reducing
redundancy in Gaussian counts while preserving visual effects. Additionally,
LightGaussian employs distillation and pseudo-view augmentation to distill
spherical harmonics to a lower degree, allowing knowledge transfer to more
compact representations while maintaining reflectance. Furthermore, we propose
a hybrid scheme, VecTree Quantization, to quantize all attributes, resulting in
lower bitwidth representations with minimal accuracy losses.
&lt;/p&gt;
&lt;p&gt;In summary, LightGaussian achieves an averaged compression rate over 15x
while boosting the FPS from 139 to 215, enabling an efficient representation of
complex scenes on Mip-NeRF 360, Tank and Temple datasets.
&lt;/p&gt;
&lt;p&gt;Project website: https://lightgaussian.github.io/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1&quot;&gt;Zhiwen Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kevin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_K/0/1/0/all/0/1&quot;&gt;Kairun Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zehao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Dejia Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhangyang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17280">
<title>Does VLN Pretraining Work with Nonsensical or Irrelevant Instructions?. (arXiv:2311.17280v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17280</link>
<description rdf:parseType="Literal">&lt;p&gt;Data augmentation via back-translation is common when pretraining
Vision-and-Language Navigation (VLN) models, even though the generated
instructions are noisy. But: does that noise matter? We find that nonsensical
or irrelevant language instructions during pretraining can have little effect
on downstream performance for both HAMT and VLN-BERT on R2R, and is still
better than only using clean, human data. To underscore these results, we
concoct an efficient augmentation method, Unigram + Object, which generates
nonsensical instructions that nonetheless improve downstream performance. Our
findings suggest that what matters for VLN R2R pretraining is the quantity of
visual trajectories, not the quality of instructions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_I/0/1/0/all/0/1&quot;&gt;Ishika Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1&quot;&gt;Robin Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1&quot;&gt;Jesse Thomason&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17465">
<title>AgentAvatar: Disentangling Planning, Driving and Rendering for Photorealistic Avatar Agents. (arXiv:2311.17465v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17465</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, our goal is to create interactive avatar agents that can
autonomously plan and animate nuanced facial movements realistically, from both
visual and behavioral perspectives. Given high-level inputs about the
environment and agent profile, our framework harnesses LLMs to produce a series
of detailed text descriptions of the avatar agents&apos; facial motions. These
descriptions are then processed by our task-agnostic driving engine into motion
token sequences, which are subsequently converted into continuous motion
embeddings that are further consumed by our standalone neural-based renderer to
generate the final photorealistic avatar animations. These streamlined
processes allow our framework to adapt to a variety of non-verbal avatar
interactions, both monadic and dyadic. Our extensive study, which includes
experiments on both newly compiled and existing datasets featuring two types of
agents -- one capable of monadic interaction with the environment, and the
other designed for dyadic conversation -- validates the effectiveness and
versatility of our approach. To our knowledge, we advanced a leap step by
combining LLMs and neural rendering for generalized non-verbal prediction and
photo-realistic rendering of avatar agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Duomin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1&quot;&gt;Bin Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yu Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Baoyuan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17629">
<title>Efficient Decoder for End-to-End Oriented Object Detection in Remote Sensing Images. (arXiv:2311.17629v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17629</link>
<description rdf:parseType="Literal">&lt;p&gt;Object instances in remote sensing images often distribute with
multi-orientations, varying scales, and dense distribution. These issues bring
challenges to end-to-end oriented object detectors including multi-scale
features alignment and a large number of queries. To address these limitations,
we propose an end-to-end oriented detector equipped with an efficient decoder,
which incorporates two technologies, Rotated RoI attention (RRoI attention) and
Selective Distinct Queries (SDQ). Specifically, RRoI attention effectively
focuses on oriented regions of interest through a cross-attention mechanism and
aligns multi-scale features. SDQ collects queries from intermediate decoder
layers and then filters similar queries to obtain distinct queries. The
proposed SDQ can facilitate the optimization of one-to-one label assignment,
without introducing redundant initial queries or extra auxiliary branches.
Extensive experiments on five datasets demonstrate the effectiveness of our
method. Notably, our method achieves state-of-the-art performance on DIOR-R
(67.31% mAP), DOTA-v1.5 (67.43% mAP), and DOTA-v2.0 (53.28% mAP) with the
ResNet50 backbone.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jiaqi Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1&quot;&gt;Zeyu Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yong Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Hancheng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1&quot;&gt;Wenliang Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_R/0/1/0/all/0/1&quot;&gt;Rui Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saddik_A/0/1/0/all/0/1&quot;&gt;Abdulmotaleb El Saddik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17971">
<title>GeoDream: Disentangling 2D and Geometric Priors for High-Fidelity and Consistent 3D Generation. (arXiv:2311.17971v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17971</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-3D generation by distilling pretrained large-scale text-to-image
diffusion models has shown great promise but still suffers from inconsistent 3D
geometric structures (Janus problems) and severe artifacts. The aforementioned
problems mainly stem from 2D diffusion models lacking 3D awareness during the
lifting. In this work, we present GeoDream, a novel method that incorporates
explicit generalized 3D priors with 2D diffusion priors to enhance the
capability of obtaining unambiguous 3D consistent geometric structures without
sacrificing diversity or fidelity. Specifically, we first utilize a multi-view
diffusion model to generate posed images and then construct cost volume from
the predicted image, which serves as native 3D geometric priors, ensuring
spatial consistency in 3D space. Subsequently, we further propose to harness 3D
geometric priors to unlock the great potential of 3D awareness in 2D diffusion
priors via a disentangled design. Notably, disentangling 2D and 3D priors
allows us to refine 3D geometric priors further. We justify that the refined 3D
geometric priors aid in the 3D-aware capability of 2D diffusion priors, which
in turn provides superior guidance for the refinement of 3D geometric priors.
Our numerical and visual comparisons demonstrate that GeoDream generates more
3D consistent textured meshes with high-resolution realistic renderings (i.e.,
1024 $\times$ 1024) and adheres more closely to semantic coherence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_B/0/1/0/all/0/1&quot;&gt;Baorui Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1&quot;&gt;Haoge Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Junsheng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yu-Shen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tiejun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinlong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.18068">
<title>ALSTER: A Local Spatio-Temporal Expert for Online 3D Semantic Reconstruction. (arXiv:2311.18068v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.18068</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an online 3D semantic segmentation method that incrementally
reconstructs a 3D semantic map from a stream of RGB-D frames. Unlike offline
methods, ours is directly applicable to scenarios with real-time constraints,
such as robotics or mixed reality. To overcome the inherent challenges of
online methods, we make two main contributions. First, to effectively extract
information from the input RGB-D video stream, we jointly estimate geometry and
semantic labels per frame in 3D. A key focus of our approach is to reason about
semantic entities both in the 2D input and the local 3D domain to leverage
differences in spatial context and network architectures. Our method predicts
2D features using an off-the-shelf segmentation network. The extracted 2D
features are refined by a lightweight 3D network to enable reasoning about the
local 3D structure. Second, to efficiently deal with an infinite stream of
input RGB-D frames, a subsequent network serves as a temporal expert predicting
the incremental scene updates by leveraging 2D, 3D, and past information in a
learned manner. These updates are then integrated into a global scene
representation. Using these main contributions, our method can enable scenarios
with real-time constraints and can scale to arbitrary scene sizes by processing
and updating the scene only in a local region defined by the new measurement.
Our experiments demonstrate improved results compared to existing online
methods that purely operate in local regions and show that complementary
sources of information can boost the performance. We provide a thorough
ablation study on the benefits of different architectural as well as
algorithmic design decisions. Our method yields competitive results on the
popular ScanNet benchmark and SceneNN dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weder_S/0/1/0/all/0/1&quot;&gt;Silvan Weder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Engelmann_F/0/1/0/all/0/1&quot;&gt;Francis Engelmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schonberger_J/0/1/0/all/0/1&quot;&gt;Johannes L. Sch&amp;#xf6;nberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seki_A/0/1/0/all/0/1&quot;&gt;Akihito Seki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1&quot;&gt;Marc Pollefeys&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oswald_M/0/1/0/all/0/1&quot;&gt;Martin R. Oswald&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.18605">
<title>Learning Triangular Distribution in Visual World. (arXiv:2311.18605v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.18605</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolution neural network is successful in pervasive vision tasks, including
label distribution learning, which usually takes the form of learning an
injection from the non-linear visual features to the well-defined labels.
However, how the discrepancy between features is mapped to the label
discrepancy is ambient, and its correctness is not guaranteed. To address these
problems, we study the mathematical connection between feature and its label,
presenting a general and simple framework for label distribution learning. We
propose a so-called Triangular Distribution Transform (TDT) to build an
injective function between feature and label, guaranteeing that any symmetric
feature discrepancy linearly reflects the difference between labels. The
proposed TDT can be used as a plug-in in mainstream backbone networks to
address different label distribution learning tasks. Experiments on Facial Age
Recognition, Illumination Chromaticity Estimation, and Aesthetics assessment
show that TDT achieves on-par or better results than the prior arts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Ping Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xingpeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chengtao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1&quot;&gt;Dichao Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_P/0/1/0/all/0/1&quot;&gt;Peng Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Le Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1&quot;&gt;Yanlin Qian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.18803">
<title>BioCLIP: A Vision Foundation Model for the Tree of Life. (arXiv:2311.18803v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.18803</link>
<description rdf:parseType="Literal">&lt;p&gt;Images of the natural world, collected by a variety of cameras, from drones
to individual phones, are increasingly abundant sources of biological
information. There is an explosion of computational methods and tools,
particularly computer vision, for extracting biologically relevant information
from images for science and conservation. Yet most of these are bespoke
approaches designed for a specific task and are not easily adaptable or
extendable to new questions, contexts, and datasets. A vision model for general
organismal biology questions on images is of timely need. To approach this, we
curate and release TreeOfLife-10M, the largest and most diverse ML-ready
dataset of biology images. We then develop BioCLIP, a foundation model for the
tree of life, leveraging the unique properties of biology captured by
TreeOfLife-10M, namely the abundance and variety of images of plants, animals,
and fungi, together with the availability of rich structured biological
knowledge. We rigorously benchmark our approach on diverse fine-grained biology
classification tasks, and find that BioCLIP consistently and substantially
outperforms existing baselines (by 17% to 20% absolute). Intrinsic evaluation
reveals that BioCLIP has learned a hierarchical representation conforming to
the tree of life, shedding light on its strong generalizability. Our code,
models and data will be made available at
https://github.com/Imageomics/bioclip.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stevens_S/0/1/0/all/0/1&quot;&gt;Samuel Stevens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiaman Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thompson_M/0/1/0/all/0/1&quot;&gt;Matthew J Thompson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Campolongo_E/0/1/0/all/0/1&quot;&gt;Elizabeth G Campolongo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1&quot;&gt;Chan Hee Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carlyn_D/0/1/0/all/0/1&quot;&gt;David Edward Carlyn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1&quot;&gt;Li Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dahdul_W/0/1/0/all/0/1&quot;&gt;Wasila M Dahdul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stewart_C/0/1/0/all/0/1&quot;&gt;Charles Stewart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berger_Wolf_T/0/1/0/all/0/1&quot;&gt;Tanya Berger-Wolf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1&quot;&gt;Wei-Lun Chao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yu Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.18828">
<title>One-step Diffusion with Distribution Matching Distillation. (arXiv:2311.18828v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.18828</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models generate high-quality images but require dozens of forward
passes. We introduce Distribution Matching Distillation (DMD), a procedure to
transform a diffusion model into a one-step image generator with minimal impact
on image quality. We enforce the one-step image generator match the diffusion
model at distribution level, by minimizing an approximate KL divergence whose
gradient can be expressed as the difference between 2 score functions, one of
the target distribution and the other of the synthetic distribution being
produced by our one-step generator. The score functions are parameterized as
two diffusion models trained separately on each distribution. Combined with a
simple regression loss matching the large-scale structure of the multi-step
diffusion outputs, our method outperforms all published few-step diffusion
approaches, reaching 2.62 FID on ImageNet 64x64 and 11.49 FID on zero-shot
COCO-30k, comparable to Stable Diffusion but orders of magnitude faster.
Utilizing FP16 inference, our model generates images at 20 FPS on modern
hardware.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_T/0/1/0/all/0/1&quot;&gt;Tianwei Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gharbi_M/0/1/0/all/0/1&quot;&gt;Micha&amp;#xeb;l Gharbi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Richard Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shechtman_E/0/1/0/all/0/1&quot;&gt;Eli Shechtman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durand_F/0/1/0/all/0/1&quot;&gt;Fredo Durand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1&quot;&gt;William T. Freeman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_T/0/1/0/all/0/1&quot;&gt;Taesung Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00311">
<title>3D Face Reconstruction with the Geometric Guidance of Facial Part Segmentation. (arXiv:2312.00311v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.00311</link>
<description rdf:parseType="Literal">&lt;p&gt;3D Morphable Models (3DMMs) provide promising 3D face reconstructions in
various applications. However, existing methods struggle to reconstruct faces
with extreme expressions due to deficiencies in supervisory signals, such as
sparse or inaccurate landmarks. Segmentation information contains effective
geometric contexts for face reconstruction. Certain attempts intuitively depend
on differentiable renderers to compare the rendered silhouettes of
reconstruction with segmentation, which is prone to issues like local optima
and gradient instability. In this paper, we fully utilize the facial part
segmentation geometry by introducing Part Re-projection Distance Loss (PRDL).
Specifically, PRDL transforms facial part segmentation into 2D points and
re-projects the reconstruction onto the image plane. Subsequently, by
introducing grid anchors and computing different statistical distances from
these anchors to the point sets, PRDL establishes geometry descriptors to
optimize the distribution of the point sets for face reconstruction. PRDL
exhibits a clear gradient compared to the renderer-based methods and presents
state-of-the-art reconstruction performance in extensive quantitative and
qualitative experiments. The project will be publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zidu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiangyu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianshuo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Baiqin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1&quot;&gt;Zhen Lei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00360">
<title>Efficient Multimodal Semantic Segmentation via Dual-Prompt Learning. (arXiv:2312.00360v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.00360</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal (e.g., RGB-Depth/RGB-Thermal) fusion has shown great potential for
improving semantic segmentation in complex scenes (e.g., indoor/low-light
conditions). Existing approaches often fully fine-tune a dual-branch
encoder-decoder framework with a complicated feature fusion strategy for
achieving multimodal semantic segmentation, which is training-costly due to the
massive parameter updates in feature extraction and fusion. To address this
issue, we propose a surprisingly simple yet effective dual-prompt learning
network (dubbed DPLNet) for training-efficient multimodal (e.g., RGB-D/T)
semantic segmentation. The core of DPLNet is to directly adapt a frozen
pre-trained RGB model to multimodal semantic segmentation, reducing parameter
updates. For this purpose, we present two prompt learning modules, comprising
multimodal prompt generator (MPG) and multimodal feature adapter (MFA). MPG
works to fuse the features from different modalities in a compact manner and is
inserted from shadow to deep stages to generate the multi-level multimodal
prompts that are injected into the frozen backbone, while MPG adapts prompted
multimodal features in the frozen backbone for better multimodal semantic
segmentation. Since both the MPG and MFA are lightweight, only a few trainable
parameters (3.88M, 4.4% of the pre-trained backbone parameters) are introduced
for multimodal feature fusion and learning. Using a simple decoder (3.27M
parameters), DPLNet achieves new state-of-the-art performance or is on a par
with other complex approaches on four RGB-D/T semantic segmentation datasets
while satisfying parameter efficiency. Moreover, we show that DPLNet is general
and applicable to other multimodal tasks such as salient object detection and
video semantic segmentation. Without special design, DPLNet outperforms many
complicated models. Our code will be available at
github.com/ShaohuaDong2021/DPLNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1&quot;&gt;Shaohua Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yunhe Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dongfang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1&quot;&gt;Heng Fan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00761">
<title>Deep Unlearning: Fast and Efficient Training-free Approach to Controlled Forgetting. (arXiv:2312.00761v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.00761</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine unlearning has emerged as a prominent and challenging area of
interest, driven in large part by the rising regulatory demands for industries
to delete user data upon request and the heightened awareness of privacy.
Existing approaches either retrain models from scratch or use several
finetuning steps for every deletion request, often constrained by computational
resource limitations and restricted access to the original training data. In
this work, we introduce a novel class unlearning algorithm designed to
strategically eliminate an entire class or a group of classes from the learned
model. To that end, our algorithm first estimates the Retain Space and the
Forget Space, representing the feature or activation spaces for samples from
classes to be retained and unlearned, respectively. To obtain these spaces, we
propose a novel singular value decomposition-based technique that requires
layer wise collection of network activations from a few forward passes through
the network. We then compute the shared information between these spaces and
remove it from the forget space to isolate class-discriminatory feature space
for unlearning. Finally, we project the model weights in the orthogonal
direction of the class-discriminatory space to obtain the unlearned model. We
demonstrate our algorithm&apos;s efficacy on ImageNet using a Vision Transformer
with only $\sim$1.5% drop in retain accuracy compared to the original model
while maintaining under 1% accuracy on the unlearned class samples. Further,
our algorithm consistently performs well when subject to Membership Inference
Attacks showing 7.8% improvement on average across a variety of image
classification datasets and network architectures, as compared to other
baselines while being $\sim$6x more computationally efficient.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kodge_S/0/1/0/all/0/1&quot;&gt;Sangamesh Kodge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_G/0/1/0/all/0/1&quot;&gt;Gobinda Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1&quot;&gt;Kaushik Roy&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>