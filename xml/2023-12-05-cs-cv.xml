<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-12-03T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00040" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00041" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00055" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00057" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00063" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00065" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00067" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00068" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00069" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00072" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00075" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00079" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00081" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00082" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00083" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00084" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00085" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00092" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00093" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00094" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00096" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00097" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00098" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00101" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00105" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00109" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00110" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00112" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00113" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00114" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00115" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00116" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00151" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00157" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00169" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00173" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00174" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00184" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00188" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00192" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00195" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00204" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00206" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00210" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00220" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00223" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00224" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00236" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00250" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00252" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00269" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00299" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00304" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00306" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00308" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00311" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00312" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00313" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00316" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00330" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00335" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00343" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00347" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00348" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00351" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00357" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00358" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00360" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00362" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00364" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00375" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00377" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00386" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00387" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00392" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00398" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00401" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00412" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00414" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00416" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00425" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00435" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00438" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00451" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00452" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00454" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00462" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00467" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00487" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00500" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00529" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00532" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00534" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00548" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00570" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00583" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00588" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00589" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00591" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00592" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00593" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00596" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00597" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00598" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00627" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00633" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00634" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00639" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00648" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00651" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00661" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00663" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00671" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00674" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00677" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00689" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00690" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00692" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00694" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00699" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00700" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00703" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00732" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00739" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2006.15920" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2010.04055" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.13278" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.00379" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.09020" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.14611" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.05781" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.06108" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.02003" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.04705" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.10428" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.10127" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.13098" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.13080" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.13095" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.06388" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.01811" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.04336" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.06767" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.06777" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.07618" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18670" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00854" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.13325" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14505" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00309" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01985" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07643" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01300" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.08529" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.10196" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12831" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.13670" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16911" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.00748" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.01340" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14564" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16496" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03940" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04677" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05920" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.06282" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15578" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00213" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04591" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09680" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12912" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14049" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14521" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15890" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17116" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17179" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17315" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17338" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17465" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17475" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17618" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.18765" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2312.00040">
<title>Presentation Attack detection using Wavelet Transform and Deep Residual Neural Net. (arXiv:2312.00040v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.00040</link>
<description rdf:parseType="Literal">&lt;p&gt;Biometric authentication is becoming more prevalent for secured
authentication systems. However, the biometric substances can be deceived by
the imposters in several ways. Among other imposter attacks, print attacks,
mask attacks, and replay attacks fall under the presentation attack category.
The bio-metric images, especially the iris and face, are vulnerable to
different presentation attacks. This research applies deep learning approaches
to mitigate presentation attacks in a biometric access control system. Our
contribution in this paper is two-fold: First, we applied the wavelet transform
to extract the features from the biometric images. Second, we modified the deep
residual neural net and applied it to the spoof datasets in an attempt to
detect the presentation attacks. This research applied the proposed approach to
biometric spoof datasets, namely ATVS, CASIA two class, and CASIA cropped image
sets. The datasets used in this research contain images that are captured in
both a controlled and uncontrolled environment along with different resolutions
and sizes. We obtained the best accuracy of 93% on the ATVS Iris datasets. For
CASIA two class and CASIA cropped datasets, we achieved test accuracies of 91%
and 82%, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatterjee_P/0/1/0/all/0/1&quot;&gt;Prosenjit Chatterjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yalchin_A/0/1/0/all/0/1&quot;&gt;Alex Yalchin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shelton_J/0/1/0/all/0/1&quot;&gt;Joseph Shelton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1&quot;&gt;Kaushik Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xiaohong Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Edoh_K/0/1/0/all/0/1&quot;&gt;Kossi D. Edoh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00041">
<title>Presentation Attack Detection using Convolutional Neural Networks and Local Binary Patterns. (arXiv:2312.00041v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.00041</link>
<description rdf:parseType="Literal">&lt;p&gt;The use of biometrics to authenticate users and control access to secure
areas has become extremely popular in recent years, and biometric access
control systems are frequently used by both governments and private
corporations. However, these systems may represent risks to security when
deployed without considering the possibility of biometric presentation attacks
(also known as spoofing). Presentation attacks are a serious threat because
they do not require significant time, expense, or skill to carry out while
remaining effective against many biometric systems in use today. This research
compares three different software-based methods for facial and iris
presentation attack detection in images. The first method uses Inception-v3, a
pre-trained deep Convolutional Neural Network (CNN) made by Google for the
ImageNet challenge, which is retrained for this problem. The second uses a
shallow CNN based on a modified Spoofnet architecture, which is trained
normally. The third is a texture-based method using Local Binary Patterns
(LBP). The datasets used are the ATVS-FIr dataset, which contains real and fake
iris images, and the CASIA Face Anti-Spoofing Dataset, which contains real
images as well as warped photos, cut photos, and video replay presentation
attacks. We also present a third set of results, based on cropped versions of
the CASIA images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spencer_J/0/1/0/all/0/1&quot;&gt;Justin Spencer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lawrence_D/0/1/0/all/0/1&quot;&gt;Deborah Lawrence&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatterjee_P/0/1/0/all/0/1&quot;&gt;Prosenjit Chatterjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1&quot;&gt;Kaushik Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Esterline_A/0/1/0/all/0/1&quot;&gt;Albert Esterline&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jung-Hee Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00055">
<title>LEAP: LLM-Generation of Egocentric Action Programs. (arXiv:2312.00055v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00055</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce LEAP (illustrated in Figure 1), a novel method for generating
video-grounded action programs through use of a Large Language Model (LLM).
These action programs represent the motoric, perceptual, and structural aspects
of action, and consist of sub-actions, pre- and post-conditions, and control
flows. LEAP&apos;s action programs are centered on egocentric video and employ
recent developments in LLMs both as a source for program knowledge and as an
aggregator and assessor of multimodal video information. We apply LEAP over a
majority (87\%) of the training set of the EPIC Kitchens dataset, and release
the resulting action programs as a publicly available dataset here
(https://drive.google.com/drive/folders/1Cpkw_TI1IIxXdzor0pOXG3rWJWuKU5Ex?usp=drive_link).
We employ LEAP as a secondary source of supervision, using its action programs
in a loss term applied to action recognition and anticipation networks. We
demonstrate sizable improvements in performance in both tasks due to training
with the LEAP dataset. Our method achieves 1st place on the EPIC Kitchens
Action Recognition leaderboard as of November 17 among the networks restricted
to RGB-input (see Supplementary Materials).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dessalene_E/0/1/0/all/0/1&quot;&gt;Eadom Dessalene&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maynord_M/0/1/0/all/0/1&quot;&gt;Michael Maynord&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fermuller_C/0/1/0/all/0/1&quot;&gt;Cornelia Ferm&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aloimonos_Y/0/1/0/all/0/1&quot;&gt;Yiannis Aloimonos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00057">
<title>Probabilistic Copyright Protection Can Fail for Text-to-Image Generative Models. (arXiv:2312.00057v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.00057</link>
<description rdf:parseType="Literal">&lt;p&gt;The booming use of text-to-image generative models has raised concerns about
their high risk of producing copyright-infringing content. While probabilistic
copyright protection methods provide a probabilistic guarantee against such
infringement, in this paper, we introduce Virtually Assured Amplification
Attack (VA3), a novel online attack framework that exposes the vulnerabilities
of these protection mechanisms. The proposed framework significantly amplifies
the probability of generating infringing content on the sustained interactions
with generative models and a lower-bounded success probability of each
engagement. Our theoretical and experimental results demonstrate the
effectiveness of our approach and highlight the potential risk of implementing
probabilistic copyright protection in practical applications of text-to-image
generative models. Code is available at https://github.com/South7X/VA3.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Q/0/1/0/all/0/1&quot;&gt;Qianli Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1&quot;&gt;Kenji Kawaguchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00063">
<title>MoMask: Generative Masked Modeling of 3D Human Motions. (arXiv:2312.00063v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00063</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce MoMask, a novel masked modeling framework for text-driven 3D
human motion generation. In MoMask, a hierarchical quantization scheme is
employed to represent human motion as multi-layer discrete motion tokens with
high-fidelity details. Starting at the base layer, with a sequence of motion
tokens obtained by vector quantization, the residual tokens of increasing
orders are derived and stored at the subsequent layers of the hierarchy. This
is consequently followed by two distinct bidirectional transformers. For the
base-layer motion tokens, a Masked Transformer is designated to predict
randomly masked motion tokens conditioned on text input at training stage.
During generation (i.e. inference) stage, starting from an empty sequence, our
Masked Transformer iteratively fills up the missing tokens; Subsequently, a
Residual Transformer learns to progressively predict the next-layer tokens
based on the results from current layer. Extensive experiments demonstrate that
MoMask outperforms the state-of-art methods on the text-to-motion generation
task, with an FID of 0.045 (vs e.g. 0.141 of T2M-GPT) on the HumanML3D dataset,
and 0.228 (vs 0.514) on KIT-ML, respectively. MoMask can also be seamlessly
applied in related tasks without further model fine-tuning, such as text-guided
temporal inpainting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1&quot;&gt;Chuan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Javed_M/0/1/0/all/0/1&quot;&gt;Muhammad Gohar Javed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1&quot;&gt;Li Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00065">
<title>Unsupervised Keypoints from Pretrained Diffusion Models. (arXiv:2312.00065v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00065</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised learning of keypoints and landmarks has seen significant
progress with the help of modern neural network architectures, but performance
is yet to match the supervised counterpart, making their practicability
questionable. We leverage the emergent knowledge within text-to-image diffusion
models, towards more robust unsupervised keypoints. Our core idea is to find
text embeddings that would cause the generative model to consistently attend to
compact regions in images (i.e. keypoints). To do so, we simply optimize the
text embedding such that the cross-attention maps within the denoising network
are localized as Gaussians with small standard deviations. We validate our
performance on multiple datasets: the CelebA, CUB-200-2011, Tai-Chi-HD,
DeepFashion, and Human3.6m datasets. We achieve significantly improved
accuracy, sometimes even outperforming supervised ones, particularly for data
that is non-aligned and less curated. Our code is publicly available and can be
found through our project page: https://ubc-vision.github.io/StableKeypoints/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hedlin_E/0/1/0/all/0/1&quot;&gt;Eric Hedlin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_G/0/1/0/all/0/1&quot;&gt;Gopal Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahajan_S/0/1/0/all/0/1&quot;&gt;Shweta Mahajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xingzhe He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isack_H/0/1/0/all/0/1&quot;&gt;Hossam Isack&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rhodin_A/0/1/0/all/0/1&quot;&gt;Abhishek Kar Helge Rhodin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1&quot;&gt;Andrea Tagliasacchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1&quot;&gt;Kwang Moo Yi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00067">
<title>Predicting breast cancer with AI for individual risk-adjusted MRI screening and early detection. (arXiv:2312.00067v1 [physics.med-ph])</title>
<link>http://arxiv.org/abs/2312.00067</link>
<description rdf:parseType="Literal">&lt;p&gt;Women with an increased life-time risk of breast cancer undergo supplemental
annual screening MRI. We propose to predict the risk of developing breast
cancer within one year based on the current MRI, with the objective of reducing
screening burden and facilitating early detection. An AI algorithm was
developed on 53,858 breasts from 12,694 patients who underwent screening or
diagnostic MRI and accrued over 12 years, with 2,331 confirmed cancers. A first
U-Net was trained to segment lesions and identify regions of concern. A second
convolutional network was trained to detect malignant cancer using features
extracted by the U-Net. This network was then fine-tuned to estimate the risk
of developing cancer within a year in cases that radiologists considered normal
or likely benign. Risk predictions from this AI were evaluated with a
retrospective analysis of 9,183 breasts from a high-risk screening cohort,
which were not used for training. Statistical analysis focused on the tradeoff
between number of omitted exams versus negative predictive value, and number of
potential early detections versus positive predictive value. The AI algorithm
identified regions of concern that coincided with future tumors in 52% of
screen-detected cancers. Upon directed review, a radiologist found that 71.3%
of cancers had a visible correlate on the MRI prior to diagnosis, 65% of these
correlates were identified by the AI model. Reevaluating these regions in 10%
of all cases with higher AI-predicted risk could have resulted in up to 33%
early detections by a radiologist. Additionally, screening burden could have
been reduced in 16% of lower-risk cases by recommending a later follow-up
without compromising current interval cancer rate. With increasing datasets and
improving image quality we expect this new AI-aided, adaptive screening to
meaningfully reduce screening burden and improve early detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Hirsch_L/0/1/0/all/0/1&quot;&gt;Lukas Hirsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Makse_H/0/1/0/all/0/1&quot;&gt;Hernan A. Makse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Martinez_D/0/1/0/all/0/1&quot;&gt;Danny F. Martinez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Hughes_M/0/1/0/all/0/1&quot;&gt;Mary Hughes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Eskreis_Winkler_S/0/1/0/all/0/1&quot;&gt;Sarah Eskreis-Winkler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Pinker_K/0/1/0/all/0/1&quot;&gt;Katja Pinker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Morris_E/0/1/0/all/0/1&quot;&gt;Elizabeth Morris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Parra_L/0/1/0/all/0/1&quot;&gt;Lucas C. Parra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Sutton_E/0/1/0/all/0/1&quot;&gt;Elizabeth J. Sutton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00068">
<title>GLiDR: Topologically Regularized Graph Generative Network for Sparse LiDAR Point Clouds. (arXiv:2312.00068v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.00068</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparse LiDAR point clouds cause severe loss of detail of static structures
and reduce the density of static points available for navigation. Reduced
density can be detrimental to navigation under several scenarios. We observe
that despite high sparsity, in most cases, the global topology of LiDAR
outlining the static structures can be inferred. We utilize this property to
obtain a backbone skeleton of a static LiDAR scan in the form of a single
connected component that is a proxy to its global topology. We utilize the
backbone to augment new points along static structures to overcome sparsity.
Newly introduced points could correspond to existing static structures or to
static points that were earlier obstructed by dynamic objects. To the best of
our knowledge, we are the first to use this strategy for sparse LiDAR point
clouds. Existing solutions close to our approach fail to identify and preserve
the global static LiDAR topology and generate sub-optimal points. We propose
GLiDR, a Graph Generative network that is topologically regularized using
0-dimensional Persistent Homology (PH) constraints. This enables GLiDR to
introduce newer static points along a topologically consistent global static
LiDAR backbone. GLiDR generates precise static points using 32x sparser dynamic
scans and performs better than the baselines across three datasets. The newly
introduced static points allow GLiDR to outperform LiDAR-based navigation using
SLAM in several settings. GLiDR generates a valuable byproduct - an accurate
binary segmentation mask of static and dynamic objects that is helpful for
navigation planning and safety in constrained environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1&quot;&gt;Prashant Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhat_K/0/1/0/all/0/1&quot;&gt;Kshitij Madhav Bhat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nadkarni_V/0/1/0/all/0/1&quot;&gt;Vedang Bhupesh Shenvi Nadkarni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalra_P/0/1/0/all/0/1&quot;&gt;Prem Kalra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00069">
<title>SICKLE: A Multi-Sensor Satellite Imagery Dataset Annotated with Multiple Key Cropping Parameters. (arXiv:2312.00069v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00069</link>
<description rdf:parseType="Literal">&lt;p&gt;The availability of well-curated datasets has driven the success of Machine
Learning (ML) models. Despite greater access to earth observation data in
agriculture, there is a scarcity of curated and labelled datasets, which limits
the potential of its use in training ML models for remote sensing (RS) in
agriculture. To this end, we introduce a first-of-its-kind dataset called
SICKLE, which constitutes a time-series of multi-resolution imagery from 3
distinct satellites: Landsat-8, Sentinel-1 and Sentinel-2. Our dataset
constitutes multi-spectral, thermal and microwave sensors during January 2018 -
March 2021 period. We construct each temporal sequence by considering the
cropping practices followed by farmers primarily engaged in paddy cultivation
in the Cauvery Delta region of Tamil Nadu, India; and annotate the
corresponding imagery with key cropping parameters at multiple resolutions
(i.e. 3m, 10m and 30m). Our dataset comprises 2,370 season-wise samples from
388 unique plots, having an average size of 0.38 acres, for classifying 21 crop
types across 4 districts in the Delta, which amounts to approximately 209,000
satellite images. Out of the 2,370 samples, 351 paddy samples from 145 plots
are annotated with multiple crop parameters; such as the variety of paddy, its
growing season and productivity in terms of per-acre yields. Ours is also one
among the first studies that consider the growing season activities pertinent
to crop phenology (spans sowing, transplanting and harvesting dates) as
parameters of interest. We benchmark SICKLE on three tasks: crop type, crop
phenology (sowing, transplanting, harvesting), and yield prediction
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sani_D/0/1/0/all/0/1&quot;&gt;Depanshu Sani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahato_S/0/1/0/all/0/1&quot;&gt;Sandeep Mahato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saini_S/0/1/0/all/0/1&quot;&gt;Sourabh Saini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_H/0/1/0/all/0/1&quot;&gt;Harsh Kumar Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Devshali_C/0/1/0/all/0/1&quot;&gt;Charu Chandra Devshali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anand_S/0/1/0/all/0/1&quot;&gt;Saket Anand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_G/0/1/0/all/0/1&quot;&gt;Gaurav Arora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayaraman_T/0/1/0/all/0/1&quot;&gt;Thiagarajan Jayaraman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00072">
<title>CRAFT: Contextual Re-Activation of Filters for face recogntion Training. (arXiv:2312.00072v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00072</link>
<description rdf:parseType="Literal">&lt;p&gt;The first layer of a deep CNN backbone applies filters to an image to extract
the basic features available to later layers. During training, some filters may
go inactive, mean ing all weights in the filter approach zero. An inactive fil
ter in the final model represents a missed opportunity to extract a useful
feature. This phenomenon is especially prevalent in specialized CNNs such as
for face recogni tion (as opposed to, e.g., ImageNet). For example, in one the
most widely face recognition model (ArcFace), about half of the convolution
filters in the first layer are inactive. We propose a novel approach designed
and tested specif ically for face recognition networks, known as &quot;CRAFT:
Contextual Re-Activation of Filters for Face Recognition Training&quot;. CRAFT
identifies inactive filters during training and reinitializes them based on the
context of strong filters at that stage in training. We show that CRAFT reduces
fraction of inactive filters from 44% to 32% on average and discovers filter
patterns not found by standard training. Compared to standard training without
reactivation, CRAFT demonstrates enhanced model accuracy on standard
face-recognition benchmark datasets including AgeDB-30, CPLFW, LFW, CALFW, and
CFP-FP, as well as on more challenging datasets like IJBB and IJBC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatta_A/0/1/0/all/0/1&quot;&gt;Aman Bhatta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00075">
<title>Accelerating Neural Field Training via Soft Mining. (arXiv:2312.00075v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00075</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an approach to accelerate Neural Field training by efficiently
selecting sampling locations. While Neural Fields have recently become popular,
it is often trained by uniformly sampling the training domain, or through
handcrafted heuristics. We show that improved convergence and final training
quality can be achieved by a soft mining technique based on importance
sampling: rather than either considering or ignoring a pixel completely, we
weigh the corresponding loss by a scalar. To implement our idea we use Langevin
Monte-Carlo sampling. We show that by doing so, regions with higher error are
being selected more frequently, leading to more than 2x improvement in
convergence speed. The code and related resources for this study are publicly
available at https://ubc-vision.github.io/nf-soft-mining/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kheradmand_S/0/1/0/all/0/1&quot;&gt;Shakiba Kheradmand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rebain_D/0/1/0/all/0/1&quot;&gt;Daniel Rebain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_G/0/1/0/all/0/1&quot;&gt;Gopal Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isack_H/0/1/0/all/0/1&quot;&gt;Hossam Isack&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kar_A/0/1/0/all/0/1&quot;&gt;Abhishek Kar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1&quot;&gt;Andrea Tagliasacchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1&quot;&gt;Kwang Moo Yi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00079">
<title>HiFi Tuner: High-Fidelity Subject-Driven Fine-Tuning for Diffusion Models. (arXiv:2312.00079v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00079</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores advancements in high-fidelity personalized image
generation through the utilization of pre-trained text-to-image diffusion
models. While previous approaches have made significant strides in generating
versatile scenes based on text descriptions and a few input images, challenges
persist in maintaining the subject fidelity within the generated images. In
this work, we introduce an innovative algorithm named HiFi Tuner to enhance the
appearance preservation of objects during personalized image generation. Our
proposed method employs a parameter-efficient fine-tuning framework, comprising
a denoising process and a pivotal inversion process. Key enhancements include
the utilization of mask guidance, a novel parameter regularization technique,
and the incorporation of step-wise subject representations to elevate the
sample fidelity. Additionally, we propose a reference-guided generation
approach that leverages the pivotal inversion of a reference image to mitigate
unwanted subject variations and artifacts. We further extend our method to a
novel image editing task: substituting the subject in an image through textual
manipulations. Experimental evaluations conducted on the DreamBooth dataset
using the Stable Diffusion model showcase promising results. Fine-tuning solely
on textual embeddings improves CLIP-T score by 3.6 points and improves DINO
score by 9.6 points over Textual Inversion. When fine-tuning all parameters,
HiFi Tuner improves CLIP-T score by 1.2 points and improves DINO score by 1.2
points over DreamBooth, establishing a new state of the art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhonghao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1&quot;&gt;Wei Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1&quot;&gt;Zhisheng Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasegawa_Johnson_M/0/1/0/all/0/1&quot;&gt;Mark Hasegawa-Johnson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Humphrey Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_T/0/1/0/all/0/1&quot;&gt;Tingbo Hou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00081">
<title>Synthesize, Diagnose, and Optimize: Towards Fine-Grained Vision-Language Understanding. (arXiv:2312.00081v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00081</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision language models (VLM) have demonstrated remarkable performance across
various downstream tasks. However, understanding fine-grained visual-linguistic
concepts, such as attributes and inter-object relationships, remains a
significant challenge. While several benchmarks aim to evaluate VLMs in finer
granularity, their primary focus remains on the linguistic aspect, neglecting
the visual dimension. Here, we highlight the importance of evaluating VLMs from
both a textual and visual perspective. We introduce a progressive pipeline to
synthesize images that vary in a specific attribute while ensuring consistency
in all other aspects. Utilizing this data engine, we carefully design a
benchmark, SPEC, to diagnose the comprehension of object size, position,
existence, and count. Subsequently, we conduct a thorough evaluation of four
leading VLMs on SPEC. Surprisingly, their performance is close to random guess,
revealing significant limitations. With this in mind, we propose a simply yet
effective approach to optimize VLMs in fine-grained understanding, achieving
significant improvements on SPEC without compromising the zero-shot
performance. Results on two additional fine-grained benchmarks also show
consistent improvements, further validating the transferability of our
approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1&quot;&gt;Wujian Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1&quot;&gt;Sicheng Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_Z/0/1/0/all/0/1&quot;&gt;Zuyao You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_S/0/1/0/all/0/1&quot;&gt;Shiyi Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zuxuan Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00082">
<title>A Compact Implicit Neural Representation for Efficient Storage of Massive 4D Functional Magnetic Resonance Imaging. (arXiv:2312.00082v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.00082</link>
<description rdf:parseType="Literal">&lt;p&gt;Functional Magnetic Resonance Imaging (fMRI) data is a kind of widely used
four-dimensional biomedical data, demanding effective compression but
presenting unique challenges for compression due to its intricate temporal
dynamics, low signal-to-noise ratio, and complicated underlying redundancies.
This paper reports a novel compression paradigm specifically tailored for fMRI
data based on Implicit Neural Representation (INR). The proposed approach
focuses on removing the various redundancies among the time series, including
(i) conducting spatial correlation modeling for intra-region dynamics, (ii)
decomposing reusable neuronal activation patterns, and using proper
initialization together with nonlinear fusion to describe the inter-region
similarity. The above scheme properly incorporates the unique features of fMRI
data, and experimental results on publicly available datasets demonstrate the
effectiveness of the proposed method, surpassing state-of-the-art algorithms in
both conventional image quality evaluation metrics and fMRI downstream tasks.
This work in this paper paves the way for sharing massive fMRI data at low
bandwidth and high fidelity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ruoran Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_R/0/1/0/all/0/1&quot;&gt;Runzhao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xiang_W/0/1/0/all/0/1&quot;&gt;Wenxin Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yuxiao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xiao_T/0/1/0/all/0/1&quot;&gt;Tingxiong Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Suo_J/0/1/0/all/0/1&quot;&gt;Jinli Suo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00083">
<title>BAM-DETR: Boundary-Aligned Moment Detection Transformer for Temporal Sentence Grounding in Videos. (arXiv:2312.00083v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00083</link>
<description rdf:parseType="Literal">&lt;p&gt;Temporal sentence grounding aims to localize moments relevant to a language
description. Recently, DETR-like approaches have shown notable progress by
decoding the center and length of a target moment from learnable queries.
However, they suffer from the issue of center misalignment raised by the
inherent ambiguity of moment centers, leading to inaccurate predictions. To
remedy this problem, we introduce a novel boundary-oriented moment formulation.
In our paradigm, the model no longer needs to find the precise center but
instead suffices to predict any anchor point within the interval, from which
the onset and offset are directly estimated. Based on this idea, we design a
Boundary-Aligned Moment Detection Transformer (BAM-DETR), equipped with a
dual-pathway decoding process. Specifically, it refines the anchor and
boundaries within parallel pathways using global and boundary-focused
attention, respectively. This separate design allows the model to focus on
desirable regions, enabling precise refinement of moment predictions. Further,
we propose a quality-based ranking method, ensuring that proposals with high
localization qualities are prioritized over incomplete ones. Extensive
experiments verify the advantages of our methods, where our model records new
state-of-the-art results on three benchmarks. Code is at
https://github.com/Pilhyeon/BAM-DETR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_P/0/1/0/all/0/1&quot;&gt;Pilhyeon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Byun_H/0/1/0/all/0/1&quot;&gt;Hyeran Byun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00084">
<title>Can Protective Perturbation Safeguard Personal Data from Being Exploited by Stable Diffusion?. (arXiv:2312.00084v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00084</link>
<description rdf:parseType="Literal">&lt;p&gt;Stable Diffusion has established itself as a foundation model in generative
AI artistic applications, receiving widespread research and application. Some
recent fine-tuning methods have made it feasible for individuals to implant
personalized concepts onto the basic Stable Diffusion model with minimal
computational costs on small datasets. However, these innovations have also
given rise to issues like facial privacy forgery and artistic copyright
infringement. In recent studies, researchers have explored the addition of
imperceptible adversarial perturbations to images to prevent potential
unauthorized exploitation and infringements when personal data is used for
fine-tuning Stable Diffusion. Although these studies have demonstrated the
ability to protect images, it is essential to consider that these methods may
not be entirely applicable in real-world scenarios. In this paper, we
systematically evaluate the use of perturbations to protect images within a
practical threat model. The results suggest that these approaches may not be
sufficient to safeguard image privacy and copyright effectively. Furthermore,
we introduce a purification method capable of removing protected perturbations
while preserving the original image structure to the greatest extent possible.
Experiments reveal that Stable Diffusion can effectively learn from purified
images over all protective methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhengyue Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1&quot;&gt;Jinhao Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kaidi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chenan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1&quot;&gt;Rui Zhangp Zidong Dup Qi Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xing Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00085">
<title>X-Dreamer: Creating High-quality 3D Content by Bridging the Domain Gap Between Text-to-2D and Text-to-3D Generation. (arXiv:2312.00085v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00085</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent times, automatic text-to-3D content creation has made significant
progress, driven by the development of pretrained 2D diffusion models. Existing
text-to-3D methods typically optimize the 3D representation to ensure that the
rendered image aligns well with the given text, as evaluated by the pretrained
2D diffusion model. Nevertheless, a substantial domain gap exists between 2D
images and 3D assets, primarily attributed to variations in camera-related
attributes and the exclusive presence of foreground objects. Consequently,
employing 2D diffusion models directly for optimizing 3D representations may
lead to suboptimal outcomes. To address this issue, we present X-Dreamer, a
novel approach for high-quality text-to-3D content creation that effectively
bridges the gap between text-to-2D and text-to-3D synthesis. The key components
of X-Dreamer are two innovative designs: Camera-Guided Low-Rank Adaptation
(CG-LoRA) and Attention-Mask Alignment (AMA) Loss. CG-LoRA dynamically
incorporates camera information into the pretrained diffusion models by
employing camera-dependent generation for trainable parameters. This
integration enhances the alignment between the generated 3D assets and the
camera&apos;s perspective. AMA loss guides the attention map of the pretrained
diffusion model using the binary mask of the 3D object, prioritizing the
creation of the foreground object. This module ensures that the model focuses
on generating accurate and detailed foreground objects. Extensive evaluations
demonstrate the effectiveness of our proposed method compared to existing
text-to-3D approaches. Our project webpage:
https://xmuxiaoma666.github.io/Projects/X-Dreamer .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yiwei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yijun Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1&quot;&gt;Jiayi Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haowei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xiaoshuai Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_G/0/1/0/all/0/1&quot;&gt;Guannan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_A/0/1/0/all/0/1&quot;&gt;Annan Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1&quot;&gt;Rongrong Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00092">
<title>Mixture of Gaussian-distributed Prototypes with Generative Modelling for Interpretable Image Classification. (arXiv:2312.00092v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00092</link>
<description rdf:parseType="Literal">&lt;p&gt;Prototypical-part interpretable methods, e.g., ProtoPNet, enhance
interpretability by connecting classification predictions to class-specific
training prototypes, thereby offering an intuitive insight into their
decision-making. Current methods rely on a discriminative classifier trained
with point-based learning techniques that provide specific values for
prototypes. Such prototypes have relatively low representation power due to
their sparsity and potential redundancy, with each prototype containing no
variability measure. In this paper, we present a new generative learning of
prototype distributions, named Mixture of Gaussian-distributed Prototypes
(MGProto), which are represented by Gaussian mixture models (GMM). Such an
approach enables the learning of more powerful prototype representations since
each learned prototype will own a measure of variability, which naturally
reduces the sparsity given the spread of the distribution around each
prototype, and we also integrate a prototype diversity objective function into
the GMM optimisation to reduce redundancy. Incidentally, the generative nature
of MGProto offers a new and effective way for detecting out-of-distribution
samples. To improve the compactness of MGProto, we further propose to prune
Gaussian-distributed prototypes with a low prior. Experiments on CUB-200-2011,
Stanford Cars, Stanford Dogs, and Oxford-IIIT Pets datasets show that MGProto
achieves state-of-the-art classification and OoD detection performances with
encouraging interpretability results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuanhong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fengbei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCarthy_D/0/1/0/all/0/1&quot;&gt;Davis James McCarthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frazer_H/0/1/0/all/0/1&quot;&gt;Helen Frazer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1&quot;&gt;Gustavo Carneiro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00093">
<title>GraphDreamer: Compositional 3D Scene Synthesis from Scene Graphs. (arXiv:2312.00093v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00093</link>
<description rdf:parseType="Literal">&lt;p&gt;As pretrained text-to-image diffusion models become increasingly powerful,
recent efforts have been made to distill knowledge from these text-to-image
pretrained models for optimizing a text-guided 3D model. Most of the existing
methods generate a holistic 3D model from a plain text input. This can be
problematic when the text describes a complex scene with multiple objects,
because the vectorized text embeddings are inherently unable to capture a
complex description with multiple entities and relationships. Holistic 3D
modeling of the entire scene further prevents accurate grounding of text
entities and concepts. To address this limitation, we propose GraphDreamer, a
novel framework to generate compositional 3D scenes from scene graphs, where
objects are represented as nodes and their interactions as edges. By exploiting
node and edge information in scene graphs, our method makes better use of the
pretrained text-to-image diffusion model and is able to fully disentangle
different objects without image-level supervision. To facilitate modeling of
object-wise relationships, we use signed distance fields as representation and
impose a constraint to avoid inter-penetration of objects. To avoid manual
scene graph creation, we design a text prompt for ChatGPT to generate scene
graphs based on text inputs. We conduct both qualitative and quantitative
experiments to validate the effectiveness of GraphDreamer in generating
high-fidelity compositional 3D scenes with disentangled object entities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1&quot;&gt;Gege Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Weiyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1&quot;&gt;Anpei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1&quot;&gt;Andreas Geiger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00094">
<title>Fast ODE-based Sampling for Diffusion Models in Around 5 Steps. (arXiv:2312.00094v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00094</link>
<description rdf:parseType="Literal">&lt;p&gt;Sampling from diffusion models can be treated as solving the corresponding
ordinary differential equations (ODEs), with the aim of obtaining an accurate
solution with as few number of function evaluations (NFE) as possible.
Recently, various fast samplers utilizing higher-order ODE solvers have emerged
and achieved better performance than the initial first-order one. However,
these numerical methods inherently result in certain approximation errors,
which significantly degrades sample quality with extremely small NFE (e.g.,
around 5). In contrast, based on the geometric observation that each sampling
trajectory almost lies in a two-dimensional subspace embedded in the ambient
space, we propose Approximate MEan-Direction Solver (AMED-Solver) that
eliminates truncation errors by directly learning the mean direction for fast
diffusion sampling. Besides, our method can be easily used as a plugin to
further improve existing ODE-based samplers. Extensive experiments on image
synthesis with the resolution ranging from 32 to 256 demonstrate the
effectiveness of our method. With only 5 NFE, we achieve 7.14 FID on CIFAR-10,
13.75 FID on ImageNet 64$\times$64, and 12.79 FID on LSUN Bedroom. Our code is
available at https://github.com/zhyzhouu/amed-solver.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Defang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Can Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00096">
<title>OST: Refining Text Knowledge with Optimal Spatio-Temporal Descriptor for General Video Recognition. (arXiv:2312.00096v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00096</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the resource-intensive nature of training vision-language models on
expansive video data, a majority of studies have centered on adapting
pre-trained image-language models to the video domain. Dominant pipelines
propose to tackle the visual discrepancies with additional temporal learners
while overlooking the substantial discrepancy for web-scaled descriptive
narratives and concise action category names, leading to less distinct semantic
space and potential performance limitations. In this work, we prioritize the
refinement of text knowledge to facilitate generalizable video recognition. To
address the limitations of the less distinct semantic space of category names,
we prompt a large language model (LLM) to augment action class names into
Spatio-Temporal Descriptors thus bridging the textual discrepancy and serving
as a knowledge base for general recognition. Moreover, to assign the best
descriptors with different video instances, we propose Optimal Descriptor
Solver, forming the video recognition problem as solving the optimal matching
flow across frame-level representations and descriptors. Comprehensive
evaluations in zero-shot, few-shot, and fully supervised video recognition
highlight the effectiveness of our approach. Our best model achieves a
state-of-the-art zero-shot accuracy of 75.1% on Kinetics-600.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tongjia Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hongshan Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhengeng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zechuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Wei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00097">
<title>SparseDC: Depth Completion from sparse and non-uniform inputs. (arXiv:2312.00097v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00097</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose SparseDC, a model for Depth Completion of Sparse and non-uniform
depth inputs. Unlike previous methods focusing on completing fixed
distributions on benchmark datasets (e.g., NYU with 500 points, KITTI with 64
lines), SparseDC is specifically designed to handle depth maps with poor
quality in real usage. The key contributions of SparseDC are two-fold. First,
we design a simple strategy, called SFFM, to improve the robustness under
sparse input by explicitly filling the unstable depth features with stable
image features. Second, we propose a two-branch feature embedder to predict
both the precise local geometry of regions with available depth values and
accurate structures in regions with no depth. The key of the embedder is an
uncertainty-based fusion module called UFFM to balance the local and long-term
information extracted by CNNs and ViTs. Extensive indoor and outdoor
experiments demonstrate the robustness of our framework when facing sparse and
non-uniform input depths. The pre-trained model and code are available at
https://github.com/WHU-USI3DV/SparseDC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1&quot;&gt;Chen Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenxiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhe Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haiping Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1&quot;&gt;Zhen Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1&quot;&gt;Zhen Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bisheng Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00098">
<title>Identifying tourist destinations from movie scenes using Deep Learning. (arXiv:2312.00098v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00098</link>
<description rdf:parseType="Literal">&lt;p&gt;Movies wield significant influence in our lives, playing a pivotal role in
the tourism industry of any country. The inclusion of picturesque landscapes,
waterfalls, and mountains as backdrops in films serves to enhance the allure of
specific scenarios. Recognizing the impact of movies on tourism, this paper
introduces a method for identifying tourist destinations featured in films. We
propose the development of a deep learning model capable of recognizing these
locations during movie viewing. The model is trained on a dataset comprising
major tourism destinations worldwide. Through this research, the goal is to
enable viewers to identify the real-world locations depicted in movie scenes,
offering a novel way to connect cinema with global travel experiences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narayanan_M/0/1/0/all/0/1&quot;&gt;Mahendran Narayanan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00101">
<title>Towards Unsupervised Representation Learning: Learning, Evaluating and Transferring Visual Representations. (arXiv:2312.00101v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00101</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised representation learning aims at finding methods that learn
representations from data without annotation-based signals. Abstaining from
annotations not only leads to economic benefits but may - and to some extent
already does - result in advantages regarding the representation&apos;s structure,
robustness, and generalizability to different tasks. In the long run,
unsupervised methods are expected to surpass their supervised counterparts due
to the reduction of human intervention and the inherently more general setup
that does not bias the optimization towards an objective originating from
specific annotation-based signals. While major advantages of unsupervised
representation learning have been recently observed in natural language
processing, supervised methods still dominate in vision domains for most tasks.
In this dissertation, we contribute to the field of unsupervised (visual)
representation learning from three perspectives: (i) Learning representations:
We design unsupervised, backpropagation-free Convolutional Self-Organizing
Neural Networks (CSNNs) that utilize self-organization- and Hebbian-based
learning rules to learn convolutional kernels and masks to achieve deeper
backpropagation-free models. (ii) Evaluating representations: We build upon the
widely used (non-)linear evaluation protocol to define pretext- and
target-objective-independent metrics for measuring and investigating the
objective function mismatch between various unsupervised pretext tasks and
target tasks. (iii) Transferring representations: We contribute CARLANE, the
first 3-way sim-to-real domain adaptation benchmark for 2D lane detection, and
a method based on prototypical self-supervised learning. Finally, we contribute
a content-consistent unpaired image-to-image translation method that utilizes
masks, global and local discriminators, and similarity sampling to mitigate
content inconsistencies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stuhr_B/0/1/0/all/0/1&quot;&gt;Bonifaz Stuhr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00105">
<title>Improving the Robustness of Quantized Deep Neural Networks to White-Box Attacks using Stochastic Quantization and Information-Theoretic Ensemble Training. (arXiv:2312.00105v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00105</link>
<description rdf:parseType="Literal">&lt;p&gt;Most real-world applications that employ deep neural networks (DNNs) quantize
them to low precision to reduce the compute needs. We present a method to
improve the robustness of quantized DNNs to white-box adversarial attacks. We
first tackle the limitation of deterministic quantization to fixed ``bins&apos;&apos; by
introducing a differentiable Stochastic Quantizer (SQ). We explore the
hypothesis that different quantizations may collectively be more robust than
each quantized DNN. We formulate a training objective to encourage different
quantized DNNs to learn different representations of the input image. The
training objective captures diversity and accuracy via mutual information
between ensemble members. Through experimentation, we demonstrate substantial
improvement in robustness against $L_\infty$ attacks even if the attacker is
allowed to backpropagate through SQ (e.g., &amp;gt; 50\% accuracy to PGD(5/255) on
CIFAR10 without adversarial training), compared to vanilla DNNs as well as
existing ensembles of quantized DNNs. We extend the method to detect attacks
and generate robustness profiles in the adversarial information plane (AIP),
towards a unified analysis of different threat models by correlating the MI and
accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farkya_S/0/1/0/all/0/1&quot;&gt;Saurabh Farkya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raghavan_A/0/1/0/all/0/1&quot;&gt;Aswin Raghavan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziskind_A/0/1/0/all/0/1&quot;&gt;Avi Ziskind&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00109">
<title>Scaffold-GS: Structured 3D Gaussians for View-Adaptive Rendering. (arXiv:2312.00109v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00109</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural rendering methods have significantly advanced photo-realistic 3D scene
rendering in various academic and industrial applications. The recent 3D
Gaussian Splatting method has achieved the state-of-the-art rendering quality
and speed combining the benefits of both primitive-based representations and
volumetric representations. However, it often leads to heavily redundant
Gaussians that try to fit every training view, neglecting the underlying scene
geometry. Consequently, the resulting model becomes less robust to significant
view changes, texture-less area and lighting effects. We introduce Scaffold-GS,
which uses anchor points to distribute local 3D Gaussians, and predicts their
attributes on-the-fly based on viewing direction and distance within the view
frustum. Anchor growing and pruning strategies are developed based on the
importance of neural Gaussians to reliably improve the scene coverage. We show
that our method effectively reduces redundant Gaussians while delivering
high-quality rendering. We also demonstrates an enhanced capability to
accommodate scenes with varying levels-of-detail and view-dependent
observations, without sacrificing the rendering speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1&quot;&gt;Tao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1&quot;&gt;Mulin Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Linning Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiangli_Y/0/1/0/all/0/1&quot;&gt;Yuanbo Xiangli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Limin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dahua Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1&quot;&gt;Bo Dai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00110">
<title>CLIP-QDA: An Explainable Concept Bottleneck Model. (arXiv:2312.00110v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00110</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce an explainable algorithm designed from a
multi-modal foundation model, that performs fast and explainable image
classification. Drawing inspiration from CLIP-based Concept Bottleneck Models
(CBMs), our method creates a latent space where each neuron is linked to a
specific word. Observing that this latent space can be modeled with simple
distributions, we use a Mixture of Gaussians (MoG) formalism to enhance the
interpretability of this latent space. Then, we introduce CLIP-QDA, a
classifier that only uses statistical values to infer labels from the concepts.
In addition, this formalism allows for both local and global explanations.
These explanations come from the inner design of our architecture, our work is
part of a new family of greybox models, combining performances of opaque
foundation models and the interpretability of transparent models. Our empirical
findings show that in instances where the MoG assumption holds, CLIP-QDA
achieves similar accuracy with state-of-the-art methods CBMs. Our explanations
compete with existing XAI methods while being faster to compute.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kazmierczak_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;mi Kazmierczak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berthier_E/0/1/0/all/0/1&quot;&gt;Elo&amp;#xef;se Berthier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frehse_G/0/1/0/all/0/1&quot;&gt;Goran Frehse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Franchi_G/0/1/0/all/0/1&quot;&gt;Gianni Franchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00112">
<title>DynMF: Neural Motion Factorization for Real-time Dynamic View Synthesis with 3D Gaussian Splatting. (arXiv:2312.00112v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00112</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurately and efficiently modeling dynamic scenes and motions is considered
so challenging a task due to temporal dynamics and motion complexity. To
address these challenges, we propose DynMF, a compact and efficient
representation that decomposes a dynamic scene into a few neural trajectories.
We argue that the per-point motions of a dynamic scene can be decomposed into a
small set of explicit or learned trajectories. Our carefully designed neural
framework consisting of a tiny set of learned basis queried only in time allows
for rendering speed similar to 3D Gaussian Splatting, surpassing 120 FPS, while
at the same time, requiring only double the storage compared to static scenes.
Our neural representation adequately constrains the inherently underconstrained
motion field of a dynamic scene leading to effective and fast optimization.
This is done by biding each point to motion coefficients that enforce the
per-point sharing of basis trajectories. By carefully applying a sparsity loss
to the motion coefficients, we are able to disentangle the motions that
comprise the scene, independently control them, and generate novel motion
combinations that have never been seen before. We can reach state-of-the-art
render quality within just 5 minutes of training and in less than half an hour,
we can synthesize novel views of dynamic scenes with superior photorealistic
quality. Our representation is interpretable, efficient, and expressive enough
to offer real-time view synthesis of complex dynamic scene motions, in
monocular and multi-view scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kratimenos_A/0/1/0/all/0/1&quot;&gt;Agelos Kratimenos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1&quot;&gt;Jiahui Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1&quot;&gt;Kostas Daniilidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00113">
<title>Event-based Continuous Color Video Decompression from Single Frames. (arXiv:2312.00113v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00113</link>
<description rdf:parseType="Literal">&lt;p&gt;We present ContinuityCam, a novel approach to generate a continuous video
from a single static RGB image, using an event camera. Conventional cameras
struggle with high-speed motion capture due to bandwidth and dynamic range
limitations. Event cameras are ideal sensors to solve this problem because they
encode compressed change information at high temporal resolution. In this work,
we propose a novel task called event-based continuous color video
decompression, pairing single static color frames and events to reconstruct
temporally continuous videos. Our approach combines continuous long-range
motion modeling with a feature-plane-based synthesis neural integration model,
enabling frame prediction at arbitrary times within the events. Our method does
not rely on additional frames except for the initial image, increasing, thus,
the robustness to sudden light changes, minimizing the prediction latency, and
decreasing the bandwidth requirement. We introduce a novel single objective
beamsplitter setup that acquires aligned images and events and a novel and
challenging Event Extreme Decompression Dataset (E2D2) that tests the method in
various lighting and motion profiles. We thoroughly evaluate our method through
benchmarking reconstruction as well as various downstream tasks. Our approach
significantly outperforms the event- and image- based baselines in the proposed
task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziyun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamann_F/0/1/0/all/0/1&quot;&gt;Friedhelm Hamann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaney_K/0/1/0/all/0/1&quot;&gt;Kenneth Chaney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1&quot;&gt;Wen Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1&quot;&gt;Guillermo Gallego&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1&quot;&gt;Kostas Daniilidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00114">
<title>Un-EvMoSeg: Unsupervised Event-based Independent Motion Segmentation. (arXiv:2312.00114v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00114</link>
<description rdf:parseType="Literal">&lt;p&gt;Event cameras are a novel type of biologically inspired vision sensor known
for their high temporal resolution, high dynamic range, and low power
consumption. Because of these properties, they are well-suited for processing
fast motions that require rapid reactions. Although event cameras have recently
shown competitive performance in unsupervised optical flow estimation,
performance in detecting independently moving objects (IMOs) is lacking behind,
although event-based methods would be suited for this task based on their low
latency and HDR properties. Previous approaches to event-based IMO segmentation
have been heavily dependent on labeled data. However, biological vision systems
have developed the ability to avoid moving objects through daily tasks without
being given explicit labels. In this work, we propose the first event framework
that generates IMO pseudo-labels using geometric constraints. Due to its
unsupervised nature, our method can handle an arbitrary number of not
predetermined objects and is easily scalable to datasets where expensive IMO
labels are not readily available. We evaluate our approach on the EVIMO dataset
and show that it performs competitively with supervised methods, both
quantitatively and qualitatively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziyun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jinyuan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1&quot;&gt;Kostas Daniilidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00115">
<title>A Video is Worth 10,000 Words: Training and Benchmarking with Diverse Captions for Better Long Video Retrieval. (arXiv:2312.00115v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00115</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing long video retrieval systems are trained and tested in the
paragraph-to-video retrieval regime, where every long video is described by a
single long paragraph. This neglects the richness and variety of possible valid
descriptions of a video, which could be described in moment-by-moment detail,
or in a single phrase summary, or anything in between. To provide a more
thorough evaluation of the capabilities of long video retrieval systems, we
propose a pipeline that leverages state-of-the-art large language models to
carefully generate a diverse set of synthetic captions for long videos. We
validate this pipeline&apos;s fidelity via rigorous human inspection. We then
benchmark a representative set of video language models on these synthetic
captions using a few long video datasets, showing that they struggle with the
transformed data, especially the shortest captions. We also propose a
lightweight fine-tuning method, where we use a contrastive loss to learn a
hierarchical embedding loss based on the differing levels of information among
the various captions. Our method improves performance both on the downstream
paragraph-to-video retrieval task (+1.1% R@1 on ActivityNet), as well as for
the various long video retrieval metrics we compute using our synthetic data
(+3.6% R@1 for short descriptions on ActivityNet). For data access and other
details, please refer to our project website at
https://mgwillia.github.io/10k-words.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gwilliam_M/0/1/0/all/0/1&quot;&gt;Matthew Gwilliam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cogswell_M/0/1/0/all/0/1&quot;&gt;Michael Cogswell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1&quot;&gt;Meng Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sikka_K/0/1/0/all/0/1&quot;&gt;Karan Sikka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1&quot;&gt;Abhinav Shrivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Divakaran_A/0/1/0/all/0/1&quot;&gt;Ajay Divakaran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00116">
<title>S2ST: Image-to-Image Translation in the Seed Space of Latent Diffusion. (arXiv:2312.00116v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00116</link>
<description rdf:parseType="Literal">&lt;p&gt;Image-to-image translation (I2IT) refers to the process of transforming
images from a source domain to a target domain while maintaining a fundamental
connection in terms of image content. In the past few years, remarkable
advancements in I2IT were achieved by Generative Adversarial Networks (GANs),
which nevertheless struggle with translations requiring high precision.
Recently, Diffusion Models have established themselves as the engine of choice
for image generation. In this paper we introduce S2ST, a novel framework
designed to accomplish global I2IT in complex photorealistic images, such as
day-to-night or clear-to-rain translations of automotive scenes. S2ST operates
within the seed space of a Latent Diffusion Model, thereby leveraging the
powerful image priors learned by the latter. We show that S2ST surpasses
state-of-the-art GAN-based I2IT methods, as well as diffusion-based approaches,
for complex automotive scenes, improving fidelity while respecting the target
domain&apos;s appearance across a variety of domains. Notably, S2ST obviates the
necessity for training domain-specific translation networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Greenberg_O/0/1/0/all/0/1&quot;&gt;Or Greenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kishon_E/0/1/0/all/0/1&quot;&gt;Eran Kishon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lischinski_D/0/1/0/all/0/1&quot;&gt;Dani Lischinski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00151">
<title>Which way is `right&apos;?: Uncovering limitations of Vision-and-Language Navigation model. (arXiv:2312.00151v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00151</link>
<description rdf:parseType="Literal">&lt;p&gt;The challenging task of Vision-and-Language Navigation (VLN) requires
embodied agents to follow natural language instructions to reach a goal
location or object (e.g. `walk down the hallway and turn left at the piano&apos;).
For agents to complete this task successfully, they must be able to ground
objects referenced into the instruction (e.g.`piano&apos;) into the visual scene as
well as ground directional phrases (e.g.`turn left&apos;) into actions. In this work
we ask the following question -- to what degree are spatial and directional
language cues informing the navigation model&apos;s decisions? We propose a series
of simple masking experiments to inspect the model&apos;s reliance on different
parts of the instruction. Surprisingly we uncover that certain top performing
models rely only on the noun tokens of the instructions. We propose two
training methods to alleviate this concerning limitation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hahn_M/0/1/0/all/0/1&quot;&gt;Meera Hahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raj_A/0/1/0/all/0/1&quot;&gt;Amit Raj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rehg_J/0/1/0/all/0/1&quot;&gt;James M. Rehg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00157">
<title>Universal Backdoor Attacks. (arXiv:2312.00157v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.00157</link>
<description rdf:parseType="Literal">&lt;p&gt;Web-scraped datasets are vulnerable to data poisoning, which can be used for
backdooring deep image classifiers during training. Since training on large
datasets is expensive, a model is trained once and re-used many times. Unlike
adversarial examples, backdoor attacks often target specific classes rather
than any class learned by the model. One might expect that targeting many
classes through a naive composition of attacks vastly increases the number of
poison samples. We show this is not necessarily true and more efficient,
universal data poisoning attacks exist that allow controlling
misclassifications from any source class into any target class with a small
increase in poison samples. Our idea is to generate triggers with salient
characteristics that the model can learn. The triggers we craft exploit a
phenomenon we call inter-class poison transferability, where learning a trigger
from one class makes the model more vulnerable to learning triggers for other
classes. We demonstrate the effectiveness and robustness of our universal
backdoor attacks by controlling models with up to 6,000 classes while poisoning
only 0.15% of the training dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_B/0/1/0/all/0/1&quot;&gt;Benjamin Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lukas_N/0/1/0/all/0/1&quot;&gt;Nils Lukas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kerschbaum_F/0/1/0/all/0/1&quot;&gt;Florian Kerschbaum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00169">
<title>Integration of Swin UNETR and statistical shape modeling for a semi-automated segmentation of the knee and biomechanical modeling of articular cartilage. (arXiv:2312.00169v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00169</link>
<description rdf:parseType="Literal">&lt;p&gt;Simulation studies like finite element (FE) modeling provide insight into
knee joint mechanics without patient experimentation. Generic FE models
represent biomechanical behavior of the tissue by overlooking variations in
geometry, loading, and material properties of a population. On the other hand,
subject-specific models include these specifics, resulting in enhanced
predictive precision. However, creating such models is laborious and
time-intensive. The present study aimed to enhance subject-specific knee joint
FE modeling by incorporating a semi-automated segmentation algorithm. This
segmentation was a 3D Swin UNETR for an initial segmentation of the femur and
tibia, followed by a statistical shape model (SSM) adjustment to improve
surface roughness and continuity. Five hundred and seven magnetic resonance
images (MRIs) from the Osteoarthritis Initiative (OAI) database were used to
build and validate the segmentation model. A semi-automated FE model was
developed using this semi-automated segmentation. On the other hand, a manual
FE model was developed through manual segmentation (i.e., the gold standard
approach). Both FE models were subjected to gait loading. The predicted
mechanical response of manual and semi-automated FE models were compared. In
the result, our semi-automated segmentation achieved Dice similarity
coefficient (DSC) over 98% for both femur and tibia. The mechanical results
(max principal stress, max principal strain, fluid pressure, fibril strain, and
contact area) showed no significant differences between the manual and
semi-automated FE models, indicating the effectiveness of the proposed
semi-automated segmentation in creating accurate knee joint FE models. (
https://data.mendeley.com/datasets/k5hdc9cz7w/1 ).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kakavand_R/0/1/0/all/0/1&quot;&gt;Reza Kakavand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palizi_M/0/1/0/all/0/1&quot;&gt;Mehrdad Palizi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tahghighi_P/0/1/0/all/0/1&quot;&gt;Peyman Tahghighi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmadi_R/0/1/0/all/0/1&quot;&gt;Reza Ahmadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gianchandani_N/0/1/0/all/0/1&quot;&gt;Neha Gianchandani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adeeb_S/0/1/0/all/0/1&quot;&gt;Samer Adeeb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Souza_R/0/1/0/all/0/1&quot;&gt;Roberto Souza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Edwards_W/0/1/0/all/0/1&quot;&gt;W. Brent Edwards&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Komeili_A/0/1/0/all/0/1&quot;&gt;Amin Komeili&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00173">
<title>Fool the Hydra: Adversarial Attacks against Multi-view Object Detection Systems. (arXiv:2312.00173v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00173</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial patches exemplify the tangible manifestation of the threat posed
by adversarial attacks on Machine Learning (ML) models in real-world scenarios.
Robustness against these attacks is of the utmost importance when designing
computer vision applications, especially for safety-critical domains such as
CCTV systems. In most practical situations, monitoring open spaces requires
multi-view systems to overcome acquisition challenges such as occlusion
handling. Multiview object systems are able to combine data from multiple
views, and reach reliable detection results even in difficult environments.
Despite its importance in real-world vision applications, the vulnerability of
multiview systems to adversarial patches is not sufficiently investigated. In
this paper, we raise the following question: Does the increased performance and
information sharing across views offer as a by-product robustness to
adversarial patches? We first conduct a preliminary analysis showing promising
robustness against off-the-shelf adversarial patches, even in an extreme
setting where we consider patches applied to all views by all persons in
Wildtrack benchmark. However, we challenged this observation by proposing two
new attacks: (i) In the first attack, targeting a multiview CNN, we maximize
the global loss by proposing gradient projection to the different views and
aggregating the obtained local gradients. (ii) In the second attack, we focus
on a Transformer-based multiview framework. In addition to the focal loss, we
also maximize the transformer-specific loss by dissipating its attention
blocks. Our results show a large degradation in the detection performance of
victim multiview systems with our first patch attack reaching an attack success
rate of 73% , while our second proposed attack reduced the performance of its
target detector by 62%
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tarchoun_B/0/1/0/all/0/1&quot;&gt;Bilel Tarchoun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alam_Q/0/1/0/all/0/1&quot;&gt;Quazi Mishkatul Alam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abu_Ghazaleh_N/0/1/0/all/0/1&quot;&gt;Nael Abu-Ghazaleh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alouani_I/0/1/0/all/0/1&quot;&gt;Ihsen Alouani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00174">
<title>Compression of end-to-end non-autoregressive image-to-speech system for low-resourced devices. (arXiv:2312.00174v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2312.00174</link>
<description rdf:parseType="Literal">&lt;p&gt;People with visual impairments have difficulty accessing touchscreen-enabled
personal computing devices like mobile phones and laptops. The image-to-speech
(ITS) systems can assist them in mitigating this problem, but their huge model
size makes it extremely hard to be deployed on low-resourced embedded devices.
In this paper, we aim to overcome this challenge by developing an efficient
endto-end neural architecture for generating audio from tiny segments of
display content on low-resource devices. We introduced a vision
transformers-based image encoder and utilized knowledge distillation to
compress the model from 6.1 million to 2.46 million parameters. Human and
automatic evaluation results show that our approach leads to a very minimal
drop in performance and can speed up the inference time by 22%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Srinivasagan_G/0/1/0/all/0/1&quot;&gt;Gokul Srinivasagan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Deisher_M/0/1/0/all/0/1&quot;&gt;Michael Deisher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Georges_M/0/1/0/all/0/1&quot;&gt;Munir Georges&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00184">
<title>Galaxy Classification: A machine learning approach for classifying shapes using numerical data. (arXiv:2312.00184v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00184</link>
<description rdf:parseType="Literal">&lt;p&gt;The classification of galaxies as spirals or ellipticals is a crucial task in
understanding their formation and evolution. With the arrival of large-scale
astronomical surveys, such as the Sloan Digital Sky Survey (SDSS), astronomers
now have access to images of a vast number of galaxies. However, the visual
inspection of these images is an impossible task for humans due to the sheer
number of galaxies to be analyzed. To solve this problem, the Galaxy Zoo
project was created to engage thousands of citizen scientists to classify the
galaxies based on their visual features. In this paper, we present a machine
learning model for galaxy classification using numerical data from the Galaxy
Zoo[5] project. Our model utilizes a convolutional neural network architecture
to extract features from galaxy images and classify them into spirals or
ellipticals. We demonstrate the effectiveness of our model by comparing its
performance with that of human classifiers using a subset of the Galaxy Zoo
dataset. Our results show that our model achieves high accuracy in classifying
galaxies and has the potential to significantly enhance our understanding of
the formation and evolution of galaxies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guruprasad_A/0/1/0/all/0/1&quot;&gt;Anusha Guruprasad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00188">
<title>REACT: Recognize Every Action Everywhere All At Once. (arXiv:2312.00188v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00188</link>
<description rdf:parseType="Literal">&lt;p&gt;Group Activity Recognition (GAR) is a fundamental problem in computer vision,
with diverse applications in sports video analysis, video surveillance, and
social scene understanding. Unlike conventional action recognition, GAR aims to
classify the actions of a group of individuals as a whole, requiring a deep
understanding of their interactions and spatiotemporal relationships. To
address the challenges in GAR, we present REACT (\textbf{R}ecognize
\textbf{E}very \textbf{Act}ion Everywhere All At Once), a novel architecture
inspired by the transformer encoder-decoder model explicitly designed to model
complex contextual relationships within videos, including multi-modality and
spatio-temporal features. Our architecture features a cutting-edge
Vision-Language Encoder block for integrated temporal, spatial, and multi-modal
interaction modeling. This component efficiently encodes spatiotemporal
interactions, even with sparsely sampled frames, and recovers essential local
information. Our Action Decoder Block refines the joint understanding of text
and video data, allowing us to precisely retrieve bounding boxes, enhancing the
link between semantics and visual reality. At the core, our Actor Fusion Block
orchestrates a fusion of actor-specific data and textual features, striking a
balance between specificity and context. Our method outperforms
state-of-the-art GAR approaches in extensive experiments, demonstrating
superior accuracy in recognizing and understanding group activities. Our
architecture&apos;s potential extends to diverse real-world applications, offering
empirical evidence of its performance gains. This work significantly advances
the field of group activity recognition, providing a robust framework for
nuanced scene comprehension.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chappa_N/0/1/0/all/0/1&quot;&gt;Naga VS Raviteja Chappa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1&quot;&gt;Pha Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dobbs_P/0/1/0/all/0/1&quot;&gt;Page Daniel Dobbs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luu_K/0/1/0/all/0/1&quot;&gt;Khoa Luu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00192">
<title>Benchmarking and Enhancing Disentanglement in Concept-Residual Models. (arXiv:2312.00192v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.00192</link>
<description rdf:parseType="Literal">&lt;p&gt;Concept bottleneck models (CBMs) are interpretable models that first predict
a set of semantically meaningful features, i.e., concepts, from observations
that are subsequently used to condition a downstream task. However, the model&apos;s
performance strongly depends on the engineered features and can severely suffer
from incomplete sets of concepts. Prior works have proposed a side channel -- a
residual -- that allows for unconstrained information flow to the downstream
task, thus improving model performance but simultaneously introducing
information leakage, which is undesirable for interpretability. This work
proposes three novel approaches to mitigate information leakage by
disentangling concepts and residuals, investigating the critical balance
between model performance and interpretability. Through extensive empirical
analysis on the CUB, OAI, and CIFAR 100 datasets, we assess the performance of
each disentanglement method and provide insights into when they work best.
Further, we show how each method impacts the ability to intervene over the
concepts and their subsequent impact on task performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zabounidis_R/0/1/0/all/0/1&quot;&gt;Renos Zabounidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oguntola_I/0/1/0/all/0/1&quot;&gt;Ini Oguntola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1&quot;&gt;Konghao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Campbell_J/0/1/0/all/0/1&quot;&gt;Joseph Campbell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stepputtis_S/0/1/0/all/0/1&quot;&gt;Simon Stepputtis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sycara_K/0/1/0/all/0/1&quot;&gt;Katia Sycara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00195">
<title>Raising the Bar of AI-generated Image Detection with CLIP. (arXiv:2312.00195v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00195</link>
<description rdf:parseType="Literal">&lt;p&gt;Aim of this work is to explore the potential of pre-trained vision-language
models (VLMs) for universal detection of AI-generated images. We develop a
lightweight detection strategy based on CLIP features and study its performance
in a wide variety of challenging scenarios. We find that, unlike previous
belief, it is neither necessary nor convenient to use a large domain-specific
dataset for training. On the contrary, by using only a handful of example
images from a single generative model, a CLIP-based detector exhibits a
surprising generalization ability and high robustness across several different
architectures, including recent commercial tools such as Dalle-3, Midjourney
v5, and Firefly. We match the SoTA on in-distribution data, and improve largely
above it in terms of generalization to out-of-distribution data (+6% in terms
of AUC) and robustness to impaired/laundered data (+13%). Our project is
available at https://grip-unina.github.io/ClipBased-SyntheticImageDetection/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cozzolino_D/0/1/0/all/0/1&quot;&gt;Davide Cozzolino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poggi_G/0/1/0/all/0/1&quot;&gt;Giovanni Poggi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corvi_R/0/1/0/all/0/1&quot;&gt;Riccardo Corvi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1&quot;&gt;Matthias Nie&amp;#xdf;ner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verdoliva_L/0/1/0/all/0/1&quot;&gt;Luisa Verdoliva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00204">
<title>DNS SLAM: Dense Neural Semantic-Informed SLAM. (arXiv:2312.00204v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00204</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, coordinate-based neural implicit representations have shown
promising results for the task of Simultaneous Localization and Mapping (SLAM).
While achieving impressive performance on small synthetic scenes, these methods
often suffer from oversmoothed reconstructions, especially for complex
real-world scenes. In this work, we introduce DNS SLAM, a novel neural RGB-D
semantic SLAM approach featuring a hybrid representation. Relying only on 2D
semantic priors, we propose the first semantic neural SLAM method that trains
class-wise scene representations while providing stable camera tracking at the
same time. Our method integrates multi-view geometry constraints with
image-based feature extraction to improve appearance details and to output
color, density, and semantic class information, enabling many downstream
applications. To further enable real-time tracking, we introduce a lightweight
coarse scene representation which is trained in a self-supervised manner in
latent space. Our experimental results achieve state-of-the-art performance on
both synthetic data and real-world data tracking while maintaining a
commendable operational speed on off-the-shelf hardware. Further, our method
outputs class-wise decomposed reconstructions with better texture capturing
appearance and geometric details.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kunyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niemeyer_M/0/1/0/all/0/1&quot;&gt;Michael Niemeyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1&quot;&gt;Nassir Navab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1&quot;&gt;Federico Tombari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00206">
<title>SparseGS: Real-Time 360{\deg} Sparse View Synthesis using Gaussian Splatting. (arXiv:2312.00206v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00206</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of novel view synthesis has grown significantly in popularity
recently with the introduction of Neural Radiance Fields (NeRFs) and other
implicit scene representation methods. A recent advance, 3D Gaussian Splatting
(3DGS), leverages an explicit representation to achieve real-time rendering
with high-quality results. However, 3DGS still requires an abundance of
training views to generate a coherent scene representation. In few shot
settings, similar to NeRF, 3DGS tends to overfit to training views, causing
background collapse and excessive floaters, especially as the number of
training views are reduced. We propose a method to enable training coherent
3DGS-based radiance fields of 360 scenes from sparse training views. We find
that using naive depth priors is not sufficient and integrate depth priors with
generative and explicit constraints to reduce background collapse, remove
floaters, and enhance consistency from unseen viewpoints. Experiments show that
our method outperforms base 3DGS by up to 30.5% and NeRF-based methods by up to
15.6% in LPIPS on the MipNeRF-360 dataset with substantially less training and
inference cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Haolin Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muttukuru_S/0/1/0/all/0/1&quot;&gt;Sairisheek Muttukuru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Upadhyay_R/0/1/0/all/0/1&quot;&gt;Rishi Upadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chari_P/0/1/0/all/0/1&quot;&gt;Pradyumna Chari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kadambi_A/0/1/0/all/0/1&quot;&gt;Achuta Kadambi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00210">
<title>DREAM: Diffusion Rectification and Estimation-Adaptive Models. (arXiv:2312.00210v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00210</link>
<description rdf:parseType="Literal">&lt;p&gt;We present DREAM, a novel training framework representing Diffusion
Rectification and Estimation-Adaptive Models, requiring minimal code changes
(just three lines) yet significantly enhancing the alignment of training with
sampling in diffusion models. DREAM features two components: diffusion
rectification, which adjusts training to reflect the sampling process, and
estimation adaptation, which balances perception against distortion. When
applied to image super-resolution (SR), DREAM adeptly navigates the tradeoff
between minimizing distortion and preserving high image quality. Experiments
demonstrate DREAM&apos;s superiority over standard diffusion-based SR methods,
showing a $2$ to $3\times $ faster training convergence and a $10$ to
$20\times$ reduction in necessary sampling steps to achieve comparable or
superior results. We hope DREAM will inspire a rethinking of diffusion model
training paradigms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jinxin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_T/0/1/0/all/0/1&quot;&gt;Tianyu Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jiachen Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zharkov_I/0/1/0/all/0/1&quot;&gt;Ilya Zharkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhihui Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1&quot;&gt;Luming Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00220">
<title>Multi-Modal Video Topic Segmentation with Dual-Contrastive Domain Adaptation. (arXiv:2312.00220v1 [cs.MM])</title>
<link>http://arxiv.org/abs/2312.00220</link>
<description rdf:parseType="Literal">&lt;p&gt;Video topic segmentation unveils the coarse-grained semantic structure
underlying videos and is essential for other video understanding tasks. Given
the recent surge in multi-modal, relying solely on a single modality is
arguably insufficient. On the other hand, prior solutions for similar tasks
like video scene/shot segmentation cater to short videos with clear visual
shifts but falter for long videos with subtle changes, such as livestreams. In
this paper, we introduce a multi-modal video topic segmenter that utilizes both
video transcripts and frames, bolstered by a cross-modal attention mechanism.
Furthermore, we propose a dual-contrastive learning framework adhering to the
unsupervised domain adaptation paradigm, enhancing our model&apos;s adaptability to
longer, more semantically complex videos. Experiments on short and long video
corpora demonstrate that our proposed solution, significantly surpasses
baseline methods in terms of both accuracy and transferability, in both intra-
and cross-domain settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1&quot;&gt;Linzi Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1&quot;&gt;Quan Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caba_F/0/1/0/all/0/1&quot;&gt;Fabian Caba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1&quot;&gt;Franck Dernoncourt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1&quot;&gt;Seunghyun Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhaowen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1&quot;&gt;Trung Bui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carenini_G/0/1/0/all/0/1&quot;&gt;Giuseppe Carenini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00223">
<title>Convolutional Neural Networks for Segmentation of Malignant Pleural Mesothelioma: Analysis of Probability Map Thresholds (CALGB 30901, Alliance). (arXiv:2312.00223v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.00223</link>
<description rdf:parseType="Literal">&lt;p&gt;Malignant pleural mesothelioma (MPM) is the most common form of mesothelioma.
To assess response to treatment, tumor measurements are acquired and evaluated
based on a patient&apos;s longitudinal computed tomography (CT) scans. Tumor volume,
however, is the more accurate metric for assessing tumor burden and response.
Automated segmentation methods using deep learning can be employed to acquire
volume, which otherwise is a tedious task performed manually. The deep
learning-based tumor volume and contours can then be compared with a standard
reference to assess the robustness of the automated segmentations. The purpose
of this study was to evaluate the impact of probability map threshold on MPM
tumor delineations generated using a convolutional neural network (CNN).
Eighty-eight CT scans from 21 MPM patients were segmented by a VGG16/U-Net CNN.
A radiologist modified the contours generated at a 0.5 probability threshold.
Percent difference of tumor volume and overlap using the Dice Similarity
Coefficient (DSC) were compared between the standard reference provided by the
radiologist and CNN outputs for thresholds ranging from 0.001 to 0.9. CNN
annotations consistently yielded smaller tumor volumes than radiologist
contours. Reducing the probability threshold from 0.5 to 0.1 decreased the
absolute percent volume difference, on average, from 43.96% to 24.18%. Median
and mean DSC ranged from 0.58 to 0.60, with a peak at a threshold of 0.5; no
distinct threshold was found for percent volume difference. No single output
threshold in the CNN probability maps was optimal for both tumor volume and
DSC. This work underscores the need to assess tumor volume and spatial overlap
when evaluating CNN performance. While automated segmentations may yield
comparable tumor volumes to that of the reference standard, the spatial region
delineated by the CNN at a specific threshold is equally important.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shenouda_M/0/1/0/all/0/1&quot;&gt;Mena Shenouda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gudmundsson_E/0/1/0/all/0/1&quot;&gt;Eyj&amp;#xf3;lfur Gudmundsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Feng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Straus_C/0/1/0/all/0/1&quot;&gt;Christopher M. Straus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kindler_H/0/1/0/all/0/1&quot;&gt;Hedy L. Kindler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dudek_A/0/1/0/all/0/1&quot;&gt;Arkadiusz Z. Dudek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Stinchcombe_T/0/1/0/all/0/1&quot;&gt;Thomas Stinchcombe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaofei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Starkey_A/0/1/0/all/0/1&quot;&gt;Adam Starkey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Armato_S/0/1/0/all/0/1&quot;&gt;Samuel G. Armato III&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00224">
<title>Unsupervised textile defect detection using convolutional neural networks. (arXiv:2312.00224v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00224</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we propose a novel motif-based approach for unsupervised
textile anomaly detection that combines the benefits of traditional
convolutional neural networks with those of an unsupervised learning paradigm.
It consists of five main steps: preprocessing, automatic pattern period
extraction, patch extraction, features selection and anomaly detection. This
proposed approach uses a new dynamic and heuristic method for feature selection
which avoids the drawbacks of initialization of the number of filters (neurons)
and their weights, and those of the backpropagation mechanism such as the
vanishing gradients, which are common practice in the state-of-the-art methods.
The design and training of the network are performed in a dynamic and input
domain-based manner and, thus, no ad-hoc configurations are required. Before
building the model, only the number of layers and the stride are defined. We do
not initialize the weights randomly nor do we define the filter size or number
of filters as conventionally done in CNN-based approaches. This reduces effort
and time spent on hyperparameter initialization and fine-tuning. Only one
defect-free sample is required for training and no further labeled data is
needed. The trained network is then used to detect anomalies on defective
fabric samples. We demonstrate the effectiveness of our approach on the
Patterned Fabrics benchmark dataset. Our algorithm yields reliable and
competitive results (on recall, precision, accuracy and f1- measure) compared
to state-of-the-art unsupervised approaches, in less time, with efficient
training in a single epoch and a lower computational cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koulali_I/0/1/0/all/0/1&quot;&gt;Imane Koulali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eskil_M/0/1/0/all/0/1&quot;&gt;M. Taner Eskil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00236">
<title>Brainformer: Modeling MRI Brain Functions to Machine Vision. (arXiv:2312.00236v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00236</link>
<description rdf:parseType="Literal">&lt;p&gt;&quot;Perception is reality&quot;. Human perception plays a vital role in forming
beliefs and understanding reality. Exploring how the human brain works in the
visual system facilitates bridging the gap between human visual perception and
computer vision models. However, neuroscientists study the brain via
Neuroimaging, i.e., Functional Magnetic Resonance Imaging (fMRI), to discover
the brain&apos;s functions. These approaches face interpretation challenges where
fMRI data can be complex and require expertise. Therefore, neuroscientists make
inferences about cognitive processes based on patterns of brain activities,
which can lead to potential misinterpretation or limited functional
understanding. In this work, we first present a simple yet effective
Brainformer approach, a novel Transformer-based framework, to analyze the
patterns of fMRI in the human perception system from the machine learning
perspective. Secondly, we introduce a novel mechanism incorporating fMRI, which
represents the human brain activities, as the supervision for the machine
vision model. This work also introduces a novel perspective on transferring
knowledge from human perception to neural networks. Through our experiments, we
demonstrated that by leveraging fMRI information, the machine vision model can
achieve potential results compared to the current State-of-the-art methods in
various image recognition tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_X/0/1/0/all/0/1&quot;&gt;Xuan-Bac Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Samee U. Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luu_K/0/1/0/all/0/1&quot;&gt;Khoa Luu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00250">
<title>Advancements and Trends in Ultra-High-Resolution Image Processing: An Overview. (arXiv:2312.00250v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00250</link>
<description rdf:parseType="Literal">&lt;p&gt;Currently, to further improve visual enjoyment, Ultra-High-Definition (UHD)
images are catching wide attention. Here, UHD images are usually referred to as
having a resolution greater than or equal to $3840 \times 2160$. However, since
the imaging equipment is subject to environmental noise or equipment jitter,
UHD images are prone to contrast degradation, blurring, low dynamic range, etc.
To address these issues, a large number of algorithms for UHD image enhancement
have been proposed. In this paper, we introduce the current state of UHD image
enhancement from two perspectives, one is the application field and the other
is the technology. In addition, we briefly explore its trends.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1&quot;&gt;Zhuoran Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1&quot;&gt;Boxue Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00252">
<title>PyNeRF: Pyramidal Neural Radiance Fields. (arXiv:2312.00252v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00252</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Fields (NeRFs) can be dramatically accelerated by spatial
grid representations. However, they do not explicitly reason about scale and so
introduce aliasing artifacts when reconstructing scenes captured at different
camera distances. Mip-NeRF and its extensions propose scale-aware renderers
that project volumetric frustums rather than point samples but such approaches
rely on positional encodings that are not readily compatible with grid methods.
We propose a simple modification to grid-based models by training model heads
at different spatial grid resolutions. At render time, we simply use coarser
grids to render samples that cover larger volumes. Our method can be easily
applied to existing accelerated NeRF methods and significantly improves
rendering quality (reducing error rates by 20-90% across synthetic and
unbounded real-world scenes) while incurring minimal performance overhead (as
each model head is quick to evaluate). Compared to Mip-NeRF, we reduce error
rates by 20% while training over 60x faster.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turki_H/0/1/0/all/0/1&quot;&gt;Haithem Turki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zollhofer_M/0/1/0/all/0/1&quot;&gt;Michael Zollh&amp;#xf6;fer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richardt_C/0/1/0/all/0/1&quot;&gt;Christian Richardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1&quot;&gt;Deva Ramanan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00269">
<title>Adaptability of Computer Vision at the Tactical Edge: Addressing Environmental Uncertainty. (arXiv:2312.00269v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00269</link>
<description rdf:parseType="Literal">&lt;p&gt;Computer Vision (CV) systems are increasingly being adopted into Command and
Control (C2) systems to improve intelligence analysis on the battlefield, the
tactical edge. CV systems leverage Artificial Intelligence (AI) algorithms to
help visualize and interpret the environment, enhancing situational awareness.
However, the adaptability of CV systems at the tactical edge remains
challenging due to rapidly changing environments and objects which can confuse
the deployed models. A CV model leveraged in this environment can become
uncertain in its predictions, as the environment and the objects existing in
the environment begin to change. Additionally, mission objectives can rapidly
change leading to adjustments in technology, camera angles, and image
resolutions. All of which can negatively affect the performance of and
potentially introduce uncertainty into the system. When the training
environment and/or technology differs from the deployment environment, CV
models can perform unexpectedly. Unfortunately, most scenarios at the tactical
edge do not incorporate Uncertainty Quantification (UQ) into their deployed C2
and CV systems. This concept paper explores the idea of synchronizing robust
data operations and model fine-tuning driven by UQ all at the tactical edge.
Specifically, curating datasets and training child models based on the
residuals of predictions, using these child models to calculate prediction
intervals (PI), and then using these PI to calibrate the deployed models. By
incorporating UQ into the core operations surrounding C2 and CV systems at the
tactical edge, we can help drive purposeful adaptability on the battlefield.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moore_H/0/1/0/all/0/1&quot;&gt;Hayden Moore&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00299">
<title>QIENet: Quantitative irradiance estimation network using recurrent neural network based on satellite remote sensing data. (arXiv:2312.00299v1 [stat.AP])</title>
<link>http://arxiv.org/abs/2312.00299</link>
<description rdf:parseType="Literal">&lt;p&gt;Global horizontal irradiance (GHI) plays a vital role in estimating solar
energy resources, which are used to generate sustainable green energy. In order
to estimate GHI with high spatial resolution, a quantitative irradiance
estimation network, named QIENet, is proposed. Specifically, the temporal and
spatial characteristics of remote sensing data of the satellite Himawari-8 are
extracted and fused by recurrent neural network (RNN) and convolution
operation, respectively. Not only remote sensing data, but also GHI-related
time information (hour, day, and month) and geographical information (altitude,
longitude, and latitude), are used as the inputs of QIENet. The satellite
spectral channels B07 and B11 - B15 and time are recommended as model inputs
for QIENet according to the spatial distributions of annual solar energy.
Meanwhile, QIENet is able to capture the impact of various clouds on hourly GHI
estimates. More importantly, QIENet does not overestimate ground observations
and can also reduce RMSE by 27.51%/18.00%, increase R2 by 20.17%/9.42%, and
increase r by 8.69%/3.54% compared with ERA5/NSRDB. Furthermore, QIENet is
capable of providing a high-fidelity hourly GHI database with spatial
resolution 0.02{\deg} * 0.02{\deg}(approximately 2km * 2km) for many applied
energy fields.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nie_L/0/1/0/all/0/1&quot;&gt;Longfeng Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuntian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dongxiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinyue Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yuan_W/0/1/0/all/0/1&quot;&gt;Wentian Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00304">
<title>Developmental Pretraining (DPT) for Image Classification Networks. (arXiv:2312.00304v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.00304</link>
<description rdf:parseType="Literal">&lt;p&gt;In the backdrop of increasing data requirements of Deep Neural Networks for
object recognition that is growing more untenable by the day, we present
Developmental PreTraining (DPT) as a possible solution. DPT is designed as a
curriculum-based pre-training approach designed to rival traditional
pre-training techniques that are data-hungry. These training approaches also
introduce unnecessary features that could be misleading when the network is
employed in a downstream classification task where the data is sufficiently
different from the pre-training data and is scarce. We design the curriculum
for DPT by drawing inspiration from human infant visual development. DPT
employs a phased approach where carefully-selected primitive and universal
features like edges and shapes are taught to the network participating in our
pre-training regime. A model that underwent the DPT regime is tested against
models with randomised weights to evaluate the viability of DPT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajesh_N/0/1/0/all/0/1&quot;&gt;Niranjan Rajesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1&quot;&gt;Debayan Gupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00306">
<title>RadioGalaxyNET: Dataset and Novel Computer Vision Algorithms for the Detection of Extended Radio Galaxies and Infrared Hosts. (arXiv:2312.00306v1 [astro-ph.IM])</title>
<link>http://arxiv.org/abs/2312.00306</link>
<description rdf:parseType="Literal">&lt;p&gt;Creating radio galaxy catalogues from next-generation deep surveys requires
automated identification of associated components of extended sources and their
corresponding infrared hosts. In this paper, we introduce RadioGalaxyNET, a
multimodal dataset, and a suite of novel computer vision algorithms designed to
automate the detection and localization of multi-component extended radio
galaxies and their corresponding infrared hosts. The dataset comprises 4,155
instances of galaxies in 2,800 images with both radio and infrared channels.
Each instance provides information about the extended radio galaxy class, its
corresponding bounding box encompassing all components, the pixel-level
segmentation mask, and the keypoint position of its corresponding infrared host
galaxy. RadioGalaxyNET is the first dataset to include images from the highly
sensitive Australian Square Kilometre Array Pathfinder (ASKAP) radio telescope,
corresponding infrared images, and instance-level annotations for galaxy
detection. We benchmark several object detection algorithms on the dataset and
propose a novel multimodal approach to simultaneously detect radio galaxies and
the positions of infrared hosts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Gupta_N/0/1/0/all/0/1&quot;&gt;Nikhel Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Hayder_Z/0/1/0/all/0/1&quot;&gt;Zeeshan Hayder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Norris_R/0/1/0/all/0/1&quot;&gt;Ray P. Norris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Huynh_M/0/1/0/all/0/1&quot;&gt;Minh Huynh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Petersson_L/0/1/0/all/0/1&quot;&gt;Lars Petersson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00308">
<title>A knowledge-based data-driven (KBDD) framework for all-day identification of cloud types using satellite remote sensing. (arXiv:2312.00308v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00308</link>
<description rdf:parseType="Literal">&lt;p&gt;Cloud types, as a type of meteorological data, are of particular significance
for evaluating changes in rainfall, heatwaves, water resources, floods and
droughts, food security and vegetation cover, as well as land use. In order to
effectively utilize high-resolution geostationary observations, a
knowledge-based data-driven (KBDD) framework for all-day identification of
cloud types based on spectral information from Himawari-8/9 satellite sensors
is designed. And a novel, simple and efficient network, named CldNet, is
proposed. Compared with widely used semantic segmentation networks, including
SegNet, PSPNet, DeepLabV3+, UNet, and ResUnet, our proposed model CldNet with
an accuracy of 80.89+-2.18% is state-of-the-art in identifying cloud types and
has increased by 32%, 46%, 22%, 2%, and 39%, respectively. With the assistance
of auxiliary information (e.g., satellite zenith/azimuth angle, solar
zenith/azimuth angle), the accuracy of CldNet-W using visible and near-infrared
bands and CldNet-O not using visible and near-infrared bands on the test
dataset is 82.23+-2.14% and 73.21+-2.02%, respectively. Meanwhile, the total
parameters of CldNet are only 0.46M, making it easy for edge deployment. More
importantly, the trained CldNet without any fine-tuning can predict cloud types
with higher spatial resolution using satellite spectral data with spatial
resolution 0.02{\deg}*0.02{\deg}, which indicates that CldNet possesses a
strong generalization ability. In aggregate, the KBDD framework using CldNet is
a highly effective cloud-type identification system capable of providing a
high-fidelity, all-day, spatiotemporal cloud-type database for many climate
assessment fields.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1&quot;&gt;Longfeng Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuntian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1&quot;&gt;Mengge Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1&quot;&gt;Changqi Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dongxiao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00311">
<title>3D Face Reconstruction with the Geometric Guidance of Facial Part Segmentation. (arXiv:2312.00311v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00311</link>
<description rdf:parseType="Literal">&lt;p&gt;3D Morphable Models (3DMMs) provide promising 3D face reconstructions in
various applications. However, existing methods struggle to reconstruct faces
with extreme expressions due to deficiencies in supervisory signals, such as
sparse or inaccurate landmarks. Segmentation information contains effective
geometric contexts for face reconstruction. Certain attempts intuitively depend
on differentiable renderers to compare the rendered silhouettes of
reconstruction with segmentation, which is prone to issues like local optima
and gradient instability. In this paper, we fully utilize the facial part
segmentation geometry by introducing Part Re-projection Distance Loss (PRDL).
Specifically, PRDL transforms facial part segmentation into 2D points and
re-projects the reconstruction onto the image plane. Subsequently, by
introducing grid anchors and computing different statistical distances from
these anchors to the point sets, PRDL establishes geometry descriptors to
optimize the distribution of the point sets for face reconstruction. PRDL
exhibits a clear gradient compared to the renderer-based methods and presents
state-of-the-art reconstruction performance in extensive quantitative and
qualitative experiments. The project will be publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zidu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiangyu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianshuo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Baiqin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1&quot;&gt;Zhen Lei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00312">
<title>Segment Anything Model-guided Collaborative Learning Network for Scribble-supervised Polyp Segmentation. (arXiv:2312.00312v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00312</link>
<description rdf:parseType="Literal">&lt;p&gt;Polyp segmentation plays a vital role in accurately locating polyps at an
early stage, which holds significant clinical importance for the prevention of
colorectal cancer. Various polyp segmentation methods have been developed using
fully-supervised deep learning techniques. However, pixel-wise annotation for
polyp images by physicians during the diagnosis is both time-consuming and
expensive. Moreover, visual foundation models such as the Segment Anything
Model (SAM) have shown remarkable performance. Nevertheless, directly applying
SAM to medical segmentation may not produce satisfactory results due to the
inherent absence of medical knowledge. In this paper, we propose a novel
SAM-guided Collaborative Learning Network (SAM-CLNet) for scribble-supervised
polyp segmentation, enabling a collaborative learning process between our
segmentation network and SAM to boost the model performance. Specifically, we
first propose a Cross-level Enhancement and Aggregation Network (CEA-Net) for
weakly-supervised polyp segmentation. Within CEA-Net, we propose a Cross-level
Enhancement Module (CEM) that integrates the adjacent features to enhance the
representation capabilities of different resolution features. Additionally, a
Feature Aggregation Module (FAM) is employed to capture richer features across
multiple levels. Moreover, we present a box-augmentation strategy that combines
the segmentation maps generated by CEA-Net with scribble annotations to create
more precise prompts. These prompts are then fed into SAM, generating
segmentation SAM-guided masks, which can provide additional supervision to
train CEA-Net effectively. Furthermore, we present an Image-level Filtering
Mechanism to filter out unreliable SAM-guided masks. Extensive experimental
results show that our SAM-CLNet outperforms state-of-the-art weakly-supervised
segmentation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yiming Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1&quot;&gt;Tao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yunqi Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yizhe Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Ye Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Huazhu Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00313">
<title>Improving Normalization with the James-Stein Estimator. (arXiv:2312.00313v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00313</link>
<description rdf:parseType="Literal">&lt;p&gt;Stein&apos;s paradox holds considerable sway in high-dimensional statistics,
highlighting that the sample mean, traditionally considered the de facto
estimator, might not be the most efficacious in higher dimensions. To address
this, the James-Stein estimator proposes an enhancement by steering the sample
means toward a more centralized mean vector. In this paper, first, we establish
that normalization layers in deep learning use inadmissible estimators for mean
and variance. Next, we introduce a novel method to employ the James-Stein
estimator to improve the estimation of mean and variance within normalization
layers. We evaluate our method on different computer vision tasks: image
classification, semantic segmentation, and 3D object classification. Through
these evaluations, it is evident that our improved normalization layers
consistently yield superior accuracy across all tasks without extra
computational burden. Moreover, recognizing that a plethora of shrinkage
estimators surpass the traditional estimator in performance, we study two other
prominent shrinkage estimators: Ridge and LASSO. Additionally, we provide
visual representations to intuitively demonstrate the impact of shrinkage on
the estimated layer statistics. Finally, we study the effect of regularization
and batch size on our modified batch normalization. The studies show that our
method is less sensitive to batch size and regularization, improving accuracy
under various setups.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khoshsirat_S/0/1/0/all/0/1&quot;&gt;Seyedalireza Khoshsirat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kambhamettu_C/0/1/0/all/0/1&quot;&gt;Chandra Kambhamettu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00316">
<title>Improving Efficiency of DNN-based Relocalization Module for Autonomous Driving with Server-side Computing. (arXiv:2312.00316v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00316</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we present a novel framework for camera relocation in
autonomous vehicles, leveraging deep neural networks (DNN). While existing
literature offers various DNN-based camera relocation methods, their deployment
is hindered by their high computational demands during inference. In contrast,
our approach addresses this challenge through edge cloud collaboration.
Specifically, we strategically offload certain modules of the neural network to
the server and evaluate the inference time of data frames under different
network segmentation schemes to guide our offloading decisions. Our findings
highlight the vital role of server-side offloading in DNN-based camera
relocation for autonomous vehicles, and we also discuss the results of data
fusion. Finally, we validate the effectiveness of our proposed framework
through experimental evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dengbo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jieren Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Boyi Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00330">
<title>StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style Adapter. (arXiv:2312.00330v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00330</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-video (T2V) models have shown remarkable capabilities in generating
diverse videos. However, they struggle to produce user-desired stylized videos
due to (i) text&apos;s inherent clumsiness in expressing specific styles and (ii)
the generally degraded style fidelity. To address these challenges, we
introduce StyleCrafter, a generic method that enhances pre-trained T2V models
with a style control adapter, enabling video generation in any style by
providing a reference image. Considering the scarcity of stylized video
datasets, we propose to first train a style control adapter using style-rich
image datasets, then transfer the learned stylization ability to video
generation through a tailor-made finetuning paradigm. To promote content-style
disentanglement, we remove style descriptions from the text prompt and extract
style information solely from the reference image using a decoupling learning
strategy. Additionally, we design a scale-adaptive fusion module to balance the
influences of text-based content features and image-based style features, which
helps generalization across various text and style combinations. StyleCrafter
efficiently generates high-quality stylized videos that align with the content
of the texts and resemble the style of the reference images. Experiments
demonstrate that our approach is more flexible and efficient than existing
competitors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Gongye Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1&quot;&gt;Menghan Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haoxin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1&quot;&gt;Jinbo Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xintao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yujiu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1&quot;&gt;Ying Shan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00335">
<title>Learning Anatomically Consistent Embedding for Chest Radiography. (arXiv:2312.00335v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00335</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised learning (SSL) approaches have recently shown substantial
success in learning visual representations from unannotated images. Compared
with photographic images, medical images acquired with the same imaging
protocol exhibit high consistency in anatomy. To exploit this anatomical
consistency, this paper introduces a novel SSL approach, called PEAC (patch
embedding of anatomical consistency), for medical image analysis. Specifically,
in this paper, we propose to learn global and local consistencies via stable
grid-based matching, transfer pre-trained PEAC models to diverse downstream
tasks, and extensively demonstrate that (1) PEAC achieves significantly better
performance than the existing state-of-the-art fully/self-supervised methods,
and (2) PEAC captures the anatomical structure consistency across views of the
same patient and across patients of different genders, weights, and healthy
statuses, which enhances the interpretability of our method for medical image
analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Ziyu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1&quot;&gt;Haozhe Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1&quot;&gt;Jiaxuan Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1&quot;&gt;Xiaowei Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gotway_M/0/1/0/all/0/1&quot;&gt;Michael Gotway&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Jianming Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00343">
<title>OpenStereo: A Comprehensive Benchmark for Stereo Matching and Strong Baseline. (arXiv:2312.00343v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00343</link>
<description rdf:parseType="Literal">&lt;p&gt;Stereo matching, a pivotal technique in computer vision, plays a crucial role
in robotics, autonomous navigation, and augmented reality. Despite the
development of numerous impressive methods in recent years, replicating their
results and determining the most suitable architecture for practical
application remains challenging. Addressing this gap, our paper introduces a
comprehensive benchmark focusing on practical applicability rather than solely
on performance enhancement. Specifically, we develop a flexible and efficient
stereo matching codebase, called OpenStereo. OpenStereo includes training and
inference codes of more than 12 network models, making it, to our knowledge,
the most complete stereo matching toolbox available. Based on OpenStereo, we
conducted experiments on the SceneFlow dataset and have achieved or surpassed
the performance metrics reported in the original paper. Additionally, we
conduct an in-depth revisitation of recent developments in stereo matching
through ablative experiments. These investigations inspired the creation of
StereoBase, a simple yet strong baseline model. Our extensive comparative
analyses of StereoBase against numerous contemporary stereo matching methods on
the SceneFlow dataset demonstrate its remarkably strong performance. The source
code is available at https://github.com/XiandaGuo/OpenStereo.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xianda Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Juntao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chenming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yiqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1&quot;&gt;Yiqun Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Tian Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Long Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00347">
<title>RTQ: Rethinking Video-language Understanding Based on Image-text Model. (arXiv:2312.00347v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00347</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in video-language understanding have been established on
the foundation of image-text models, resulting in promising outcomes due to the
shared knowledge between images and videos. However, video-language
understanding presents unique challenges due to the inclusion of highly complex
semantic details, which result in information redundancy, temporal dependency,
and scene complexity. Current techniques have only partially tackled these
issues, and our quantitative analysis indicates that some of these methods are
complementary. In light of this, we propose a novel framework called RTQ
(Refine, Temporal model, and Query), which addresses these challenges
simultaneously. The approach involves refining redundant information within
frames, modeling temporal relations among frames, and querying task-specific
information from the videos. Remarkably, our model demonstrates outstanding
performance even in the absence of video-language pre-training, and the results
are comparable with or superior to those achieved by state-of-the-art
pre-training methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yaoyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_T/0/1/0/all/0/1&quot;&gt;Tian Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1&quot;&gt;Jingjing Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1&quot;&gt;Liqiang Nie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00348">
<title>Student Activity Recognition in Classroom Environments using Transfer Learning. (arXiv:2312.00348v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00348</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent advances in artificial intelligence and deep learning facilitate
automation in various applications including home automation, smart
surveillance systems, and healthcare among others. Human Activity Recognition
is one of its emerging applications, which can be implemented in a classroom
environment to enhance safety, efficiency, and overall educational quality.
This paper proposes a system for detecting and recognizing the activities of
students in a classroom environment. The dataset has been structured and
recorded by the authors since a standard dataset for this task was not
available at the time of this study. Transfer learning, a widely adopted method
within the field of deep learning, has proven to be helpful in complex tasks
like image and video processing. Pretrained models including VGG-16, ResNet-50,
InceptionV3, and Xception are used for feature extraction and classification
tasks. Xception achieved an accuracy of 93%, on the novel classroom dataset,
outperforming the other three models in consideration. The system proposed in
this study aims to introduce a safer and more productive learning environment
for students and educators.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1&quot;&gt;Anagha Deshpande&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deshpande_V/0/1/0/all/0/1&quot;&gt;Vedant Deshpande&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00351">
<title>Manipulating the Label Space for In-Context Classification. (arXiv:2312.00351v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00351</link>
<description rdf:parseType="Literal">&lt;p&gt;After pre-training by generating the next word conditional on previous words,
the Language Model (LM) acquires the ability of In-Context Learning (ICL) that
can learn a new task conditional on the context of the given in-context
examples (ICEs). Similarly, visually-conditioned Language Modelling is also
used to train Vision-Language Models (VLMs) with ICL ability. However, such
VLMs typically exhibit weaker classification abilities compared to contrastive
learning-based models like CLIP, since the Language Modelling objective does
not directly contrast whether an object is paired with a text. To improve the
ICL of classification, using more ICEs to provide more knowledge is a
straightforward way. However, this may largely increase the selection time, and
more importantly, the inclusion of additional in-context images tends to extend
the length of the in-context sequence beyond the processing capacity of a VLM.
To alleviate these limitations, we propose to manipulate the label space of
each ICE to increase its knowledge density, allowing for fewer ICEs to convey
as much information as a larger set would. Specifically, we propose two
strategies which are Label Distribution Enhancement and Visual Descriptions
Enhancement to improve In-context classification performance on diverse
datasets, including the classic ImageNet and more fine-grained datasets like
CUB-200. Specifically, using our approach on ImageNet, we increase accuracy
from 74.70\% in a 4-shot setting to 76.21\% with just 2 shots. surpassing CLIP
by 0.67\%. On CUB-200, our method raises 1-shot accuracy from 48.86\% to
69.05\%, 12.15\% higher than CLIP. The code is given in
https://anonymous.4open.science/r/MLS_ICC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haokun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yuhang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zihan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1&quot;&gt;Xin Geng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00357">
<title>A Generalizable Deep Learning System for Cardiac MRI. (arXiv:2312.00357v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.00357</link>
<description rdf:parseType="Literal">&lt;p&gt;Cardiac MRI allows for a comprehensive assessment of myocardial structure,
function, and tissue characteristics. Here we describe a foundational vision
system for cardiac MRI, capable of representing the breadth of human
cardiovascular disease and health. Our deep learning model is trained via
self-supervised contrastive learning, by which visual concepts in cine-sequence
cardiac MRI scans are learned from the raw text of the accompanying radiology
reports. We train and evaluate our model on data from four large academic
clinical institutions in the United States. We additionally showcase the
performance of our models on the UK BioBank, and two additional publicly
available external datasets. We explore emergent zero-shot capabilities of our
system, and demonstrate remarkable performance across a range of tasks;
including the problem of left ventricular ejection fraction regression, and the
diagnosis of 35 different conditions such as cardiac amyloidosis and
hypertrophic cardiomyopathy. We show that our deep learning system is capable
of not only understanding the staggering complexity of human cardiovascular
disease, but can be directed towards clinical problems of interest yielding
impressive, clinical grade diagnostic accuracy with a fraction of the training
data typically required for such tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shad_R/0/1/0/all/0/1&quot;&gt;Rohan Shad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zakka_C/0/1/0/all/0/1&quot;&gt;Cyril Zakka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kaur_D/0/1/0/all/0/1&quot;&gt;Dhamanpreet Kaur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fong_R/0/1/0/all/0/1&quot;&gt;Robyn Fong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Filice_R/0/1/0/all/0/1&quot;&gt;Ross Warren Filice&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mongan_J/0/1/0/all/0/1&quot;&gt;John Mongan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kalianos_K/0/1/0/all/0/1&quot;&gt;Kimberly Kalianos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khandwala_N/0/1/0/all/0/1&quot;&gt;Nishith Khandwala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Eng_D/0/1/0/all/0/1&quot;&gt;David Eng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Leipzig_M/0/1/0/all/0/1&quot;&gt;Matthew Leipzig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Witschey_W/0/1/0/all/0/1&quot;&gt;Walter Witschey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Feria_A/0/1/0/all/0/1&quot;&gt;Alejandro de Feria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ferrari_V/0/1/0/all/0/1&quot;&gt;Victor Ferrari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ashley_E/0/1/0/all/0/1&quot;&gt;Euan Ashley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Acker_M/0/1/0/all/0/1&quot;&gt;Michael A. Acker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Langlotz_C/0/1/0/all/0/1&quot;&gt;Curtis Langlotz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hiesinger_W/0/1/0/all/0/1&quot;&gt;William Hiesinger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00358">
<title>Impact of Data Augmentation on QCNNs. (arXiv:2312.00358v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2312.00358</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, Classical Convolutional Neural Networks (CNNs) have been
applied for image recognition successfully. Quantum Convolutional Neural
Networks (QCNNs) are proposed as a novel generalization to CNNs by using
quantum mechanisms. The quantum mechanisms lead to an efficient training
process in QCNNs by reducing the size of input from $N$ to $log_2N$. This paper
implements and compares both CNNs and QCNNs by testing losses and prediction
accuracy on three commonly used datasets. The datasets include the MNIST
hand-written digits, Fashion MNIST and cat/dog face images. Additionally, data
augmentation (DA), a technique commonly used in CNNs to improve the performance
of classification by generating similar images based on original inputs, is
also implemented in QCNNs. Surprisingly, the results showed that data
augmentation didn&apos;t improve QCNNs performance. The reasons and logic behind
this result are discussed, hoping to expand our understanding of Quantum
machine learning theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Zhouli_L/0/1/0/all/0/1&quot;&gt;Leting Zhouli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peiyong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Parampalli_U/0/1/0/all/0/1&quot;&gt;Udaya Parampalli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00360">
<title>Efficient Multimodal Semantic Segmentation via Dual-Prompt Learning. (arXiv:2312.00360v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00360</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal (e.g., RGB-Depth/RGB-Thermal) fusion has shown great potential for
improving semantic segmentation in complex scenes (e.g., indoor/low-light
conditions). Existing approaches often fully fine-tune a dual-branch
encoder-decoder framework with a complicated feature fusion strategy for
achieving multimodal semantic segmentation, which is training-costly due to the
massive parameter updates in feature extraction and fusion. To address this
issue, we propose a surprisingly simple yet effective dual-prompt learning
network (dubbed DPLNet) for training-efficient multimodal (e.g., RGB-D/T)
semantic segmentation. The core of DPLNet is to directly adapt a frozen
pre-trained RGB model to multimodal semantic segmentation, reducing parameter
updates. For this purpose, we present two prompt learning modules, comprising
multimodal prompt generator (MPG) and multimodal feature adapter (MFA). MPG
works to fuse the features from different modalities in a compact manner and is
inserted from shadow to deep stages to generate the multi-level multimodal
prompts that are injected into the frozen backbone, while MPG adapts prompted
multimodal features in the frozen backbone for better multimodal semantic
segmentation. Since both the MPG and MFA are lightweight, only a few trainable
parameters (3.88M, 4.4% of the pre-trained backbone parameters) are introduced
for multimodal feature fusion and learning. Using a simple decoder (3.27M
parameters), DPLNet achieves new state-of-the-art performance or is on a par
with other complex approaches on four RGB-D/T semantic segmentation datasets
while satisfying parameter efficiency. Moreover, we show that DPLNet is general
and applicable to other multimodal tasks such as salient object detection and
video semantic segmentation. Without special design, DPLNet outperforms many
complicated models. Our code will be available at
github.com/ShaohuaDong2021/DPLNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1&quot;&gt;Shaohua Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yunhe Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dongfang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1&quot;&gt;Heng Fan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00362">
<title>Dancing with Images: Video Distillation via Static-Dynamic Disentanglement. (arXiv:2312.00362v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00362</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, dataset distillation has paved the way towards efficient machine
learning, especially for image datasets. However, the distillation for videos,
characterized by an exclusive temporal dimension, remains an underexplored
domain. In this work, we provide the first systematic study of video
distillation and introduce a taxonomy to categorize temporal compression. Our
investigation reveals that the temporal information is usually not well learned
during distillation , and the temporal dimension of synthetic data contributes
little. The observations motivate our unified framework of disentangling the
dynamic and static information in the videos. It first distills the videos into
still images as static memory and then compensates the dynamic and motion
information with a learnable dynamic memory block. Our method achieves
state-of-the-art on video datasets at different scales, with notably smaller
storage expenditure. Our code will be publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yue Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Cewu Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yong-Lu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00364">
<title>Benchmarking Multi-Domain Active Learning on Image Classification. (arXiv:2312.00364v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.00364</link>
<description rdf:parseType="Literal">&lt;p&gt;Active learning aims to enhance model performance by strategically labeling
informative data points. While extensively studied, its effectiveness on
large-scale, real-world datasets remains underexplored. Existing research
primarily focuses on single-source data, ignoring the multi-domain nature of
real-world data. We introduce a multi-domain active learning benchmark to
bridge this gap. Our benchmark demonstrates that traditional single-domain
active learning strategies are often less effective than random selection in
multi-domain scenarios. We also introduce CLIP-GeoYFCC, a novel large-scale
image dataset built around geographical domains, in contrast to existing
genre-based domain datasets. Analysis on our benchmark shows that all
multi-domain strategies exhibit significant tradeoffs, with no strategy
outperforming across all datasets or all metrics, emphasizing the need for
future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiayi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taori_R/0/1/0/all/0/1&quot;&gt;Rohan Taori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1&quot;&gt;Tatsunori B. Hashimoto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00375">
<title>Text-Guided 3D Face Synthesis -- From Generation to Editing. (arXiv:2312.00375v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00375</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-guided 3D face synthesis has achieved remarkable results by leveraging
text-to-image (T2I) diffusion models. However, most existing works focus solely
on the direct generation, ignoring the editing, restricting them from
synthesizing customized 3D faces through iterative adjustments. In this paper,
we propose a unified text-guided framework from face generation to editing. In
the generation stage, we propose a geometry-texture decoupled generation to
mitigate the loss of geometric details caused by coupling. Besides, decoupling
enables us to utilize the generated geometry as a condition for texture
generation, yielding highly geometry-texture aligned results. We further employ
a fine-tuned texture diffusion model to enhance texture quality in both RGB and
YUV space. In the editing stage, we first employ a pre-trained diffusion model
to update facial geometry or texture based on the texts. To enable sequential
editing, we introduce a UV domain consistency preservation regularization,
preventing unintentional changes to irrelevant facial attributes. Besides, we
propose a self-guided consistency weight strategy to improve editing efficacy
while preserving consistency. Through comprehensive experiments, we showcase
our method&apos;s superiority in face synthesis. Project page:
https://faceg2e.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yunjie Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1&quot;&gt;Yapeng Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zhipeng Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lincheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Haoqian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1&quot;&gt;Kun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Weiwei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xin Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00377">
<title>SynFundus: Generating a synthetic fundus images dataset with millions of samples and multi-disease annotations. (arXiv:2312.00377v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00377</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of medical imaging, the scarcity of large-scale datasets due to
privacy restrictions stands as a significant barrier to develop large models
for medical. To address this issue, we introduce SynFundus-1M, a high-quality
synthetic dataset with over 1 million retinal fundus images and extensive
disease and pathologies annotations, which is generated by a Denoising
Diffusion Probabilistic Model. The SynFundus-Generator and SynFundus-1M achieve
superior Frechet Inception Distance (FID) scores compared to existing methods
on main-stream public real datasets. Furthermore, the ophthalmologists
evaluation validate the difficulty in discerning these synthetic images from
real ones, confirming the SynFundus-1M&apos;s authenticity. Through extensive
experiments, we demonstrate that both CNN and ViT can benifit from SynFundus-1M
by pretraining or training directly. Compared to datasets like ImageNet or
EyePACS, models train on SynFundus-1M not only achieve better performance but
also faster convergence on various downstream tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1&quot;&gt;Fangxin Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yehui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lei Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00386">
<title>Local monotone operator learning using non-monotone operators: MnM-MOL. (arXiv:2312.00386v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.00386</link>
<description rdf:parseType="Literal">&lt;p&gt;The recovery of magnetic resonance (MR) images from undersampled measurements
is a key problem that has seen extensive research in recent years. Unrolled
approaches, which rely on end-to-end training of convolutional neural network
(CNN) blocks within iterative reconstruction algorithms, offer state-of-the-art
performance. These algorithms require a large amount of memory during training,
making them difficult to employ in high-dimensional applications. Deep
equilibrium (DEQ) models and the recent monotone operator learning (MOL)
approach were introduced to eliminate the need for unrolling, thus reducing the
memory demand during training. Both approaches require a Lipschitz constraint
on the network to ensure that the forward and backpropagation iterations
converge. Unfortunately, the constraint often results in reduced performance
compared to unrolled methods. The main focus of this work is to relax the
constraint on the CNN block in two different ways. Inspired by
convex-non-convex regularization strategies, we now impose the monotone
constraint on the sum of the gradient of the data term and the CNN block,
rather than constrain the CNN itself to be a monotone operator. This approach
enables the CNN to learn possibly non-monotone score functions, which can
translate to improved performance. In addition, we only restrict the operator
to be monotone in a local neighborhood around the image manifold. Our
theoretical results show that the proposed algorithm is guaranteed to converge
to the fixed point and that the solution is robust to input perturbations,
provided that it is initialized close to the true solution. Our empirical
results show that the relaxed constraints translate to improved performance and
that the approach enjoys robustness to input perturbations similar to MOL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+John_M/0/1/0/all/0/1&quot;&gt;Maneesh John&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chand_J/0/1/0/all/0/1&quot;&gt;Jyothi Rikhab Chand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jacob_M/0/1/0/all/0/1&quot;&gt;Mathews Jacob&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00387">
<title>Partition-based K-space Synthesis for Multi-contrast Parallel Imaging. (arXiv:2312.00387v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.00387</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-contrast magnetic resonance imaging is a significant and essential
medical imaging technique.However, multi-contrast imaging has longer
acquisition time and is easy to cause motion artifacts. In particular, the
acquisition time for a T2-weighted image is prolonged due to its longer
repetition time (TR). On the contrary, T1-weighted image has a shorter TR.
Therefore,utilizing complementary information across T1 and T2-weighted image
is a way to decrease the overall imaging time. Previous T1-assisted T2
reconstruction methods have mostly focused on image domain using whole-based
image fusion approaches. The image domain reconstruction method has the defects
of high computational complexity and limited flexibility. To address this
issue, we propose a novel multi-contrast imaging method called partition-based
k-space synthesis (PKS) which can achieve super reconstruction quality of
T2-weighted image by feature fusion. Concretely, we first decompose
fully-sampled T1 k-space data and under-sampled T2 k-space data into two
sub-data, separately. Then two new objects are constructed by combining the two
sub-T1/T2 data. After that, the two new objects as the whole data to realize
the reconstruction of T2-weighted image. Finally, the objective T2 is
synthesized by extracting the sub-T2 data of each part. Experimental results
showed that our combined technique can achieve comparable or better results
than using traditional k-space parallel imaging(SAKE) that processes each
contrast independently.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yuxia Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhonghui Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaoling Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Minghui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shanshan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qiegen Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00392">
<title>Study and Survey on Gesture Recognition Systems. (arXiv:2312.00392v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00392</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, there has been a considerable amount of research in the
Gesture Recognition domain, mainly owing to the technological advancements in
Computer Vision. Various new applications have been conceptualised and
developed in this field. This paper discusses the implementation of gesture
recognition systems in multiple sectors such as gaming, healthcare, home
appliances, industrial robots, and virtual reality. Different methodologies for
capturing gestures are compared and contrasted throughout this survey. Various
data sources and data acquisition techniques have been discussed. The role of
gestures in sign language has been studied and existing approaches have been
reviewed. Common challenges faced while building gesture recognition systems
have also been explored.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deshpande_K/0/1/0/all/0/1&quot;&gt;Kshitij Deshpande&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mashalkar_V/0/1/0/all/0/1&quot;&gt;Varad Mashalkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mhaisekar_K/0/1/0/all/0/1&quot;&gt;Kaustubh Mhaisekar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naikwadi_A/0/1/0/all/0/1&quot;&gt;Amaan Naikwadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghotkar_A/0/1/0/all/0/1&quot;&gt;Archana Ghotkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00398">
<title>Learning to Estimate Critical Gait Parameters from Single-View RGB Videos with Transformer-Based Attention Network. (arXiv:2312.00398v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00398</link>
<description rdf:parseType="Literal">&lt;p&gt;Musculoskeletal diseases and cognitive impairments in patients lead to
difficulties in movement as well as negative effects on their psychological
health. Clinical gait analysis, a vital tool for early diagnosis and treatment,
traditionally relies on expensive optical motion capture systems. Recent
advances in computer vision and deep learning have opened the door to more
accessible and cost-effective alternatives. This paper introduces a novel
spatio-temporal Transformer network to estimate critical gait parameters from
RGB videos captured by a single-view camera. Empirical evaluations on a public
dataset of cerebral palsy patients indicate that the proposed framework
surpasses current state-of-the-art approaches and show significant improvements
in predicting general gait parameters (including Walking Speed, Gait Deviation
Index - GDI, and Knee Flexion Angle at Maximum Extension), while utilizing
fewer parameters and alleviating the need for manual feature extraction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1&quot;&gt;Quoc Hung T. Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1&quot;&gt;Hieu H. Pham&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00401">
<title>VIoTGPT: Learning to Schedule Vision Tools towards Intelligent Video Internet of Things. (arXiv:2312.00401v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00401</link>
<description rdf:parseType="Literal">&lt;p&gt;Video Internet of Things (VIoT) has shown full potential in collecting an
unprecedented volume of video data. Learning to schedule perceiving models and
analyzing the collected videos intelligently will be potential sparks for VIoT.
In this paper, to address the challenges posed by the fine-grained and
interrelated vision tool usage of VIoT, we build VIoTGPT, the framework based
on LLMs to correctly interact with humans, query knowledge videos, and invoke
vision models to accomplish complicated tasks. To support VIoTGPT and related
future works, we meticulously crafted the training dataset and established
benchmarks involving 11 representative vision models across three categories
based on semi-automatic annotations. To guide LLM to act as the intelligent
agent towards intelligent VIoT, we resort to ReAct instruction tuning based on
the collected VIoT dataset to learn the tool capability. Quantitative and
qualitative experimental results and analyses demonstrate the effectiveness of
VIoTGPT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yaoyao Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_M/0/1/0/all/0/1&quot;&gt;Mengshi Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1&quot;&gt;Yuhan Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Huadong Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00412">
<title>SCHEME: Scalable Channer Mixer for Vision Transformers. (arXiv:2312.00412v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00412</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformers have received significant attention due to their
impressive performance in many vision tasks. While the token mixer or attention
block has been studied in great detail, the channel mixer or feature mixing
block (FFN or MLP) has not been explored in depth albeit it accounts for a bulk
of the parameters and computation in a model. In this work, we study whether
sparse feature mixing can replace the dense connections and confirm this with a
block diagonal MLP structure that improves the accuracy by supporting larger
expansion ratios. To improve the feature clusters formed by this structure and
thereby further improve the accuracy, a lightweight, parameter-free, channel
covariance attention (CCA) mechanism is introduced as a parallel branch during
training. This design of CCA enables gradual feature mixing across channel
groups during training whose contribution decays to zero as the training
progresses to convergence. This allows the CCA block to be discarded during
inference, thus enabling enhanced performance with no additional computational
cost. The resulting $\textit{Scalable CHannEl MixEr}$ (SCHEME) can be plugged
into any ViT architecture to obtain a gamut of models with different trade-offs
between complexity and performance by controlling the block diagonal structure
size in the MLP. This is shown by the introduction of a new family of
SCHEMEformer models. Experiments on image classification, object detection, and
semantic segmentation, with different ViT backbones, consistently demonstrate
substantial accuracy gains over existing designs, especially under lower FLOPs
regimes. For example, the SCHEMEformer establishes a new SOTA of 79.7% accuracy
for ViTs using pure attention mixers on ImageNet-1K at 1.77G FLOPs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sridhar_D/0/1/0/all/0/1&quot;&gt;Deepak Sridhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunsheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasconcelos_N/0/1/0/all/0/1&quot;&gt;Nuno Vasconcelos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00414">
<title>Large-scale Vision-Language Models Learn Super Images for Efficient and High-Performance Partially Relevant Video Retrieval. (arXiv:2312.00414v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00414</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose an efficient and high-performance method for
partially relevant video retrieval (PRVR), which aims to retrieve untrimmed
long videos that contain at least one relevant moment to the input text query.
In terms of both efficiency and performance, the overlooked bottleneck of
previous studies is the visual encoding of dense frames. This guides
researchers to choose lightweight visual backbones, yielding sub-optimal
retrieval performance due to their limited capabilities of learned visual
representations. However, it is undesirable to simply replace them with
high-performance large-scale vision-and-language models (VLMs) due to their low
efficiency. To address these issues, instead of dense frames, we focus on super
images, which are created by rearranging the video frames in a $N \times N$
grid layout. This reduces the number of visual encodings to $\frac{1}{N^2}$ and
compensates for the low efficiency of large-scale VLMs, allowing us to adopt
them as powerful encoders. Surprisingly, we discover that with a simple
query-image attention trick, VLMs generalize well to super images effectively
and demonstrate promising zero-shot performance against SOTA methods
efficiently. In addition, we propose a fine-tuning approach by incorporating a
few trainable modules into the VLM backbones. The experimental results
demonstrate that our approaches efficiently achieve the best performance on
ActivityNet Captions and TVR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nishimura_T/0/1/0/all/0/1&quot;&gt;Taichi Nishimura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakada_S/0/1/0/all/0/1&quot;&gt;Shota Nakada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kondo_M/0/1/0/all/0/1&quot;&gt;Masayoshi Kondo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00416">
<title>Towards Explaining Satellite Based Poverty Predictions with Convolutional Neural Networks. (arXiv:2312.00416v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00416</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep convolutional neural networks (CNNs) have been shown to predict poverty
and development indicators from satellite images with surprising accuracy. This
paper presents a first attempt at analyzing the CNNs responses in detail and
explaining the basis for the predictions. The CNN model, while trained on
relatively low resolution day- and night-time satellite images, is able to
outperform human subjects who look at high-resolution images in ranking the
Wealth Index categories. Multiple explainability experiments performed on the
model indicate the importance of the sizes of the objects, pixel colors in the
image, and provide a visualization of the importance of different structures in
input images. A visualization is also provided of type images that maximize the
network prediction of Wealth Index, which provides clues on what the CNN
prediction is based on.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarmadi_H/0/1/0/all/0/1&quot;&gt;Hamid Sarmadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rognvaldsson_T/0/1/0/all/0/1&quot;&gt;Thorsteinn R&amp;#xf6;gnvaldsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carlsson_N/0/1/0/all/0/1&quot;&gt;Nils Roger Carlsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ohlsson_M/0/1/0/all/0/1&quot;&gt;Mattias Ohlsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wahab_I/0/1/0/all/0/1&quot;&gt;Ibrahim Wahab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hall_O/0/1/0/all/0/1&quot;&gt;Ola Hall&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00425">
<title>A Low-Power Neuromorphic Approach for Efficient Eye-Tracking. (arXiv:2312.00425v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00425</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a neuromorphic methodology for eye tracking, harnessing
pure event data captured by a Dynamic Vision Sensor (DVS) camera. The framework
integrates a directly trained Spiking Neuron Network (SNN) regression model and
leverages a state-of-the-art low power edge neuromorphic processor - Speck,
collectively aiming to advance the precision and efficiency of eye-tracking
systems. First, we introduce a representative event-based eye-tracking dataset,
&quot;Ini-30&quot;, which was collected with two glass-mounted DVS cameras from thirty
volunteers. Then,a SNN model, based on Integrate And Fire (IAF) neurons, named
&quot;Retina&quot;, is described , featuring only 64k parameters (6.63x fewer than the
latest) and achieving pupil tracking error of only 3.24 pixels in a 64x64 DVS
input. The continous regression output is obtained by means of convolution
using a non-spiking temporal 1D filter slided across the output spiking layer.
Finally, we evaluate Retina on the neuromorphic processor, showing an
end-to-end power between 2.89-4.8 mW and a latency of 5.57-8.01 mS dependent on
the time window. We also benchmark our model against the latest event-based
eye-tracking method, &quot;3ET&quot;, which was built upon event frames. Results show
that Retina achieves superior precision with 1.24px less pupil centroid error
and reduced computational complexity with 35 times fewer MAC operations. We
hope this work will open avenues for further investigation of close-loop
neuromorphic solutions and true event-based training pursuing edge performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonazzi_P/0/1/0/all/0/1&quot;&gt;Pietro Bonazzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_S/0/1/0/all/0/1&quot;&gt;Sizhen Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lippolis_G/0/1/0/all/0/1&quot;&gt;Giovanni Lippolis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yawei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheik_S/0/1/0/all/0/1&quot;&gt;Sadique Sheik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magno_M/0/1/0/all/0/1&quot;&gt;Michele Magno&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00435">
<title>Enhancing Image Captioning with Neural Models. (arXiv:2312.00435v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00435</link>
<description rdf:parseType="Literal">&lt;p&gt;This research explores the realm of neural image captioning using deep
learning models. The study investigates the performance of different neural
architecture configurations, focusing on the inject architecture, and proposes
a novel quality metric for evaluating caption generation. Through extensive
experimentation and analysis, this work sheds light on the challenges and
opportunities in image captioning, providing insights into model behavior and
overfitting. The results reveal that while the merge models exhibit a larger
vocabulary and higher ROUGE scores, the inject architecture generates relevant
and concise image captions. The study also highlights the importance of
refining training data and optimizing hyperparameters for improved model
performance. This research contributes to the growing body of knowledge in
neural image captioning and encourages further exploration in the field,
emphasizing the democratization of artificial intelligence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatnagar_P/0/1/0/all/0/1&quot;&gt;Pooja Bhatnagar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mrunaal_S/0/1/0/all/0/1&quot;&gt;Sai Mrunaal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamnure_S/0/1/0/all/0/1&quot;&gt;Sachin Kamnure&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00438">
<title>Dolphins: Multimodal Language Model for Driving. (arXiv:2312.00438v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00438</link>
<description rdf:parseType="Literal">&lt;p&gt;The quest for fully autonomous vehicles (AVs) capable of navigating complex
real-world scenarios with human-like understanding and responsiveness. In this
paper, we introduce Dolphins, a novel vision-language model architected to
imbibe human-like abilities as a conversational driving assistant. Dolphins is
adept at processing multimodal inputs comprising video (or image) data, text
instructions, and historical control signals to generate informed outputs
corresponding to the provided instructions. Building upon the open-sourced
pretrained Vision-Language Model, OpenFlamingo, we first enhance Dolphins&apos;s
reasoning capabilities through an innovative Grounded Chain of Thought (GCoT)
process. Then we tailored Dolphins to the driving domain by constructing
driving-specific instruction data and conducting instruction tuning. Through
the utilization of the BDD-X dataset, we designed and consolidated four
distinct AV tasks into Dolphins to foster a holistic understanding of intricate
driving scenarios. As a result, the distinctive features of Dolphins are
characterized into two dimensions: (1) the ability to provide a comprehensive
understanding of complex and long-tailed open-world driving scenarios and solve
a spectrum of AV tasks, and (2) the emergence of human-like capabilities
including gradient-free instant adaptation via in-context learning and error
recovery via reflection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yingzi Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yulong Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiachen Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1&quot;&gt;Marco Pavone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Chaowei Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00451">
<title>FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting. (arXiv:2312.00451v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00451</link>
<description rdf:parseType="Literal">&lt;p&gt;Novel view synthesis from limited observations remains an important and
persistent task. However, high efficiency in existing NeRF-based few-shot view
synthesis is often compromised to obtain an accurate 3D representation. To
address this challenge, we propose a few-shot view synthesis framework based on
3D Gaussian Splatting that enables real-time and photo-realistic view synthesis
with as few as three training views. The proposed method, dubbed FSGS, handles
the extremely sparse initialized SfM points with a thoughtfully designed
Gaussian Unpooling process. Our method iteratively distributes new Gaussians
around the most representative locations, subsequently infilling local details
in vacant areas. We also integrate a large-scale pre-trained monocular depth
estimator within the Gaussians optimization process, leveraging online
augmented views to guide the geometric optimization towards an optimal
solution. Starting from sparse points observed from limited input viewpoints,
our FSGS can accurately grow into unseen regions, comprehensively covering the
scene and boosting the rendering quality of novel views. Overall, FSGS achieves
state-of-the-art performance in both accuracy and rendering efficiency across
diverse datasets, including LLFF, Mip-NeRF360, and Blender. Project website:
https://zehaozhu.github.io/FSGS/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zehao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1&quot;&gt;Zhiwen Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yifan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhangyang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00452">
<title>Towards Generalizable Referring Image Segmentation via Target Prompt and Visual Coherence. (arXiv:2312.00452v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00452</link>
<description rdf:parseType="Literal">&lt;p&gt;Referring image segmentation (RIS) aims to segment objects in an image
conditioning on free-from text descriptions. Despite the overwhelming progress,
it still remains challenging for current approaches to perform well on cases
with various text expressions or with unseen visual entities, limiting its
further application. In this paper, we present a novel RIS approach, which
substantially improves the generalization ability by addressing the two
dilemmas mentioned above. Specially, to deal with unconstrained texts, we
propose to boost a given expression with an explicit and crucial prompt, which
complements the expression in a unified context, facilitating target capturing
in the presence of linguistic style changes. Furthermore, we introduce a
multi-modal fusion aggregation module with visual guidance from a powerful
pretrained model to leverage spatial relations and pixel coherences to handle
the incomplete target masks and false positive irregular clumps which often
appear on unseen visual entities. Extensive experiments are conducted in the
zero-shot cross-dataset settings and the proposed approach achieves consistent
gains compared to the state-of-the-art, e.g., 4.15\%, 5.45\%, and 4.64\% mIoU
increase on RefCOCO, RefCOCO+ and ReferIt respectively, demonstrating its
effectiveness. Additionally, the results on GraspNet-RIS show that our approach
also generalizes well to new scenarios with large domain shifts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yajie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_P/0/1/0/all/0/1&quot;&gt;Pu Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Haoxiang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1&quot;&gt;Shichao Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qingjie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1&quot;&gt;Di Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunhong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00454">
<title>An Encoding Framework for Binarized Images using HyperDimensional Computing. (arXiv:2312.00454v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00454</link>
<description rdf:parseType="Literal">&lt;p&gt;Hyperdimensional Computing (HDC) is a brain-inspired and light-weight machine
learning method. It has received significant attention in the literature as a
candidate to be applied in the wearable internet of things, near-sensor
artificial intelligence applications and on-device processing. HDC is
computationally less complex than traditional deep learning algorithms and
typically achieves moderate to good classification performance. A key aspect
that determines the performance of HDC is the encoding of the input data to the
hyperdimensional (HD) space. This article proposes a novel light-weight
approach relying only on native HD arithmetic vector operations to encode
binarized images that preserves similarity of patterns at nearby locations by
using point of interest selection and local linear mapping. The method reaches
an accuracy of 97.35% on the test set for the MNIST data set and 84.12% for the
Fashion-MNIST data set. These results outperform other studies using baseline
HDC with different encoding approaches and are on par with more complex hybrid
HDC models. The proposed encoding approach also demonstrates a higher
robustness to noise and blur compared to the baseline encoding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smets_L/0/1/0/all/0/1&quot;&gt;Laura Smets&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leekwijck_W/0/1/0/all/0/1&quot;&gt;Werner Van Leekwijck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1&quot;&gt;Ing Jyh Tsang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Latre_S/0/1/0/all/0/1&quot;&gt;Steven Latr&amp;#xe9;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00462">
<title>Learning Unorthogonalized Matrices for Rotation Estimation. (arXiv:2312.00462v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00462</link>
<description rdf:parseType="Literal">&lt;p&gt;Estimating 3D rotations is a common procedure for 3D computer vision. The
accuracy depends heavily on the rotation representation. One form of
representation -- rotation matrices -- is popular due to its continuity,
especially for pose estimation tasks. The learning process usually incorporates
orthogonalization to ensure orthonormal matrices. Our work reveals, through
gradient analysis, that common orthogonalization procedures based on the
Gram-Schmidt process and singular value decomposition will slow down training
efficiency. To this end, we advocate removing orthogonalization from the
learning process and learning unorthogonalized `Pseudo&apos; Rotation Matrices
(PRoM). An optimization analysis shows that PRoM converges faster and to a
better solution. By replacing the orthogonalization incorporated representation
with our proposed PRoM in various rotation-related tasks, we achieve
state-of-the-art results on large-scale benchmarks for human pose estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_K/0/1/0/all/0/1&quot;&gt;Kerui Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhihao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shiyong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jianzhuang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Songcen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Youliang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mi_M/0/1/0/all/0/1&quot;&gt;Michael Bi Mi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1&quot;&gt;Kenji Kawaguchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1&quot;&gt;Angela Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00467">
<title>Unfolder: Fast localization and image rectification of a document with a crease from folding in half. (arXiv:2312.00467v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00467</link>
<description rdf:parseType="Literal">&lt;p&gt;Presentation of folded documents is not an uncommon case in modern society.
Digitizing such documents by capturing them with a smartphone camera can be
tricky since a crease can divide the document contents into separate planes. To
unfold the document, one could hold the edges potentially obscuring it in a
captured image. While there are many geometrical rectification methods, they
were usually developed for arbitrary bends and folds. We consider such
algorithms and propose a novel approach Unfolder developed specifically for
images of documents with a crease from folding in half. Unfolder is robust to
projective distortions of the document image and does not fragment the image in
the vicinity of a crease after rectification. A new Folded Document Images
dataset was created to investigate the rectification accuracy of folded (2, 3,
4, and 8 folds) documents. The dataset includes 1600 images captured when
document placed on a table and when held in hand. The Unfolder algorithm
allowed for a recognition error rate of 0.33, which is better than the advanced
neural network methods DocTr (0.44) and DewarpNet (0.57). The average runtime
for Unfolder was only 0.25 s/image on an iPhone XR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ershov_A/0/1/0/all/0/1&quot;&gt;A.M. Ershov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tropin_D/0/1/0/all/0/1&quot;&gt;D.V. Tropin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Limonova_E/0/1/0/all/0/1&quot;&gt;E.E. Limonova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikolaev_D/0/1/0/all/0/1&quot;&gt;D.P. Nikolaev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arlazarov_V/0/1/0/all/0/1&quot;&gt;V.V. Arlazarov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00487">
<title>Explainable AI in Diagnosing and Anticipating Leukemia Using Transfer Learning Method. (arXiv:2312.00487v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00487</link>
<description rdf:parseType="Literal">&lt;p&gt;This research paper focuses on Acute Lymphoblastic Leukemia (ALL), a form of
blood cancer prevalent in children and teenagers, characterized by the rapid
proliferation of immature white blood cells (WBCs). These atypical cells can
overwhelm healthy cells, leading to severe health consequences. Early and
accurate detection of ALL is vital for effective treatment and improving
survival rates. Traditional diagnostic methods are time-consuming, costly, and
prone to errors. The paper proposes an automated detection approach using
computer-aided diagnostic (CAD) models, leveraging deep learning techniques to
enhance the accuracy and efficiency of leukemia diagnosis. The study utilizes
various transfer learning models like ResNet101V2, VGG19, InceptionV3, and
InceptionResNetV2 for classifying ALL. The methodology includes using the Local
Interpretable Model-Agnostic Explanations (LIME) for ensuring the validity and
reliability of the AI system&apos;s predictions. This approach is critical for
overcoming the &quot;black box&quot; nature of AI, where decisions made by models are
often opaque and unaccountable. The paper highlights that the proposed method
using the InceptionV3 model achieved an impressive 98.38% accuracy,
outperforming other tested models. The results, verified by the LIME algorithm,
showcase the potential of this method in accurately identifying ALL, providing
a valuable tool for medical practitioners. The research underscores the impact
of explainable artificial intelligence (XAI) in medical diagnostics, paving the
way for more transparent and trustworthy AI applications in healthcare.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abir_W/0/1/0/all/0/1&quot;&gt;Wahidul Hasan Abir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uddin_M/0/1/0/all/0/1&quot;&gt;Md. Fahim Uddin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khanam_F/0/1/0/all/0/1&quot;&gt;Faria Rahman Khanam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Mohammad Monirujjaman Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00500">
<title>Global Localization: Utilizing Relative Spatio-Temporal Geometric Constraints from Adjacent and Distant Cameras. (arXiv:2312.00500v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00500</link>
<description rdf:parseType="Literal">&lt;p&gt;Re-localizing a camera from a single image in a previously mapped area is
vital for many computer vision applications in robotics and augmented/virtual
reality. In this work, we address the problem of estimating the 6 DoF camera
pose relative to a global frame from a single image. We propose to leverage a
novel network of relative spatial and temporal geometric constraints to guide
the training of a Deep Network for localization. We employ simultaneously
spatial and temporal relative pose constraints that are obtained not only from
adjacent camera frames but also from camera frames that are distant in the
spatio-temporal space of the scene. We show that our method, through these
constraints, is capable of learning to localize when little or very sparse
ground-truth 3D coordinates are available. In our experiments, this is less
than 1% of available ground-truth data. We evaluate our method on 3 common
visual localization datasets and show that it outperforms other direct pose
estimation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Altillawi_M/0/1/0/all/0/1&quot;&gt;Mohammad Altillawi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pataki_Z/0/1/0/all/0/1&quot;&gt;Zador Pataki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shile Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziyuan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00529">
<title>Algorithm-based diagnostic application for diabetic retinopathy detection. (arXiv:2312.00529v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.00529</link>
<description rdf:parseType="Literal">&lt;p&gt;Diabetic retinopathy (DR) is a growing health problem worldwide and is a
leading cause of visual impairment and blindness, especially among working
people aged 20-65. Its incidence is increasing along with the number of
diabetes cases, and it is more common in developed countries than in developing
countries. Recent research in the field of diabetic retinopathy diagnosis is
using advanced technologies, such as analysis of images obtained by
ophthalmoscopy. Automatic methods for analyzing eye images based on neural
networks, deep learning and image analysis algorithms can improve the
efficiency of diagnosis. This paper describes an automatic DR diagnosis method
that includes processing and analysis of ophthalmoscopic images of the eye. It
uses morphological algorithms to identify the optic disc and lesions
characteristic of DR, such as microaneurysms, hemorrhages and exudates.
Automated DR diagnosis has the potential to improve the efficiency of early
detection of this disease and contribute to reducing the number of cases of
diabetes-related visual impairment. The final step was to create an application
with a graphical user interface that allowed retinal images taken at
cooperating ophthalmology offices to be uploaded to the server. These images
were then analyzed using a developed algorithm to make a diagnosis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cisek_A/0/1/0/all/0/1&quot;&gt;Agnieszka Cisek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Korycinska_K/0/1/0/all/0/1&quot;&gt;Karolina Korycinska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pyziak_L/0/1/0/all/0/1&quot;&gt;Leszek Pyziak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Malicka_M/0/1/0/all/0/1&quot;&gt;Marzena Malicka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wiecek_T/0/1/0/all/0/1&quot;&gt;Tomasz Wiecek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gruzel_G/0/1/0/all/0/1&quot;&gt;Grzegorz Gruzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Szmuc_K/0/1/0/all/0/1&quot;&gt;Kamil Szmuc&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cebulski_J/0/1/0/all/0/1&quot;&gt;Jozef Cebulski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Spyra_M/0/1/0/all/0/1&quot;&gt;Mariusz Spyra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00532">
<title>DeepDR: Deep Structure-Aware RGB-D Inpainting for Diminished Reality. (arXiv:2312.00532v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00532</link>
<description rdf:parseType="Literal">&lt;p&gt;Diminished reality (DR) refers to the removal of real objects from the
environment by virtually replacing them with their background. Modern DR
frameworks use inpainting to hallucinate unobserved regions. While recent deep
learning-based inpainting is promising, the DR use case is complicated by the
need to generate coherent structure and 3D geometry (i.e., depth), in
particular for advanced applications, such as 3D scene editing. In this paper,
we propose DeepDR, a first RGB-D inpainting framework fulfilling all
requirements of DR: Plausible image and geometry inpainting with coherent
structure, running at real-time frame rates, with minimal temporal artifacts.
Our structure-aware generative network allows us to explicitly condition color
and depth outputs on the scene semantics, overcoming the difficulty of
reconstructing sharp and consistent boundaries in regions with complex
backgrounds. Experimental results show that the proposed framework can
outperform related work qualitatively and quantitatively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gsaxner_C/0/1/0/all/0/1&quot;&gt;Christina Gsaxner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mori_S/0/1/0/all/0/1&quot;&gt;Shohei Mori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmalstieg_D/0/1/0/all/0/1&quot;&gt;Dieter Schmalstieg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Egger_J/0/1/0/all/0/1&quot;&gt;Jan Egger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paar_G/0/1/0/all/0/1&quot;&gt;Gerhard Paar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bailer_W/0/1/0/all/0/1&quot;&gt;Werner Bailer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalkofen_D/0/1/0/all/0/1&quot;&gt;Denis Kalkofen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00534">
<title>LiDAR-based curb detection for ground truth annotation in automated driving validation. (arXiv:2312.00534v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00534</link>
<description rdf:parseType="Literal">&lt;p&gt;Curb detection is essential for environmental awareness in Automated Driving
(AD), as it typically limits drivable and non-drivable areas. Annotated data
are necessary for developing and validating an AD function. However, the number
of public datasets with annotated point cloud curbs is scarce. This paper
presents a method for detecting 3D curbs in a sequence of point clouds captured
from a LiDAR sensor, which consists of two main steps. First, our approach
detects the curbs at each scan using a segmentation deep neural network. Then,
a sequence-level processing step estimates the 3D curbs in the reconstructed
point cloud using the odometry of the vehicle. From these 3D points of the
curb, we obtain polylines structured following ASAM OpenLABEL standard. These
detections can be used as pre-annotations in labelling pipelines to efficiently
generate curb-related ground truth data. We validate our approach through an
experiment in which different human annotators were required to annotate curbs
in a group of LiDAR-based sequences with and without our automatically
generated pre-annotations. The results show that the manual annotation time is
reduced by 50.99% thanks to our detections, keeping the data quality level.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Apellaniz_J/0/1/0/all/0/1&quot;&gt;Jose Luis Apell&amp;#xe1;niz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_M/0/1/0/all/0/1&quot;&gt;Mikel Garc&amp;#xed;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aranjuelo_N/0/1/0/all/0/1&quot;&gt;Nerea Aranjuelo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barandiaran_J/0/1/0/all/0/1&quot;&gt;Javier Barandiar&amp;#xe1;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nieto_M/0/1/0/all/0/1&quot;&gt;Marcos Nieto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00548">
<title>Domain Adaptive Imitation Learning with Visual Observation. (arXiv:2312.00548v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.00548</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we consider domain-adaptive imitation learning with visual
observation, where an agent in a target domain learns to perform a task by
observing expert demonstrations in a source domain. Domain adaptive imitation
learning arises in practical scenarios where a robot, receiving visual sensory
data, needs to mimic movements by visually observing other robots from
different angles or observing robots of different shapes. To overcome the
domain shift in cross-domain imitation learning with visual observation, we
propose a novel framework for extracting domain-independent behavioral features
from input observations that can be used to train the learner, based on dual
feature extraction and image reconstruction. Empirical results demonstrate that
our approach outperforms previous algorithms for imitation learning from visual
observation with domain shift.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1&quot;&gt;Sungho Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1&quot;&gt;Seungyul Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1&quot;&gt;Woojun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chae_J/0/1/0/all/0/1&quot;&gt;Jongseong Chae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_W/0/1/0/all/0/1&quot;&gt;Whiyoung Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1&quot;&gt;Youngchul Sung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00570">
<title>Generative models for visualising abstract social processes: Guiding streetview image synthesis of StyleGAN2 with indices of deprivation. (arXiv:2312.00570v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00570</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel application of Generative Adverserial Networks
(GANs) to study visual aspects of social processes. I train a a StyleGAN2-model
on a custom dataset of 14,564 images of London, sourced from Google Streetview
taken in London. After training, I invert the images in the training set,
finding points in the model&apos;s latent space that correspond to them, and compare
results from three inversion techniques. I connect each data point with
metadata from the Indices of Multiple Deprivation, describing income, health
and environmental quality in the area where the photographs were taken. It is
then possible to map which parts of the model&apos;s latent space encode visual
features that are distinctive for health, income and environmental quality, and
condition the synthesis of new images based on these factors. The synthetic
images created reflect visual features of social processes that were previously
unknown and difficult to study, describing recurring visual differences between
deprived and privileged areas in London. GANs are known for their capability to
produce a continuous range of images that exhibit visual differences. The paper
tests how to exploit this ability through visual comparisons in still images as
well as through an interactive website where users can guide image synthesis
with sliders. Though conditioned synthesis has its limitations and the results
are difficult to validate, the paper points to the potential for generative
models to be repurposed to be parts of social scientific methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knuutila_A/0/1/0/all/0/1&quot;&gt;Aleksi Knuutila&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00583">
<title>MD-Splatting: Learning Metric Deformation from 4D Gaussians in Highly Deformable Scenes. (arXiv:2312.00583v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00583</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate 3D tracking in highly deformable scenes with occlusions and shadows
can facilitate new applications in robotics, augmented reality, and generative
AI. However, tracking under these conditions is extremely challenging due to
the ambiguity that arises with large deformations, shadows, and occlusions. We
introduce MD-Splatting, an approach for simultaneous 3D tracking and novel view
synthesis, using video captures of a dynamic scene from various camera poses.
MD-Splatting builds on recent advances in Gaussian splatting, a method that
learns the properties of a large number of Gaussians for state-of-the-art and
fast novel view synthesis. MD-Splatting learns a deformation function to
project a set of Gaussians with non-metric, thus canonical, properties into
metric space. The deformation function uses a neural-voxel encoding and a
multilayer perceptron (MLP) to infer Gaussian position, rotation, and a shadow
scalar. We enforce physics-inspired regularization terms based on local
rigidity, conservation of momentum, and isometry, which leads to trajectories
with smaller trajectory errors. MD-Splatting achieves high-quality 3D tracking
on highly deformable scenes with shadows and occlusions. Compared to
state-of-the-art, we improve 3D tracking by an average of 23.9 %, while
simultaneously achieving high-quality novel view synthesis. With sufficient
texture such as in scene 6, MD-Splatting achieves a median tracking error of
3.39 mm on a cloth of 1 x 1 meters in size. Project website:
https://md-splatting.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duisterhof_B/0/1/0/all/0/1&quot;&gt;Bardienus P. Duisterhof&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mandi_Z/0/1/0/all/0/1&quot;&gt;Zhao Mandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yunchao Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jia-Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1&quot;&gt;Mike Zheng Shou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Shuran Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ichnowski_J/0/1/0/all/0/1&quot;&gt;Jeffrey Ichnowski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00588">
<title>LucidDreaming: Controllable Object-Centric 3D Generation. (arXiv:2312.00588v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00588</link>
<description rdf:parseType="Literal">&lt;p&gt;With the recent development of generative models, Text-to-3D generations have
also seen significant growth. Nonetheless, achieving precise control over 3D
generation continues to be an arduous task, as using text to control often
leads to missing objects and imprecise locations. Contemporary strategies for
enhancing controllability in 3D generation often entail the introduction of
additional parameters, such as customized diffusion models. This often induces
hardness in adapting to different diffusion models or creating distinct
objects.
&lt;/p&gt;
&lt;p&gt;In this paper, we present LucidDreaming as an effective pipeline capable of
fine-grained control over 3D generation. It requires only minimal input of 3D
bounding boxes, which can be deduced from a simple text prompt using a Large
Language Model. Specifically, we propose clipped ray sampling to separately
render and optimize objects with user specifications. We also introduce
object-centric density blob bias, fostering the separation of generated
objects. With individual rendering and optimizing of objects, our method excels
not only in controlled content generation from scratch but also within the
pre-trained NeRF scenes. In such scenarios, existing generative approaches
often disrupt the integrity of the original scene, and current editing methods
struggle to synthesize new content in empty spaces. We show that our method
exhibits remarkable adaptability across a spectrum of mainstream Score
Distillation Sampling-based 3D generation frameworks, and achieves superior
alignment of 3D content when compared to baseline approaches. We also provide a
dataset of prompts with 3D bounding boxes, benchmarking 3D spatial
controllability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhaoning Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Ming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00589">
<title>Merlin:Empowering Multimodal LLMs with Foresight Minds. (arXiv:2312.00589v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00589</link>
<description rdf:parseType="Literal">&lt;p&gt;Humans possess the remarkable ability to foresee the future to a certain
extent based on present observations, a skill we term as foresight minds.
However, this capability remains largely under explored within existing
Multimodal Large Language Models (MLLMs), hindering their capacity to learn the
fundamental principles of how things operate and the intentions behind the
observed subjects. To address this issue, we introduce the integration of
future modeling into the existing learning frameworks of MLLMs. By utilizing
the subject trajectory, a highly structured representation of a consecutive
frame sequence, as a learning objective, we aim to bridge the gap between the
past and the future. We propose two innovative methods to empower MLLMs with
foresight minds, Foresight Pre-Training (FPT) and Foresight Instruction-Tuning
(FIT), which are inspired by the modern learning paradigm of LLMs.
Specifically, FPT jointly training various tasks centered on trajectories,
enabling MLLMs to learn how to attend and predict entire trajectories from a
given initial observation. Then, FIT requires MLLMs to first predict
trajectories of related objects and then reason about potential future events
based on them. Aided by FPT and FIT, we build a novel and unified MLLM named
Merlin that supports multi-images input and analysis about potential actions of
multiple objects for the future reasoning. Experimental results show Merlin
powerful foresight minds with impressive performance on both future reasoning
and visual comprehension tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_E/0/1/0/all/0/1&quot;&gt;En Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Liang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yana Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jinrong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Dongming Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1&quot;&gt;Lingyu Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1&quot;&gt;Haoran Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tiancai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1&quot;&gt;Zheng Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiangyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_W/0/1/0/all/0/1&quot;&gt;Wenbing Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00591">
<title>Less is More: Learning Reference Knowledge Using No-Reference Image Quality Assessment. (arXiv:2312.00591v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00591</link>
<description rdf:parseType="Literal">&lt;p&gt;Image Quality Assessment (IQA) with reference images have achieved great
success by imitating the human vision system, in which the image quality is
effectively assessed by comparing the query image with its pristine reference
image. However, for the images in the wild, it is quite difficult to access
accurate reference images. We argue that it is possible to learn reference
knowledge under the No-Reference Image Quality Assessment (NR-IQA) setting,
which is effective and efficient empirically. Concretely, by innovatively
introducing a novel feature distillation method in IQA, we propose a new
framework to learn comparative knowledge from non-aligned reference images. And
then, to achieve fast convergence and avoid overfitting, we further propose an
inductive bias regularization. Such a framework not only solves the congenital
defects of NR-IQA but also improves the feature extraction framework, enabling
it to express more abundant quality information. Surprisingly, our method
utilizes less input while obtaining a more significant improvement compared to
the teacher models. Extensive experiments on eight standard NR-IQA datasets
demonstrate the superior performance to the state-of-the-art NR-IQA methods,
i.e., achieving the PLCC values of 0.917 (vs. 0.884 in LIVEC) and 0.686 (vs.
0.661 in LIVEFB).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xudong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Jingyuan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1&quot;&gt;Xiawu Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1&quot;&gt;Runze Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1&quot;&gt;Enwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yuting Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yunhang Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Ke Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yutao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_P/0/1/0/all/0/1&quot;&gt;Pingyang Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1&quot;&gt;Rongrong Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00592">
<title>Tracking Object Positions in Reinforcement Learning: A Metric for Keypoint Detection (extended version). (arXiv:2312.00592v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.00592</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning (RL) for robot control typically requires a detailed
representation of the environment state, including information about
task-relevant objects not directly measurable. Keypoint detectors, such as
spatial autoencoders (SAEs), are a common approach to extracting a
low-dimensional representation from high-dimensional image data. SAEs aim at
spatial features such as object positions, which are often useful
representations in robotic RL. However, whether an SAE is actually able to
track objects in the scene and thus yields a spatial state representation well
suited for RL tasks has rarely been examined due to a lack of established
metrics. In this paper, we propose to assess the performance of an SAE instance
by measuring how well keypoints track ground truth objects in images. We
present a computationally lightweight metric and use it to evaluate common
baseline SAE architectures on image data from a simulated robot task. We find
that common SAEs differ substantially in their spatial extraction capability.
Furthermore, we validate that SAEs that perform well in our metric achieve
superior performance when used in downstream RL. Thus, our metric is an
effective and lightweight indicator of RL performance before executing
expensive RL training. Building on these insights, we identify three key
modifications of SAE architectures to improve tracking performance. We make our
code available at anonymous.4open.science/r/sae-rl.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cramer_E/0/1/0/all/0/1&quot;&gt;Emma Cramer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reiher_J/0/1/0/all/0/1&quot;&gt;Jonas Reiher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trimpe_S/0/1/0/all/0/1&quot;&gt;Sebastian Trimpe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00593">
<title>Event Recognition in Laparoscopic Gynecology Videos with Hybrid Transformers. (arXiv:2312.00593v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00593</link>
<description rdf:parseType="Literal">&lt;p&gt;Analyzing laparoscopic surgery videos presents a complex and multifaceted
challenge, with applications including surgical training, intra-operative
surgical complication prediction, and post-operative surgical assessment.
Identifying crucial events within these videos is a significant prerequisite in
a majority of these applications. In this paper, we introduce a comprehensive
dataset tailored for relevant event recognition in laparoscopic gynecology
videos. Our dataset includes annotations for critical events associated with
major intra-operative challenges and post-operative complications. To validate
the precision of our annotations, we assess event recognition performance using
several CNN-RNN architectures. Furthermore, we introduce and evaluate a hybrid
transformer architecture coupled with a customized training-inference framework
to recognize four specific events in laparoscopic surgery videos. Leveraging
the Transformer networks, our proposed architecture harnesses inter-frame
dependencies to counteract the adverse effects of relevant content occlusion,
motion blur, and surgical scene variation, thus significantly enhancing event
recognition accuracy. Moreover, we present a frame sampling strategy designed
to manage variations in surgical scenes and the surgeons&apos; skill level,
resulting in event recognition with high temporal resolution. We empirically
demonstrate the superiority of our proposed methodology in event recognition
compared to conventional CNN-RNN architectures through a series of extensive
experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nasirihaghighi_S/0/1/0/all/0/1&quot;&gt;Sahar Nasirihaghighi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghamsarian_N/0/1/0/all/0/1&quot;&gt;Negin Ghamsarian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Husslein_H/0/1/0/all/0/1&quot;&gt;Heinrich Husslein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schoeffmann_K/0/1/0/all/0/1&quot;&gt;Klaus Schoeffmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00596">
<title>BCN: Batch Channel Normalization for Image Classification. (arXiv:2312.00596v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00596</link>
<description rdf:parseType="Literal">&lt;p&gt;Normalization techniques have been widely used in the field of deep learning
due to their capability of enabling higher learning rates and are less careful
in initialization. However, the effectiveness of popular normalization
technologies is typically limited to specific areas. Unlike the standard Batch
Normalization (BN) and Layer Normalization (LN), where BN computes the mean and
variance along the (N,H,W) dimensions and LN computes the mean and variance
along the (C,H,W) dimensions (N, C, H and W are the batch, channel, spatial
height and width dimension, respectively), this paper presents a novel
normalization technique called Batch Channel Normalization (BCN). To exploit
both the channel and batch dependence and adaptively and combine the advantages
of BN and LN based on specific datasets or tasks, BCN separately normalizes
inputs along the (N, H, W) and (C, H, W) axes, then combines the normalized
outputs based on adaptive parameters. As a basic block, BCN can be easily
integrated into existing models for various applications in the field of
computer vision. Empirical results show that the proposed technique can be
seamlessly applied to various versions of CNN or Vision Transformer
architecture. The code is publicly available at
https://github.com/AfifaKhaled/BatchChannel-Normalization
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khaled_A/0/1/0/all/0/1&quot;&gt;Afifa Khaled&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_J/0/1/0/all/0/1&quot;&gt;Jia Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1&quot;&gt;Kun He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00597">
<title>UAVs and Birds: Enhancing Short-Range Navigation through Budgerigar Flight Studies. (arXiv:2312.00597v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.00597</link>
<description rdf:parseType="Literal">&lt;p&gt;This study delves into the flight behaviors of Budgerigars (Melopsittacus
undulatus) to gain insights into their flight trajectories and movements. Using
3D reconstruction from stereo video camera recordings, we closely examine the
velocity and acceleration patterns during three flight motion takeoff, flying
and landing. The findings not only contribute to our understanding of bird
behaviors but also hold significant implications for the advancement of
algorithms in Unmanned Aerial Vehicles (UAVs). The research aims to bridge the
gap between biological principles observed in birds and the application of
these insights in developing more efficient and autonomous UAVs. In the context
of the increasing use of drones, this study focuses on the biologically
inspired principles drawn from bird behaviors, particularly during takeoff,
flying and landing flight, to enhance UAV capabilities. The dataset created for
this research sheds light on Budgerigars&apos; takeoff, flying, and landing
techniques, emphasizing their ability to control speed across different
situations and surfaces. The study underscores the potential of incorporating
these principles into UAV algorithms, addressing challenges related to
short-range navigation, takeoff, flying, and landing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1&quot;&gt;Md. Mahmudur Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_S/0/1/0/all/0/1&quot;&gt;Sajid Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1&quot;&gt;Showren Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeba_S/0/1/0/all/0/1&quot;&gt;Sadia Jahan Zeba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karmaker_D/0/1/0/all/0/1&quot;&gt;Debajyoti Karmaker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00598">
<title>Learning from One Continuous Video Stream. (arXiv:2312.00598v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00598</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a framework for online learning from a single continuous video
stream -- the way people and animals learn, without mini-batches, data
augmentation or shuffling. This poses great challenges given the high
correlation between consecutive video frames and there is very little prior
work on it. Our framework allows us to do a first deep dive into the topic and
includes a collection of streams and tasks composed from two existing video
datasets, plus methodology for performance evaluation that considers both
adaptation and generalization. We employ pixel-to-pixel modelling as a
practical and flexible way to switch between pre-training and single-stream
evaluation as well as between arbitrary tasks, without ever requiring changes
to models and always using the same pixel loss. Equipped with this framework we
obtained large single-stream learning gains from pre-training with a novel
family of future prediction tasks, found that momentum hurts, and that the pace
of weight updates matters. The combination of these insights leads to matching
the performance of IID learning with batch size 1, when using the same
architecture and without costly replay buffers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o Carreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+King_M/0/1/0/all/0/1&quot;&gt;Michael King&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patraucean_V/0/1/0/all/0/1&quot;&gt;Viorica P&amp;#x103;tr&amp;#x103;ucean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gokay_D/0/1/0/all/0/1&quot;&gt;Dilara Gokay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ionescu_C/0/1/0/all/0/1&quot;&gt;C&amp;#x103;t&amp;#x103;lin Ionescu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zoran_D/0/1/0/all/0/1&quot;&gt;Daniel Zoran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heyward_J/0/1/0/all/0/1&quot;&gt;Joseph Heyward&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doersch_C/0/1/0/all/0/1&quot;&gt;Carl Doersch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aytar_Y/0/1/0/all/0/1&quot;&gt;Yusuf Aytar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damen_D/0/1/0/all/0/1&quot;&gt;Dima Damen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1&quot;&gt;Andrew Zisserman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00627">
<title>Rethinking the Domain Gap in Near-infrared Face Recognition. (arXiv:2312.00627v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00627</link>
<description rdf:parseType="Literal">&lt;p&gt;Heterogeneous face recognition (HFR) involves the intricate task of matching
face images across the visual domains of visible (VIS) and near-infrared (NIR).
While much of the existing literature on HFR identifies the domain gap as a
primary challenge and directs efforts towards bridging it at either the input
or feature level, our work deviates from this trend. We observe that large
neural networks, unlike their smaller counterparts, when pre-trained on large
scale homogeneous VIS data, demonstrate exceptional zero-shot performance in
HFR, suggesting that the domain gap might be less pronounced than previously
believed. By approaching the HFR problem as one of low-data fine-tuning, we
introduce a straightforward framework: comprehensive pre-training, succeeded by
a regularized fine-tuning strategy, that matches or surpasses the current
state-of-the-art on four publicly available benchmarks. Corresponding codes can
be found at https://github.com/michaeltrs/RethinkNIRVIS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tarasiou_M/0/1/0/all/0/1&quot;&gt;Michail Tarasiou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1&quot;&gt;Jiankang Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zafeiriou_S/0/1/0/all/0/1&quot;&gt;Stefanos Zafeiriou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00633">
<title>Towards Efficient 3D Object Detection in Bird&apos;s-Eye-View Space for Autonomous Driving: A Convolutional-Only Approach. (arXiv:2312.00633v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00633</link>
<description rdf:parseType="Literal">&lt;p&gt;3D object detection in Bird&apos;s-Eye-View (BEV) space has recently emerged as a
prevalent approach in the field of autonomous driving. Despite the demonstrated
improvements in accuracy and velocity estimation compared to perspective view
methods, the deployment of BEV-based techniques in real-world autonomous
vehicles remains challenging. This is primarily due to their reliance on
vision-transformer (ViT) based architectures, which introduce quadratic
complexity with respect to the input resolution. To address this issue, we
propose an efficient BEV-based 3D detection framework called BEVENet, which
leverages a convolutional-only architectural design to circumvent the
limitations of ViT models while maintaining the effectiveness of BEV-based
methods. Our experiments show that BEVENet is 3$\times$ faster than
contemporary state-of-the-art (SOTA) approaches on the NuScenes challenge,
achieving a mean average precision (mAP) of 0.456 and a nuScenes detection
score (NDS) of 0.555 on the NuScenes validation dataset, with an inference
speed of 47.6 frames per second. To the best of our knowledge, this study
stands as the first to achieve such significant efficiency improvements for
BEV-based methods, highlighting their enhanced feasibility for real-world
autonomous driving applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuxin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1&quot;&gt;Qiang Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1&quot;&gt;Mengying Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yuxin Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeo_C/0/1/0/all/0/1&quot;&gt;Chaikiat Yeo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yiheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zihang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Nini Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hsuanhan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaojun Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00634">
<title>A Recent Survey of Vision Transformers for Medical Image Segmentation. (arXiv:2312.00634v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.00634</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical image segmentation plays a crucial role in various healthcare
applications, enabling accurate diagnosis, treatment planning, and disease
monitoring. In recent years, Vision Transformers (ViTs) have emerged as a
promising technique for addressing the challenges in medical image
segmentation. In medical images, structures are usually highly interconnected
and globally distributed. ViTs utilize their multi-scale attention mechanism to
model the long-range relationships in the images. However, they do lack
image-related inductive bias and translational invariance, potentially
impacting their performance. Recently, researchers have come up with various
ViT-based approaches that incorporate CNNs in their architectures, known as
Hybrid Vision Transformers (HVTs) to capture local correlation in addition to
the global information in the images. This survey paper provides a detailed
review of the recent advancements in ViTs and HVTs for medical image
segmentation. Along with the categorization of ViT and HVT-based medical image
segmentation approaches we also present a detailed overview of their real-time
applications in several medical image modalities. This survey may serve as a
valuable resource for researchers, healthcare practitioners, and students in
understanding the state-of-the-art approaches for ViT-based medical image
segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Asifullah Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rauf_Z/0/1/0/all/0/1&quot;&gt;Zunaira Rauf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Abdul Rehman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rathore_S/0/1/0/all/0/1&quot;&gt;Saima Rathore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Saddam Hussain Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shah_S/0/1/0/all/0/1&quot;&gt;Sahar Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Farooq_U/0/1/0/all/0/1&quot;&gt;Umair Farooq&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Asif_H/0/1/0/all/0/1&quot;&gt;Hifsa Asif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Asif_A/0/1/0/all/0/1&quot;&gt;Aqsa Asif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zahoora_U/0/1/0/all/0/1&quot;&gt;Umme Zahoora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khalil_R/0/1/0/all/0/1&quot;&gt;Rafi Ullah Khalil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qamar_S/0/1/0/all/0/1&quot;&gt;Suleman Qamar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Asif_U/0/1/0/all/0/1&quot;&gt;Umme Hani Asif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khan_F/0/1/0/all/0/1&quot;&gt;Faiza Babar Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Majid_A/0/1/0/all/0/1&quot;&gt;Abdul Majid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gwak_J/0/1/0/all/0/1&quot;&gt;Jeonghwan Gwak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00639">
<title>EvE: Exploiting Generative Priors for Radiance Field Enrichment. (arXiv:2312.00639v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00639</link>
<description rdf:parseType="Literal">&lt;p&gt;Modeling large-scale scenes from unconstrained image collections in-the-wild
has proven to be a major challenge in computer vision. Existing methods
tackling in-the-wild neural rendering operate in a closed-world setting, where
knowledge is limited to a scene&apos;s captured images within a training set. We
propose EvE, which is, to the best of our knowledge, the first method
leveraging generative priors to improve in-the-wild scene modeling. We employ
pre-trained generative networks to enrich K-Planes representations with
extrinsic knowledge. To this end, we define an alternating training procedure
to conduct optimization guidance of K-Planes trained on the training set. We
carry out extensive experiments and verify the merit of our method on synthetic
data as well as real tourism photo collections. EvE enhances rendered scenes
with richer details and outperforms the state of the art on the task of novel
view synthesis in-the-wild. Our project page can be found at
https://eve-nvs.github.io .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kassab_K/0/1/0/all/0/1&quot;&gt;Karim Kassab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schnepf_A/0/1/0/all/0/1&quot;&gt;Antoine Schnepf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Franceschi_J/0/1/0/all/0/1&quot;&gt;Jean-Yves Franceschi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caraffa_L/0/1/0/all/0/1&quot;&gt;Laurent Caraffa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mary_J/0/1/0/all/0/1&quot;&gt;Jeremie Mary&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gouet_Brunet_V/0/1/0/all/0/1&quot;&gt;Val&amp;#xe9;rie Gouet-Brunet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00648">
<title>SPOT: Self-Training with Patch-Order Permutation for Object-Centric Learning with Autoregressive Transformers. (arXiv:2312.00648v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00648</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised object-centric learning aims to decompose scenes into
interpretable object entities, termed slots. Slot-based auto-encoders stand out
as a prominent method for this task. Within them, crucial aspects include
guiding the encoder to generate object-specific slots and ensuring the decoder
utilizes them during reconstruction. This work introduces two novel techniques,
(i) an attention-based self-training approach, which distills superior
slot-based attention masks from the decoder to the encoder, enhancing object
segmentation, and (ii) an innovative patch-order permutation strategy for
autoregressive transformers that strengthens the role of slot vectors in
reconstruction. The effectiveness of these strategies is showcased
experimentally. The combined approach significantly surpasses prior slot-based
autoencoder methods in unsupervised object segmentation, especially with
complex real-world images. We provide the implementation code at
https://github.com/gkakogeorgiou/spot .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kakogeorgiou_I/0/1/0/all/0/1&quot;&gt;Ioannis Kakogeorgiou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gidaris_S/0/1/0/all/0/1&quot;&gt;Spyros Gidaris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karantzalos_K/0/1/0/all/0/1&quot;&gt;Konstantinos Karantzalos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Komodakis_N/0/1/0/all/0/1&quot;&gt;Nikos Komodakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00651">
<title>TrackDiffusion: Multi-object Tracking Data Generation via Diffusion Models. (arXiv:2312.00651v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00651</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have gained prominence in generating data for perception
tasks such as image classification and object detection. However, the potential
in generating high-quality tracking sequences, a crucial aspect in the field of
video perception, has not been fully investigated. To address this gap, we
propose TrackDiffusion, a novel architecture designed to generate continuous
video sequences from the tracklets. TrackDiffusion represents a significant
departure from the traditional layout-to-image (L2I) generation and copy-paste
synthesis focusing on static image elements like bounding boxes by empowering
image diffusion models to encompass dynamic and continuous tracking
trajectories, thereby capturing complex motion nuances and ensuring instance
consistency among video frames. For the first time, we demonstrate that the
generated video sequences can be utilized for training multi-object tracking
(MOT) systems, leading to significant improvement in tracker performance.
Experimental results show that our model significantly enhances instance
consistency in generated video sequences, leading to improved perceptual
metrics. Our approach achieves an improvement of 8.7 in TrackAP and 11.8 in
TrackAP$_{50}$ on the YTVIS dataset, underscoring its potential to redefine the
standards of video data generation for MOT tasks and beyond.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Pengxiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhili Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1&quot;&gt;Lanqing Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuge_Y/0/1/0/all/0/1&quot;&gt;Yunzhi Zhuge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeung_D/0/1/0/all/0/1&quot;&gt;Dit-Yan Yeung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Huchuan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1&quot;&gt;Xu Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00661">
<title>Dual-Domain Multi-Contrast MRI Reconstruction with Synthesis-based Fusion Network. (arXiv:2312.00661v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.00661</link>
<description rdf:parseType="Literal">&lt;p&gt;Purpose: To develop an efficient dual-domain reconstruction framework for
multi-contrast MRI, with the focus on minimising cross-contrast misalignment in
both the image and the frequency domains to enhance optimisation. Theory and
Methods: Our proposed framework, based on deep learning, facilitates the
optimisation for under-sampled target contrast using fully-sampled reference
contrast that is quicker to acquire. The method consists of three key steps: 1)
Learning to synthesise data resembling the target contrast from the reference
contrast; 2) Registering the multi-contrast data to reduce inter-scan motion;
and 3) Utilising the registered data for reconstructing the target contrast.
These steps involve learning in both domains with regularisation applied to
ensure their consistency. We also compare the reconstruction performance with
existing deep learning-based methods using a dataset of brain MRI scans.
Results: Extensive experiments demonstrate the superiority of our proposed
framework, for up to an 8-fold acceleration rate, compared to state-of-the-art
algorithms. Comprehensive analysis and ablation studies further present the
effectiveness of the proposed components. Conclusion:Our dual-domain framework
offers a promising approach to multi-contrast MRI reconstruction. It can also
be integrated with existing methods to further enhance the reconstruction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Junwei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lio_P/0/1/0/all/0/1&quot;&gt;Pietro Li&amp;#xf2;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00663">
<title>Generalized Label-Efficient 3D Scene Parsing via Hierarchical Feature Aligned Pre-Training and Region-Aware Fine-tuning. (arXiv:2312.00663v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00663</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural network models have achieved remarkable progress in 3D scene
understanding while trained in the closed-set setting and with full labels.
However, the major bottleneck for current 3D recognition approaches is that
they do not have the capacity to recognize any unseen novel classes beyond the
training categories in diverse kinds of real-world applications. In the
meantime, current state-of-the-art 3D scene understanding approaches primarily
require high-quality labels to train neural networks, which merely perform well
in a fully supervised manner. This work presents a generalized and simple
framework for dealing with 3D scene understanding when the labeled scenes are
quite limited. To extract knowledge for novel categories from the pre-trained
vision-language models, we propose a hierarchical feature-aligned pre-training
and knowledge distillation strategy to extract and distill meaningful
information from large-scale vision-language models, which helps benefit the
open-vocabulary scene understanding tasks. To leverage the boundary
information, we propose a novel energy-based loss with boundary awareness
benefiting from the region-level boundary predictions. To encourage latent
instance discrimination and to guarantee efficiency, we propose the
unsupervised region-level semantic contrastive learning scheme for point
clouds, using confident predictions of the neural network to discriminate the
intermediate feature embeddings at multiple stages. Extensive experiments with
both indoor and outdoor scenes demonstrated the effectiveness of our approach
in both data-efficient learning and open-world few-shot learning. All codes,
models, and data are made publicly available at:
https://drive.google.com/drive/folders/1M58V-PtR8DBEwD296zJkNg_m2qq-MTAP?usp=sharing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kangcheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong-Jin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1&quot;&gt;Kai Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Ming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Baoquan Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00671">
<title>CellMixer: Annotation-free Semantic Cell Segmentation of Heterogeneous Cell Populations. (arXiv:2312.00671v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00671</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, several unsupervised cell segmentation methods have been
presented, trying to omit the requirement of laborious pixel-level annotations
for the training of a cell segmentation model. Most if not all of these methods
handle the instance segmentation task by focusing on the detection of different
cell instances ignoring their type. While such models prove adequate for
certain tasks, like cell counting, other applications require the
identification of each cell&apos;s type. In this paper, we present CellMixer, an
innovative annotation-free approach for the semantic segmentation of
heterogeneous cell populations. Our augmentation-based method enables the
training of a segmentation model from image-level labels of homogeneous cell
populations. Our results show that CellMixer can achieve competitive
segmentation performance across multiple cell types and imaging modalities,
demonstrating the method&apos;s scalability and potential for broader applications
in medical imaging, cellular biology, and diagnostics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naouar_M/0/1/0/all/0/1&quot;&gt;Mehdi Naouar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalweit_G/0/1/0/all/0/1&quot;&gt;Gabriel Kalweit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klett_A/0/1/0/all/0/1&quot;&gt;Anusha Klett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vogt_Y/0/1/0/all/0/1&quot;&gt;Yannick Vogt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silvestrini_P/0/1/0/all/0/1&quot;&gt;Paula Silvestrini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramirez_D/0/1/0/all/0/1&quot;&gt;Diana Laura Infante Ramirez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mertelsmann_R/0/1/0/all/0/1&quot;&gt;Roland Mertelsmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boedecker_J/0/1/0/all/0/1&quot;&gt;Joschka Boedecker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalweit_M/0/1/0/all/0/1&quot;&gt;Maria Kalweit&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00674">
<title>LightCLIP: Learning Multi-Level Interaction for Lightweight Vision-Language Models. (arXiv:2312.00674v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00674</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision-language pre-training like CLIP has shown promising performance on
various downstream tasks such as zero-shot image classification and image-text
retrieval. Most of the existing CLIP-alike works usually adopt relatively large
image encoders like ResNet50 and ViT, while the lightweight counterparts are
rarely discussed. In this paper, we propose a multi-level interaction paradigm
for training lightweight CLIP models. Firstly, to mitigate the problem that
some image-text pairs are not strictly one-to-one correspondence, we improve
the conventional global instance-level alignment objective by softening the
label of negative samples progressively. Secondly, a relaxed bipartite matching
based token-level alignment objective is introduced for finer-grained alignment
between image patches and textual words. Moreover, based on the observation
that the accuracy of CLIP model does not increase correspondingly as the
parameters of text encoder increase, an extra objective of masked language
modeling (MLM) is leveraged for maximizing the potential of the shortened text
encoder. In practice, an auxiliary fusion module injecting unmasked image
embedding into masked text embedding at different network stages is proposed
for enhancing the MLM. Extensive experiments show that without introducing
additional computational cost during inference, the proposed method achieves a
higher performance on multiple downstream tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1&quot;&gt;Ying Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1&quot;&gt;Wei He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1&quot;&gt;Kai Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yehui Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1&quot;&gt;Tianyu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_F/0/1/0/all/0/1&quot;&gt;Fanyi Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunhe Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00677">
<title>Unsupervised Adaptive Implicit Neural Representation Learning for Scan-Specific MRI Reconstruction. (arXiv:2312.00677v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.00677</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent studies on MRI reconstruction, advances have shown significant
promise for further accelerating the MRI acquisition. Most state-of-the-art
methods require a large amount of fully-sampled data to optimise reconstruction
models, which is impractical and expensive under certain clinical settings. On
the other hand, for unsupervised scan-specific reconstruction methods,
overfitting is likely to happen due to insufficient supervision, while
restrictions on acceleration rates and under-sampling patterns further limit
their applicability. To this end, we propose an unsupervised, adaptive
coarse-to-fine framework that enhances reconstruction quality without being
constrained by the sparsity levels or patterns in under-sampling. The framework
employs an implicit neural representation for scan-specific MRI reconstruction,
learning a mapping from multi-dimensional coordinates to their corresponding
signal intensities. Moreover, we integrate a novel learning strategy that
progressively refines the use of acquired k-space signals for self-supervision.
This approach effectively adjusts the proportion of supervising signals from
unevenly distributed information across different frequency bands, thus
mitigating the issue of overfitting while improving the overall reconstruction.
Comprehensive evaluation on a public dataset, including both 2D and 3D data,
has shown that our method outperforms current state-of-the-art scan-specific
MRI reconstruction techniques, for up to 8-fold under-sampling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Junwei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lio_P/0/1/0/all/0/1&quot;&gt;Pietro Li&amp;#xf2;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00689">
<title>Infrared Image Super-Resolution via GAN. (arXiv:2312.00689v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.00689</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability of generative models to accurately fit data distributions has
resulted in their widespread adoption and success in fields such as computer
vision and natural language processing. In this chapter, we provide a brief
overview of the application of generative models in the domain of infrared (IR)
image super-resolution, including a discussion of the various challenges and
adversarial training methods employed. We propose potential areas for further
investigation and advancement in the application of generative models for IR
image super-resolution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yongsong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Omachi_S/0/1/0/all/0/1&quot;&gt;Shinichiro Omachi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00690">
<title>Open-vocabulary object 6D pose estimation. (arXiv:2312.00690v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00690</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce the new setting of open-vocabulary object 6D pose estimation, in
which a textual prompt is used to specify the object of interest. In contrast
to existing approaches, in our setting (i) the object of interest is specified
solely through the textual prompt, (ii) no object model (e.g. CAD or video
sequence) is required at inference, (iii) the object is imaged from two
different viewpoints of two different scenes, and (iv) the object was not
observed during the training phase. To operate in this setting, we introduce a
novel approach that leverages a Vision-Language Model to segment the object of
interest from two distinct scenes and to estimate its relative 6D pose. The key
of our approach is a carefully devised strategy to fuse object-level
information provided by the prompt with local image features, resulting in a
feature space that can generalize to novel concepts. We validate our approach
on a new benchmark based on two popular datasets, REAL275 and Toyota-Light,
which collectively encompass 39 object instances appearing in four thousand
image pairs. The results demonstrate that our approach outperforms both a
well-established hand-crafted method and a recent deep learning-based baseline
in estimating the relative 6D pose of objects in different scenes. Project
website: https://jcorsetti.github.io/oryon-website/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corsetti_J/0/1/0/all/0/1&quot;&gt;Jaime Corsetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boscaini_D/0/1/0/all/0/1&quot;&gt;Davide Boscaini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_C/0/1/0/all/0/1&quot;&gt;Changjae Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cavallaro_A/0/1/0/all/0/1&quot;&gt;Andrea Cavallaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poiesi_F/0/1/0/all/0/1&quot;&gt;Fabio Poiesi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00692">
<title>VisionaryVR: An Optical Simulation Tool for Evaluating and Optimizing Vision Correction Solutions in Virtual Reality. (arXiv:2312.00692v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00692</link>
<description rdf:parseType="Literal">&lt;p&gt;Developing and evaluating vision science methods require robust and efficient
tools for assessing their performance in various real-world scenarios. This
study presents a novel virtual reality (VR) simulation tool that simulates
real-world optical methods while giving high experimental control to the
experiment. The tool incorporates an experiment controller, to smoothly and
easily handle multiple conditions, a generic eye-tracking controller, that
works with most common VR eye-trackers, a configurable defocus simulator, and a
generic VR questionnaire loader to assess participants&apos; behavior in virtual
reality. This VR-based simulation tool bridges the gap between theoretical and
applied research on new optical methods, corrections, and therapies. It enables
vision scientists to increase their research tools with a robust, realistic,
and fast research environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hosp_B/0/1/0/all/0/1&quot;&gt;Benedikt W. Hosp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dechant_M/0/1/0/all/0/1&quot;&gt;Martin Dechant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sauer_Y/0/1/0/all/0/1&quot;&gt;Yannick Sauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwala_R/0/1/0/all/0/1&quot;&gt;Rajat Agarwala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wahl_S/0/1/0/all/0/1&quot;&gt;Siegfried Wahl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00694">
<title>Object Detector Differences when using Synthetic and Real Training Data. (arXiv:2312.00694v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00694</link>
<description rdf:parseType="Literal">&lt;p&gt;To train well-performing generalizing neural networks, sufficiently large and
diverse datasets are needed. Collecting data while adhering to privacy
legislation becomes increasingly difficult and annotating these large datasets
is both a resource-heavy and time-consuming task. An approach to overcome these
difficulties is to use synthetic data since it is inherently scalable and can
be automatically annotated. However, how training on synthetic data affects the
layers of a neural network is still unclear. In this paper, we train the YOLOv3
object detector on real and synthetic images from city environments. We perform
a similarity analysis using Centered Kernel Alignment (CKA) to explore the
effects of training on synthetic data on a layer-wise basis. The analysis
captures the architecture of the detector while showing both different and
similar patterns between different models. With this similarity analysis we
want to give insights on how training synthetic data affects each layer and to
give a better understanding of the inner workings of complex neural networks.
The results show that the largest similarity between a detector trained on real
data and a detector trained on synthetic data was in the early layers, and the
largest difference was in the head part. The results also show that no major
difference in performance or similarity could be seen between frozen and
unfrozen backbone.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ljungqvist_M/0/1/0/all/0/1&quot;&gt;Martin Georg Ljungqvist&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nordander_O/0/1/0/all/0/1&quot;&gt;Otto Nordander&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skans_M/0/1/0/all/0/1&quot;&gt;Markus Skans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mildner_A/0/1/0/all/0/1&quot;&gt;Arvid Mildner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tony Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nugues_P/0/1/0/all/0/1&quot;&gt;Pierre Nugues&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00699">
<title>Rethinking Detection Based Table Structure Recognition for Visually Rich Documents. (arXiv:2312.00699v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00699</link>
<description rdf:parseType="Literal">&lt;p&gt;Table Structure Recognition (TSR) aims at transforming unstructured table
images into structured formats, such as HTML sequences. One type of popular
solution is using detection models to detect components of a table, such as
columns and rows, then applying a rule-based post-processing method to convert
detection results into HTML sequences. However, existing detection-based
studies often have the following limitations. First, these studies usually pay
more attention to improving the detection performance, which does not
necessarily lead to better performance regarding cell-level metrics, such as
TEDS. Second, some solutions over-simplify the problem and can miss some
critical information. Lastly, even though some studies defined the problem to
detect more components to provide as much information as other types of
solutions, these studies ignore the fact this problem definition is a
multi-label detection because row, projected row header and column header can
share identical bounding boxes. Besides, there is often a performance gap
between two-stage and transformer-based detection models regarding the
structure-only TEDS, even though they have similar performance regarding the
COCO metrics. Therefore, we revisit the limitations of existing detection-based
solutions, compare two-stage and transformer-based detection models, and
identify the key design aspects for the success of a two-stage detection model
for the TSR task, including the multi-class problem definition, the aspect
ratio for anchor box generation, and the feature generation of the backbone
network. We applied simple methods to improve these aspects of the Cascade
R-CNN model, achieved state-of-the-art performance, and improved the baseline
Cascade R-CNN model by 19.32%, 11.56% and 14.77% regarding the structure-only
TEDS on SciTSR, FinTabNet, and PubTables1M datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1&quot;&gt;Bin Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simsek_M/0/1/0/all/0/1&quot;&gt;Murat Simsek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kantarci_B/0/1/0/all/0/1&quot;&gt;Burak Kantarci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alkheir_A/0/1/0/all/0/1&quot;&gt;Ala Abu Alkheir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00700">
<title>GIFT: Generative Interpretable Fine-Tuning Transformers. (arXiv:2312.00700v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00700</link>
<description rdf:parseType="Literal">&lt;p&gt;We present GIFT (Generative Interpretable Fine-tuning Transformers) for
fine-tuning pretrained (often large) Transformer models at downstream tasks in
a parameter-efficient way with built-in interpretability. Our GIFT is a deep
parameter-residual learning method, which addresses two problems in fine-tuning
a pretrained Transformer model: Where to apply the parameter-efficient
fine-tuning (PEFT) to be extremely lightweight yet sufficiently expressive, and
How to learn the PEFT to better exploit the knowledge of the pretrained model
in a direct way? For the former, we select the final projection (linear) layer
in the multi-head self-attention of a Transformer model, and verify its
effectiveness. For the latter, in contrast to the prior art that directly
introduce new model parameters (often in low-rank approximation form) to be
learned in fine-tuning with downstream data, we propose a method for learning
to generate the fine-tuning parameters. Our GIFT is a hyper-Transformer which
take as input the pretrained parameters of the projection layer to generate its
fine-tuning parameters using a proposed Parameter-to-Cluster Attention (PaCa).
The PaCa results in a simple clustering-based forward explainer that plays the
role of semantic segmentation in testing. In experiments, our proposed GIFT is
tested on the VTAB benchmark and the fine-grained visual classification (FGVC)
benchmark. It obtains significantly better performance than the prior art. Our
code is available at https://github.com/savadikarc/gift
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savadikar_C/0/1/0/all/0/1&quot;&gt;Chinmay Savadikar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1&quot;&gt;Xi Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tianfu Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00703">
<title>PointBeV: A Sparse Approach to BeV Predictions. (arXiv:2312.00703v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00703</link>
<description rdf:parseType="Literal">&lt;p&gt;Bird&apos;s-eye View (BeV) representations have emerged as the de-facto shared
space in driving applications, offering a unified space for sensor data fusion
and supporting various downstream tasks. However, conventional models use grids
with fixed resolution and range and face computational inefficiencies due to
the uniform allocation of resources across all cells. To address this, we
propose PointBeV, a novel sparse BeV segmentation model operating on sparse BeV
cells instead of dense grids. This approach offers precise control over memory
usage, enabling the use of long temporal contexts and accommodating
memory-constrained platforms. PointBeV employs an efficient two-pass strategy
for training, enabling focused computation on regions of interest. At inference
time, it can be used with various memory/performance trade-offs and flexibly
adjusts to new specific use cases. PointBeV achieves state-of-the-art results
on the nuScenes dataset for vehicle, pedestrian, and lane segmentation,
showcasing superior performance in static and temporal settings despite being
trained solely with sparse signals. We will release our code along with two new
efficient modules used in the architecture: Sparse Feature Pulling, designed
for the effective extraction of features from images to BeV, and Submanifold
Attention, which enables efficient temporal modeling. Our code is available at
https://github.com/valeoai/PointBeV.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chambon_L/0/1/0/all/0/1&quot;&gt;Loick Chambon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zablocki_E/0/1/0/all/0/1&quot;&gt;Eloi Zablocki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mickael Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bartoccioni_F/0/1/0/all/0/1&quot;&gt;Florent Bartoccioni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1&quot;&gt;Patrick Perez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1&quot;&gt;Matthieu Cord&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00732">
<title>Gaussian Grouping: Segment and Edit Anything in 3D Scenes. (arXiv:2312.00732v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00732</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent Gaussian Splatting achieves high-quality and real-time novel-view
synthesis of the 3D scenes. However, it is solely concentrated on the
appearance and geometry modeling, while lacking in fine-grained object-level
scene understanding. To address this issue, we propose Gaussian Grouping, which
extends Gaussian Splatting to jointly reconstruct and segment anything in
open-world 3D scenes. We augment each Gaussian with a compact Identity
Encoding, allowing the Gaussians to be grouped according to their object
instance or stuff membership in the 3D scene. Instead of resorting to expensive
3D labels, we supervise the Identity Encodings during the differentiable
rendering by leveraging the 2D mask predictions by SAM, along with introduced
3D spatial consistency regularization. Comparing to the implicit NeRF
representation, we show that the discrete and grouped 3D Gaussians can
reconstruct, segment and edit anything in 3D with high visual quality, fine
granularity and efficiency. Based on Gaussian Grouping, we further propose a
local Gaussian Editing scheme, which shows efficacy in versatile scene editing
applications, including 3D object removal, inpainting, colorization and scene
recomposition. Our code and models will be at
https://github.com/lkeab/gaussian-grouping.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1&quot;&gt;Mingqiao Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1&quot;&gt;Martin Danelljan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1&quot;&gt;Fisher Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ke_L/0/1/0/all/0/1&quot;&gt;Lei Ke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00739">
<title>Adversarial Score Distillation: When score distillation meets GAN. (arXiv:2312.00739v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00739</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing score distillation methods are sensitive to classifier-free guidance
(CFG) scale: manifested as over-smoothness or instability at small CFG scales,
while over-saturation at large ones. To explain and analyze these issues, we
revisit the derivation of Score Distillation Sampling (SDS) and decipher
existing score distillation with the Wasserstein Generative Adversarial Network
(WGAN) paradigm. With the WGAN paradigm, we find that existing score
distillation either employs a fixed sub-optimal discriminator or conducts
incomplete discriminator optimization, resulting in the scale-sensitive issue.
We propose the Adversarial Score Distillation (ASD), which maintains an
optimizable discriminator and updates it using the complete optimization
objective. Experiments show that the proposed ASD performs favorably in 2D
distillation and text-to-3D tasks against existing methods. Furthermore, to
explore the generalization ability of our WGAN paradigm, we extend ASD to the
image editing task, which achieves competitive results. The project page and
code are at https://github.com/2y7c3/ASD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1&quot;&gt;Min Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingkai Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Junyao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xuesong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2006.15920">
<title>Interpreting and Disentangling Feature Components of Various Complexity from DNNs. (arXiv:2006.15920v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2006.15920</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper aims to define, quantify, and analyze the feature complexity that
is learned by a DNN. We propose a generic definition for the feature
complexity. Given the feature of a certain layer in the DNN, our method
disentangles feature components of different complexity orders from the
feature. We further design a set of metrics to evaluate the reliability, the
effectiveness, and the significance of over-fitting of these feature
components. Furthermore, we successfully discover a close relationship between
the feature complexity and the performance of DNNs. As a generic mathematical
tool, the feature complexity and the proposed metrics can also be used to
analyze the success of network compression and knowledge distillation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1&quot;&gt;Jie Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mingjie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zexu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Quanshi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2010.04055">
<title>A Unified Approach to Interpreting and Boosting Adversarial Transferability. (arXiv:2010.04055v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2010.04055</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we use the interaction inside adversarial perturbations to
explain and boost the adversarial transferability. We discover and prove the
negative correlation between the adversarial transferability and the
interaction inside adversarial perturbations. The negative correlation is
further verified through different DNNs with various inputs. Moreover, this
negative correlation can be regarded as a unified perspective to understand
current transferability-boosting methods. To this end, we prove that some
classic methods of enhancing the transferability essentially decease
interactions inside adversarial perturbations. Based on this, we propose to
directly penalize interactions during the attacking process, which
significantly improves the adversarial transferability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1&quot;&gt;Jie Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1&quot;&gt;Shuyun Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiangming Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yisen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Quanshi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.13278">
<title>Practical Blind Image Denoising via Swin-Conv-UNet and Data Synthesis. (arXiv:2203.13278v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.13278</link>
<description rdf:parseType="Literal">&lt;p&gt;While recent years have witnessed a dramatic upsurge of exploiting deep
neural networks toward solving image denoising, existing methods mostly rely on
simple noise assumptions, such as additive white Gaussian noise (AWGN), JPEG
compression noise and camera sensor noise, and a general-purpose blind
denoising method for real images remains unsolved. In this paper, we attempt to
solve this problem from the perspective of network architecture design and
training data synthesis. Specifically, for the network architecture design, we
propose a swin-conv block to incorporate the local modeling ability of residual
convolutional layer and non-local modeling ability of swin transformer block,
and then plug it as the main building block into the widely-used image-to-image
translation UNet architecture. For the training data synthesis, we design a
practical noise degradation model which takes into consideration different
kinds of noise (including Gaussian, Poisson, speckle, JPEG compression, and
processed camera sensor noises) and resizing, and also involves a random
shuffle strategy and a double degradation strategy. Extensive experiments on
AGWN removal and real image denoising demonstrate that the new network
architecture design achieves state-of-the-art performance and the new
degradation model can help to significantly improve the practicability. We
believe our work can provide useful insights into current denoising research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yawei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Jingyun Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jiezhang Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yulun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Hao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1&quot;&gt;Deng-Ping Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1&quot;&gt;Radu Timofte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1&quot;&gt;Luc Van Gool&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.00379">
<title>NeRF: Neural Radiance Field in 3D Vision, A Comprehensive Review. (arXiv:2210.00379v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.00379</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Field (NeRF) has recently become a significant development in
the field of Computer Vision, allowing for implicit, neural network-based scene
representation and novel view synthesis. NeRF models have found diverse
applications in robotics, urban mapping, autonomous navigation, virtual
reality/augmented reality, and more. Due to the growing popularity of NeRF and
its expanding research area, we present a comprehensive survey of NeRF papers
from the past two years. Our survey is organized into architecture and
application-based taxonomies and provides an introduction to the theory of NeRF
and its training via differentiable volume rendering. We also present a
benchmark comparison of the performance and speed of key NeRF models. By
creating this survey, we hope to introduce new researchers to NeRF, provide a
helpful reference for influential works in this field, as well as motivate
future research directions with our discussion section.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1&quot;&gt;Kyle Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yina Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;Hongjie He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1&quot;&gt;Dening Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Linlin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jonathan Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.09020">
<title>Defects of Convolutional Decoder Networks in Frequency Representation. (arXiv:2210.09020v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.09020</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we prove the representation defects of a cascaded
convolutional decoder network, considering the capacity of representing
different frequency components of an input sample. We conduct the discrete
Fourier transform on each channel of the feature map in an intermediate layer
of the decoder network. Then, we extend the 2D circular convolution theorem to
represent the forward and backward propagations through convolutional layers in
the frequency domain. Based on this, we prove three defects in representing
feature spectrums. First, we prove that the convolution operation, the
zero-padding operation, and a set of other settings all make a convolutional
decoder network more likely to weaken high-frequency components. Second, we
prove that the upsampling operation generates a feature spectrum, in which
strong signals repetitively appear at certain frequencies. Third, we prove that
if the frequency components in the input sample and frequency components in the
target output for regression have a small shift, then the decoder usually
cannot be effectively learned.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1&quot;&gt;Ling Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1&quot;&gt;Wen Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhanpeng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuefeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Quanshi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.14611">
<title>Automatic Diagnosis of Myocarditis Disease in Cardiac MRI Modality using Deep Transformers and Explainable Artificial Intelligence. (arXiv:2210.14611v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.14611</link>
<description rdf:parseType="Literal">&lt;p&gt;Myocarditis is a significant cardiovascular disease (CVD) that poses a threat
to the health of many individuals by causing damage to the myocardium. The
occurrence of microbes and viruses, including the likes of HIV, plays a crucial
role in the development of myocarditis disease (MCD). The images produced
during cardiac magnetic resonance imaging (CMRI) scans are low contrast, which
can make it challenging to diagnose cardiovascular diseases. In other hand,
checking numerous CMRI slices for each CVD patient can be a challenging task
for medical doctors. To overcome the existing challenges, researchers have
suggested the use of artificial intelligence (AI)-based computer-aided
diagnosis systems (CADS). The presented paper outlines a CADS for the detection
of MCD from CMR images, utilizing deep learning (DL) methods. The proposed CADS
consists of several steps, including dataset, preprocessing, feature
extraction, classification, and post-processing. First, the Z-Alizadeh dataset
was selected for the experiments. Subsequently, the CMR images underwent
various preprocessing steps, including denoising, resizing, as well as data
augmentation (DA) via CutMix and MixUp techniques. In the following, the most
current deep pre-trained and transformer models are used for feature extraction
and classification on the CMR images. The findings of our study reveal that
transformer models exhibit superior performance in detecting MCD as opposed to
pre-trained architectures. In terms of DL architectures, the Turbulence Neural
Transformer (TNT) model exhibited impressive accuracy, reaching 99.73%
utilizing a 10-fold cross-validation approach. Additionally, to pinpoint areas
of suspicion for MCD in CMRI images, the Explainable-based Grad Cam method was
employed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jafari_M/0/1/0/all/0/1&quot;&gt;Mahboobeh Jafari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shoeibi_A/0/1/0/all/0/1&quot;&gt;Afshin Shoeibi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghassemi_N/0/1/0/all/0/1&quot;&gt;Navid Ghassemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heras_J/0/1/0/all/0/1&quot;&gt;Jonathan Heras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_S/0/1/0/all/0/1&quot;&gt;Sai Ho Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beheshti_A/0/1/0/all/0/1&quot;&gt;Amin Beheshti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yu-Dong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shui-Hua Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alizadehsani_R/0/1/0/all/0/1&quot;&gt;Roohallah Alizadehsani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gorriz_J/0/1/0/all/0/1&quot;&gt;Juan M. Gorriz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Acharya_U/0/1/0/all/0/1&quot;&gt;U. Rajendra Acharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rokny_H/0/1/0/all/0/1&quot;&gt;Hamid Alinejad Rokny&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.05781">
<title>Demystify Transformers &amp; Convolutions in Modern Image Deep Networks. (arXiv:2211.05781v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.05781</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision transformers have gained popularity recently, leading to the
development of new vision backbones with improved features and consistent
performance gains. However, these advancements are not solely attributable to
novel feature transformation designs; certain benefits also arise from advanced
network-level and block-level architectures. This paper aims to identify the
real gains of popular convolution and attention operators through a detailed
study. We find that the key difference among these feature transformation
modules, such as attention or convolution, lies in their spatial feature
aggregation approach, known as the &quot;spatial token mixer&quot; (STM). To facilitate
an impartial comparison, we introduce a unified architecture to neutralize the
impact of divergent network-level and block-level designs. Subsequently,
various STMs are integrated into this unified framework for comprehensive
comparative analysis. Our experiments on various tasks and an analysis of
inductive bias show a significant performance boost due to advanced
network-level and block-level designs, but performance differences persist
among different STMs. Our detailed analysis also reveals various findings about
different STMs, such as effective receptive fields and invariance tests. All
models and codes used in this study are publicly available at
\url{https://github.com/OpenGVLab/STM-Evaluation}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xiaowei Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1&quot;&gt;Min Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weiyun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Sitong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1&quot;&gt;Linjie Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenhai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xizhou Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1&quot;&gt;Lewei Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jie Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaogang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1&quot;&gt;Jifeng Dai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.06108">
<title>RaLiBEV: Radar and LiDAR BEV Fusion Learning for Anchor Box Free Object Detection Systems. (arXiv:2211.06108v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.06108</link>
<description rdf:parseType="Literal">&lt;p&gt;In autonomous driving, LiDAR and radar play important roles in the perception
of the surrounding environment. LiDAR provides accurate 3D spatial sensing
information but cannot work in adverse weather like fog. On the other hand, the
radar signal can be diffracted when encountering raindrops or mist particles
thanks to its wavelength, but it suffers from large noise. Recent
state-of-the-art works reveal that fusion of radar and LiDAR can lead to robust
detection in adverse weather. The existing works adopt convolutional neural
network architecture to extract features from each sensor data, then align and
aggregate the two branch features to predict object detection results. However,
these methods have low accuracy of bounding box estimations due to a simple
design of label assignment and fusion strategies. In this paper, we propose a
bird&apos;s-eye view fusion learning-based anchor box-free object detection system,
which fuses the feature derived from the radar range-azimuth heatmap and the
LiDAR point cloud to estimate possible objects. Different label assignment
strategies have been designed to facilitate the consistency between the
classification of foreground or background anchor points and the corresponding
bounding box regressions. Furthermore, the performance of the proposed object
detector is further enhanced by employing a novel interactive transformer
module. The superior performance of the methods proposed in this paper has been
demonstrated using the recently published Oxford Radar RobotCar dataset. Our
system&apos;s average precision significantly outperforms the state-of-the-art
method by 13.1% and 19.0% at IoU of 0.8 under &apos;Clear+Foggy&apos; training conditions
for &apos;Clear&apos; and &apos;Foggy&apos; testing, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yanlong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jianan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1&quot;&gt;Qing-Long Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_G/0/1/0/all/0/1&quot;&gt;Gang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1&quot;&gt;Bing Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.02003">
<title>Bayesian Learning with Information Gain Provably Bounds Risk for a Robust Adversarial Defense. (arXiv:2212.02003v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.02003</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a new algorithm to learn a deep neural network model robust
against adversarial attacks. Previous algorithms demonstrate an adversarially
trained Bayesian Neural Network (BNN) provides improved robustness. We
recognize the adversarial learning approach for approximating the multi-modal
posterior distribution of a Bayesian model can lead to mode collapse;
consequently, the model&apos;s achievements in robustness and performance are
sub-optimal. Instead, we first propose preventing mode collapse to better
approximate the multi-modal posterior distribution. Second, based on the
intuition that a robust model should ignore perturbations and only consider the
informative content of the input, we conceptualize and formulate an information
gain objective to measure and force the information learned from both benign
and adversarial training instances to be similar. Importantly. we prove and
demonstrate that minimizing the information gain objective allows the
adversarial risk to approach the conventional empirical risk. We believe our
efforts provide a step toward a basis for a principled method of adversarially
training BNNs. Our model demonstrate significantly improved robustness--up to
20%--compared with adversarial training and Adv-BNN under PGD attacks with
0.035 distortion on both CIFAR-10 and STL-10 datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doan_B/0/1/0/all/0/1&quot;&gt;Bao Gia Doan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbasnejad_E/0/1/0/all/0/1&quot;&gt;Ehsan Abbasnejad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Javen Qinfeng Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ranasinghe_D/0/1/0/all/0/1&quot;&gt;Damith C. Ranasinghe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.04705">
<title>Physics-based Indirect Illumination for Inverse Rendering. (arXiv:2212.04705v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.04705</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a physics-based inverse rendering method that learns the
illumination, geometry, and materials of a scene from posed multi-view RGB
images. To model the illumination of a scene, existing inverse rendering works
either completely ignore the indirect illumination or model it by coarse
approximations, leading to sub-optimal illumination, geometry, and material
prediction of the scene. In this work, we propose a physics-based illumination
model that first locates surface points through an efficient refined sphere
tracing algorithm, then explicitly traces the incoming indirect lights at each
surface point based on reflection. Then, we estimate each identified indirect
light through an efficient neural network. Moreover, we utilize the Leibniz&apos;s
integral rule to resolve non-differentiability in the proposed illumination
model caused by boundary lights inspired by differentiable irradiance in
computer graphics. As a result, the proposed differentiable illumination model
can be learned end-to-end together with geometry and materials estimation. As a
side product, our physics-based inverse rendering model also facilitates
flexible and realistic material editing as well as relighting. Extensive
experiments on synthetic and real-world datasets demonstrate that the proposed
method performs favorably against existing inverse rendering methods on novel
view synthesis and inverse rendering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Youming Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xueting Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sifei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Ming-Hsuan Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.10428">
<title>HouseCat6D -- A Large-Scale Multi-Modal Category Level 6D Object Perception Dataset with Household Objects in Realistic Scenarios. (arXiv:2212.10428v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.10428</link>
<description rdf:parseType="Literal">&lt;p&gt;Estimating 6D object poses is a major challenge in 3D computer vision.
Building on successful instance-level approaches, research is shifting towards
category-level pose estimation for practical applications. Current
category-level datasets, however, fall short in annotation quality and pose
variety. Addressing this, we introduce HouseCat6D, a new category-level 6D pose
dataset. It features 1) multi-modality with Polarimetric RGB and Depth
(RGBD+P), 2) encompasses 194 diverse objects across 10 household categories,
including two photometrically challenging ones, and 3) provides high-quality
pose annotations with an error range of only 1.35 mm to 1.74 mm. The dataset
also includes 4) 41 large-scale scenes with comprehensive viewpoint and
occlusion coverage, 5) a checkerboard-free environment, and 6) dense 6D
parallel-jaw robotic grasp annotations. Additionally, we present benchmark
results for leading category-level pose estimation networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1&quot;&gt;HyunJun Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1&quot;&gt;Guangyao Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Shun-Cheng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruhkamp_P/0/1/0/all/0/1&quot;&gt;Patrick Ruhkamp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schieber_H/0/1/0/all/0/1&quot;&gt;Hannah Schieber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rizzoli_G/0/1/0/all/0/1&quot;&gt;Giulia Rizzoli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Pengyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hongcheng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garattoni_L/0/1/0/all/0/1&quot;&gt;Lorenzo Garattoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meier_S/0/1/0/all/0/1&quot;&gt;Sven Meier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1&quot;&gt;Daniel Roth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1&quot;&gt;Nassir Navab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1&quot;&gt;Benjamin Busam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.10127">
<title>Improving Open-Set Semi-Supervised Learning with Self-Supervision. (arXiv:2301.10127v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.10127</link>
<description rdf:parseType="Literal">&lt;p&gt;Open-set semi-supervised learning (OSSL) embodies a practical scenario within
semi-supervised learning, wherein the unlabeled training set encompasses
classes absent from the labeled set. Many existing OSSL methods assume that
these out-of-distribution data are harmful and put effort into excluding data
belonging to unknown classes from the training objective. In contrast, we
propose an OSSL framework that facilitates learning from all unlabeled data
through self-supervision. Additionally, we utilize an energy-based score to
accurately recognize data belonging to the known classes, making our method
well-suited for handling uncurated data in deployment. We show through
extensive experimental evaluations that our method yields state-of-the-art
results on many of the evaluated benchmark problems in terms of closed-set
accuracy and open-set recognition when compared with existing methods for OSSL.
Our code is available at https://github.com/walline/ssl-tf2-sefoss.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wallin_E/0/1/0/all/0/1&quot;&gt;Erik Wallin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Svensson_L/0/1/0/all/0/1&quot;&gt;Lennart Svensson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kahl_F/0/1/0/all/0/1&quot;&gt;Fredrik Kahl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hammarstrand_L/0/1/0/all/0/1&quot;&gt;Lars Hammarstrand&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.13098">
<title>CHeart: A Conditional Spatio-Temporal Generative Model for Cardiac Anatomy. (arXiv:2301.13098v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.13098</link>
<description rdf:parseType="Literal">&lt;p&gt;Two key questions in cardiac image analysis are to assess the anatomy and
motion of the heart from images; and to understand how they are associated with
non-imaging clinical factors such as gender, age and diseases. While the first
question can often be addressed by image segmentation and motion tracking
algorithms, our capability to model and to answer the second question is still
limited. In this work, we propose a novel conditional generative model to
describe the 4D spatio-temporal anatomy of the heart and its interaction with
non-imaging clinical factors. The clinical factors are integrated as the
conditions of the generative modelling, which allows us to investigate how
these factors influence the cardiac anatomy. We evaluate the model performance
in mainly two tasks, anatomical sequence completion and sequence generation.
The model achieves a high performance in anatomical sequence completion,
comparable to or outperforming other state-of-the-art generative models. In
terms of sequence generation, given clinical conditions, the model can generate
realistic synthetic 4D sequential anatomies that share similar distributions
with the real data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qiao_M/0/1/0/all/0/1&quot;&gt;Mengyun Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qiu_H/0/1/0/all/0/1&quot;&gt;Huaqi Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Marvao_A/0/1/0/all/0/1&quot;&gt;Antonio de Marvao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+ORegan_D/0/1/0/all/0/1&quot;&gt;Declan P. O&amp;#x27;Regan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1&quot;&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bai_W/0/1/0/all/0/1&quot;&gt;Wenjia Bai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.13080">
<title>Does a Neural Network Really Encode Symbolic Concepts?. (arXiv:2302.13080v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.13080</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, a series of studies have tried to extract interactions between
input variables modeled by a DNN and define such interactions as concepts
encoded by the DNN. However, strictly speaking, there still lacks a solid
guarantee whether such interactions indeed represent meaningful concepts.
Therefore, in this paper, we examine the trustworthiness of interaction
concepts from four perspectives. Extensive empirical studies have verified that
a well-trained DNN usually encodes sparse, transferable, and discriminative
concepts, which is partially aligned with human intuition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mingjie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Quanshi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.13095">
<title>Bayesian Neural Networks Avoid Encoding Complex and Perturbation-Sensitive Concepts. (arXiv:2302.13095v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.13095</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we focus on mean-field variational Bayesian Neural Networks
(BNNs) and explore the representation capacity of such BNNs by investigating
which types of concepts are less likely to be encoded by the BNN. It has been
observed and studied that a relatively small set of interactive concepts
usually emerge in the knowledge representation of a sufficiently-trained neural
network, and such concepts can faithfully explain the network output. Based on
this, our study proves that compared to standard deep neural networks (DNNs),
it is less likely for BNNs to encode complex concepts. Experiments verify our
theoretical proofs. Note that the tendency to encode less complex concepts does
not necessarily imply weak representation power, considering that complex
concepts exhibit low generalization power and high adversarial vulnerability.
The code is available at https://github.com/sjtu-xai-lab/BNN-concepts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Q/0/1/0/all/0/1&quot;&gt;Qihan Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1&quot;&gt;Huiqi Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yunuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lou_S/0/1/0/all/0/1&quot;&gt;Siyu Lou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Quanshi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.06388">
<title>Generalized 3D Self-supervised Learning Framework via Prompted Foreground-Aware Feature Contrast. (arXiv:2303.06388v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.06388</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive learning has recently demonstrated great potential for
unsupervised pre-training in 3D scene understanding tasks. However, most
existing work randomly selects point features as anchors while building
contrast, leading to a clear bias toward background points that often dominate
in 3D scenes. Also, object awareness and foreground-to-background
discrimination are neglected, making contrastive learning less effective. To
tackle these issues, we propose a general foreground-aware feature contrast
FAC++ framework to learn more effective point cloud representations in
pre-training. FAC++ consists of two novel contrast designs to construct more
effective and informative contrast pairs. The first is building positive pairs
within the same foreground segment where points tend to have the same
semantics. The second is that we prevent over-discrimination between 3D
segments/objects and encourage grouped foreground-to-background distinctions at
the segment level with adaptive feature learning in a Siamese correspondence
network, which adaptively learns feature correlations within and across point
cloud views effectively. Moreover, we have designed the foreground-prompted
regional sampling to enhance more balanced foreground-aware learning, which is
termed FAC++. Visualization with point activation maps shows that our contrast
pairs capture clear correspondences among foreground regions during
pre-training. Quantitative experiments also show that FAC++ achieves superior
knowledge transfer and data efficiency in various downstream 3D semantic
segmentation, instance segmentation as well as object detection tasks. All
codes, data, and models are available at:
https://github.com/KangchengLiu/FAC_Foreground_Aware_Contrast
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kangcheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1&quot;&gt;Xinhu Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chaoqun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1&quot;&gt;Kai Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Ming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Baoquan Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.01811">
<title>HarsanyiNet: Computing Accurate Shapley Values in a Single Forward Propagation. (arXiv:2304.01811v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.01811</link>
<description rdf:parseType="Literal">&lt;p&gt;The Shapley value is widely regarded as a trustworthy attribution metric.
However, when people use Shapley values to explain the attribution of input
variables of a deep neural network (DNN), it usually requires a very high
computational cost to approximate relatively accurate Shapley values in
real-world applications. Therefore, we propose a novel network architecture,
the HarsanyiNet, which makes inferences on the input sample and simultaneously
computes the exact Shapley values of the input variables in a single forward
propagation. The HarsanyiNet is designed on the theoretical foundation that the
Shapley value can be reformulated as the redistribution of Harsanyi
interactions encoded by the network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lou_S/0/1/0/all/0/1&quot;&gt;Siyu Lou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Keyan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Quanshi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.04336">
<title>Split, Merge, and Refine: Fitting Tight Bounding Boxes via Over-Segmentation and Iterative Search. (arXiv:2304.04336v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.04336</link>
<description rdf:parseType="Literal">&lt;p&gt;Achieving tight bounding boxes of a shape while guaranteeing complete
boundness is an essential task for efficient geometric operations and
unsupervised semantic part detection. But previous methods fail to achieve both
full coverage and tightness. Neural-network-based methods are not suitable for
these goals due to the non-differentiability of the objective, while classic
iterative search methods suffer from their sensitivity to the initialization.
We propose a novel framework for finding a set of tight bounding boxes of a 3D
shape via over-segmentation and iterative merging and refinement. Our result
shows that utilizing effective search methods with appropriate objectives is
the key to producing bounding boxes with both properties. We employ an existing
pre-segmentation to split the shape and obtain over-segmentation. Then, we
apply hierarchical merging with our novel tightness-aware merging and stopping
criteria. To overcome the sensitivity to the initialization, we also define
actions to refine the bounding box parameters in an Markov Decision Process
(MDP) setup with a soft reward function promoting a wider exploration. Lastly,
we further improve the refinement step with Monte Carlo Tree Search (MCTS)
based multi-action space exploration. By thoughtful evaluation on diverse 3D
shapes, we demonstrate full coverage, tightness, and an adequate number of
bounding boxes of our method without requiring any training data or
supervision. It thus can be applied to various downstream tasks in computer
vision and graphics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1&quot;&gt;Chanhyeok Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sung_M/0/1/0/all/0/1&quot;&gt;Minhyuk Sung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.06767">
<title>RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment. (arXiv:2304.06767v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.06767</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative foundation models are susceptible to implicit biases that can
arise from extensive unsupervised training data. Such biases can produce
suboptimal samples, skewed outcomes, and unfairness, with potentially serious
consequences. Consequently, aligning these models with human ethics and
preferences is an essential step toward ensuring their responsible and
effective deployment in real-world applications. Prior research has primarily
employed Reinforcement Learning from Human Feedback (RLHF) to address this
problem, where generative models are fine-tuned with RL algorithms guided by a
human-feedback-informed reward model. However, the inefficiencies and
instabilities associated with RL algorithms frequently present substantial
obstacles to the successful alignment, necessitating the development of a more
robust and streamlined approach. To this end, we introduce a new framework,
Reward rAnked FineTuning (RAFT), designed to align generative models
effectively. Utilizing a reward model and a sufficient number of samples, our
approach selects the high-quality samples, discarding those that exhibit
undesired behavior, and subsequently enhancing the model by fine-tuning on
these filtered samples. Our studies show that RAFT can effectively improve the
model performance in both reward learning and other automated metrics in both
large language models and diffusion models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hanze Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1&quot;&gt;Wei Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_D/0/1/0/all/0/1&quot;&gt;Deepanshu Goyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yihan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chow_W/0/1/0/all/0/1&quot;&gt;Winnie Chow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_R/0/1/0/all/0/1&quot;&gt;Rui Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diao_S/0/1/0/all/0/1&quot;&gt;Shizhe Diao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jipeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shum_K/0/1/0/all/0/1&quot;&gt;Kashun Shum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.06777">
<title>Generating high-quality 3DMPCs by adaptive data acquisition and NeREF-based radiometric calibration with UGV plant phenotyping system. (arXiv:2305.06777v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.06777</link>
<description rdf:parseType="Literal">&lt;p&gt;Fusion of 3D and MS imaging data has a great potential for high-throughput
plant phenotyping of structural and biochemical as well as physiological traits
simultaneously, which is important for decision support in agriculture and for
crop breeders in selecting the best genotypes. However, lacking of 3D data
integrity of various plant canopy structures and low-quality of MS images
caused by the complex illumination effects make a great challenge, especially
at the proximal imaging scale. Therefore, this study proposed a novel approach
for adaptive data acquisition and radiometric calibration to generate
high-quality 3DMPCs of plants. An efficient NBV planning method based on an UGV
plant phenotyping system with a multi-sensor-equipped robotic arm was proposed
to achieve adaptive data acquisition. The NeREF was employed to predict the DN
values of the hemispherical reference for radiometric calibration. For NBV
planning, the average total time for single plant at a joint speed of 1.55
rad/s was about 62.8 s, with an average reduction of 18.0% compared to the
unplanned. The integrity of the whole-plant data was improved by an average of
23.6% compared to the fixed viewpoints alone. Compared with the ASD
measurements, the RMSE of the reflectance spectra obtained from 3DMPCs at
different regions of interest was 0.08 with an average decrease of 58.93%
compared to the results obtained from the single-frame of MS images without 3D
radiometric calibration. The 3D-calibrated plant 3DMPCs improved the predictive
accuracy of PLSR for chlorophyll content, with an average increase of 0.07 in
R2 and an average decrease of 21.25% in RMSE. Our approach introduced a fresh
perspective on generating high-quality 3DMPCs of plants under the natural light
condition, enabling more precise analysis of plant morphological and
physiological parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xie_P/0/1/0/all/0/1&quot;&gt;Pengyao Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zhihong Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Du_R/0/1/0/all/0/1&quot;&gt;Ruiming Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cen_H/0/1/0/all/0/1&quot;&gt;Haiyan Cen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.07618">
<title>Uncertainty Estimation and Out-of-Distribution Detection for Deep Learning-Based Image Reconstruction using the Local Lipschitz. (arXiv:2305.07618v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.07618</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate image reconstruction is at the heart of diagnostics in medical
imaging. Supervised deep learning-based approaches have been investigated for
solving inverse problems including image reconstruction. However, these trained
models encounter unseen data distributions that are widely shifted from
training data during deployment. Therefore, it is essential to assess whether a
given input falls within the training data distribution for diagnostic
purposes. Uncertainty estimation approaches exist but focus on providing an
uncertainty map to radiologists, rather than assessing the training
distribution fit. In this work, we propose a method based on the local
Lipschitz-based metric to distinguish out-of-distribution images from
in-distribution with an area under the curve of 99.94%. Empirically, we
demonstrate a very strong relationship between the local Lipschitz value and
mean absolute error (MAE), supported by a high Spearman&apos;s rank correlation
coefficient of 0.8475, which determines the uncertainty estimation threshold
for optimal model performance. Through the identification of false positives,
the local Lipschitz and MAE relationship was used to guide data augmentation
and reduce model uncertainty. Our study was validated using the AUTOMAP
architecture for sensor-to-image Magnetic Resonance Imaging (MRI)
reconstruction. We compare our proposed approach with baseline methods:
Monte-Carlo dropout and deep ensembles, and further analysis included MRI
denoising and Computed Tomography (CT) sparse-to-full view reconstruction using
UNET architectures. We show that our approach is applicable to various
architectures and learned functions, especially in the realm of medical image
reconstruction, where preserving the diagnostic accuracy of reconstructed
images remains paramount.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhutto_D/0/1/0/all/0/1&quot;&gt;Danyal F. Bhutto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1&quot;&gt;Bo Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jeremiah Z. Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koonjoo_N/0/1/0/all/0/1&quot;&gt;Neha Koonjoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongwei B. Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosen_B/0/1/0/all/0/1&quot;&gt;Bruce R. Rosen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosen_M/0/1/0/all/0/1&quot;&gt;Matthew S. Rosen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18670">
<title>SAVE: Spectral-Shift-Aware Adaptation of Image Diffusion Models for Text-driven Video Editing. (arXiv:2305.18670v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18670</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-Image (T2I) diffusion models have achieved remarkable success in
synthesizing high-quality images conditioned on text prompts. Recent methods
have tried to replicate the success by either training text-to-video (T2V)
models on a very large number of text-video pairs or adapting T2I models on
text-video pairs independently. Although the latter is computationally less
expensive, it still takes a significant amount of time for per-video adaption.
To address this issue, we propose SAVE, a novel spectral-shift-aware adaptation
framework, in which we fine-tune the spectral shift of the parameter space
instead of the parameters themselves. Specifically, we take the spectral
decomposition of the pre-trained T2I weights and only update the singular
values while freezing the corresponding singular vectors. In addition, we
introduce a spectral shift regularizer aimed at placing tighter constraints on
larger singular values compared to smaller ones. This form of regularization
enables the model to grasp finer details within the video that align with the
provided textual descriptions. We also offer theoretical justification for our
proposed regularization technique. Since we are only dealing with spectral
shifts, the proposed method reduces the adaptation time significantly (approx.
10 times) and has fewer resource constraints for training. Such attributes
posit SAVE to be more suitable for real-world applications, e.g. editing
undesirable content during video streaming. We validate the effectiveness of
SAVE with an extensive experimental evaluation under different settings, e.g.
style transfer, object replacement, privacy preservation, etc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karim_N/0/1/0/all/0/1&quot;&gt;Nazmul Karim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khalid_U/0/1/0/all/0/1&quot;&gt;Umar Khalid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joneidi_M/0/1/0/all/0/1&quot;&gt;Mohsen Joneidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahnavard_N/0/1/0/all/0/1&quot;&gt;Nazanin Rahnavard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00854">
<title>Spatio-Angular Convolutions for Super-resolution in Diffusion MRI. (arXiv:2306.00854v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00854</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion MRI (dMRI) is a widely used imaging modality, but requires long
scanning times to acquire high resolution datasets. By leveraging the unique
geometry present within this domain, we present a novel approach to dMRI
angular super-resolution that extends upon the parametric continuous
convolution (PCConv) framework. We introduce several additions to the operation
including a Fourier feature mapping, global coordinates, and domain specific
context. Using this framework, we build a fully parametric continuous
convolution network (PCCNN) and compare against existing models. We demonstrate
the PCCNN performs competitively while using significantly less parameters.
Moreover, we show that this formulation generalises well to clinically relevant
downstream analyses such as fixel-based analysis, and neurite orientation
dispersion and density imaging.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lyon_M/0/1/0/all/0/1&quot;&gt;Matthew Lyon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Armitage_P/0/1/0/all/0/1&quot;&gt;Paul Armitage&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Alvarez_M/0/1/0/all/0/1&quot;&gt;Mauricio A &amp;#xc1;lvarez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.13325">
<title>Differentiable Display Photometric Stereo. (arXiv:2306.13325v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.13325</link>
<description rdf:parseType="Literal">&lt;p&gt;Photometric stereo leverages variations in illumination conditions to
reconstruct surface normals. Display photometric stereo, which employs a
conventional monitor as an illumination source, has the potential to overcome
limitations often encountered in bulky and difficult-to-use conventional
setups. In this paper, we present differentiable display photometric stereo
(DDPS), addressing an often overlooked challenge in display photometric stereo:
the design of display patterns. Departing from using heuristic display
patterns, DDPS learns the display patterns that yield accurate normal
reconstruction for a target system in an end-to-end manner. To this end, we
propose a differentiable framework that couples basis-illumination image
formation with analytic photometric-stereo reconstruction. The differentiable
framework facilitates the effective learning of display patterns via
auto-differentiation. Also, for training supervision, we propose to use 3D
printing for creating a real-world training dataset, enabling accurate
reconstruction on the target real-world setup. Finally, we exploit that
conventional LCD monitors emit polarized light, which allows for the optical
separation of diffuse and specular reflections when combined with a
polarization camera, leading to accurate normal reconstruction. Extensive
evaluation of DDPS shows improved normal-reconstruction accuracy compared to
heuristic patterns and demonstrates compelling properties such as robustness to
pattern initialization, calibration errors, and simplifications in image
formation and reconstruction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1&quot;&gt;Seokjun Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1&quot;&gt;Seungwoo Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nam_G/0/1/0/all/0/1&quot;&gt;Giljoo Nam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seungyong Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baek_S/0/1/0/all/0/1&quot;&gt;Seung-Hwan Baek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14505">
<title>AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor. (arXiv:2306.14505v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14505</link>
<description rdf:parseType="Literal">&lt;p&gt;Magnetic resonance imaging (MRI) is commonly used for brain tumor
segmentation, which is critical for patient evaluation and treatment planning.
To reduce the labor and expertise required for labeling, weakly-supervised
semantic segmentation (WSSS) methods with class activation mapping (CAM) have
been proposed. However, existing CAM methods suffer from low resolution due to
strided convolution and pooling layers, resulting in inaccurate predictions. In
this study, we propose a novel CAM method, Attentive Multiple-Exit CAM
(AME-CAM), that extracts activation maps from multiple resolutions to
hierarchically aggregate and improve prediction accuracy. We evaluate our
method on the BraTS 2021 dataset and show that it outperforms state-of-the-art
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yu-Jen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xinrong Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yiyu Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_T/0/1/0/all/0/1&quot;&gt;Tsung-Yi Ho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00309">
<title>Adversarial Attacks and Defenses on 3D Point Cloud Classification: A Survey. (arXiv:2307.00309v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00309</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has successfully solved a wide range of tasks in 2D vision as a
dominant AI technique. Recently, deep learning on 3D point clouds is becoming
increasingly popular for addressing various tasks in this field. Despite
remarkable achievements, deep learning algorithms are vulnerable to adversarial
attacks. These attacks are imperceptible to the human eye but can easily fool
deep neural networks in the testing and deployment stage. To encourage future
research, this survey summarizes the current progress on adversarial attack and
defense techniques on point cloud classification.This paper first introduces
the principles and characteristics of adversarial attacks and summarizes and
analyzes adversarial example generation methods in recent years. Additionally,
it provides an overview of defense strategies, organized into data-focused and
model-focused methods. Finally, it presents several current challenges and
potential future research directions in this domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naderi_H/0/1/0/all/0/1&quot;&gt;Hanieh Naderi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bajic_I/0/1/0/all/0/1&quot;&gt;Ivan V. Baji&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01985">
<title>Task-Specific Alignment and Multiple Level Transformer for Few-Shot Action Recognition. (arXiv:2307.01985v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01985</link>
<description rdf:parseType="Literal">&lt;p&gt;In the research field of few-shot learning, the main difference between
image-based and video-based is the additional temporal dimension. In recent
years, some works have used the Transformer to deal with frames, then get the
attention feature and the enhanced prototype, and the results are competitive.
However, some video frames may relate little to the action, and only using
single frame-level or segment-level features may not mine enough information.
We address these problems sequentially through an end-to-end method named
&quot;Task-Specific Alignment and Multiple-level Transformer Network (TSA-MLT)&quot;. The
first module (TSA) aims at filtering the action-irrelevant frames for action
duration alignment. Affine Transformation for frame sequence in the time
dimension is used for linear sampling. The second module (MLT) focuses on the
Multiple-level feature of the support prototype and query sample to mine more
information for the alignment, which operates on different level features. We
adopt a fusion loss according to a fusion distance that fuses the L2 sequence
distance, which focuses on temporal order alignment, and the Optimal Transport
distance, which focuses on measuring the gap between the appearance and
semantics of the videos. Extensive experiments show our method achieves
state-of-the-art results on the HMDB51 and UCF101 datasets and a competitive
result on the benchmark of Kinetics and something 2-something V2 datasets. Our
code is available at the URL: https://github.com/cofly2014/tsa-mlt.git
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1&quot;&gt;Fei Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Li Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;YiWang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jing Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07643">
<title>AECIF-Net: An Attention-Enhanced Co-Interactive Fusion Network for Automated Structural Condition Assessment in Visual Inspection. (arXiv:2307.07643v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07643</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficiently monitoring the condition of civil infrastructures necessitates
automating the structural condition assessment in visual inspection. This paper
proposes an Attention-Enhanced Co-Interactive Fusion Network (AECIF-Net) for
automatic structural condition assessment in visual bridge inspection.
AECIF-Net can simultaneously parse structural elements and segment surface
defects on the elements in inspection images. It integrates two task-specific
relearning subnets to extract task-specific features from an overall feature
embedding. A co-interactive feature fusion module further captures the spatial
correlation and facilitates information sharing between tasks. Experimental
results demonstrate that the proposed AECIF-Net outperforms the current
state-of-the-art approaches, achieving promising performance with 92.11% mIoU
for element segmentation and 87.16% mIoU for corrosion segmentation on the test
set of the new benchmark dataset Steel Bridge Condition Inspection Visual
(SBCIV). An ablation study verifies the merits of the designs for AECIF-Net,
and a case study demonstrates its capability to automate structural condition
assessment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chenyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Zhaozheng Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1&quot;&gt;Ruwen Qin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01300">
<title>Revisiting DETR Pre-training for Object Detection. (arXiv:2308.01300v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.01300</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivated by the remarkable achievements of DETR-based approaches on COCO
object detection and segmentation benchmarks, recent endeavors have been
directed towards elevating their performance through self-supervised
pre-training of Transformers while preserving a frozen backbone. Noteworthy
advancements in accuracy have been documented in certain studies. Our
investigation delved deeply into a representative approach, DETReg, and its
performance assessment in the context of emerging models like
$\mathcal{H}$-Deformable-DETR. Regrettably, DETReg proves inadequate in
enhancing the performance of robust DETR-based models under full data
conditions. To dissect the underlying causes, we conduct extensive experiments
on COCO and PASCAL VOC probing elements such as the selection of pre-training
datasets and strategies for pre-training target generation. By contrast, we
employ an optimized approach named Simple Self-training which leads to marked
enhancements through the combination of an improved box predictor and the
Objects$365$ benchmark. The culmination of these endeavors results in a
remarkable AP score of $59.3\%$ on the COCO val set, outperforming
$\mathcal{H}$-Deformable-DETR + Swin-L without pre-training by $1.4\%$.
Moreover, a series of synthetic pre-training datasets, generated by merging
contemporary image-to-text(LLaVA) and text-to-image (SDXL) models,
significantly amplifies object detection capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1&quot;&gt;Weicong Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bohan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1&quot;&gt;Yiduo Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_B/0/1/0/all/0/1&quot;&gt;Bojian Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1&quot;&gt;Xiangyu Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yuhui Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.08529">
<title>Diagnosing Human-object Interaction Detectors. (arXiv:2308.08529v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.08529</link>
<description rdf:parseType="Literal">&lt;p&gt;We have witnessed significant progress in human-object interaction (HOI)
detection. The reliance on mAP (mean Average Precision) scores as a summary
metric, however, does not provide sufficient insight into the nuances of model
performance (e.g., why one model is better than another), which can hinder
further innovation in this field. To address this issue, in this paper, we
introduce a diagnosis toolbox to provide detailed quantitative break-down
analysis of HOI detection models, inspired by the success of object detection
diagnosis toolboxes. We first conduct holistic investigations in the pipeline
of HOI detection. By defining a set of errors and the oracles to fix each of
them, we can have a quantitative analysis of the significance of different
errors according to the mAP improvement obtained from fixing each error. We
then delve into two sub-tasks of HOI detection: human-object pair detection and
interaction classification, respectively. For the first detection task, we
compute the coverage of ground-truth human-object pairs as well as the
noisiness level in the detection results. For the second classification task,
we measure a model&apos;s performance of differentiating positive and negative
detection results and also classifying the actual interactions when the
human-object pairs are correctly detected. We analyze eight state-of-the-art
HOI detection models and provide valuable diagnosis insights to foster future
research. For instance, our diagnosis shows that state-of-the-art model RLIPv2
outperforms others mainly because it significantly improves the multi-label
interaction classification accuracy. Our toolbox is applicable for different
methods across different datasets and available at
https://github.com/neu-vi/Diag-HOI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1&quot;&gt;Fangrui Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yiming Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1&quot;&gt;Weidi Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Huaizu Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.10196">
<title>Blind Face Restoration for Under-Display Camera via Dictionary Guided Transformer. (arXiv:2308.10196v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.10196</link>
<description rdf:parseType="Literal">&lt;p&gt;By hiding the front-facing camera below the display panel, Under-Display
Camera (UDC) provides users with a full-screen experience. However, due to the
characteristics of the display, images taken by UDC suffer from significant
quality degradation. Methods have been proposed to tackle UDC image restoration
and advances have been achieved. There are still no specialized methods and
datasets for restoring UDC face images, which may be the most common problem in
the UDC scene. To this end, considering color filtering, brightness
attenuation, and diffraction in the imaging process of UDC, we propose a
two-stage network UDC Degradation Model Network named UDC-DMNet to synthesize
UDC images by modeling the processes of UDC imaging. Then we use UDC-DMNet and
high-quality face images from FFHQ and CelebA-Test to create UDC face training
datasets FFHQ-P/T and testing datasets CelebA-Test-P/T for UDC face
restoration. We propose a novel dictionary-guided transformer network named
DGFormer. Introducing the facial component dictionary and the characteristics
of the UDC image in the restoration makes DGFormer capable of addressing blind
face restoration in UDC scenarios. Experiments show that our DGFormer and
UDC-DMNet achieve state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1&quot;&gt;Jingfan Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaoxu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kaihao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1&quot;&gt;Wenhan Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xiaocun Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12831">
<title>EFormer: Enhanced Transformer towards Semantic-Contour Features of Foreground for Portraits Matting. (arXiv:2308.12831v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12831</link>
<description rdf:parseType="Literal">&lt;p&gt;The portrait matting task aims to extract an alpha matte with complete
semantics and finely-detailed contours. In comparison to CNN-based approaches,
transformers with self-attention module have a better capacity to capture
long-range dependencies and low-frequency semantic information of a portrait.
However, the recent research shows that self-attention mechanism struggles with
modeling high-frequency contour information and capturing fine contour details,
which can lead to bias while predicting the portrait&apos;s contours. To deal with
this issue, we propose EFormer to enhance the model&apos;s attention towards both of
the low-frequency semantic and high-frequency contour features. For the
high-frequency contours, our research demonstrates that cross-attention module
between different resolutions can guide our model to allocate attention
appropriately to these contour regions. Supported on this, we can successfully
extract the high-frequency detail information around the portrait&apos;s contours,
which are previously ignored by self-attention. Based on cross-attention
module, we further build a semantic and contour detector (SCD) to accurately
capture both of the low-frequency semantic and high-frequency contour features.
And we design contour-edge extraction branch and semantic extraction branch to
extract refined high-frequency contour features and complete low-frequency
semantic information, respectively. Finally, we fuse the two kinds of features
and leverage segmentation head to generate a predicted portrait matte.
Experiments on VideoMatte240K (JPEG SD Format) and Adobe Image Matting (AIM)
datasets demonstrate that EFormer outperforms previous portrait matte methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zitao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_Q/0/1/0/all/0/1&quot;&gt;Qiguang Miao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1&quot;&gt;Peipei Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xi_Y/0/1/0/all/0/1&quot;&gt;Yue Xi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.13670">
<title>Linear Oscillation: A Novel Activation Function for Vision Transformer. (arXiv:2308.13670v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.13670</link>
<description rdf:parseType="Literal">&lt;p&gt;Activation functions are the linchpins of deep learning, profoundly
influencing both the representational capacity and training dynamics of neural
networks. They shape not only the nature of representations but also optimize
convergence rates and enhance generalization potential. Appreciating this
critical role, we present the Linear Oscillation (LoC) activation function,
defined as $f(x) = x \times \sin(\alpha x + \beta)$. Distinct from conventional
activation functions which primarily introduce non-linearity, LoC seamlessly
blends linear trajectories with oscillatory deviations. The nomenclature
&quot;Linear Oscillation&quot; is a nod to its unique attribute of infusing linear
activations with harmonious oscillations, capturing the essence of the
&quot;Importance of Confusion&quot;. This concept of &quot;controlled confusion&quot; within
network activations is posited to foster more robust learning, particularly in
contexts that necessitate discerning subtle patterns. Our empirical studies
reveal that, when integrated into diverse neural architectures, the LoC
activation function consistently outperforms established counterparts like ReLU
and Sigmoid. The stellar performance exhibited by the avant-garde Vision
Transformer model using LoC further validates its efficacy. This study
illuminates the remarkable benefits of the LoC over other prominent activation
functions. It champions the notion that intermittently introducing deliberate
complexity or &quot;confusion&quot; during training can spur more profound and nuanced
learning. This accentuates the pivotal role of judiciously selected activation
functions in shaping the future of neural network training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_J/0/1/0/all/0/1&quot;&gt;Juyoung Yun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16911">
<title>PointLLM: Empowering Large Language Models to Understand Point Clouds. (arXiv:2308.16911v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.16911</link>
<description rdf:parseType="Literal">&lt;p&gt;The unprecedented advancements in Large Language Models (LLMs) have shown a
profound impact on natural language processing but are yet to fully embrace the
realm of 3D understanding. This paper introduces PointLLM, a preliminary effort
to fill this gap, enabling LLMs to understand point clouds and offering a new
avenue beyond 2D visual data. PointLLM understands colored object point clouds
with human instructions and generates contextually appropriate responses,
illustrating its grasp of point clouds and common sense. Specifically, it
leverages a point cloud encoder with a powerful LLM to effectively fuse
geometric, appearance, and linguistic information. We collect a novel dataset
comprising 660K simple and 70K complex point-text instruction pairs to enable a
two-stage training strategy: aligning latent spaces and subsequently
instruction-tuning the unified model. To rigorously evaluate the perceptual and
generalization capabilities of PointLLM, we establish two benchmarks:
Generative 3D Object Classification and 3D Object Captioning, assessed through
three different methods, including human evaluation, GPT-4/ChatGPT evaluation,
and traditional metrics. Experimental results reveal PointLLM&apos;s superior
performance over existing 2D and 3D baselines, with a notable achievement in
human-evaluated object captioning tasks where it surpasses human annotators in
over 50% of the samples. Codes, datasets, and benchmarks are available at
https://github.com/OpenRobotLab/PointLLM .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Runsen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaolong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yilun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1&quot;&gt;Jiangmiao Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dahua Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.00748">
<title>PathLDM: Text conditioned Latent Diffusion Model for Histopathology. (arXiv:2309.00748v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.00748</link>
<description rdf:parseType="Literal">&lt;p&gt;To achieve high-quality results, diffusion models must be trained on large
datasets. This can be notably prohibitive for models in specialized domains,
such as computational pathology. Conditioning on labeled data is known to help
in data-efficient model training. Therefore, histopathology reports, which are
rich in valuable clinical information, are an ideal choice as guidance for a
histopathology generative model. In this paper, we introduce PathLDM, the first
text-conditioned Latent Diffusion Model tailored for generating high-quality
histopathology images. Leveraging the rich contextual information provided by
pathology text reports, our approach fuses image and textual data to enhance
the generation process. By utilizing GPT&apos;s capabilities to distill and
summarize complex text reports, we establish an effective conditioning
mechanism. Through strategic conditioning and necessary architectural
enhancements, we achieved a SoTA FID score of 7.64 for text-to-image generation
on the TCGA-BRCA dataset, significantly outperforming the closest
text-conditioned competitor with FID 30.1.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yellapragada_S/0/1/0/all/0/1&quot;&gt;Srikar Yellapragada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Graikos_A/0/1/0/all/0/1&quot;&gt;Alexandros Graikos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prasanna_P/0/1/0/all/0/1&quot;&gt;Prateek Prasanna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kurc_T/0/1/0/all/0/1&quot;&gt;Tahsin Kurc&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saltz_J/0/1/0/all/0/1&quot;&gt;Joel Saltz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samaras_D/0/1/0/all/0/1&quot;&gt;Dimitris Samaras&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.01340">
<title>MDSC: Towards Evaluating the Style Consistency Between Music and Dance. (arXiv:2309.01340v3 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2309.01340</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose MDSC(Music-Dance-Style Consistency), the first evaluation metric
that assesses to what degree the dance moves and music match. Existing metrics
can only evaluate the motion fidelity and diversity and the degree of rhythmic
matching between music and dance. MDSC measures how stylistically correlated
the generated dance motion sequences and the conditioning music sequences are.
We found that directly measuring the embedding distance between motion and
music is not an optimal solution. We instead tackle this through modeling it as
a clustering problem. Specifically, 1) we pre-train a music encoder and a
motion encoder, then 2) we learn to map and align the motion and music
embedding in joint space by jointly minimizing the intra-cluster distance and
maximizing the inter-cluster distance, and 3) for evaluation purposes, we
encode the dance moves into embedding and measure the intra-cluster and
inter-cluster distances, as well as the ratio between them. We evaluate our
metric on the results of several music-conditioned motion generation methods,
combined with user study, we found that our proposed metric is a robust
evaluation metric in measuring the music-dance style correlation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zixiang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Weiyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Baoyuan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14564">
<title>Generative Escher Meshes. (arXiv:2309.14564v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14564</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a fully-automatic, text-guided generative method for
producing periodic, repeating, tile-able 2D art, such as the one seen on
floors, mosaics, ceramics, and the work of M.C. Escher. In contrast to the
standard concept of a seamless texture, i.e., square images that are seamless
when tiled, our method generates non-square tilings which comprise solely of
repeating copies of the same object. It achieves this by optimizing both
geometry and color of a 2D mesh, in order to generate a non-square tile in the
shape and appearance of the desired object, with close to no additional
background details. We enable geometric optimization of tilings by our key
technical contribution: an unconstrained, differentiable parameterization of
the space of all possible tileable shapes for a given symmetry group. Namely,
we prove that modifying the laplacian used in a 2D mesh-mapping technique -
Orbifold Tutte Embedding - can achieve all possible tiling configurations for a
chosen planar symmetry group. We thus consider both the mesh&apos;s tile-shape and
its texture as optimizable parameters, rendering the textured mesh via a
differentiable renderer. We leverage a trained image diffusion model to define
a loss on the resulting image, thereby updating the mesh&apos;s parameters based on
its appearance matching the text prompt. We show our method is able to produce
plausible, appealing results, with non-trivial tiles, for a variety of
different periodic tiling patterns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aigerman_N/0/1/0/all/0/1&quot;&gt;Noam Aigerman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Groueix_T/0/1/0/all/0/1&quot;&gt;Thibault Groueix&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16496">
<title>CCEdit: Creative and Controllable Video Editing via Diffusion Models. (arXiv:2309.16496v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.16496</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present CCEdit, a versatile generative video editing
framework based on diffusion models. Our approach employs a novel trident
network structure that separates structure and appearance control, ensuring
precise and creative editing capabilities. Utilizing the foundational
ControlNet architecture, we maintain the structural integrity of the video
during editing. The incorporation of an additional appearance branch enables
users to exert fine-grained control over the edited key frame. These two side
branches seamlessly integrate into the main branch, which is constructed upon
existing text-to-image (T2I) generation models, through learnable temporal
layers. The versatility of our framework is demonstrated through a diverse
range of choices in both structure representations and personalized T2I models,
as well as the option to provide the edited key frame. To facilitate
comprehensive evaluation, we introduce the BalanceCC benchmark dataset,
comprising 100 videos and 4 target prompts for each video. Our extensive user
studies compare CCEdit with eight state-of-the-art video editing methods. The
outcomes demonstrate CCEdit&apos;s substantial superiority over all other methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1&quot;&gt;Ruoyu Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weng_W/0/1/0/all/0/1&quot;&gt;Wenming Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanhui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yuhui Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1&quot;&gt;Jianmin Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1&quot;&gt;Chong Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhibo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1&quot;&gt;Baining Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03940">
<title>Hard View Selection for Self-Supervised Learning. (arXiv:2310.03940v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03940</link>
<description rdf:parseType="Literal">&lt;p&gt;Many Self-Supervised Learning (SSL) methods train their models to be
invariant to different &quot;views&quot; of an image input for which a good data
augmentation pipeline is crucial. While considerable efforts were directed
towards improving pre-text tasks, architectures, or robustness (e.g., Siamese
networks or teacher-softmax centering), the majority of these methods remain
strongly reliant on the random sampling of operations within the image
augmentation pipeline, such as the random resized crop or color distortion
operation. In this paper, we argue that the role of the view generation and its
effect on performance has so far received insufficient attention. To address
this, we propose an easy, learning-free, yet powerful Hard View Selection (HVS)
strategy designed to extend the random view generation to expose the pretrained
model to harder samples during SSL training. It encompasses the following
iterative steps: 1) randomly sample multiple views and create pairs of two
views, 2) run forward passes for each view pair on the currently trained model,
3) adversarially select the pair yielding the worst loss, and 4) run the
backward pass with the selected pair. In our empirical analysis we show that
under the hood, HVS increases task difficulty by controlling the Intersection
over Union of views during pretraining. With only 300-epoch pretraining, HVS is
able to closely rival the 800-epoch DINO baseline which remains very favorable
even when factoring in the slowdown induced by the additional forwards of HVS.
Additionally, HVS consistently achieves accuracy improvements on ImageNet
between 0.4% and 1.9% on linear evaluation and similar improvements on transfer
tasks across multiple SSL methods, such as DINO, SimSiam, iBOT, and SimCLR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferreira_F/0/1/0/all/0/1&quot;&gt;Fabio Ferreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rapant_I/0/1/0/all/0/1&quot;&gt;Ivo Rapant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1&quot;&gt;Frank Hutter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04677">
<title>AG-CRC: Anatomy-Guided Colorectal Cancer Segmentation in CT with Imperfect Anatomical Knowledge. (arXiv:2310.04677v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04677</link>
<description rdf:parseType="Literal">&lt;p&gt;When delineating lesions from medical images, a human expert can always keep
in mind the anatomical structure behind the voxels. However, although
high-quality (though not perfect) anatomical information can be retrieved from
computed tomography (CT) scans with modern deep learning algorithms, it is
still an open problem how these automatically generated organ masks can assist
in addressing challenging lesion segmentation tasks, such as the segmentation
of colorectal cancer (CRC). In this paper, we develop a novel Anatomy-Guided
segmentation framework to exploit the auto-generated organ masks to aid CRC
segmentation from CT, namely AG-CRC. First, we obtain multi-organ segmentation
(MOS) masks with existing MOS models (e.g., TotalSegmentor) and further derive
a more robust organ of interest (OOI) mask that may cover most of the
colon-rectum and CRC voxels. Then, we propose an anatomy-guided training patch
sampling strategy by optimizing a heuristic gain function that considers both
the proximity of important regions (e.g., the tumor or organs of interest) and
sample diversity. Third, we design a novel self-supervised learning scheme
inspired by the topology of tubular organs like the colon to boost the model
performance further. Finally, we employ a masked loss scheme to guide the model
to focus solely on the essential learning region. We extensively evaluate the
proposed method on two CRC segmentation datasets, where substantial performance
improvement (5% to 9% in Dice) is achieved over current state-of-the-art
medical image segmentation models, and the ablation studies further evidence
the efficacy of every proposed component.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rongzhao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bai_Z/0/1/0/all/0/1&quot;&gt;Zhian Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yu_R/0/1/0/all/0/1&quot;&gt;Ruoying Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pang_W/0/1/0/all/0/1&quot;&gt;Wenrao Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lingyun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Lifeng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaofan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_W/0/1/0/all/0/1&quot;&gt;Weiguo Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05920">
<title>SimPLR: A Simple and Plain Transformer for Object Detection and Segmentation. (arXiv:2310.05920v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05920</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to detect objects in images at varying scales has played a
pivotal role in the design of modern object detectors. Despite considerable
progress in removing hand-crafted components and simplifying the architecture
with transformers, multi-scale feature maps and/or pyramid design remain a key
factor for their empirical success. In this paper, we show that this reliance
on either feature pyramids or an hierarchical backbone is unnecessary and a
transformer-based detector with scale-aware attention enables the plain
detector `SimPLR&apos; whose backbone and detection head are both non-hierarchical
and operate on single-scale features. The plain architecture allows SimPLR to
effectively take advantages of self-supervised learning and scaling approaches
with ViTs, yielding competitive performance compared to hierarchical and
multi-scale counterparts. We demonstrate through our experiments that when
scaling to larger ViT backbones, SimPLR indicates better performance than
end-to-end segmentation models (Mask2Former) and plain-backbone detectors
(ViTDet), while consistently being faster. The code will be released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Duy-Kien Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oswald_M/0/1/0/all/0/1&quot;&gt;Martin R. Oswald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1&quot;&gt;Cees G. M. Snoek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.06282">
<title>MuseChat: A Conversational Music Recommendation System for Videos. (arXiv:2310.06282v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.06282</link>
<description rdf:parseType="Literal">&lt;p&gt;Music recommendation for videos attracts growing interest in multi-modal
research. However, existing systems focus primarily on content compatibility,
often ignoring the users&apos; preferences. Their inability to interact with users
for further refinements or to provide explanations leads to a less satisfying
experience. We address these issues with MuseChat, a first-of-its-kind
dialogue-based recommendation system that personalizes music suggestions for
videos. Our system consists of two key functionalities with associated modules:
recommendation and reasoning. The recommendation module takes a video along
with optional information including previous suggested music and user&apos;s
preference as inputs and retrieves an appropriate music matching the context.
The reasoning module, equipped with the power of Large Language Model
(Vicuna-7B) and extended to multi-modal inputs, is able to provide reasonable
explanation for the recommended music. To evaluate the effectiveness of
MuseChat, we build a large-scale dataset, conversational music recommendation
for videos, that simulates a two-turn interaction between a user and a
recommender based on accurate music track information. Experiment results show
that MuseChat achieves significant improvements over existing video-based music
retrieval methods as well as offers strong interpretability and
interactability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1&quot;&gt;Zhikang Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiulong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Polak_P/0/1/0/all/0/1&quot;&gt;Pawel Polak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Peng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15578">
<title>VMAF Re-implementation on PyTorch: Some Experimental Results. (arXiv:2310.15578v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15578</link>
<description rdf:parseType="Literal">&lt;p&gt;Based on the standard VMAF implementation we propose an implementation of
VMAF using PyTorch framework. For this implementation comparisons with the
standard (libvmaf) show the discrepancy $\lesssim 10^{-2}$ in VMAF units. We
investigate gradients computation when using VMAF as an objective function and
demonstrate that training using this function does not result in ill-behaving
gradients. The implementation is then used to train a preprocessing filter. It
is demonstrated that its performance is superior to the unsharp masking filter.
The resulting filter is also easy for implementation and can be applied in
video processing tasks for video copression improvement. This is confirmed by
the results of numerical experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aistov_K/0/1/0/all/0/1&quot;&gt;Kirill Aistov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koroteev_M/0/1/0/all/0/1&quot;&gt;Maxim Koroteev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00213">
<title>Consistent Video-to-Video Transfer Using Synthetic Dataset. (arXiv:2311.00213v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00213</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a novel and efficient approach for text-based video-to-video
editing that eliminates the need for resource-intensive per-video-per-model
finetuning. At the core of our approach is a synthetic paired video dataset
tailored for video-to-video transfer tasks. Inspired by Instruct Pix2Pix&apos;s
image transfer via editing instruction, we adapt this paradigm to the video
domain. Extending the Prompt-to-Prompt to videos, we efficiently generate
paired samples, each with an input video and its edited counterpart. Alongside
this, we introduce the Long Video Sampling Correction during sampling, ensuring
consistent long videos across batches. Our method surpasses current methods
like Tune-A-Video, heralding substantial progress in text-based video-to-video
editing and suggesting exciting avenues for further exploration and deployment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jiaxin Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1&quot;&gt;Tianjun Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1&quot;&gt;Tong He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04591">
<title>Rethinking Event-based Human Pose Estimation with 3D Event Representations. (arXiv:2311.04591v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.04591</link>
<description rdf:parseType="Literal">&lt;p&gt;Human pose estimation is a fundamental and appealing task in computer vision.
Traditional frame-based cameras and videos are commonly applied, yet, they
become less reliable in scenarios under high dynamic range or heavy motion
blur. In contrast, event cameras offer a robust solution for navigating these
challenging contexts. Predominant methodologies incorporate event cameras into
learning frameworks by accumulating events into event frames. However, such
methods tend to marginalize the intrinsic asynchronous and high temporal
resolution characteristics of events. This disregard leads to a loss in
essential temporal dimension data, crucial for discerning distinct actions. To
address this issue and to unlock the 3D potential of event information, we
introduce two 3D event representations: the Rasterized Event Point Cloud
(RasEPC) and the Decoupled Event Voxel (DEV). The RasEPC collates events within
concise temporal slices at identical positions, preserving 3D attributes with
statistical cues and markedly mitigating memory and computational demands.
Meanwhile, the DEV representation discretizes events into voxels and projects
them across three orthogonal planes, utilizing decoupled event attention to
retrieve 3D cues from the 2D planes. Furthermore, we develop and release
EV-3DPW, a synthetic event-based dataset crafted to facilitate training and
quantitative analysis in outdoor scenes. On the public real-world DHP19
dataset, our event point cloud technique excels in real-time mobile
predictions, while the decoupled event voxel method achieves the highest
accuracy. Experiments on EV-3DPW demonstrate that the robustness of our
proposed 3D representation methods compared to traditional RGB images and event
frame techniques under the same backbones. Our code and dataset have been made
publicly available at https://github.com/MasterHow/EventPointPose.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1&quot;&gt;Xiaoting Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Hao Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiaan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ze Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1&quot;&gt;Yaozu Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_H/0/1/0/all/0/1&quot;&gt;Huajian Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kailun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kaiwei Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09680">
<title>Trustworthy Large Models in Vision: A Survey. (arXiv:2311.09680v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09680</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid progress of Large Models (LMs) has recently revolutionized various
fields of deep learning with remarkable grades, ranging from Natural Language
Processing (NLP) to Computer Vision (CV). However, LMs are increasingly
challenged and criticized by academia and industry due to their powerful
performance but untrustworthy behavior, which urgently needs to be alleviated
by reliable methods. Despite the abundance of literature on trustworthy LMs in
NLP, a systematic survey specifically delving into the trustworthiness of LMs
in CV remains absent. In order to mitigate this gap, we summarize four relevant
concerns that obstruct the trustworthy usage in vision of LMs in this survey,
including 1) human misuse, 2) vulnerability, 3) inherent issue and 4)
interpretability. By highlighting corresponding challenge, countermeasures, and
discussion in each topic, we hope this survey will facilitate readers&apos;
understanding of this field, promote alignment of LMs with human expectations
and enable trustworthy LMs to serve as welfare rather than disaster for human
society.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Ziyan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Li Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jun Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12912">
<title>Q-Seg: Quantum Annealing-based Unsupervised Image Segmentation. (arXiv:2311.12912v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12912</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we present Q-Seg, a novel unsupervised image segmentation
method based on quantum annealing, tailored for existing quantum hardware. We
formulate the pixel-wise segmentation problem, which assimilates spectral and
spatial information of the image, as a graph-cut optimization task. Our method
efficiently leverages the interconnected qubit topology of the D-Wave Advantage
device, offering superior scalability over existing quantum approaches and
outperforming state-of-the-art classical methods. Our empirical evaluations on
synthetic datasets reveal that Q-Seg offers better runtime performance against
the classical optimizer Gurobi. Furthermore, we evaluate our method on
segmentation of Earth Observation images, an area of application where the
amount of labeled data is usually very limited. In this case, Q-Seg
demonstrates near-optimal results in flood mapping detection with respect to
classical supervised state-of-the-art machine learning methods. Also, Q-Seg
provides enhanced segmentation for forest coverage compared to existing
annotated masks. Thus, Q-Seg emerges as a viable alternative for real-world
applications using available quantum hardware, particularly in scenarios where
the lack of labeled data and computational runtime are critical.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1&quot;&gt;Supreeth Mysore Venkatesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Macaluso_A/0/1/0/all/0/1&quot;&gt;Antonio Macaluso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nuske_M/0/1/0/all/0/1&quot;&gt;Marlon Nuske&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klusch_M/0/1/0/all/0/1&quot;&gt;Matthias Klusch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1&quot;&gt;Andreas Dengel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14049">
<title>Assessment of Deep Learning Segmentation for Real-Time Free-Breathing Cardiac Magnetic Resonance Imaging. (arXiv:2311.14049v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.14049</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, a variety of deep learning networks for cardiac MRI (CMR)
segmentation have been developed and analyzed. However, nearly all of them are
focused on cine CMR under breathold. In this work, accuracy of deep learning
methods is assessed for volumetric analysis (via segmentation) of the left
ventricle in real-time free-breathing CMR at rest and under exercise stress.
Data from healthy volunteers (n=15) for cine and real-time free-breathing CMR
were analyzed retrospectively. Segmentations of a commercial software (comDL)
and a freely available neural network (nnU-Net), were compared to a reference
created via the manual correction of comDL segmentation. Segmentation of left
ventricular endocardium (LV), left ventricular myocardium (MYO), and right
ventricle (RV) is evaluated for both end-systolic and end-diastolic phases and
analyzed with Dice&apos;s coefficient (DC). The volumetric analysis includes LV
end-diastolic volume (EDV), LV end-systolic volume (ESV), and LV ejection
fraction (EF). For cine CMR, nnU-Net and comDL achieve a DC above 0.95 for LV
and 0.9 for MYO, and RV. For real-time CMR, the accuracy of nnU-Net exceeds
that of comDL overall. For real-time CMR at rest, nnU-Net achieves a DC of 0.94
for LV, 0.89 for MYO, and 0.90 for RV; mean absolute differences between
nnU-Net and reference are 2.9mL for EDV, 3.5mL for ESV and 2.6% for EF. For
real-time CMR under exercise stress, nnU-Net achieves a DC of 0.92 for LV, 0.85
for MYO, and 0.83 for RV; mean absolute differences between nnU-Net and
reference are 11.4mL for EDV, 2.9mL for ESV and 3.6% for EF. Deep learning
methods designed or trained for cine CMR segmentation can perform well on
real-time CMR. For real-time free-breathing CMR at rest, the performance of
deep learning methods is comparable to inter-observer variability in cine CMR
and is usable or fully automatic segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schilling_M/0/1/0/all/0/1&quot;&gt;Martin Schilling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Unterberg_Buchwald_C/0/1/0/all/0/1&quot;&gt;Christina Unterberg-Buchwald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lotz_J/0/1/0/all/0/1&quot;&gt;Joachim Lotz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Uecker_M/0/1/0/all/0/1&quot;&gt;Martin Uecker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14521">
<title>GaussianEditor: Swift and Controllable 3D Editing with Gaussian Splatting. (arXiv:2311.14521v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.14521</link>
<description rdf:parseType="Literal">&lt;p&gt;3D editing plays a crucial role in many areas such as gaming and virtual
reality. Traditional 3D editing methods, which rely on representations like
meshes and point clouds, often fall short in realistically depicting complex
scenes. On the other hand, methods based on implicit 3D representations, like
Neural Radiance Field (NeRF), render complex scenes effectively but suffer from
slow processing speeds and limited control over specific scene areas. In
response to these challenges, our paper presents GaussianEditor, an innovative
and efficient 3D editing algorithm based on Gaussian Splatting (GS), a novel 3D
representation. GaussianEditor enhances precision and control in editing
through our proposed Gaussian semantic tracing, which traces the editing target
throughout the training process. Additionally, we propose Hierarchical Gaussian
splatting (HGS) to achieve stabilized and fine results under stochastic
generative guidance from 2D diffusion models. We also develop editing
strategies for efficient object removal and integration, a challenging task for
existing methods. Our comprehensive experiments demonstrate GaussianEditor&apos;s
superior control, efficacy, and rapid performance, marking a significant
advancement in 3D editing. Project Page:
https://buaacyw.github.io/gaussian-editor/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yiwen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zilong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Feng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yikai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1&quot;&gt;Zhongang Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huaping Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1&quot;&gt;Guosheng Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15890">
<title>Stability-Informed Initialization of Neural Ordinary Differential Equations. (arXiv:2311.15890v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.15890</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper addresses the training of Neural Ordinary Differential Equations
(neural ODEs), and in particular explores the interplay between numerical
integration techniques, stability regions, step size, and initialization
techniques. It is shown how the choice of integration technique implicitly
regularizes the learned model, and how the solver&apos;s corresponding stability
region affects training and prediction performance. From this analysis, a
stability-informed parameter initialization technique is introduced. The
effectiveness of the initialization method is displayed across several learning
benchmarks and industrial applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Westny_T/0/1/0/all/0/1&quot;&gt;Theodor Westny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohammadi_A/0/1/0/all/0/1&quot;&gt;Arman Mohammadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_D/0/1/0/all/0/1&quot;&gt;Daniel Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frisk_E/0/1/0/all/0/1&quot;&gt;Erik Frisk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17116">
<title>REF$^2$-NeRF: Reflection and Refraction aware Neural Radiance Field. (arXiv:2311.17116v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17116</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, significant progress has been made in the study of methods for 3D
reconstruction from multiple images using implicit neural representations,
exemplified by the neural radiance field (NeRF) method. Such methods, which are
based on volume rendering, can model various light phenomena, and various
extended methods have been proposed to accommodate different scenes and
situations. However, when handling scenes with multiple glass objects, e.g.,
objects in a glass showcase, modeling the target scene accurately has been
challenging due to the presence of multiple reflection and refraction effects.
Thus, this paper proposes a NeRF-based modeling method for scenes containing a
glass case. In the proposed method, refraction and reflection are modeled using
elements that are dependent and independent of the viewer&apos;s perspective. This
approach allows us to estimate the surfaces where refraction occurs, i.e.,
glass surfaces, and enables the separation and modeling of both direct and
reflected light components. Compared to existing methods, the proposed method
enables more accurate modeling of both glass refraction and the overall scene.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1&quot;&gt;Wooseok Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fukiage_T/0/1/0/all/0/1&quot;&gt;Taiki Fukiage&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oishi_T/0/1/0/all/0/1&quot;&gt;Takeshi Oishi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17179">
<title>SatCLIP: Global, General-Purpose Location Embeddings with Satellite Imagery. (arXiv:2311.17179v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17179</link>
<description rdf:parseType="Literal">&lt;p&gt;Geographic location is essential for modeling tasks in fields ranging from
ecology to epidemiology to the Earth system sciences. However, extracting
relevant and meaningful characteristics of a location can be challenging, often
entailing expensive data fusion or data distillation from global imagery
datasets. To address this challenge, we introduce Satellite Contrastive
Location-Image Pretraining (SatCLIP), a global, general-purpose geographic
location encoder that learns an implicit representation of locations from
openly available satellite imagery. Trained location encoders provide vector
embeddings summarizing the characteristics of any given location for convenient
usage in diverse downstream tasks. We show that SatCLIP embeddings, pretrained
on globally sampled multi-spectral Sentinel-2 satellite data, can be used in
various predictive tasks that depend on location information but not
necessarily satellite imagery, including temperature prediction, animal
recognition in imagery, and population density estimation. Across tasks,
SatCLIP embeddings consistently outperform embeddings from existing pretrained
location encoders, ranging from models trained on natural images to models
trained on semantic context. SatCLIP embeddings also help to improve geographic
generalization. This demonstrates the potential of general-purpose location
encoders and opens the door to learning meaningful representations of our
planet from the vast, varied, and largely untapped modalities of geospatial
data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klemmer_K/0/1/0/all/0/1&quot;&gt;Konstantin Klemmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rolf_E/0/1/0/all/0/1&quot;&gt;Esther Rolf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robinson_C/0/1/0/all/0/1&quot;&gt;Caleb Robinson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mackey_L/0/1/0/all/0/1&quot;&gt;Lester Mackey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russwurm_M/0/1/0/all/0/1&quot;&gt;Marc Ru&amp;#xdf;wurm&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17315">
<title>Explaining CLIP&apos;s performance disparities on data from blind/low vision users. (arXiv:2311.17315v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17315</link>
<description rdf:parseType="Literal">&lt;p&gt;Large multi-modal models (LMMs) hold the potential to usher in a new era of
automated visual assistance for people who are blind or low vision (BLV). Yet,
these models have not been systematically evaluated on data captured by BLV
users. We address this by empirically assessing CLIP, a widely-used LMM likely
to underpin many assistive technologies. Testing 25 CLIP variants in a
zero-shot classification task, we find that their accuracy is 15 percentage
points lower on average for images captured by BLV users than web-crawled
images. This disparity stems from CLIP&apos;s sensitivities to 1) image content
(e.g. not recognizing disability objects as well as other objects); 2) image
quality (e.g. not being robust to lighting variation); and 3) text content
(e.g. not recognizing objects described by tactile adjectives as well as visual
ones). We delve deeper with a textual analysis of three common pre-training
datasets: LAION-400M, LAION-2B and DataComp-1B, showing that disability content
is rarely mentioned. We then provide three examples that illustrate how the
performance disparities extend to three downstream models underpinned by CLIP:
OWL-ViT, CLIPSeg and DALL-E2. We find that few-shot learning with as few as 5
images can mitigate CLIP&apos;s quality-of-service disparities for BLV users in some
scenarios, which we discuss alongside a set of other possible mitigations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Massiceti_D/0/1/0/all/0/1&quot;&gt;Daniela Massiceti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Longden_C/0/1/0/all/0/1&quot;&gt;Camilla Longden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Slowik_A/0/1/0/all/0/1&quot;&gt;Agnieszka S&amp;#x142;owik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wills_S/0/1/0/all/0/1&quot;&gt;Samuel Wills&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grayson_M/0/1/0/all/0/1&quot;&gt;Martin Grayson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morrison_C/0/1/0/all/0/1&quot;&gt;Cecily Morrison&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17338">
<title>VideoAssembler: Identity-Consistent Video Generation with Reference Entities using Diffusion Model. (arXiv:2311.17338v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17338</link>
<description rdf:parseType="Literal">&lt;p&gt;Identity-consistent video generation seeks to synthesize videos that are
guided by both textual prompts and reference images of entities. Current
approaches typically utilize cross-attention layers to integrate the appearance
of the entity, which predominantly captures semantic attributes, resulting in
compromised fidelity of entities. Moreover, these methods necessitate iterative
fine-tuning for each new entity encountered, thereby limiting their
applicability. To address these challenges, we introduce VideoAssembler, a
novel end-to-end framework for identity-consistent video generation that can
conduct inference directly when encountering new entities. VideoAssembler is
adept at producing videos that are not only flexible with respect to the input
reference entities but also responsive to textual conditions. Additionally, by
modulating the quantity of input images for the entity, VideoAssembler enables
the execution of tasks ranging from image-to-video generation to sophisticated
video editing. VideoAssembler comprises two principal components: the Reference
Entity Pyramid (REP) encoder and the Entity-Prompt Attention Fusion (EPAF)
module. The REP encoder is designed to infuse comprehensive appearance details
into the denoising stages of the stable diffusion model. Concurrently, the EPAF
module is utilized to integrate text-aligned features effectively. Furthermore,
to mitigate the challenge of scarce data, we present a methodology for the
preprocessing of training data. Our evaluation of the VideoAssembler framework
on the UCF-101, MSR-VTT, and DAVIS datasets indicates that it achieves good
performances in both quantitative and qualitative analyses (346.84 in FVD and
48.01 in IS on UCF-101). Our project page is at
https://gulucaptain.github.io/videoassembler/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Haoyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1&quot;&gt;Tianyi Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jiaxi Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zuxuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yu-Gang Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17465">
<title>AgentAvatar: Disentangling Planning, Driving and Rendering for Photorealistic Avatar Agents. (arXiv:2311.17465v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17465</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, our goal is to create interactive avatar agents that can
autonomously plan and animate nuanced facial movements realistically, from both
visual and behavioral perspectives. Given high-level inputs about the
environment and agent profile, our framework harnesses LLMs to produce a series
of detailed text descriptions of the avatar agents&apos; facial motions. These
descriptions are then processed by our task-agnostic driving engine into motion
token sequences, which are subsequently converted into continuous motion
embeddings that are further consumed by our standalone neural-based renderer to
generate the final photorealistic avatar animations. These streamlined
processes allow our framework to adapt to a variety of non-verbal avatar
interactions, both monadic and dyadic. Our extensive study, which includes
experiments on both newly compiled and existing datasets featuring two types of
agents -- one capable of monadic interaction with the environment, and the
other designed for dyadic conversation -- validates the effectiveness and
versatility of our approach. To our knowledge, we advanced a leap step by
combining LLMs and neural rendering for generalized non-verbal prediction and
photo-realistic rendering of avatar agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Duomin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1&quot;&gt;Bin Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yu Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Baoyuan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17475">
<title>CLiSA: A Hierarchical Hybrid Transformer Model using Orthogonal Cross Attention for Satellite Image Cloud Segmentation. (arXiv:2311.17475v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17475</link>
<description rdf:parseType="Literal">&lt;p&gt;Clouds in optical satellite images are a major concern since their presence
hinders the ability to carry accurate analysis as well as processing. Presence
of clouds also affects the image tasking schedule and results in wastage of
valuable storage space on ground as well as space-based systems. Due to these
reasons, deriving accurate cloud masks from optical remote-sensing images is an
important task. Traditional methods such as threshold-based, spatial filtering
for cloud detection in satellite images suffer from lack of accuracy. In recent
years, deep learning algorithms have emerged as a promising approach to solve
image segmentation problems as it allows pixel-level classification and
semantic-level segmentation. In this paper, we introduce a deep-learning model
based on hybrid transformer architecture for effective cloud mask generation
named CLiSA - Cloud segmentation via Lipschitz Stable Attention network. In
this context, we propose an concept of orthogonal self-attention combined with
hierarchical cross attention model, and we validate its Lipschitz stability
theoretically and empirically. We design the whole setup under adversarial
setting in presence of Lov\&apos;asz-Softmax loss. We demonstrate both qualitative
and quantitative outcomes for multiple satellite image datasets including
Landsat-8, Sentinel-2, and Cartosat-2s. Performing comparative study we show
that our model performs preferably against other state-of-the-art methods and
also provides better generalization in precise cloud extraction from satellite
multi-spectral (MX) images. We also showcase different ablation studies to
endorse our choices corresponding to different architectural elements and
objective functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1&quot;&gt;Subhajit Paul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Ashutosh Gupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17618">
<title>ShapeGPT: 3D Shape Generation with A Unified Multi-modal Language Model. (arXiv:2311.17618v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17618</link>
<description rdf:parseType="Literal">&lt;p&gt;The advent of large language models, enabling flexibility through
instruction-driven approaches, has revolutionized many traditional generative
tasks, but large models for 3D data, particularly in comprehensively handling
3D shapes with other modalities, are still under-explored. By achieving
instruction-based shape generations, versatile multimodal generative shape
models can significantly benefit various fields like 3D virtual construction
and network-aided design. In this work, we present ShapeGPT, a shape-included
multi-modal framework to leverage strong pre-trained language models to address
multiple shape-relevant tasks. Specifically, ShapeGPT employs a
word-sentence-paragraph framework to discretize continuous shapes into shape
words, further assembles these words for shape sentences, as well as integrates
shape with instructional text for multi-modal paragraphs. To learn this
shape-language model, we use a three-stage training scheme, including shape
representation, multimodal alignment, and instruction-based generation, to
align shape-language codebooks and learn the intricate correlations among these
modalities. Extensive experiments demonstrate that ShapeGPT achieves comparable
performance across shape-relevant tasks, including text-to-shape,
shape-to-text, shape completion, and shape editing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_F/0/1/0/all/0/1&quot;&gt;Fukun Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1&quot;&gt;Biao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zibo Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1&quot;&gt;Jiayuan Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1&quot;&gt;Gang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Taihao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tao Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.18765">
<title>MLLMs-Augmented Visual-Language Representation Learning. (arXiv:2311.18765v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.18765</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual-language pre-training (VLP) has achieved remarkable success in
multi-modal tasks, largely attributed to the availability of large-scale
image-text datasets. In this work, we demonstrate that multi-modal large
language models (MLLMs) can enhance visual-language representation learning by
improving data quality. Our approach is simple, utilizing MLLMs to extend
multiple captions for each image. To prevent the bias introduced by MLLMs&apos;
hallucinations and intrinsic caption styles, we propose &quot;text shearing&quot; to
maintain the same length for extended captions as that of the original
captions. In image-text retrieval, our method consistently obtains 5.6 ~ 35.0%
and 16.8 ~ 46.1% improvement on R@1 under the fine-tuning and zero-shot
settings, respectively. Notably, we obtain zero-shot results that are
comparable to fine-tuning on target datasets, which encourages more exploration
of the versatile use of MLLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yanqing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1&quot;&gt;Wenqi Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1&quot;&gt;Mike Zheng Shou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kaipeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1&quot;&gt;Yang You&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>