<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-10-08T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03747" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03749" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03753" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03754" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03756" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03759" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03760" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03762" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03766" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03770" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03778" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03779" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03780" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03813" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03823" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03840" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03882" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03890" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03912" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03916" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03919" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03925" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03940" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03951" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03964" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03965" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03967" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03977" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03981" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04010" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04041" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04055" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04072" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04074" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04081" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04102" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04128" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04148" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04171" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04178" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04205" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04218" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04232" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04241" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04266" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04270" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04276" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04285" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04288" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04295" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04304" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04306" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04323" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04345" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04353" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04373" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04381" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04395" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04406" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04407" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04413" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1806.06298" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2104.03893" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2111.02272" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.06865" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.15376" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.01306" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.06281" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.04205" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.06096" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.07877" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.10888" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.14993" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.03584" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14405" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17154" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17590" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18030" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04959" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07863" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11363" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11695" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03887" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11804" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11838" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.05938" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.09826" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14293" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14674" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16108" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.17255" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.17277" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00100" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00771" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00785" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01320" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02239" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02687" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02842" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03149" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03205" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03281" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03605" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.05608" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2310.03747">
<title>A Knowledge-Driven Cross-view Contrastive Learning for EEG Representation. (arXiv:2310.03747v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2310.03747</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the abundant neurophysiological information in the
electroencephalogram (EEG) signal, EEG signals integrated with deep learning
methods have gained substantial traction across numerous real-world tasks.
However, the development of supervised learning methods based on EEG signals
has been hindered by the high cost and significant label discrepancies to
manually label large-scale EEG datasets. Self-supervised frameworks are adopted
in vision and language fields to solve this issue, but the lack of EEG-specific
theoretical foundations hampers their applicability across various tasks. To
solve these challenges, this paper proposes a knowledge-driven cross-view
contrastive learning framework (KDC2), which integrates neurological theory to
extract effective representations from EEG with limited labels. The KDC2 method
creates scalp and neural views of EEG signals, simulating the internal and
external representation of brain activity. Sequentially, inter-view and
cross-view contrastive learning pipelines in combination with various
augmentation methods are applied to capture neural features from different
views. By modeling prior neural knowledge based on homologous neural
information consistency theory, the proposed method extracts invariant and
complementary neural knowledge to generate combined representations.
Experimental results on different downstream tasks demonstrate that our method
outperforms state-of-the-art methods, highlighting the superior generalization
of neural knowledge-supported EEG representations across various brain tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Weng_W/0/1/0/all/0/1&quot;&gt;Weining Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yang Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qihui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yingying Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Miao_C/0/1/0/all/0/1&quot;&gt;Chunyan Miao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yiqiang Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03749">
<title>SCVCNet: Sliding cross-vector convolution network for cross-task and inter-individual-set EEG-based cognitive workload recognition. (arXiv:2310.03749v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2310.03749</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a generic approach for applying the cognitive workload
recognizer by exploiting common electroencephalogram (EEG) patterns across
different human-machine tasks and individual sets. We propose a neural network
called SCVCNet, which eliminates task- and individual-set-related interferences
in EEGs by analyzing finer-grained frequency structures in the power spectral
densities. The SCVCNet utilizes a sliding cross-vector convolution (SCVC)
operation, where paired input layers representing the theta and alpha power are
employed. By extracting the weights from a kernel matrix&apos;s central row and
column, we compute the weighted sum of the two vectors around a specified scalp
location. Next, we introduce an inter-frequency-point feature integration
module to fuse the SCVC feature maps. Finally, we combined the two modules with
the output-channel pooling and classification layers to construct the model. To
train the SCVCNet, we employ the regularized least-square method with ridge
regression and the extreme learning machine theory. We validate its performance
using three databases, each consisting of distinct tasks performed by
independent participant groups. The average accuracy (0.6813 and 0.6229) and F1
score (0.6743 and 0.6076) achieved in two different validation paradigms show
partially higher performance than the previous works. All features and
algorithms are available on website:https://github.com/7ohnKeats/SCVCNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Li Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhan_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianhua Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Zhong Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03753">
<title>ECGNet: A generative adversarial network (GAN) approach to the synthesis of 12-lead ECG signals from single lead inputs. (arXiv:2310.03753v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2310.03753</link>
<description rdf:parseType="Literal">&lt;p&gt;Electrocardiography (ECG) signal generation has been heavily explored using
generative adversarial networks (GAN) because the implementation of 12-lead
ECGs is not always feasible. The GAN models have achieved remarkable results in
reproducing ECG signals but are only designed for multiple lead inputs and the
features the GAN model preserves have not been identified-limiting the
generated signals use in cardiovascular disease (CVD)-predictive models. This
paper presents ECGNet which is a procedure that generates a complete set of
12-lead ECG signals from any single lead input using a GAN framework with a
bidirectional long short-term memory (LSTM) generator and a convolutional
neural network (CNN) discriminator. Cross and auto-correlation analysis
performed on the generated signals identifies features conserved during the
signal generation-i.e., features that can characterize the unique-nature of
each signal and thus likely indicators of CVD. Finally, by using ECG signals
annotated with the CVD-indicative features detailed by the correlation analysis
as inputs for a CVD-onset-predictive CNN model, we overcome challenges
preventing the prediction of multiple-CVD targets. Our models are experimented
on 15s 12-lead ECG dataset recorded using MyoVista&apos;s wavECG. Functional outcome
data for each patient is recorded and used in the CVD-predictive model. Our
best GAN model achieves state-of-the-art accuracy with Frechet Distance (FD)
scores of 4.73, 4.89, 5.18, 4.77, 4.71, and 5.55 on the V1-V6 pre-cordial leads
respectively and shows strength in preserving the P-Q segments and R-peaks in
the generated signals. To the best of our knowledge, ECGNet is the first to
predict all of the remaining eleven leads from the input of any single lead.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bagga_M/0/1/0/all/0/1&quot;&gt;Max Bagga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jeon_H/0/1/0/all/0/1&quot;&gt;Hyunbae Jeon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Issokson_A/0/1/0/all/0/1&quot;&gt;Alex Issokson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03754">
<title>EMGTFNet: Fuzzy Vision Transformer to decode Upperlimb sEMG signals for Hand Gestures Recognition. (arXiv:2310.03754v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2310.03754</link>
<description rdf:parseType="Literal">&lt;p&gt;Myoelectric control is an area of electromyography of increasing interest
nowadays, particularly in applications such as Hand Gesture Recognition (HGR)
for bionic prostheses. Today&apos;s focus is on pattern recognition using Machine
Learning and, more recently, Deep Learning methods. Despite achieving good
results on sparse sEMG signals, the latter models typically require large
datasets and training times. Furthermore, due to the nature of stochastic sEMG
signals, traditional models fail to generalize samples for atypical or noisy
values. In this paper, we propose the design of a Vision Transformer (ViT)
based architecture with a Fuzzy Neural Block (FNB) called EMGTFNet to perform
Hand Gesture Recognition from surface electromyography (sEMG) signals. The
proposed EMGTFNet architecture can accurately classify a variety of hand
gestures without any need for data augmentation techniques, transfer learning
or a significant increase in the number of parameters in the network. The
accuracy of the proposed model is tested using the publicly available NinaPro
database consisting of 49 different hand gestures. Experiments yield an average
test accuracy of 83.57\% \&amp;amp; 3.5\% using a 200 ms window size and only 56,793
trainable parameters. Our results outperform the ViT without FNB, thus
demonstrating that including FNB improves its performance. Our proposal
framework EMGTFNet reported the significant potential for its practical
application for prosthetic control.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cordova_J/0/1/0/all/0/1&quot;&gt;Joseph Cherre C&amp;#xf3;rdova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Flores_C/0/1/0/all/0/1&quot;&gt;Christian Flores&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Andreu_Perez_J/0/1/0/all/0/1&quot;&gt;Javier Andreu-Perez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03756">
<title>A Multi-channel EEG Data Analysis for Poor Neuro-prognostication in Comatose Patients with Self and Cross-channel Attention Mechanism. (arXiv:2310.03756v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2310.03756</link>
<description rdf:parseType="Literal">&lt;p&gt;This work investigates the predictive potential of bipolar
electroencephalogram (EEG) recordings towards efficient prediction of poor
neurological outcomes. A retrospective design using a hybrid deep learning
approach is utilized to optimize an objective function aiming for high
specificity, i.e., true positive rate (TPR) with reduced false positives (&amp;lt;
0.05). A multi-channel EEG array of 18 bipolar channel pairs from a randomly
selected 5-minute segment in an hour is kept. In order to determine the outcome
prediction, a combination of a feature encoder with 1-D convolutional layers,
learnable position encoding, a context network with attention mechanisms, and
finally, a regressor and classifier blocks are used. The feature encoder
extricates local temporal and spatial features, while the following position
encoding and attention mechanisms attempt to capture global temporal
dependencies. Results: The proposed framework by our team, OUS IVS, when
validated on the challenge hidden validation data, exhibited a score of 0.57.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qadir_H/0/1/0/all/0/1&quot;&gt;Hemin Ali Qadir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nesaragi_N/0/1/0/all/0/1&quot;&gt;Naimahmed Nesaragi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Halvorsen_P/0/1/0/all/0/1&quot;&gt;Per Steiner Halvorsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Balasingham_I/0/1/0/all/0/1&quot;&gt;Ilangko Balasingham&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03759">
<title>A Novel Deep Learning Technique for Morphology Preserved Fetal ECG Extraction from Mother ECG using 1D-CycleGAN. (arXiv:2310.03759v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2310.03759</link>
<description rdf:parseType="Literal">&lt;p&gt;Monitoring the electrical pulse of fetal heart through a non-invasive fetal
electrocardiogram (fECG) can easily detect abnormalities in the developing
heart to significantly reduce the infant mortality rate and post-natal
complications. Due to the overlapping of maternal and fetal R-peaks, the low
amplitude of the fECG, systematic and ambient noises, typical signal extraction
methods, such as adaptive filters, independent component analysis, empirical
mode decomposition, etc., are unable to produce satisfactory fECG. While some
techniques can produce accurate QRS waves, they often ignore other important
aspects of the ECG. Our approach, which is based on 1D CycleGAN, can
reconstruct the fECG signal from the mECG signal while maintaining the
morphology due to extensive preprocessing and appropriate framework. The
performance of our solution was evaluated by combining two available datasets
from Physionet, &quot;Abdominal and Direct Fetal ECG Database&quot; and &quot;Fetal
electrocardiograms, direct and abdominal with reference heartbeat annotations&quot;,
where it achieved an average PCC and Spectral-Correlation score of 88.4% and
89.4%, respectively. It detects the fQRS of the signal with accuracy,
precision, recall and F1 score of 92.6%, 97.6%, 94.8% and 96.4%, respectively.
It can also accurately produce the estimation of fetal heart rate and R-R
interval with an error of 0.25% and 0.27%, respectively. The main contribution
of our work is that, unlike similar studies, it can retain the morphology of
the ECG signal with high fidelity. The accuracy of our solution for fetal heart
rate and R-R interval length is comparable to existing state-of-the-art
techniques. This makes it a highly effective tool for early diagnosis of fetal
heart diseases and regular health checkups of the fetus.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Basak_P/0/1/0/all/0/1&quot;&gt;Promit Basak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sakib_A/0/1/0/all/0/1&quot;&gt;A.H.M Nazmus Sakib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chowdhury_M/0/1/0/all/0/1&quot;&gt;Muhammad E. H. Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Al_Emadi_N/0/1/0/all/0/1&quot;&gt;Nasser Al-Emadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yalcin_H/0/1/0/all/0/1&quot;&gt;Huseyin Cagatay Yalcin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pedersen_S/0/1/0/all/0/1&quot;&gt;Shona Pedersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mahmud_S/0/1/0/all/0/1&quot;&gt;Sakib Mahmud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kiranyaz_S/0/1/0/all/0/1&quot;&gt;Serkan Kiranyaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Al_Maadeed_S/0/1/0/all/0/1&quot;&gt;Somaya Al-Maadeed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03760">
<title>Investigating Deep Neural Network Architecture and Feature Extraction Designs for Sensor-based Human Activity Recognition. (arXiv:2310.03760v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2310.03760</link>
<description rdf:parseType="Literal">&lt;p&gt;The extensive ubiquitous availability of sensors in smart devices and the
Internet of Things (IoT) has opened up the possibilities for implementing
sensor-based activity recognition. As opposed to traditional sensor time-series
processing and hand-engineered feature extraction, in light of deep learning&apos;s
proven effectiveness across various domains, numerous deep methods have been
explored to tackle the challenges in activity recognition, outperforming the
traditional signal processing and traditional machine learning approaches. In
this work, by performing extensive experimental studies on two human activity
recognition datasets, we investigate the performance of common deep learning
and machine learning approaches as well as different training mechanisms (such
as contrastive learning), and various feature representations extracted from
the sensor time-series data and measure their effectiveness for the human
activity recognition task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ahangarani_D/0/1/0/all/0/1&quot;&gt;Danial Ahangarani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shirazi_M/0/1/0/all/0/1&quot;&gt;Mohammad Shirazi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ashraf_N/0/1/0/all/0/1&quot;&gt;Navid Ashraf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03762">
<title>Optimizing Multicarrier Multiantenna Systems for LoS Channel Charting. (arXiv:2310.03762v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2310.03762</link>
<description rdf:parseType="Literal">&lt;p&gt;Channel charting (CC) consists in learning a mapping between the space of raw
channel observations, made available from pilot-based channel estimation in
multicarrier multiantenna system, and a low-dimensional space where close
points correspond to channels of user equipments (UEs) close spatially. Among
the different methods of learning this mapping, some rely on a distance measure
between channel vectors. Such a distance should reliably reflect the local
spatial neighborhoods of the UEs. The recently proposed phase-insensitive (PI)
distance exhibits good properties in this regards, but suffers from ambiguities
due to both its periodic and oscillatory aspects, making users far away from
each other appear closer in some cases. In this paper, a thorough theoretical
analysis of the said distance and its limitations is provided, giving insights
on how they can be mitigated. Guidelines for designing systems capable of
learning quality charts are consequently derived. Experimental validation is
then conducted on synthetic and realistic data in different scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yassine_T/0/1/0/all/0/1&quot;&gt;Taha Yassine&lt;/a&gt; (IRT b-com, Hypermedia), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Magoarou_L/0/1/0/all/0/1&quot;&gt;Luc Le Magoarou&lt;/a&gt; (INSA Rennes, IETR), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Crussiere_M/0/1/0/all/0/1&quot;&gt;Matthieu Crussi&amp;#xe8;re&lt;/a&gt; (IETR), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Paquelet_S/0/1/0/all/0/1&quot;&gt;Stephane Paquelet&lt;/a&gt; (IRT b-com)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03766">
<title>Literature Based Discovery (LBD): Towards Hypothesis Generation and Knowledge Discovery in Biomedical Text Mining. (arXiv:2310.03766v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2310.03766</link>
<description rdf:parseType="Literal">&lt;p&gt;Biomedical knowledge is growing in an astounding pace with a majority of this
knowledge is represented as scientific publications. Text mining tools and
methods represents automatic approaches for extracting hidden patterns and
trends from this semi structured and unstructured data. In Biomedical Text
mining, Literature Based Discovery (LBD) is the process of automatically
discovering novel associations between medical terms otherwise mentioned in
disjoint literature sets. LBD approaches proven to be successfully reducing the
discovery time of potential associations that are hidden in the vast amount of
scientific literature. The process focuses on creating concept profiles for
medical terms such as a disease or symptom and connecting it with a drug and
treatment based on the statistical significance of the shared profiles. This
knowledge discovery approach introduced in 1989 still remains as a core task in
text mining. Currently the ABC principle based two approaches namely open
discovery and closed discovery are mostly explored in LBD process. This review
starts with general introduction about text mining followed by biomedical text
mining and introduces various literature resources such as MEDLINE, UMLS, MESH,
and SemMedDB. This is followed by brief introduction of the core ABC principle
and its associated two approaches open discovery and closed discovery in LBD
process. This review also discusses the deep learning applications in LBD by
reviewing the role of transformer models and neural networks based LBD models
and its future aspects. Finally, reviews the key biomedical discoveries
generated through LBD approaches in biomedicine and conclude with the current
limitations and future directions of LBD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhasuran_B/0/1/0/all/0/1&quot;&gt;Balu Bhasuran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murugesan_G/0/1/0/all/0/1&quot;&gt;Gurusamy Murugesan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Natarajan_J/0/1/0/all/0/1&quot;&gt;Jeyakumar Natarajan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03770">
<title>Progressive reduced order modeling: empowering data-driven modeling with selective knowledge transfer. (arXiv:2310.03770v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.03770</link>
<description rdf:parseType="Literal">&lt;p&gt;Data-driven modeling can suffer from a constant demand for data, leading to
reduced accuracy and impractical for engineering applications due to the high
cost and scarcity of information. To address this challenge, we propose a
progressive reduced order modeling framework that minimizes data cravings and
enhances data-driven modeling&apos;s practicality. Our approach selectively
transfers knowledge from previously trained models through gates, similar to
how humans selectively use valuable knowledge while ignoring unuseful
information. By filtering relevant information from previous models, we can
create a surrogate model with minimal turnaround time and a smaller training
set that can still achieve high accuracy. We have tested our framework in
several cases, including transport in porous media, gravity-driven flow, and
finite deformation in hyperelastic materials. Our results illustrate that
retaining information from previous models and utilizing a valuable portion of
that knowledge can significantly improve the accuracy of the current model. We
have demonstrated the importance of progressive knowledge transfer and its
impact on model accuracy with reduced training samples. For instance, our
framework with four parent models outperforms the no-parent counterpart trained
on data nine times larger. Our research unlocks data-driven modeling&apos;s
potential for practical engineering applications by mitigating the data
scarcity issue. Our proposed framework is a significant step toward more
efficient and cost-effective data-driven modeling, fostering advancements
across various fields.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kadeethum_T/0/1/0/all/0/1&quot;&gt;Teeratorn Kadeethum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OMalley_D/0/1/0/all/0/1&quot;&gt;Daniel O&amp;#x27;Malley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1&quot;&gt;Youngsoo Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Viswanathan_H/0/1/0/all/0/1&quot;&gt;Hari S. Viswanathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_H/0/1/0/all/0/1&quot;&gt;Hongkyu Yoon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03778">
<title>Lightweight Boosting Models for User Response Prediction Using Adversarial Validation. (arXiv:2310.03778v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.03778</link>
<description rdf:parseType="Literal">&lt;p&gt;The ACM RecSys Challenge 2023, organized by ShareChat, aims to predict the
probability of the app being installed. This paper describes the lightweight
solution to this challenge. We formulate the task as a user response prediction
task. For rapid prototyping for the task, we propose a lightweight solution
including the following steps: 1) using adversarial validation, we effectively
eliminate uninformative features from a dataset; 2) to address noisy continuous
features and categorical features with a large number of unique values, we
employ feature engineering techniques.; 3) we leverage Gradient Boosted
Decision Trees (GBDT) for their exceptional performance and scalability. The
experiments show that a single LightGBM model, without additional ensembling,
performs quite well. Our team achieved ninth place in the challenge with the
final leaderboard score of 6.059065. Code for our approach can be found here:
https://github.com/choco9966/recsys-challenge-2023.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyeonwoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1&quot;&gt;Wonsung Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03779">
<title>HandMeThat: Human-Robot Communication in Physical and Social Environments. (arXiv:2310.03779v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.03779</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce HandMeThat, a benchmark for a holistic evaluation of instruction
understanding and following in physical and social environments. While previous
datasets primarily focused on language grounding and planning, HandMeThat
considers the resolution of human instructions with ambiguities based on the
physical (object states and relations) and social (human actions and goals)
information. HandMeThat contains 10,000 episodes of human-robot interactions.
In each episode, the robot first observes a trajectory of human actions towards
her internal goal. Next, the robot receives a human instruction and should take
actions to accomplish the subgoal set through the instruction. In this paper,
we present a textual interface for our benchmark, where the robot interacts
with a virtual environment through textual commands. We evaluate several
baseline models on HandMeThat, and show that both offline and online
reinforcement learning algorithms perform poorly on HandMeThat, suggesting
significant room for future work on physical and social human-robot
communications and interactions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1&quot;&gt;Yanming Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1&quot;&gt;Jiayuan Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1&quot;&gt;Joshua B. Tenenbaum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03780">
<title>Automating Human Tutor-Style Programming Feedback: Leveraging GPT-4 Tutor Model for Hint Generation and GPT-3.5 Student Model for Hint Validation. (arXiv:2310.03780v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.03780</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative AI and large language models hold great promise in enhancing
programming education by automatically generating individualized feedback for
students. We investigate the role of generative AI models in providing human
tutor-style programming hints to help students resolve errors in their buggy
programs. Recent works have benchmarked state-of-the-art models for various
feedback generation scenarios; however, their overall quality is still inferior
to human tutors and not yet ready for real-world deployment. In this paper, we
seek to push the limits of generative AI models toward providing high-quality
programming hints and develop a novel technique, GPT4Hints-GPT3.5Val. As a
first step, our technique leverages GPT-4 as a ``tutor&apos;&apos; model to generate
hints -- it boosts the generative quality by using symbolic information of
failing test cases and fixes in prompts. As a next step, our technique
leverages GPT-3.5, a weaker model, as a ``student&apos;&apos; model to further validate
the hint quality -- it performs an automatic quality validation by simulating
the potential utility of providing this feedback. We show the efficacy of our
technique via extensive evaluation using three real-world datasets of Python
programs covering a variety of concepts ranging from basic algorithms to
regular expressions and data analysis using pandas library.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phung_T/0/1/0/all/0/1&quot;&gt;Tung Phung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Padurean_V/0/1/0/all/0/1&quot;&gt;Victor-Alexandru P&amp;#x103;durean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Anjali Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brooks_C/0/1/0/all/0/1&quot;&gt;Christopher Brooks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cambronero_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Cambronero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gulwani_S/0/1/0/all/0/1&quot;&gt;Sumit Gulwani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singla_A/0/1/0/all/0/1&quot;&gt;Adish Singla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soares_G/0/1/0/all/0/1&quot;&gt;Gustavo Soares&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03813">
<title>Accurate Cold-start Bundle Recommendation via Popularity-based Coalescence and Curriculum Heating. (arXiv:2310.03813v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2310.03813</link>
<description rdf:parseType="Literal">&lt;p&gt;How can we accurately recommend cold-start bundles to users? The cold-start
problem in bundle recommendation is critical in practical scenarios since new
bundles are continuously created for various marketing purposes. Despite its
importance, no previous studies have addressed cold-start bundle
recommendation. Moreover, existing methods for cold-start item recommendation
overly rely on historical information, even for unpopular bundles, failing to
tackle the primary challenge of the highly skewed distribution of bundle
interactions. In this work, we propose CoHeat (Popularity-based Coalescence and
Curriculum Heating), an accurate approach for the cold-start bundle
recommendation. CoHeat tackles the highly skewed distribution of bundle
interactions by incorporating both historical and affiliation information based
on the bundle&apos;s popularity when estimating the user-bundle relationship.
Furthermore, CoHeat effectively learns latent representations by exploiting
curriculum learning and contrastive learning. CoHeat demonstrates superior
performance in cold-start bundle recommendation, achieving up to 193% higher
nDCG@20 compared to the best competitor.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeon_H/0/1/0/all/0/1&quot;&gt;Hyunsik Jeon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jong-eun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_J/0/1/0/all/0/1&quot;&gt;Jeongin Yun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_U/0/1/0/all/0/1&quot;&gt;U Kang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03823">
<title>ECAvg: An Edge-Cloud Collaborative Learning Approach using Averaged Weights. (arXiv:2310.03823v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.03823</link>
<description rdf:parseType="Literal">&lt;p&gt;The use of edge devices together with cloud provides a collaborative
relationship between both classes of devices where one complements the
shortcomings of the other. Resource-constraint edge devices can benefit from
the abundant computing power provided by servers by offloading computationally
intensive tasks to the server. Meanwhile, edge devices can leverage their close
proximity to the data source to perform less computationally intensive tasks on
the data. In this paper, we propose a collaborative edge-cloud paradigm called
ECAvg in which edge devices pre-train local models on their respective datasets
and transfer the models to the server for fine-tuning. The server averages the
pre-trained weights into a global model, which is fine-tuned on the combined
data from the various edge devices. The local (edge) models are then updated
with the weights of the global (server) model. We implement a CIFAR-10
classification task using MobileNetV2, a CIFAR-100 classification task using
ResNet50, and an MNIST classification using a neural network with a single
hidden layer. We observed performance improvement in the CIFAR-10 and CIFAR-100
classification tasks using our approach, where performance improved on the
server model with averaged weights and the edge models had a better performance
after model update. On the MNIST classification, averaging weights resulted in
a drop in performance on both the server and edge models due to negative
transfer learning. From the experiment results, we conclude that our approach
is successful when implemented on deep neural networks such as MobileNetV2 and
ResNet50 instead of simple neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mih_A/0/1/0/all/0/1&quot;&gt;Atah Nuh Mih&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1&quot;&gt;Hung Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawnine_A/0/1/0/all/0/1&quot;&gt;Asfia Kawnine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wachowicz_M/0/1/0/all/0/1&quot;&gt;Monica Wachowicz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03840">
<title>Contextualized Structural Self-supervised Learning for Ontology Matching. (arXiv:2310.03840v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.03840</link>
<description rdf:parseType="Literal">&lt;p&gt;Ontology matching (OM) entails the identification of semantic relationships
between concepts within two or more knowledge graphs (KGs) and serves as a
critical step in integrating KGs from various sources. Recent advancements in
deep OM models have harnessed the power of transformer-based language models
and the advantages of knowledge graph embedding. Nevertheless, these OM models
still face persistent challenges, such as a lack of reference alignments,
runtime latency, and unexplored different graph structures within an end-to-end
framework. In this study, we introduce a novel self-supervised learning OM
framework with input ontologies, called LaKERMap. This framework capitalizes on
the contextual and structural information of concepts by integrating implicit
knowledge into transformers. Specifically, we aim to capture multiple
structural contexts, encompassing both local and global interactions, by
employing distinct training objectives. To assess our methods, we utilize the
Bio-ML datasets and tasks. The findings from our innovative approach reveal
that LaKERMap surpasses state-of-the-art systems in terms of alignment quality
and inference time. Our models and codes are available here:
https://github.com/ellenzhuwang/lakermap.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhu Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03882">
<title>Small batch deep reinforcement learning. (arXiv:2310.03882v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.03882</link>
<description rdf:parseType="Literal">&lt;p&gt;In value-based deep reinforcement learning with replay memories, the batch
size parameter specifies how many transitions to sample for each gradient
update. Although critical to the learning process, this value is typically not
adjusted when proposing new algorithms. In this work we present a broad
empirical study that suggests {\em reducing} the batch size can result in a
number of significant performance gains; this is surprising, as the general
tendency when training neural networks is towards larger batch sizes for
improved performance. We complement our experimental findings with a set of
empirical analyses towards better understanding this phenomenon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Obando_Ceron_J/0/1/0/all/0/1&quot;&gt;Johan Obando-Ceron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellemare_M/0/1/0/all/0/1&quot;&gt;Marc G. Bellemare&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castro_P/0/1/0/all/0/1&quot;&gt;Pablo Samuel Castro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03890">
<title>Accelerated Neural Network Training with Rooted Logistic Objectives. (arXiv:2310.03890v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.03890</link>
<description rdf:parseType="Literal">&lt;p&gt;Many neural networks deployed in the real world scenarios are trained using
cross entropy based loss functions. From the optimization perspective, it is
known that the behavior of first order methods such as gradient descent
crucially depend on the separability of datasets. In fact, even in the most
simplest case of binary classification, the rate of convergence depends on two
factors: (1) condition number of data matrix, and (2) separability of the
dataset. With no further pre-processing techniques such as
over-parametrization, data augmentation etc., separability is an intrinsic
quantity of the data distribution under consideration. We focus on the
landscape design of the logistic function and derive a novel sequence of {\em
strictly} convex functions that are at least as strict as logistic loss. The
minimizers of these functions coincide with those of the minimum norm solution
wherever possible. The strict convexity of the derived function can be extended
to finetune state-of-the-art models and applications. In empirical experimental
analysis, we apply our proposed rooted logistic objective to multiple deep
models, e.g., fully-connected neural networks and transformers, on various of
classification benchmarks. Our results illustrate that training with rooted
loss function is converged faster and gains performance improvements.
Furthermore, we illustrate applications of our novel rooted loss function in
generative modeling based downstream applications, such as finetuning StyleGAN
model with the rooted loss. The code implementing our losses and models can be
found here for open source software development purposes:
https://anonymous.4open.science/r/rooted_loss.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veluswami_P/0/1/0/all/0/1&quot;&gt;Praveen Raj Veluswami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_H/0/1/0/all/0/1&quot;&gt;Harsh Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1&quot;&gt;Sathya N. Ravi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03912">
<title>RTDK-BO: High Dimensional Bayesian Optimization with Reinforced Transformer Deep kernels. (arXiv:2310.03912v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.03912</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian Optimization (BO), guided by Gaussian process (GP) surrogates, has
proven to be an invaluable technique for efficient, high-dimensional, black-box
optimization, a critical problem inherent to many applications such as
industrial design and scientific computing. Recent contributions have
introduced reinforcement learning (RL) to improve the optimization performance
on both single function optimization and \textit{few-shot} multi-objective
optimization. However, even few-shot techniques fail to exploit similarities
shared between closely related objectives. In this paper, we combine recent
developments in Deep Kernel Learning (DKL) and attention-based Transformer
models to improve the modeling powers of GP surrogates with meta-learning. We
propose a novel method for improving meta-learning BO surrogates by
incorporating attention mechanisms into DKL, empowering the surrogates to adapt
to contextual information gathered during the BO process. We combine this
Transformer Deep Kernel with a learned acquisition function trained with
continuous Soft Actor-Critic Reinforcement Learning to aid in exploration. This
Reinforced Transformer Deep Kernel (RTDK-BO) approach yields state-of-the-art
results in continuous high-dimensional optimization problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shmakov_A/0/1/0/all/0/1&quot;&gt;Alexander Shmakov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naug_A/0/1/0/all/0/1&quot;&gt;Avisek Naug&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gundecha_V/0/1/0/all/0/1&quot;&gt;Vineet Gundecha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghorbanpour_S/0/1/0/all/0/1&quot;&gt;Sahand Ghorbanpour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gutierrez_R/0/1/0/all/0/1&quot;&gt;Ricardo Luna Gutierrez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babu_A/0/1/0/all/0/1&quot;&gt;Ashwin Ramesh Babu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guillen_A/0/1/0/all/0/1&quot;&gt;Antonio Guillen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1&quot;&gt;Soumyendu Sarkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03916">
<title>Toward a Foundation Model for Time Series Data. (arXiv:2310.03916v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.03916</link>
<description rdf:parseType="Literal">&lt;p&gt;A foundation model is a machine learning model trained on a large and diverse
set of data, typically using self-supervised learning-based pre-training
techniques, that can be adapted to various downstream tasks. However, current
research on time series pre-training has mostly focused on models pre-trained
solely on data from a single domain, resulting in a lack of knowledge about
other types of time series. However, current research on time series
pre-training has predominantly focused on models trained exclusively on data
from a single domain. As a result, these models possess domain-specific
knowledge that may not be easily transferable to time series from other
domains. In this paper, we aim to develop an effective time series foundation
model by leveraging unlabeled samples from multiple domains. To achieve this,
we repurposed the publicly available UCR Archive and evaluated four existing
self-supervised learning-based pre-training methods, along with a novel method,
on the datasets. We tested these methods using four popular neural network
architectures for time series to understand how the pre-training methods
interact with different network designs. Our experimental results show that
pre-training improves downstream classification tasks by enhancing the
convergence of the fine-tuning process. Furthermore, we found that the proposed
pre-training method, when combined with the Transformer model, outperforms the
alternatives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1&quot;&gt;Chin-Chia Michael Yeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1&quot;&gt;Xin Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huiyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yujie Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Der_A/0/1/0/all/0/1&quot;&gt;Audrey Der&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_V/0/1/0/all/0/1&quot;&gt;Vivian Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1&quot;&gt;Zhongfang Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junpeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03919">
<title>An Efficient Content-based Time Series Retrieval System. (arXiv:2310.03919v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2310.03919</link>
<description rdf:parseType="Literal">&lt;p&gt;A Content-based Time Series Retrieval (CTSR) system is an information
retrieval system for users to interact with time series emerged from multiple
domains, such as finance, healthcare, and manufacturing. For example, users
seeking to learn more about the source of a time series can submit the time
series as a query to the CTSR system and retrieve a list of relevant time
series with associated metadata. By analyzing the retrieved metadata, users can
gather more information about the source of the time series. Because the CTSR
system is required to work with time series data from diverse domains, it needs
a high-capacity model to effectively measure the similarity between different
time series. On top of that, the model within the CTSR system has to compute
the similarity scores in an efficient manner as the users interact with the
system in real-time. In this paper, we propose an effective and efficient CTSR
model that outperforms alternative models, while still providing reasonable
inference runtimes. To demonstrate the capability of the proposed method in
solving business problems, we compare it against alternative models using our
in-house transaction data. Our findings reveal that the proposed model is the
most suitable solution compared to others for our transaction data problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1&quot;&gt;Chin-Chia Michael Yeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huiyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1&quot;&gt;Xin Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junpeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_V/0/1/0/all/0/1&quot;&gt;Vivian Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yujie Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Der_A/0/1/0/all/0/1&quot;&gt;Audrey Der&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1&quot;&gt;Zhongfang Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phillips_J/0/1/0/all/0/1&quot;&gt;Jeff M. Phillips&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03925">
<title>Multitask Learning for Time Series Data\\with 2D Convolution. (arXiv:2310.03925v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.03925</link>
<description rdf:parseType="Literal">&lt;p&gt;Multitask learning (MTL) aims to develop a unified model that can handle a
set of closely related tasks simultaneously. By optimizing the model across
multiple tasks, MTL generally surpasses its non-MTL counterparts in terms of
generalizability. Although MTL has been extensively researched in various
domains such as computer vision, natural language processing, and
recommendation systems, its application to time series data has received
limited attention. In this paper, we investigate the application of MTL to the
time series classification (TSC) problem. However, when we integrate the
state-of-the-art 1D convolution-based TSC model with MTL, the performance of
the TSC model actually deteriorates. By comparing the 1D convolution-based
models with the Dynamic Time Warping (DTW) distance function, it appears that
the underwhelming results stem from the limited expressive power of the 1D
convolutional layers. To overcome this challenge, we propose a novel design for
a 2D convolution-based model that enhances the model&apos;s expressiveness.
Leveraging this advantage, our proposed method outperforms competing approaches
on both the UCR Archive and an industrial transaction TSC dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1&quot;&gt;Chin-Chia Michael Yeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1&quot;&gt;Xin Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junpeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huiyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yujie Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Der_A/0/1/0/all/0/1&quot;&gt;Audrey Der&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1&quot;&gt;Zhongfang Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03940">
<title>Hard View Selection for Contrastive Learning. (arXiv:2310.03940v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.03940</link>
<description rdf:parseType="Literal">&lt;p&gt;Many Contrastive Learning (CL) methods train their models to be invariant to
different &quot;views&quot; of an image input for which a good data augmentation pipeline
is crucial. While considerable efforts were directed towards improving pre-text
tasks, architectures, or robustness (e.g., Siamese networks or teacher-softmax
centering), the majority of these methods remain strongly reliant on the random
sampling of operations within the image augmentation pipeline, such as the
random resized crop or color distortion operation. In this paper, we argue that
the role of the view generation and its effect on performance has so far
received insufficient attention. To address this, we propose an easy,
learning-free, yet powerful Hard View Selection (HVS) strategy designed to
extend the random view generation to expose the pretrained model to harder
samples during CL training. It encompasses the following iterative steps: 1)
randomly sample multiple views and create pairs of two views, 2) run forward
passes for each view pair on the currently trained model, 3) adversarially
select the pair yielding the worst loss, and 4) run the backward pass with the
selected pair. In our empirical analysis we show that under the hood, HVS
increases task difficulty by controlling the Intersection over Union of views
during pretraining. With only 300-epoch pretraining, HVS is able to closely
rival the 800-epoch DINO baseline which remains very favorable even when
factoring in the slowdown induced by the additional forwards of HVS.
Additionally, HVS consistently achieves accuracy improvements on ImageNet
between 0.55% and 1.9% on linear evaluation and similar improvements on
transfer tasks across multiple CL methods, such as DINO, SimSiam, and SimCLR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferreira_F/0/1/0/all/0/1&quot;&gt;Fabio Ferreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rapant_I/0/1/0/all/0/1&quot;&gt;Ivo Rapant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1&quot;&gt;Frank Hutter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03951">
<title>Chain of Natural Language Inference for Reducing Large Language Model Ungrounded Hallucinations. (arXiv:2310.03951v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.03951</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) can generate fluent natural language texts when
given relevant documents as background context. This ability has attracted
considerable interest in developing industry applications of LLMs. However,
LLMs are prone to generate hallucinations that are not supported by the
provided sources. In this paper, we propose a hierarchical framework to detect
and mitigate such ungrounded hallucination. Our framework uses Chain of Natural
Language Inference (CoNLI) for hallucination detection and hallucination
reduction via post-editing. Our approach achieves state-of-the-art performance
on hallucination detection and enhances text quality through rewrite, using
LLMs without any fine-tuning or domain-specific prompt engineering. We show
that this simple plug-and-play framework can serve as an effective choice for
hallucination detection and reduction, achieving competitive performance across
various contexts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_D/0/1/0/all/0/1&quot;&gt;Deren Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yaxi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mengya/0/1/0/all/0/1&quot;&gt;Mengya&lt;/a&gt; (Mia)Hu, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mingyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_V/0/1/0/all/0/1&quot;&gt;Vincent Yun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ching_E/0/1/0/all/0/1&quot;&gt;Emily Ching&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamal_E/0/1/0/all/0/1&quot;&gt;Eslam Kamal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03964">
<title>A Learnable Counter-condition Analysis Framework for Functional Connectivity-based Neurological Disorder Diagnosis. (arXiv:2310.03964v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.03964</link>
<description rdf:parseType="Literal">&lt;p&gt;To understand the biological characteristics of neurological disorders with
functional connectivity (FC), recent studies have widely utilized deep
learning-based models to identify the disease and conducted post-hoc analyses
via explainable models to discover disease-related biomarkers. Most existing
frameworks consist of three stages, namely, feature selection, feature
extraction for classification, and analysis, where each stage is implemented
separately. However, if the results at each stage lack reliability, it can
cause misdiagnosis and incorrect analysis in afterward stages. In this study,
we propose a novel unified framework that systemically integrates diagnoses
(i.e., feature selection and feature extraction) and explanations. Notably, we
devised an adaptive attention network as a feature selection approach to
identify individual-specific disease-related connections. We also propose a
functional network relational encoder that summarizes the global topological
properties of FC by learning the inter-network relations without pre-defined
edges between functional networks. Last but not least, our framework provides a
novel explanatory power for neuroscientific interpretation, also termed
counter-condition analysis. We simulated the FC that reverses the diagnostic
information (i.e., counter-condition FC): converting a normal brain to be
abnormal and vice versa. We validated the effectiveness of our framework by
using two large resting-state functional magnetic resonance imaging (fMRI)
datasets, Autism Brain Imaging Data Exchange (ABIDE) and REST-meta-MDD, and
demonstrated that our framework outperforms other competing methods for disease
identification. Furthermore, we analyzed the disease-related neurological
patterns based on counter-condition analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_E/0/1/0/all/0/1&quot;&gt;Eunsong Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heo_D/0/1/0/all/0/1&quot;&gt;Da-woon Heo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jiwon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suk_H/0/1/0/all/0/1&quot;&gt;Heung-Il Suk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03965">
<title>Thought Propagation: An Analogical Approach to Complex Reasoning with Large Language Models. (arXiv:2310.03965v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.03965</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have achieved remarkable success in reasoning
tasks with the development of prompting methods. However, existing prompting
approaches cannot reuse insights of solving similar problems and suffer from
accumulated errors in multi-step reasoning, since they prompt LLMs to reason
\textit{from scratch}. To address these issues, we propose
\textbf{\textit{Thought Propagation} (TP)}, which explores the analogous
problems and leverages their solutions to enhance the complex reasoning ability
of LLMs. These analogous problems are related to the input one, with reusable
solutions and problem-solving strategies. Thus, it is promising to propagate
insights of solving previous analogous problems to inspire new problem-solving.
To achieve this, TP first prompts LLMs to propose and solve a set of analogous
problems that are related to the input one. Then, TP reuses the results of
analogous problems to directly yield a new solution or derive a
knowledge-intensive plan for execution to amend the initial solution obtained
from scratch. TP is compatible with existing prompting approaches, allowing
plug-and-play generalization and enhancement in a wide range of tasks without
much labor in task-specific prompt engineering. Experiments across three
challenging tasks demonstrate TP enjoys a substantial improvement over the
baselines by an average of 12\% absolute increase in finding the optimal
solutions in Shortest-path Reasoning, 13\% improvement of human preference in
Creative Writing, and 15\% enhancement in the task completion rate of LLM-Agent
Planning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Junchi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1&quot;&gt;Ran He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ying_R/0/1/0/all/0/1&quot;&gt;Rex Ying&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03967">
<title>Sub-token ViT Embedding via Stochastic Resonance Transformers. (arXiv:2310.03967v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.03967</link>
<description rdf:parseType="Literal">&lt;p&gt;We discover the presence of quantization artifacts in Vision Transformers
(ViTs), which arise due to the image tokenization step inherent in these
architectures. These artifacts result in coarsely quantized features, which
negatively impact performance, especially on downstream dense prediction tasks.
We present a zero-shot method to improve how pre-trained ViTs handle spatial
quantization. In particular, we propose to ensemble the features obtained from
perturbing input images via sub-token spatial translations, inspired by
Stochastic Resonance, a method traditionally applied to climate dynamics and
signal processing. We term our method ``Stochastic Resonance Transformer&quot;
(SRT), which we show can effectively super-resolve features of pre-trained
ViTs, capturing more of the local fine-grained structures that might otherwise
be neglected as a result of tokenization. SRT can be applied at any layer, on
any task, and does not require any fine-tuning. The advantage of the former is
evident when applied to monocular depth prediction, where we show that
ensembling model outputs are detrimental while applying SRT on intermediate ViT
features outperforms the baseline models by an average of 4.7% and 14.9% on the
RMSE and RMSE-log metrics across three different architectures. When applied to
semi-supervised video object segmentation, SRT also improves over the baseline
models uniformly across all metrics, and by an average of 2.4% in F&amp;amp;J score. We
further show that these quantization artifacts can be attenuated to some extent
via self-distillation. On the unsupervised salient region segmentation, SRT
improves upon the base model by an average of 2.1% on the maxF metric. Finally,
despite operating purely on pixel-level features, SRT generalizes to non-dense
prediction tasks such as image retrieval and object discovery, yielding
consistent improvements of up to 2.6% and 1.0% respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lao_D/0/1/0/all/0/1&quot;&gt;Dong Lao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yangchao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tian Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1&quot;&gt;Alex Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1&quot;&gt;Stefano Soatto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03977">
<title>Perfect Alignment May be Poisonous to Graph Contrastive Learning. (arXiv:2310.03977v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.03977</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Contrastive Learning (GCL) aims to learn node representations by
aligning positive pairs and separating negative ones. However, limited research
has been conducted on the inner law behind specific augmentations used in
graph-based learning. What kind of augmentation will help downstream
performance, how does contrastive learning actually influence downstream tasks,
and why the magnitude of augmentation matters? This paper seeks to address
these questions by establishing a connection between augmentation and
downstream performance, as well as by investigating the generalization of
contrastive learning. Our findings reveal that GCL contributes to downstream
tasks mainly by separating different classes rather than gathering nodes of the
same class. So perfect alignment and augmentation overlap which draw all
intra-class samples the same can not explain the success of contrastive
learning. Then in order to comprehend how augmentation aids the contrastive
learning process, we conduct further investigations into its generalization,
finding that perfect alignment that draw positive pair the same could help
contrastive loss but is poisonous to generalization, on the contrary, imperfect
alignment enhances the model&apos;s generalization ability. We analyse the result by
information theory and graph spectrum theory respectively, and propose two
simple but effective methods to verify the theories. The two methods could be
easily applied to various GCL algorithms and extensive experiments are
conducted to prove its effectiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jingyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Huayi Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03981">
<title>CUPre: Cross-domain Unsupervised Pre-training for Few-Shot Cell Segmentation. (arXiv:2310.03981v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.03981</link>
<description rdf:parseType="Literal">&lt;p&gt;While pre-training on object detection tasks, such as Common Objects in
Contexts (COCO) [1], could significantly boost the performance of cell
segmentation, it still consumes on massive fine-annotated cell images [2] with
bounding boxes, masks, and cell types for every cell in every image, to
fine-tune the pre-trained model. To lower the cost of annotation, this work
considers the problem of pre-training DNN models for few-shot cell
segmentation, where massive unlabeled cell images are available but only a
small proportion is annotated. Hereby, we propose Cross-domain Unsupervised
Pre-training, namely CUPre, transferring the capability of object detection and
instance segmentation for common visual objects (learned from COCO) to the
visual domain of cells using unlabeled images. Given a standard COCO
pre-trained network with backbone, neck, and head modules, CUPre adopts an
alternate multi-task pre-training (AMT2) procedure with two sub-tasks -- in
every iteration of pre-training, AMT2 first trains the backbone with cell
images from multiple cell datasets via unsupervised momentum contrastive
learning (MoCo) [3], and then trains the whole model with vanilla COCO datasets
via instance segmentation. After pre-training, CUPre fine-tunes the whole model
on the cell segmentation task using a few annotated images. We carry out
extensive experiments to evaluate CUPre using LIVECell [2] and BBBC038 [4]
datasets in few-shot instance segmentation settings. The experiment shows that
CUPre can outperform existing pre-training methods, achieving the highest
average precision (AP) for few-shot cell segmentation and detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1&quot;&gt;Weibin Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuhong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qingzhong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yanwu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Zhaozheng Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Haoyi Xiong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04010">
<title>Excision and Recovery: Enhancing Surface Anomaly Detection with Attention-based Single Deterministic Masking. (arXiv:2310.04010v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.04010</link>
<description rdf:parseType="Literal">&lt;p&gt;Anomaly detection (AD) in surface inspection is an essential yet challenging
task in manufacturing due to the quantity imbalance problem of scarce abnormal
data. To overcome the above, a reconstruction encoder-decoder (ED) such as
autoencoder or U-Net which is trained with only anomaly-free samples is widely
adopted, in the hope that unseen abnormals should yield a larger reconstruction
error than normal. Over the past years, researches on self-supervised
reconstruction-by-inpainting have been reported. They mask out suspected
defective regions for inpainting in order to make them invisible to the
reconstruction ED to deliberately cause inaccurate reconstruction for
abnormals. However, their limitation is multiple random masking to cover the
whole input image due to defective regions not being known in advance. We
propose a novel reconstruction-by-inpainting method dubbed Excision and
Recovery (EAR) that features single deterministic masking. For this, we exploit
a pre-trained spatial attention model to predict potential suspected defective
regions that should be masked out. We also employ a variant of U-Net as our ED
to further limit the reconstruction ability of the U-Net model for abnormals,
in which skip connections of different layers can be selectively disabled. In
the training phase, all the skip connections are switched on to fully take the
benefits from the U-Net architecture. In contrast, for inferencing, we only
keep deeper skip connections with shallower connections off. We validate the
effectiveness of EAR using an MNIST pre-trained attention for a commonly used
surface AD dataset, KolektorSDD2. The experimental results show that EAR
achieves both better AD performance and higher throughput than state-of-the-art
methods. We expect that the proposed EAR model can be widely adopted as
training and inference strategies for AD purposes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1&quot;&gt;YeongHyeon Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1&quot;&gt;Sungho Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Myung Jin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yeonho Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1&quot;&gt;Juneho Yi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04041">
<title>Observation-Guided Diffusion Probabilistic Models. (arXiv:2310.04041v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.04041</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel diffusion model called observation-guided diffusion
probabilistic model (OGDM), which effectively addresses the trade-off between
quality control and fast sampling. Our approach reestablishes the training
objective by integrating the guidance of the observation process with the
Markov chain in a principled way. This is achieved by introducing an additional
loss term derived from the observation based on the conditional discriminator
on noise level, which employs Bernoulli distribution indicating whether its
input lies on the (noisy) real manifold or not. This strategy allows us to
optimize the more accurate negative log-likelihood induced in the inference
stage especially when the number of function evaluations is limited. The
proposed training method is also advantageous even when incorporated only into
the fine-tuning process, and it is compatible with various fast inference
strategies since our method yields better denoising networks using the exactly
same inference procedure without incurring extra computational cost. We
demonstrate the effectiveness of the proposed training algorithm using diverse
inference methods on strong diffusion model baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1&quot;&gt;Junoh Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jinyoung Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1&quot;&gt;Sungik Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1&quot;&gt;Bohyung Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04055">
<title>Kick Bad Guys Out! Zero-Knowledge-Proof-Based Anomaly Detection in Federated Learning. (arXiv:2310.04055v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2310.04055</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) systems are vulnerable to malicious clients that
submit poisoned local models to achieve their adversarial goals, such as
preventing the convergence of the global model or inducing the global model to
misclassify some data. Many existing defense mechanisms are impractical in
real-world FL systems, as they require prior knowledge of the number of
malicious clients or rely on re-weighting or modifying submissions. This is
because adversaries typically do not announce their intentions before
attacking, and re-weighting might change aggregation results even in the
absence of attacks. To address these challenges in real FL systems, this paper
introduces a cutting-edge anomaly detection approach with the following
features: i) Detecting the occurrence of attacks and performing defense
operations only when attacks happen; ii) Upon the occurrence of an attack,
further detecting the malicious client models and eliminating them without
harming the benign ones; iii) Ensuring honest execution of defense mechanisms
at the server by leveraging a zero-knowledge proof mechanism. We validate the
superior performance of the proposed approach with extensive experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1&quot;&gt;Shanshan Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wenxuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buyukates_B/0/1/0/all/0/1&quot;&gt;Baturalp Buyukates&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1&quot;&gt;Weizhao Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yuhang Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qifan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1&quot;&gt;Salman Avestimehr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1&quot;&gt;Chaoyang He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04072">
<title>AI Regulation in Europe: From the AI Act to Future Regulatory Challenges. (arXiv:2310.04072v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2310.04072</link>
<description rdf:parseType="Literal">&lt;p&gt;This chapter provides a comprehensive discussion on AI regulation in the
European Union, contrasting it with the more sectoral and self-regulatory
approach in the UK. It argues for a hybrid regulatory strategy that combines
elements from both philosophies, emphasizing the need for agility and safe
harbors to ease compliance. The paper examines the AI Act as a pioneering
legislative effort to address the multifaceted challenges posed by AI,
asserting that, while the Act is a step in the right direction, it has
shortcomings that could hinder the advancement of AI technologies. The paper
also anticipates upcoming regulatory challenges, such as the management of
toxic content, environmental concerns, and hybrid threats. It advocates for
immediate action to create protocols for regulated access to high-performance,
potentially open-source AI systems. Although the AI Act is a significant
legislative milestone, it needs additional refinement and global collaboration
for the effective governance of rapidly evolving AI technologies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hacker_P/0/1/0/all/0/1&quot;&gt;Philipp Hacker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04074">
<title>Automatic Aspect Extraction from Scientific Texts. (arXiv:2310.04074v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.04074</link>
<description rdf:parseType="Literal">&lt;p&gt;Being able to extract from scientific papers their main points, key insights,
and other important information, referred to here as aspects, might facilitate
the process of conducting a scientific literature review. Therefore, the aim of
our research is to create a tool for automatic aspect extraction from
Russian-language scientific texts of any domain. In this paper, we present a
cross-domain dataset of scientific texts in Russian, annotated with such
aspects as Task, Contribution, Method, and Conclusion, as well as a baseline
algorithm for aspect extraction, based on the multilingual BERT model
fine-tuned on our data. We show that there are some differences in aspect
representation in different domains, but even though our model was trained on a
limited number of scientific domains, it is still able to generalize to new
domains, as was proved by cross-domain experiments. The code and the dataset
are available at
\url{https://github.com/anna-marshalova/automatic-aspect-extraction-from-scientific-texts}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marshalova_A/0/1/0/all/0/1&quot;&gt;Anna Marshalova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bruches_E/0/1/0/all/0/1&quot;&gt;Elena Bruches&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Batura_T/0/1/0/all/0/1&quot;&gt;Tatiana Batura&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04081">
<title>A Deeply Supervised Semantic Segmentation Method Based on GAN. (arXiv:2310.04081v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.04081</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the field of intelligent transportation has witnessed rapid
advancements, driven by the increasing demand for automation and efficiency in
transportation systems. Traffic safety, one of the tasks integral to
intelligent transport systems, requires accurately identifying and locating
various road elements, such as road cracks, lanes, and traffic signs. Semantic
segmentation plays a pivotal role in achieving this task, as it enables the
partition of images into meaningful regions with accurate boundaries. In this
study, we propose an improved semantic segmentation model that combines the
strengths of adversarial learning with state-of-the-art semantic segmentation
techniques. The proposed model integrates a generative adversarial network
(GAN) framework into the traditional semantic segmentation model, enhancing the
model&apos;s performance in capturing complex and subtle features in transportation
images. The effectiveness of our approach is demonstrated by a significant
boost in performance on the road crack dataset compared to the existing
methods, \textit{i.e.,} SEGAN. This improvement can be attributed to the
synergistic effect of adversarial learning and semantic segmentation, which
leads to a more refined and accurate representation of road structures and
conditions. The enhanced model not only contributes to better detection of road
cracks but also to a wide range of applications in intelligent transportation,
such as traffic sign recognition, vehicle detection, and lane segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wei Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Q/0/1/0/all/0/1&quot;&gt;Qiyu Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1&quot;&gt;Zeng Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04102">
<title>Nash Welfare and Facility Location. (arXiv:2310.04102v1 [cs.GT])</title>
<link>http://arxiv.org/abs/2310.04102</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of locating a facility to serve a set of agents
located along a line. The Nash welfare objective function, defined as the
product of the agents&apos; utilities, is known to provide a compromise between
fairness and efficiency in resource allocation problems. We apply this welfare
notion to the facility location problem, converting individual costs to
utilities and analyzing the facility placement that maximizes the Nash welfare.
We give a polynomial-time approximation algorithm to compute this facility
location, and prove results suggesting that it achieves a good balance of
fairness and efficiency. Finally, we take a mechanism design perspective and
propose a strategy-proof mechanism with a bounded approximation ratio for Nash
welfare.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_A/0/1/0/all/0/1&quot;&gt;Alexander Lam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aziz_H/0/1/0/all/0/1&quot;&gt;Haris Aziz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walsh_T/0/1/0/all/0/1&quot;&gt;Toby Walsh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04128">
<title>Reinforcement Learning with Fast and Forgetful Memory. (arXiv:2310.04128v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.04128</link>
<description rdf:parseType="Literal">&lt;p&gt;Nearly all real world tasks are inherently partially observable,
necessitating the use of memory in Reinforcement Learning (RL). Most model-free
approaches summarize the trajectory into a latent Markov state using memory
models borrowed from Supervised Learning (SL), even though RL tends to exhibit
different training and efficiency characteristics. Addressing this discrepancy,
we introduce Fast and Forgetful Memory, an algorithm-agnostic memory model
designed specifically for RL. Our approach constrains the model search space
via strong structural priors inspired by computational psychology. It is a
drop-in replacement for recurrent neural networks (RNNs) in recurrent RL
algorithms, achieving greater reward than RNNs across various recurrent
benchmarks and algorithms without changing any hyperparameters. Moreover, Fast
and Forgetful Memory exhibits training speeds two orders of magnitude faster
than RNNs, attributed to its logarithmic time and linear space complexity. Our
implementation is available at https://github.com/proroklab/ffm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morad_S/0/1/0/all/0/1&quot;&gt;Steven Morad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kortvelesy_R/0/1/0/all/0/1&quot;&gt;Ryan Kortvelesy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liwicki_S/0/1/0/all/0/1&quot;&gt;Stephan Liwicki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prorok_A/0/1/0/all/0/1&quot;&gt;Amanda Prorok&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04148">
<title>Self-Supervised Neuron Segmentation with Multi-Agent Reinforcement Learning. (arXiv:2310.04148v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.04148</link>
<description rdf:parseType="Literal">&lt;p&gt;The performance of existing supervised neuron segmentation methods is highly
dependent on the number of accurate annotations, especially when applied to
large scale electron microscopy (EM) data. By extracting semantic information
from unlabeled data, self-supervised methods can improve the performance of
downstream tasks, among which the mask image model (MIM) has been widely used
due to its simplicity and effectiveness in recovering original information from
masked images. However, due to the high degree of structural locality in EM
images, as well as the existence of considerable noise, many voxels contain
little discriminative information, making MIM pretraining inefficient on the
neuron segmentation task. To overcome this challenge, we propose a
decision-based MIM that utilizes reinforcement learning (RL) to automatically
search for optimal image masking ratio and masking strategy. Due to the vast
exploration space, using single-agent RL for voxel prediction is impractical.
Therefore, we treat each input patch as an agent with a shared behavior policy,
allowing for multi-agent collaboration. Furthermore, this multi-agent model can
capture dependencies between voxels, which is beneficial for the downstream
segmentation task. Experiments conducted on representative EM datasets
demonstrate that our approach has a significant advantage over alternative
self-supervised methods on the task of neuron segmentation. Code is available
at \url{https://github.com/ydchen0806/dbMiM}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yinda Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shenglong Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Xiong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04171">
<title>Dynamic Relation-Attentive Graph Neural Networks for Fraud Detection. (arXiv:2310.04171v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.04171</link>
<description rdf:parseType="Literal">&lt;p&gt;Fraud detection aims to discover fraudsters deceiving other users by, for
example, leaving fake reviews or making abnormal transactions. Graph-based
fraud detection methods consider this task as a classification problem with two
classes: frauds or normal. We address this problem using Graph Neural Networks
(GNNs) by proposing a dynamic relation-attentive aggregation mechanism. Based
on the observation that many real-world graphs include different types of
relations, we propose to learn a node representation per relation and aggregate
the node representations using a learnable attention function that assigns a
different attention coefficient to each relation. Furthermore, we combine the
node representations from different layers to consider both the local and
global structures of a target node, which is beneficial to improving the
performance of fraud detection on graphs with heterophily. By employing dynamic
graph attention in all the aggregation processes, our method adaptively
computes the attention coefficients for each node. Experimental results show
that our method, DRAG, outperforms state-of-the-art fraud detection methods on
real-world benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Heehyeon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jinhyeok Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whang_J/0/1/0/all/0/1&quot;&gt;Joyce Jiyoung Whang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04178">
<title>Introducing the Attribution Stability Indicator: a Measure for Time Series XAI Attributions. (arXiv:2310.04178v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.04178</link>
<description rdf:parseType="Literal">&lt;p&gt;Given the increasing amount and general complexity of time series data in
domains such as finance, weather forecasting, and healthcare, there is a
growing need for state-of-the-art performance models that can provide
interpretable insights into underlying patterns and relationships. Attribution
techniques enable the extraction of explanations from time series models to
gain insights but are hard to evaluate for their robustness and
trustworthiness. We propose the Attribution Stability Indicator (ASI), a
measure to incorporate robustness and trustworthiness as properties of
attribution techniques for time series into account. We extend a perturbation
analysis with correlations of the original time series to the perturbed
instance and the attributions to include wanted properties in the measure. We
demonstrate the wanted properties based on an analysis of the attributions in a
dimension-reduced space and the ASI scores distribution over three whole time
series classification datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlegel_U/0/1/0/all/0/1&quot;&gt;Udo Schlegel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keim_D/0/1/0/all/0/1&quot;&gt;Daniel A. Keim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04205">
<title>Keyword Augmented Retrieval: Novel framework for Information Retrieval integrated with speech interface. (arXiv:2310.04205v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2310.04205</link>
<description rdf:parseType="Literal">&lt;p&gt;Retrieving answers in a quick and low cost manner without hallucinations from
a combination of structured and unstructured data using Language models is a
major hurdle which prevents employment of Language models in knowledge
retrieval automation. This becomes accentuated when one wants to integrate a
speech interface. Besides, for commercial search and chatbot applications,
complete reliance on commercial large language models (LLMs) like GPT 3.5 etc.
can be very costly. In this work, authors have addressed this problem by first
developing a keyword based search framework which augments discovery of the
context to be provided to the large language model. The keywords in turn are
generated by LLM and cached for comparison with keywords generated by LLM
against the query raised. This significantly reduces time and cost to find the
context within documents. Once the context is set, LLM uses that to provide
answers based on a prompt tailored for Q&amp;amp;A. This research work demonstrates
that use of keywords in context identification reduces the overall inference
time and cost of information retrieval. Given this reduction in inference time
and cost with the keyword augmented retrieval framework, a speech based
interface for user input and response readout was integrated. This allowed a
seamless interaction with the language model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Purwar_A/0/1/0/all/0/1&quot;&gt;Anupam Purwar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sundar_R/0/1/0/all/0/1&quot;&gt;Rahul Sundar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04218">
<title>A Fixed-Parameter Tractable Algorithm for Counting Markov Equivalence Classes with the same Skeleton. (arXiv:2310.04218v1 [cs.DS])</title>
<link>http://arxiv.org/abs/2310.04218</link>
<description rdf:parseType="Literal">&lt;p&gt;Causal DAGs (also known as Bayesian networks) are a popular tool for encoding
conditional dependencies between random variables. In a causal DAG, the random
variables are modeled as vertices in the DAG, and it is stipulated that every
random variable is independent of its ancestors conditioned on its parents. It
is possible, however, for two different causal DAGs on the same set of random
variables to encode exactly the same set of conditional dependencies. Such
causal DAGs are said to be Markov equivalent, and equivalence classes of Markov
equivalent DAGs are known as Markov Equivalent Classes (MECs). Beautiful
combinatorial characterizations of MECs have been developed in the past few
decades, and it is known, in particular that all DAGs in the same MEC must have
the same &apos;&apos;skeleton&apos;&apos; (underlying undirected graph) and v-structures (induced
subgraph of the form $a\rightarrow b \leftarrow c$).
&lt;/p&gt;
&lt;p&gt;These combinatorial characterizations also suggest several natural
algorithmic questions. One of these is: given an undirected graph $G$ as input,
how many distinct Markov equivalence classes have the skeleton $G$? Much work
has been devoted in the last few years to this and other closely related
problems. However, to the best of our knowledge, a polynomial time algorithm
for the problem remains unknown.
&lt;/p&gt;
&lt;p&gt;In this paper, we make progress towards this goal by giving a fixed parameter
tractable algorithm for the above problem, with the parameters being the
treewidth and the maximum degree of the input graph $G$. The main technical
ingredient in our work is a construction we refer to as shadow, which lets us
create a &quot;local description&apos;&apos; of long-range constraints imposed by the
combinatorial characterizations of MECs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_V/0/1/0/all/0/1&quot;&gt;Vidya Sagar Sharma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04232">
<title>The WayHome: Long-term Motion Prediction on Dynamically Scaled. (arXiv:2310.04232v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2310.04232</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the key challenges for autonomous vehicles is the ability to
accurately predict the motion of other objects in the surrounding environment,
such as pedestrians or other vehicles. In this contribution, a novel motion
forecasting approach for autonomous vehicles is developed, inspired by the work
of Gilles et al. [1]. We predict multiple heatmaps with a neuralnetwork-based
model for every traffic participant in the vicinity of the autonomous vehicle;
with one heatmap per timestep. The heatmaps are used as input to a novel
sampling algorithm that extracts coordinates corresponding to the most likely
future positions. We experiment with different encoders and decoders, as well
as a comparison of two loss functions. Additionally, a new grid-scaling
technique is introduced, showing further improved performance. Overall, our
approach improves stateof-the-art miss rate performance for the
function-relevant prediction interval of 3 seconds while being competitive in
longer prediction intervals (up to eight seconds). The evaluation is done on
the public 2022 Waymo motion challenge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scheerer_K/0/1/0/all/0/1&quot;&gt;Kay Scheerer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michalke_T/0/1/0/all/0/1&quot;&gt;Thomas Michalke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mathes_J/0/1/0/all/0/1&quot;&gt;Juergen Mathes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04241">
<title>Comparing Auxiliary Tasks for Learning Representations for Reinforcement Learning. (arXiv:2310.04241v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.04241</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning state representations has gained steady popularity in reinforcement
learning (RL) due to its potential to improve both sample efficiency and
returns on many environments. A straightforward and efficient method is to
generate representations with a distinct neural network trained on an auxiliary
task, i.e. a task that differs from the actual RL task. While a whole range of
such auxiliary tasks has been proposed in the literature, a comparison on
typical continuous control benchmark environments is computationally expensive
and has, to the best of our knowledge, not been performed before. This paper
presents such a comparison of common auxiliary tasks, based on hundreds of
agents trained with state-of-the-art off-policy RL algorithms. We compare
possible improvements in both sample efficiency and returns for environments
ranging from simple pendulum to a complex simulated robotics task. Our findings
show that representation learning with auxiliary tasks is beneficial for
environments of higher dimension and complexity, and that learning environment
dynamics is preferable to predicting rewards. We believe these insights will
enable other researchers to make more informed decisions on how to utilize
representation learning for their specific problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lange_M/0/1/0/all/0/1&quot;&gt;Moritz Lange&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krystiniak_N/0/1/0/all/0/1&quot;&gt;Noah Krystiniak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Engelhardt_R/0/1/0/all/0/1&quot;&gt;Raphael C. Engelhardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konen_W/0/1/0/all/0/1&quot;&gt;Wolfgang Konen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiskott_L/0/1/0/all/0/1&quot;&gt;Laurenz Wiskott&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04266">
<title>DRIFT: Deep Reinforcement Learning for Intelligent Floating Platforms Trajectories. (arXiv:2310.04266v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2310.04266</link>
<description rdf:parseType="Literal">&lt;p&gt;This investigation introduces a novel deep reinforcement learning-based suite
to control floating platforms in both simulated and real-world environments.
Floating platforms serve as versatile test-beds to emulate microgravity
environments on Earth. Our approach addresses the system and environmental
uncertainties in controlling such platforms by training policies capable of
precise maneuvers amid dynamic and unpredictable conditions. Leveraging
state-of-the-art deep reinforcement learning techniques, our suite achieves
robustness, adaptability, and good transferability from simulation to reality.
Our Deep Reinforcement Learning (DRL) framework provides advantages such as
fast training times, large-scale testing capabilities, rich visualization
options, and ROS bindings for integration with real-world robotic systems.
Beyond policy development, our suite provides a comprehensive platform for
researchers, offering open-access at
https://github.com/elharirymatteo/RANS/tree/ICRA24.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+El_Hariry_M/0/1/0/all/0/1&quot;&gt;Matteo El-Hariry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richard_A/0/1/0/all/0/1&quot;&gt;Antoine Richard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muralidharan_V/0/1/0/all/0/1&quot;&gt;Vivek Muralidharan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yalcin_B/0/1/0/all/0/1&quot;&gt;Baris Can Yalcin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geist_M/0/1/0/all/0/1&quot;&gt;Matthieu Geist&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olivares_Mendez_M/0/1/0/all/0/1&quot;&gt;Miguel Olivares-Mendez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04270">
<title>A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks. (arXiv:2310.04270v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.04270</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, Large Language Models (LLM) have demonstrated impressive capability
to solve a wide range of tasks. However, despite their success across various
tasks, no prior work has investigated their capability in the biomedical domain
yet. To this end, this paper aims to evaluate the performance of LLMs on
benchmark biomedical tasks. For this purpose, we conduct a comprehensive
evaluation of 4 popular LLMs in 6 diverse biomedical tasks across 26 datasets.
To the best of our knowledge, this is the first work that conducts an extensive
evaluation and comparison of various LLMs in the biomedical domain.
Interestingly, we find based on our evaluation that in biomedical datasets that
have smaller training sets, zero-shot LLMs even outperform the current
state-of-the-art fine-tuned biomedical models. This suggests that pretraining
on large text corpora makes LLMs quite specialized even in the biomedical
domain. We also find that not a single LLM can outperform other LLMs in all
tasks, with the performance of different LLMs may vary depending on the task.
While their performance is still quite poor in comparison to the biomedical
models that were fine-tuned on large training sets, our findings demonstrate
that LLMs have the potential to be a valuable tool for various biomedical tasks
that lack large annotated data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jahan_I/0/1/0/all/0/1&quot;&gt;Israt Jahan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laskar_M/0/1/0/all/0/1&quot;&gt;Md Tahmid Rahman Laskar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1&quot;&gt;Chun Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jimmy Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04276">
<title>From task structures to world models: What do LLMs know?. (arXiv:2310.04276v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.04276</link>
<description rdf:parseType="Literal">&lt;p&gt;In what sense does a large language model have knowledge? The answer to this
question extends beyond the capabilities of a particular AI system, and
challenges our assumptions about the nature of knowledge and intelligence. We
answer by granting LLMs &quot;instrumental knowledge&quot;; knowledge defined by a
certain set of abilities. We then ask how such knowledge is related to the more
ordinary, &quot;worldly&quot; knowledge exhibited by human agents, and explore this in
terms of the degree to which instrumental knowledge can be said to incorporate
the structured world models of cognitive science. We discuss ways LLMs could
recover degrees of worldly knowledge, and suggest such recovery will be
governed by an implicit, resource-rational tradeoff between world models and
task demands.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yildirim_I/0/1/0/all/0/1&quot;&gt;Ilker Yildirim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paul_L/0/1/0/all/0/1&quot;&gt;L.A. Paul&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04285">
<title>Assessing Robustness via Score-Based Adversarial Image Generation. (arXiv:2310.04285v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.04285</link>
<description rdf:parseType="Literal">&lt;p&gt;Most adversarial attacks and defenses focus on perturbations within small
$\ell_p$-norm constraints. However, $\ell_p$ threat models cannot capture all
relevant semantic-preserving perturbations, and hence, the scope of robustness
evaluations is limited. In this work, we introduce Score-Based Adversarial
Generation (ScoreAG), a novel framework that leverages the advancements in
score-based generative models to generate adversarial examples beyond
$\ell_p$-norm constraints, so-called unrestricted adversarial examples,
overcoming their limitations. Unlike traditional methods, ScoreAG maintains the
core semantics of images while generating realistic adversarial examples,
either by transforming existing images or synthesizing new ones entirely from
scratch. We further exploit the generative capability of ScoreAG to purify
images, empirically enhancing the robustness of classifiers. Our extensive
empirical evaluation demonstrates that ScoreAG matches the performance of
state-of-the-art attacks and defenses across multiple benchmarks. This work
highlights the importance of investigating adversarial examples bounded by
semantics rather than $\ell_p$-norm constraints. ScoreAG represents an
important step towards more encompassing robustness assessments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kollovieh_M/0/1/0/all/0/1&quot;&gt;Marcel Kollovieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gosch_L/0/1/0/all/0/1&quot;&gt;Lukas Gosch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholten_Y/0/1/0/all/0/1&quot;&gt;Yan Scholten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lienen_M/0/1/0/all/0/1&quot;&gt;Marten Lienen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1&quot;&gt;Stephan G&amp;#xfc;nnemann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04288">
<title>Searching for Optimal Runtime Assurance via Reachability and Reinforcement Learning. (arXiv:2310.04288v1 [eess.SY])</title>
<link>http://arxiv.org/abs/2310.04288</link>
<description rdf:parseType="Literal">&lt;p&gt;A runtime assurance system (RTA) for a given plant enables the exercise of an
untrusted or experimental controller while assuring safety with a backup (or
safety) controller. The relevant computational design problem is to create a
logic that assures safety by switching to the safety controller as needed,
while maximizing some performance criteria, such as the utilization of the
untrusted controller. Existing RTA design strategies are well-known to be
overly conservative and, in principle, can lead to safety violations. In this
paper, we formulate the optimal RTA design problem and present a new approach
for solving it. Our approach relies on reward shaping and reinforcement
learning. It can guarantee safety and leverage machine learning technologies
for scalability. We have implemented this algorithm and present experimental
results comparing our approach with state-of-the-art reachability and
simulation-based RTA approaches in a number of scenarios using aircraft models
in 3D space with complex safety requirements. Our approach can guarantee safety
while increasing utilization of the experimental controller over existing
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Miller_K/0/1/0/all/0/1&quot;&gt;Kristina Miller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zeitler_C/0/1/0/all/0/1&quot;&gt;Christopher K. Zeitler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shen_W/0/1/0/all/0/1&quot;&gt;William Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hobbs_K/0/1/0/all/0/1&quot;&gt;Kerianne Hobbs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mitra_S/0/1/0/all/0/1&quot;&gt;Sayan Mitra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schierman_J/0/1/0/all/0/1&quot;&gt;John Schierman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Viswanathan_M/0/1/0/all/0/1&quot;&gt;Mahesh Viswanathan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04295">
<title>Identifying Representations for Intervention Extrapolation. (arXiv:2310.04295v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.04295</link>
<description rdf:parseType="Literal">&lt;p&gt;The premise of identifiable and causal representation learning is to improve
the current representation learning paradigm in terms of generalizability or
robustness. Despite recent progress in questions of identifiability, more
theoretical results demonstrating concrete advantages of these methods for
downstream tasks are needed. In this paper, we consider the task of
intervention extrapolation: predicting how interventions affect an outcome,
even when those interventions are not observed at training time, and show that
identifiable representations can provide an effective solution to this task
even if the interventions affect the outcome non-linearly. Our setup includes
an outcome Y, observed features X, which are generated as a non-linear
transformation of latent features Z, and exogenous action variables A, which
influence Z. The objective of intervention extrapolation is to predict how
interventions on A that lie outside the training support of A affect Y. Here,
extrapolation becomes possible if the effect of A on Z is linear and the
residual when regressing Z on A has full support. As Z is latent, we combine
the task of intervention extrapolation with identifiable representation
learning, which we call Rep4Ex: we aim to map the observed features X into a
subspace that allows for non-linear extrapolation in A. We show using Wiener&apos;s
Tauberian theorem that the hidden representation is identifiable up to an
affine transformation in Z-space, which is sufficient for intervention
extrapolation. The identifiability is characterized by a novel constraint
describing the linearity assumption of A on Z. Based on this insight, we
propose a method that enforces the linear invariance constraint and can be
combined with any type of autoencoder. We validate our theoretical findings
through synthetic experiments and show that our approach succeeds in predicting
the effects of unseen interventions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saengkyongam_S/0/1/0/all/0/1&quot;&gt;Sorawit Saengkyongam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosenfeld_E/0/1/0/all/0/1&quot;&gt;Elan Rosenfeld&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravikumar_P/0/1/0/all/0/1&quot;&gt;Pradeep Ravikumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfister_N/0/1/0/all/0/1&quot;&gt;Niklas Pfister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1&quot;&gt;Jonas Peters&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04304">
<title>Coding by Design: GPT-4 empowers Agile Model Driven Development. (arXiv:2310.04304v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2310.04304</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating code from a natural language using Large Language Models (LLMs)
such as ChatGPT, seems groundbreaking. Yet, with more extensive use, it&apos;s
evident that this approach has its own limitations. The inherent ambiguity of
natural language presents challenges for complex software designs. Accordingly,
our research offers an Agile Model-Driven Development (MDD) approach that
enhances code auto-generation using OpenAI&apos;s GPT-4. Our work emphasizes
&quot;Agility&quot; as a significant contribution to the current MDD method, particularly
when the model undergoes changes or needs deployment in a different programming
language. Thus, we present a case-study showcasing a multi-agent simulation
system of an Unmanned Vehicle Fleet. In the first and second layer of our
approach, we constructed a textual representation of the case-study using
Unified Model Language (UML) diagrams. In the next layer, we introduced two
sets of constraints that minimize model ambiguity. Object Constraints Language
(OCL) is applied to fine-tune the code constructions details, while FIPA
ontology is used to shape communication semantics and protocols. Ultimately,
leveraging GPT-4, our last layer auto-generates code in both Java and Python.
The Java code is deployed within the JADE framework, while the Python code is
deployed in PADE framework. Concluding our research, we engaged in a
comprehensive evaluation of the generated code. From a behavioural standpoint,
the auto-generated code aligned perfectly with the expected UML sequence
diagram. Structurally, we compared the complexity of code derived from UML
diagrams constrained solely by OCL to that influenced by both OCL and
FIPA-ontology. Results indicate that ontology-constrained model produce
inherently more intricate code, but it remains manageable and low-risk for
further testing and maintenance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadik_A/0/1/0/all/0/1&quot;&gt;Ahmed R. Sadik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brulin_S/0/1/0/all/0/1&quot;&gt;Sebastian Brulin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olhofer_M/0/1/0/all/0/1&quot;&gt;Markus Olhofer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04306">
<title>Towards A Robust Group-level Emotion Recognition via Uncertainty-Aware Learning. (arXiv:2310.04306v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.04306</link>
<description rdf:parseType="Literal">&lt;p&gt;Group-level emotion recognition (GER) is an inseparable part of human
behavior analysis, aiming to recognize an overall emotion in a multi-person
scene. However, the existing methods are devoted to combing diverse emotion
cues while ignoring the inherent uncertainties under unconstrained
environments, such as congestion and occlusion occurring within a group.
Additionally, since only group-level labels are available, inconsistent emotion
predictions among individuals in one group can confuse the network. In this
paper, we propose an uncertainty-aware learning (UAL) method to extract more
robust representations for GER. By explicitly modeling the uncertainty of each
individual, we utilize stochastic embedding drawn from a Gaussian distribution
instead of deterministic point embedding. This representation captures the
probabilities of different emotions and generates diverse predictions through
this stochasticity during the inference stage. Furthermore,
uncertainty-sensitive scores are adaptively assigned as the fusion weights of
individuals&apos; face within each group. Moreover, we develop an image enhancement
module to enhance the model&apos;s robustness against severe noise. The overall
three-branch model, encompassing face, object, and scene component, is guided
by a proportional-weighted fusion strategy and integrates the proposed
uncertainty-aware method to produce the final group-level output. Experimental
results demonstrate the effectiveness and generalization ability of our method
across three widely used databases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1&quot;&gt;Qing Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Q/0/1/0/all/0/1&quot;&gt;Qirong Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jialin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaohua Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1&quot;&gt;Wenming Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04323">
<title>Adjustable Robust Reinforcement Learning for Online 3D Bin Packing. (arXiv:2310.04323v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.04323</link>
<description rdf:parseType="Literal">&lt;p&gt;Designing effective policies for the online 3D bin packing problem (3D-BPP)
has been a long-standing challenge, primarily due to the unpredictable nature
of incoming box sequences and stringent physical constraints. While current
deep reinforcement learning (DRL) methods for online 3D-BPP have shown
promising results in optimizing average performance over an underlying box
sequence distribution, they often fail in real-world settings where some
worst-case scenarios can materialize. Standard robust DRL algorithms tend to
overly prioritize optimizing the worst-case performance at the expense of
performance under normal problem instance distribution. To address these
issues, we first introduce a permutation-based attacker to investigate the
practical robustness of both DRL-based and heuristic methods proposed for
solving online 3D-BPP. Then, we propose an adjustable robust reinforcement
learning (AR2L) framework that allows efficient adjustment of robustness
weights to achieve the desired balance of the policy&apos;s performance in average
and worst-case environments. Specifically, we formulate the objective function
as a weighted sum of expected and worst-case returns, and derive the lower
performance bound by relating to the return under a mixture dynamics. To
realize this lower bound, we adopt an iterative procedure that searches for the
associated mixture dynamics and improves the corresponding policy. We integrate
this procedure into two popular robust adversarial algorithms to develop the
exact and approximate AR2L algorithms. Experiments demonstrate that AR2L is
versatile in the sense that it improves policy robustness while maintaining an
acceptable level of performance for the nominal case.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yuxin Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yize Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1&quot;&gt;Fangzhen Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04345">
<title>Neur2RO: Neural Two-Stage Robust Optimization. (arXiv:2310.04345v1 [math.OC])</title>
<link>http://arxiv.org/abs/2310.04345</link>
<description rdf:parseType="Literal">&lt;p&gt;Robust optimization provides a mathematical framework for modeling and
solving decision-making problems under worst-case uncertainty. This work
addresses two-stage robust optimization (2RO) problems (also called adjustable
robust optimization), wherein first-stage and second-stage decisions are made
before and after uncertainty is realized, respectively. This results in a
nested min-max-min optimization problem which is extremely challenging
computationally, especially when the decisions are discrete. We propose
Neur2RO, an efficient machine learning-driven instantiation of
column-and-constraint generation (CCG), a classical iterative algorithm for
2RO. Specifically, we learn to estimate the value function of the second-stage
problem via a novel neural network architecture that is easy to optimize over
by design. Embedding our neural network into CCG yields high-quality solutions
quickly as evidenced by experiments on two 2RO benchmarks, knapsack and capital
budgeting. For knapsack, Neur2RO finds solutions that are within roughly $2\%$
of the best-known values in a few seconds compared to the three hours of the
state-of-the-art exact branch-and-price algorithm; for larger and more complex
instances, Neur2RO finds even better solutions. For capital budgeting, Neur2RO
outperforms three variants of the $k$-adaptability algorithm, particularly on
the largest instances, with a 5 to 10-fold reduction in solution time. Our code
and data are available at https://github.com/khalil-research/Neur2RO.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Dumouchelle_J/0/1/0/all/0/1&quot;&gt;Justin Dumouchelle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Julien_E/0/1/0/all/0/1&quot;&gt;Esther Julien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kurtz_J/0/1/0/all/0/1&quot;&gt;Jannis Kurtz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Khalil_E/0/1/0/all/0/1&quot;&gt;Elias B. Khalil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04353">
<title>A Language-Agent Approach to Formal Theorem-Proving. (arXiv:2310.04353v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.04353</link>
<description rdf:parseType="Literal">&lt;p&gt;Language agents, which use a large language model (LLM) capable of in-context
learning to interact with an external environment, have recently emerged as a
promising approach to control tasks. We present the first language-agent
approach to formal theorem-proving. Our method, COPRA, uses a high-capacity,
black-box LLM (GPT-4) as part of a policy for a stateful backtracking search.
During the search, the policy can select proof tactics and retrieve lemmas and
definitions from an external database. Each selected tactic is executed in the
underlying proof framework, and the execution feedback is used to build the
prompt for the next policy invocation. The search also tracks selected
information from its history and uses it to reduce hallucinations and
unnecessary LLM queries.
&lt;/p&gt;
&lt;p&gt;We evaluate COPRA on the miniF2F benchmark for Lean and a set of Coq tasks
from the Compcert project. On these benchmarks, COPRA is significantly better
than one-shot invocations of GPT-4, as well as state-of-the-art models
fine-tuned on proof data, at finding correct proofs quickly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1&quot;&gt;Amitayush Thakur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yeming Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1&quot;&gt;Swarat Chaudhuri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04373">
<title>Confronting Reward Model Overoptimization with Constrained RLHF. (arXiv:2310.04373v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.04373</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models are typically aligned with human preferences by
optimizing $\textit{reward models}$ (RMs) fitted to human feedback. However,
human preferences are multi-faceted, and it is increasingly common to derive
reward from a composition of simpler reward models which each capture a
different aspect of language quality. This itself presents a challenge, as it
is difficult to appropriately weight these component RMs when combining them.
Compounding this difficulty, because any RM is only a proxy for human
evaluation, this process is vulnerable to $\textit{overoptimization}$, wherein
past a certain point, accumulating higher reward is associated with worse human
ratings. In this paper, we perform, to our knowledge, the first study on
overoptimization in composite RMs, showing that correlation between component
RMs has a significant effect on the locations of these points. We then
introduce an approach to solve this issue using constrained reinforcement
learning as a means of preventing the agent from exceeding each RM&apos;s threshold
of usefulness. Our method addresses the problem of weighting component RMs by
learning dynamic weights, naturally given by the Lagrange multipliers. As a
result, each RM stays within the range at which it is an effective proxy,
improving evaluation performance. Finally, we introduce an adaptive method
using gradient-free optimization to identify and optimize towards these points
during a single run.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moskovitz_T/0/1/0/all/0/1&quot;&gt;Ted Moskovitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Aaditya K. Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strouse_D/0/1/0/all/0/1&quot;&gt;DJ Strouse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sandholm_T/0/1/0/all/0/1&quot;&gt;Tuomas Sandholm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1&quot;&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1&quot;&gt;Anca D. Dragan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McAleer_S/0/1/0/all/0/1&quot;&gt;Stephen McAleer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04381">
<title>Hermes: Unlocking Security Analysis of Cellular Network Protocols by Synthesizing Finite State Machines from Natural Language Specifications. (arXiv:2310.04381v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2310.04381</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present Hermes, an end-to-end framework to automatically
generate formal representations from natural language cellular specifications.
We first develop a neural constituency parser, NEUTREX, to process
transition-relevant texts and extract transition components (i.e., states,
conditions, and actions). We also design a domain-specific language to
translate these transition components to logical formulas by leveraging
dependency parse trees. Finally, we compile these logical formulas to generate
transitions and create the formal model as finite state machines. To
demonstrate the effectiveness of Hermes, we evaluate it on 4G NAS, 5G NAS, and
5G RRC specifications and obtain an overall accuracy of 81-87%, which is a
substantial improvement over the state-of-the-art. Our security analysis of the
extracted models uncovers 3 new vulnerabilities and identifies 19 previous
attacks in 4G and 5G specifications, and 7 deviations in commercial 4G
basebands.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ishtiaq_A/0/1/0/all/0/1&quot;&gt;Abdullah Al Ishtiaq&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1&quot;&gt;Sarkar Snigdha Sarathi Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rashid_S/0/1/0/all/0/1&quot;&gt;Syed Md Mukit Rashid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ranjbar_A/0/1/0/all/0/1&quot;&gt;Ali Ranjbar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1&quot;&gt;Kai Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tianwei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zhezheng Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weixuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akon_M/0/1/0/all/0/1&quot;&gt;Mujtahid Akon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hussain_S/0/1/0/all/0/1&quot;&gt;Syed Rafiul Hussain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04395">
<title>Leveraging Self-Consistency for Data-Efficient Amortized Bayesian Inference. (arXiv:2310.04395v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.04395</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a method to improve the efficiency and accuracy of amortized
Bayesian inference (ABI) by leveraging universal symmetries in the
probabilistic joint model $p(\theta, y)$ of parameters $\theta$ and data $y$.
In a nutshell, we invert Bayes&apos; theorem and estimate the marginal likelihood
based on approximate representations of the joint model. Upon perfect
approximation, the marginal likelihood is constant across all parameter values
by definition. However, approximation error leads to undesirable variance in
the marginal likelihood estimates across different parameter values. We
formulate violations of this symmetry as a loss function to accelerate the
learning dynamics of conditional neural density estimators. We apply our method
to a bimodal toy problem with an explicit likelihood (likelihood-based) and a
realistic model with an implicit likelihood (simulation-based).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmitt_M/0/1/0/all/0/1&quot;&gt;Marvin Schmitt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Habermann_D/0/1/0/all/0/1&quot;&gt;Daniel Habermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burkner_P/0/1/0/all/0/1&quot;&gt;Paul-Christian B&amp;#xfc;rkner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kothe_U/0/1/0/all/0/1&quot;&gt;Ullrich K&amp;#xf6;the&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radev_S/0/1/0/all/0/1&quot;&gt;Stefan T. Radev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04406">
<title>Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models. (arXiv:2310.04406v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.04406</link>
<description rdf:parseType="Literal">&lt;p&gt;While large language models (LLMs) have demonstrated impressive performance
on a range of decision-making tasks, they rely on simple acting processes and
fall short of broad deployment as autonomous agents. We introduce LATS
(Language Agent Tree Search), a general framework that synergizes the
capabilities of LLMs in planning, acting, and reasoning. Drawing inspiration
from Monte Carlo tree search in model-based reinforcement learning, LATS
employs LLMs as agents, value functions, and optimizers, repurposing their
latent strengths for enhanced decision-making. What is crucial in this method
is the use of an environment for external feedback, which offers a more
deliberate and adaptive problem-solving mechanism that moves beyond the
limitations of existing techniques. Our experimental evaluation across diverse
domains, such as programming, HotPotQA, and WebShop, illustrates the
applicability of LATS for both reasoning and acting. In particular, LATS
achieves 94.4\% for programming on HumanEval with GPT-4 and an average score of
75.9 for web browsing on WebShop with GPT-3.5, demonstrating the effectiveness
and generality of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1&quot;&gt;Andy Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1&quot;&gt;Kai Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shlapentokh_Rothman_M/0/1/0/all/0/1&quot;&gt;Michal Shlapentokh-Rothman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haohan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Xiong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04407">
<title>Policy-Gradient Training of Language Models for Ranking. (arXiv:2310.04407v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.04407</link>
<description rdf:parseType="Literal">&lt;p&gt;Text retrieval plays a crucial role in incorporating factual knowledge for
decision making into language processing pipelines, ranging from chat-based web
search to question answering systems. Current state-of-the-art text retrieval
models leverage pre-trained large language models (LLMs) to achieve competitive
performance, but training LLM-based retrievers via typical contrastive losses
requires intricate heuristics, including selecting hard negatives and using
additional supervision as learning signals. This reliance on heuristics stems
from the fact that the contrastive loss itself is heuristic and does not
directly optimize the downstream metrics of decision quality at the end of the
processing pipeline. To address this issue, we introduce Neural PG-RANK, a
novel training algorithm that learns to rank by instantiating a LLM as a
Plackett-Luce ranking policy. Neural PG-RANK provides a principled method for
end-to-end training of retrieval models as part of larger decision systems via
policy gradient, with little reliance on complex heuristics, and it effectively
unifies the training objective with downstream decision-making quality. We
conduct extensive experiments on various text retrieval benchmarks. The results
demonstrate that when the training objective aligns with the evaluation setup,
Neural PG-RANK yields remarkable in-domain performance improvement, with
substantial out-of-domain generalization to some critical datasets employed in
downstream question answering tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1&quot;&gt;Ge Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1&quot;&gt;Jonathan D. Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cardie_C/0/1/0/all/0/1&quot;&gt;Claire Cardie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brantley_K/0/1/0/all/0/1&quot;&gt;Kiant&amp;#xe9; Brantley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joachim_T/0/1/0/all/0/1&quot;&gt;Thorsten Joachim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04413">
<title>Beyond Uniform Sampling: Offline Reinforcement Learning with Imbalanced Datasets. (arXiv:2310.04413v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.04413</link>
<description rdf:parseType="Literal">&lt;p&gt;Offline policy learning is aimed at learning decision-making policies using
existing datasets of trajectories without collecting additional data. The
primary motivation for using reinforcement learning (RL) instead of supervised
learning techniques such as behavior cloning is to find a policy that achieves
a higher average return than the trajectories constituting the dataset.
However, we empirically find that when a dataset is dominated by suboptimal
trajectories, state-of-the-art offline RL algorithms do not substantially
improve over the average return of trajectories in the dataset. We argue this
is due to an assumption made by current offline RL algorithms of staying close
to the trajectories in the dataset. If the dataset primarily consists of
sub-optimal trajectories, this assumption forces the policy to mimic the
suboptimal actions. We overcome this issue by proposing a sampling strategy
that enables the policy to only be constrained to ``good data&quot; rather than all
actions in the dataset (i.e., uniform sampling). We present a realization of
the sampling strategy and an algorithm that can be used as a plug-and-play
module in standard offline RL algorithms. Our evaluation demonstrates
significant performance gains in 72 imbalanced datasets, D4RL dataset, and
across three different offline RL algorithms. Code is available at
https://github.com/Improbable-AI/dw-offline-rl.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_Z/0/1/0/all/0/1&quot;&gt;Zhang-Wei Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Aviral Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karnik_S/0/1/0/all/0/1&quot;&gt;Sathwik Karnik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhandwaldar_A/0/1/0/all/0/1&quot;&gt;Abhishek Bhandwaldar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1&quot;&gt;Akash Srivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pajarinen_J/0/1/0/all/0/1&quot;&gt;Joni Pajarinen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laroche_R/0/1/0/all/0/1&quot;&gt;Romain Laroche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Abhishek Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1&quot;&gt;Pulkit Agrawal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.06298">
<title>Deformable Generator Networks: Unsupervised Disentanglement of Appearance and Geometry. (arXiv:1806.06298v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.06298</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a deformable generator model to disentangle the appearance and
geometric information for both image and video data in a purely unsupervised
manner. The appearance generator network models the information related to
appearance, including color, illumination, identity or category, while the
geometric generator performs geometric warping, such as rotation and
stretching, through generating deformation field which is used to warp the
generated appearance to obtain the final image or video sequences. Two
generators take independent latent vectors as input to disentangle the
appearance and geometric information from image or video sequences. For video
data, a nonlinear transition model is introduced to both the appearance and
geometric generators to capture the dynamics over time. The proposed scheme is
general and can be easily integrated into different generative models. An
extensive set of qualitative and quantitative experiments shows that the
appearance and geometric information can be well disentangled, and the learned
geometric generator can be conveniently transferred to other image datasets to
facilitate knowledge transfer tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1&quot;&gt;Xianglei Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1&quot;&gt;Ruiqi Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1&quot;&gt;Tian Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Song-Chun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Ying Nian Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2104.03893">
<title>Multimodal Fusion of EMG and Vision for Human Grasp Intent Inference in Prosthetic Hand Control. (arXiv:2104.03893v4 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2104.03893</link>
<description rdf:parseType="Literal">&lt;p&gt;Objective: For lower arm amputees, robotic prosthetic hands promise to regain
the capability to perform daily living activities. Current control methods
based on physiological signals such as electromyography (EMG) are prone to
yielding poor inference outcomes due to motion artifacts, muscle fatigue, and
many more. Vision sensors are a major source of information about the
environment state and can play a vital role in inferring feasible and intended
gestures. However, visual evidence is also susceptible to its own artifacts,
most often due to object occlusion, lighting changes, etc. Multimodal evidence
fusion using physiological and vision sensor measurements is a natural approach
due to the complementary strengths of these modalities. Methods: In this paper,
we present a Bayesian evidence fusion framework for grasp intent inference
using eye-view video, eye-gaze, and EMG from the forearm processed by neural
network models. We analyze individual and fused performance as a function of
time as the hand approaches the object to grasp it. For this purpose, we have
also developed novel data processing and augmentation techniques to train
neural network components. Results: Our results indicate that, on average,
fusion improves the instantaneous upcoming grasp type classification accuracy
while in the reaching phase by 13.66% and 14.8%, relative to EMG and visual
evidence individually, resulting in an overall fusion accuracy of 95.3%.
Conclusion: Our experimental data analyses demonstrate that EMG and visual
evidence show complementary strengths, and as a consequence, fusion of
multimodal evidence can outperform each individual evidence modality at any
given time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zandigohar_M/0/1/0/all/0/1&quot;&gt;Mehrshad Zandigohar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1&quot;&gt;Mo Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharif_M/0/1/0/all/0/1&quot;&gt;Mohammadreza Sharif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunay_S/0/1/0/all/0/1&quot;&gt;Sezen Yagmur Gunay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furmanek_M/0/1/0/all/0/1&quot;&gt;Mariusz P. Furmanek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yarossi_M/0/1/0/all/0/1&quot;&gt;Mathew Yarossi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonato_P/0/1/0/all/0/1&quot;&gt;Paolo Bonato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Onal_C/0/1/0/all/0/1&quot;&gt;Cagdas Onal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Padir_T/0/1/0/all/0/1&quot;&gt;Taskin Padir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erdogmus_D/0/1/0/all/0/1&quot;&gt;Deniz Erdogmus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schirner_G/0/1/0/all/0/1&quot;&gt;Gunar Schirner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2111.02272">
<title>Convolutional Motif Kernel Networks. (arXiv:2111.02272v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2111.02272</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial neural networks show promising performance in detecting
correlations within data that are associated with specific outcomes. However,
the black-box nature of such models can hinder the knowledge advancement in
research fields by obscuring the decision process and preventing scientist to
fully conceptualize predicted outcomes. Furthermore, domain experts like
healthcare providers need explainable predictions to assess whether a predicted
outcome can be trusted in high stakes scenarios and to help them integrating a
model into their own routine. Therefore, interpretable models play a crucial
role for the incorporation of machine learning into high stakes scenarios like
healthcare. In this paper we introduce Convolutional Motif Kernel Networks, a
neural network architecture that involves learning a feature representation
within a subspace of the reproducing kernel Hilbert space of the position-aware
motif kernel function. The resulting model enables to directly interpret and
evaluate prediction outcomes by providing a biologically and medically
meaningful explanation without the need for additional post-hoc analysis. We
show that our model is able to robustly learn on small datasets and reaches
state-of-the-art performance on relevant healthcare prediction tasks. Our
proposed method can be utilized on DNA and protein sequences. Furthermore, we
show that the proposed method learns biologically meaningful concepts directly
from data using an end-to-end learning scheme.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ditz_J/0/1/0/all/0/1&quot;&gt;Jonas C. Ditz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reuter_B/0/1/0/all/0/1&quot;&gt;Bernhard Reuter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfeifer_N/0/1/0/all/0/1&quot;&gt;Nico Pfeifer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.06865">
<title>Calibration of Derivative Pricing Models: a Multi-Agent Reinforcement Learning Perspective. (arXiv:2203.06865v4 [q-fin.CP] UPDATED)</title>
<link>http://arxiv.org/abs/2203.06865</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the most fundamental questions in quantitative finance is the
existence of continuous-time diffusion models that fit market prices of a given
set of options. Traditionally, one employs a mix of intuition, theoretical and
empirical analysis to find models that achieve exact or approximate fits. Our
contribution is to show how a suitable game theoretical formulation of this
problem can help solve this question by leveraging existing developments in
modern deep multi-agent reinforcement learning to search in the space of
stochastic processes. Our experiments show that we are able to learn local
volatility, as well as path-dependence required in the volatility process to
minimize the price of a Bermudan option. Our algorithm can be seen as a
particle method \textit{\`{a} la} Guyon \textit{et} Henry-Labordere where
particles, instead of being designed to ensure $\sigma_{loc}(t,S_t)^2 =
\mathbb{E}[\sigma_t^2|S_t]$, are learning RL-driven agents cooperating towards
more general calibration targets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Vadori_N/0/1/0/all/0/1&quot;&gt;Nelson Vadori&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.15376">
<title>Reinforcement Learning with a Terminator. (arXiv:2205.15376v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.15376</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the problem of reinforcement learning with exogenous termination.
We define the Termination Markov Decision Process (TerMDP), an extension of the
MDP framework, in which episodes may be interrupted by an external
non-Markovian observer. This formulation accounts for numerous real-world
situations, such as a human interrupting an autonomous driving agent for
reasons of discomfort. We learn the parameters of the TerMDP and leverage the
structure of the estimation problem to provide state-wise confidence bounds. We
use these to construct a provably-efficient algorithm, which accounts for
termination, and bound its regret. Motivated by our theoretical analysis, we
design and implement a scalable approach, which combines optimism (w.r.t.
termination) and a dynamic discount factor, incorporating the termination
probability. We deploy our method on high-dimensional driving and MinAtar
benchmarks. Additionally, we test our approach on human data in a driving
setting. Our results demonstrate fast convergence and significant improvement
over various baseline approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tennenholtz_G/0/1/0/all/0/1&quot;&gt;Guy Tennenholtz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merlis_N/0/1/0/all/0/1&quot;&gt;Nadav Merlis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shani_L/0/1/0/all/0/1&quot;&gt;Lior Shani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1&quot;&gt;Shie Mannor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shalit_U/0/1/0/all/0/1&quot;&gt;Uri Shalit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1&quot;&gt;Gal Chechik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hallak_A/0/1/0/all/0/1&quot;&gt;Assaf Hallak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dalal_G/0/1/0/all/0/1&quot;&gt;Gal Dalal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.01306">
<title>Deceptive Planning for Resource Allocation. (arXiv:2206.01306v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2206.01306</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider a team of autonomous agents that navigate in an adversarial
environment and aim to achieve a task by allocating their resources over a set
of target locations. An adversary in the environment observes the autonomous
team&apos;s behavior to infer their objective and responds against the team. In this
setting, we propose strategies for controlling the density of the autonomous
team so that they can deceive the adversary regarding their objective while
achieving the desired final resource allocation. We first develop a prediction
algorithm based on the principle of maximum entropy to express the team&apos;s
behavior expected by the adversary. Then, by measuring the deceptiveness via
Kullback-Leibler divergence, we devise convex optimization-based planning
algorithms that deceive the adversary by either exaggerating the behavior
towards a decoy allocation strategy or creating ambiguity regarding the final
allocation strategy. A user study with $320$ participants demonstrates that the
proposed algorithms are effective for deception and reveal the inherent biases
of participants towards proximate goals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shenghui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Savas_Y/0/1/0/all/0/1&quot;&gt;Yagiz Savas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Karabag_M/0/1/0/all/0/1&quot;&gt;Mustafa O. Karabag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Sadler_B/0/1/0/all/0/1&quot;&gt;Brian M. Sadler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Topcu_U/0/1/0/all/0/1&quot;&gt;Ufuk Topcu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.06281">
<title>TwiRGCN: Temporally Weighted Graph Convolution for Question Answering over Temporal Knowledge Graphs. (arXiv:2210.06281v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2210.06281</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years have witnessed much interest in temporal reasoning over
knowledge graphs (KG) for complex question answering (QA), but there remains a
substantial gap in human capabilities. We explore how to generalize relational
graph convolutional networks (RGCN) for temporal KGQA. Specifically, we propose
a novel, intuitive and interpretable scheme to modulate the messages passed
through a KG edge during convolution, based on the relevance of its associated
time period to the question. We also introduce a gating device to predict if
the answer to a complex temporal question is likely to be a KG entity or time
and use this prediction to guide our scoring mechanism. We evaluate the
resulting system, which we call TwiRGCN, on TimeQuestions, a recently released,
challenging dataset for multi-hop complex temporal QA. We show that TwiRGCN
significantly outperforms state-of-the-art systems on this dataset across
diverse question types. Notably, TwiRGCN improves accuracy by 9--10 percentage
points for the most difficult ordinal and implicit question types.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1&quot;&gt;Aditya Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saxena_A/0/1/0/all/0/1&quot;&gt;Apoorv Saxena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_C/0/1/0/all/0/1&quot;&gt;Chitrank Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kazemi_S/0/1/0/all/0/1&quot;&gt;Seyed Mehran Kazemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1&quot;&gt;Partha Talukdar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakrabarti_S/0/1/0/all/0/1&quot;&gt;Soumen Chakrabarti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.04205">
<title>Preserving Semantics in Textual Adversarial Attacks. (arXiv:2211.04205v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2211.04205</link>
<description rdf:parseType="Literal">&lt;p&gt;The growth of hateful online content, or hate speech, has been associated
with a global increase in violent crimes against minorities [23]. Harmful
online content can be produced easily, automatically and anonymously. Even
though, some form of auto-detection is already achieved through text
classifiers in NLP, they can be fooled by adversarial attacks. To strengthen
existing systems and stay ahead of attackers, we need better adversarial
attacks. In this paper, we show that up to 70% of adversarial examples
generated by adversarial attacks should be discarded because they do not
preserve semantics. We address this core weakness and propose a new, fully
supervised sentence embedding technique called Semantics-Preserving-Encoder
(SPE). Our method outperforms existing sentence encoders used in adversarial
attacks by achieving 1.2x - 5.1x better real attack success rate. We release
our code as a plugin that can be used in any existing adversarial attack to
improve its quality and speed up its execution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herel_D/0/1/0/all/0/1&quot;&gt;David Herel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cisneros_H/0/1/0/all/0/1&quot;&gt;Hugo Cisneros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mikolov_T/0/1/0/all/0/1&quot;&gt;Tomas Mikolov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.06096">
<title>Implicit Convolutional Kernels for Steerable CNNs. (arXiv:2212.06096v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.06096</link>
<description rdf:parseType="Literal">&lt;p&gt;Steerable convolutional neural networks (CNNs) provide a general framework
for building neural networks equivariant to translations and other
transformations belonging to an origin-preserving group $G$, such as
reflections and rotations. They rely on standard convolutions with
$G$-steerable kernels obtained by analytically solving the group-specific
equivariance constraint imposed onto the kernel space. As the solution is
tailored to a particular group $G$, the implementation of a kernel basis does
not generalize to other symmetry transformations, which complicates the
development of general group equivariant models. We propose using implicit
neural representation via multi-layer perceptrons (MLPs) to parameterize
$G$-steerable kernels. The resulting framework offers a simple and flexible way
to implement Steerable CNNs and generalizes to any group $G$ for which a
$G$-equivariant MLP can be built. We prove the effectiveness of our method on
multiple tasks, including N-body simulations, point cloud classification and
molecular property prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhdanov_M/0/1/0/all/0/1&quot;&gt;Maksim Zhdanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoffmann_N/0/1/0/all/0/1&quot;&gt;Nico Hoffmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cesa_G/0/1/0/all/0/1&quot;&gt;Gabriele Cesa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.07877">
<title>Manifestations of Xenophobia in AI Systems. (arXiv:2212.07877v2 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2212.07877</link>
<description rdf:parseType="Literal">&lt;p&gt;Xenophobia is one of the key drivers of marginalisation, discrimination, and
conflict, yet many prominent machine learning (ML) fairness frameworks fail to
comprehensively measure or mitigate the resulting xenophobic harms. Here we aim
to bridge this conceptual gap and help facilitate safe and ethical design of
artificial intelligence (AI) solutions. We ground our analysis of the impact of
xenophobia by first identifying distinct types of xenophobic harms, and then
applying this framework across a number of prominent AI application domains,
reviewing the potential interplay between AI and xenophobia on social media and
recommendation systems, healthcare, immigration, employment, as well as biases
in large pre-trained models. These help inform our recommendations towards an
inclusive, xenophilic design of future AI systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomasev_N/0/1/0/all/0/1&quot;&gt;Nenad Tomasev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maynard_J/0/1/0/all/0/1&quot;&gt;Jonathan Leader Maynard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gabriel_I/0/1/0/all/0/1&quot;&gt;Iason Gabriel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.10888">
<title>Learning Robust, Agile, Natural Legged Locomotion Skills in the Wild. (arXiv:2304.10888v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2304.10888</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, reinforcement learning has become a promising and polular solution
for robot legged locomotion. Compared to model-based control, reinforcement
learning based controllers can achieve better robustness against uncertainties
of environments through sim-to-real learning. However, the corresponding
learned gaits are in general overly conservative and unatural. In this paper,
we propose a new framework for learning robust, agile and natural legged
locomotion skills over challenging terrain. We incorporate an adversarial
training branch based on real animal locomotion data upon a teacher-student
training pipeline for robust sim-to-real transfer. Empirical results on both
simulation and real world of a quadruped robot demonstrate that our proposed
algorithm enables robustly traversing challenging terrains such as stairs,
rocky ground and slippery floor with only proprioceptive perception. Meanwhile,
the gaits are more agile, natural, and energy efficient compared to the
baselines. Both qualitative and quantitative results are presented in this
paper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yikai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zheyuan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jianyu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.14993">
<title>ChatGPT in the Classroom: An Analysis of Its Strengths and Weaknesses for Solving Undergraduate Computer Science Questions. (arXiv:2304.14993v4 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2304.14993</link>
<description rdf:parseType="Literal">&lt;p&gt;ChatGPT is an AI language model developed by OpenAI that can understand and
generate human-like text. It can be used for a variety of use cases such as
language generation, question answering, text summarization, chatbot
development, language translation, sentiment analysis, content creation,
personalization, text completion, and storytelling. While ChatGPT has garnered
significant positive attention, it has also generated a sense of apprehension
and uncertainty in academic circles. There is concern that students may
leverage ChatGPT to complete take-home assignments and exams and obtain
favorable grades without genuinely acquiring knowledge. This paper adopts a
quantitative approach to demonstrate ChatGPT&apos;s high degree of unreliability in
answering a diverse range of questions pertaining to topics in undergraduate
computer science. Our analysis shows that students may risk self-sabotage by
blindly depending on ChatGPT to complete assignments and exams. We build upon
this analysis to provide constructive recommendations to both students and
instructors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_I/0/1/0/all/0/1&quot;&gt;Ishika Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Budhiraja_R/0/1/0/all/0/1&quot;&gt;Ritvik Budhiraja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dev_H/0/1/0/all/0/1&quot;&gt;Harshal Dev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kadia_J/0/1/0/all/0/1&quot;&gt;Jahnvi Kadia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ataullah_M/0/1/0/all/0/1&quot;&gt;M. Osama Ataullah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitra_S/0/1/0/all/0/1&quot;&gt;Sayan Mitra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1&quot;&gt;Dhruv Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akolekar_H/0/1/0/all/0/1&quot;&gt;Harshal D. Akolekar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.03584">
<title>Now It Sounds Like You: Learning Personalized Vocabulary On Device. (arXiv:2305.03584v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.03584</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, Federated Learning (FL) has shown significant advancements
in its ability to perform various natural language processing (NLP) tasks. This
work focuses on applying personalized FL for on-device language modeling. Due
to limitations of memory and latency, these models cannot support the
complexity of sub-word tokenization or beam search decoding, resulting in the
decision to deploy a closed-vocabulary language model. However,
closed-vocabulary models are unable to handle out-of-vocabulary (OOV) words
belonging to specific users. To address this issue, We propose a novel
technique called &quot;OOV expansion&quot; that improves OOV coverage and increases model
accuracy while minimizing the impact on memory and latency. This method
introduces a personalized &quot;OOV adapter&quot; that effectively transfers knowledge
from a central model and learns word embedding for personalized vocabulary. OOV
expansion significantly outperforms standard FL personalization methods on a
set of common FL benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sid Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shenoy_A/0/1/0/all/0/1&quot;&gt;Ashish Shenoy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chuang_P/0/1/0/all/0/1&quot;&gt;Pierce Chuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_J/0/1/0/all/0/1&quot;&gt;John Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14405">
<title>NeuralMatrix: Compute the Entire Neural Networks with Linear Matrix Operations for Efficient Inference. (arXiv:2305.14405v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14405</link>
<description rdf:parseType="Literal">&lt;p&gt;The inherent diversity of computation types within individual deep neural
network (DNN) models necessitates a corresponding variety of computation units
within hardware processors, leading to a significant constraint on computation
efficiency during neural network execution. In this study, we introduce
NeuralMatrix, a framework that transforms the computation of entire DNNs into
linear matrix operations, effectively enabling their execution with one
general-purpose matrix multiplication (GEMM) accelerator. By surmounting the
constraints posed by the diverse computation types required by individual
network models, this approach provides both generality, allowing a wide range
of DNN models to be executed using a single GEMM accelerator and
application-specific acceleration levels without extra special function units,
which are validated through main stream DNNs and their variant models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1&quot;&gt;Ruiqi Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jie Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xin He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yiran Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_A/0/1/0/all/0/1&quot;&gt;An Zou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17154">
<title>On convex decision regions in deep network representations. (arXiv:2305.17154v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17154</link>
<description rdf:parseType="Literal">&lt;p&gt;Current work on human-machine alignment aims at understanding machine-learned
latent spaces and their correspondence to human representations.
G{\&quot;a}rdenfors&apos; conceptual spaces is a prominent framework for understanding
human representations. Convexity of object regions in conceptual spaces is
argued to promote generalizability, few-shot learning, and interpersonal
alignment. Based on these insights, we investigate the notion of convexity of
concept regions in machine-learned latent spaces. We develop a set of tools for
measuring convexity in sampled data and evaluate emergent convexity in layered
representations of state-of-the-art deep networks. We show that convexity is
robust to basic re-parametrization and, hence, meaningful as a quality of
machine-learned latent spaces. We find that approximate convexity is pervasive
in neural representations in multiple application domains, including models of
images, audio, human activity, text, and medical images. Generally, we observe
that fine-tuning increases the convexity of label regions. We find evidence
that pretraining convexity of class label regions predicts subsequent
fine-tuning performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tetkova_L/0/1/0/all/0/1&quot;&gt;Lenka T&amp;#x11b;tkov&amp;#xe1;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brusch_T/0/1/0/all/0/1&quot;&gt;Thea Br&amp;#xfc;sch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scheidt_T/0/1/0/all/0/1&quot;&gt;Teresa Karen Scheidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mager_F/0/1/0/all/0/1&quot;&gt;Fabian Martin Mager&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aagaard_R/0/1/0/all/0/1&quot;&gt;Rasmus &amp;#xd8;rtoft Aagaard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foldager_J/0/1/0/all/0/1&quot;&gt;Jonathan Foldager&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alstrom_T/0/1/0/all/0/1&quot;&gt;Tommy Sonne Alstr&amp;#xf8;m&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hansen_L/0/1/0/all/0/1&quot;&gt;Lars Kai Hansen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17590">
<title>Integrating Action Knowledge and LLMs for Task Planning and Situation Handling in Open Worlds. (arXiv:2305.17590v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17590</link>
<description rdf:parseType="Literal">&lt;p&gt;Task planning systems have been developed to help robots use human knowledge
(about actions) to complete long-horizon tasks. Most of them have been
developed for &quot;closed worlds&quot; while assuming the robot is provided with
complete world knowledge. However, the real world is generally open, and the
robots frequently encounter unforeseen situations that can potentially break
the planner&apos;s completeness. Could we leverage the recent advances on
pre-trained Large Language Models (LLMs) to enable classical planning systems
to deal with novel situations?
&lt;/p&gt;
&lt;p&gt;This paper introduces a novel framework, called COWP, for open-world task
planning and situation handling. COWP dynamically augments the robot&apos;s action
knowledge, including the preconditions and effects of actions, with
task-oriented commonsense knowledge. COWP embraces the openness from LLMs, and
is grounded to specific domains via action knowledge. For systematic
evaluations, we collected a dataset that includes 1,085 execution-time
situations. Each situation corresponds to a state instance wherein a robot is
potentially unable to complete a task using a solution that normally works.
Experimental results show that our approach outperforms competitive baselines
from the literature in the success rate of service tasks. Additionally, we have
demonstrated COWP using a mobile manipulator. Supplementary materials are
available at: https://cowplanning.github.io/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Yan Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaohan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amiri_S/0/1/0/all/0/1&quot;&gt;Saeid Amiri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_N/0/1/0/all/0/1&quot;&gt;Nieqing Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaminski_A/0/1/0/all/0/1&quot;&gt;Andy Kaminski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Esselink_C/0/1/0/all/0/1&quot;&gt;Chad Esselink&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shiqi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18030">
<title>Automated Search-Space Generation Neural Architecture Search. (arXiv:2305.18030v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18030</link>
<description rdf:parseType="Literal">&lt;p&gt;To search an optimal sub-network within a general deep neural network (DNN),
existing neural architecture search (NAS) methods typically rely on
handcrafting a search space beforehand. Such requirements make it challenging
to extend them onto general scenarios without significant human expertise and
manual intervention. To overcome the limitations, we propose Automated
Search-Space Generation Neural Architecture Search (ASGNAS), perhaps the first
automated system to train general DNNs that cover all candidate connections and
operations and produce high-performing sub-networks in the one shot manner.
Technologically, ASGNAS delivers three noticeable contributions to minimize
human efforts: (i) automated search space generation for general DNNs; (ii) a
Hierarchical Half-Space Projected Gradient (H2SPG) that leverages the hierarchy
and dependency within generated search space to ensure the network validity
during optimization, and reliably produces a solution with both high
performance and hierarchical group sparsity; and (iii) automated sub-network
construction upon the H2SPG solution. Numerically, we demonstrate the
effectiveness of ASGNAS on a variety of general DNNs, including RegNet,
StackedUnets, SuperResNet, and DARTS, over benchmark datasets such as CIFAR10,
Fashion-MNIST, ImageNet, STL-10 , and SVNH. The sub-networks computed by ASGNAS
achieve competitive even superior performance compared to the starting full
DNNs and other state-of-the-arts. The library will be released at
https://github.com/tianyic/only_train_once.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1&quot;&gt;Luming Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_T/0/1/0/all/0/1&quot;&gt;Tianyu Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zharkov_I/0/1/0/all/0/1&quot;&gt;Ilya Zharkov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04959">
<title>FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning and Federated LLMs. (arXiv:2306.04959v3 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04959</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces FedMLSecurity, a benchmark designed to simulate
adversarial attacks and corresponding defense mechanisms in Federated Learning
(FL). As an integral module of the open-sourced library FedML that facilitates
FL algorithm development and performance comparison, FedMLSecurity enhances
FedML&apos;s capabilities to evaluate security issues and potential remedies in FL.
FedMLSecurity comprises two major components: FedMLAttacker that simulates
attacks injected during FL training, and FedMLDefender that simulates defensive
mechanisms to mitigate the impacts of the attacks. FedMLSecurity is
open-sourced and can be customized to a wide range of machine learning models
(e.g., Logistic Regression, ResNet, GAN, etc.) and federated optimizers (e.g.,
FedAVG, FedOPT, FedNOVA, etc.). FedMLSecurity can also be applied to Large
Language Models (LLMs) easily, demonstrating its adaptability and applicability
in various scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1&quot;&gt;Shanshan Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buyukates_B/0/1/0/all/0/1&quot;&gt;Baturalp Buyukates&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zijian Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1&quot;&gt;Han Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1&quot;&gt;Weizhao Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Lichao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaoyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wenxuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1&quot;&gt;Chulin Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yuhang Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qifan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuhui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1&quot;&gt;Salman Avestimehr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1&quot;&gt;Chaoyang He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07863">
<title>Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control. (arXiv:2306.07863v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07863</link>
<description rdf:parseType="Literal">&lt;p&gt;Building agents using large language models (LLMs) to control computers is an
emerging research field, where the agent perceives computer states and performs
actions to accomplish complex tasks. Previous computer agents have demonstrated
the benefits of in-context learning (ICL); however, their performance is
hindered by several issues. First, the limited context length of LLMs and
complex computer states restrict the number of exemplars, as a single webpage
can consume the entire context. Second, the exemplars in current methods, such
as high-level plans and multi-choice questions, cannot represent complete
trajectories, leading to suboptimal performance in tasks that require many
steps or repeated actions. Third, existing computer agents rely on
task-specific exemplars and overlook the similarity among tasks, resulting in
poor generalization to novel tasks. To address these challenges, we introduce
Synapse, featuring three key components: i) state abstraction, which filters
out task-irrelevant information from raw states, allowing more exemplars within
the limited context, ii) trajectory-as-exemplar prompting, which prompts the
LLM with complete trajectories of the abstracted states and actions for
improved multi-step decision-making, and iii) exemplar memory, which stores the
embeddings of exemplars and retrieves them via similarity search for
generalization to novel tasks. We evaluate Synapse on MiniWoB++, a standard
task suite, and Mind2Web, a real-world website benchmark. In MiniWoB++, Synapse
achieves a 99.2% average success rate (a 10% relative improvement) across 64
tasks using demonstrations from only 48 tasks. Notably, Synapse is the first
ICL method to solve the book-flight task in MiniWoB++. Synapse also exhibits a
53% relative improvement in average step success rate over the previous
state-of-the-art prompting scheme in Mind2Web.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1&quot;&gt;Longtao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rundong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinrun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1&quot;&gt;Bo An&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11363">
<title>Masked Diffusion Models Are Fast Distribution Learners. (arXiv:2306.11363v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11363</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have emerged as the \emph{de-facto} generative model for
image synthesis, yet they entail significant training overhead, hindering the
technique&apos;s broader adoption in the research community. We observe that these
models are commonly trained to learn all fine-grained visual information from
scratch, thus motivating our investigation on its necessity. In this work, we
show that it suffices to set up pre-training stage to initialize a diffusion
model by encouraging it to learn some primer distribution of the unknown real
image distribution. Then the pre-trained model can be fine-tuned for specific
generation tasks efficiently. To approximate the primer distribution, our
approach centers on masking a high proportion (e.g., up to 90\%) of an input
image and employing masked denoising score matching to denoise visible areas.
Utilizing the learned primer distribution in subsequent fine-tuning, we
efficiently train a ViT-based diffusion model on CelebA-HQ $256 \times 256$ in
the raw pixel space, achieving superior training acceleration compared to
denoising diffusion probabilistic model (DDPM) counterpart and a new FID score
record of 6.73 for ViT-based diffusion models. Moreover, our masked
pre-training technique can be universally applied to various diffusion models
that directly generate images in the pixel space, aiding in the learning of
pre-trained models with superior generalizability. For instance, a diffusion
model pre-trained on VGGFace2 attains a 46\% quality improvement through
fine-tuning on only 10\% data from a different dataset. Our code is available
at \url{https://github.com/jiachenlei/maskdm}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1&quot;&gt;Jiachen Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qinglong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1&quot;&gt;Peng Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ba_Z/0/1/0/all/0/1&quot;&gt;Zhongjie Ba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1&quot;&gt;Zhan Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhibo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhenguang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1&quot;&gt;Kui Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11695">
<title>A Simple and Effective Pruning Approach for Large Language Models. (arXiv:2306.11695v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11695</link>
<description rdf:parseType="Literal">&lt;p&gt;As their size increases, Large Languages Models (LLMs) are natural candidates
for network pruning methods: approaches that drop a subset of network weights
while striving to preserve performance. Existing methods, however, require
either retraining, which is rarely affordable for billion-scale LLMs, or
solving a weight reconstruction problem reliant on second-order information,
which may also be computationally expensive. In this paper, we introduce a
novel, straightforward yet effective pruning method, termed Wanda (Pruning by
Weights and activations), designed to induce sparsity in pretrained LLMs.
Motivated by the recent observation of emergent large magnitude features in
LLMs, our approach prunes weights with the smallest magnitudes multiplied by
the corresponding input activations, on a per-output basis. Notably, Wanda
requires no retraining or weight update, and the pruned LLM can be used as is.
We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2
across various language benchmarks. Wanda significantly outperforms the
established baseline of magnitude pruning and performs competitively against
recent method involving intensive weight update. Code is available at
https://github.com/locuslab/wanda.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;Mingjie Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhuang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bair_A/0/1/0/all/0/1&quot;&gt;Anna Bair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1&quot;&gt;J. Zico Kolter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03887">
<title>Improving Prototypical Part Networks with Reward Reweighing, Reselection, and Retraining. (arXiv:2307.03887v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03887</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, work has gone into developing deep interpretable methods for
image classification that clearly attributes a model&apos;s output to specific
features of the data. One such of these methods is the Prototypical Part
Network (ProtoPNet), which attempts to classify images based on meaningful
parts of the input. While this method results in interpretable classifications,
it often learns to classify from spurious or inconsistent parts of the image.
Hoping to remedy this, we take inspiration from the recent developments in
Reinforcement Learning with Human Feedback (RLHF) to fine-tune these
prototypes. By collecting human annotations of prototypes quality via a 1-5
scale on the CUB-200-2011 dataset, we construct a reward model that learns
human preferences and identify non-spurious prototypes. In place of a full RL
update, we propose the Reweighed, Reselected, and Retrained Prototypical Part
Network (R3-ProtoPNet), which adds an additional three steps to the ProtoPNet
training loop. The first two steps are reward-based reweighting and
reselection, which align prototypes with human feedback. The final step is
retraining to realign the model&apos;s features with the updated prototypes. We find
that R3-ProtoPNet improves the overall meaningfulness of the prototypes, and
maintains or improves individual model performance. When multiple trained
R3-ProtoPNets are incorporated into an ensemble, we find increases in both
interpretability and predictive performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Netzorg_R/0/1/0/all/0/1&quot;&gt;Robin Netzorg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiaxun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Bin Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11804">
<title>Adversarial Illusions in Multi-Modal Embeddings. (arXiv:2308.11804v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11804</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-modal embeddings encode images, sounds, texts, videos, etc. into a
single embedding space, aligning representations across modalities (e.g.,
associate an image of a dog with a barking sound). We show that multi-modal
embeddings can be vulnerable to an attack we call &quot;adversarial illusions.&quot;
Given an image or a sound, an adversary can perturb it so as to make its
embedding close to an arbitrary, adversary-chosen input in another modality.
This enables the adversary to align any image and any sound with any text.
&lt;/p&gt;
&lt;p&gt;Adversarial illusions exploit proximity in the embedding space and are thus
agnostic to downstream tasks. Using ImageBind embeddings, we demonstrate how
adversarially aligned inputs, generated without knowledge of specific
downstream tasks, mislead image generation, text generation, and zero-shot
classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagdasaryan_E/0/1/0/all/0/1&quot;&gt;Eugene Bagdasaryan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jha_R/0/1/0/all/0/1&quot;&gt;Rishi Jha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tingwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shmatikov_V/0/1/0/all/0/1&quot;&gt;Vitaly Shmatikov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11838">
<title>A Benchmark Study on Calibration. (arXiv:2308.11838v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11838</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks are increasingly utilized in various machine learning
tasks. However, as these models grow in complexity, they often face calibration
issues, despite enhanced prediction accuracy. Many studies have endeavored to
improve calibration performance through data preprocessing, the use of specific
loss functions, and training frameworks. Yet, investigations into calibration
properties have been somewhat overlooked. Our study leverages the Neural
Architecture Search (NAS) search space, offering an exhaustive model
architecture space for thorough calibration properties exploration. We
specifically create a model calibration dataset. This dataset evaluates 90
bin-based and 12 additional calibration measurements across 117,702 unique
neural networks within the widely employed NATS-Bench search space. Our
analysis aims to answer several longstanding questions in the field, using our
proposed dataset: (i) Can model calibration be generalized across different
tasks? (ii) Can robustness be used as a calibration measurement? (iii) How
reliable are calibration metrics? (iv) Does a post-hoc calibration method
affect all models uniformly? (v) How does calibration interact with accuracy?
(vi) What is the impact of bin size on calibration measurement? (vii) Which
architectural designs are beneficial for calibration? Additionally, our study
bridges an existing gap by exploring calibration within NAS. By providing this
dataset, we enable further research into NAS calibration. As far as we are
aware, our research represents the first large-scale investigation into
calibration properties and the premier study of calibration issues within NAS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_L/0/1/0/all/0/1&quot;&gt;Linwei Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Younan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1&quot;&gt;Haolan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1&quot;&gt;Minjing Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chang Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.05938">
<title>Answering Subjective Induction Questions on Products by Summarizing Multi-sources Multi-viewpoints Knowledge. (arXiv:2309.05938v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.05938</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a new task in the field of Answering Subjective Induction
Question on Products (SUBJPQA). The answer to this kind of question is
non-unique, but can be interpreted from many perspectives. For example, the
answer to &apos;whether the phone is heavy&apos; has a variety of different viewpoints. A
satisfied answer should be able to summarize these subjective opinions from
multiple sources and provide objective knowledge, such as the weight of a
phone. That is quite different from the traditional QA task, in which the
answer to a factoid question is unique and can be found from a single data
source. To address this new task, we propose a three-steps method. We first
retrieve all answer-related clues from multiple knowledge sources on facts and
opinions. The implicit commonsense facts are also collected to supplement the
necessary but missing contexts. We then capture their relevance with the
questions by interactive attention. Next, we design a reinforcement-based
summarizer to aggregate all these knowledgeable clues. Based on a
template-controlled decoder, we can output a comprehensive and
multi-perspective answer. Due to the lack of a relevant evaluated benchmark set
for the new task, we construct a large-scale dataset, named SupQA, consisting
of 48,352 samples across 15 product domains. Evaluation results show the
effectiveness of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yufeng Zhang&lt;/a&gt; (1 and 2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Meng-xiang Wang&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jianxing Yu&lt;/a&gt; (1, 2 and 4) ((1) School of Artificial Intelligence, Sun Yat-sen University, Zhuhai 519082 (2) Guangdong Key Laboratory of Big Data Analysis and Processing, 510006, China (3) China National Institute of Standardization, 100088, China (4) Pazhou Lab, Guangzhou, 510330, China)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.09826">
<title>Efficient Avoidance of Vulnerabilities in Auto-completed Smart Contract Code Using Vulnerability-constrained Decoding. (arXiv:2309.09826v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2309.09826</link>
<description rdf:parseType="Literal">&lt;p&gt;Auto-completing code enables developers to speed up coding significantly.
Recent advances in transformer-based large language model (LLM) technologies
have been applied to code synthesis. However, studies show that many of such
synthesized codes contain vulnerabilities. We propose a novel
vulnerability-constrained decoding approach to reduce the amount of vulnerable
code generated by such models. Using a small dataset of labeled vulnerable
lines of code, we fine-tune an LLM to include vulnerability labels when
generating code, acting as an embedded classifier. Then, during decoding, we
deny the model to generate these labels to avoid generating vulnerable code. To
evaluate the method, we chose to automatically complete Ethereum Blockchain
smart contracts (SCs) as the case study due to the strict requirements of SC
security. We first fine-tuned the 6-billion-parameter GPT-J model using 186,397
Ethereum SCs after removing the duplication from 2,217,692 SCs. The fine-tuning
took more than one week using ten GPUs. The results showed that our fine-tuned
model could synthesize SCs with an average BLEU (BiLingual Evaluation
Understudy) score of 0.557. However, many codes in the auto-completed SCs were
vulnerable. Using the code before the vulnerable line of 176 SCs containing
different types of vulnerabilities to auto-complete the code, we found that
more than 70% of the auto-completed codes were insecure. Thus, we further
fine-tuned the model on other 941 vulnerable SCs containing the same types of
vulnerabilities and applied vulnerability-constrained decoding. The fine-tuning
took only one hour with four GPUs. We then auto-completed the 176 SCs again and
found that our approach could identify 62% of the code to be generated as
vulnerable and avoid generating 67% of them, indicating the approach could
efficiently and effectively avoid vulnerabilities in the auto-completed code.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Storhaug_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Storhaug&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jingyue Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1&quot;&gt;Tianyuan Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14293">
<title>NAS-NeRF: Generative Neural Architecture Search for Neural Radiance Fields. (arXiv:2309.14293v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14293</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural radiance fields (NeRFs) enable high-quality novel view synthesis, but
their high computational complexity limits deployability. While existing
neural-based solutions strive for efficiency, they use one-size-fits-all
architectures regardless of scene complexity. The same architecture may be
unnecessarily large for simple scenes but insufficient for complex ones. Thus,
there is a need to dynamically optimize the neural network component of NeRFs
to achieve a balance between computational complexity and specific targets for
synthesis quality. We introduce NAS-NeRF, a generative neural architecture
search strategy that generates compact, scene-specialized NeRF architectures by
balancing architecture complexity and target synthesis quality metrics. Our
method incorporates constraints on target metrics and budgets to guide the
search towards architectures tailored for each scene. Experiments on the
Blender synthetic dataset show the proposed NAS-NeRF can generate architectures
up to 5.74$\times$ smaller, with 4.19$\times$ fewer FLOPs, and 1.93$\times$
faster on a GPU than baseline NeRFs, without suffering a drop in SSIM.
Furthermore, we illustrate that NAS-NeRF can also achieve architectures up to
23$\times$ smaller, with 22$\times$ fewer FLOPs, and 4.7$\times$ faster than
baseline NeRFs with only a 5.3% average SSIM drop. Our source code is also made
publicly available at https://saeejithnair.github.io/NAS-NeRF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1&quot;&gt;Saeejith Nair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuhao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shafiee_M/0/1/0/all/0/1&quot;&gt;Mohammad Javad Shafiee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1&quot;&gt;Alexander Wong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14674">
<title>Leveraging Herpangina Data to Enhance Hospital-level Prediction of Hand-Foot-and-Mouth Disease Admissions Using UPTST. (arXiv:2309.14674v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14674</link>
<description rdf:parseType="Literal">&lt;p&gt;Outbreaks of hand-foot-and-mouth disease(HFMD) have been associated with
significant morbidity and, in severe cases, mortality. Accurate forecasting of
daily admissions of pediatric HFMD patients is therefore crucial for aiding the
hospital in preparing for potential outbreaks and mitigating nosocomial
transmissions. To address this pressing need, we propose a novel
transformer-based model with a U-net shape, utilizing the patching strategy and
the joint prediction strategy that capitalizes on insights from herpangina, a
disease closely correlated with HFMD. This model also integrates representation
learning by introducing reconstruction loss as an auxiliary loss. The results
show that our U-net Patching Time Series Transformer (UPTST) model outperforms
existing approaches in both long- and short-arm prediction accuracy of HFMD at
hospital-level. Furthermore, the exploratory extension experiments show that
the model&apos;s capabilities extend beyond prediction of infectious disease,
suggesting broader applicability in various domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1&quot;&gt;Guoqi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1&quot;&gt;Hailun Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Huan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Ximing Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16108">
<title>Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words. (arXiv:2309.16108v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.16108</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformer (ViT) has emerged as a powerful architecture in the realm
of modern computer vision. However, its application in certain imaging fields,
such as microscopy and satellite imaging, presents unique challenges. In these
domains, images often contain multiple channels, each carrying semantically
distinct and independent information. Furthermore, the model must demonstrate
robustness to sparsity in input channels, as they may not be densely available
during training or testing. In this paper, we propose a modification to the ViT
architecture that enhances reasoning across the input channels and introduce
Hierarchical Channel Sampling (HCS) as an additional regularization technique
to ensure robustness when only partial channels are presented during test time.
Our proposed model, ChannelViT, constructs patch tokens independently from each
input channel and utilizes a learnable channel embedding that is added to the
patch tokens, similar to positional embeddings. We evaluate the performance of
ChannelViT on ImageNet, JUMP-CP (microscopy cell imaging), and So2Sat
(satellite imaging). Our results show that ChannelViT outperforms ViT on
classification tasks and generalizes well, even when a subset of input channels
is used during testing. Across our experiments, HCS proves to be a powerful
regularizer, independent of the architecture employed, suggesting itself as a
straightforward technique for robust ViT training. Lastly, we find that
ChannelViT generalizes effectively even when there is limited access to all
channels during training, highlighting its potential for multi-channel imaging
under real-world conditions with sparse sensors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1&quot;&gt;Yujia Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sivanandan_S/0/1/0/all/0/1&quot;&gt;Srinivasan Sivanandan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karaletsos_T/0/1/0/all/0/1&quot;&gt;Theofanis Karaletsos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.17255">
<title>Knowledge Graphs for the Life Sciences: Recent Developments, Challenges and Opportunities. (arXiv:2309.17255v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2309.17255</link>
<description rdf:parseType="Literal">&lt;p&gt;The term life sciences refers to the disciplines that study living organisms
and life processes, and include chemistry, biology, medicine, and a range of
other related disciplines. Research efforts in life sciences are heavily
data-driven, as they produce and consume vast amounts of scientific data, much
of which is intrinsically relational and graph-structured.
&lt;/p&gt;
&lt;p&gt;The volume of data and the complexity of scientific concepts and relations
referred to therein promote the application of advanced knowledge-driven
technologies for managing and interpreting data, with the ultimate aim to
advance scientific discovery.
&lt;/p&gt;
&lt;p&gt;In this survey and position paper, we discuss recent developments and
advances in the use of graph-based technologies in life sciences and set out a
vision for how these technologies will impact these fields into the future. We
focus on three broad topics: the construction and management of Knowledge
Graphs (KGs), the use of KGs and associated technologies in the discovery of
new knowledge, and the use of KGs in artificial intelligence applications to
support explanations (explainable AI). We select a few exemplary use cases for
each topic, discuss the challenges and open research questions within these
topics, and conclude with a perspective and outlook that summarizes the
overarching challenges and their potential solutions as a guide for future
research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiaoyan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hang Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hastings_J/0/1/0/all/0/1&quot;&gt;Janna Hastings&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jimenez_Ruiz_E/0/1/0/all/0/1&quot;&gt;Ernesto Jim&amp;#xe9;nez-Ruiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopez_V/0/1/0/all/0/1&quot;&gt;Vanessa L&amp;#xf3;pez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monnin_P/0/1/0/all/0/1&quot;&gt;Pierre Monnin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pesquita_C/0/1/0/all/0/1&quot;&gt;Catia Pesquita&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skoda_P/0/1/0/all/0/1&quot;&gt;Petr &amp;#x160;koda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tamma_V/0/1/0/all/0/1&quot;&gt;Valentina Tamma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.17277">
<title>Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT-4. (arXiv:2309.17277v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2309.17277</link>
<description rdf:parseType="Literal">&lt;p&gt;Unlike perfect information games, where all elements are known to every
player, imperfect information games emulate the real-world complexities of
decision-making under uncertain or incomplete information. GPT-4, the recent
breakthrough in large language models (LLMs) trained on massive passive data,
is notable for its knowledge retrieval and reasoning abilities. This paper
delves into the applicability of GPT-4&apos;s learned knowledge for imperfect
information games. To achieve this, we introduce \textbf{Suspicion-Agent}, an
innovative agent that leverages GPT-4&apos;s capabilities for performing in
imperfect information games. With proper prompt engineering to achieve
different functions, Suspicion-Agent based on GPT-4 demonstrates remarkable
adaptability across a range of imperfect information card games. Importantly,
GPT-4 displays a strong high-order theory of mind (ToM) capacity, meaning it
can understand others and intentionally impact others&apos; behavior. Leveraging
this, we design a planning strategy that enables GPT-4 to competently play
against different opponents, adapting its gameplay style as needed, while
requiring only the game rules and descriptions of observations as input. In the
experiments, we qualitatively showcase the capabilities of Suspicion-Agent
across three different imperfect information games and then quantitatively
evaluate it in Leduc Hold&apos;em. The results show that Suspicion-Agent can
potentially outperform traditional algorithms designed for imperfect
information games, without any specialized training or examples. In order to
encourage and foster deeper insights within the community, we make our
game-related data publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jiaxian Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_P/0/1/0/all/0/1&quot;&gt;Paul Yoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Bill Yuchen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iwasawa_Y/0/1/0/all/0/1&quot;&gt;Yusuke Iwasawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matsuo_Y/0/1/0/all/0/1&quot;&gt;Yutaka Matsuo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00100">
<title>Multilingual Natural Language Processing Model for Radiology Reports -- The Summary is all you need!. (arXiv:2310.00100v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00100</link>
<description rdf:parseType="Literal">&lt;p&gt;The impression section of a radiology report summarizes important radiology
findings and plays a critical role in communicating these findings to
physicians. However, the preparation of these summaries is time-consuming and
error-prone for radiologists. Recently, numerous models for radiology report
summarization have been developed. Nevertheless, there is currently no model
that can summarize these reports in multiple languages. Such a model could
greatly improve future research and the development of Deep Learning models
that incorporate data from patients with different ethnic backgrounds. In this
study, the generation of radiology impressions in different languages was
automated by fine-tuning a model, publicly available, based on a multilingual
text-to-text Transformer to summarize findings available in English,
Portuguese, and German radiology reports. In a blind test, two board-certified
radiologists indicated that for at least 70% of the system-generated summaries,
the quality matched or exceeded the corresponding human-written summaries,
suggesting substantial clinical reliability. Furthermore, this study showed
that the multilingual model outperformed other models that specialized in
summarizing radiology reports in only one language, as well as models that were
not specifically designed for summarizing radiology reports, such as ChatGPT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lindo_M/0/1/0/all/0/1&quot;&gt;Mariana Lindo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_A/0/1/0/all/0/1&quot;&gt;Ana Sofia Santos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferreira_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Ferreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianning Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luijten_G/0/1/0/all/0/1&quot;&gt;Gijs Luijten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Correia_G/0/1/0/all/0/1&quot;&gt;Gustavo Correia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Moon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleesiek_J/0/1/0/all/0/1&quot;&gt;Jens Kleesiek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Egger_J/0/1/0/all/0/1&quot;&gt;Jan Egger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alves_V/0/1/0/all/0/1&quot;&gt;Victor Alves&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00771">
<title>Pre-training with Synthetic Data Helps Offline Reinforcement Learning. (arXiv:2310.00771v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00771</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, it has been shown that for offline deep reinforcement learning
(DRL), pre-training Decision Transformer with a large language corpus can
improve downstream performance (Reid et al., 2022). A natural question to ask
is whether this performance gain can only be achieved with language
pre-training, or can be achieved with simpler pre-training schemes which do not
involve language. In this paper, we first show that language is not essential
for improved performance, and indeed pre-training with synthetic IID data for a
small number of updates can match the performance gains from pre-training with
a large language corpus; moreover, pre-training with data generated by a
one-step Markov chain can further improve the performance. Inspired by these
experimental results, we then consider pre-training Conservative Q-Learning
(CQL), a popular offline DRL algorithm, which is Q-learning-based and typically
employs a Multi-Layer Perceptron (MLP) backbone. Surprisingly, pre-training
with simple synthetic data for a small number of updates can also improve CQL,
providing consistent performance improvement on D4RL Gym locomotion datasets.
The results of this paper not only illustrate the importance of pre-training
for offline DRL but also show that the pre-training data can be synthetic and
generated with remarkably simple mechanisms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zecheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Che Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1&quot;&gt;Zixuan Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ross_K/0/1/0/all/0/1&quot;&gt;Keith Ross&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00785">
<title>BooookScore: A systematic exploration of book-length summarization in the era of LLMs. (arXiv:2310.00785v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00785</link>
<description rdf:parseType="Literal">&lt;p&gt;Summarizing book-length documents (&amp;gt;100K tokens) that exceed the context
window size of large language models (LLMs) requires first breaking the input
document into smaller chunks and then prompting an LLM to merge, update, and
compress chunk-level summaries. Despite the complexity and importance of this
task, it has yet to be meaningfully studied due to the challenges of
evaluation: existing book-length summarization datasets (e.g., BookSum) are in
the pretraining data of most public LLMs, and existing evaluation methods
struggle to capture errors made by modern LLM summarizers. In this paper, we
present the first study of the coherence of LLM-based book-length summarizers
implemented via two prompting workflows: (1) hierarchically merging chunk-level
summaries, and (2) incrementally updating a running summary. We obtain 1193
fine-grained human annotations on GPT-4 generated summaries of 100
recently-published books and identify eight common types of coherence errors
made by LLMs. Because human evaluation is expensive and time-consuming, we
develop an automatic metric, BooookScore, that measures the proportion of
sentences in a summary that do not contain any of the identified error types.
BooookScore has high agreement with human annotations and allows us to
systematically evaluate the impact of many other critical parameters (e.g.,
chunk size, base LLM) while saving $15K and 500 hours in human evaluation
costs. We find that closed-source LLMs such as GPT-4 and Claude 2 produce
summaries with higher BooookScore than the oft-repetitive ones generated by
LLaMA 2. Incremental updating yields lower BooookScore but higher level of
detail than hierarchical merging, a trade-off sometimes preferred by human
annotators. We release code and annotations after blind review to spur more
principled research on book-length summarization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1&quot;&gt;Yapei Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1&quot;&gt;Kyle Lo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_T/0/1/0/all/0/1&quot;&gt;Tanya Goyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1&quot;&gt;Mohit Iyyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01320">
<title>Avalon&apos;s Game of Thoughts: Battle Against Deception through Recursive Contemplation. (arXiv:2310.01320v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01320</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent breakthroughs in large language models (LLMs) have brought remarkable
success in the field of LLM-as-Agent. Nevertheless, a prevalent assumption is
that the information processed by LLMs is consistently honest, neglecting the
pervasive deceptive or misleading information in human society and AI-generated
content. This oversight makes LLMs susceptible to malicious manipulations,
potentially resulting in detrimental outcomes. This study utilizes the
intricate Avalon game as a testbed to explore LLMs&apos; potential in deceptive
environments. Avalon, full of misinformation and requiring sophisticated logic,
manifests as a &quot;Game-of-Thoughts&quot;. Inspired by the efficacy of humans&apos;
recursive thinking and perspective-taking in the Avalon game, we introduce a
novel framework, Recursive Contemplation (ReCon), to enhance LLMs&apos; ability to
identify and counteract deceptive information. ReCon combines formulation and
refinement contemplation processes; formulation contemplation produces initial
thoughts and speech, while refinement contemplation further polishes them.
Additionally, we incorporate first-order and second-order perspective
transitions into these processes respectively. Specifically, the first-order
allows an LLM agent to infer others&apos; mental states, and the second-order
involves understanding how others perceive the agent&apos;s mental state. After
integrating ReCon with different LLMs, extensive experiment results from the
Avalon game indicate its efficacy in aiding LLMs to discern and maneuver around
deceptive information without extra fine-tuning and data. Finally, we offer a
possible explanation for the efficacy of ReCon and explore the current
limitations of LLMs in terms of safety, reasoning, speaking style, and format,
potentially furnishing insights for subsequent research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shenzhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1&quot;&gt;Zilong Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_S/0/1/0/all/0/1&quot;&gt;Siyuan Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qisen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_A/0/1/0/all/0/1&quot;&gt;Andrew Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chaofei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Shiji Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1&quot;&gt;Gao Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02239">
<title>MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens. (arXiv:2310.02239v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02239</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have garnered significant attention for their
advancements in natural language processing, demonstrating unparalleled prowess
in text comprehension and generation. Yet, the simultaneous generation of
images with coherent textual narratives remains an evolving frontier. In
response, we introduce an innovative interleaved vision-and-language generation
technique anchored by the concept of &quot;generative vokens,&quot; acting as the bridge
for harmonized image-text outputs. Our approach is characterized by a
distinctive two-staged training strategy focusing on description-free
multimodal generation, where the training requires no comprehensive
descriptions of images. To bolster model integrity, classifier-free guidance is
incorporated, enhancing the effectiveness of vokens on image generation. Our
model, MiniGPT-5, exhibits substantial improvement over the baseline Divter
model on the MMDialog dataset and consistently delivers superior or comparable
multimodal outputs in human evaluations on the VIST dataset, highlighting its
efficacy across diverse benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1&quot;&gt;Kaizhi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xuehai He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Eric Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02520">
<title>MedDiffusion: Boosting Health Risk Prediction via Diffusion-based Data Augmentation. (arXiv:2310.02520v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02520</link>
<description rdf:parseType="Literal">&lt;p&gt;Health risk prediction is one of the fundamental tasks under predictive
modeling in the medical domain, which aims to forecast the potential health
risks that patients may face in the future using their historical Electronic
Health Records (EHR). Researchers have developed several risk prediction models
to handle the unique challenges of EHR data, such as its sequential nature,
high dimensionality, and inherent noise. These models have yielded impressive
results. Nonetheless, a key issue undermining their effectiveness is data
insufficiency. A variety of data generation and augmentation methods have been
introduced to mitigate this issue by expanding the size of the training data
set through the learning of underlying data distributions. However, the
performance of these methods is often limited due to their task-unrelated
design. To address these shortcomings, this paper introduces a novel,
end-to-end diffusion-based risk prediction model, named MedDiffusion. It
enhances risk prediction performance by creating synthetic patient data during
training to enlarge sample space. Furthermore, MedDiffusion discerns hidden
relationships between patient visits using a step-wise attention mechanism,
enabling the model to automatically retain the most vital information for
generating high-quality data. Experimental evaluation on four real-world
medical datasets demonstrates that MedDiffusion outperforms 14 cutting-edge
baselines in terms of PR-AUC, F1, and Cohen&apos;s Kappa. We also conduct ablation
studies and benchmark our model against GAN-based alternatives to further
validate the rationality and adaptability of our model design. Additionally, we
analyze generated data to offer fresh insights into the model&apos;s
interpretability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yuan Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1&quot;&gt;Suhan Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiaqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaochen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Ziyi Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaqing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1&quot;&gt;Houping Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huai_M/0/1/0/all/0/1&quot;&gt;Mengdi Huai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Ting Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1&quot;&gt;Fenglong Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02687">
<title>USB-NeRF: Unrolling Shutter Bundle Adjusted Neural Radiance Fields. (arXiv:2310.02687v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02687</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Fields (NeRF) has received much attention recently due to its
impressive capability to represent 3D scene and synthesize novel view images.
Existing works usually assume that the input images are captured by a global
shutter camera. Thus, rolling shutter (RS) images cannot be trivially applied
to an off-the-shelf NeRF algorithm for novel view synthesis. Rolling shutter
effect would also affect the accuracy of the camera pose estimation (e.g. via
COLMAP), which further prevents the success of NeRF algorithm with RS images.
In this paper, we propose Unrolling Shutter Bundle Adjusted Neural Radiance
Fields (USB-NeRF). USB-NeRF is able to correct rolling shutter distortions and
recover accurate camera motion trajectory simultaneously under the framework of
NeRF, by modeling the physical image formation process of a RS camera.
Experimental results demonstrate that USB-NeRF achieves better performance
compared to prior works, in terms of RS effect removal, novel view image
synthesis as well as camera motion estimation. Furthermore, our algorithm can
also be used to recover high-fidelity high frame-rate global shutter video from
a sequence of RS images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Moyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Lingzhe Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_B/0/1/0/all/0/1&quot;&gt;Bangyan Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1&quot;&gt;Peidong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02842">
<title>Sweeping Heterogeneity with Smart MoPs: Mixture of Prompts for LLM Task Adaptation. (arXiv:2310.02842v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02842</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have the ability to solve a variety of tasks,
such as text summarization and mathematical questions, just out of the box, but
they are often trained with a single task in mind. Due to high computational
costs, the current trend is to use prompt instruction tuning to better adjust
monolithic, pretrained LLMs for new -- but often individual -- downstream
tasks. Thus, how one would expand prompt tuning to handle -- concomitantly --
heterogeneous tasks and data distributions is a widely open question. To
address this gap, we suggest the use of \emph{Mixture of Prompts}, or MoPs,
associated with smart gating functionality: the latter -- whose design is one
of the contributions of this paper -- can identify relevant skills embedded in
different groups of prompts and dynamically assign combined experts (i.e.,
collection of prompts), based on the target task. Additionally, MoPs are
empirically agnostic to any model compression technique applied -- for
efficiency reasons -- as well as instruction data source and task composition.
In practice, MoPs can simultaneously mitigate prompt training &quot;interference&quot; in
multi-task, multi-source scenarios (e.g., task and data heterogeneity across
sources), as well as possible implications from model approximations. As a
highlight, MoPs manage to decrease final perplexity from $\sim20\%$ up to
$\sim70\%$, as compared to baselines, in the federated scenario, and from $\sim
3\%$ up to $\sim30\%$ in the centralized scenario.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dun_C/0/1/0/all/0/1&quot;&gt;Chen Dun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_M/0/1/0/all/0/1&quot;&gt;Mirian Hipolito Garcia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1&quot;&gt;Guoqing Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1&quot;&gt;Ahmed Hassan Awadallah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kyrillidis_A/0/1/0/all/0/1&quot;&gt;Anastasios Kyrillidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sim_R/0/1/0/all/0/1&quot;&gt;Robert Sim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03149">
<title>Attributing Learned Concepts in Neural Networks to Training Data. (arXiv:2310.03149v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03149</link>
<description rdf:parseType="Literal">&lt;p&gt;By now there is substantial evidence that deep learning models learn certain
human-interpretable features as part of their internal representations of data.
As having the right (or wrong) concepts is critical to trustworthy machine
learning systems, it is natural to ask which inputs from the model&apos;s original
training set were most important for learning a concept at a given layer. To
answer this, we combine data attribution methods with methods for probing the
concepts learned by a model. Training network and probe ensembles for two
concept datasets on a range of network layers, we use the recently developed
TRAK method for large-scale data attribution. We find some evidence for
convergence, where removing the 10,000 top attributing images for a concept and
retraining the model does not change the location of the concept in the network
nor the probing sparsity of the concept. This suggests that rather than being
highly dependent on a few specific examples, the features that inform the
development of a concept are spread in a more diffuse manner across its
exemplars, implying robustness in concept formation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konz_N/0/1/0/all/0/1&quot;&gt;Nicholas Konz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Godfrey_C/0/1/0/all/0/1&quot;&gt;Charles Godfrey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shapiro_M/0/1/0/all/0/1&quot;&gt;Madelyn Shapiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_J/0/1/0/all/0/1&quot;&gt;Jonathan Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kvinge_H/0/1/0/all/0/1&quot;&gt;Henry Kvinge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1&quot;&gt;Davis Brown&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03205">
<title>A Large-Scale 3D Face Mesh Video Dataset via Neural Re-parameterized Optimization. (arXiv:2310.03205v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03205</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose NeuFace, a 3D face mesh pseudo annotation method on videos via
neural re-parameterized optimization. Despite the huge progress in 3D face
reconstruction methods, generating reliable 3D face labels for in-the-wild
dynamic videos remains challenging. Using NeuFace optimization, we annotate the
per-view/-frame accurate and consistent face meshes on large-scale face videos,
called the NeuFace-dataset. We investigate how neural re-parameterization helps
to reconstruct image-aligned facial details on 3D meshes via gradient analysis.
By exploiting the naturalness and diversity of 3D faces in our dataset, we
demonstrate the usefulness of our dataset for 3D face-related tasks: improving
the reconstruction accuracy of an existing 3D face reconstruction model and
learning 3D facial motion prior. Code and datasets will be available at
https://neuface-dataset.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Youwang_K/0/1/0/all/0/1&quot;&gt;Kim Youwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hyun_L/0/1/0/all/0/1&quot;&gt;Lee Hyun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sung_Bin_K/0/1/0/all/0/1&quot;&gt;Kim Sung-Bin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nam_S/0/1/0/all/0/1&quot;&gt;Suekyeong Nam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ju_J/0/1/0/all/0/1&quot;&gt;Janghoon Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_T/0/1/0/all/0/1&quot;&gt;Tae-Hyun Oh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03281">
<title>A 5&apos; UTR Language Model for Decoding Untranslated Regions of mRNA and Function Predictions. (arXiv:2310.03281v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03281</link>
<description rdf:parseType="Literal">&lt;p&gt;The 5&apos; UTR, a regulatory region at the beginning of an mRNA molecule, plays a
crucial role in regulating the translation process and impacts the protein
expression level. Language models have showcased their effectiveness in
decoding the functions of protein and genome sequences. Here, we introduced a
language model for 5&apos; UTR, which we refer to as the UTR-LM. The UTR-LM is
pre-trained on endogenous 5&apos; UTRs from multiple species and is further
augmented with supervised information including secondary structure and minimum
free energy. We fine-tuned the UTR-LM in a variety of downstream tasks. The
model outperformed the best-known benchmark by up to 42% for predicting the
Mean Ribosome Loading, and by up to 60% for predicting the Translation
Efficiency and the mRNA Expression Level. The model also applies to identifying
unannotated Internal Ribosome Entry Sites within the untranslated region and
improves the AUPR from 0.37 to 0.52 compared to the best baseline. Further, we
designed a library of 211 novel 5&apos; UTRs with high predicted values of
translation efficiency and evaluated them via a wet-lab assay. Experiment
results confirmed that our top designs achieved a 32.5% increase in protein
production level relative to well-established 5&apos; UTR optimized for
therapeutics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_Y/0/1/0/all/0/1&quot;&gt;Yanyi Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1&quot;&gt;Dan Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yupeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kaixuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yue Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cong_L/0/1/0/all/0/1&quot;&gt;Le Cong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jason Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengdi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03605">
<title>FASER: Binary Code Similarity Search through the use of Intermediate Representations. (arXiv:2310.03605v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03605</link>
<description rdf:parseType="Literal">&lt;p&gt;Being able to identify functions of interest in cross-architecture software
is useful whether you are analysing for malware, securing the software supply
chain or conducting vulnerability research. Cross-Architecture Binary Code
Similarity Search has been explored in numerous studies and has used a wide
range of different data sources to achieve its goals. The data sources
typically used draw on common structures derived from binaries such as function
control flow graphs or binary level call graphs, the output of the disassembly
process or the outputs of a dynamic analysis approach. One data source which
has received less attention is binary intermediate representations. Binary
Intermediate representations possess two interesting properties: they are cross
architecture by their very nature and encode the semantics of a function
explicitly to support downstream usage. Within this paper we propose Function
as a String Encoded Representation (FASER) which combines long document
transformers with the use of intermediate representations to create a model
capable of cross architecture function search without the need for manual
feature engineering, pre-training or a dynamic analysis step. We compare our
approach against a series of baseline approaches for two tasks; A general
function search task and a targeted vulnerability search task. Our approach
demonstrates strong performance across both tasks, performing better than all
baseline approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Collyer_J/0/1/0/all/0/1&quot;&gt;Josh Collyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watson_T/0/1/0/all/0/1&quot;&gt;Tim Watson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phillips_I/0/1/0/all/0/1&quot;&gt;Iain Phillips&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.05608">
<title>Differentiable Outlier Detection Enable Robust Deep Multimodal Analysis. (arXiv:2302.05608v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2302.05608</link>
<description rdf:parseType="Literal">&lt;p&gt;Often, deep network models are purely inductive during training and while
performing inference on unseen data. Thus, when such models are used for
predictions, it is well known that they often fail to capture the semantic
information and implicit dependencies that exist among objects (or concepts) on
a population level. Moreover, it is still unclear how domain or prior modal
knowledge can be specified in a backpropagation friendly manner, especially in
large-scale and noisy settings. In this work, we propose an end-to-end vision
and language model incorporating explicit knowledge graphs. We also introduce
an interactive out-of-distribution (OOD) layer using implicit network operator.
The layer is used to filter noise that is brought by external knowledge base.
In practice, we apply our model on several vision and language downstream tasks
including visual question answering, visual reasoning, and image-text retrieval
on different datasets. Our experiments show that it is possible to design
models that perform similarly to state-of-art results but with significantly
fewer samples and training time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Medya_S/0/1/0/all/0/1&quot;&gt;Sourav Medya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1&quot;&gt;Sathya N. Ravi&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>