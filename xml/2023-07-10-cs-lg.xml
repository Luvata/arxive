<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.LG updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Machine Learning (cs.LG) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-09T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03197" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03201" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03206" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03210" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03211" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03212" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03217" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03223" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03254" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03266" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03270" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03288" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03290" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03298" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03305" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03306" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03311" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03315" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03323" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03327" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03332" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03334" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03337" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03347" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03357" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03359" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03360" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03363" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03364" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03374" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03377" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03380" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03381" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03385" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03393" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03406" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03410" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03411" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03423" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03427" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03430" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03476" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03486" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03487" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03493" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03494" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03500" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03515" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03544" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03550" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03565" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03571" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03576" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03577" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03586" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03587" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03590" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03595" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03630" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03641" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03672" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03675" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03678" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03679" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03681" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03683" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03690" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03694" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03704" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03712" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03716" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03719" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03723" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03738" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03748" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03750" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2001.04413" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2004.11145" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2107.01943" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.09836" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.05745" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.03466" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.08815" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.09706" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.09928" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.13085" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.11007" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.01785" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.01888" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.04882" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.05953" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.06489" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.06573" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.09811" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.10936" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.11498" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.00719" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.10886" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.03314" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.08981" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.10289" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.10433" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.00589" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.00800" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.01483" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.16982" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.17716" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.11834" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.14102" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.07368" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17760" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04719" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05412" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10278" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.12006" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.12898" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.13264" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14275" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16050" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16873" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17258" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00125" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01166" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01879" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02150" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02484" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03003" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.03197">
<title>Analyzing the vulnerabilities in SplitFed Learning: Assessing the robustness against Data Poisoning Attacks. (arXiv:2307.03197v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03197</link>
<description rdf:parseType="Literal">&lt;p&gt;Distributed Collaborative Machine Learning (DCML) is a potential alternative
to address the privacy concerns associated with centralized machine learning.
The Split learning (SL) and Federated Learning (FL) are the two effective
learning approaches in DCML. Recently there have been an increased interest on
the hybrid of FL and SL known as the SplitFed Learning (SFL). This research is
the earliest attempt to study, analyze and present the impact of data poisoning
attacks in SFL. We propose three kinds of novel attack strategies namely
untargeted, targeted and distance-based attacks for SFL. All the attacks
strategies aim to degrade the performance of the DCML-based classifier. We test
the proposed attack strategies for two different case studies on
Electrocardiogram signal classification and automatic handwritten digit
recognition. A series of attack experiments were conducted by varying the
percentage of malicious clients and the choice of the model split layer between
the clients and the server. The results after the comprehensive analysis of
attack strategies clearly convey that untargeted and distance-based poisoning
attacks have greater impacts in evading the classifier outcomes compared to
targeted attacks in SFL
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ismail_A/0/1/0/all/0/1&quot;&gt;Aysha Thahsin Zahir Ismail&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shukla_R/0/1/0/all/0/1&quot;&gt;Raj Mani Shukla&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03201">
<title>Scaling Laws Do Not Scale. (arXiv:2307.03201v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03201</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work has proposed a power law relationship, referred to as ``scaling
laws,&apos;&apos; between the performance of artificial intelligence (AI) models and
aspects of those models&apos; design (e.g., dataset size). In other words, as the
size of a dataset (or model parameters, etc) increases, the performance of a
given model trained on that dataset will correspondingly increase. However,
while compelling in the aggregate, this scaling law relationship overlooks the
ways that metrics used to measure performance may be precarious and contested,
or may not correspond with how different groups of people may perceive the
quality of models&apos; output. In this paper, we argue that as the size of datasets
used to train large AI models grows, the number of distinct communities
(including demographic groups) whose data is included in a given dataset is
likely to grow, each of whom may have different values. As a result, there is
an increased risk that communities represented in a dataset may have values or
preferences not captured by (or in the worst case, at odds with) the metrics
used to evaluate model performance for scaling laws. We end the paper with
implications for AI scaling laws -- that models may not, in fact, continue to
improve as the datasets get larger -- at least not for all people or
communities impacted by those models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diaz_F/0/1/0/all/0/1&quot;&gt;Fernando Diaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madaio_M/0/1/0/all/0/1&quot;&gt;Michael Madaio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03206">
<title>Optimal Bandwidth Selection for DENCLUE. (arXiv:2307.03206v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03206</link>
<description rdf:parseType="Literal">&lt;p&gt;In modern day industry, clustering algorithms are daily routines of algorithm
engineers. Although clustering algorithms experienced rapid growth before 2010.
Innovation related to the research topic has stagnated after deep learning
became the de facto industrial standard for machine learning applications. In
2007, a density-based clustering algorithm named DENCLUE was invented to solve
clustering problem for nonlinear data structures. However, its parameter
selection problem was largely neglected until 2011. In this paper, we propose a
new approach to compute the optimal parameters for the DENCLUE algorithm, and
discuss its performance in the experiment section.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03210">
<title>Sparse Graphical Linear Dynamical Systems. (arXiv:2307.03210v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03210</link>
<description rdf:parseType="Literal">&lt;p&gt;Time-series datasets are central in numerous fields of science and
engineering, such as biomedicine, Earth observation, and network analysis.
Extensive research exists on state-space models (SSMs), which are powerful
mathematical tools that allow for probabilistic and interpretable learning on
time series. Estimating the model parameters in SSMs is arguably one of the
most complicated tasks, and the inclusion of prior knowledge is known to both
ease the interpretation but also to complicate the inferential tasks. Very
recent works have attempted to incorporate a graphical perspective on some of
those model parameters, but they present notable limitations that this work
addresses. More generally, existing graphical modeling tools are designed to
incorporate either static information, focusing on statistical dependencies
among independent random variables (e.g., graphical Lasso approach), or dynamic
information, emphasizing causal relationships among time series samples (e.g.,
graphical Granger approaches). However, there are no joint approaches combining
static and dynamic graphical modeling within the context of SSMs. This work
proposes a novel approach to fill this gap by introducing a joint graphical
modeling framework that bridges the static graphical Lasso model and a
causal-based graphical approach for the linear-Gaussian SSM. We present DGLASSO
(Dynamic Graphical Lasso), a new inference method within this framework that
implements an efficient block alternating majorization-minimization algorithm.
The algorithm&apos;s convergence is established by departing from modern tools from
nonlinear analysis. Experimental validation on synthetic and real weather
variability data showcases the effectiveness of the proposed model and
inference algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chouzenoux_E/0/1/0/all/0/1&quot;&gt;Emilie Chouzenoux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elvira_V/0/1/0/all/0/1&quot;&gt;Victor Elvira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03211">
<title>PseudoCell: Hard Negative Mining as Pseudo Labeling for Deep Learning-Based Centroblast Cell Detection. (arXiv:2307.03211v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/2307.03211</link>
<description rdf:parseType="Literal">&lt;p&gt;Patch classification models based on deep learning have been utilized in
whole-slide images (WSI) of H&amp;amp;E-stained tissue samples to assist pathologists
in grading follicular lymphoma patients. However, these approaches still
require pathologists to manually identify centroblast cells and provide refined
labels for optimal performance. To address this, we propose PseudoCell, an
object detection framework to automate centroblast detection in WSI (source
code is available at https://github.com/IoBT-VISTEC/PseudoCell.git). This
framework incorporates centroblast labels from pathologists and combines them
with pseudo-negative labels obtained from undersampled false-positive
predictions using the cell&apos;s morphological features. By employing PseudoCell,
pathologists&apos; workload can be reduced as it accurately narrows down the areas
requiring their attention during examining tissue. Depending on the confidence
threshold, PseudoCell can eliminate 58.18-99.35% of non-centroblasts tissue
areas on WSI. This study presents a practical centroblast prescreening method
that does not require pathologists&apos; refined labels for improvement. Detailed
guidance on the practical implementation of PseudoCell is provided in the
discussion section.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Seesawad_N/0/1/0/all/0/1&quot;&gt;Narongrid Seesawad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ittichaiwong_P/0/1/0/all/0/1&quot;&gt;Piyalitt Ittichaiwong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Sudhawiyangkul_T/0/1/0/all/0/1&quot;&gt;Thapanun Sudhawiyangkul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Sawangjai_P/0/1/0/all/0/1&quot;&gt;Phattarapong Sawangjai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Thuwajit_P/0/1/0/all/0/1&quot;&gt;Peti Thuwajit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Boonsakan_P/0/1/0/all/0/1&quot;&gt;Paisarn Boonsakan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Sripodok_S/0/1/0/all/0/1&quot;&gt;Supasan Sripodok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Veerakanjana_K/0/1/0/all/0/1&quot;&gt;Kanyakorn Veerakanjana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Luenam_P/0/1/0/all/0/1&quot;&gt;Phoomraphee Luenam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Charngkaew_K/0/1/0/all/0/1&quot;&gt;Komgrid Charngkaew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Pongpaibul_A/0/1/0/all/0/1&quot;&gt;Ananya Pongpaibul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Angkathunyakul_N/0/1/0/all/0/1&quot;&gt;Napat Angkathunyakul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hnoohom_N/0/1/0/all/0/1&quot;&gt;Narit Hnoohom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Yuenyong_S/0/1/0/all/0/1&quot;&gt;Sumeth Yuenyong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Thuwajit_C/0/1/0/all/0/1&quot;&gt;Chanitra Thuwajit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wilaiprasitporn_T/0/1/0/all/0/1&quot;&gt;Theerawit Wilaiprasitporn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03212">
<title>Region-Wise Attentive Multi-View Representation Learning for Urban Region Embeddings. (arXiv:2307.03212v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03212</link>
<description rdf:parseType="Literal">&lt;p&gt;Urban region embedding is an important and yet highly challenging issue due
to the complexity and constantly changing nature of urban data. To address the
challenges, we propose a Region-Wise Multi-View Representation Learning (ROMER)
to capture multi-view dependencies and learn expressive representations of
urban regions without the constraints of rigid neighbourhood region conditions.
Our model focus on learn urban region representation from multi-source urban
data. First, we capture the multi-view correlations from mobility flow
patterns, POI semantics and check-in dynamics. Then, we adopt global graph
attention networks to learn similarity of any two vertices in graphs. To
comprehensively consider and share features of multiple views, a two-stage
fusion module is further proposed to learn weights with external attention to
fuse multi-view embeddings. Extensive experiments for two downstream tasks on
real-world datasets demonstrate that our model outperforms state-of-the-art
methods by up to 17\% improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_W/0/1/0/all/0/1&quot;&gt;Weiliang Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Q/0/1/0/all/0/1&quot;&gt;Qianqian Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03217">
<title>Quantification of Uncertainty with Adversarial Models. (arXiv:2307.03217v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03217</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantifying uncertainty is important for actionable predictions in real-world
applications. A crucial part of predictive uncertainty quantification is the
estimation of epistemic uncertainty, which is defined as an integral of the
product between a divergence function and the posterior. Current methods such
as Deep Ensembles or MC dropout underperform at estimating the epistemic
uncertainty, since they primarily consider the posterior when sampling models.
We suggest Quantification of Uncertainty with Adversarial Models (QUAM) to
better estimate the epistemic uncertainty. QUAM identifies regions where the
whole product under the integral is large, not just the posterior.
Consequently, QUAM has lower approximation error of the epistemic uncertainty
compared to previous methods. Models for which the product is large correspond
to adversarial models (not adversarial examples!). Adversarial models have both
a high posterior as well as a high divergence between their predictions and
that of a reference model. Our experiments show that QUAM excels in capturing
epistemic uncertainty for deep learning models and outperforms previous methods
on challenging tasks in the vision domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schweighofer_K/0/1/0/all/0/1&quot;&gt;Kajetan Schweighofer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aichberger_L/0/1/0/all/0/1&quot;&gt;Lukas Aichberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ielanskyi_M/0/1/0/all/0/1&quot;&gt;Mykyta Ielanskyi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klambauer_G/0/1/0/all/0/1&quot;&gt;G&amp;#xfc;nter Klambauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hochreiter_S/0/1/0/all/0/1&quot;&gt;Sepp Hochreiter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03223">
<title>Neural Network Field Theories: Non-Gaussianity, Actions, and Locality. (arXiv:2307.03223v1 [hep-th])</title>
<link>http://arxiv.org/abs/2307.03223</link>
<description rdf:parseType="Literal">&lt;p&gt;Both the path integral measure in field theory and ensembles of neural
networks describe distributions over functions. When the central limit theorem
can be applied in the infinite-width (infinite-$N$) limit, the ensemble of
networks corresponds to a free field theory. Although an expansion in $1/N$
corresponds to interactions in the field theory, others, such as in a small
breaking of the statistical independence of network parameters, can also lead
to interacting theories. These other expansions can be advantageous over the
$1/N$-expansion, for example by improved behavior with respect to the universal
approximation theorem. Given the connected correlators of a field theory, one
can systematically reconstruct the action order-by-order in the expansion
parameter, using a new Feynman diagram prescription whose vertices are the
connected correlators. This method is motivated by the Edgeworth expansion and
allows one to derive actions for neural network field theories. Conversely, the
correspondence allows one to engineer architectures realizing a given field
theory by representing action deformations as deformations of neural network
parameter densities. As an example, $\phi^4$ theory is realized as an
infinite-$N$ neural network field theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/hep-th/1/au:+Demirtas_M/0/1/0/all/0/1&quot;&gt;Mehmet Demirtas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-th/1/au:+Halverson_J/0/1/0/all/0/1&quot;&gt;James Halverson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-th/1/au:+Maiti_A/0/1/0/all/0/1&quot;&gt;Anindita Maiti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-th/1/au:+Schwartz_M/0/1/0/all/0/1&quot;&gt;Matthew D. Schwartz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-th/1/au:+Stoner_K/0/1/0/all/0/1&quot;&gt;Keegan Stoner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03254">
<title>Vision Language Transformers: A Survey. (arXiv:2307.03254v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03254</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision language tasks, such as answering questions about or generating
captions that describe an image, are difficult tasks for computers to perform.
A relatively recent body of research has adapted the pretrained transformer
architecture introduced in \citet{vaswani2017attention} to vision language
modeling. Transformer models have greatly improved performance and versatility
over previous vision language models. They do so by pretraining models on a
large generic datasets and transferring their learning to new tasks with minor
changes in architecture and parameter values. This type of transfer learning
has become the standard modeling practice in both natural language processing
and computer vision. Vision language transformers offer the promise of
producing similar advancements in tasks which require both vision and language.
In this paper, we provide a broad synthesis of the currently available research
on vision language transformer models and offer some analysis of their
strengths, limitations and some open questions that remain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fields_C/0/1/0/all/0/1&quot;&gt;Clayton Fields&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kennington_C/0/1/0/all/0/1&quot;&gt;Casey Kennington&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03266">
<title>Empirical Analysis of a Segmentation Foundation Model in Prostate Imaging. (arXiv:2307.03266v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.03266</link>
<description rdf:parseType="Literal">&lt;p&gt;Most state-of-the-art techniques for medical image segmentation rely on
deep-learning models. These models, however, are often trained on
narrowly-defined tasks in a supervised fashion, which requires expensive
labeled datasets. Recent advances in several machine learning domains, such as
natural language generation have demonstrated the feasibility and utility of
building foundation models that can be customized for various downstream tasks
with little to no labeled data. This likely represents a paradigm shift for
medical imaging, where we expect that foundation models may shape the future of
the field. In this paper, we consider a recently developed foundation model for
medical image segmentation, UniverSeg. We conduct an empirical evaluation study
in the context of prostate imaging and compare it against the conventional
approach of training a task-specific segmentation model. Our results and
discussion highlight several important factors that will likely be important in
the development and adoption of foundation models for medical image
segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Heejong Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Butoi_V/0/1/0/all/0/1&quot;&gt;Victor Ion Butoi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dalca_A/0/1/0/all/0/1&quot;&gt;Adrian V. Dalca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sabuncu_M/0/1/0/all/0/1&quot;&gt;Mert R. Sabuncu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03270">
<title>A Comprehensive Multi-scale Approach for Speech and Dynamics Synchrony in Talking Head Generation. (arXiv:2307.03270v1 [cs.GR])</title>
<link>http://arxiv.org/abs/2307.03270</link>
<description rdf:parseType="Literal">&lt;p&gt;Animating still face images with deep generative models using a speech input
signal is an active research topic and has seen important recent progress.
However, much of the effort has been put into lip syncing and rendering quality
while the generation of natural head motion, let alone the audio-visual
correlation between head motion and speech, has often been neglected. In this
work, we propose a multi-scale audio-visual synchrony loss and a multi-scale
autoregressive GAN to better handle short and long-term correlation between
speech and the dynamics of the head and lips. In particular, we train a stack
of syncer models on multimodal input pyramids and use these models as guidance
in a multi-scale generator network to produce audio-aligned motion unfolding
over diverse time scales. Our generator operates in the facial landmark domain,
which is a standard low-dimensional head representation. The experiments show
significant improvements over the state of the art in head motion dynamics
quality and in multi-scale audio-visual synchrony both in the landmark domain
and in the image domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Airale_L/0/1/0/all/0/1&quot;&gt;Louis Airale&lt;/a&gt; (UGA, LIG), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaufreydaz_D/0/1/0/all/0/1&quot;&gt;Dominique Vaufreydaz&lt;/a&gt; (LIG), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1&quot;&gt;Xavier Alameda-Pineda&lt;/a&gt; (UGA)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03288">
<title>Optimal Scalarizations for Sublinear Hypervolume Regret. (arXiv:2307.03288v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03288</link>
<description rdf:parseType="Literal">&lt;p&gt;Scalarization is a general technique that can be deployed in any
multiobjective setting to reduce multiple objectives into one, such as recently
in RLHF for training reward models that align human preferences. Yet some have
dismissed this classical approach because linear scalarizations are known to
miss concave regions of the Pareto frontier. To that end, we aim to find simple
non-linear scalarizations that can explore a diverse set of $k$ objectives on
the Pareto frontier, as measured by the dominated hypervolume. We show that
hypervolume scalarizations with uniformly random weights are surprisingly
optimal for provably minimizing the hypervolume regret, achieving an optimal
sublinear regret bound of $O(T^{-1/k})$, with matching lower bounds that
preclude any algorithm from doing better asymptotically. As a theoretical case
study, we consider the multiobjective stochastic linear bandits problem and
demonstrate that by exploiting the sublinear regret bounds of the hypervolume
scalarizations, we can derive a novel non-Euclidean analysis that produces
improved hypervolume regret bounds of $\tilde{O}( d T^{-1/2} + T^{-1/k})$. We
support our theory with strong empirical performance of using simple
hypervolume scalarizations that consistently outperforms both the linear and
Chebyshev scalarizations, as well as standard multiobjective algorithms in
bayesian optimization, such as EHVI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qiuyi Zhang&lt;/a&gt; (Richard)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03290">
<title>OmniBoost: Boosting Throughput of Heterogeneous Embedded Devices under Multi-DNN Workload. (arXiv:2307.03290v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03290</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern Deep Neural Networks (DNNs) exhibit profound efficiency and accuracy
properties. This has introduced application workloads that comprise of multiple
DNN applications, raising new challenges regarding workload distribution.
Equipped with a diverse set of accelerators, newer embedded system present
architectural heterogeneity, which current run-time controllers are unable to
fully utilize. To enable high throughput in multi-DNN workloads, such a
controller is ought to explore hundreds of thousands of possible solutions to
exploit the underlying heterogeneity. In this paper, we propose OmniBoost, a
lightweight and extensible multi-DNN manager for heterogeneous embedded
devices. We leverage stochastic space exploration and we combine it with a
highly accurate performance estimator to observe a x4.6 average throughput
boost compared to other state-of-the-art methods. The evaluation was performed
on the HiKey970 development board.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karatzas_A/0/1/0/all/0/1&quot;&gt;Andreas Karatzas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anagnostopoulos_I/0/1/0/all/0/1&quot;&gt;Iraklis Anagnostopoulos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03298">
<title>Equivariant Spherical CNN for Data Efficient and High-Performance Medical Image Processing. (arXiv:2307.03298v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.03298</link>
<description rdf:parseType="Literal">&lt;p&gt;This work highlights the significance of equivariant networks as efficient
and high-performance approaches for tomography applications. Our study builds
upon the limitations of Convolutional Neural Networks (CNNs), which have shown
promise in post-processing various medical imaging systems. However, the
efficiency of conventional CNNs heavily relies on an undiminished and proper
training set. To tackle this issue, in this study, we introduce an equivariant
network, aiming to reduce CNN&apos;s dependency on specific training sets. We
evaluate the efficacy of equivariant CNNs on spherical signals for tomographic
medical imaging problems. Our results demonstrate superior quality and
computational efficiency of spherical CNNs (SCNNs) in denoising and
reconstructing benchmark problems. Furthermore, we propose a novel approach to
employ SCNNs as a complement to conventional image reconstruction tools,
enhancing the outcomes while reducing reliance on the training set. Across all
cases, we observe a significant decrease in computational costs while
maintaining the same or higher quality of image processing using SCNNs compared
to CNNs. Additionally, we explore the potential of this network for broader
tomography applications, particularly those requiring omnidirectional
representation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hashemi_A/0/1/0/all/0/1&quot;&gt;Amirreza Hashemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yuemeng Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sabet_H/0/1/0/all/0/1&quot;&gt;Hamid Sabet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03305">
<title>A Vulnerability of Attribution Methods Using Pre-Softmax Scores. (arXiv:2307.03305v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03305</link>
<description rdf:parseType="Literal">&lt;p&gt;We discuss a vulnerability involving a category of attribution methods used
to provide explanations for the outputs of convolutional neural networks
working as classifiers. It is known that this type of networks are vulnerable
to adversarial attacks, in which imperceptible perturbations of the input may
alter the outputs of the model. In contrast, here we focus on effects that
small modifications in the model may cause on the attribution method without
altering the model outputs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lerma_M/0/1/0/all/0/1&quot;&gt;Miguel Lerma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucas_M/0/1/0/all/0/1&quot;&gt;Mirtha Lucas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03306">
<title>When Fair Classification Meets Noisy Protected Attributes. (arXiv:2307.03306v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03306</link>
<description rdf:parseType="Literal">&lt;p&gt;The operationalization of algorithmic fairness comes with several practical
challenges, not the least of which is the availability or reliability of
protected attributes in datasets. In real-world contexts, practical and legal
impediments may prevent the collection and use of demographic data, making it
difficult to ensure algorithmic fairness. While initial fairness algorithms did
not consider these limitations, recent proposals aim to achieve algorithmic
fairness in classification by incorporating noisiness in protected attributes
or not using protected attributes at all.
&lt;/p&gt;
&lt;p&gt;To the best of our knowledge, this is the first head-to-head study of fair
classification algorithms to compare attribute-reliant, noise-tolerant and
attribute-blind algorithms along the dual axes of predictivity and fairness. We
evaluated these algorithms via case studies on four real-world datasets and
synthetic perturbations. Our study reveals that attribute-blind and
noise-tolerant fair classifiers can potentially achieve similar level of
performance as attribute-reliant algorithms, even when protected attributes are
noisy. However, implementing them in practice requires careful nuance. Our
study provides insights into the practical implications of using fair
classification algorithms in scenarios where protected attributes are noisy or
partially available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1&quot;&gt;Avijit Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kvitca_P/0/1/0/all/0/1&quot;&gt;Pablo Kvitca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_C/0/1/0/all/0/1&quot;&gt;Christo Wilson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03311">
<title>On Invariance, Equivariance, Correlation and Convolution of Spherical Harmonic Representations for Scalar and Vectorial Data. (arXiv:2307.03311v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03311</link>
<description rdf:parseType="Literal">&lt;p&gt;The mathematical representations of data in the Spherical Harmonic (SH)
domain has recently regained increasing interest in the machine learning
community. This technical report gives an in-depth introduction to the
theoretical foundation and practical implementation of SH representations,
summarizing works on rotation invariant and equivariant features, as well as
convolutions and exact correlations of signals on spheres. In extension, these
methods are then generalized from scalar SH representations to Vectorial
Harmonics (VH), providing the same capabilities for 3d vector fields on spheres
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1&quot;&gt;Janis Keuper&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03315">
<title>Assisting Clinical Decisions for Scarcely Available Treatment via Disentangled Latent Representation. (arXiv:2307.03315v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03315</link>
<description rdf:parseType="Literal">&lt;p&gt;Extracorporeal membrane oxygenation (ECMO) is an essential life-supporting
modality for COVID-19 patients who are refractory to conventional therapies.
However, the proper treatment decision has been the subject of significant
debate and it remains controversial about who benefits from this scarcely
available and technically complex treatment option. To support clinical
decisions, it is a critical need to predict the treatment need and the
potential treatment and no-treatment responses. Targeting this clinical
challenge, we propose Treatment Variational AutoEncoder (TVAE), a novel
approach for individualized treatment analysis. TVAE is specifically designed
to address the modeling challenges like ECMO with strong treatment selection
bias and scarce treatment cases. TVAE conceptualizes the treatment decision as
a multi-scale problem. We model a patient&apos;s potential treatment assignment and
the factual and counterfactual outcomes as part of their intrinsic
characteristics that can be represented by a deep latent variable model. The
factual and counterfactual prediction errors are alleviated via a
reconstruction regularization scheme together with semi-supervision, and the
selection bias and the scarcity of treatment cases are mitigated by the
disentangled and distribution-matched latent space and the label-balancing
generative strategy. We evaluate TVAE on two real-world COVID-19 datasets: an
international dataset collected from 1651 hospitals across 63 countries, and a
institutional dataset collected from 15 hospitals. The results show that TVAE
outperforms state-of-the-art treatment effect models in predicting both the
propensity scores and factual outcomes on heterogeneous COVID-19 datasets.
Additional experiments also show TVAE outperforms the best existing models in
individual treatment effect estimation on the synthesized IHDP benchmark
dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_B/0/1/0/all/0/1&quot;&gt;Bing Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Said_A/0/1/0/all/0/1&quot;&gt;Ahmed Sameh Said&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Ziqi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hanyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1&quot;&gt;Neel Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hanqing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Payne_P/0/1/0/all/0/1&quot;&gt;Philip Payne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Chenyang Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03323">
<title>Machine Learning to detect cyber-attacks and discriminating the types of power system disturbances. (arXiv:2307.03323v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03323</link>
<description rdf:parseType="Literal">&lt;p&gt;This research proposes a machine learning-based attack detection model for
power systems, specifically targeting smart grids. By utilizing data and logs
collected from Phasor Measuring Devices (PMUs), the model aims to learn system
behaviors and effectively identify potential security boundaries. The proposed
approach involves crucial stages including dataset pre-processing, feature
selection, model creation, and evaluation. To validate our approach, we used a
dataset used, consist of 15 separate datasets obtained from different PMUs,
relay snort alarms and logs. Three machine learning models: Random Forest,
Logistic Regression, and K-Nearest Neighbour were built and evaluated using
various performance metrics. The findings indicate that the Random Forest model
achieves the highest performance with an accuracy of 90.56% in detecting power
system disturbances and has the potential in assisting operators in
decision-making processes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuyizere_D/0/1/0/all/0/1&quot;&gt;Diane Tuyizere&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ihabwikuzo_R/0/1/0/all/0/1&quot;&gt;Remy Ihabwikuzo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03327">
<title>Encoder-Decoder Networks for Self-Supervised Pretraining and Downstream Signal Bandwidth Regression on Digital Antenna Arrays. (arXiv:2307.03327v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03327</link>
<description rdf:parseType="Literal">&lt;p&gt;This work presents the first applications of self-supervised learning applied
to data from digital antenna arrays. Encoder-decoder networks are pretrained on
digital array data to perform a self-supervised noisy-reconstruction task
called channel in-painting, in which the network infers the contents of array
data that has been masked with zeros. The self-supervised step requires no
human-labeled data. The encoder architecture and weights from pretraining are
then transferred to a new network with a task-specific decoder, and the new
network is trained on a small volume of labeled data. We show that pretraining
on the unlabeled data allows the new network to perform the task of bandwidth
regression on the digital array data better than an equivalent network that is
trained on the same labeled data from random initialization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharjea_R/0/1/0/all/0/1&quot;&gt;Rajib Bhattacharjea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+West_N/0/1/0/all/0/1&quot;&gt;Nathan West&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03332">
<title>ACDNet: Attention-guided Collaborative Decision Network for Effective Medication Recommendation. (arXiv:2307.03332v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03332</link>
<description rdf:parseType="Literal">&lt;p&gt;Medication recommendation using Electronic Health Records (EHR) is
challenging due to complex medical data. Current approaches extract
longitudinal information from patient EHR to personalize recommendations.
However, existing models often lack sufficient patient representation and
overlook the importance of considering the similarity between a patient&apos;s
medication records and specific medicines. Therefore, an Attention-guided
Collaborative Decision Network (ACDNet) for medication recommendation is
proposed in this paper. Specifically, ACDNet utilizes attention mechanism and
Transformer to effectively capture patient health conditions and medication
records by modeling their historical visits at both global and local levels.
ACDNet also employs a collaborative decision framework, utilizing the
similarity between medication records and medicine representation to facilitate
the recommendation process. The experimental results on two extensive medical
datasets, MIMIC-III and MIMIC-IV, clearly demonstrate that ACDNet outperforms
state-of-the-art models in terms of Jaccard, PR-AUC, and F1 score, reaffirming
its superiority. Moreover, the ablation experiments provide solid evidence of
the effectiveness of each module in ACDNet, validating their contribution to
the overall performance. Furthermore, a detailed case study reinforces the
effectiveness of ACDNet in medication recommendation based on EHR data,
showcasing its practical value in real-world healthcare scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mi_J/0/1/0/all/0/1&quot;&gt;Jiacong Mi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zu_Y/0/1/0/all/0/1&quot;&gt;Yi Zu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhuoyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jieyue He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03334">
<title>Variational quantum regression algorithm with encoded data structure. (arXiv:2307.03334v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2307.03334</link>
<description rdf:parseType="Literal">&lt;p&gt;Variational quantum algorithms (VQAs) prevail to solve practical problems
such as combinatorial optimization, quantum chemistry simulation, quantum
machine learning, and quantum error correction on noisy quantum computers. For
variational quantum machine learning, a variational algorithm with model
interpretability built into the algorithm is yet to be exploited. In this
paper, we construct a quantum regression algorithm and identify the direct
relation of variational parameters to learned regression coefficients, while
employing a circuit that directly encodes the data in quantum amplitudes
reflecting the structure of the classical data table. The algorithm is
particularly suitable for well-connected qubits. With compressed encoding and
digital-analog gate operation, the run time complexity is logarithmically more
advantageous than that for digital 2-local gate native hardware with the number
of data entries encoded, a decent improvement in noisy intermediate-scale
quantum computers and a minor improvement for large-scale quantum computing Our
suggested method of compressed binary encoding offers a remarkable reduction in
the number of physical qubits needed when compared to the traditional
one-hot-encoding technique with the same input data. The algorithm inherently
performs linear regression but can also be used easily for nonlinear regression
by building nonlinear features into the training data. In terms of measured
cost function which distinguishes a good model from a poor one for model
training, it will be effective only when the number of features is much less
than the number of records for the encoded data structure to be observable. To
echo this finding and mitigate hardware noise in practice, the ensemble model
training from the quantum regression model learning with important feature
selection from regularization is incorporated and illustrated numerically.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;C.-C. Joseph Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Bennink_R/0/1/0/all/0/1&quot;&gt;Ryan S. Bennink&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03337">
<title>Personalized Prediction of Recurrent Stress Events Using Self-Supervised Learning on Multimodal Time-Series Data. (arXiv:2307.03337v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03337</link>
<description rdf:parseType="Literal">&lt;p&gt;Chronic stress can significantly affect physical and mental health. The
advent of wearable technology allows for the tracking of physiological signals,
potentially leading to innovative stress prediction and intervention methods.
However, challenges such as label scarcity and data heterogeneity render stress
prediction difficult in practice. To counter these issues, we have developed a
multimodal personalized stress prediction system using wearable biosignal data.
We employ self-supervised learning (SSL) to pre-train the models on each
subject&apos;s data, allowing the models to learn the baseline dynamics of the
participant&apos;s biosignals prior to fine-tuning the stress prediction task. We
test our model on the Wearable Stress and Affect Detection (WESAD) dataset,
demonstrating that our SSL models outperform non-SSL models while utilizing
less than 5% of the annotations. These results suggest that our approach can
personalize stress prediction to each user with minimal annotations. This
paradigm has the potential to enable personalized prediction of a variety of
recurring health events using complex multimodal data streams.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1&quot;&gt;Tanvir Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Washington_P/0/1/0/all/0/1&quot;&gt;Peter Washington&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03347">
<title>Distilling Universal and Joint Knowledge for Cross-Domain Model Compression on Time Series Data. (arXiv:2307.03347v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03347</link>
<description rdf:parseType="Literal">&lt;p&gt;For many real-world time series tasks, the computational complexity of
prevalent deep leaning models often hinders the deployment on resource-limited
environments (e.g., smartphones). Moreover, due to the inevitable domain shift
between model training (source) and deploying (target) stages, compressing
those deep models under cross-domain scenarios becomes more challenging.
Although some of existing works have already explored cross-domain knowledge
distillation for model compression, they are either biased to source data or
heavily tangled between source and target data. To this end, we design a novel
end-to-end framework called Universal and joint knowledge distillation (UNI-KD)
for cross-domain model compression. In particular, we propose to transfer both
the universal feature-level knowledge across source and target domains and the
joint logit-level knowledge shared by both domains from the teacher to the
student model via an adversarial learning scheme. More specifically, a
feature-domain discriminator is employed to align teacher&apos;s and student&apos;s
representations for universal knowledge transfer. A data-domain discriminator
is utilized to prioritize the domain-shared samples for joint knowledge
transfer. Extensive experimental results on four time series datasets
demonstrate the superiority of our proposed method over state-of-the-art (SOTA)
benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Min Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoli Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_K/0/1/0/all/0/1&quot;&gt;Kezhi Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhenghua Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03357">
<title>Stability and Generalization of Stochastic Compositional Gradient Descent Algorithms. (arXiv:2307.03357v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03357</link>
<description rdf:parseType="Literal">&lt;p&gt;Many machine learning tasks can be formulated as a stochastic compositional
optimization (SCO) problem such as reinforcement learning, AUC maximization,
and meta-learning, where the objective function involves a nested composition
associated with an expectation. While a significant amount of studies has been
devoted to studying the convergence behavior of SCO algorithms, there is little
work on understanding their generalization, i.e., how these learning algorithms
built from training examples would behave on future test examples. In this
paper, we provide the stability and generalization analysis of stochastic
compositional gradient descent algorithms through the lens of algorithmic
stability in the framework of statistical learning theory. Firstly, we
introduce a stability concept called compositional uniform stability and
establish its quantitative relation with generalization for SCO problems. Then,
we establish the compositional uniform stability results for two popular
stochastic compositional gradient descent algorithms, namely SCGD and SCSC.
Finally, we derive dimension-independent excess risk bounds for SCGD and SCSC
by trade-offing their stability results and optimization errors. To the best of
our knowledge, these are the first-ever-known results on stability and
generalization analysis of stochastic compositional gradient descent
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Ming Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xiyuan Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Tianbao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ying_Y/0/1/0/all/0/1&quot;&gt;Yiming Ying&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03359">
<title>CSCLog: A Component Subsequence Correlation-Aware Log Anomaly Detection Method. (arXiv:2307.03359v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03359</link>
<description rdf:parseType="Literal">&lt;p&gt;Anomaly detection based on system logs plays an important role in intelligent
operations, which is a challenging task due to the extremely complex log
patterns. Existing methods detect anomalies by capturing the sequential
dependencies in log sequences, which ignore the interactions of subsequences.
To this end, we propose CSCLog, a Component Subsequence Correlation-Aware Log
anomaly detection method, which not only captures the sequential dependencies
in subsequences, but also models the implicit correlations of subsequences.
Specifically, subsequences are extracted from log sequences based on components
and the sequential dependencies in subsequences are captured by Long Short-Term
Memory Networks (LSTMs). An implicit correlation encoder is introduced to model
the implicit correlations of subsequences adaptively. In addition, Graph
Convolution Networks (GCNs) are employed to accomplish the information
interactions of subsequences. Finally, attention mechanisms are exploited to
fuse the embeddings of all subsequences. Extensive experiments on four publicly
available log datasets demonstrate the effectiveness of CSCLog, outperforming
the best baseline by an average of 7.41% in Macro F1-Measure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Ling Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1&quot;&gt;Chaodu Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1&quot;&gt;Dachao Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Feifei Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03360">
<title>Evaluating Biased Attitude Associations of Language Models in an Intersectional Context. (arXiv:2307.03360v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2307.03360</link>
<description rdf:parseType="Literal">&lt;p&gt;Language models are trained on large-scale corpora that embed implicit biases
documented in psychology. Valence associations (pleasantness/unpleasantness) of
social groups determine the biased attitudes towards groups and concepts in
social cognition. Building on this established literature, we quantify how
social groups are valenced in English language models using a sentence template
that provides an intersectional context. We study biases related to age,
education, gender, height, intelligence, literacy, race, religion, sex, sexual
orientation, social class, and weight. We present a concept projection approach
to capture the valence subspace through contextualized word embeddings of
language models. Adapting the projection-based approach to embedding
association tests that quantify bias, we find that language models exhibit the
most biased attitudes against gender identity, social class, and sexual
orientation signals in language. We find that the largest and better-performing
model that we study is also more biased as it effectively captures bias
embedded in sociocultural data. We validate the bias evaluation method by
overperforming on an intrinsic valence evaluation task. The approach enables us
to measure complex intersectional biases as they are known to manifest in the
outputs and applications of language models that perpetuate historical biases.
Moreover, our approach contributes to design justice as it studies the
associations of groups underrepresented in language such as transgender and
homosexual individuals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabbaghi_S/0/1/0/all/0/1&quot;&gt;Shiva Omrani Sabbaghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolfe_R/0/1/0/all/0/1&quot;&gt;Robert Wolfe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caliskan_A/0/1/0/all/0/1&quot;&gt;Aylin Caliskan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03363">
<title>Federated Unlearning via Active Forgetting. (arXiv:2307.03363v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03363</link>
<description rdf:parseType="Literal">&lt;p&gt;The increasing concerns regarding the privacy of machine learning models have
catalyzed the exploration of machine unlearning, i.e., a process that removes
the influence of training data on machine learning models. This concern also
arises in the realm of federated learning, prompting researchers to address the
federated unlearning problem. However, federated unlearning remains
challenging. Existing unlearning methods can be broadly categorized into two
approaches, i.e., exact unlearning and approximate unlearning. Firstly,
implementing exact unlearning, which typically relies on the
partition-aggregation framework, in a distributed manner does not improve time
efficiency theoretically. Secondly, existing federated (approximate) unlearning
methods suffer from imprecise data influence estimation, significant
computational burden, or both. To this end, we propose a novel federated
unlearning framework based on incremental learning, which is independent of
specific models and federated settings. Our framework differs from existing
federated unlearning methods that rely on approximate retraining or data
influence estimation. Instead, we leverage new memories to overwrite old ones,
imitating the process of \textit{active forgetting} in neurology. Specifically,
the model, intended to unlearn, serves as a student model that continuously
learns from randomly initiated teacher models. To preserve catastrophic
forgetting of non-target data, we utilize elastic weight consolidation to
elastically constrain weight change. Extensive experiments on three benchmark
datasets demonstrate the efficiency and effectiveness of our proposed method.
The result of backdoor attacks demonstrates that our proposed method achieves
satisfying completeness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chaochao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1&quot;&gt;Xiaolin Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiaming Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03364">
<title>Distilled Pruning: Using Synthetic Data to Win the Lottery. (arXiv:2307.03364v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03364</link>
<description rdf:parseType="Literal">&lt;p&gt;This work introduces a novel approach to pruning deep learning models by
using distilled data. Unlike conventional strategies which primarily focus on
architectural or algorithmic optimization, our method reconsiders the role of
data in these scenarios. Distilled datasets capture essential patterns from
larger datasets, and we demonstrate how to leverage this capability to enable a
computationally efficient pruning process. Our approach can find sparse,
trainable subnetworks (a.k.a. Lottery Tickets) up to 5x faster than Iterative
Magnitude Pruning at comparable sparsity on CIFAR-10. The experimental results
highlight the potential of using distilled data for resource-efficient neural
network pruning, model compression, and neural architecture search.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McDermott_L/0/1/0/all/0/1&quot;&gt;Luke McDermott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cummings_D/0/1/0/all/0/1&quot;&gt;Daniel Cummings&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03374">
<title>STG-MTL: Scalable Task Grouping for Multi-Task Learning Using Data Map. (arXiv:2307.03374v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03374</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-Task Learning (MTL) is a powerful technique that has gained popularity
due to its performance improvement over traditional Single-Task Learning (STL).
However, MTL is often challenging because there is an exponential number of
possible task groupings, which can make it difficult to choose the best one,
and some groupings might produce performance degradation due to negative
interference between tasks. Furthermore, existing solutions are severely
suffering from scalability issues, limiting any practical application. In our
paper, we propose a new data-driven method that addresses these challenges and
provides a scalable and modular solution for classification task grouping based
on hand-crafted features, specifically Data Maps, which capture the training
behavior for each classification task during the MTL training. We experiment
with the method demonstrating its effectiveness, even on an unprecedented
number of tasks (up to 100).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sherif_A/0/1/0/all/0/1&quot;&gt;Ammar Sherif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abid_A/0/1/0/all/0/1&quot;&gt;Abubakar Abid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elattar_M/0/1/0/all/0/1&quot;&gt;Mustafa Elattar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+ElHelw_M/0/1/0/all/0/1&quot;&gt;Mohamed ElHelw&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03377">
<title>Mitigating Negative Transfer with Task Awareness for Sexism, Hate Speech, and Toxic Language Detection. (arXiv:2307.03377v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.03377</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a novelty approach to mitigate the negative transfer
problem. In the field of machine learning, the common strategy is to apply the
Single-Task Learning approach in order to train a supervised model to solve a
specific task. Training a robust model requires a lot of data and a significant
amount of computational resources, making this solution unfeasible in cases
where data are unavailable or expensive to gather. Therefore another solution,
based on the sharing of information between tasks, has been developed:
Multi-Task Learning (MTL). Despite the recent developments regarding MTL, the
problem of negative transfer has still to be solved. Negative transfer is a
phenomenon that occurs when noisy information is shared between tasks,
resulting in a drop in performance. This paper proposes a new approach to
mitigate the negative transfer problem based on the task awareness concept. The
proposed approach results in diminishing the negative transfer together with an
improvement of performance over classic MTL solution. Moreover, the proposed
approach has been implemented in two unified architectures to detect Sexism,
Hate Speech, and Toxic Language in text comments. The proposed architectures
set a new state-of-the-art both in EXIST-2021 and HatEval-2019 benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paula_A/0/1/0/all/0/1&quot;&gt;Angel Felipe Magnoss&amp;#xe3;o de Paula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosso_P/0/1/0/all/0/1&quot;&gt;Paolo Rosso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spina_D/0/1/0/all/0/1&quot;&gt;Damiano Spina&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03380">
<title>On Formal Feature Attribution and Its Approximation. (arXiv:2307.03380v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.03380</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years have witnessed the widespread use of artificial intelligence
(AI) algorithms and machine learning (ML) models. Despite their tremendous
success, a number of vital problems like ML model brittleness, their fairness,
and the lack of interpretability warrant the need for the active developments
in explainable artificial intelligence (XAI) and formal ML model verification.
The two major lines of work in XAI include feature selection methods, e.g.
Anchors, and feature attribution techniques, e.g. LIME and SHAP. Despite their
promise, most of the existing feature selection and attribution approaches are
susceptible to a range of critical issues, including explanation unsoundness
and out-of-distribution sampling. A recent formal approach to XAI (FXAI)
although serving as an alternative to the above and free of these issues
suffers from a few other limitations. For instance and besides the scalability
limitation, the formal approach is unable to tackle the feature attribution
problem. Additionally, a formal explanation despite being formally sound is
typically quite large, which hampers its applicability in practical settings.
Motivated by the above, this paper proposes a way to apply the apparatus of
formal XAI to the case of feature attribution based on formal explanation
enumeration. Formal feature attribution (FFA) is argued to be advantageous over
the existing methods, both formal and non-formal. Given the practical
complexity of the problem, the paper then proposes an efficient technique for
approximating exact FFA. Finally, it offers experimental evidence of the
effectiveness of the proposed approximate FFA in comparison to the existing
feature attribution algorithms not only in terms of feature importance and but
also in terms of their relative order.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jinqiang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ignatiev_A/0/1/0/all/0/1&quot;&gt;Alexey Ignatiev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stuckey_P/0/1/0/all/0/1&quot;&gt;Peter J. Stuckey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03381">
<title>Teaching Arithmetic to Small Transformers. (arXiv:2307.03381v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03381</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models like GPT-4 exhibit emergent capabilities across
general-purpose tasks, such as basic arithmetic, when trained on extensive text
data, even though these tasks are not explicitly encoded by the unsupervised,
next-token prediction objective. This study investigates how small
transformers, trained from random initialization, can efficiently learn
arithmetic operations such as addition, multiplication, and elementary
functions like square root, using the next-token prediction objective. We first
demonstrate that conventional training data is not the most effective for
arithmetic learning, and simple formatting changes can significantly improve
accuracy. This leads to sharp phase transitions as a function of training data
scale, which, in some cases, can be explained through connections to low-rank
matrix completion. Building on prior work, we then train on chain-of-thought
style data that includes intermediate step results. Even in the complete
absence of pretraining, this approach significantly and simultaneously improves
accuracy, sample complexity, and convergence speed. We also study the interplay
between arithmetic and text data during training and examine the effects of
few-shot prompting, pretraining, and model scale. Additionally, we discuss
length generalization challenges. Our work highlights the importance of
high-quality, instructive data that considers the particular characteristics of
the next-word prediction objective for rapidly eliciting arithmetic
capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1&quot;&gt;Nayoung Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sreenivasan_K/0/1/0/all/0/1&quot;&gt;Kartik Sreenivasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jason D. Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kangwook Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papailiopoulos_D/0/1/0/all/0/1&quot;&gt;Dimitris Papailiopoulos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03385">
<title>AI-UPV at EXIST 2023 -- Sexism Characterization Using Large Language Models Under The Learning with Disagreements Regime. (arXiv:2307.03385v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.03385</link>
<description rdf:parseType="Literal">&lt;p&gt;With the increasing influence of social media platforms, it has become
crucial to develop automated systems capable of detecting instances of sexism
and other disrespectful and hateful behaviors to promote a more inclusive and
respectful online environment. Nevertheless, these tasks are considerably
challenging considering different hate categories and the author&apos;s intentions,
especially under the learning with disagreements regime. This paper describes
AI-UPV team&apos;s participation in the EXIST (sEXism Identification in Social
neTworks) Lab at CLEF 2023. The proposed approach aims at addressing the task
of sexism identification and characterization under the learning with
disagreements paradigm by training directly from the data with disagreements,
without using any aggregated label. Yet, performances considering both soft and
hard evaluations are reported. The proposed system uses large language models
(i.e., mBERT and XLM-RoBERTa) and ensemble strategies for sexism identification
and classification in English and Spanish. In particular, our system is
articulated in three different pipelines. The ensemble approach outperformed
the individual large language models obtaining the best performances both
adopting a soft and a hard label evaluation. This work describes the
participation in all the three EXIST tasks, considering a soft evaluation, it
obtained fourth place in Task 2 at EXIST and first place in Task 3, with the
highest ICM-Soft of -2.32 and a normalized ICM-Soft of 0.79. The source code of
our approaches is publicly available at
https://github.com/AngelFelipeMP/Sexism-LLM-Learning-With-Disagreement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paula_A/0/1/0/all/0/1&quot;&gt;Angel Felipe Magnoss&amp;#xe3;o de Paula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rizzi_G/0/1/0/all/0/1&quot;&gt;Giulia Rizzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fersini_E/0/1/0/all/0/1&quot;&gt;Elisabetta Fersini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spina_D/0/1/0/all/0/1&quot;&gt;Damiano Spina&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03393">
<title>Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs. (arXiv:2307.03393v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03393</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning on Graphs has attracted immense attention due to its wide real-world
applications. The most popular pipeline for learning on graphs with textual
node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes
shallow text embedding as initial node representations, which has limitations
in general knowledge and profound semantic understanding. In recent years,
Large Language Models (LLMs) have been proven to possess extensive common
knowledge and powerful semantic comprehension abilities that have
revolutionized existing workflows to handle text data. In this paper, we aim to
explore the potential of LLMs in graph machine learning, especially the node
classification task, and investigate two possible pipelines: LLMs-as-Enhancers
and LLMs-as-Predictors. The former leverages LLMs to enhance nodes&apos; text
attributes with their massive knowledge and then generate predictions through
GNNs. The latter attempts to directly employ LLMs as standalone predictors. We
conduct comprehensive and systematical studies on these two pipelines under
various settings. From comprehensive empirical results, we make original
observations and find new insights that open new possibilities and suggest
promising directions to leverage LLMs for learning on graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhikai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1&quot;&gt;Haitao Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1&quot;&gt;Wei Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1&quot;&gt;Hongzhi Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xiaochi Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuaiqiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1&quot;&gt;Dawei Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1&quot;&gt;Wenqi Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiliang Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03406">
<title>Goal-Conditioned Predictive Coding as an Implicit Planner for Offline Reinforcement Learning. (arXiv:2307.03406v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03406</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work has demonstrated the effectiveness of formulating decision making
as a supervised learning problem on offline-collected trajectories. However,
the benefits of performing sequence modeling on trajectory data is not yet
clear. In this work we investigate if sequence modeling has the capability to
condense trajectories into useful representations that can contribute to policy
learning. To achieve this, we adopt a two-stage framework that first summarizes
trajectories with sequence modeling techniques, and then employs these
representations to learn a policy along with a desired goal. This design allows
many existing supervised offline RL methods to be considered as specific
instances of our framework. Within this framework, we introduce
Goal-Conditioned Predicitve Coding (GCPC), an approach that brings powerful
trajectory representations and leads to performant policies. We conduct
extensive empirical evaluations on AntMaze, FrankaKitchen and Locomotion
environments, and observe that sequence modeling has a significant impact on
some decision making tasks. In addition, we demonstrate that GCPC learns a
goal-conditioned latent representation about the future, which serves as an
&quot;implicit planner&quot;, and enables competitive performance on all three
benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1&quot;&gt;Zilai Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Ce Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shijie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1&quot;&gt;Chen Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03410">
<title>Scalable High-Dimensional Multivariate Linear Regression for Feature-Distributed Data. (arXiv:2307.03410v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2307.03410</link>
<description rdf:parseType="Literal">&lt;p&gt;Feature-distributed data, referred to data partitioned by features and stored
across multiple computing nodes, are increasingly common in applications with a
large number of features. This paper proposes a two-stage relaxed greedy
algorithm (TSRGA) for applying multivariate linear regression to such data. The
main advantage of TSRGA is that its communication complexity does not depend on
the feature dimension, making it highly scalable to very large data sets. In
addition, for multivariate response variables, TSRGA can be used to yield
low-rank coefficient estimates. The fast convergence of TSRGA is validated by
simulation experiments. Finally, we apply the proposed TSRGA in a financial
application that leverages unstructured data from the 10-K reports,
demonstrating its usefulness in applications with many dense large-dimensional
matrices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shuo-Chieh Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tsay_R/0/1/0/all/0/1&quot;&gt;Ruey S. Tsay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03411">
<title>Learning from Heterogeneity: A Dynamic Learning Framework for Hypergraphs. (arXiv:2307.03411v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03411</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph neural network (GNN) has gained increasing popularity in recent years
owing to its capability and flexibility in modeling complex graph structure
data. Among all graph learning methods, hypergraph learning is a technique for
exploring the implicit higher-order correlations when training the embedding
space of the graph. In this paper, we propose a hypergraph learning framework
named LFH that is capable of dynamic hyperedge construction and attentive
embedding update utilizing the heterogeneity attributes of the graph.
Specifically, in our framework, the high-quality features are first generated
by the pairwise fusion strategy that utilizes explicit graph structure
information when generating initial node embedding. Afterwards, a hypergraph is
constructed through the dynamic grouping of implicit hyperedges, followed by
the type-specific hypergraph learning process. To evaluate the effectiveness of
our proposed framework, we conduct comprehensive experiments on several popular
datasets with eleven state-of-the-art models on both node classification and
link prediction tasks, which fall into categories of homogeneous pairwise graph
learning, heterogeneous pairwise graph learning, and hypergraph learning. The
experiment results demonstrate a significant performance gain (average 12.5% in
node classification and 13.3% in link prediction) compared with recent
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tiehua Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuze Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1&quot;&gt;Zhishu Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xingjun Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaowei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1&quot;&gt;Jun Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1&quot;&gt;Jiong Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03423">
<title>Hyperspectral and Multispectral Image Fusion Using the Conditional Denoising Diffusion Probabilistic Model. (arXiv:2307.03423v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.03423</link>
<description rdf:parseType="Literal">&lt;p&gt;Hyperspectral images (HSI) have a large amount of spectral information
reflecting the characteristics of matter, while their spatial resolution is low
due to the limitations of imaging technology. Complementary to this are
multispectral images (MSI), e.g., RGB images, with high spatial resolution but
insufficient spectral bands. Hyperspectral and multispectral image fusion is a
technique for acquiring ideal images that have both high spatial and high
spectral resolution cost-effectively. Many existing HSI and MSI fusion
algorithms rely on known imaging degradation models, which are often not
available in practice. In this paper, we propose a deep fusion method based on
the conditional denoising diffusion probabilistic model, called DDPM-Fus.
Specifically, the DDPM-Fus contains the forward diffusion process which
gradually adds Gaussian noise to the high spatial resolution HSI (HrHSI) and
another reverse denoising process which learns to predict the desired HrHSI
from its noisy version conditioning on the corresponding high spatial
resolution MSI (HrMSI) and low spatial resolution HSI (LrHSI). Once the
training is completes, the proposed DDPM-Fus implements the reverse process on
the test HrMSI and LrHSI to generate the fused HrHSI. Experiments conducted on
one indoor and two remote sensing datasets show the superiority of the proposed
model when compared with other advanced deep learningbased fusion methods. The
codes of this work will be opensourced at this address:
https://github.com/shuaikaishi/DDPMFus for reproducibility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shi_S/0/1/0/all/0/1&quot;&gt;Shuaikai Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lijun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jie Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03427">
<title>Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer. (arXiv:2307.03427v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.03427</link>
<description rdf:parseType="Literal">&lt;p&gt;Survival prediction is crucial for cancer patients as it provides early
prognostic information for treatment planning. Recently, deep survival models
based on deep learning and medical images have shown promising performance for
survival prediction. However, existing deep survival models are not well
developed in utilizing multi-modality images (e.g., PET-CT) and in extracting
region-specific information (e.g., the prognostic information in Primary Tumor
(PT) and Metastatic Lymph Node (MLN) regions). In view of this, we propose a
merging-diverging learning framework for survival prediction from
multi-modality images. This framework has a merging encoder to fuse
multi-modality information and a diverging decoder to extract region-specific
information. In the merging encoder, we propose a Hybrid Parallel
Cross-Attention (HPCA) block to effectively fuse multi-modality features via
parallel convolutional layers and cross-attention transformers. In the
diverging decoder, we propose a Region-specific Attention Gate (RAG) block to
screen out the features related to lesion regions. Our framework is
demonstrated on survival prediction from PET-CT images in Head and Neck (H&amp;amp;N)
cancer, by designing an X-shape merging-diverging hybrid transformer network
(named XSurv). Our XSurv combines the complementary information in PET and CT
images and extracts the region-specific prognostic information in PT and MLN
regions. Extensive experiments on the public dataset of HEad and neCK TumOR
segmentation and outcome prediction challenge (HECKTOR 2022) demonstrate that
our XSurv outperforms state-of-the-art survival prediction methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Meng_M/0/1/0/all/0/1&quot;&gt;Mingyuan Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bi_L/0/1/0/all/0/1&quot;&gt;Lei Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fulham_M/0/1/0/all/0/1&quot;&gt;Michael Fulham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Feng_D/0/1/0/all/0/1&quot;&gt;Dagan Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jinman Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03430">
<title>Differential Privacy for Clustering Under Continual Observation. (arXiv:2307.03430v1 [cs.DS])</title>
<link>http://arxiv.org/abs/2307.03430</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of clustering privately a dataset in $\mathbb{R}^d$
that undergoes both insertion and deletion of points. Specifically, we give an
$\varepsilon$-differentially private clustering mechanism for the $k$-means
objective under continual observation. This is the first approximation
algorithm for that problem with an additive error that depends only
logarithmically in the number $T$ of updates. The multiplicative error is
almost the same as non privately. To do so we show how to perform dimension
reduction under continual observation and combine it with a differentially
private greedy approximation algorithm for $k$-means. We also partially extend
our results to the $k$-median problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tour_M/0/1/0/all/0/1&quot;&gt;Max Dupr&amp;#xe9; la Tour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henzinger_M/0/1/0/all/0/1&quot;&gt;Monika Henzinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saulpic_D/0/1/0/all/0/1&quot;&gt;David Saulpic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03476">
<title>Unpaired Multi-View Graph Clustering with Cross-View Structure Matching. (arXiv:2307.03476v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03476</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-view clustering (MVC), which effectively fuses information from
multiple views for better performance, has received increasing attention. Most
existing MVC methods assume that multi-view data are fully paired, which means
that the mappings of all corresponding samples between views are pre-defined or
given in advance. However, the data correspondence is often incomplete in
real-world applications due to data corruption or sensor differences, referred
as the data-unpaired problem (DUP) in multi-view literature. Although several
attempts have been made to address the DUP issue, they suffer from the
following drawbacks: 1) Most methods focus on the feature representation while
ignoring the structural information of multi-view data, which is essential for
clustering tasks; 2) Existing methods for partially unpaired problems rely on
pre-given cross-view alignment information, resulting in their inability to
handle fully unpaired problems; 3) Their inevitable parameters degrade the
efficiency and applicability of the models. To tackle these issues, we propose
a novel parameter-free graph clustering framework termed Unpaired Multi-view
Graph Clustering framework with Cross-View Structure Matching (UPMGC-SM).
Specifically, unlike the existing methods, UPMGC-SM effectively utilizes the
structural information from each view to refine cross-view correspondences.
Besides, our UPMGC-SM is a unified framework for both the fully and partially
unpaired multi-view graph clustering. Moreover, existing graph clustering
methods can adopt our UPMGC-SM to enhance their ability for unpaired scenarios.
Extensive experiments demonstrate the effectiveness and generalization of our
proposed framework for both paired and unpaired datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yi Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Siwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1&quot;&gt;Qing Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1&quot;&gt;Weixuan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1&quot;&gt;Ke Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1&quot;&gt;Xinhang Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinwang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03486">
<title>Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive Learning. (arXiv:2307.03486v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03486</link>
<description rdf:parseType="Literal">&lt;p&gt;Discovering achievements with a hierarchical structure on procedurally
generated environments poses a significant challenge. This requires agents to
possess a broad range of abilities, including generalization and long-term
reasoning. Many prior methods are built upon model-based or hierarchical
approaches, with the belief that an explicit module for long-term planning
would be beneficial for learning hierarchical achievements. However, these
methods require an excessive amount of environment interactions or large model
sizes, limiting their practicality. In this work, we identify that proximal
policy optimization (PPO), a simple and versatile model-free algorithm,
outperforms the prior methods with recent implementation practices. Moreover,
we find that the PPO agent can predict the next achievement to be unlocked to
some extent, though with low confidence. Based on this observation, we propose
a novel contrastive learning method, called achievement distillation, that
strengthens the agent&apos;s capability to predict the next achievement. Our method
exhibits a strong capacity for discovering hierarchical achievements and shows
state-of-the-art performance on the challenging Crafter environment using fewer
model parameters in a sample-efficient regime.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1&quot;&gt;Seungyong Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeom_J/0/1/0/all/0/1&quot;&gt;Junyoung Yeom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_B/0/1/0/all/0/1&quot;&gt;Bumsoo Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1&quot;&gt;Hyun Oh Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03487">
<title>Learning Theory of Distribution Regression with Neural Networks. (arXiv:2307.03487v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2307.03487</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we aim at establishing an approximation theory and a learning
theory of distribution regression via a fully connected neural network (FNN).
In contrast to the classical regression methods, the input variables of
distribution regression are probability measures. Then we often need to perform
a second-stage sampling process to approximate the actual information of the
distribution. On the other hand, the classical neural network structure
requires the input variable to be a vector. When the input samples are
probability distributions, the traditional deep neural network method cannot be
directly used and the difficulty arises for distribution regression. A
well-defined neural network structure for distribution inputs is intensively
desirable. There is no mathematical model and theoretical analysis on neural
network realization of distribution regression. To overcome technical
difficulties and address this issue, we establish a novel fully connected
neural network framework to realize an approximation theory of functionals
defined on the space of Borel probability measures. Furthermore, based on the
established functional approximation results, in the hypothesis space induced
by the novel FNN structure with distribution inputs, almost optimal learning
rates for the proposed distribution regression model up to logarithmic terms
are derived via a novel two-stage error decomposition technique.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhongjie Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhan Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_D/0/1/0/all/0/1&quot;&gt;Ding-Xuan Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03493">
<title>ITA: An Energy-Efficient Attention and Softmax Accelerator for Quantized Transformers. (arXiv:2307.03493v1 [cs.AR])</title>
<link>http://arxiv.org/abs/2307.03493</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer networks have emerged as the state-of-the-art approach for
natural language processing tasks and are gaining popularity in other domains
such as computer vision and audio processing. However, the efficient hardware
acceleration of transformer models poses new challenges due to their high
arithmetic intensities, large memory requirements, and complex dataflow
dependencies. In this work, we propose ITA, a novel accelerator architecture
for transformers and related models that targets efficient inference on
embedded systems by exploiting 8-bit quantization and an innovative softmax
implementation that operates exclusively on integer values. By computing
on-the-fly in streaming mode, our softmax implementation minimizes data
movement and energy consumption. ITA achieves competitive energy efficiency
with respect to state-of-the-art transformer accelerators with 16.9 TOPS/W,
while outperforming them in area efficiency with 5.93 TOPS/mm$^2$ in 22 nm
fully-depleted silicon-on-insulator technology at 0.8 V.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islamoglu_G/0/1/0/all/0/1&quot;&gt;Gamze &amp;#x130;slamo&amp;#x11f;lu&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scherer_M/0/1/0/all/0/1&quot;&gt;Moritz Scherer&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paulin_G/0/1/0/all/0/1&quot;&gt;Gianna Paulin&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fischer_T/0/1/0/all/0/1&quot;&gt;Tim Fischer&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_V/0/1/0/all/0/1&quot;&gt;Victor J.B. Jung&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garofalo_A/0/1/0/all/0/1&quot;&gt;Angelo Garofalo&lt;/a&gt; (1 and 2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1&quot;&gt;Luca Benini&lt;/a&gt; (1 and 2) ((1) ETH Z&amp;#xfc;rich, (2) University of Bologna)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03494">
<title>HoughLaneNet: Lane Detection with Deep Hough Transform and Dynamic Convolution. (arXiv:2307.03494v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03494</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of lane detection has garnered considerable attention in the field
of autonomous driving due to its complexity. Lanes can present difficulties for
detection, as they can be narrow, fragmented, and often obscured by heavy
traffic. However, it has been observed that the lanes have a geometrical
structure that resembles a straight line, leading to improved lane detection
results when utilizing this characteristic. To address this challenge, we
propose a hierarchical Deep Hough Transform (DHT) approach that combines all
lane features in an image into the Hough parameter space. Additionally, we
refine the point selection method and incorporate a Dynamic Convolution Module
to effectively differentiate between lanes in the original image. Our network
architecture comprises a backbone network, either a ResNet or Pyramid Vision
Transformer, a Feature Pyramid Network as the neck to extract multi-scale
features, and a hierarchical DHT-based feature aggregation head to accurately
segment each lane. By utilizing the lane features in the Hough parameter space,
the network learns dynamic convolution kernel parameters corresponding to each
lane, allowing the Dynamic Convolution Module to effectively differentiate
between lane features. Subsequently, the lane features are fed into the feature
decoder, which predicts the final position of the lane. Our proposed network
structure demonstrates improved performance in detecting heavily occluded or
worn lane images, as evidenced by our extensive experimental results, which
show that our method outperforms or is on par with state-of-the-art techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jia-Qi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_H/0/1/0/all/0/1&quot;&gt;Hao-Bin Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jun-Long Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1&quot;&gt;Ariel Shamir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Miao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03500">
<title>DEFT: Exploiting Gradient Norm Difference between Model Layers for Scalable Gradient Sparsification. (arXiv:2307.03500v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03500</link>
<description rdf:parseType="Literal">&lt;p&gt;Gradient sparsification is a widely adopted solution for reducing the
excessive communication traffic in distributed deep learning. However, most
existing gradient sparsifiers have relatively poor scalability because of
considerable computational cost of gradient selection and/or increased
communication traffic owing to gradient build-up. To address these challenges,
we propose a novel gradient sparsification scheme, DEFT, that partitions the
gradient selection task into sub tasks and distributes them to workers. DEFT
differs from existing sparsifiers, wherein every worker selects gradients among
all gradients. Consequently, the computational cost can be reduced as the
number of workers increases. Moreover, gradient build-up can be eliminated
because DEFT allows workers to select gradients in partitions that are
non-intersecting (between workers). Therefore, even if the number of workers
increases, the communication traffic can be maintained as per user requirement.
&lt;/p&gt;
&lt;p&gt;To avoid the loss of significance of gradient selection, DEFT selects more
gradients in the layers that have a larger gradient norm than the other layers.
Because every layer has a different computational load, DEFT allocates layers
to workers using a bin-packing algorithm to maintain a balanced load of
gradient selection between workers. In our empirical evaluation, DEFT shows a
significant improvement in training performance in terms of speed in gradient
selection over existing sparsifiers while achieving high convergence
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_D/0/1/0/all/0/1&quot;&gt;Daegun Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1&quot;&gt;Sangyoon Oh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03515">
<title>Incentive Allocation in Vertical Federated Learning Based on Bankruptcy Problem. (arXiv:2307.03515v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03515</link>
<description rdf:parseType="Literal">&lt;p&gt;Vertical federated learning (VFL) is a promising approach for collaboratively
training machine learning models using private data partitioned vertically
across different parties. Ideally in a VFL setting, the active party (party
possessing features of samples with labels) benefits by improving its machine
learning model through collaboration with some passive parties (parties
possessing additional features of the same samples without labels) in a privacy
preserving manner. However, motivating passive parties to participate in VFL
can be challenging. In this paper, we focus on the problem of allocating
incentives to the passive parties by the active party based on their
contributions to the VFL process. We formulate this problem as a variant of the
Nucleolus game theory concept, known as the Bankruptcy Problem, and solve it
using the Talmud&apos;s division rule. We evaluate our proposed method on synthetic
and real-world datasets and show that it ensures fairness and stability in
incentive allocation among passive parties who contribute their data to the
federated model. Additionally, we compare our method to the existing solution
of calculating Shapley values and show that our approach provides a more
efficient solution with fewer computations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Afsana Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thij_M/0/1/0/all/0/1&quot;&gt;Marijn ten Thij&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thuijsman_F/0/1/0/all/0/1&quot;&gt;Frank Thuijsman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilbik_A/0/1/0/all/0/1&quot;&gt;Anna Wilbik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03544">
<title>Roman Numeral Analysis with Graph Neural Networks: Onset-wise Predictions from Note-wise Features. (arXiv:2307.03544v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2307.03544</link>
<description rdf:parseType="Literal">&lt;p&gt;Roman Numeral analysis is the important task of identifying chords and their
functional context in pieces of tonal music. This paper presents a new approach
to automatic Roman Numeral analysis in symbolic music. While existing
techniques rely on an intermediate lossy representation of the score, we
propose a new method based on Graph Neural Networks (GNNs) that enable the
direct description and processing of each individual note in the score. The
proposed architecture can leverage notewise features and interdependencies
between notes but yield onset-wise representation by virtue of our novel edge
contraction algorithm. Our results demonstrate that ChordGNN outperforms
existing state-of-the-art models, achieving higher accuracy in Roman Numeral
analysis on the reference datasets. In addition, we investigate variants of our
model using proposed techniques such as NADE, and post-processing of the chord
predictions. The full source code for this work is available at
https://github.com/manoskary/chordgnn
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karystinaios_E/0/1/0/all/0/1&quot;&gt;Emmanouil Karystinaios&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Widmer_G/0/1/0/all/0/1&quot;&gt;Gerhard Widmer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03550">
<title>DWReCO at CheckThat! 2023: Enhancing Subjectivity Detection through Style-based Data Sampling. (arXiv:2307.03550v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.03550</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper describes our submission for the subjectivity detection task at
the CheckThat! Lab. To tackle class imbalances in the task, we have generated
additional training materials with GPT-3 models using prompts of different
styles from a subjectivity checklist based on journalistic perspective. We used
the extended training set to fine-tune language-specific transformer models.
Our experiments in English, German and Turkish demonstrate that different
subjective styles are effective across all languages. In addition, we observe
that the style-based oversampling is better than paraphrasing in Turkish and
English. Lastly, the GPT-3 models sometimes produce lacklustre results when
generating style-based texts in non-English languages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlicht_I/0/1/0/all/0/1&quot;&gt;Ipek Baris Schlicht&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khellaf_L/0/1/0/all/0/1&quot;&gt;Lynn Khellaf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Altiok_D/0/1/0/all/0/1&quot;&gt;Defne Altiok&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03565">
<title>MALIBO: Meta-learning for Likelihood-free Bayesian Optimization. (arXiv:2307.03565v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03565</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian optimization (BO) is a popular method to optimize costly black-box
functions. While traditional BO optimizes each new target task from scratch,
meta-learning has emerged as a way to leverage knowledge from related tasks to
optimize new tasks faster. However, existing meta-learning BO methods rely on
surrogate models that suffer from scalability issues and are sensitive to
observations with different scales and noise types across tasks. Moreover, they
often overlook the uncertainty associated with task similarity. This leads to
unreliable task adaptation when only limited observations are obtained or when
the new tasks differ significantly from the related tasks. To address these
limitations, we propose a novel meta-learning BO approach that bypasses the
surrogate model and directly learns the utility of queries across tasks. Our
method explicitly models task uncertainty and includes an auxiliary model to
enable robust adaptation to new tasks. Extensive experiments show that our
method demonstrates strong anytime performance and outperforms state-of-the-art
meta-learning BO methods in various benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Jiarong Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Falkner_S/0/1/0/all/0/1&quot;&gt;Stefan Falkner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berkenkamp_F/0/1/0/all/0/1&quot;&gt;Felix Berkenkamp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vanschoren_J/0/1/0/all/0/1&quot;&gt;Joaquin Vanschoren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03571">
<title>Smoothing the Edges: A General Framework for Smooth Optimization in Sparse Regularization using Hadamard Overparametrization. (arXiv:2307.03571v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03571</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a smooth method for (structured) sparsity in $\ell_q$
and $\ell_{p,q}$ regularized optimization problems. Optimization of these
non-smooth and possibly non-convex problems typically relies on specialized
procedures. In contrast, our general framework is compatible with prevalent
first-order optimization methods like Stochastic Gradient Descent and
accelerated variants without any required modifications. This is accomplished
through a smooth optimization transfer, comprising an overparametrization of
selected model parameters using Hadamard products and a change of penalties. In
the overparametrized problem, smooth and convex $\ell_2$ regularization of the
surrogate parameters induces non-smooth and non-convex $\ell_q$ or $\ell_{p,q}$
regularization in the original parametrization. We show that our approach
yields not only matching global minima but also equivalent local minima. This
is particularly useful in non-convex sparse regularization, where finding
global minima is NP-hard and local minima are known to generalize well. We
provide a comprehensive overview consolidating various literature strands on
sparsity-inducing parametrizations and propose meaningful extensions to
existing approaches. The feasibility of our approach is evaluated through
numerical experiments, which demonstrate that its performance is on par with or
surpasses commonly used implementations of convex and non-convex regularization
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolb_C/0/1/0/all/0/1&quot;&gt;Chris Kolb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_C/0/1/0/all/0/1&quot;&gt;Christian L. M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bischl_B/0/1/0/all/0/1&quot;&gt;Bernd Bischl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rugamer_D/0/1/0/all/0/1&quot;&gt;David R&amp;#xfc;gamer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03576">
<title>One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention. (arXiv:2307.03576v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03576</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent works have empirically analyzed in-context learning and shown that
transformers trained on synthetic linear regression tasks can learn to
implement ridge regression, which is the Bayes-optimal predictor, given
sufficient capacity [Aky\&quot;urek et al., 2023], while one-layer transformers with
linear self-attention and no MLP layer will learn to implement one step of
gradient descent (GD) on a least-squares linear regression objective [von
Oswald et al., 2022]. However, the theory behind these observations remains
poorly understood. We theoretically study transformers with a single layer of
linear self-attention, trained on synthetic noisy linear regression data.
First, we mathematically show that when the covariates are drawn from a
standard Gaussian distribution, the one-layer transformer which minimizes the
pre-training loss will implement a single step of GD on the least-squares
linear regression objective. Then, we find that changing the distribution of
the covariates and weight vector to a non-isotropic Gaussian distribution has a
strong impact on the learned algorithm: the global minimizer of the
pre-training loss now implements a single step of $\textit{pre-conditioned}$
GD. However, if only the distribution of the responses is changed, then this
does not have a large effect on the learned algorithm: even when the response
comes from a more general family of $\textit{nonlinear}$ functions, the global
minimizer of the pre-training loss still implements a single step of GD on a
least-squares linear regression objective.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahankali_A/0/1/0/all/0/1&quot;&gt;Arvind Mahankali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1&quot;&gt;Tatsunori B. Hashimoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1&quot;&gt;Tengyu Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03577">
<title>Programmable Synthetic Tabular Data Generation. (arXiv:2307.03577v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03577</link>
<description rdf:parseType="Literal">&lt;p&gt;Large amounts of tabular data remain underutilized due to privacy, data
quality, and data sharing limitations. While training a generative model
producing synthetic data resembling the original distribution addresses some of
these issues, most applications require additional constraints from the
generated data. Existing synthetic data approaches are limited as they
typically only handle specific constraints, e.g., differential privacy (DP) or
increased fairness, and lack an accessible interface for declaring general
specifications. In this work, we introduce ProgSyn, the first programmable
synthetic tabular data generation algorithm that allows for comprehensive
customization over the generated data. To ensure high data quality while
adhering to custom specifications, ProgSyn pre-trains a generative model on the
original dataset and fine-tunes it on a differentiable loss automatically
derived from the provided specifications. These can be programmatically
declared using statistical and logical expressions, supporting a wide range of
requirements (e.g., DP or fairness, among others). We conduct an extensive
experimental evaluation of ProgSyn on a number of constraints, achieving a new
state-of-the-art on some, while remaining general. For instance, at the same
fairness level we achieve 2.3% higher downstream accuracy than the
state-of-the-art in fair synthetic data generation on the Adult dataset.
Overall, ProgSyn provides a versatile and accessible framework for generating
constrained synthetic tabular data, allowing for specifications that generalize
beyond the capabilities of prior work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vero_M/0/1/0/all/0/1&quot;&gt;Mark Vero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balunovic_M/0/1/0/all/0/1&quot;&gt;Mislav Balunovi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vechev_M/0/1/0/all/0/1&quot;&gt;Martin Vechev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03586">
<title>ContextLabeler Dataset: physical and virtual sensors data collected from smartphone usage in-the-wild. (arXiv:2307.03586v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2307.03586</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper describes a data collection campaign and the resulting dataset
derived from smartphone sensors characterizing the daily life activities of 3
volunteers in a period of two weeks. The dataset is released as a collection of
CSV files containing more than 45K data samples, where each sample is composed
by 1332 features related to a heterogeneous set of physical and virtual
sensors, including motion sensors, running applications, devices in proximity,
and weather conditions. Moreover, each data sample is associated with a ground
truth label that describes the user activity and the situation in which she was
involved during the sensing experiment (e.g., working, at restaurant, and doing
sport activity). To avoid introducing any bias during the data collection, we
performed the sensing experiment in-the-wild, that is, by using the volunteers&apos;
devices, and without defining any constraint related to the user&apos;s behavior.
For this reason, the collected dataset represents a useful source of real data
to both define and evaluate a broad set of novel context-aware solutions (both
algorithms and protocols) that aim to adapt their behavior according to the
changes in the user&apos;s situation in a mobile environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Campana_M/0/1/0/all/0/1&quot;&gt;Mattia Giovanni Campana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delmastro_F/0/1/0/all/0/1&quot;&gt;Franca Delmastro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03587">
<title>BOF-UCB: A Bayesian-Optimistic Frequentist Algorithm for Non-Stationary Contextual Bandits. (arXiv:2307.03587v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03587</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel Bayesian-Optimistic Frequentist Upper Confidence Bound
(BOF-UCB) algorithm for stochastic contextual linear bandits in non-stationary
environments. This unique combination of Bayesian and frequentist principles
enhances adaptability and performance in dynamic settings. The BOF-UCB
algorithm utilizes sequential Bayesian updates to infer the posterior
distribution of the unknown regression parameter, and subsequently employs a
frequentist approach to compute the Upper Confidence Bound (UCB) by maximizing
the expected reward over the posterior distribution. We provide theoretical
guarantees of BOF-UCB&apos;s performance and demonstrate its effectiveness in
balancing exploration and exploitation on synthetic datasets and classical
control tasks in a reinforcement learning setting. Our results show that
BOF-UCB outperforms existing methods, making it a promising solution for
sequential decision-making in non-stationary environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Werge_N/0/1/0/all/0/1&quot;&gt;Nicklas Werge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akgul_A/0/1/0/all/0/1&quot;&gt;Abdullah Akg&amp;#xfc;l&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kandemir_M/0/1/0/all/0/1&quot;&gt;Melih Kandemir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03590">
<title>Accelerated Optimization Landscape of Linear-Quadratic Regulator. (arXiv:2307.03590v1 [math.OC])</title>
<link>http://arxiv.org/abs/2307.03590</link>
<description rdf:parseType="Literal">&lt;p&gt;Linear-quadratic regulator (LQR) is a landmark problem in the field of
optimal control, which is the concern of this paper. Generally, LQR is
classified into state-feedback LQR (SLQR) and output-feedback LQR (OLQR) based
on whether the full state is obtained. It has been suggested in existing
literature that both the SLQR and the OLQR could be viewed as
\textit{constrained nonconvex matrix optimization} problems in which the only
variable to be optimized is the feedback gain matrix. In this paper, we
introduce a first-order accelerated optimization framework of handling the LQR
problem, and give its convergence analysis for the cases of SLQR and OLQR,
respectively.
&lt;/p&gt;
&lt;p&gt;Specifically, a Lipschiz Hessian property of LQR performance criterion is
presented, which turns out to be a crucial property for the application of
modern optimization techniques. For the SLQR problem, a continuous-time hybrid
dynamic system is introduced, whose solution trajectory is shown to converge
exponentially to the optimal feedback gain with Nesterov-optimal order
$1-\frac{1}{\sqrt{\kappa}}$ ($\kappa$ the condition number). Then, the
symplectic Euler scheme is utilized to discretize the hybrid dynamic system,
and a Nesterov-type method with a restarting rule is proposed that preserves
the continuous-time convergence rate, i.e., the discretized algorithm admits
the Nesterov-optimal convergence order. For the OLQR problem, a Hessian-free
accelerated framework is proposed, which is a two-procedure method consisting
of semiconvex function optimization and negative curvature exploitation. In a
time $\mathcal{O}(\epsilon^{-7/4}\log(1/\epsilon))$, the method can find an
$\epsilon$-stationary point of the performance criterion; this entails that the
method improves upon the $\mathcal{O}(\epsilon^{-2})$ complexity of vanilla
gradient descent. Moreover, our method provides the second-order guarantee of
stationary point.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Feng_L/0/1/0/all/0/1&quot;&gt;Lechen Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ni_Y/0/1/0/all/0/1&quot;&gt;Yuan-Hua Ni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03595">
<title>GEANN: Scalable Graph Augmentations for Multi-Horizon Time Series Forecasting. (arXiv:2307.03595v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03595</link>
<description rdf:parseType="Literal">&lt;p&gt;Encoder-decoder deep neural networks have been increasingly studied for
multi-horizon time series forecasting, especially in real-world applications.
However, to forecast accurately, these sophisticated models typically rely on a
large number of time series examples with substantial history. A rapidly
growing topic of interest is forecasting time series which lack sufficient
historical data -- often referred to as the ``cold start&apos;&apos; problem. In this
paper, we introduce a novel yet simple method to address this problem by
leveraging graph neural networks (GNNs) as a data augmentation for enhancing
the encoder used by such forecasters. These GNN-based features can capture
complex inter-series relationships, and their generation process can be
optimized end-to-end with the forecasting task. We show that our architecture
can use either data-driven or domain knowledge-defined graphs, scaling to
incorporate information from multiple very large graphs with millions of nodes.
In our target application of demand forecasting for a large e-commerce
retailer, we demonstrate on both a small dataset of 100K products and a large
dataset with over 2 million products that our method improves overall
performance over competitive baseline models. More importantly, we show that it
brings substantially more gains to ``cold start&apos;&apos; products such as those newly
launched or recently out-of-stock.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Sitan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolff_M/0/1/0/all/0/1&quot;&gt;Malcolm Wolff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramasubramanian_S/0/1/0/all/0/1&quot;&gt;Shankar Ramasubramanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quenneville_Belair_V/0/1/0/all/0/1&quot;&gt;Vincent Quenneville-Belair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metha_R/0/1/0/all/0/1&quot;&gt;Ronak Metha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1&quot;&gt;Michael W. Mahoney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03630">
<title>PAC bounds of continuous Linear Parameter-Varying systems related to neural ODEs. (arXiv:2307.03630v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03630</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of learning Neural Ordinary Differential Equations
(neural ODEs) within the context of Linear Parameter-Varying (LPV) systems in
continuous-time. LPV systems contain bilinear systems which are known to be
universal approximators for non-linear systems. Moreover, a large class of
neural ODEs can be embedded into LPV systems. As our main contribution we
provide Probably Approximately Correct (PAC) bounds under stability for LPV
systems related to neural ODEs. The resulting bounds have the advantage that
they do not depend on the integration interval.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Racz_D/0/1/0/all/0/1&quot;&gt;D&amp;#xe1;niel R&amp;#xe1;cz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petreczky_M/0/1/0/all/0/1&quot;&gt;Mih&amp;#xe1;ly Petreczky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daroczy_B/0/1/0/all/0/1&quot;&gt;B&amp;#xe1;lint Dar&amp;#xf3;czy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03641">
<title>Online Network Source Optimization with Graph-Kernel MAB. (arXiv:2307.03641v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03641</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose Grab-UCB, a graph-kernel multi-arms bandit algorithm to learn
online the optimal source placement in large scale networks, such that the
reward obtained from a priori unknown network processes is maximized. The
uncertainty calls for online learning, which suffers however from the curse of
dimensionality. To achieve sample efficiency, we describe the network processes
with an adaptive graph dictionary model, which typically leads to sparse
spectral representations. This enables a data-efficient learning framework,
whose learning rate scales with the dimension of the spectral representation
model instead of the one of the network. We then propose Grab-UCB, an online
sequential decision strategy that learns the parameters of the spectral
representation while optimizing the action strategy. We derive the performance
guarantees that depend on network parameters, which further influence the
learning curve of the sequential decision strategy We introduce a
computationally simplified solving method, Grab-arm-Light, an algorithm that
walks along the edges of the polytope representing the objective function.
Simulations results show that the proposed online learning algorithm
outperforms baseline offline methods that typically separate the learning phase
from the testing one. The results confirm the theoretical findings, and further
highlight the gain of the proposed online learning strategy in terms of
cumulative regret, sample efficiency and computational complexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toni_L/0/1/0/all/0/1&quot;&gt;Laura Toni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frossard_P/0/1/0/all/0/1&quot;&gt;Pascal Frossard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03672">
<title>Simulation-free Schr\&quot;odinger bridges via score and flow matching. (arXiv:2307.03672v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03672</link>
<description rdf:parseType="Literal">&lt;p&gt;We present simulation-free score and flow matching ([SF]$^2$M), a
simulation-free objective for inferring stochastic dynamics given unpaired
source and target samples drawn from arbitrary distributions. Our method
generalizes both the score-matching loss used in the training of diffusion
models and the recently proposed flow matching loss used in the training of
continuous normalizing flows. [SF]$^2$M interprets continuous-time stochastic
generative modeling as a Schr\&quot;odinger bridge (SB) problem. It relies on static
entropy-regularized optimal transport, or a minibatch approximation, to
efficiently learn the SB without simulating the learned stochastic process. We
find that [SF]$^2$M is more efficient and gives more accurate solutions to the
SB problem than simulation-based methods from prior work. Finally, we apply
[SF]$^2$M to the problem of learning cell dynamics from snapshot data. Notably,
[SF]$^2$M is the first method to accurately model cell dynamics in high
dimensions and can recover known gene regulatory networks from simulated data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_A/0/1/0/all/0/1&quot;&gt;Alexander Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malkin_N/0/1/0/all/0/1&quot;&gt;Nikolay Malkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fatras_K/0/1/0/all/0/1&quot;&gt;Kilian Fatras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atanackovic_L/0/1/0/all/0/1&quot;&gt;Lazar Atanackovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yanlei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huguet_G/0/1/0/all/0/1&quot;&gt;Guillaume Huguet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolf_G/0/1/0/all/0/1&quot;&gt;Guy Wolf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03675">
<title>GeoPhy: Differentiable Phylogenetic Inference via Geometric Gradients of Tree Topologies. (arXiv:2307.03675v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03675</link>
<description rdf:parseType="Literal">&lt;p&gt;Phylogenetic inference, grounded in molecular evolution models, is essential
for understanding the evolutionary relationships in biological data. Accounting
for the uncertainty of phylogenetic tree variables, which include tree
topologies and evolutionary distances on branches, is crucial for accurately
inferring species relationships from molecular data and tasks requiring
variable marginalization. Variational Bayesian methods are key to developing
scalable, practical models; however, it remains challenging to conduct
phylogenetic inference without restricting the combinatorially vast number of
possible tree topologies. In this work, we introduce a novel, fully
differentiable formulation of phylogenetic inference that leverages a unique
representation of topological distributions in continuous geometric spaces.
Through practical considerations on design spaces and control variates for
gradient estimations, our approach, GeoPhy, enables variational inference
without limiting the topological candidates. In experiments using real
benchmark datasets, GeoPhy significantly outperformed other approximate
Bayesian methods that considered whole topologies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mimori_T/0/1/0/all/0/1&quot;&gt;Takahiro Mimori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamada_M/0/1/0/all/0/1&quot;&gt;Michiaki Hamada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03678">
<title>Evaluating the Effectiveness of Large Language Models in Representing Textual Descriptions of Geometry and Spatial Relations. (arXiv:2307.03678v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.03678</link>
<description rdf:parseType="Literal">&lt;p&gt;This research focuses on assessing the ability of large language models
(LLMs) in representing geometries and their spatial relations. We utilize LLMs
including GPT-2 and BERT to encode the well-known text (WKT) format of
geometries and then feed their embeddings into classifiers and regressors to
evaluate the effectiveness of the LLMs-generated embeddings for geometric
attributes. The experiments demonstrate that while the LLMs-generated
embeddings can preserve geometry types and capture some spatial relations (up
to 73% accuracy), challenges remain in estimating numeric values and retrieving
spatially related objects. This research highlights the need for improvement in
terms of capturing the nuances and complexities of the underlying geospatial
data and integrating domain knowledge to support various GeoAI applications
using foundation models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1&quot;&gt;Yuhan Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Song Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03679">
<title>Undecimated Wavelet Transform for Word Embedded Semantic Marginal Autoencoder in Security improvement and Denoising different Languages. (arXiv:2307.03679v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.03679</link>
<description rdf:parseType="Literal">&lt;p&gt;By combining the undecimated wavelet transform within a Word Embedded
Semantic Marginal Autoencoder (WESMA), this research study provides a novel
strategy for improving security measures and denoising multiple languages. The
incorporation of these strategies is intended to address the issues of
robustness, privacy, and multilingualism in data processing applications. The
undecimated wavelet transform is used as a feature extraction tool to identify
prominent language patterns and structural qualities in the input data. The
proposed system may successfully capture significant information while
preserving the temporal and geographical links within the data by employing
this transform. This improves security measures by increasing the system&apos;s
ability to detect abnormalities, discover hidden patterns, and distinguish
between legitimate content and dangerous threats. The Word Embedded Semantic
Marginal Autoencoder also functions as an intelligent framework for
dimensionality and noise reduction. The autoencoder effectively learns the
underlying semantics of the data and reduces noise components by exploiting
word embeddings and semantic context. As a result, data quality and accuracy
are increased in following processing stages. The suggested methodology is
tested using a diversified dataset that includes several languages and security
scenarios. The experimental results show that the proposed approach is
effective in attaining security enhancement and denoising capabilities across
multiple languages. The system is strong in dealing with linguistic variances,
producing consistent outcomes regardless of the language used. Furthermore,
incorporating the undecimated wavelet transform considerably improves the
system&apos;s ability to efficiently address complex security concerns
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+S_S/0/1/0/all/0/1&quot;&gt;Shreyanth S&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03681">
<title>Guideline for Trustworthy Artificial Intelligence -- AI Assessment Catalog. (arXiv:2307.03681v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2307.03681</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial Intelligence (AI) has made impressive progress in recent years and
represents a key technology that has a crucial impact on the economy and
society. However, it is clear that AI and business models based on it can only
reach their full potential if AI applications are developed according to high
quality standards and are effectively protected against new AI risks. For
instance, AI bears the risk of unfair treatment of individuals when processing
personal data e.g., to support credit lending or staff recruitment decisions.
The emergence of these new risks is closely linked to the fact that the
behavior of AI applications, particularly those based on Machine Learning (ML),
is essentially learned from large volumes of data and is not predetermined by
fixed programmed rules.
&lt;/p&gt;
&lt;p&gt;Thus, the issue of the trustworthiness of AI applications is crucial and is
the subject of numerous major publications by stakeholders in politics,
business and society. In addition, there is mutual agreement that the
requirements for trustworthy AI, which are often described in an abstract way,
must now be made clear and tangible. One challenge to overcome here relates to
the fact that the specific quality criteria for an AI application depend
heavily on the application context and possible measures to fulfill them in
turn depend heavily on the AI technology used. Lastly, practical assessment
procedures are needed to evaluate whether specific AI applications have been
developed according to adequate quality standards. This AI assessment catalog
addresses exactly this point and is intended for two target groups: Firstly, it
provides developers with a guideline for systematically making their AI
applications trustworthy. Secondly, it guides assessors and auditors on how to
examine AI applications for trustworthiness in a structured way.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poretschkin_M/0/1/0/all/0/1&quot;&gt;Maximilian Poretschkin&lt;/a&gt; (1 and 2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmitz_A/0/1/0/all/0/1&quot;&gt;Anna Schmitz&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akila_M/0/1/0/all/0/1&quot;&gt;Maram Akila&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adilova_L/0/1/0/all/0/1&quot;&gt;Linara Adilova&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Becker_D/0/1/0/all/0/1&quot;&gt;Daniel Becker&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cremers_A/0/1/0/all/0/1&quot;&gt;Armin B. Cremers&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hecker_D/0/1/0/all/0/1&quot;&gt;Dirk Hecker&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Houben_S/0/1/0/all/0/1&quot;&gt;Sebastian Houben&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mock_M/0/1/0/all/0/1&quot;&gt;Michael Mock&lt;/a&gt; (1 and 2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosenzweig_J/0/1/0/all/0/1&quot;&gt;Julia Rosenzweig&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sicking_J/0/1/0/all/0/1&quot;&gt;Joachim Sicking&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulz_E/0/1/0/all/0/1&quot;&gt;Elena Schulz&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Voss_A/0/1/0/all/0/1&quot;&gt;Angelika Voss&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wrobel_S/0/1/0/all/0/1&quot;&gt;Stefan Wrobel&lt;/a&gt; (1 and 2) ((1) Fraunhofer Institute for Intelligent Analysis and Information Systems IAIS, Sankt Augustin, Germany, (2) Department of Computer Science, University of Bonn, Bonn, Germany)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03683">
<title>Differentiable Turbulence. (arXiv:2307.03683v1 [physics.flu-dyn])</title>
<link>http://arxiv.org/abs/2307.03683</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning is increasingly becoming a promising pathway to improving the
accuracy of sub-grid scale (SGS) turbulence closure models for large eddy
simulations (LES). We leverage the concept of differentiable turbulence,
whereby an end-to-end differentiable solver is used in combination with
physics-inspired choices of deep learning architectures to learn highly
effective and versatile SGS models for two-dimensional turbulent flow. We
perform an in-depth analysis of the inductive biases in the chosen
architectures, finding that the inclusion of small-scale non-local features is
most critical to effective SGS modeling, while large-scale features can improve
pointwise accuracy of the a-posteriori solution field. The filtered velocity
gradient tensor can be mapped directly to the SGS stress via decomposition of
the inputs and outputs into isotropic, deviatoric, and anti-symmetric
components. We see that the model can generalize to a variety of flow
configurations, including higher and lower Reynolds numbers and different
forcing conditions. We show that the differentiable physics paradigm is more
successful than offline, a-priori learning, and that hybrid solver-in-the-loop
approaches to deep learning offer an ideal balance between computational
efficiency, accuracy, and generalization. Our experiments provide physics-based
recommendations for deep-learning based SGS modeling for generalizable closure
modeling of turbulence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Shankar_V/0/1/0/all/0/1&quot;&gt;Varun Shankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Maulik_R/0/1/0/all/0/1&quot;&gt;Romit Maulik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Viswanathan_V/0/1/0/all/0/1&quot;&gt;Venkatasubramanian Viswanathan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03690">
<title>Suppressing unknown disturbances to dynamical systems using machine learning. (arXiv:2307.03690v1 [eess.SY])</title>
<link>http://arxiv.org/abs/2307.03690</link>
<description rdf:parseType="Literal">&lt;p&gt;Identifying and suppressing unknown disturbances to dynamical systems is a
problem with applications in many different fields. In this Letter, we present
a model-free method to identify and suppress an unknown disturbance to an
unknown system based only on previous observations of the system under the
influence of a known forcing function. We find that, under very mild
restrictions on the training function, our method is able to robustly identify
and suppress a large class of unknown disturbances. We illustrate our scheme
with an example where a chaotic disturbance to the Lorenz system is identified
and suppressed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Restrepo_J/0/1/0/all/0/1&quot;&gt;Juan G. Restrepo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Skardal_P/0/1/0/all/0/1&quot;&gt;Per Sebastian Skardal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03694">
<title>Scalable Membership Inference Attacks via Quantile Regression. (arXiv:2307.03694v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03694</link>
<description rdf:parseType="Literal">&lt;p&gt;Membership inference attacks are designed to determine, using black box
access to trained models, whether a particular example was used in training or
not. Membership inference can be formalized as a hypothesis testing problem.
The most effective existing attacks estimate the distribution of some test
statistic (usually the model&apos;s confidence on the true label) on points that
were (and were not) used in training by training many \emph{shadow models} --
i.e. models of the same architecture as the model being attacked, trained on a
random subsample of data. While effective, these attacks are extremely
computationally expensive, especially when the model under attack is large.
&lt;/p&gt;
&lt;p&gt;We introduce a new class of attacks based on performing quantile regression
on the distribution of confidence scores induced by the model under attack on
points that are not used in training. We show that our method is competitive
with state-of-the-art shadow model attacks, while requiring substantially less
compute because our attack requires training only a single model. Moreover,
unlike shadow model attacks, our proposed attack does not require any knowledge
of the architecture of the model under attack and is therefore truly
``black-box&quot;. We show the efficacy of this approach in an extensive series of
experiments on various datasets and model architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertran_M/0/1/0/all/0/1&quot;&gt;Martin Bertran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Shuai Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kearns_M/0/1/0/all/0/1&quot;&gt;Michael Kearns&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morgenstern_J/0/1/0/all/0/1&quot;&gt;Jamie Morgenstern&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1&quot;&gt;Aaron Roth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Steven Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03704">
<title>Equivariant Single View Pose Prediction Via Induced and Restricted Representations. (arXiv:2307.03704v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03704</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning about the three-dimensional world from two-dimensional images is a
fundamental problem in computer vision. An ideal neural network architecture
for such tasks would leverage the fact that objects can be rotated and
translated in three dimensions to make predictions about novel images. However,
imposing SO(3)-equivariance on two-dimensional inputs is difficult because the
group of three-dimensional rotations does not have a natural action on the
two-dimensional plane. Specifically, it is possible that an element of SO(3)
will rotate an image out of plane. We show that an algorithm that learns a
three-dimensional representation of the world from two dimensional images must
satisfy certain geometric consistency properties which we formulate as
SO(2)-equivariance constraints. We use the induced and restricted
representations of SO(2) on SO(3) to construct and classify architectures which
satisfy these geometric consistency constraints. We prove that any architecture
which respects said consistency constraints can be realized as an instance of
our construction. We show that three previously proposed neural architectures
for 3D pose prediction are special cases of our construction. We propose a new
algorithm that is a learnable generalization of previously considered methods.
We test our architecture on three pose predictions task and achieve SOTA
results on both the PASCAL3D+ and SYMSOL pose estimation tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Howell_O/0/1/0/all/0/1&quot;&gt;Owen Howell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klee_D/0/1/0/all/0/1&quot;&gt;David Klee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biza_O/0/1/0/all/0/1&quot;&gt;Ondrej Biza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Linfeng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walters_R/0/1/0/all/0/1&quot;&gt;Robin Walters&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03712">
<title>INT-FP-QSim: Mixed Precision and Formats For Large Language Models and Vision Transformers. (arXiv:2307.03712v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03712</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent rise of large language models (LLMs) has resulted in increased
efforts towards running LLMs at reduced precision. Running LLMs at lower
precision supports resource constraints and furthers their democratization,
enabling users to run billion-parameter LLMs on their personal devices. To
supplement this ongoing effort, we propose INT-FP-QSim: an open-source
simulator that enables flexible evaluation of LLMs and vision transformers at
various numerical precisions and formats. INT-FP-QSim leverages existing
open-source repositories such as TensorRT, QPytorch and AIMET for a combined
simulator that supports various floating point and integer formats. With the
help of our simulator, we survey the impact of different numerical formats on
the performance of LLMs and vision transformers at 4-bit weights and 4-bit or
8-bit activations. We also compare recently proposed methods like Adaptive
Block Floating Point, SmoothQuant, GPTQ and RPTQ on the model performances. We
hope INT-FP-QSim will enable researchers to flexibly simulate models at various
precisions to support further research in quantization of LLMs and vision
transformers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nair_L/0/1/0/all/0/1&quot;&gt;Lakshmi Nair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernadskiy_M/0/1/0/all/0/1&quot;&gt;Mikhail Bernadskiy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madhavan_A/0/1/0/all/0/1&quot;&gt;Arulselvan Madhavan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1&quot;&gt;Craig Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basumallik_A/0/1/0/all/0/1&quot;&gt;Ayon Basumallik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bunandar_D/0/1/0/all/0/1&quot;&gt;Darius Bunandar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03716">
<title>SAR: Generalization of Physiological Agility and Dexterity via Synergistic Action Representation. (arXiv:2307.03716v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.03716</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning effective continuous control policies in high-dimensional systems,
including musculoskeletal agents, remains a significant challenge. Over the
course of biological evolution, organisms have developed robust mechanisms for
overcoming this complexity to learn highly sophisticated strategies for motor
control. What accounts for this robust behavioral flexibility? Modular control
via muscle synergies, i.e. coordinated muscle co-contractions, is considered to
be one putative mechanism that enables organisms to learn muscle control in a
simplified and generalizable action space. Drawing inspiration from this
evolved motor control strategy, we use physiologically accurate human hand and
leg models as a testbed for determining the extent to which a Synergistic
Action Representation (SAR) acquired from simpler tasks facilitates learning
more complex tasks. We find in both cases that SAR-exploiting policies
significantly outperform end-to-end reinforcement learning. Policies trained
with SAR were able to achieve robust locomotion on a wide set of terrains with
high sample efficiency, while baseline approaches failed to learn meaningful
behaviors. Additionally, policies trained with SAR on a multiobject
manipulation task significantly outperformed (&amp;gt;70% success) baseline approaches
(&amp;lt;20% success). Both of these SAR-exploiting policies were also found to
generalize zero-shot to out-of-domain environmental conditions, while policies
that did not adopt SAR failed to generalize. Finally, we establish the
generality of SAR on broader high-dimensional control problems using a robotic
manipulation task set and a full-body humanoid locomotion task. To the best of
our knowledge, this investigation is the first of its kind to present an
end-to-end pipeline for discovering synergies and using this representation to
learn high-dimensional continuous control across a wide diversity of tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berg_C/0/1/0/all/0/1&quot;&gt;Cameron Berg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caggiano_V/0/1/0/all/0/1&quot;&gt;Vittorio Caggiano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1&quot;&gt;Vikash Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03719">
<title>Polybot: Training One Policy Across Robots While Embracing Variability. (arXiv:2307.03719v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.03719</link>
<description rdf:parseType="Literal">&lt;p&gt;Reusing large datasets is crucial to scale vision-based robotic manipulators
to everyday scenarios due to the high cost of collecting robotic datasets.
However, robotic platforms possess varying control schemes, camera viewpoints,
kinematic configurations, and end-effector morphologies, posing significant
challenges when transferring manipulation skills from one platform to another.
To tackle this problem, we propose a set of key design decisions to train a
single policy for deployment on multiple robotic platforms. Our framework first
aligns the observation and action spaces of our policy across embodiments via
utilizing wrist cameras and a unified, but modular codebase. To bridge the
remaining domain shift, we align our policy&apos;s internal representations across
embodiments through contrastive learning. We evaluate our method on a dataset
collected over 60 hours spanning 6 tasks and 3 robots with varying joint
configurations and sizes: the WidowX 250S, the Franka Emika Panda, and the
Sawyer. Our results demonstrate significant improvements in success rate and
sample efficiency for our policy when using new task data collected on a
different robot, validating our proposed design decisions. More details and
videos can be found on our anonymized project website:
https://sites.google.com/view/polybot-multirobot
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jonathan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1&quot;&gt;Dorsa Sadigh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1&quot;&gt;Chelsea Finn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03723">
<title>Steel Surface Roughness Parameter Calculations Using Lasers and Machine Learning Models. (arXiv:2307.03723v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03723</link>
<description rdf:parseType="Literal">&lt;p&gt;Control of surface texture in strip steel is essential to meet customer
requirements during galvanizing and temper rolling processes. Traditional
methods rely on post-production stylus measurements, while on-line techniques
offer non-contact and real-time measurements of the entire strip. However,
ensuring accurate measurement is imperative for their effective utilization in
the manufacturing pipeline. Moreover, accurate on-line measurements enable
real-time adjustments of manufacturing processing parameters during production,
ensuring consistent quality and the possibility of closed-loop control of the
temper mill. In this study, we leverage state-of-the-art machine learning
models to enhance the transformation of on-line measurements into significantly
a more accurate Ra surface roughness metric. By comparing a selection of
data-driven approaches, including both deep learning and non-deep learning
methods, to the close-form transformation, we evaluate their potential for
improving surface texture control in temper strip steel manufacturing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milne_A/0/1/0/all/0/1&quot;&gt;Alex Milne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xianghua Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03738">
<title>QIGen: Generating Efficient Kernels for Quantized Inference on Large Language Models. (arXiv:2307.03738v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03738</link>
<description rdf:parseType="Literal">&lt;p&gt;We present ongoing work on a new automatic code generation approach for
supporting quantized generative inference on LLMs such as LLaMA or OPT on
off-the-shelf CPUs. Our approach is informed by the target architecture and a
performance model, including both hardware characteristics and method-specific
accuracy constraints. Results on CPU-based inference for LLaMA models show that
our approach can lead to high performance and high accuracy, comparing
favorably to the best existing open-source solution. A preliminary
implementation is available at https://github.com/IST-DASLab/QIGen.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pegolotti_T/0/1/0/all/0/1&quot;&gt;Tommaso Pegolotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frantar_E/0/1/0/all/0/1&quot;&gt;Elias Frantar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1&quot;&gt;Dan Alistarh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puschel_M/0/1/0/all/0/1&quot;&gt;Markus P&amp;#xfc;schel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03748">
<title>Incentive-Theoretic Bayesian Inference for Collaborative Science. (arXiv:2307.03748v1 [stat.ME])</title>
<link>http://arxiv.org/abs/2307.03748</link>
<description rdf:parseType="Literal">&lt;p&gt;Contemporary scientific research is a distributed, collaborative endeavor,
carried out by teams of researchers, regulatory institutions, funding agencies,
commercial partners, and scientific bodies, all interacting with each other and
facing different incentives. To maintain scientific rigor, statistical methods
should acknowledge this state of affairs. To this end, we study hypothesis
testing when there is an agent (e.g., a researcher or a pharmaceutical company)
with a private prior about an unknown parameter and a principal (e.g., a
policymaker or regulator) who wishes to make decisions based on the parameter
value. The agent chooses whether to run a statistical trial based on their
private prior and then the result of the trial is used by the principal to
reach a decision. We show how the principal can conduct statistical inference
that leverages the information that is revealed by an agent&apos;s strategic
behavior -- their choice to run a trial or not. In particular, we show how the
principal can design a policy to elucidate partial information about the
agent&apos;s private prior beliefs and use this to control the posterior probability
of the null. One implication is a simple guideline for the choice of
significance threshold in clinical trials: the type-I error level should be set
to be strictly less than the cost of the trial divided by the firm&apos;s profit if
the trial is successful.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bates_S/0/1/0/all/0/1&quot;&gt;Stephen Bates&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jordan_M/0/1/0/all/0/1&quot;&gt;Michael I. Jordan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sklar_M/0/1/0/all/0/1&quot;&gt;Michael Sklar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Soloff_J/0/1/0/all/0/1&quot;&gt;Jake A. Soloff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03750">
<title>When does the ID algorithm fail?. (arXiv:2307.03750v1 [stat.ME])</title>
<link>http://arxiv.org/abs/2307.03750</link>
<description rdf:parseType="Literal">&lt;p&gt;The ID algorithm solves the problem of identification of interventional
distributions of the form p(Y | do(a)) in graphical causal models, and has been
formulated in a number of ways [12, 9, 6]. The ID algorithm is sound (outputs
the correct functional of the observed data distribution whenever p(Y | do(a))
is identified in the causal model represented by the input graph), and complete
(explicitly flags as a failure any input p(Y | do(a)) whenever this
distribution is not identified in the causal model represented by the input
graph).
&lt;/p&gt;
&lt;p&gt;The reference [9] provides a result, the so called &quot;hedge criterion&quot;
(Corollary 3), which aims to give a graphical characterization of situations
when the ID algorithm fails to identify its input in terms of a structure in
the input graph called the hedge. While the ID algorithm is, indeed, a sound
and complete algorithm, and the hedge structure does arise whenever the input
distribution is not identified, Corollary 3 presented in [9] is incorrect as
stated. In this note, I outline the modern presentation of the ID algorithm,
discuss a simple counterexample to Corollary 3, and provide a number of
graphical characterizations of the ID algorithm failing to identify its input
distribution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shpitser_I/0/1/0/all/0/1&quot;&gt;Ilya Shpitser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2001.04413">
<title>Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning. (arXiv:2001.04413v6 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2001.04413</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning is also known as hierarchical learning, where the learner
_learns_ to represent a complicated target function by decomposing it into a
sequence of simpler functions to reduce sample and time complexity. This paper
formally analyzes how multi-layer neural networks can perform such hierarchical
learning _efficiently_ and _automatically_ by SGD on the training objective.
&lt;/p&gt;
&lt;p&gt;On the conceptual side, we present a theoretical characterizations of how
certain types of deep (i.e. super-constant layer) neural networks can still be
sample and time efficiently trained on some hierarchical tasks, when no
existing algorithm (including layerwise training, kernel method, etc) is known
to be efficient. We establish a new principle called &quot;backward feature
correction&quot;, where the errors in the lower-level features can be automatically
corrected when training together with the higher-level layers. We believe this
is a key behind how deep learning is performing deep (hierarchical) learning,
as opposed to layerwise learning or simulating some non-hierarchical method.
&lt;/p&gt;
&lt;p&gt;On the technical side, we show for every input dimension $d &amp;gt; 0$, there is a
concept class of degree $\omega(1)$ multi-variate polynomials so that, using
$\omega(1)$-layer neural networks as learners, SGD can learn any function from
this class in $\mathsf{poly}(d)$ time to any $\frac{1}{\mathsf{poly}(d)}$
error, through learning to represent it as a composition of $\omega(1)$ layers
of quadratic functions using &quot;backward feature correction.&quot; In contrast, we do
not know any other simpler algorithm (including layerwise training, applying
kernel method sequentially, training a two-layer network, etc) that can learn
this concept class in $\mathsf{poly}(d)$ time even to any $d^{-0.01}$ error. As
a side result, we prove $d^{\omega(1)}$ lower bounds for several
non-hierarchical learners, including any kernel methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allen_Zhu_Z/0/1/0/all/0/1&quot;&gt;Zeyuan Allen-Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanzhi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2004.11145">
<title>F2A2: Flexible Fully-decentralized Approximate Actor-critic for Cooperative Multi-agent Reinforcement Learning. (arXiv:2004.11145v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2004.11145</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional centralized multi-agent reinforcement learning (MARL) algorithms
are sometimes unpractical in complicated applications, due to non-interactivity
between agents, curse of dimensionality and computation complexity. Hence,
several decentralized MARL algorithms are motivated. However, existing
decentralized methods only handle the fully cooperative setting where massive
information needs to be transmitted in training. The block coordinate gradient
descent scheme they used for successive independent actor and critic steps can
simplify the calculation, but it causes serious bias. In this paper, we propose
a flexible fully decentralized actor-critic MARL framework, which can combine
most of actor-critic methods, and handle large-scale general cooperative
multi-agent setting. A primal-dual hybrid gradient descent type algorithm
framework is designed to learn individual agents separately for
decentralization. From the perspective of each agent, policy improvement and
value evaluation are jointly optimized, which can stabilize multi-agent policy
learning. Furthermore, our framework can achieve scalability and stability for
large-scale environment and reduce information transmission, by the parameter
sharing mechanism and a novel modeling-other-agents methods based on
theory-of-mind and online supervised learning. Sufficient experiments in
cooperative Multi-agent Particle Environment and StarCraft II show that our
decentralized MARL instantiation algorithms perform competitively against
conventional centralized and decentralized methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenhao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1&quot;&gt;Bo Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiangfeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Junchi Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1&quot;&gt;Hongyuan Zha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2107.01943">
<title>When and How to Fool Explainable Models (and Humans) with Adversarial Examples. (arXiv:2107.01943v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2107.01943</link>
<description rdf:parseType="Literal">&lt;p&gt;Reliable deployment of machine learning models such as neural networks
continues to be challenging due to several limitations. Some of the main
shortcomings are the lack of interpretability and the lack of robustness
against adversarial examples or out-of-distribution inputs. In this exploratory
review, we explore the possibilities and limits of adversarial attacks for
explainable machine learning models. First, we extend the notion of adversarial
examples to fit in explainable machine learning scenarios, in which the inputs,
the output classifications and the explanations of the model&apos;s decisions are
assessed by humans. Next, we propose a comprehensive framework to study whether
(and how) adversarial examples can be generated for explainable models under
human assessment, introducing and illustrating novel attack paradigms. In
particular, our framework considers a wide range of relevant yet often ignored
factors such as the type of problem, the user expertise or the objective of the
explanations, in order to identify the attack strategies that should be adopted
in each scenario to successfully deceive the model (and the human). The
intention of these contributions is to serve as a basis for a more rigorous and
realistic study of adversarial examples in the field of explainable machine
learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vadillo_J/0/1/0/all/0/1&quot;&gt;Jon Vadillo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santana_R/0/1/0/all/0/1&quot;&gt;Roberto Santana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lozano_J/0/1/0/all/0/1&quot;&gt;Jose A. Lozano&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.09836">
<title>Creativity of AI: Hierarchical Planning Model Learning for Facilitating Deep Reinforcement Learning. (arXiv:2112.09836v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2112.09836</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite of achieving great success in real-world applications, Deep
Reinforcement Learning (DRL) is still suffering from three critical issues,
i.e., data efficiency, lack of the interpretability and transferability. Recent
research shows that embedding symbolic knowledge into DRL is promising in
addressing those challenges. Inspired by this, we introduce a novel deep
reinforcement learning framework with symbolic options. Our framework features
a loop training procedure, which enables guiding the improvement of policy by
planning with planning models (including action models and hierarchical task
network models) and symbolic options learned from interactive trajectories
automatically. The learned symbolic options alleviate the dense requirement of
expert domain knowledge and provide inherent interpretability of policies.
Moreover, the transferability and data efficiency can be further improved by
planning with the symbolic planning models. To validate the effectiveness of
our framework, we conduct experiments on two domains, Montezuma&apos;s Revenge and
Office World, respectively. The results demonstrate the comparable performance,
improved data efficiency, interpretability and transferability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuo_H/0/1/0/all/0/1&quot;&gt;Hankz Hankui Zhuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1&quot;&gt;Shuting Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1&quot;&gt;Mu Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zhihao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_K/0/1/0/all/0/1&quot;&gt;Kebing Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Chao Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.05745">
<title>Deep Optimal Transport for Domain Adaptation on SPD Manifolds. (arXiv:2201.05745v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2201.05745</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, there has been significant interest in solving the domain
adaptation (DA) problem on symmetric positive definite (SPD) manifolds within
the machine learning community. This interest stems from the fact that complex
neurophysiological data generated by medical equipment, such as
electroencephalograms, magnetoencephalograms, and diffusion tensor imaging,
often exhibit a shift in data distribution across different domains. These data
representations, represented by signal covariance matrices, possess properties
of symmetry and positive definiteness. However, directly applying previous
experiences and solutions to the DA problem poses challenges due to the
manipulation complexities of covariance matrices.To address this, our research
introduces a category of deep learning-based transfer learning approaches
called deep optimal transport. This category utilizes optimal transport theory
and leverages the Log-Euclidean geometry for SPD manifolds. Additionally, we
present a comprehensive categorization of existing geometric methods to tackle
these problems effectively. This categorization provides practical solutions
for specific DA problems, including handling discrepancies in marginal and
conditional distributions between the source and target domains on the SPD
manifold. To evaluate the effectiveness, we conduct experiments on three
publicly available highly non-stationary cross-session brain-computer interface
scenarios. Moreover, we provide visualization results on the SPD cone to offer
further insights into the framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ju_C/0/1/0/all/0/1&quot;&gt;Ce Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1&quot;&gt;Cuntai Guan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.03466">
<title>Reward-Respecting Subtasks for Model-Based Reinforcement Learning. (arXiv:2202.03466v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2202.03466</link>
<description rdf:parseType="Literal">&lt;p&gt;To achieve the ambitious goals of artificial intelligence, reinforcement
learning must include planning with a model of the world that is abstract in
state and time. Deep learning has made progress with state abstraction, but
temporal abstraction has rarely been used, despite extensively developed theory
based on the options framework. One reason for this is that the space of
possible options is immense, and the methods previously proposed for option
discovery do not take into account how the option models will be used in
planning. Options are typically discovered by posing subsidiary tasks, such as
reaching a bottleneck state or maximizing the cumulative sum of a sensory
signal other than reward. Each subtask is solved to produce an option, and then
a model of the option is learned and made available to the planning process. In
most previous work, the subtasks ignore the reward on the original problem,
whereas we propose subtasks that use the original reward plus a bonus based on
a feature of the state at the time the option terminates. We show that option
models obtained from such reward-respecting subtasks are much more likely to be
useful in planning than eigenoptions, shortest path options based on bottleneck
states, or reward-respecting options generated by the option-critic. Reward
respecting subtasks strongly constrain the space of options and thereby also
provide a partial solution to the problem of option discovery. Finally, we show
how values, policies, options, and models can all be learned online and
off-policy using standard algorithms and general value functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sutton_R/0/1/0/all/0/1&quot;&gt;Richard S. Sutton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Machado_M/0/1/0/all/0/1&quot;&gt;Marlos C. Machado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holland_G/0/1/0/all/0/1&quot;&gt;G. Zacharias Holland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szepesvari_D/0/1/0/all/0/1&quot;&gt;David Szepesvari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Timbers_F/0/1/0/all/0/1&quot;&gt;Finbarr Timbers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanner_B/0/1/0/all/0/1&quot;&gt;Brian Tanner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+White_A/0/1/0/all/0/1&quot;&gt;Adam White&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.08815">
<title>GRAPHSHAP: Explaining Identity-Aware Graph Classifiers Through the Language of Motifs. (arXiv:2202.08815v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2202.08815</link>
<description rdf:parseType="Literal">&lt;p&gt;Most methods for explaining black-box classifiers (e.g. on tabular data,
images, or time series) rely on measuring the impact that removing/perturbing
features has on the model output. This forces the explanation language to match
the classifier&apos;s feature space. However, when dealing with graph data, in which
the basic features correspond to the edges describing the graph structure, this
matching between features space and explanation language might not be
appropriate. Decoupling the feature space (edges) from a desired high-level
explanation language (such as motifs) is thus a major challenge towards
developing actionable explanations for graph classification tasks. In this
paper we introduce GRAPHSHAP, a Shapley-based approach able to provide
motif-based explanations for identity-aware graph classifiers, assuming no
knowledge whatsoever about the model or its training data: the only requirement
is that the classifier can be queried as a black-box at will. For the sake of
computational efficiency we explore a progressive approximation strategy and
show how a simple kernel can efficiently approximate explanation scores, thus
allowing GRAPHSHAP to scale on scenarios with a large explanation space (i.e.
large number of motifs). We showcase GRAPHSHAP on a real-world brain-network
dataset consisting of patients affected by Autism Spectrum Disorder and a
control group. Our experiments highlight how the classification provided by a
black-box model can be effectively explained by few connectomics patterns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perotti_A/0/1/0/all/0/1&quot;&gt;Alan Perotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bajardi_P/0/1/0/all/0/1&quot;&gt;Paolo Bajardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonchi_F/0/1/0/all/0/1&quot;&gt;Francesco Bonchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panisson_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Panisson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.09706">
<title>k-strip: A novel segmentation algorithm in k-space for the application of skull stripping. (arXiv:2205.09706v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2205.09706</link>
<description rdf:parseType="Literal">&lt;p&gt;Objectives: Present a novel deep learning-based skull stripping algorithm for
magnetic resonance imaging (MRI) that works directly in the information rich
k-space.
&lt;/p&gt;
&lt;p&gt;Materials and Methods: Using two datasets from different institutions with a
total of 36,900 MRI slices, we trained a deep learning-based model to work
directly with the complex raw k-space data. Skull stripping performed by HD-BET
(Brain Extraction Tool) in the image domain were used as the ground truth.
&lt;/p&gt;
&lt;p&gt;Results: Both datasets were very similar to the ground truth (DICE scores of
92\%-98\% and Hausdorff distances of under 5.5 mm). Results on slices above the
eye-region reach DICE scores of up to 99\%, while the accuracy drops in regions
around the eyes and below, with partially blurred output. The output of k-strip
often smoothed edges at the demarcation to the skull. Binary masks are created
with an appropriate threshold.
&lt;/p&gt;
&lt;p&gt;Conclusion: With this proof-of-concept study, we were able to show the
feasibility of working in the k-space frequency domain, preserving phase
information, with consistent results. Future research should be dedicated to
discovering additional ways the k-space can be used for innovative image
analysis and further workflows.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rempe_M/0/1/0/all/0/1&quot;&gt;Moritz Rempe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mentzel_F/0/1/0/all/0/1&quot;&gt;Florian Mentzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pomykala_K/0/1/0/all/0/1&quot;&gt;Kelsey L. Pomykala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Haubold_J/0/1/0/all/0/1&quot;&gt;Johannes Haubold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nensa_F/0/1/0/all/0/1&quot;&gt;Felix Nensa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kroninger_K/0/1/0/all/0/1&quot;&gt;Kevin Kr&amp;#xf6;ninger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Egger_J/0/1/0/all/0/1&quot;&gt;Jan Egger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kleesiek_J/0/1/0/all/0/1&quot;&gt;Jens Kleesiek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.09928">
<title>Self-Supervised Time Series Representation Learning via Cross Reconstruction Transformer. (arXiv:2205.09928v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.09928</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised/self-supervised representation learning in time series is
critical since labeled samples are usually scarce in real-world scenarios.
Existing approaches mainly leverage the contrastive learning framework, which
automatically learns to understand the similar and dissimilar data pairs.
Nevertheless, they are restricted to the prior knowledge of constructing pairs,
cumbersome sampling policy, and unstable performances when encountering
sampling bias. Also, few works have focused on effectively modeling across
temporal-spectral relations to extend the capacity of representations. In this
paper, we aim at learning representations for time series from a new
perspective and propose Cross Reconstruction Transformer (CRT) to solve the
aforementioned problems in a unified way. CRT achieves time series
representation learning through a cross-domain dropping-reconstruction task.
Specifically, we transform time series into the frequency domain and randomly
drop certain parts in both time and frequency domains. Dropping can maximally
preserve the global context compared to cropping and masking. Then a
transformer architecture is utilized to adequately capture the cross-domain
correlations between temporal and spectral information through reconstructing
data in both domains, which is called Dropped Temporal-Spectral Modeling. To
discriminate the representations in global latent space, we propose Instance
Discrimination Constraint to reduce the mutual information between different
time series and sharpen the decision boundaries. Additionally, we propose a
specified curriculum learning strategy to optimize the CRT, which progressively
increases the dropping ratio in the training process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenrui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Ling Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1&quot;&gt;Shijia Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1&quot;&gt;Shenda Hong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.13085">
<title>Identifying Patient-Specific Root Causes with the Heteroscedastic Noise Model. (arXiv:2205.13085v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2205.13085</link>
<description rdf:parseType="Literal">&lt;p&gt;Complex diseases are caused by a multitude of factors that may differ between
patients even within the same diagnostic category. A few underlying root causes
may nevertheless initiate the development of disease within each patient. We
therefore focus on identifying patient-specific root causes of disease, which
we equate to the sample-specific predictivity of the exogenous error terms in a
structural equation model. We generalize from the linear setting to the
heteroscedastic noise model where $Y = m(X) + \varepsilon\sigma(X)$ with
non-linear functions $m(X)$ and $\sigma(X)$ representing the conditional mean
and mean absolute deviation, respectively. This model preserves identifiability
but introduces non-trivial challenges that require a customized algorithm
called Generalized Root Causal Inference (GRCI) to extract the error terms
correctly. GRCI recovers patient-specific root causes more accurately than
existing alternatives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Strobl_E/0/1/0/all/0/1&quot;&gt;Eric V. Strobl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lasko_T/0/1/0/all/0/1&quot;&gt;Thomas A. Lasko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.11007">
<title>Avoiding Post-Processing with Event-Based Detection in Biomedical Signals. (arXiv:2209.11007v2 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/2209.11007</link>
<description rdf:parseType="Literal">&lt;p&gt;Objective: Finding events of interest is a common task in biomedical signal
processing. The detection of epileptic seizures and signal artefacts are two
key examples. Epoch-based classification is the typical machine learning
framework to detect such signal events because of the straightforward
application of classical machine learning techniques. Usually, post-processing
is required to achieve good performance and enforce temporal dependencies.
Designing the right post-processing scheme to convert these classification
outputs into events is a tedious, and labor-intensive element of this
framework. Methods: We propose an event-based modeling framework that directly
works with events as learning targets, stepping away from ad-hoc
post-processing schemes to turn model outputs into events. We illustrate the
practical power of this framework on simulated data and real-world data,
comparing it to epoch-based modeling approaches. Results: We show that
event-based modeling (without post-processing) performs on par with or better
than epoch-based modeling with extensive post-processing. Conclusion: These
results show the power of treating events as direct learning targets, instead
of using ad-hoc post-processing to obtain them, severely reducing design
effort. Significance: The event-based modeling framework can easily be applied
to other event detection problems in signal processing, removing the need for
intensive task-specific post-processing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Seeuws_N/0/1/0/all/0/1&quot;&gt;Nick Seeuws&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vos_M/0/1/0/all/0/1&quot;&gt;Maarten De Vos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bertrand_A/0/1/0/all/0/1&quot;&gt;Alexander Bertrand&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.01785">
<title>TabLeak: Tabular Data Leakage in Federated Learning. (arXiv:2210.01785v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.01785</link>
<description rdf:parseType="Literal">&lt;p&gt;While federated learning (FL) promises to preserve privacy, recent works in
the image and text domains have shown that training updates leak private client
data. However, most high-stakes applications of FL (e.g., in healthcare and
finance) use tabular data, where the risk of data leakage has not yet been
explored. A successful attack for tabular data must address two key challenges
unique to the domain: (i) obtaining a solution to a high-variance mixed
discrete-continuous optimization problem, and (ii) enabling human assessment of
the reconstruction as unlike for image and text data, direct human inspection
is not possible. In this work we address these challenges and propose TabLeak,
the first comprehensive reconstruction attack on tabular data. TabLeak is based
on two key contributions: (i) a method which leverages a softmax relaxation and
pooled ensembling to solve the optimization problem, and (ii) an entropy-based
uncertainty quantification scheme to enable human assessment. We evaluate
TabLeak on four tabular datasets for both FedSGD and FedAvg training protocols,
and show that it successfully breaks several settings previously deemed safe.
For instance, we extract large subsets of private data at &amp;gt;90% accuracy even at
the large batch size of 128. Our findings demonstrate that current high-stakes
tabular FL is excessively vulnerable to leakage attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vero_M/0/1/0/all/0/1&quot;&gt;Mark Vero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balunovic_M/0/1/0/all/0/1&quot;&gt;Mislav Balunovi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dimitrov_D/0/1/0/all/0/1&quot;&gt;Dimitar I. Dimitrov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vechev_M/0/1/0/all/0/1&quot;&gt;Martin Vechev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.01888">
<title>Bicriteria Approximation Algorithms for Priority Matroid Median. (arXiv:2210.01888v2 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/2210.01888</link>
<description rdf:parseType="Literal">&lt;p&gt;Fairness considerations have motivated new clustering problems and algorithms
in recent years. In this paper we consider the Priority Matroid Median problem
which generalizes the Priority $k$-Median problem that has recently been
studied. The input consists of a set of facilities $\mathcal{F}$ and a set of
clients $\mathcal{C}$ that lie in a metric space $(\mathcal{F} \cup
\mathcal{C},d)$, and a matroid $\mathcal{M}=(\mathcal{F},\mathcal{I})$ over the
facilities. In addition each client $j$ has a specified radius $r_j \ge 0$ and
each facility $i \in \mathcal{F}$ has an opening cost $f_i$. The goal is to
choose a subset $S \subseteq \mathcal{F}$ of facilities to minimize the
$\sum_{i \in \mathcal{F}} f_i + \sum_{j \in \mathcal{C}} d(j,S)$ subject to two
constraints: (i) $S$ is an independent set in $\mathcal{M}$ (that is $S \in
\mathcal{I}$) and (ii) for each client $j$, its distance to an open facility is
at most $r_j$ (that is, $d(j,S) \le r_j$). For this problem we describe the
first bicriteria $(c_1,c_2)$ approximations for fixed constants $c_1,c_2$: the
radius constraints of the clients are violated by at most a factor of $c_1$ and
the objective cost is at most $c_2$ times the optimum cost. We also improve the
previously known bicriteria approximation for the uniform radius setting ($r_j
:= L$ $\forall j \in \mathcal{C}$).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bajpai_T/0/1/0/all/0/1&quot;&gt;Tanvi Bajpai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chekuri_C/0/1/0/all/0/1&quot;&gt;Chandra Chekuri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.04882">
<title>Layer Ensembles. (arXiv:2210.04882v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.04882</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Ensembles, as a type of Bayesian Neural Networks, can be used to
estimate uncertainty on the prediction of multiple neural networks by
collecting votes from each network and computing the difference in those
predictions. In this paper, we introduce a method for uncertainty estimation
that considers a set of independent categorical distributions for each layer of
the network, giving many more possible samples with overlapped layers than in
the regular Deep Ensembles. We further introduce an optimized inference
procedure that reuses common layer outputs, achieving up to 19x speed up and
reducing memory usage quadratically. We also show that the method can be
further improved by ranking samples, resulting in models that require less
memory and time to run while achieving higher uncertainty quality than Deep
Ensembles.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oleksiienko_I/0/1/0/all/0/1&quot;&gt;Illia Oleksiienko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1&quot;&gt;Alexandros Iosifidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.05953">
<title>Breadth-First Pipeline Parallelism. (arXiv:2211.05953v2 [cs.DC] UPDATED)</title>
<link>http://arxiv.org/abs/2211.05953</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Breadth-First Pipeline Parallelism, a novel training schedule
which optimizes the combination of pipeline and data parallelism. Breadth-First
Pipeline Parallelism lowers training time, cost and memory usage by combining a
high GPU utilization with a small batch size per GPU, and by making use of
fully sharded data parallelism. Experimentally, we observed an increase of up
to 43% in training throughput for a 52 billion-parameter model using a small
batch size per GPU compared to Megatron-LM, which would reduce the training
time and cost by the same amount on a large GPU cluster.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lamy_Poirier_J/0/1/0/all/0/1&quot;&gt;Joel Lamy-Poirier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.06489">
<title>Equivariance with Learned Canonicalization Functions. (arXiv:2211.06489v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.06489</link>
<description rdf:parseType="Literal">&lt;p&gt;Symmetry-based neural networks often constrain the architecture in order to
achieve invariance or equivariance to a group of transformations. In this
paper, we propose an alternative that avoids this architectural constraint by
learning to produce canonical representations of the data. These
canonicalization functions can readily be plugged into non-equivariant backbone
architectures. We offer explicit ways to implement them for some groups of
interest. We show that this approach enjoys universality while providing
interpretable insights. Our main hypothesis, supported by our empirical
results, is that learning a small neural network to perform canonicalization is
better than using predefined heuristics. Our experiments show that learning the
canonicalization function is competitive with existing techniques for learning
equivariant functions across many tasks, including image classification,
$N$-body dynamics prediction, point cloud classification and part segmentation,
while being faster across the board.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaba_S/0/1/0/all/0/1&quot;&gt;S&amp;#xe9;kou-Oumar Kaba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mondal_A/0/1/0/all/0/1&quot;&gt;Arnab Kumar Mondal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravanbakhsh_S/0/1/0/all/0/1&quot;&gt;Siamak Ravanbakhsh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.06573">
<title>On the Evolution of (Hateful) Memes by Means of Multimodal Contrastive Learning. (arXiv:2212.06573v2 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/2212.06573</link>
<description rdf:parseType="Literal">&lt;p&gt;The dissemination of hateful memes online has adverse effects on social media
platforms and the real world. Detecting hateful memes is challenging, one of
the reasons being the evolutionary nature of memes; new hateful memes can
emerge by fusing hateful connotations with other cultural ideas or symbols. In
this paper, we propose a framework that leverages multimodal contrastive
learning models, in particular OpenAI&apos;s CLIP, to identify targets of hateful
content and systematically investigate the evolution of hateful memes. We find
that semantic regularities exist in CLIP-generated embeddings that describe
semantic relationships within the same modality (images) or across modalities
(images and text). Leveraging this property, we study how hateful memes are
created by combining visual elements from multiple images or fusing textual
information with a hateful image. We demonstrate the capabilities of our
framework for analyzing the evolution of hateful memes by focusing on
antisemitic memes, particularly the Happy Merchant meme. Using our framework on
a dataset extracted from 4chan, we find 3.3K variants of the Happy Merchant
meme, with some linked to specific countries, persons, or organizations. We
envision that our framework can be used to aid human moderators by flagging new
variants of hateful memes so that moderators can manually verify them and
mitigate the problem of hateful content online.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1&quot;&gt;Yiting Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xinlei He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pierson_S/0/1/0/all/0/1&quot;&gt;Shannon Pierson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Backes_M/0/1/0/all/0/1&quot;&gt;Michael Backes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zannettou_S/0/1/0/all/0/1&quot;&gt;Savvas Zannettou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.09811">
<title>Memory-efficient NLLB-200: Language-specific Expert Pruning of a Massively Multilingual Machine Translation Model. (arXiv:2212.09811v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2212.09811</link>
<description rdf:parseType="Literal">&lt;p&gt;The recently released NLLB-200 is a set of multilingual Neural Machine
Translation models that cover 202 languages. The largest model is based on a
Mixture of Experts architecture and achieves SoTA results across many language
pairs. It contains 54.5B parameters and requires at least four 32GB GPUs just
for inference. In this work, we propose a pruning method that enables the
removal of up to 80% of experts without further finetuning and with a
negligible loss in translation quality, which makes it feasible to run the
model on a single 32GB GPU. Further analysis suggests that our pruning metrics
can identify language-specific experts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koishekenov_Y/0/1/0/all/0/1&quot;&gt;Yeskendir Koishekenov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berard_A/0/1/0/all/0/1&quot;&gt;Alexandre Berard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikoulina_V/0/1/0/all/0/1&quot;&gt;Vassilina Nikoulina&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.10936">
<title>A Memetic Algorithm with Reinforcement Learning for Sociotechnical Production Scheduling. (arXiv:2212.10936v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.10936</link>
<description rdf:parseType="Literal">&lt;p&gt;The following interdisciplinary article presents a memetic algorithm with
applying deep reinforcement learning (DRL) for solving practically oriented
dual resource constrained flexible job shop scheduling problems (DRC-FJSSP).
From research projects in industry, we recognize the need to consider flexible
machines, flexible human workers, worker capabilities, setup and processing
operations, material arrival times, complex job paths with parallel tasks for
bill of material (BOM) manufacturing, sequence-dependent setup times and
(partially) automated tasks in human-machine-collaboration. In recent years,
there has been extensive research on metaheuristics and DRL techniques but
focused on simple scheduling environments. However, there are few approaches
combining metaheuristics and DRL to generate schedules more reliably and
efficiently. In this paper, we first formulate a DRC-FJSSP to map complex
industry requirements beyond traditional job shop models. Then we propose a
scheduling framework integrating a discrete event simulation (DES) for schedule
evaluation, considering parallel computing and multicriteria optimization.
Here, a memetic algorithm is enriched with DRL to improve sequencing and
assignment decisions. Through numerical experiments with real-world production
data, we confirm that the framework generates feasible schedules efficiently
and reliably for a balanced optimization of makespan (MS) and total tardiness
(TT). Utilizing DRL instead of random metaheuristic operations leads to better
results in fewer algorithm iterations and outperforms traditional approaches in
such complex environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grumbach_F/0/1/0/all/0/1&quot;&gt;Felix Grumbach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Badr_N/0/1/0/all/0/1&quot;&gt;Nour Eldin Alaa Badr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reusch_P/0/1/0/all/0/1&quot;&gt;Pascal Reusch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trojahn_S/0/1/0/all/0/1&quot;&gt;Sebastian Trojahn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.11498">
<title>Scalable Multi-Agent Reinforcement Learning for Warehouse Logistics with Robotic and Human Co-Workers. (arXiv:2212.11498v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.11498</link>
<description rdf:parseType="Literal">&lt;p&gt;We envision a warehouse in which dozens of mobile robots and human pickers
work together to collect and deliver items within the warehouse. The
fundamental problem we tackle, called the order-picking problem, is how these
worker agents must coordinate their movement and actions in the warehouse to
maximise performance (e.g. order throughput). Established industry methods
using heuristic approaches require large engineering efforts to optimise for
innately variable warehouse configurations. In contrast, multi-agent
reinforcement learning (MARL) can be flexibly applied to diverse warehouse
configurations (e.g. size, layout, number/types of workers, item replenishment
frequency), as the agents learn through experience how to optimally cooperate
with one another. We develop hierarchical MARL algorithms in which a manager
assigns goals to worker agents, and the policies of the manager and workers are
co-trained toward maximising a global objective (e.g. pick rate). Our
hierarchical algorithms achieve significant gains in sample efficiency and
overall pick rates over baseline MARL algorithms in diverse warehouse
configurations, and substantially outperform two established industry
heuristics for order-picking systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krnjaic_A/0/1/0/all/0/1&quot;&gt;Aleksandar Krnjaic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steleac_R/0/1/0/all/0/1&quot;&gt;Raul D. Steleac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thomas_J/0/1/0/all/0/1&quot;&gt;Jonathan D. Thomas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papoudakis_G/0/1/0/all/0/1&quot;&gt;Georgios Papoudakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schafer_L/0/1/0/all/0/1&quot;&gt;Lukas Sch&amp;#xe4;fer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+To_A/0/1/0/all/0/1&quot;&gt;Andrew Wing Keung To&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lao_K/0/1/0/all/0/1&quot;&gt;Kuan-Ho Lao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cubuktepe_M/0/1/0/all/0/1&quot;&gt;Murat Cubuktepe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haley_M/0/1/0/all/0/1&quot;&gt;Matthew Haley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borsting_P/0/1/0/all/0/1&quot;&gt;Peter B&amp;#xf6;rsting&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1&quot;&gt;Stefano V. Albrecht&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.00719">
<title>Detection of Groups with Biased Representation in Ranking. (arXiv:2301.00719v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.00719</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-life tools for decision-making in many critical domains are based on
ranking results. With the increasing awareness of algorithmic fairness, recent
works have presented measures for fairness in ranking. Many of those
definitions consider the representation of different ``protected groups&apos;&apos;, in
the top-$k$ ranked items, for any reasonable $k$. Given the protected groups,
confirming algorithmic fairness is a simple task. However, the groups&apos;
definitions may be unknown in advance. In this paper, we study the problem of
detecting groups with biased representation in the top-$k$ ranked items,
eliminating the need to pre-define protected groups. The number of such groups
possible can be exponential, making the problem hard. We propose efficient
search algorithms for two different fairness measures: global representation
bounds, and proportional representation. Then we propose a method to explain
the bias in the representations of groups utilizing the notion of Shapley
values. We conclude with an experimental study, showing the scalability of our
approach and demonstrating the usefulness of the proposed algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jinyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moskovitch_Y/0/1/0/all/0/1&quot;&gt;Yuval Moskovitch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jagadish_H/0/1/0/all/0/1&quot;&gt;H. V. Jagadish&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.10886">
<title>Automatic Intrinsic Reward Shaping for Exploration in Deep Reinforcement Learning. (arXiv:2301.10886v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.10886</link>
<description rdf:parseType="Literal">&lt;p&gt;We present AIRS: Automatic Intrinsic Reward Shaping that intelligently and
adaptively provides high-quality intrinsic rewards to enhance exploration in
reinforcement learning (RL). More specifically, AIRS selects shaping function
from a predefined set based on the estimated task return in real-time,
providing reliable exploration incentives and alleviating the biased objective
problem. Moreover, we develop an intrinsic reward toolkit to provide efficient
and reliable implementations of diverse intrinsic reward approaches. We test
AIRS on various tasks of MiniGrid, Procgen, and DeepMind Control Suite.
Extensive simulation demonstrates that AIRS can outperform the benchmarking
schemes and achieve superior performance with simple architecture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1&quot;&gt;Mingqi Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1&quot;&gt;Xin Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1&quot;&gt;Wenjun Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.03314">
<title>Federated Variational Inference Methods for Structured Latent Variable Models. (arXiv:2302.03314v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2302.03314</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning methods enable model training across distributed data
sources without data leaving their original locations and have gained
increasing interest in various fields. However, existing approaches are
limited, excluding many structured probabilistic models. We present a general
and elegant solution based on structured variational inference, widely used in
Bayesian machine learning, adapted for the federated setting. Additionally, we
provide a communication-efficient variant analogous to the canonical FedAvg
algorithm. The proposed algorithms&apos; effectiveness is demonstrated, and their
performance is compared with hierarchical Bayesian neural networks and topic
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hassan_C/0/1/0/all/0/1&quot;&gt;Conor Hassan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Salomone_R/0/1/0/all/0/1&quot;&gt;Robert Salomone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mengersen_K/0/1/0/all/0/1&quot;&gt;Kerrie Mengersen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.08981">
<title>Black-Box Batch Active Learning for Regression. (arXiv:2302.08981v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.08981</link>
<description rdf:parseType="Literal">&lt;p&gt;Batch active learning is a popular approach for efficiently training machine
learning models on large, initially unlabelled datasets by repeatedly acquiring
labels for batches of data points. However, many recent batch active learning
methods are white-box approaches and are often limited to differentiable
parametric models: they score unlabeled points using acquisition functions
based on model embeddings or first- and second-order derivatives. In this
paper, we propose black-box batch active learning for regression tasks as an
extension of white-box approaches. Crucially, our method only relies on model
predictions. This approach is compatible with a wide range of machine learning
models, including regular and Bayesian deep learning models and
non-differentiable models such as random forests. It is rooted in Bayesian
principles and utilizes recent kernel-based approaches. This allows us to
extend a wide range of existing state-of-the-art white-box batch active
learning methods (BADGE, BAIT, LCMD) to black-box models. We demonstrate the
effectiveness of our approach through extensive experimental evaluations on
regression datasets, achieving surprisingly strong performance compared to
white-box approaches for deep learning models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirsch_A/0/1/0/all/0/1&quot;&gt;Andreas Kirsch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.10289">
<title>Tackling Shortcut Learning in Deep Neural Networks: An Iterative Approach with Interpretable Models. (arXiv:2302.10289v9 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.10289</link>
<description rdf:parseType="Literal">&lt;p&gt;We use concept-based interpretable models to mitigate shortcut learning.
Existing methods lack interpretability. Beginning with a Blackbox, we
iteratively carve out a mixture of interpretable experts (MoIE) and a residual
network. Each expert explains a subset of data using First Order Logic (FOL).
While explaining a sample, the FOL from biased BB-derived MoIE detects the
shortcut effectively. Finetuning the BB with Metadata Normalization (MDN)
eliminates the shortcut. The FOLs from the finetuned-BB-derived MoIE verify the
elimination of the shortcut. Our experiments show that MoIE does not hurt the
accuracy of the original BB and eliminates shortcuts effectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Shantanu Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1&quot;&gt;Ke Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arabshahi_F/0/1/0/all/0/1&quot;&gt;Forough Arabshahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Batmanghelich_K/0/1/0/all/0/1&quot;&gt;Kayhan Batmanghelich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.10433">
<title>On discrete symmetries of robotics systems: A group-theoretic and data-driven analysis. (arXiv:2302.10433v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2302.10433</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a comprehensive study on discrete morphological symmetries of
dynamical systems, which are commonly observed in biological and artificial
locomoting systems, such as legged, swimming, and flying animals/robots/virtual
characters. These symmetries arise from the presence of one or more planes/axis
of symmetry in the system&apos;s morphology, resulting in harmonious duplication and
distribution of body parts. Significantly, we characterize how morphological
symmetries extend to symmetries in the system&apos;s dynamics, optimal control
policies, and in all proprioceptive and exteroceptive measurements related to
the system&apos;s dynamics evolution. In the context of data-driven methods,
symmetry represents an inductive bias that justifies the use of data
augmentation or symmetric function approximators. To tackle this, we present a
theoretical and practical framework for identifying the system&apos;s morphological
symmetry group $\G$ and characterizing the symmetries in proprioceptive and
exteroceptive data measurements. We then exploit these symmetries using data
augmentation and $\G$-equivariant neural networks. Our experiments on both
synthetic and real-world applications provide empirical evidence of the
advantageous outcomes resulting from the exploitation of these symmetries,
including improved sample efficiency, enhanced generalization, and reduction of
trainable parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ordonez_Apraez_D/0/1/0/all/0/1&quot;&gt;Daniel Ordonez-Apraez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_M/0/1/0/all/0/1&quot;&gt;Mario Martin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agudo_A/0/1/0/all/0/1&quot;&gt;Antonio Agudo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1&quot;&gt;Francesc Moreno-Noguer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.00589">
<title>Composite Optimization Algorithms for Sigmoid Networks. (arXiv:2303.00589v3 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2303.00589</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we use composite optimization algorithms to solve sigmoid
networks. We equivalently transfer the sigmoid networks to a convex composite
optimization and propose the composite optimization algorithms based on the
linearized proximal algorithms and the alternating direction method of
multipliers. Under the assumptions of the weak sharp minima and the regularity
condition, the algorithm is guaranteed to converge to a globally optimal
solution of the objective function even in the case of non-convex and
non-smooth problems. Furthermore, the convergence results can be directly
related to the amount of training data and provide a general guide for setting
the size of sigmoid networks. Numerical experiments on Franke&apos;s function
fitting and handwritten digit recognition show that the proposed algorithms
perform satisfactorily and robustly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huixiong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ye_Q/0/1/0/all/0/1&quot;&gt;Qi Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.00800">
<title>Continuous-Time Functional Diffusion Processes. (arXiv:2303.00800v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.00800</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Functional Diffusion Processes (FDPs), which generalize
score-based diffusion models to infinite-dimensional function spaces. FDPs
require a new mathematical framework to describe the forward and backward
dynamics, and several extensions to derive practical training objectives. These
include infinite-dimensional versions of Girsanov theorem, in order to be able
to compute an ELBO, and of the sampling theorem, in order to guarantee that
functional evaluations in a countable set of points are equivalent to
infinite-dimensional functions. We use FDPs to build a new breed of generative
models in function spaces, which do not require specialized network
architectures, and that can work with any kind of continuous data. Our results
on real data show that FDPs achieve high-quality image generation, using a
simple MLP architecture with orders of magnitude fewer parameters than existing
diffusion models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Franzese_G/0/1/0/all/0/1&quot;&gt;Giulio Franzese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corallo_G/0/1/0/all/0/1&quot;&gt;Giulio Corallo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rossi_S/0/1/0/all/0/1&quot;&gt;Simone Rossi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heinonen_M/0/1/0/all/0/1&quot;&gt;Markus Heinonen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Filippone_M/0/1/0/all/0/1&quot;&gt;Maurizio Filippone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michiardi_P/0/1/0/all/0/1&quot;&gt;Pietro Michiardi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.01483">
<title>Auxiliary Functions as Koopman Observables: Data-Driven Polynomial Optimization for Dynamical Systems. (arXiv:2303.01483v2 [math.DS] UPDATED)</title>
<link>http://arxiv.org/abs/2303.01483</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a flexible data-driven method for dynamical system analysis that
does not require explicit model discovery. The method is rooted in
well-established techniques for approximating the Koopman operator from data
and is implemented as a semidefinite program that can be solved numerically.
Furthermore, the method is agnostic of whether data is generated through a
deterministic or stochastic process, so its implementation requires no prior
adjustments by the user to accommodate these different scenarios. Rigorous
convergence results justify the applicability of the method, while also
extending and uniting similar results from across the literature. Examples on
discovering Lyapunov functions, performing ergodic optimization, and bounding
extrema over attractors for both deterministic and stochastic dynamics
exemplify these convergence results and demonstrate the performance of the
method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Bramburger_J/0/1/0/all/0/1&quot;&gt;Jason J. Bramburger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Fantuzzi_G/0/1/0/all/0/1&quot;&gt;Giovanni Fantuzzi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.16982">
<title>Highly Accurate Quantum Chemical Property Prediction with Uni-Mol+. (arXiv:2303.16982v2 [physics.chem-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2303.16982</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent developments in deep learning have made remarkable progress in
speeding up the prediction of quantum chemical (QC) properties by removing the
need for expensive electronic structure calculations like density functional
theory. However, previous methods learned from 1D SMILES sequences or 2D
molecular graphs failed to achieve high accuracy as QC properties primarily
depend on the 3D equilibrium conformations optimized by electronic structure
methods, far different from the sequence-type and graph-type data. In this
paper, we propose a novel approach called Uni-Mol+ to tackle this challenge.
Uni-Mol+ first generates a raw 3D molecule conformation from inexpensive
methods such as RDKit. Then, the raw conformation is iteratively updated to its
target DFT equilibrium conformation using neural networks, and the learned
conformation will be used to predict the QC properties. To effectively learn
this update process towards the equilibrium conformation, we introduce a
two-track Transformer model backbone and train it with the QC property
prediction task. We also design a novel approach to guide the model&apos;s training
process. Our extensive benchmarking results demonstrate that the proposed
Uni-Mol+ significantly improves the accuracy of QC property prediction in
various datasets. We have made the code and model publicly available at
\url{https://github.com/dptech-corp/Uni-Mol}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Lu_S/0/1/0/all/0/1&quot;&gt;Shuqi Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Gao_Z/0/1/0/all/0/1&quot;&gt;Zhifeng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+He_D/0/1/0/all/0/1&quot;&gt;Di He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Linfeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ke_G/0/1/0/all/0/1&quot;&gt;Guolin Ke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.17716">
<title>Multiclass Online Learning and Uniform Convergence. (arXiv:2303.17716v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.17716</link>
<description rdf:parseType="Literal">&lt;p&gt;We study multiclass classification in the agnostic adversarial online
learning setting. As our main result, we prove that any multiclass concept
class is agnostically learnable if and only if its Littlestone dimension is
finite. This solves an open problem studied by Daniely, Sabato, Ben-David, and
Shalev-Shwartz (2011,2015) who handled the case when the number of classes (or
labels) is bounded. We also prove a separation between online learnability and
online uniform convergence by exhibiting an easy-to-learn class whose
sequential Rademacher complexity is unbounded.
&lt;/p&gt;
&lt;p&gt;Our learning algorithm uses the multiplicative weights algorithm, with a set
of experts defined by executions of the Standard Optimal Algorithm on
subsequences of size Littlestone dimension. We argue that the best expert has
regret at most Littlestone dimension relative to the best concept in the class.
This differs from the well-known covering technique of Ben-David, P\&apos;{a}l, and
Shalev-Shwartz (2009) for binary classification, where the best expert has
regret zero.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanneke_S/0/1/0/all/0/1&quot;&gt;Steve Hanneke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1&quot;&gt;Shay Moran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raman_V/0/1/0/all/0/1&quot;&gt;Vinod Raman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subedi_U/0/1/0/all/0/1&quot;&gt;Unique Subedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tewari_A/0/1/0/all/0/1&quot;&gt;Ambuj Tewari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.11834">
<title>Robust Tickets Can Transfer Better: Drawing More Transferable Subnetworks in Transfer Learning. (arXiv:2304.11834v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.11834</link>
<description rdf:parseType="Literal">&lt;p&gt;Transfer learning leverages feature representations of deep neural networks
(DNNs) pretrained on source tasks with rich data to empower effective
finetuning on downstream tasks. However, the pretrained models are often
prohibitively large for delivering generalizable representations, which limits
their deployment on edge devices with constrained resources. To close this gap,
we propose a new transfer learning pipeline, which leverages our finding that
robust tickets can transfer better, i.e., subnetworks drawn with properly
induced adversarial robustness can win better transferability over vanilla
lottery ticket subnetworks. Extensive experiments and ablation studies validate
that our proposed transfer learning pipeline can achieve enhanced
accuracy-sparsity trade-offs across both diverse downstream tasks and sparsity
patterns, further enriching the lottery ticket hypothesis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yonggan Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Ye Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Shang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1&quot;&gt;Jiayi Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yingyan Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.14102">
<title>SocNavGym: A Reinforcement Learning Gym for Social Navigation. (arXiv:2304.14102v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2304.14102</link>
<description rdf:parseType="Literal">&lt;p&gt;It is essential for autonomous robots to be socially compliant while
navigating in human-populated environments. Machine Learning and, especially,
Deep Reinforcement Learning have recently gained considerable traction in the
field of Social Navigation. This can be partially attributed to the resulting
policies not being bound by human limitations in terms of code complexity or
the number of variables that are handled. Unfortunately, the lack of safety
guarantees and the large data requirements by DRL algorithms make learning in
the real world unfeasible. To bridge this gap, simulation environments are
frequently used. We propose SocNavGym, an advanced simulation environment for
social navigation that can generate a wide variety of social navigation
scenarios and facilitates the development of intelligent social agents.
SocNavGym is light-weight, fast, easy-to-use, and can be effortlessly
configured to generate different types of social navigation scenarios. It can
also be configured to work with different hand-crafted and data-driven social
reward signals and to yield a variety of evaluation metrics to benchmark
agents&apos; performance. Further, we also provide a case study where a Dueling-DQN
agent is trained to learn social-navigation policies using SocNavGym. The
results provides evidence that SocNavGym can be used to train an agent from
scratch to navigate in simple as well as complex social scenarios. Our
experiments also show that the agents trained using the data-driven reward
function displays more advanced social compliance in comparison to the
heuristic-based reward function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kapoor_A/0/1/0/all/0/1&quot;&gt;Aditya Kapoor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swamy_S/0/1/0/all/0/1&quot;&gt;Sushant Swamy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manso_L/0/1/0/all/0/1&quot;&gt;Luis Manso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bachiller_P/0/1/0/all/0/1&quot;&gt;Pilar Bachiller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.07368">
<title>Decentralized Learning over Wireless Networks: The Effect of Broadcast with Random Access. (arXiv:2305.07368v2 [cs.NI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.07368</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we focus on the communication aspect of decentralized learning,
which involves multiple agents training a shared machine learning model using
decentralized stochastic gradient descent (D-SGD) over distributed data. In
particular, we investigate the impact of broadcast transmission and
probabilistic random access policy on the convergence performance of D-SGD,
considering the broadcast nature of wireless channels and the link dynamics in
the communication topology. Our results demonstrate that optimizing the access
probability to maximize the expected number of successful links is a highly
effective strategy for accelerating the system convergence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dahl_M/0/1/0/all/0/1&quot;&gt;Martin Dahl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Larsson_E/0/1/0/all/0/1&quot;&gt;Erik G. Larsson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17760">
<title>Language Models are Bounded Pragmatic Speakers: Understanding RLHF from a Bayesian Cognitive Modeling Perspective. (arXiv:2305.17760v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17760</link>
<description rdf:parseType="Literal">&lt;p&gt;How do language models &quot;think&quot;? This paper formulates a probabilistic
cognitive model called the bounded pragmatic speaker, which can characterize
the operation of different variations of language models. Specifically, we
demonstrate that large language models fine-tuned with reinforcement learning
from human feedback (Ouyang et al., 2022) embody a model of thought that
conceptually resembles a fast-and-slow model (Kahneman, 2011), which
psychologists have attributed to humans. We discuss the limitations of
reinforcement learning from human feedback as a fast-and-slow model of thought
and propose avenues for expanding this framework. In essence, our research
highlights the value of adopting a cognitive probabilistic modeling approach to
gain insights into the comprehension, evaluation, and advancement of language
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1&quot;&gt;Khanh Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04719">
<title>Don&apos;t trust your eyes: on the (un)reliability of feature visualizations. (arXiv:2306.04719v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04719</link>
<description rdf:parseType="Literal">&lt;p&gt;How do neural networks extract patterns from pixels? Feature visualizations
attempt to answer this important question by visualizing highly activating
patterns through optimization. Today, visualization methods form the foundation
of our knowledge about the internal workings of neural networks, as a type of
mechanistic interpretability. Here we ask: How reliable are feature
visualizations? We start our investigation by developing network circuits that
trick feature visualizations into showing arbitrary patterns that are
completely disconnected from normal network behavior on natural input. We then
provide evidence for a similar phenomenon occurring in standard, unmanipulated
networks: feature visualizations are processed very differently from standard
input, casting doubt on their ability to &quot;explain&quot; how neural networks process
natural images. We underpin this empirical finding by theory proving that the
set of functions that can be reliably understood by feature visualization is
extremely small and does not include general black-box neural networks.
Therefore, a promising way forward could be the development of networks that
enforce certain structures in order to ensure more reliable feature
visualizations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geirhos_R/0/1/0/all/0/1&quot;&gt;Robert Geirhos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1&quot;&gt;Roland S. Zimmermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bilodeau_B/0/1/0/all/0/1&quot;&gt;Blair Bilodeau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1&quot;&gt;Wieland Brendel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1&quot;&gt;Been Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05412">
<title>Offline Prioritized Experience Replay. (arXiv:2306.05412v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05412</link>
<description rdf:parseType="Literal">&lt;p&gt;Offline reinforcement learning (RL) is challenged by the distributional shift
problem. To address this problem, existing works mainly focus on designing
sophisticated policy constraints between the learned policy and the behavior
policy. However, these constraints are applied equally to well-performing and
inferior actions through uniform sampling, which might negatively affect the
learned policy. To alleviate this issue, we propose Offline Prioritized
Experience Replay (OPER), featuring a class of priority functions designed to
prioritize highly-rewarding transitions, making them more frequently visited
during training. Through theoretical analysis, we show that this class of
priority functions induce an improved behavior policy, and when constrained to
this improved policy, a policy-constrained offline RL algorithm is likely to
yield a better solution. We develop two practical strategies to obtain priority
weights by estimating advantages based on a fitted value network (OPER-A) or
utilizing trajectory returns (OPER-R) for quick computation. OPER is a
plug-and-play component for offline RL algorithms. As case studies, we evaluate
OPER on five different algorithms, including BC, TD3+BC, Onestep RL, CQL, and
IQL. Extensive experiments demonstrate that both OPER-A and OPER-R
significantly improve the performance for all baseline methods. Codes and
priority weights are availiable at https://github.com/sail-sg/OPER.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1&quot;&gt;Yang Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1&quot;&gt;Bingyi Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1&quot;&gt;Gao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Shiji Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1&quot;&gt;Shuicheng Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10278">
<title>Adaptive Strategies in Non-convex Optimization. (arXiv:2306.10278v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10278</link>
<description rdf:parseType="Literal">&lt;p&gt;An algorithm is said to be adaptive to a certain parameter (of the problem)
if it does not need a priori knowledge of such a parameter but performs
competitively to those that know it. This dissertation presents our work on
adaptive algorithms in following scenarios: 1. In the stochastic optimization
setting, we only receive stochastic gradients and the level of noise in
evaluating them greatly affects the convergence rate. Tuning is typically
required when without prior knowledge of the noise scale in order to achieve
the optimal rate. Considering this, we designed and analyzed noise-adaptive
algorithms that can automatically ensure (near)-optimal rates under different
noise scales without knowing it. 2. In training deep neural networks, the
scales of gradient magnitudes in each coordinate can scatter across a very wide
range unless normalization techniques, like BatchNorm, are employed. In such
situations, algorithms not addressing this problem of gradient scales can
behave very poorly. To mitigate this, we formally established the advantage of
scale-free algorithms that adapt to the gradient scales and presented its real
benefits in empirical experiments. 3. Traditional analyses in non-convex
optimization typically rely on the smoothness assumption. Yet, this condition
does not capture the properties of some deep learning objective functions,
including the ones involving Long Short-Term Memory networks and Transformers.
Instead, they satisfy a much more relaxed condition, with potentially unbounded
smoothness. Under this condition, we show that a generalized SignSGD algorithm
can theoretically match the best-known convergence rates obtained by SGD with
gradient clipping but does not need explicit clipping at all, and it can
empirically match the performance of Adam and beat others. Moreover, it can
also be made to automatically adapt to the unknown relaxed smoothness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1&quot;&gt;Zhenxun Zhuang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.12006">
<title>Learning Homogenization for Elliptic Operators. (arXiv:2306.12006v2 [math.NA] UPDATED)</title>
<link>http://arxiv.org/abs/2306.12006</link>
<description rdf:parseType="Literal">&lt;p&gt;Multiscale partial differential equations (PDEs) arise in various
applications, and several schemes have been developed to solve them
efficiently. Homogenization theory is a powerful methodology that eliminates
the small-scale dependence, resulting in simplified equations that are
computationally tractable. In the field of continuum mechanics, homogenization
is crucial for deriving constitutive laws that incorporate microscale physics
in order to formulate balance laws for the macroscopic quantities of interest.
However, obtaining homogenized constitutive laws is often challenging as they
do not in general have an analytic form and can exhibit phenomena not present
on the microscale. In response, data-driven learning of the constitutive law
has been proposed as appropriate for this task. However, a major challenge in
data-driven learning approaches for this problem has remained unexplored: the
impact of discontinuities and corner interfaces in the underlying material.
These discontinuities in the coefficients affect the smoothness of the
solutions of the underlying equations. Given the prevalence of discontinuous
materials in continuum mechanics applications, it is important to address the
challenge of learning in this context; in particular to develop underpinning
theory to establish the reliability of data-driven methods in this scientific
domain. The paper addresses this unexplored challenge by investigating the
learnability of homogenized constitutive laws for elliptic operators in the
presence of such complexities. Approximation theory is presented, and numerical
experiments are performed which validate the theory for the solution operator
defined by the cell-problem arising in homogenization for elliptic PDEs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Bhattacharya_K/0/1/0/all/0/1&quot;&gt;Kaushik Bhattacharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kovachki_N/0/1/0/all/0/1&quot;&gt;Nikola Kovachki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Rajan_A/0/1/0/all/0/1&quot;&gt;Aakila Rajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Stuart_A/0/1/0/all/0/1&quot;&gt;Andrew M. Stuart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Trautner_M/0/1/0/all/0/1&quot;&gt;Margaret Trautner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.12898">
<title>Machine-Learning-Assisted and Real-Time-Feedback-Controlled Growth of InAs/GaAs Quantum Dots. (arXiv:2306.12898v2 [cond-mat.mes-hall] UPDATED)</title>
<link>http://arxiv.org/abs/2306.12898</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-assembled InAs/GaAs quantum dots (QDs) have properties highly valuable
for developing various optoelectronic devices such as QD lasers and single
photon sources. The applications strongly rely on the density and quality of
these dots, which has motivated studies of the growth process control to
realize high-quality epi-wafers and devices. Establishing the process
parameters in molecular beam epitaxy (MBE) for a specific density of QDs is a
multidimensional optimization challenge, usually addressed through
time-consuming and iterative trial-and-error. Here, we report a real-time
feedback control method to realize the growth of QDs with arbitrary and precise
density, which is fully automated and intelligent. We developed a machine
learning (ML) model named 3D ResNet, specially designed for training RHEED
videos instead of static images and providing real-time feedback on surface
morphologies for process control. As a result, we demonstrated that ML from
previous growth could predict the post-growth density of QDs, by successfully
tuning the QD densities in near-real time from 1.5E10 cm-2 down to 3.8E8 cm-2
or up to 1.4E11 cm-2. Compared to traditional methods, our approach, with
in-situ tuning capabilities and excellent reliability, can dramatically
expedite the material optimization process and improve the reproducibility of
MBE growth, constituting significant progress for thin film growth techniques.
The concepts and methodologies proved feasible in this work are promising to be
applied to a variety of material growth processes, which will revolutionize
semiconductor manufacturing for microelectronic and optoelectronic industries.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Chao Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Zhan_W/0/1/0/all/0/1&quot;&gt;Wenkang Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Xin_K/0/1/0/all/0/1&quot;&gt;Kaiyao Xin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Manyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jian Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhaofeng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Xu_B/0/1/0/all/0/1&quot;&gt;Bo Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Wei_Z/0/1/0/all/0/1&quot;&gt;Zhongming Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Zhao_C/0/1/0/all/0/1&quot;&gt;Chao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhanguo Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.13264">
<title>FedSelect: Customized Selection of Parameters for Fine-Tuning during Personalized Federated Learning. (arXiv:2306.13264v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.13264</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in federated learning (FL) seek to increase client-level
performance by fine-tuning client parameters on local data or personalizing
architectures for the local task. Existing methods for such personalization
either prune a global model or fine-tune a global model on a local client
distribution. However, these existing methods either personalize at the expense
of retaining important global knowledge, or predetermine network layers for
fine-tuning, resulting in suboptimal storage of global knowledge within client
models. Enlightened by the lottery ticket hypothesis, we first introduce a
hypothesis for finding optimal client subnetworks to locally fine-tune while
leaving the rest of the parameters frozen. We then propose a novel FL
framework, FedSelect, using this procedure that directly personalizes both
client subnetwork structure and parameters, via the simultaneous discovery of
optimal parameters for personalization and the rest of parameters for global
aggregation during training. We show that this method achieves promising
results on CIFAR-10.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tamirisa_R/0/1/0/all/0/1&quot;&gt;Rishub Tamirisa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Won_J/0/1/0/all/0/1&quot;&gt;John Won&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Chengjun Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arel_R/0/1/0/all/0/1&quot;&gt;Ron Arel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1&quot;&gt;Andy Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14275">
<title>Enhancing Adversarial Training via Reweighting Optimization Trajectory. (arXiv:2306.14275v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14275</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the fact that adversarial training has become the de facto method for
improving the robustness of deep neural networks, it is well-known that vanilla
adversarial training suffers from daunting robust overfitting, resulting in
unsatisfactory robust generalization. A number of approaches have been proposed
to address these drawbacks such as extra regularization, adversarial weights
perturbation, and training with more data over the last few years. However, the
robust generalization improvement is yet far from satisfactory. In this paper,
we approach this challenge with a brand new perspective -- refining historical
optimization trajectories. We propose a new method named \textbf{Weighted
Optimization Trajectories (WOT)} that leverages the optimization trajectories
of adversarial training in time. We have conducted extensive experiments to
demonstrate the effectiveness of WOT under various state-of-the-art adversarial
attacks. Our results show that WOT integrates seamlessly with the existing
adversarial training methods and consistently overcomes the robust overfitting
issue, resulting in better adversarial robustness. For example, WOT boosts the
robust accuracy of AT-PGD under AA-$L_{\infty}$ attack by 1.53\% $\sim$ 6.11\%
and meanwhile increases the clean accuracy by 0.55\%$\sim$5.47\% across SVHN,
CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tianjin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shiwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianlong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1&quot;&gt;Meng Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Li Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menkovski_V/0/1/0/all/0/1&quot;&gt;Vlaod Menkovski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1&quot;&gt;Lu Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_Y/0/1/0/all/0/1&quot;&gt;Yulong Pei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1&quot;&gt;Mykola Pechenizkiy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16050">
<title>Evaluating Similitude and Robustness of Deep Image Denoising Models via Adversarial Attack. (arXiv:2306.16050v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16050</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) have shown superior performance comparing to
traditional image denoising algorithms. However, DNNs are inevitably vulnerable
while facing adversarial attacks. In this paper, we propose an adversarial
attack method named denoising-PGD which can successfully attack all the current
deep denoising models while keep the noise distribution almost unchanged. We
surprisingly find that the current mainstream non-blind denoising models
(DnCNN, FFDNet, ECNDNet, BRDNet), blind denoising models (DnCNN-B, Noise2Noise,
RDDCNN-B, FAN), plug-and-play (DPIR, CurvPnP) and unfolding denoising models
(DeamNet) almost share the same adversarial sample set on both grayscale and
color images, respectively. Shared adversarial sample set indicates that all
these models are similar in term of local behaviors at the neighborhood of all
the test samples. Thus, we further propose an indicator to measure the local
similarity of models, called robustness similitude. Non-blind denoising models
are found to have high robustness similitude across each other, while
hybrid-driven models are also found to have high robustness similitude with
pure data-driven non-blind denoising models. According to our robustness
assessment, data-driven non-blind denoising models are the most robust. We use
adversarial training to complement the vulnerability to adversarial attacks.
Moreover, the model-driven image denoising BM3D shows resistance on adversarial
attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_J/0/1/0/all/0/1&quot;&gt;Jie Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiebao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zhichang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1&quot;&gt;Wangmeng Zuo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16873">
<title>Understanding the Overfitting of the Episodic Meta-training. (arXiv:2306.16873v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16873</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the success of two-stage few-shot classification methods, in the
episodic meta-training stage, the model suffers severe overfitting. We
hypothesize that it is caused by over-discrimination, i.e., the model learns to
over-rely on the superficial features that fit for base class discrimination
while suppressing the novel class generalization. To penalize
over-discrimination, we introduce knowledge distillation techniques to keep
novel generalization knowledge from the teacher model during training.
Specifically, we select the teacher model as the one with the best validation
accuracy during meta-training and restrict the symmetric Kullback-Leibler (SKL)
divergence between the output distribution of the linear classifier of the
teacher model and that of the student model. This simple approach outperforms
the standard meta-training process. We further propose the Nearest Neighbor
Symmetric Kullback-Leibler (NNSKL) divergence for meta-training to push the
limits of knowledge distillation techniques. NNSKL takes few-shot tasks as
input and penalizes the output of the nearest neighbor classifier, which
possesses an impact on the relationships between query embedding and support
centers. By combining SKL and NNSKL in meta-training, the model achieves even
better performance and surpasses state-of-the-art results on several
benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hui_S/0/1/0/all/0/1&quot;&gt;Siqi Hui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Sanping Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+deng_Y/0/1/0/all/0/1&quot;&gt;Ye deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinjun Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17258">
<title>Suffering Toasters -- A New Self-Awareness Test for AI. (arXiv:2306.17258v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17258</link>
<description rdf:parseType="Literal">&lt;p&gt;A widely accepted definition of intelligence in the context of Artificial
Intelligence (AI) still eludes us. Due to our exceedingly rapid development of
AI paradigms, architectures, and tools, the prospect of naturally arising AI
consciousness seems more likely than ever. In this paper, we claim that all
current intelligence tests are insufficient to point to the existence or lack
of intelligence \textbf{as humans intuitively perceive it}. We draw from ideas
in the philosophy of science, psychology, and other areas of research to
provide a clearer definition of the problems of artificial intelligence,
self-awareness, and agency. We furthermore propose a new heuristic approach to
test for artificial self-awareness and outline a possible implementation.
Finally, we discuss some of the questions that arise from this new heuristic,
be they philosophical or implementation-oriented.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolfson_I/0/1/0/all/0/1&quot;&gt;Ira Wolfson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00125">
<title>RObotic MAnipulation Network (ROMAN) $\unicode{x2013}$ Hybrid Hierarchical Learning for Solving Complex Sequential Tasks. (arXiv:2307.00125v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00125</link>
<description rdf:parseType="Literal">&lt;p&gt;Solving long sequential tasks poses a significant challenge in embodied
artificial intelligence. Enabling a robotic system to perform diverse
sequential tasks with a broad range of manipulation skills is an active area of
research. In this work, we present a Hybrid Hierarchical Learning framework,
the Robotic Manipulation Network (ROMAN), to address the challenge of solving
multiple complex tasks over long time horizons in robotic manipulation. ROMAN
achieves task versatility and robust failure recovery by integrating
behavioural cloning, imitation learning, and reinforcement learning. It
consists of a central manipulation network that coordinates an ensemble of
various neural networks, each specialising in distinct re-combinable sub-tasks
to generate their correct in-sequence actions for solving complex long-horizon
manipulation tasks. Experimental results show that by orchestrating and
activating these specialised manipulation experts, ROMAN generates correct
sequential activations for accomplishing long sequences of sophisticated
manipulation tasks and achieving adaptive behaviours beyond demonstrations,
while exhibiting robustness to various sensory noises. These results
demonstrate the significance and versatility of ROMAN&apos;s dynamic adaptability
featuring autonomous failure recovery capabilities, and highlight its potential
for various autonomous manipulation tasks that demand adaptive motor skills.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Triantafyllidis_E/0/1/0/all/0/1&quot;&gt;Eleftherios Triantafyllidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Acero_F/0/1/0/all/0/1&quot;&gt;Fernando Acero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhaocheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhibin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01166">
<title>Coupled Gradient Flows for Strategic Non-Local Distribution Shift. (arXiv:2307.01166v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01166</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel framework for analyzing the dynamics of distribution shift
in real-world systems that captures the feedback loop between learning
algorithms and the distributions on which they are deployed. Prior work largely
models feedback-induced distribution shift as adversarial or via an overly
simplistic distribution-shift structure. In contrast, we propose a coupled
partial differential equation model that captures fine-grained changes in the
distribution over time by accounting for complex dynamics that arise due to
strategic responses to algorithmic decision-making, non-local endogenous
population interactions, and other exogenous sources of distribution shift. We
consider two common settings in machine learning: cooperative settings with
information asymmetries, and competitive settings where a learner faces
strategic users. For both of these settings, when the algorithm retrains via
gradient descent, we prove asymptotic convergence of the retraining procedure
to a steady-state, both in finite and in infinite dimensions, obtaining
explicit rates in terms of the model parameters. To do so we derive new results
on the convergence of coupled PDEs that extends what is known on multi-species
systems. Empirically, we show that our approach captures well-documented forms
of distribution shifts like polarization and disparate impacts that simpler
models cannot capture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Conger_L/0/1/0/all/0/1&quot;&gt;Lauren Conger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoffmann_F/0/1/0/all/0/1&quot;&gt;Franca Hoffmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazumdar_E/0/1/0/all/0/1&quot;&gt;Eric Mazumdar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ratliff_L/0/1/0/all/0/1&quot;&gt;Lillian Ratliff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01879">
<title>Stability Analysis Framework for Particle-based Distance GANs with Wasserstein Gradient Flow. (arXiv:2307.01879v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01879</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we investigate the training process of generative networks
that use a type of probability density distance named particle-based distance
as the objective function, e.g. MMD GAN, Cram\&apos;er GAN, EIEG GAN. However, these
GANs often suffer from the problem of unstable training. In this paper, we
analyze the stability of the training process of these GANs from the
perspective of probability density dynamics. In our framework, we regard the
discriminator $D$ in these GANs as a feature transformation mapping that maps
high dimensional data into a feature space, while the generator $G$ maps random
variables to samples that resemble real data in terms of feature space. This
perspective enables us to perform stability analysis for the training of GANs
using the Wasserstein gradient flow of the probability density function. We
find that the training process of the discriminator is usually unstable due to
the formulation of $\min_G \max_D E(G, D)$ in GANs. To address this issue, we
add a stabilizing term in the discriminator loss function. We conduct
experiments to validate our stability analysis and stabilizing method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chuqi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yue Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1&quot;&gt;Yang Xiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02150">
<title>Harmonizing Feature Attributions Across Deep Learning Architectures: Enhancing Interpretability and Consistency. (arXiv:2307.02150v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02150</link>
<description rdf:parseType="Literal">&lt;p&gt;Ensuring the trustworthiness and interpretability of machine learning models
is critical to their deployment in real-world applications. Feature attribution
methods have gained significant attention, which provide local explanations of
model predictions by attributing importance to individual input features. This
study examines the generalization of feature attributions across various deep
learning architectures, such as convolutional neural networks (CNNs) and vision
transformers. We aim to assess the feasibility of utilizing a feature
attribution method as a future detector and examine how these features can be
harmonized across multiple models employing distinct architectures but trained
on the same data distribution. By exploring this harmonization, we aim to
develop a more coherent and optimistic understanding of feature attributions,
enhancing the consistency of local explanations across diverse deep-learning
models. Our findings highlight the potential for harmonized feature attribution
methods to improve interpretability and foster trust in machine learning
applications, regardless of the underlying architecture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kadir_M/0/1/0/all/0/1&quot;&gt;Md Abdul Kadir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Addluri_G/0/1/0/all/0/1&quot;&gt;Gowtham Krishna Addluri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sonntag_D/0/1/0/all/0/1&quot;&gt;Daniel Sonntag&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02484">
<title>Elastic Decision Transformer. (arXiv:2307.02484v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02484</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces Elastic Decision Transformer (EDT), a significant
advancement over the existing Decision Transformer (DT) and its variants.
Although DT purports to generate an optimal trajectory, empirical evidence
suggests it struggles with trajectory stitching, a process involving the
generation of an optimal or near-optimal trajectory from the best parts of a
set of sub-optimal trajectories. The proposed EDT differentiates itself by
facilitating trajectory stitching during action inference at test time,
achieved by adjusting the history length maintained in DT. Further, the EDT
optimizes the trajectory by retaining a longer history when the previous
trajectory is optimal and a shorter one when it is sub-optimal, enabling it to
&quot;stitch&quot; with a more optimal trajectory. Extensive experimentation demonstrates
EDT&apos;s ability to bridge the performance gap between DT-based and Q
Learning-based approaches. In particular, the EDT outperforms Q Learning-based
methods in a multi-task regime on the D4RL locomotion benchmark and Atari
games. Videos are available at: https://kristery.github.io/edt/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yueh-Hua Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaolong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamaya_M/0/1/0/all/0/1&quot;&gt;Masashi Hamaya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03003">
<title>Improving the Efficiency of Human-in-the-Loop Systems: Adding Artificial to Human Experts. (arXiv:2307.03003v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03003</link>
<description rdf:parseType="Literal">&lt;p&gt;Information systems increasingly leverage artificial intelligence (AI) and
machine learning (ML) to generate value from vast amounts of data. However, ML
models are imperfect and can generate incorrect classifications. Hence,
human-in-the-loop (HITL) extensions to ML models add a human review for
instances that are difficult to classify. This study argues that continuously
relying on human experts to handle difficult model classifications leads to a
strong increase in human effort, which strains limited resources. To address
this issue, we propose a hybrid system that creates artificial experts that
learn to classify data instances from unknown classes previously reviewed by
human experts. Our hybrid system assesses which artificial expert is suitable
for classifying an instance from an unknown class and automatically assigns it.
Over time, this reduces human effort and increases the efficiency of the
system. Our experiments demonstrate that our approach outperforms traditional
HITL systems for several benchmarks on image classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jakubik_J/0/1/0/all/0/1&quot;&gt;Johannes Jakubik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weber_D/0/1/0/all/0/1&quot;&gt;Daniel Weber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hemmer_P/0/1/0/all/0/1&quot;&gt;Patrick Hemmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vossing_M/0/1/0/all/0/1&quot;&gt;Michael V&amp;#xf6;ssing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Satzger_G/0/1/0/all/0/1&quot;&gt;Gerhard Satzger&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>