<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AI</link>
    <description>cs.AI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 17 Oct 2025 04:00:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Decision Oriented Technique (DOTechnique): Finding Model Validity Through Decision-Maker Context</title>
      <link>https://arxiv.org/abs/2510.13858</link>
      <description>arXiv:2510.13858v1 Announce Type: new 
Abstract: Model validity is as critical as the model itself, especially when guiding decision-making processes. Traditional approaches often rely on predefined validity frames, which may not always be available or sufficient. This paper introduces the Decision Oriented Technique (DOTechnique), a novel method for determining model validity based on decision consistency rather than output similarity. By evaluating whether surrogate models lead to equivalent decisions compared to high-fidelity models, DOTechnique enables efficient identification of validity regions, even in the absence of explicit validity boundaries. The approach integrates domain constraints and symbolic reasoning to narrow the search space, enhancing computational efficiency. A highway lane change system serves as a motivating example, demonstrating how DOTechnique can uncover the validity region of a simulation model. The results highlight the potential of the technique to support finding model validity through decision-maker context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13858v1</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raheleh Biglari, Joachim Denil</dc:creator>
    </item>
    <item>
      <title>Do Slides Help? Multi-modal Context for Automatic Transcription of Conference Talks</title>
      <link>https://arxiv.org/abs/2510.13979</link>
      <description>arXiv:2510.13979v1 Announce Type: new 
Abstract: State-of-the-art (SOTA) Automatic Speech Recognition (ASR) systems primarily rely on acoustic information while disregarding additional multi-modal context. However, visual information are essential in disambiguation and adaptation. While most work focus on speaker images to handle noise conditions, this work also focuses on integrating presentation slides for the use cases of scientific presentation.
  In a first step, we create a benchmark for multi-modal presentation including an automatic analysis of transcribing domain-specific terminology. Next, we explore methods for augmenting speech models with multi-modal information. We mitigate the lack of datasets with accompanying slides by a suitable approach of data augmentation. Finally, we train a model using the augmented dataset, resulting in a relative reduction in word error rate of approximately 34%, across all words and 35%, for domain-specific terms compared to the baseline model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13979v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Supriti Sinhamahapatra, Jan Niehues</dc:creator>
    </item>
    <item>
      <title>Do Large Language Models Show Biases in Causal Learning? Insights from Contingency Judgment</title>
      <link>https://arxiv.org/abs/2510.13985</link>
      <description>arXiv:2510.13985v1 Announce Type: new 
Abstract: Causal learning is the cognitive process of developing the capability of making causal inferences based on available information, often guided by normative principles. This process is prone to errors and biases, such as the illusion of causality, in which people perceive a causal relationship between two variables despite lacking supporting evidence. This cognitive bias has been proposed to underlie many societal problems, including social prejudice, stereotype formation, misinformation, and superstitious thinking. In this work, we examine whether large language models are prone to developing causal illusions when faced with a classic cognitive science paradigm: the contingency judgment task. To investigate this, we constructed a dataset of 1,000 null contingency scenarios (in which the available information is not sufficient to establish a causal relationship between variables) within medical contexts and prompted LLMs to evaluate the effectiveness of potential causes. Our findings show that all evaluated models systematically inferred unwarranted causal relationships, revealing a strong susceptibility to the illusion of causality. While there is ongoing debate about whether LLMs genuinely understand causality or merely reproduce causal language without true comprehension, our findings support the latter hypothesis and raise concerns about the use of language models in domains where accurate causal reasoning is essential for informed decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13985v1</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mar\'ia Victoria Carro, Denise Alejandra Mester, Francisca Gauna Selasco, Giovanni Franco Gabriel Marraffini, Mario Alejandro Leiva, Gerardo I. Simari, Mar\'ia Vanina Martinez</dc:creator>
    </item>
    <item>
      <title>GammaZero: Learning To Guide POMDP Belief Space Search With Graph Representations</title>
      <link>https://arxiv.org/abs/2510.14035</link>
      <description>arXiv:2510.14035v1 Announce Type: new 
Abstract: We introduce an action-centric graph representation framework for learning to guide planning in Partially Observable Markov Decision Processes (POMDPs). Unlike existing approaches that require domain-specific neural architectures and struggle with scalability, GammaZero leverages a unified graph-based belief representation that enables generalization across problem sizes within a domain. Our key insight is that belief states can be systematically transformed into action-centric graphs where structural patterns learned on small problems transfer to larger instances. We employ a graph neural network with a decoder architecture to learn value functions and policies from expert demonstrations on computationally tractable problems, then apply these learned heuristics to guide Monte Carlo tree search on larger problems. Experimental results on standard POMDP benchmarks demonstrate that GammaZero achieves comparable performance to BetaZero when trained and tested on the same-sized problems, while uniquely enabling zero-shot generalization to problems 2-4 times larger than those seen during training, maintaining solution quality with reduced search requirements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14035v1</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rajesh Mangannavar, Prasad Tadepalli</dc:creator>
    </item>
    <item>
      <title>Position: Require Frontier AI Labs To Release Small "Analog" Models</title>
      <link>https://arxiv.org/abs/2510.14053</link>
      <description>arXiv:2510.14053v1 Announce Type: new 
Abstract: Recent proposals for regulating frontier AI models have sparked concerns about the cost of safety regulation, and most such regulations have been shelved due to the safety-innovation tradeoff. This paper argues for an alternative regulatory approach that ensures AI safety while actively promoting innovation: mandating that large AI laboratories release small, openly accessible analog models (scaled-down versions) trained similarly to and distilled from their largest proprietary models.
  Analog models serve as public proxies, allowing broad participation in safety verification, interpretability research, and algorithmic transparency without forcing labs to disclose their full-scale models. Recent research demonstrates that safety and interpretability methods developed using these smaller models generalize effectively to frontier-scale systems. By enabling the wider research community to directly investigate and innovate upon accessible analogs, our policy substantially reduces the regulatory burden and accelerates safety advancements.
  This mandate promises minimal additional costs, leveraging reusable resources like data and infrastructure, while significantly contributing to the public good. Our hope is not only that this policy be adopted, but that it illustrates a broader principle supporting fundamental research in machine learning: deeper understanding of models relaxes the safety-innovation tradeoff and lets us have more of both.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14053v1</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Shriyash Upadhyay, Chaithanya Bandi, Narmeen Oozeer, Philip Quirke</dc:creator>
    </item>
    <item>
      <title>Generating Fair Consensus Statements with Social Choice on Token-Level MDPs</title>
      <link>https://arxiv.org/abs/2510.14106</link>
      <description>arXiv:2510.14106v1 Announce Type: new 
Abstract: Current frameworks for consensus statement generation with large language models lack the inherent structure needed to provide provable fairness guarantees when aggregating diverse free-form opinions. We model the task as a multi-objective, token-level Markov Decision Process (MDP), where each objective corresponds to an agent's preference. Token-level rewards for each agent are derived from their policy (e.g., a personalized language model). This approach utilizes the finding that such policies implicitly define optimal Q-functions, providing a principled way to quantify rewards at each generation step without a value function (Rafailov et al., 2024). This MDP formulation creates a formal structure amenable to analysis using principles from social choice theory. We propose two approaches grounded in social choice theory. First, we propose a stochastic generation policy guaranteed to be in the ex-ante core, extending core stability concepts from voting theory to text generation. This policy is derived from an underlying distribution over complete statements that maximizes proportional fairness (Nash Welfare). Second, for generating a single statement, we target the maximization of egalitarian welfare using search algorithms within the MDP framework. Empirically, experiments using language models to instantiate agent policies show that search guided by the egalitarian objective generates consensus statements with improved worst-case agent alignment compared to baseline methods, including the Habermas Machine (Tessler et al., 2024).</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14106v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.GT</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carter Blair, Kate Larson</dc:creator>
    </item>
    <item>
      <title>STEMS: Spatial-Temporal Enhanced Safe Multi-Agent Coordination for Building Energy Management</title>
      <link>https://arxiv.org/abs/2510.14112</link>
      <description>arXiv:2510.14112v1 Announce Type: new 
Abstract: Building energy management is essential for achieving carbon reduction goals, improving occupant comfort, and reducing energy costs. Coordinated building energy management faces critical challenges in exploiting spatial-temporal dependencies while ensuring operational safety across multi-building systems. Current multi-building energy systems face three key challenges: insufficient spatial-temporal information exploitation, lack of rigorous safety guarantees, and system complexity. This paper proposes Spatial-Temporal Enhanced Safe Multi-Agent Coordination (STEMS), a novel safety-constrained multi-agent reinforcement learning framework for coordinated building energy management. STEMS integrates two core components: (1) a spatial-temporal graph representation learning framework using a GCN-Transformer fusion architecture to capture inter-building relationships and temporal patterns, and (2) a safety-constrained multi-agent RL algorithm incorporating Control Barrier Functions to provide mathematical safety guarantees. Extensive experiments on real-world building datasets demonstrate STEMS's superior performance over existing methods, showing that STEMS achieves 21% cost reduction, 18% emission reduction, and dramatically reduces safety violations from 35.1% to 5.6% while maintaining optimal comfort with only 0.13 discomfort proportion. The framework also demonstrates strong robustness during extreme weather conditions and maintains effectiveness across different building types.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14112v1</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huiliang Zhang, Di Wu, Arnaud Zinflou, Benoit Boulet</dc:creator>
    </item>
    <item>
      <title>Formalizing the Safety, Security, and Functional Properties of Agentic AI Systems</title>
      <link>https://arxiv.org/abs/2510.14133</link>
      <description>arXiv:2510.14133v1 Announce Type: new 
Abstract: Agentic AI systems, which leverage multiple autonomous agents and Large Language Models (LLMs), are increasingly used to address complex, multi-step tasks. The safety, security, and functionality of these systems are critical, especially in high-stakes applications. However, the current ecosystem of inter-agent communication is fragmented, with protocols such as the Model Context Protocol (MCP) for tool access and the Agent-to-Agent (A2A) protocol for coordination being analyzed in isolation. This fragmentation creates a semantic gap that prevents the rigorous analysis of system properties and introduces risks such as architectural misalignment and exploitable coordination issues. To address these challenges, we introduce a modeling framework for agentic AI systems composed of two foundational models. The first, the host agent model, formalizes the top-level entity that interacts with the user, decomposes tasks, and orchestrates their execution by leveraging external agents and tools. The second, the task lifecycle model, details the states and transitions of individual sub-tasks from creation to completion, providing a fine-grained view of task management and error handling. Together, these models provide a unified semantic framework for reasoning about the behavior of multi-AI agent systems. Grounded in this framework, we define 17 properties for the host agent and 14 for the task lifecycle, categorized into liveness, safety, completeness, and fairness. Expressed in temporal logic, these properties enable formal verification of system behavior, detection of coordination edge cases, and prevention of deadlocks and security vulnerabilities. Through this effort, we introduce the first rigorously grounded, domain-agnostic framework for the systematic analysis, design, and deployment of correct, reliable, and robust agentic AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14133v1</guid>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.MA</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Edoardo Allegrini, Ananth Shreekumar, Z. Berkay Celik</dc:creator>
    </item>
    <item>
      <title>A Multimodal Approach to Heritage Preservation in the Context of Climate Change</title>
      <link>https://arxiv.org/abs/2510.14136</link>
      <description>arXiv:2510.14136v1 Announce Type: new 
Abstract: Cultural heritage sites face accelerating degradation due to climate change, yet tradi- tional monitoring relies on unimodal analysis (visual inspection or environmental sen- sors alone) that fails to capture the complex interplay between environmental stres- sors and material deterioration. We propose a lightweight multimodal architecture that fuses sensor data (temperature, humidity) with visual imagery to predict degradation severity at heritage sites. Our approach adapts PerceiverIO with two key innovations: (1) simplified encoders (64D latent space) that prevent overfitting on small datasets (n=37 training samples), and (2) Adaptive Barlow Twins loss that encourages modality complementarity rather than redundancy. On data from Strasbourg Cathedral, our model achieves 76.9% accu- racy, a 43% improvement over standard multimodal architectures (VisualBERT, Trans- former) and 25% over vanilla PerceiverIO. Ablation studies reveal that sensor-only achieves 61.5% while image-only reaches 46.2%, confirming successful multimodal synergy. A systematic hyperparameter study identifies an optimal moderate correlation target ({\tau} =0.3) that balances align- ment and complementarity, achieving 69.2% accuracy compared to other {\tau} values ({\tau} =0.1/0.5/0.7: 53.8%, {\tau} =0.9: 61.5%). This work demonstrates that architectural sim- plicity combined with contrastive regularization enables effective multimodal learning in data-scarce heritage monitoring contexts, providing a foundation for AI-driven con- servation decision support systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14136v1</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Roqui, Ad\`ele Cormier, nistor Grozavu, Ann Bourges</dc:creator>
    </item>
    <item>
      <title>CodeEvolve: An open source evolutionary coding agent for algorithm discovery and optimization</title>
      <link>https://arxiv.org/abs/2510.14150</link>
      <description>arXiv:2510.14150v1 Announce Type: new 
Abstract: In this work, we introduce CodeEvolve, an open-source evolutionary coding agent that unites Large Language Models (LLMs) with genetic algorithms to solve complex computational problems. Our framework adapts powerful evolutionary concepts to the LLM domain, building upon recent methods for generalized scientific discovery. CodeEvolve employs an island-based genetic algorithm to maintain population diversity and increase throughput, introduces a novel inspiration-based crossover mechanism that leverages the LLMs context window to combine features from successful solutions, and implements meta-prompting strategies for dynamic exploration of the solution space. We conduct a rigorous evaluation of CodeEvolve on a subset of the mathematical benchmarks used to evaluate Google DeepMind's closed-source AlphaEvolve. Our findings show that our method surpasses AlphaEvolve's performance on several challenging problems. To foster collaboration and accelerate progress, we release our complete framework as an open-source repository.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14150v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Henrique Assump\c{c}\~ao, Diego Ferreira, Leandro Campos, Fabricio Murai</dc:creator>
    </item>
    <item>
      <title>Combining Reinforcement Learning and Behavior Trees for NPCs in Video Games with AMD Schola</title>
      <link>https://arxiv.org/abs/2510.14154</link>
      <description>arXiv:2510.14154v1 Announce Type: new 
Abstract: While the rapid advancements in the reinforcement learning (RL) research community have been remarkable, the adoption in commercial video games remains slow. In this paper, we outline common challenges the Game AI community faces when using RL-driven NPCs in practice, and highlight the intersection of RL with traditional behavior trees (BTs) as a crucial juncture to be explored further. Although the BT+RL intersection has been suggested in several research papers, its adoption is rare. We demonstrate the viability of this approach using AMD Schola -- a plugin for training RL agents in Unreal Engine -- by creating multi-task NPCs in a complex 3D environment inspired by the commercial video game ``The Last of Us". We provide detailed methodologies for jointly training RL models with BTs while showcasing various skills.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14154v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tian Liu, Alex Cann, Ian Colbert, Mehdi Saeedi</dc:creator>
    </item>
    <item>
      <title>JEDA: Query-Free Clinical Order Search from Ambient Dialogues</title>
      <link>https://arxiv.org/abs/2510.14169</link>
      <description>arXiv:2510.14169v1 Announce Type: new 
Abstract: Clinical conversations mix explicit directives (order a chest X-ray) with implicit reasoning (the cough worsened overnight, we should check for pneumonia). Many systems rely on LLM rewriting, adding latency, instability, and opacity that hinder real-time ordering. We present JEDA (Joint Embedding for Direct and Ambient clinical orders), a domain-initialized bi-encoder that retrieves canonical orders directly and, in a query-free mode, encodes a short rolling window of ambient dialogue to trigger retrieval. Initialized from PubMedBERT and fine-tuned with a duplicate-safe contrastive objective, JEDA aligns heterogeneous expressions of intent to shared order concepts. Training uses constrained LLM guidance to tie each signed order to complementary formulations (command only, context only, command+context, context+reasoning), producing clearer inter-order separation, tighter query extendash order coupling, and stronger generalization. The query-free mode is noise-resilient, reducing sensitivity to disfluencies and ASR errors by conditioning on a short window rather than a single utterance. Deployed in practice, JEDA yields large gains and substantially outperforms its base encoder and recent open embedders (Linq Embed Mistral, SFR Embedding, GTE Qwen, BGE large, Embedding Gemma). The result is a fast, interpretable, LLM-free retrieval layer that links ambient context to actionable clinical orders in real time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14169v1</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Praphul Singh, Corey Barrett, Sumana Srivasta, Amitabh Saikia, Irfan Bulu, Sri Gadde, Krishnaram Kenthapadi</dc:creator>
    </item>
    <item>
      <title>ARM-FM: Automated Reward Machines via Foundation Models for Compositional Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2510.14176</link>
      <description>arXiv:2510.14176v1 Announce Type: new 
Abstract: Reinforcement learning (RL) algorithms are highly sensitive to reward function specification, which remains a central challenge limiting their broad applicability. We present ARM-FM: Automated Reward Machines via Foundation Models, a framework for automated, compositional reward design in RL that leverages the high-level reasoning capabilities of foundation models (FMs). Reward machines (RMs) -- an automata-based formalism for reward specification -- are used as the mechanism for RL objective specification, and are automatically constructed via the use of FMs. The structured formalism of RMs yields effective task decompositions, while the use of FMs enables objective specifications in natural language. Concretely, we (i) use FMs to automatically generate RMs from natural language specifications; (ii) associate language embeddings with each RM automata-state to enable generalization across tasks; and (iii) provide empirical evidence of ARM-FM's effectiveness in a diverse suite of challenging environments, including evidence of zero-shot generalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14176v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roger Creus Castanyer, Faisal Mohamed, Pablo Samuel Castro, Cyrus Neary, Glen Berseth</dc:creator>
    </item>
    <item>
      <title>Implementation of AI in Precision Medicine</title>
      <link>https://arxiv.org/abs/2510.14194</link>
      <description>arXiv:2510.14194v1 Announce Type: new 
Abstract: Artificial intelligence (AI) has become increasingly central to precision medicine by enabling the integration and interpretation of multimodal data, yet implementation in clinical settings remains limited. This paper provides a scoping review of literature from 2019-2024 on the implementation of AI in precision medicine, identifying key barriers and enablers across data quality, clinical reliability, workflow integration, and governance. Through an ecosystem-based framework, we highlight the interdependent relationships shaping real-world translation and propose future directions to support trustworthy and sustainable implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14194v1</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>G\"oktu\u{g} Bender, Samer Faraj, Anand Bhardwaj</dc:creator>
    </item>
    <item>
      <title>Echoes of Human Malice in Agents: Benchmarking LLMs for Multi-Turn Online Harassment Attacks</title>
      <link>https://arxiv.org/abs/2510.14207</link>
      <description>arXiv:2510.14207v1 Announce Type: new 
Abstract: Large Language Model (LLM) agents are powering a growing share of interactive web applications, yet remain vulnerable to misuse and harm. Prior jailbreak research has largely focused on single-turn prompts, whereas real harassment often unfolds over multi-turn interactions. In this work, we present the Online Harassment Agentic Benchmark consisting of: (i) a synthetic multi-turn harassment conversation dataset, (ii) a multi-agent (e.g., harasser, victim) simulation informed by repeated game theory, (iii) three jailbreak methods attacking agents across memory, planning, and fine-tuning, and (iv) a mixed-methods evaluation framework. We utilize two prominent LLMs, LLaMA-3.1-8B-Instruct (open-source) and Gemini-2.0-flash (closed-source). Our results show that jailbreak tuning makes harassment nearly guaranteed with an attack success rate of 95.78--96.89% vs. 57.25--64.19% without tuning in Llama, and 99.33% vs. 98.46% without tuning in Gemini, while sharply reducing refusal rate to 1-2% in both models. The most prevalent toxic behaviors are Insult with 84.9--87.8% vs. 44.2--50.8% without tuning, and Flaming with 81.2--85.1% vs. 31.5--38.8% without tuning, indicating weaker guardrails compared to sensitive categories such as sexual or racial harassment. Qualitative evaluation further reveals that attacked agents reproduce human-like aggression profiles, such as Machiavellian/psychopathic patterns under planning, and narcissistic tendencies with memory. Counterintuitively, closed-source and open-source models exhibit distinct escalation trajectories across turns, with closed-source models showing significant vulnerability. Overall, our findings show that multi-turn and theory-grounded attacks not only succeed at high rates but also mimic human-like harassment dynamics, motivating the development of robust safety guardrails to ultimately keep online platforms safe and responsible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14207v1</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Trilok Padhi, Pinxian Lu, Abdulkadir Erol, Tanmay Sutar, Gauri Sharma, Mina Sonmez, Munmun De Choudhury, Ugur Kursuncu</dc:creator>
    </item>
    <item>
      <title>LiveResearchBench: A Live Benchmark for User-Centric Deep Research in the Wild</title>
      <link>https://arxiv.org/abs/2510.14240</link>
      <description>arXiv:2510.14240v1 Announce Type: new 
Abstract: Deep research -- producing comprehensive, citation-grounded reports by searching and synthesizing information from hundreds of live web sources -- marks an important frontier for agentic systems. To rigorously evaluate this ability, four principles are essential: tasks should be (1) user-centric, reflecting realistic information needs, (2) dynamic, requiring up-to-date information beyond parametric knowledge, (3) unambiguous, ensuring consistent interpretation across users, and (4) multi-faceted and search-intensive, requiring search over numerous web sources and in-depth analysis. Existing benchmarks fall short of these principles, often focusing on narrow domains or posing ambiguous questions that hinder fair comparison. Guided by these principles, we introduce LiveResearchBench, a benchmark of 100 expert-curated tasks spanning daily life, enterprise, and academia, each requiring extensive, dynamic, real-time web search and synthesis. Built with over 1,500 hours of human labor, LiveResearchBench provides a rigorous basis for systematic evaluation. To evaluate citation-grounded long-form reports, we introduce DeepEval, a comprehensive suite covering both content- and report-level quality, including coverage, presentation, citation accuracy and association, consistency and depth of analysis. DeepEval integrates four complementary evaluation protocols, each designed to ensure stable assessment and high agreement with human judgments. Using LiveResearchBench and DeepEval, we conduct a comprehensive evaluation of 17 frontier deep research systems, including single-agent web search, single-agent deep research, and multi-agent systems. Our analysis reveals current strengths, recurring failure modes, and key system components needed to advance reliable, insightful deep research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14240v1</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jiayu Wang, Yifei Ming, Riya Dulepet, Qinglin Chen, Austin Xu, Zixuan Ke, Frederic Sala, Aws Albarghouthi, Caiming Xiong, Shafiq Joty</dc:creator>
    </item>
    <item>
      <title>Towards Agentic Self-Learning LLMs in Search Environment</title>
      <link>https://arxiv.org/abs/2510.14253</link>
      <description>arXiv:2510.14253v1 Announce Type: new 
Abstract: We study whether self-learning can scale LLM-based agents without relying on human-curated datasets or predefined rule-based rewards. Through controlled experiments in a search-agent setting, we identify two key determinants of scalable agent training: the source of reward signals and the scale of agent task data. We find that rewards from a Generative Reward Model (GRM) outperform rigid rule-based signals for open-domain learning, and that co-evolving the GRM with the policy further boosts performance. Increasing the volume of agent task data-even when synthetically generated-substantially enhances agentic capabilities. Building on these insights, we propose \textbf{Agentic Self-Learning} (ASL), a fully closed-loop, multi-role reinforcement learning framework that unifies task generation, policy execution, and evaluation within a shared tool environment and LLM backbone. ASL coordinates a Prompt Generator, a Policy Model, and a Generative Reward Model to form a virtuous cycle of harder task setting, sharper verification, and stronger solving. Empirically, ASL delivers steady, round-over-round gains, surpasses strong RLVR baselines (e.g., Search-R1) that plateau or degrade, and continues improving under zero-labeled-data conditions, indicating superior sample efficiency and robustness. We further show that GRM verification capacity is the main bottleneck: if frozen, it induces reward hacking and stalls progress; continual GRM training on the evolving data distribution mitigates this, and a small late-stage injection of real verification data raises the performance ceiling. This work establishes reward source and data scale as critical levers for open-domain agent learning and demonstrates the efficacy of multi-role co-evolution for scalable, self-improving agents. The data and code of this paper are released at https://github.com/forangel2014/Towards-Agentic-Self-Learning</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14253v1</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wangtao Sun, Xiang Cheng, Jialin Fan, Yao Xu, Xing Yu, Shizhu He, Jun Zhao, Kang Liu</dc:creator>
    </item>
    <item>
      <title>MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning</title>
      <link>https://arxiv.org/abs/2510.14265</link>
      <description>arXiv:2510.14265v1 Announce Type: new 
Abstract: With the advancement of powerful large-scale reasoning models, effectively evaluating the reasoning capabilities of these models has become increasingly important. However, existing benchmarks designed to assess the reasoning abilities of large models tend to be limited in scope and lack the flexibility to adapt their difficulty according to the evolving reasoning capacities of the models. To address this, we propose MorphoBench, a benchmark that incorporates multidisciplinary questions to evaluate the reasoning capabilities of large models and can adjust and update question difficulty based on the reasoning abilities of advanced models. Specifically, we curate the benchmark by selecting and collecting complex reasoning questions from existing benchmarks and sources such as Olympiad-level competitions. Additionally, MorphoBench adaptively modifies the analytical challenge of questions by leveraging key statements generated during the model's reasoning process. Furthermore, it includes questions generated using simulation software, enabling dynamic adjustment of benchmark difficulty with minimal resource consumption. We have gathered over 1,300 test questions and iteratively adjusted the difficulty of MorphoBench based on the reasoning capabilities of models such as o3 and GPT-5. MorphoBench enhances the comprehensiveness and validity of model reasoning evaluation, providing reliable guidance for improving both the reasoning abilities and scientific robustness of large models. The code has been released in https://github.com/OpenDCAI/MorphoBench.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14265v1</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xukai Wang, Xuanbo Liu, Mingrui Chen, Haitian Zhong, Xuanlin Yang, Bohan Zeng, Jinbo Hu, Hao Liang, Junbo Niu, Xuchen Li, Ruitao Wu, Ruichuan An, Yang Shi, Liu Liu, Xu-Yao Zhang, Qiang Liu, Zhouchen Lin, Wentao Zhang, Bin Dong</dc:creator>
    </item>
    <item>
      <title>A Guardrail for Safety Preservation: When Safety-Sensitive Subspace Meets Harmful-Resistant Null-Space</title>
      <link>https://arxiv.org/abs/2510.14301</link>
      <description>arXiv:2510.14301v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved remarkable success in diverse tasks, yet their safety alignment remains fragile during adaptation. Even when fine-tuning on benign data or with low-rank adaptation, pre-trained safety behaviors are easily degraded, leading to harmful responses in the fine-tuned models. To address this challenge, we propose GuardSpace, a guardrail framework for preserving safety alignment throughout fine-tuning, composed of two key components: a safety-sensitive subspace and a harmful-resistant null space. First, we explicitly decompose pre-trained weights into safety-relevant and safety-irrelevant components using covariance-preconditioned singular value decomposition, and initialize low-rank adapters from the safety-irrelevant ones, while freezing safety-relevant components to preserve their associated safety mechanism. Second, we construct a null space projector that restricts adapter updates from altering safe outputs on harmful prompts, thereby maintaining the original refusal behavior. Experiments with various pre-trained models on multiple downstream tasks demonstrate that GuardSpace achieves superior performance over existing methods. Notably, for Llama-2-7B-Chat fine-tuned on GSM8K, GuardSpace outperforms the state-of-the-art method AsFT, reducing the average harmful score from 14.4% to 3.6%, while improving the accuracy from from 26.0% to 28.0%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14301v1</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bingjie Zhang, Yibo Yang,  Renzhe, Dandan Guo, Jindong Gu, Philip Torr, Bernard Ghanem</dc:creator>
    </item>
    <item>
      <title>Terrarium: Revisiting the Blackboard for Multi-Agent Safety, Privacy, and Security Studies</title>
      <link>https://arxiv.org/abs/2510.14312</link>
      <description>arXiv:2510.14312v1 Announce Type: new 
Abstract: A multi-agent system (MAS) powered by large language models (LLMs) can automate tedious user tasks such as meeting scheduling that requires inter-agent collaboration. LLMs enable nuanced protocols that account for unstructured private data, user constraints, and preferences. However, this design introduces new risks, including misalignment and attacks by malicious parties that compromise agents or steal user data. In this paper, we propose the Terrarium framework for fine-grained study on safety, privacy, and security in LLM-based MAS. We repurpose the blackboard design, an early approach in multi-agent systems, to create a modular, configurable testbed for multi-agent collaboration. We identify key attack vectors such as misalignment, malicious agents, compromised communication, and data poisoning. We implement three collaborative MAS scenarios with four representative attacks to demonstrate the framework's flexibility. By providing tools to rapidly prototype, evaluate, and iterate on defenses and designs, Terrarium aims to accelerate progress toward trustworthy multi-agent systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14312v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CR</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mason Nakamura, Abhinav Kumar, Saaduddin Mahmud, Sahar Abdelnabi, Shlomo Zilberstein, Eugene Bagdasarian</dc:creator>
    </item>
    <item>
      <title>Metacognitive Self-Correction for Multi-Agent System via Prototype-Guided Next-Execution Reconstruction</title>
      <link>https://arxiv.org/abs/2510.14319</link>
      <description>arXiv:2510.14319v1 Announce Type: new 
Abstract: Large Language Model based multi-agent systems (MAS) excel at collaborative problem solving but remain brittle to cascading errors: a single faulty step can propagate across agents and disrupt the trajectory. In this paper, we present MASC, a metacognitive framework that endows MAS with real-time, unsupervised, step-level error detection and self-correction. MASC rethinks detection as history-conditioned anomaly scoring via two complementary designs: (1) Next-Execution Reconstruction, which predicts the embedding of the next step from the query and interaction history to capture causal consistency, and (2) Prototype-Guided Enhancement, which learns a prototype prior over normal-step embeddings and uses it to stabilize reconstruction and anomaly scoring under sparse context (e.g., early steps). When an anomaly step is flagged, MASC triggers a correction agent to revise the acting agent's output before information flows downstream. On the Who&amp;When benchmark, MASC consistently outperforms all baselines, improving step-level error detection by up to 8.47% AUC-ROC ; When plugged into diverse MAS frameworks, it delivers consistent end-to-end gains across architectures, confirming that our metacognitive monitoring and targeted correction can mitigate error propagation with minimal overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14319v1</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xu Shen, Qi Zhang, Song Wang, Zhen Tan, Xinyu Zhao, Laura Yao, Vaishnav Tadiparthi, Hossein Nourkhiz Mahjoub, Ehsan Moradi Pari, Kwonjoon Lee, Tianlong Chen</dc:creator>
    </item>
    <item>
      <title>AI for Service: Proactive Assistance with AI Glasses</title>
      <link>https://arxiv.org/abs/2510.14359</link>
      <description>arXiv:2510.14359v1 Announce Type: new 
Abstract: In an era where AI is evolving from a passive tool into an active and adaptive companion, we introduce AI for Service (AI4Service), a new paradigm that enables proactive and real-time assistance in daily life. Existing AI services remain largely reactive, responding only to explicit user commands. We argue that a truly intelligent and helpful assistant should be capable of anticipating user needs and taking actions proactively when appropriate. To realize this vision, we propose Alpha-Service, a unified framework that addresses two fundamental challenges: Know When to intervene by detecting service opportunities from egocentric video streams, and Know How to provide both generalized and personalized services. Inspired by the von Neumann computer architecture and based on AI glasses, Alpha-Service consists of five key components: an Input Unit for perception, a Central Processing Unit for task scheduling, an Arithmetic Logic Unit for tool utilization, a Memory Unit for long-term personalization, and an Output Unit for natural human interaction. As an initial exploration, we implement Alpha-Service through a multi-agent system deployed on AI glasses. Case studies, including a real-time Blackjack advisor, a museum tour guide, and a shopping fit assistant, demonstrate its ability to seamlessly perceive the environment, infer user intent, and provide timely and useful assistance without explicit prompts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14359v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zichen Wen, Yiyu Wang, Chenfei Liao, Boxue Yang, Junxian Li, Weifeng Liu, Haocong He, Bolong Feng, Xuyang Liu, Yuanhuiyi Lyu, Xu Zheng, Xuming Hu, Linfeng Zhang</dc:creator>
    </item>
    <item>
      <title>Can MLLMs Absorb Math Reasoning Abilities from LLMs as Free Lunch?</title>
      <link>https://arxiv.org/abs/2510.14387</link>
      <description>arXiv:2510.14387v1 Announce Type: new 
Abstract: Math reasoning has been one crucial ability of large language models (LLMs), where significant advancements have been achieved in recent years. However, most efforts focus on LLMs by curating high-quality annotation data and intricate training (or inference) paradigms, while the math reasoning performance of multi-modal LLMs (MLLMs) remains lagging behind. Since the MLLM typically consists of an LLM and a vision block, we wonder: Can MLLMs directly absorb math reasoning abilities from off-the-shelf math LLMs without tuning? Recent model-merging approaches may offer insights into this question. However, they overlook the alignment between the MLLM and LLM, where we find that there is a large gap between their parameter spaces, resulting in lower performance. Our empirical evidence reveals two key factors behind this issue: the identification of crucial reasoning-associated layers in the model and the mitigation of the gaps in parameter space. Based on the empirical insights, we propose IP-Merging that first identifies the reasoning-associated parameters in both MLLM and Math LLM, then projects them into the subspace of MLLM, aiming to maintain the alignment, and finally merges parameters in this subspace. IP-Merging is a tuning-free approach since parameters are directly adjusted. Extensive experiments demonstrate that our IP-Merging method can enhance the math reasoning ability of MLLMs directly from Math LLMs without compromising their other capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14387v1</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yijie Hu, Zihao Zhou, Kaizhu Huang, Xiaowei Huang, Qiufeng Wang</dc:creator>
    </item>
    <item>
      <title>Hi-Agent: Hierarchical Vision-Language Agents for Mobile Device Control</title>
      <link>https://arxiv.org/abs/2510.14388</link>
      <description>arXiv:2510.14388v1 Announce Type: new 
Abstract: Building agents that autonomously operate mobile devices has attracted increasing attention. While Vision-Language Models (VLMs) show promise, most existing approaches rely on direct state-to-action mappings, which lack structured reasoning and planning, and thus generalize poorly to novel tasks or unseen UI layouts. We introduce Hi-Agent, a trainable hierarchical vision-language agent for mobile control, featuring a high-level reasoning model and a low-level action model that are jointly optimized. For efficient training, we reformulate multi-step decision-making as a sequence of single-step subgoals and propose a foresight advantage function, which leverages execution feedback from the low-level model to guide high-level optimization. This design alleviates the path explosion issue encountered by Group Relative Policy Optimization (GRPO) in long-horizon tasks and enables stable, critic-free joint training. Hi-Agent achieves a new State-Of-The-Art (SOTA) 87.9% task success rate on the Android-in-the-Wild (AitW) benchmark, significantly outperforming prior methods across three paradigms: prompt-based (AppAgent: 17.7%), supervised (Filtered BC: 54.5%), and reinforcement learning-based (DigiRL: 71.9%). It also demonstrates competitive zero-shot generalization on the ScreenSpot-v2 benchmark. On the more challenging AndroidWorld benchmark, Hi-Agent also scales effectively with larger backbones, showing strong adaptability in high-complexity mobile control scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14388v1</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhe Wu, Hongjin Lu, Junliang Xing, Changhao Zhang, Yin Zhu, Yuhao Yang, Yuheng Jing, Kai Li, Kun Shao, Jianye Hao, Jun Wang, Yuanchun Shi</dc:creator>
    </item>
    <item>
      <title>IMAGINE: Integrating Multi-Agent System into One Model for Complex Reasoning and Planning</title>
      <link>https://arxiv.org/abs/2510.14406</link>
      <description>arXiv:2510.14406v1 Announce Type: new 
Abstract: Although large language models (LLMs) have made significant strides across various tasks, they still face significant challenges in complex reasoning and planning. For example, even with carefully designed prompts and prior information explicitly provided, GPT-4o achieves only a 7% Final Pass Rate on the TravelPlanner dataset in the sole-planning mode. Similarly, even in the thinking mode, Qwen3-8B-Instruct and DeepSeek-R1-671B, only achieve Final Pass Rates of 5.9% and 40%, respectively. Although well-organized Multi-Agent Systems (MAS) can offer improved collective reasoning, they often suffer from high reasoning costs due to multi-round internal interactions, long per-response latency, and difficulties in end-to-end training. To address these challenges, we propose a general and scalable framework called IMAGINE, short for Integrating Multi-Agent System into One Model. This framework not only integrates the reasoning and planning capabilities of MAS into a single, compact model, but also significantly surpass the capabilities of the MAS through a simple end-to-end training. Through this pipeline, a single small-scale model is not only able to acquire the structured reasoning and planning capabilities of a well-organized MAS but can also significantly outperform it. Experimental results demonstrate that, when using Qwen3-8B-Instruct as the base model and training it with our method, the model achieves an 82.7% Final Pass Rate on the TravelPlanner benchmark, far exceeding the 40% of DeepSeek-R1-671B, while maintaining a much smaller model size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14406v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xikai Zhang, Bo Wang, Likang Xiao, Yongzhi Li, Quan Chen, Wenju Wu, Liu Liu</dc:creator>
    </item>
    <item>
      <title>Eliminating Negative Occurrences of Derived Predicates from PDDL Axioms</title>
      <link>https://arxiv.org/abs/2510.14412</link>
      <description>arXiv:2510.14412v1 Announce Type: new 
Abstract: Axioms are a feature of the Planning Domain Definition Language PDDL that can be considered as a generalization of database query languages such as Datalog. The PDDL standard restricts negative occurrences of predicates in axiom bodies to predicates that are directly set by actions and not derived by axioms. In the literature, authors often deviate from this limitation and only require that the set of axioms is stratifiable. Both variants can express exactly the same queries as least fixed-point logic, indicating that negative occurrences of derived predicates can be eliminated. We present the corresponding transformation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14412v1</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Claudia Grundke, Gabriele R\"oger</dc:creator>
    </item>
    <item>
      <title>Helmsman: Autonomous Synthesis of Federated Learning Systems via Multi-Agent Collaboration</title>
      <link>https://arxiv.org/abs/2510.14512</link>
      <description>arXiv:2510.14512v1 Announce Type: new 
Abstract: Federated Learning (FL) offers a powerful paradigm for training models on decentralized data, but its promise is often undermined by the immense complexity of designing and deploying robust systems. The need to select, combine, and tune strategies for multifaceted challenges like data heterogeneity and system constraints has become a critical bottleneck, resulting in brittle, bespoke solutions. To address this, we introduce Helmsman, a novel multi-agent system that automates the end-to-end synthesis of federated learning systems from high-level user specifications. It emulates a principled research and development workflow through three collaborative phases: (1) interactive human-in-the-loop planning to formulate a sound research plan, (2) modular code generation by supervised agent teams, and (3) a closed-loop of autonomous evaluation and refinement in a sandboxed simulation environment. To facilitate rigorous evaluation, we also introduce AgentFL-Bench, a new benchmark comprising 16 diverse tasks designed to assess the system-level generation capabilities of agentic systems in FL. Extensive experiments demonstrate that our approach generates solutions competitive with, and often superior to, established hand-crafted baselines. Our work represents a significant step towards the automated engineering of complex decentralized AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14512v1</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoyuan Li, Mathias Funk, Aaqib Saeed</dc:creator>
    </item>
    <item>
      <title>JSPLIT: A Taxonomy-based Solution for Prompt Bloating in Model Context Protocol</title>
      <link>https://arxiv.org/abs/2510.14537</link>
      <description>arXiv:2510.14537v1 Announce Type: new 
Abstract: AI systems are continually evolving and advancing, and user expectations are concurrently increasing, with a growing demand for interactions that go beyond simple text-based interaction with Large Language Models (LLMs). Today's applications often require LLMs to interact with external tools, marking a shift toward more complex agentic systems. To support this, standards such as the Model Context Protocol (MCP) have emerged, enabling agents to access tools by including a specification of the capabilities of each tool within the prompt. Although this approach expands what agents can do, it also introduces a growing problem: prompt bloating. As the number of tools increases, the prompts become longer, leading to high prompt token costs, increased latency, and reduced task success resulting from the selection of tools irrelevant to the prompt. To address this issue, we introduce JSPLIT, a taxonomy-driven framework designed to help agents manage prompt size more effectively when using large sets of MCP tools. JSPLIT organizes the tools into a hierarchical taxonomy and uses the user's prompt to identify and include only the most relevant tools, based on both the query and the taxonomy structure. In this paper, we describe the design of the taxonomy, the tool selection algorithm, and the dataset used to evaluate JSPLIT. Our results show that JSPLIT significantly reduces prompt size without significantly compromising the agent's ability to respond effectively. As the number of available tools for the agent grows substantially, JSPLIT even improves the tool selection accuracy of the agent, effectively reducing costs while simultaneously improving task success in high-complexity agent environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14537v1</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Emanuele Antonioni, Stefan Markovic, Anirudha Shankar, Jaime Bernardo, Lovro Markovic, Silvia Pareti, Benedetto Proietti</dc:creator>
    </item>
    <item>
      <title>Symbol Grounding in Neuro-Symbolic AI: A Gentle Introduction to Reasoning Shortcuts</title>
      <link>https://arxiv.org/abs/2510.14538</link>
      <description>arXiv:2510.14538v1 Announce Type: new 
Abstract: Neuro-symbolic (NeSy) AI aims to develop deep neural networks whose predictions comply with prior knowledge encoding, e.g. safety or structural constraints. As such, it represents one of the most promising avenues for reliable and trustworthy AI. The core idea behind NeSy AI is to combine neural and symbolic steps: neural networks are typically responsible for mapping low-level inputs into high-level symbolic concepts, while symbolic reasoning infers predictions compatible with the extracted concepts and the prior knowledge. Despite their promise, it was recently shown that - whenever the concepts are not supervised directly - NeSy models can be affected by Reasoning Shortcuts (RSs). That is, they can achieve high label accuracy by grounding the concepts incorrectly. RSs can compromise the interpretability of the model's explanations, performance in out-of-distribution scenarios, and therefore reliability. At the same time, RSs are difficult to detect and prevent unless concept supervision is available, which is typically not the case. However, the literature on RSs is scattered, making it difficult for researchers and practitioners to understand and tackle this challenging problem. This overview addresses this issue by providing a gentle introduction to RSs, discussing their causes and consequences in intuitive terms. It also reviews and elucidates existing theoretical characterizations of this phenomenon. Finally, it details methods for dealing with RSs, including mitigation and awareness strategies, and maps their benefits and limitations. By reformulating advanced material in a digestible form, this overview aims to provide a unifying perspective on RSs to lower the bar to entry for tackling them. Ultimately, we hope this overview contributes to the development of reliable NeSy and trustworthy AI models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14538v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emanuele Marconato, Samuele Bortolotti, Emile van Krieken, Paolo Morettin, Elena Umili, Antonio Vergari, Efthymia Tsamoura, Andrea Passerini, Stefano Teso</dc:creator>
    </item>
    <item>
      <title>LLM Agents Beyond Utility: An Open-Ended Perspective</title>
      <link>https://arxiv.org/abs/2510.14548</link>
      <description>arXiv:2510.14548v1 Announce Type: new 
Abstract: Recent LLM agents have made great use of chain of thought reasoning and function calling. As their capabilities grow, an important question arises: can this software represent not only a smart problem-solving tool, but an entity in its own right, that can plan, design immediate tasks, and reason toward broader, more ambiguous goals? To study this question, we adopt an open-ended experimental setting where we augment a pretrained LLM agent with the ability to generate its own tasks, accumulate knowledge, and interact extensively with its environment. We study the resulting open-ended agent qualitatively. It can reliably follow complex multi-step instructions, store and reuse information across runs, and propose and solve its own tasks, though it remains sensitive to prompt design, prone to repetitive task generation, and unable to form self-representations. These findings illustrate both the promise and current limits of adapting pretrained LLMs toward open-endedness, and point to future directions for training agents to manage memory, explore productively, and pursue abstract long-term goals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14548v1</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Asen Nachkov, Xi Wang, Luc Van Gool</dc:creator>
    </item>
    <item>
      <title>ColorBench: Benchmarking Mobile Agents with Graph-Structured Framework for Complex Long-Horizon Tasks</title>
      <link>https://arxiv.org/abs/2510.14621</link>
      <description>arXiv:2510.14621v1 Announce Type: new 
Abstract: The rapid advancement of multimodal large language models has enabled agents to operate mobile devices by directly interacting with graphical user interfaces, opening new possibilities for mobile automation. However, real-world mobile tasks are often complex and allow for multiple valid solutions. This contradicts current mobile agent evaluation standards: offline static benchmarks can only validate a single predefined "golden path", while online dynamic testing is constrained by the complexity and non-reproducibility of real devices, making both approaches inadequate for comprehensively assessing agent capabilities. To bridge the gap between offline and online evaluation and enhance testing stability, this paper introduces a novel graph-structured benchmarking framework. By modeling the finite states observed during real-device interactions, it achieves static simulation of dynamic behaviors. Building on this, we develop ColorBench, a benchmark focused on complex long-horizon tasks. It supports evaluation of multiple valid solutions, subtask completion rate statistics, and atomic-level capability analysis. ColorBench contains 175 tasks (74 single-app, 101 cross-app) with an average length of over 13 steps. Each task includes at least two correct paths and several typical error paths, enabling quasi-dynamic interaction. By evaluating ColorBench across various baselines, we discover limitations of existing models and propose improvement directions and feasible technical pathways to enhance agents' performance on complex, long-horizon problems based on experimental results. Code and data are available at: https://github.com/MadeAgents/ColorBench.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14621v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuanyi Song, Heyuan Huang, Qiqiang Lin, Yin Zhao, Xiangmou Qu, Jun Wang, Xingyu Lou, Weiwen Liu, Zhuosheng Zhang, Jun Wang, Yong Yu, Weinan Zhang, Zhaoxiang Wang</dc:creator>
    </item>
    <item>
      <title>Beyond Hallucinations: The Illusion of Understanding in Large Language Models</title>
      <link>https://arxiv.org/abs/2510.14665</link>
      <description>arXiv:2510.14665v1 Announce Type: new 
Abstract: Large language models (LLMs) are becoming deeply embedded in human communication and decision-making, yet they inherit the ambiguity, bias, and lack of direct access to truth inherent in language itself. While their outputs are fluent, emotionally resonant, and coherent, they are generated through statistical prediction rather than grounded reasoning. This creates the risk of hallucination, responses that sound convincing but lack factual validity. Building on Geoffrey Hinton's observation that AI mirrors human intuition rather than reasoning, this paper argues that LLMs operationalize System 1 cognition at scale: fast, associative, and persuasive, but without reflection or falsification. To address this, we introduce the Rose-Frame, a three-dimensional framework for diagnosing cognitive and epistemic drift in human-AI interaction. The three axes are: (i) Map vs. Territory, which distinguishes representations of reality (epistemology) from reality itself (ontology); (ii) Intuition vs. Reason, drawing on dual-process theory to separate fast, emotional judgments from slow, reflective thinking; and (iii) Conflict vs. Confirmation, which examines whether ideas are critically tested through disagreement or simply reinforced through mutual validation. Each dimension captures a distinct failure mode, and their combination amplifies misalignment. Rose-Frame does not attempt to fix LLMs with more data or rules. Instead, it offers a reflective tool that makes both the model's limitations and the user's assumptions visible, enabling more transparent and critically aware AI deployment. It reframes alignment as cognitive governance: intuition, whether human or artificial, must remain governed by human reason. Only by embedding reflective, falsifiable oversight can we align machine fluency with human understanding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14665v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rikard Rosenbacke, Carl Rosenbacke, Victor Rosenbacke, Martin McKee</dc:creator>
    </item>
    <item>
      <title>Machine Learning and Public Health: Identifying and Mitigating Algorithmic Bias through a Systematic Review</title>
      <link>https://arxiv.org/abs/2510.14669</link>
      <description>arXiv:2510.14669v1 Announce Type: new 
Abstract: Machine learning (ML) promises to revolutionize public health through improved surveillance, risk stratification, and resource allocation. However, without systematic attention to algorithmic bias, ML may inadvertently reinforce existing health disparities. We present a systematic literature review of algorithmic bias identification, discussion, and reporting in Dutch public health ML research from 2021 to 2025. To this end, we developed the Risk of Algorithmic Bias Assessment Tool (RABAT) by integrating elements from established frameworks (Cochrane Risk of Bias, PROBAST, Microsoft Responsible AI checklist) and applied it to 35 peer-reviewed studies. Our analysis reveals pervasive gaps: although data sampling and missing data practices are well documented, most studies omit explicit fairness framing, subgroup analyses, and transparent discussion of potential harms. In response, we introduce a four-stage fairness-oriented framework called ACAR (Awareness, Conceptualization, Application, Reporting), with guiding questions derived from our systematic literature review to help researchers address fairness across the ML lifecycle. We conclude with actionable recommendations for public health ML practitioners to consistently consider algorithmic bias and foster transparency, ensuring that algorithmic innovations advance health equity rather than undermine it.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14669v1</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sara Altamirano, Arjan Vreeken, Sennay Ghebreab</dc:creator>
    </item>
    <item>
      <title>TITAN: Graph-Executable Reasoning for Cyber Threat Intelligence</title>
      <link>https://arxiv.org/abs/2510.14670</link>
      <description>arXiv:2510.14670v1 Announce Type: new 
Abstract: TITAN (Threat Intelligence Through Automated Navigation) is a framework that connects natural-language cyber threat queries with executable reasoning over a structured knowledge graph. It integrates a path planner model, which predicts logical relation chains from text, and a graph executor that traverses the TITAN Ontology to retrieve factual answers and supporting evidence. Unlike traditional retrieval systems, TITAN operates on a typed, bidirectional graph derived from MITRE, allowing reasoning to move clearly and reversibly between threats, behaviors, and defenses. To support training and evaluation, we introduce the TITAN Dataset, a corpus of 88209 examples (Train: 74258; Test: 13951) pairing natural language questions with executable reasoning paths and step by step Chain of Thought explanations. Empirical evaluations show that TITAN enables models to generate syntactically valid and semantically coherent reasoning paths that can be deterministically executed on the underlying graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14670v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CR</category>
      <category>cs.IR</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Marco Simoni, Aleksandar Fontana, Andrea Saracino, Paolo Mori</dc:creator>
    </item>
    <item>
      <title>NAEL: Non-Anthropocentric Ethical Logic</title>
      <link>https://arxiv.org/abs/2510.14676</link>
      <description>arXiv:2510.14676v1 Announce Type: new 
Abstract: We introduce NAEL (Non-Anthropocentric Ethical Logic), a novel ethical framework for artificial agents grounded in active inference and symbolic reasoning. Departing from conventional, human-centred approaches to AI ethics, NAEL formalizes ethical behaviour as an emergent property of intelligent systems minimizing global expected free energy in dynamic, multi-agent environments. We propose a neuro-symbolic architecture to allow agents to evaluate the ethical consequences of their actions in uncertain settings. The proposed system addresses the limitations of existing ethical models by allowing agents to develop context-sensitive, adaptive, and relational ethical behaviour without presupposing anthropomorphic moral intuitions. A case study involving ethical resource distribution illustrates NAEL's dynamic balancing of self-preservation, epistemic learning, and collective welfare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14676v1</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bianca Maria Lerma, Rafael Pe\~naloza</dc:creator>
    </item>
    <item>
      <title>Practical, Utilitarian Algorithm Configuration</title>
      <link>https://arxiv.org/abs/2510.14683</link>
      <description>arXiv:2510.14683v1 Announce Type: new 
Abstract: Utilitarian algorithm configuration identifies a parameter setting for a given algorithm that maximizes a user's utility. Utility functions offer a theoretically well-grounded approach to optimizing decision-making under uncertainty and are flexible enough to capture a user's preferences over algorithm runtimes (e.g., they can describe a sharp cutoff after which a solution is no longer required, a per-hour cost for compute, or diminishing returns from algorithms that take longer to run). COUP is a recently-introduced utilitarian algorithm configuration procedure which was designed mainly to offer strong theoretical guarantees about the quality of the configuration it returns, with less attention paid to its practical performance. This paper closes that gap, bringing theoretically-grounded, utilitarian algorithm configuration to the point where it is competitive with widely used, heuristic configuration procedures that offer no performance guarantees. We present a series of improvements to COUP that improve its empirical performance without degrading its theoretical guarantees and demonstrate their benefit experimentally. Using a case study, we also illustrate ways of exploring the robustness of a given solution to the algorithm selection problem to variations in the utility function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14683v1</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Devon Graham, Kevin Leyton-Brown</dc:creator>
    </item>
    <item>
      <title>Purifying Task Vectors in Knowledge-Aware Subspace for Model Merging</title>
      <link>https://arxiv.org/abs/2510.14697</link>
      <description>arXiv:2510.14697v1 Announce Type: new 
Abstract: Model merging aims to integrate task-specific abilities from individually fine-tuned models into a single model without extra training. In recent model merging methods, task vector has become a fundamental building block, as it can encapsulate the residual information from finetuning. However, the merged model often suffers from notable performance degradation due to the conflicts caused by task-irrelevant redundancy in task vectors. Existing efforts in overcoming redundancy by randomly dropping elements in the parameter space involves randomness and lacks knowledge awareness. To address these challenges, in this study, we propose Purifying TAsk Vectors (PAVE) in knowledge-aware subspace. Concretely, we sample some training examples from each task, and feed them into their corresponding fine-tuned models to acquire the covariance matrices before linear layers. We then perform a context-oriented singular value decomposition, which accentuates the weight components most relevant to the target knowledge. As a result, we can split fine-tuned model weights into task-relevant and redundant components in the knowledge-aware subspace, and purify the task vector by pruning the redundant components. To induce fair pruning efforts across models, we further introduce a spectral rank allocation strategy by optimizing a normalized activated pruning error. The task vector purification by our method as a plug-and-play scheme is applicable across various task vector-based merging methods to improve their performance. In experiments, we demonstrate the effectiveness of PAVE across a diverse set of merging methods, tasks, and model architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14697v1</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bang An, Yibo Yang, Philip Torr, Bernard Ghanem</dc:creator>
    </item>
    <item>
      <title>Cognitive-Aligned Spatio-Temporal Large Language Models For Next Point-of-Interest Prediction</title>
      <link>https://arxiv.org/abs/2510.14702</link>
      <description>arXiv:2510.14702v1 Announce Type: new 
Abstract: The next point-of-interest (POI) recommendation task aims to predict the users' immediate next destinations based on their preferences and historical check-ins, holding significant value in location-based services. Recently, large language models (LLMs) have shown great potential in recommender systems, which treat the next POI prediction in a generative manner. However, these LLMs, pretrained primarily on vast corpora of unstructured text, lack the native understanding of structured geographical entities and sequential mobility patterns required for next POI prediction tasks. Moreover, in industrial-scale POI prediction applications, incorporating world knowledge and alignment of human cognition, such as seasons, weather conditions, holidays, and users' profiles (such as habits, occupation, and preferences), can enhance the user experience while improving recommendation performance. To address these issues, we propose CoAST (Cognitive-Aligned Spatial-Temporal LLMs), a framework employing natural language as an interface, allowing for the incorporation of world knowledge, spatio-temporal trajectory patterns, profiles, and situational information. Specifically, CoAST mainly comprises of 2 stages: (1) Recommendation Knowledge Acquisition through continued pretraining on the enriched spatial-temporal trajectory data of the desensitized users; (2) Cognitive Alignment to align cognitive judgments with human preferences using enriched training data through Supervised Fine-Tuning (SFT) and a subsequent Reinforcement Learning (RL) phase. Extensive offline experiments on various real-world datasets and online experiments deployed in "Guess Where You Go" of AMAP App homepage demonstrate the effectiveness of CoAST.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14702v1</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Penglong Zhai, Jie Li, Fanyi Di, Yue Liu, Yifang Yuan, Jie Huang, Peng Wu, Sicong Wang, Mingyang Yin, Tingting Hu, Yao Xu, Xin Li</dc:creator>
    </item>
    <item>
      <title>ToolPRM: Fine-Grained Inference Scaling of Structured Outputs for Function Calling</title>
      <link>https://arxiv.org/abs/2510.14703</link>
      <description>arXiv:2510.14703v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly demonstrating strong capabilities as autonomous agents, with function calling serving as a core mechanism for interaction with the environment. Meanwhile, inference scaling has become a cutting-edge technique to enhance LLM performance by allocating more computational resources during the inference process. However, current research on inference scaling primarily focuses on unstructured output generation tasks, leaving its application in structured outputs, like function calling, largely underexplored. To bridge this gap, we propose an inference scaling framework that combines fine-grained beam search with a process reward model, ToolPRM, which scores the internal steps of each single function call. To train ToolPRM, we construct the first fine-grained intra-call process supervision dataset, automatically annotated with function-masking techniques to provide step-level rewards for structured tool-use reasoning. Extensive experiments demonstrate that ToolPRM beats the coarse-grained and outcome reward models in terms of predictive accuracy, indicating its stronger capability in supervising the function calling inference process. Inference scaling technique equipped with ToolPRM also significantly improves the backbone model performance across various function calling tasks and benchmarks. More importantly, we reveal a key principle for applying inference scaling techniques to structured outputs: "explore more but retain less" due to the unrecoverability characteristics of structured function calling generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14703v1</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianghao Lin, Yuanyuan Shi, Xin Peng, Renjie Ding, Hairui Wang, Yuxuan Peng, Bizhe Bai, Weixi Song, Fengshuo Bai, Huacan Chai, Weinan Zhang, Fei Huang, Ying Wen</dc:creator>
    </item>
    <item>
      <title>SimKO: Simple Pass@K Policy Optimization</title>
      <link>https://arxiv.org/abs/2510.14807</link>
      <description>arXiv:2510.14807v1 Announce Type: new 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has advanced the reasoning capabilities of large language models (LLMs). However, prevailing RLVR methods exhibit a systematic bias toward exploitation over exploration, as evidenced by improved pass@1 but reduced pass@K (K&gt;1) performance. To understand this issue, we analyze training dynamics of RLVR methods by tracking the token-level probability distributions over vocabulary candidates. Our analysis reveals a consistent probability concentration effect where the top-1 candidate increasingly accumulates probability mass and suppresses that of other candidates. More importantly, stronger over-concentration correlates with worse pass@K performance. Inspired by this finding, we propose Simple Pass@K Optimization (SimKO), a method designed to mitigate the over-concentration issue, thereby encouraging exploration. SimKO operates in an asymmetrical manner. For verified-correct responses, it boosts the probabilities of the top-K candidates. For verified-incorrect responses, it applies stronger penalties to the top-1 candidate. We observe that this asymmetric design is particularly effective at mitigating over-concentration when applied at tokens with high entropy. Across various math and logical-reasoning benchmarks, SimKO consistently yields higher pass@K for a wide range of K, providing a simple way to improve RLVR's exploration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14807v1</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruotian Peng, Yi Ren, Zhouliang Yu, Weiyang Liu, Yandong Wen</dc:creator>
    </item>
    <item>
      <title>Agentic NL2SQL to Reduce Computational Costs</title>
      <link>https://arxiv.org/abs/2510.14808</link>
      <description>arXiv:2510.14808v1 Announce Type: new 
Abstract: Translating natural language queries into SQL queries (NL2SQL or Text-to-SQL) has recently been empowered by large language models (LLMs). Using LLMs to perform NL2SQL methods on a large collection of SQL databases necessitates processing large quantities of meta-information about the databases, which in turn results in lengthy prompts with many tokens and high processing costs. To address this challenge, we introduce Datalake Agent, an agentic system designed to enable an LLM to solve NL2SQL tasks more efficiently. Instead of utilizing direct solvers for NL2SQL that call the LLM once with all meta-information in the prompt, the Datalake Agent employs an interactive loop to reduce the utilized meta-information. Within the loop, the LLM is used in a reasoning framework that selectively requests only the necessary information to solve a table question answering task. We evaluate the Datalake Agent on a collection of 23 databases with 100 table question answering tasks. The Datalake Agent reduces the tokens used by the LLM by up to 87\% and thus allows for substantial cost reductions while maintaining competitive performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14808v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dominik Jehle, Lennart Purucker, Frank Hutter</dc:creator>
    </item>
    <item>
      <title>RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2510.14828</link>
      <description>arXiv:2510.14828v1 Announce Type: new 
Abstract: Improving the reasoning capabilities of embodied agents is crucial for robots to complete complex human instructions in long-view manipulation tasks successfully. Despite the success of large language models and vision language models based on Supervised Fine-Tuning (SFT) in planning tasks, they continue facing challenges in performing long-horizon manipulation tasks in complex real-world environments, owing to their restricted common sense and reasoning capabilities. Considering that aligning general-purpose vision language models to robotic planning tasks via supervised fine-tuning suffers from poor generalization and insufficient physical understanding, we propose RoboGPT-R1, a two-stage fine-tuning framework for embodied planning. In this framework, supervised training acquires foundational knowledge through expert sequences, followed by RL to address the model's shortcomings in visual-spatial understanding and reasoning. To achieve physical understanding and action sequence consistency in multi-step reasoning tasks, we design a rule-based reward function that simultaneously considers long-horizon performance and action constraint in the environment. The reasoning model, trained on Qwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini, by 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the EmbodiedBench benchmark.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14828v1</guid>
      <category>cs.AI</category>
      <category>cs.RO</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinrui Liu, Bingyan Nie, Boyu Li, Yaran Chen, Yuze Wang, Shunsen He, Haoran Li</dc:creator>
    </item>
    <item>
      <title>Boosting Instruction Following at Scale</title>
      <link>https://arxiv.org/abs/2510.14842</link>
      <description>arXiv:2510.14842v1 Announce Type: new 
Abstract: A typical approach developers follow to influence an LLM's behavior in an application is through careful manipulation of the prompt, such as by adding or modifying instructions. However, merely adding more instructions provides little assurance that they will actually be followed. We introduce Instruction Boosting as a post-generation method to increase the reliability of LLM prompt instructions. We show that Instruction Boosting improves the instruction following rate by up to 7 points for two instructions and up to 4 points for ten instructions. To demonstrate these results we introduce SCALEDIF, a benchmark with a scaled instruction volume of up to ten instructions per data sample. We also present an analysis of the commonly observed trend that performance degrades as more instructions are added. We show that an important factor contributing to this trend is the degree of tension and conflict that arises as the number of instructions is increased. We contribute a quantitative conflict scoring tool that explains the observed performance trends and provides feedback to developers on the impact that additional prompt instructions have on a model's performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14842v1</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ben Elder, Evelyn Duesterwald, Vinod Muthusamy</dc:creator>
    </item>
    <item>
      <title>Where to Search: Measure the Prior-Structured Search Space of LLM Agents</title>
      <link>https://arxiv.org/abs/2510.14846</link>
      <description>arXiv:2510.14846v1 Announce Type: new 
Abstract: The generate-filter-refine (iterative paradigm) based on large language models (LLMs) has achieved progress in reasoning, programming, and program discovery in AI+Science. However, the effectiveness of search depends on where to search, namely, how to encode the domain prior into an operationally structured hypothesis space. To this end, this paper proposes a compact formal theory that describes and measures LLM-assisted iterative search guided by domain priors. We represent an agent as a fuzzy relation operator on inputs and outputs to capture feasible transitions; the agent is thereby constrained by a fixed safety envelope. To describe multi-step reasoning/search, we weight all reachable paths by a single continuation parameter and sum them to obtain a coverage generating function; this induces a measure of reachability difficulty; and it provides a geometric interpretation of search on the graph induced by the safety envelope. We further provide the simplest testable inferences and validate them via a majority-vote instantiation. This theory offers a workable language and operational tools to measure agents and their search spaces, proposing a systematic formal description of iterative search constructed by LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14846v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LO</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhuo-Yang Song</dc:creator>
    </item>
    <item>
      <title>LabOS: The AI-XR Co-Scientist That Sees and Works With Humans</title>
      <link>https://arxiv.org/abs/2510.14861</link>
      <description>arXiv:2510.14861v1 Announce Type: new 
Abstract: Modern science advances fastest when thought meets action. LabOS represents the first AI co-scientist that unites computational reasoning with physical experimentation through multimodal perception, self-evolving agents, and Entended-Reality(XR)-enabled human-AI collaboration. By connecting multi-model AI agents, smart glasses, and human-AI collaboration, LabOS allows AI to see what scientists see, understand experimental context, and assist in real-time execution. Across applications--from cancer immunotherapy target discovery to stem-cell engineering -- LabOS shows that AI can move beyond computational design to participation, turning the laboratory into an intelligent, collaborative environment where human and machine discovery evolve together.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14861v1</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Le Cong, Zaixi Zhang, Xiaotong Wang, Yin Di, Ruofan Jin, Michal Gerasimiuk, Yinkai Wang, Ravi K. Dinesh, David Smerkous, Alex Smerkous, Xuekun Wu, Shilong Liu, Peishan Li, Yi Zhu, Simran Serrao, Ning Zhao, Imran A. Mohammad, John B. Sunwoo, Joseph C. Wu, Mengdi Wang</dc:creator>
    </item>
    <item>
      <title>The Gatekeeper Knows Enough</title>
      <link>https://arxiv.org/abs/2510.14881</link>
      <description>arXiv:2510.14881v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly deployed as autonomous agents, yet their practical utility is fundamentally constrained by a limited context window and state desynchronization resulting from the LLMs' stateless nature and inefficient context management. These limitations lead to unreliable output, unpredictable behavior, and inefficient resource usage, particularly when interacting with large, structured, and sensitive knowledge systems such as codebases and documents. To address these challenges, we introduce the Gatekeeper Protocol, a novel, domain-agnostic framework that governs agent-system interactions. Our protocol mandates that the agent first operate and reason on a minimalist, low-fidelity "latent state" representation of the system to strategically request high-fidelity context on demand. All interactions are mediated through a unified JSON format that serves as a declarative, state-synchronized protocol, ensuring the agent's model of the system remains verifiably grounded in the system's reality. We demonstrate the efficacy of this protocol with Sage, a reference implementation of the Gatekeeper Protocol for software development. Our results show that this approach significantly increases agent reliability, improves computational efficiency by minimizing token consumption, and enables scalable interaction with complex systems, creating a foundational methodology for building more robust, predictable, and grounded AI agents for any structured knowledge domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14881v1</guid>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fikresilase Wondmeneh Abebayew</dc:creator>
    </item>
    <item>
      <title>Mapping Smarter, Not Harder: A Test-Time Reinforcement Learning Agent That Improves Without Labels or Model Updates</title>
      <link>https://arxiv.org/abs/2510.14900</link>
      <description>arXiv:2510.14900v1 Announce Type: new 
Abstract: The Enterprise Intelligence Platform must integrate logs from numerous third-party vendors in order to perform various downstream tasks. However, vendor documentation is often unavailable at test time. It is either misplaced, mismatched, poorly formatted, or incomplete, which makes schema mapping challenging. We introduce a reinforcement learning agent that can self-improve without labeled examples or model weight updates. During inference, the agent: 1) Identifies ambiguous field-mapping attempts. 2) Generates targeted web-search queries to gather external evidence. 3) Applies a confidence-based reward to iteratively refine its mappings. To demonstrate this concept, we converted Microsoft Defender for Endpoint logs into a common schema. Our method increased mapping accuracy from 56.4\%(LLM-only) to 72.73\%(RAG) to 93.94\% over 100 iterations using GPT-4o. At the same time, it reduced the number of low-confidence mappings requiring expert review by 85\%. This new approach provides an evidence-driven, transparent method for solving future industry problems, paving the way for more robust, accountable, scalable, efficient, flexible, adaptable, and collaborative solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14900v1</guid>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wen-Kwang Tsao, Yao-Ching Yu, Chien-Ming Huang</dc:creator>
    </item>
    <item>
      <title>Budget-aware Test-time Scaling via Discriminative Verification</title>
      <link>https://arxiv.org/abs/2510.14913</link>
      <description>arXiv:2510.14913v1 Announce Type: new 
Abstract: Test-time scaling is a powerful strategy for boosting the performance of large language models on complex reasoning tasks. While state-of-the-art approaches often employ generative verifiers to select the best solution from a pool of candidates, this method incurs prohibitive computational costs, limiting its practicality. In this work, we shift the focus to a more budget-aware paradigm: discriminative verification. We conduct a thorough empirical analysis and demonstrate that while discriminative verifiers may underperform in isolation, combining them with self-consistency in a hybrid approach creates a powerful and efficient test-time scaling mechanism. Notably, under a fixed compute budget, this hybrid approach surpasses state-of-the-art generative verification by a significant margin: achieving up to 15.3\% higher accuracy on AIME2025. Our findings establish that for practical, real-world applications, budget-aware scaling with discriminative verifiers is not only a "free" upgrade over self-consistency, but also a more effective and efficient alternative to costly generative techniques. Code is available at https://github.com/wang-research-lab/verification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14913v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyle Montgomery, Sijun Tan, Yuqi Chen, Siyuan Zhuang, Tianjun Zhang, Raluca Ada Popa, Chenguang Wang</dc:creator>
    </item>
    <item>
      <title>TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG</title>
      <link>https://arxiv.org/abs/2510.14922</link>
      <description>arXiv:2510.14922v1 Announce Type: new 
Abstract: Depression is a widespread mental health disorder, yet its automatic detection remains challenging. Prior work has explored unimodal and multimodal approaches, with multimodal systems showing promise by leveraging complementary signals. However, existing studies are limited in scope, lack systematic comparisons of features, and suffer from inconsistent evaluation protocols. We address these gaps by systematically exploring feature representations and modelling strategies across EEG, together with speech and text. We evaluate handcrafted features versus pre-trained embeddings, assess the effectiveness of different neural encoders, compare unimodal, bimodal, and trimodal configurations, and analyse fusion strategies with attention to the role of EEG. Consistent subject-independent splits are applied to ensure robust, reproducible benchmarking. Our results show that (i) the combination of EEG, speech and text modalities enhances multimodal detection, (ii) pretrained embeddings outperform handcrafted features, and (iii) carefully designed trimodal models achieve state-of-the-art performance. Our work lays the groundwork for future research in multimodal depression detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14922v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>eess.AS</category>
      <category>eess.SP</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Annisaa Fitri Nurfidausi, Eleonora Mancini, Paolo Torroni</dc:creator>
    </item>
    <item>
      <title>Stable but Miscalibrated: A Kantian View on Overconfidence from Filters to Large Language Models</title>
      <link>https://arxiv.org/abs/2510.14925</link>
      <description>arXiv:2510.14925v1 Announce Type: new 
Abstract: We reinterpret Kant's Critique of Pure Reason as a theory of feedback stability, viewing reason as a regulator that keeps inference within the bounds of possible experience. We formalize this intuition via a composite instability index (H-Risk) combining spectral margin, conditioning, temporal sensitivity, and innovation amplification. In linear-Gaussian simulations, higher H-Risk predicts overconfident errors even under formal stability, revealing a gap between nominal and epistemic stability. Extending to large language models (LLMs), we find that fragile internal dynamics correlate with miscalibration and hallucination, while critique-style prompts show mixed effects on calibration and hallucination. These results suggest a structural bridge between Kantian self-limitation and feedback control, offering a principled lens for diagnosing -- and selectively reducing -- overconfidence in reasoning systems. This is a preliminary version; supplementary experiments and broader replication will be reported in a future revision.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14925v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akira Okutomi</dc:creator>
    </item>
    <item>
      <title>GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for Step-Level Reasoning</title>
      <link>https://arxiv.org/abs/2510.14942</link>
      <description>arXiv:2510.14942v1 Announce Type: new 
Abstract: Process Reward Models (PRMs) aim to improve multi-step reasoning in Large Language Models (LLMs) by supervising intermediate steps and identifying errors. However, building effective PRMs remains challenging due to the lack of scalable, high-quality annotations. Existing approaches rely on costly human labeling, LLM-based self-evaluation that is prone to hallucination, or Monte Carlo (MC) estimation, which infers step quality solely from rollout outcomes and often introduces noisy, misaligned supervision due to credit misattribution. These issues result in three core limitations: noisy rewards, low factual fidelity, and misalignment with step-level reasoning objectives. To address these challenges, we introduce GroundedPRM, a tree-guided and fidelity-aware framework for automatic process supervision. To reduce reward noise and enable fine-grained credit assignment, we construct structured reasoning paths via Monte Carlo Tree Search (MCTS). To eliminate hallucinated supervision, we validate each intermediate step using an external tool, providing execution-grounded correctness signals. To combine both step-level validation and global outcome assessment, we design a hybrid reward aggregation mechanism that fuses tool-based verification with MCTS-derived feedback. Finally, we format the reward signal into a rationale-enhanced, generative structure to promote interpretability and compatibility with instruction-tuned LLMs. GroundedPRM is trained on only 40K automatically labeled samples, amounting to just 10% of the data used by the best-performing PRM trained with auto-labeled supervision. Nevertheless, it achieves up to a 26% relative improvement in average performance on ProcessBench. When used for reward-guided greedy search, GroundedPRM outperforms even PRMs trained with human-labeled supervision, offering a scalable and verifiable path toward high-quality process-level reasoning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14942v1</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yao Zhang, Yu Wu, Haowei Zhang, Weiguo Li, Haokun Chen, Jingpei Wu, Guohao Li, Zhen Han, Volker Tresp</dc:creator>
    </item>
    <item>
      <title>Agentic Design of Compositional Machines</title>
      <link>https://arxiv.org/abs/2510.14980</link>
      <description>arXiv:2510.14980v1 Announce Type: new 
Abstract: The design of complex machines stands as both a marker of human intelligence and a foundation of engineering practice. Given recent advances in large language models (LLMs), we ask whether they, too, can learn to create. We approach this question through the lens of compositional machine design: a task in which machines are assembled from standardized components to meet functional demands like locomotion or manipulation in a simulated physical environment. To support this investigation, we introduce BesiegeField, a testbed built on the machine-building game Besiege, which enables part-based construction, physical simulation and reward-driven evaluation. Using BesiegeField, we benchmark state-of-the-art LLMs with agentic workflows and identify key capabilities required for success, including spatial reasoning, strategic assembly, and instruction-following. As current open-source models fall short, we explore reinforcement learning (RL) as a path to improvement: we curate a cold-start dataset, conduct RL finetuning experiments, and highlight open challenges at the intersection of language, machine design, and physical reasoning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14980v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenqian Zhang, Weiyang Liu, Zhen Liu</dc:creator>
    </item>
    <item>
      <title>Generative AI in Heritage Practice: Improving the Accessibility of Heritage Guidance</title>
      <link>https://arxiv.org/abs/2510.13811</link>
      <description>arXiv:2510.13811v1 Announce Type: cross 
Abstract: This paper discusses the potential for integrating Generative Artificial Intelligence (GenAI) into professional heritage practice with the aim of enhancing the accessibility of public-facing guidance documents. We developed HAZEL, a GenAI chatbot fine-tuned to assist with revising written guidance relating to heritage conservation and interpretation. Using quantitative assessments, we compare HAZEL's performance to that of ChatGPT (GPT-4) in a series of tasks related to the guidance writing process. The results of this comparison indicate a slightly better performance of HAZEL over ChatGPT, suggesting that the GenAI chatbot is more effective once the underlying large language model (LLM) has been fine-tuned. However, we also note significant limitations, particularly in areas requiring cultural sensitivity and more advanced technical expertise. These findings suggest that, while GenAI cannot replace human heritage professionals in technical authoring tasks, its potential to automate and expedite certain aspects of guidance writing could offer valuable benefits to heritage organisations, especially in resource-constrained contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13811v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jessica Witte, Edmund Lee, Lisa Brausem, Verity Shillabeer, Chiara Bonacchi</dc:creator>
    </item>
    <item>
      <title>Reversing the Lens: Using Explainable AI to Understand Human Expertise</title>
      <link>https://arxiv.org/abs/2510.13814</link>
      <description>arXiv:2510.13814v1 Announce Type: cross 
Abstract: Both humans and machine learning models learn from experience, particularly in safety- and reliability-critical domains. While psychology seeks to understand human cognition, the field of Explainable AI (XAI) develops methods to interpret machine learning models. This study bridges these domains by applying computational tools from XAI to analyze human learning. We modeled human behavior during a complex real-world task -- tuning a particle accelerator -- by constructing graphs of operator subtasks. Applying techniques such as community detection and hierarchical clustering to archival operator data, we reveal how operators decompose the problem into simpler components and how these problem-solving structures evolve with expertise. Our findings illuminate how humans develop efficient strategies in the absence of globally optimal solutions, and demonstrate the utility of XAI-based methods for quantitatively studying human cognition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13814v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roussel Rahman, Aashwin Ananda Mishra, Wan-Lin Hu</dc:creator>
    </item>
    <item>
      <title>GQVis: A Dataset of Genomics Data Questions and Visualizations for Generative AI</title>
      <link>https://arxiv.org/abs/2510.13816</link>
      <description>arXiv:2510.13816v1 Announce Type: cross 
Abstract: Data visualization is a fundamental tool in genomics research, enabling the exploration, interpretation, and communication of complex genomic features. While machine learning models show promise for transforming data into insightful visualizations, current models lack the training foundation for domain-specific tasks. In an effort to provide a foundational resource for genomics-focused model training, we present a framework for generating a dataset that pairs abstract, low-level questions about genomics data with corresponding visualizations. Building on prior work with statistical plots, our approach adapts to the complexity of genomics data and the specialized representations used to depict them. We further incorporate multiple linked queries and visualizations, along with justifications for design choices, figure captions, and image alt-texts for each item in the dataset. We use genomics data retrieved from three distinct genomics data repositories (4DN, ENCODE, Chromoscope) to produce GQVis: a dataset consisting of 1.14 million single-query data points, 628k query pairs, and 589k query chains. The GQVis dataset and generation code are available at https://huggingface.co/datasets/HIDIVE/GQVis and https://github.com/hms-dbmi/GQVis-Generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13816v1</guid>
      <category>q-bio.GN</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Skylar Sargent Walters, Arthea Valderrama, Thomas C. Smits, David Kou\v{r}il, Huyen N. Nguyen, Sehi L'Yi, Devin Lange, Nils Gehlenborg</dc:creator>
    </item>
    <item>
      <title>Leveraging Wireless Sensor Networks for Real-Time Monitoring and Control of Industrial Environments</title>
      <link>https://arxiv.org/abs/2510.13820</link>
      <description>arXiv:2510.13820v1 Announce Type: cross 
Abstract: This research proposes an extensive technique for monitoring and controlling the industrial parameters using Internet of Things (IoT) technology based on wireless communication. We proposed a system based on NRF transceivers to establish a strong Wireless Sensor Network (WSN), enabling transfer of real-time data from multiple sensors to a central setup that is driven by ARDUINO microcontrollers. Different key parameters, crucial for industrial setup such as temperature, humidity, soil moisture and fire detection, are monitored and displayed on an LCD screen, enabling factory administration to oversee the industrial operations remotely over the internet. Our proposed system bypasses the need for physical presence for monitoring by addressing the shortcomings of conventional wired communication systems. Other than monitoring, there is an additional feature to remotely control these parameters by controlling the speed of DC motors through online commands. Given the rising incidence of industrial fires over the worldwide between 2020 and 2024 due to an array of hazards, this system with dual functionality boosts the overall operational efficiency and safety. This overall integration of IoT and Wireless Sensor Network (WSN) reduces the potential risks linked with physical monitoring, providing rapid responses in emergency scenarios, including the activation of firefighting equipment. The results show that innovations in wireless communication perform an integral part in industrial process automation and safety, paving the way to more intelligent and responsive operating environments. Overall, this study highlights the potential for change of IoT-enabled systems to revolutionize monitoring and control in a variety of industrial applications, resulting in increased productivity and safety.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13820v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Muhammad Junaid Asif, Shazia Saqib, Rana Fayyaz Ahmad, Hamza Khan</dc:creator>
    </item>
    <item>
      <title>A2AS: Agentic AI Runtime Security and Self-Defense</title>
      <link>https://arxiv.org/abs/2510.13825</link>
      <description>arXiv:2510.13825v1 Announce Type: cross 
Abstract: The A2AS framework is introduced as a security layer for AI agents and LLM-powered applications, similar to how HTTPS secures HTTP. A2AS enforces certified behavior, activates model self-defense, and ensures context window integrity. It defines security boundaries, authenticates prompts, applies security rules and custom policies, and controls agentic behavior, enabling a defense-in-depth strategy. The A2AS framework avoids latency overhead, external dependencies, architectural changes, model retraining, and operational complexity. The BASIC security model is introduced as the A2AS foundation: (B) Behavior certificates enable behavior enforcement, (A) Authenticated prompts enable context window integrity, (S) Security boundaries enable untrusted input isolation, (I) In-context defenses enable secure model reasoning, (C) Codified policies enable application-specific rules. This first paper in the series introduces the BASIC security model and the A2AS framework, exploring their potential toward establishing the A2AS industry standard.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13825v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eugene Neelou, Ivan Novikov, Max Moroz, Om Narayan, Tiffany Saade, Mika Ayenson, Ilya Kabanov, Jen Ozmen, Edward Lee, Vineeth Sai Narajala, Emmanuel Guilherme Junior, Ken Huang, Huseyin Gulsin, Jason Ross, Marat Vyshegorodtsev, Adelin Travers, Idan Habler, Rahul Jadav</dc:creator>
    </item>
    <item>
      <title>Towards Neurocognitive-Inspired Intelligence: From AI's Structural Mimicry to Human-Like Functional Cognition</title>
      <link>https://arxiv.org/abs/2510.13826</link>
      <description>arXiv:2510.13826v1 Announce Type: cross 
Abstract: Artificial intelligence has advanced significantly through deep learning, reinforcement learning, and large language and vision models. However, these systems often remain task specific, struggle to adapt to changing conditions, and cannot generalize in ways similar to human cognition. Additionally, they mainly focus on mimicking brain structures, which often leads to black-box models with limited transparency and adaptability. Inspired by the structure and function of biological cognition, this paper introduces the concept of "Neurocognitive-Inspired Intelligence (NII)," a hybrid approach that combines neuroscience, cognitive science, computer vision, and AI to develop more general, adaptive, and robust intelligent systems capable of rapid learning, learning from less data, and leveraging prior experience. These systems aim to emulate the human brain's ability to flexibly learn, reason, remember, perceive, and act in real-world settings with minimal supervision. We review the limitations of current AI methods, define core principles of neurocognitive-inspired intelligence, and propose a modular, biologically inspired architecture that emphasizes integration, embodiment, and adaptability. We also discuss potential implementation strategies and outline various real-world applications, from robotics to education and healthcare. Importantly, this paper offers a hybrid roadmap for future research, laying the groundwork for building AI systems that more closely resemble human cognition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13826v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Noorbakhsh Amiri Golilarz, Hassan S. Al Khatib, Shahram Rahimi</dc:creator>
    </item>
    <item>
      <title>Bridging the Semantic Gap: Contrastive Rewards for Multilingual Text-to-SQL</title>
      <link>https://arxiv.org/abs/2510.13827</link>
      <description>arXiv:2510.13827v1 Announce Type: cross 
Abstract: Current Text-to-SQL methods are evaluated and only focused on executable queries, overlooking the semantic alignment challenge -- both in terms of the semantic meaning of the query and the correctness of the execution results. Even execution accuracy itself shows significant drops when moving from English to other languages, with an average decline of 6 percentage points across non-English languages. We address these challenges by presenting a new framework that combines Group Relative Policy Optimization (GRPO) within a multilingual contrastive reward signal to enhance both task efficiency and semantic accuracy in Text-to-SQL systems in cross-lingual scenarios. Our method teaches models to obtain better correspondence between SQL generation and user intent by combining a reward signal based on semantic similarity. On the seven-language MultiSpider dataset, fine-tuning the LLaMA-3-3B model with GRPO improved the execution accuracy up to 87.4 percent (+26 pp over zero-shot) and semantic accuracy up to 52.29 percent (+32.86 pp). Adding our contrastive reward signal in the GRPO framework further improved the average semantic accuracy to 59.14 percent (+6.85 pp, up to +10 pp for Vietnamese). Our experiments showcase that a smaller, parameter-efficient 3B LLaMA model fine-tuned with our contrastive reward signal outperforms a much larger zero-shot 8B LLaMA model, with an uplift of 7.43 pp in execution accuracy (from 81.43 percent on the 8B model to 88.86 percent on the 3B model), and nearly matches its semantic accuracy (59.14 percent vs. 68.57 percent) -- all using just 3,000 reinforcement learning training examples. These results demonstrate how we can improve the performance of Text-to-SQL systems with contrastive rewards for directed semantic alignment, without requiring large-scale training datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13827v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ashish Kattamuri, Ishita Prasad, Meetu Malhotra, Arpita Vats, Rahul Raja, Albert Lie</dc:creator>
    </item>
    <item>
      <title>A Linguistics-Aware LLM Watermarking via Syntactic Predictability</title>
      <link>https://arxiv.org/abs/2510.13829</link>
      <description>arXiv:2510.13829v1 Announce Type: cross 
Abstract: As large language models (LLMs) continue to advance rapidly, reliable governance tools have become critical. Publicly verifiable watermarking is particularly essential for fostering a trustworthy AI ecosystem. A central challenge persists: balancing text quality against detection robustness. Recent studies have sought to navigate this trade-off by leveraging signals from model output distributions (e.g., token-level entropy); however, their reliance on these model-specific signals presents a significant barrier to public verification, as the detection process requires access to the logits of the underlying model. We introduce STELA, a novel framework that aligns watermark strength with the linguistic degrees of freedom inherent in language. STELA dynamically modulates the signal using part-of-speech (POS) n-gram-modeled linguistic indeterminacy, weakening it in grammatically constrained contexts to preserve quality and strengthen it in contexts with greater linguistic flexibility to enhance detectability. Our detector operates without access to any model logits, thus facilitating publicly verifiable detection. Through extensive experiments on typologically diverse languages-analytic English, isolating Chinese, and agglutinative Korean-we show that STELA surpasses prior methods in detection robustness. Our code is available at https://github.com/Shinwoo-Park/stela_watermark.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13829v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shinwoo Park, Hyejin Park, Hyeseon Ahn, Yo-Sub Han</dc:creator>
    </item>
    <item>
      <title>Users as Annotators: LLM Preference Learning from Comparison Mode</title>
      <link>https://arxiv.org/abs/2510.13830</link>
      <description>arXiv:2510.13830v1 Announce Type: cross 
Abstract: Pairwise preference data have played an important role in the alignment of large language models (LLMs). Each sample of such data consists of a prompt, two different responses to the prompt, and a binary label indicating which of the two responses is better. The labels are usually annotated by professional human annotators. In this paper, we consider an alternative approach to collect pairwise preference data -- user annotation from comparison mode. With the increasingly wider adoption of LLMs among the population, users are contributing more and more of their preference labels through their daily interactions with the LLMs. The upside of such labels is that users are the best experts in judging the responses to their own queries/prompts, but the downside is the lack of quality control in these labels. In this paper, we consider a new idea of generating two responses from two different models or two different versions of the same model. The asymmetry allows us to make an inference of the user's data quality through our proposed user behavior model. We develop an expectation-maximization algorithm to estimate a latent quality factor of the user, and filter users' annotation data accordingly. The downstream task shows the effectiveness of our approach in both capturing the user behavior and data filtering for LLM alignment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13830v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhongze Cai, Xiaocheng Li</dc:creator>
    </item>
    <item>
      <title>Informed Routing in LLMs: Smarter Token-Level Computation for Faster Inference</title>
      <link>https://arxiv.org/abs/2510.13831</link>
      <description>arXiv:2510.13831v1 Announce Type: cross 
Abstract: The deployment of large language models (LLMs) in real-world applications is increasingly limited by their high inference cost. While recent advances in dynamic token-level computation allocation attempt to improve efficiency by selectively activating model components per token, existing methods rely on greedy routing--a myopic execute-or-skip mechanism that often leads to irreversible information loss and suboptimal token selection. This paper introduces informed routing, a new paradigm that proactively addresses these issues. The key insight is to assess not only a token's immediate importance but also its recoverability, i.e., how well its transformation can be approximated. To this end, we propose the Lightweight Feature Forecaster (LFF), a small predictive module that estimates a unit's output before routing decisions are made. This enables a flexible execute-or-approximate policy that preserves model fidelity while drastically reducing computation. Extensive experiments on both language modeling and reasoning tasks show that informed routing achieves state-of-the-art efficiency-performance trade-offs across multiple sparsity levels. Notably, even without final LoRA fine-tuning, our method matches or surpasses strong baselines that require full fine-tuning, all while reducing training time by over 50%. The code is available at: https://github.com/EIT-NLP/informed-routing</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13831v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chao Han, Yijuan Liang, Zihao Xuan, Daokuan Wu, Wei Zhang, Xiaoyu Shen</dc:creator>
    </item>
    <item>
      <title>Entropy Meets Importance: A Unified Head Importance-Entropy Score for Stable and Efficient Transformer Pruning</title>
      <link>https://arxiv.org/abs/2510.13832</link>
      <description>arXiv:2510.13832v1 Announce Type: cross 
Abstract: Transformer-based models have achieved remarkable performance in NLP tasks. However, their structural characteristics-multiple layers and attention heads-introduce efficiency challenges in inference and deployment. To address these challenges, various pruning methods have recently been proposed. Notably, gradient-based methods using Head Importance Scores (HIS) have gained traction for interpretability, efficiency, and ability to identify redundant heads. However, HIS alone has limitations as it captures only the gradient-driven contribution, overlooking the diversity of attention patterns. To overcome these limitations, we introduce a novel pruning criterion, HIES (Head Importance-Entropy Score), which integrates head importance scores with attention entropy, providing complementary evidence on per-head contribution. Empirically, HIES-based pruning yields up to 15.2% improvement in model quality and 2.04x improvement in stability over HIS-only methods, enabling substantial model compression without sacrificing either accuracy or stability. Code will be released upon publication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13832v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Minsik Choi, Hyegang Son, Changhoon Kim, Young Geun Kim</dc:creator>
    </item>
    <item>
      <title>ConDABench: Interactive Evaluation of Language Models for Data Analysis</title>
      <link>https://arxiv.org/abs/2510.13835</link>
      <description>arXiv:2510.13835v1 Announce Type: cross 
Abstract: Real-world data analysis tasks often come with under-specified goals and unclean data. User interaction is necessary to understand and disambiguate a user's intent, and hence, essential to solving these complex tasks. Existing benchmarks for evaluating LLMs on data analysis tasks do not capture these complexities or provide first-class support for interactivity. We introduce ConDABench, a framework for generating conversational data analysis (ConDA) benchmarks and evaluating external tools on the generated benchmarks. \bench consists of (a) a multi-agent workflow for generating realistic benchmarks from articles describing insights gained from public datasets, (b) 1,420 ConDA problems generated using this workflow, and (c) an evaluation harness that, for the first time, makes it possible to systematically evaluate conversational data analysis tools on the generated ConDA problems. Evaluation of state-of-the-art LLMs on the benchmarks reveals that while the new generation of models are better at solving more instances, they are not necessarily better at solving tasks that require sustained, long-form engagement. ConDABench is an avenue for model builders to measure progress towards truly collaborative models that can complete complex interactive tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13835v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Avik Dutta, Priyanshu Gupta, Hosein Hasanbeig, Rahul Pratap Singh, Harshit Nigam, Sumit Gulwani, Arjun Radhakrishna, Gustavo Soares, Ashish Tiwari</dc:creator>
    </item>
    <item>
      <title>SIMBA UQ: Similarity-Based Aggregation for Uncertainty Quantification in Large Language Models</title>
      <link>https://arxiv.org/abs/2510.13836</link>
      <description>arXiv:2510.13836v1 Announce Type: cross 
Abstract: When does a large language model (LLM) know what it does not know? Uncertainty quantification (UQ) provides measures of uncertainty, such as an estimate of the confidence in an LLM's generated output, and is therefore increasingly recognized as a crucial component of trusted AI systems. Black-box UQ methods do not require access to internal model information from the generating LLM and therefore have numerous real-world advantages, such as robustness to system changes, adaptability to choice of LLM, reduced costs, and computational tractability. In this paper, we investigate the effectiveness of UQ techniques that are primarily but not necessarily entirely black-box, where the consistency between a generated output and other sampled generations is used as a proxy for confidence in its correctness. We propose a high-level non-verbalized similarity-based aggregation framework that subsumes a broad swath of UQ approaches suitable for complex generative tasks, as well as introduce specific novel techniques from the framework that train confidence estimation models using small training sets. Through an empirical study with datasets spanning the diverse tasks of question answering, summarization, and text-to-SQL, we demonstrate that our proposed similarity-based methods can yield better calibrated confidences than baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13836v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Debarun Bhattacharjya, Balaji Ganesan, Junkyu Lee, Radu Marinescu, Katsiaryna Mirylenka, Michael Glass, Xiao Shou</dc:creator>
    </item>
    <item>
      <title>Seeing Hate Differently: Hate Subspace Modeling for Culture-Aware Hate Speech Detection</title>
      <link>https://arxiv.org/abs/2510.13837</link>
      <description>arXiv:2510.13837v1 Announce Type: cross 
Abstract: Hate speech detection has been extensively studied, yet existing methods often overlook a real-world complexity: training labels are biased, and interpretations of what is considered hate vary across individuals with different cultural backgrounds. We first analyze these challenges, including data sparsity, cultural entanglement, and ambiguous labeling. To address them, we propose a culture-aware framework that constructs individuals' hate subspaces. To alleviate data sparsity, we model combinations of cultural attributes. For cultural entanglement and ambiguous labels, we use label propagation to capture distinctive features of each combination. Finally, individual hate subspaces, which in turn can further enhance classification performance. Experiments show our method outperforms state-of-the-art by 1.05\% on average across all metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13837v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.SI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weibin Cai, Reza Zafarani</dc:creator>
    </item>
    <item>
      <title>Meronymic Ontology Extraction via Large Language Models</title>
      <link>https://arxiv.org/abs/2510.13839</link>
      <description>arXiv:2510.13839v1 Announce Type: cross 
Abstract: Ontologies have become essential in today's digital age as a way of organising the vast amount of readily available unstructured text. In providing formal structure to this information, ontologies have immense value and application across various domains, e.g., e-commerce, where countless product listings necessitate proper product organisation. However, the manual construction of these ontologies is a time-consuming, expensive and laborious process. In this paper, we harness the recent advancements in large language models (LLMs) to develop a fully-automated method of extracting product ontologies, in the form of meronymies, from raw review texts. We demonstrate that the ontologies produced by our method surpass an existing, BERT-based baseline when evaluating using an LLM-as-a-judge. Our investigation provides the groundwork for LLMs to be used more generally in (product or otherwise) ontology extraction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13839v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dekai Zhang, Simone Conia, Antonio Rago</dc:creator>
    </item>
    <item>
      <title>ADMIT: Few-shot Knowledge Poisoning Attacks on RAG-based Fact Checking</title>
      <link>https://arxiv.org/abs/2510.13842</link>
      <description>arXiv:2510.13842v1 Announce Type: cross 
Abstract: Knowledge poisoning poses a critical threat to Retrieval-Augmented Generation (RAG) systems by injecting adversarial content into knowledge bases, tricking Large Language Models (LLMs) into producing attacker-controlled outputs grounded in manipulated context. Prior work highlights LLMs' susceptibility to misleading or malicious retrieved content. However, real-world fact-checking scenarios are more challenging, as credible evidence typically dominates the retrieval pool. To investigate this problem, we extend knowledge poisoning to the fact-checking setting, where retrieved context includes authentic supporting or refuting evidence. We propose \textbf{ADMIT} (\textbf{AD}versarial \textbf{M}ulti-\textbf{I}njection \textbf{T}echnique), a few-shot, semantically aligned poisoning attack that flips fact-checking decisions and induces deceptive justifications, all without access to the target LLMs, retrievers, or token-level control. Extensive experiments show that ADMIT transfers effectively across 4 retrievers, 11 LLMs, and 4 cross-domain benchmarks, achieving an average attack success rate (ASR) of 86\% at an extremely low poisoning rate of $0.93 \times 10^{-6}$, and remaining robust even in the presence of strong counter-evidence. Compared with prior state-of-the-art attacks, ADMIT improves ASR by 11.2\% across all settings, exposing significant vulnerabilities in real-world RAG-based fact-checking systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13842v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yutao Wu, Xiao Liu, Yinghui Li, Yifeng Gao, Yifan Ding, Jiale Ding, Xiang Zheng, Xingjun Ma</dc:creator>
    </item>
    <item>
      <title>Serialized EHR make for good text representations</title>
      <link>https://arxiv.org/abs/2510.13843</link>
      <description>arXiv:2510.13843v1 Announce Type: cross 
Abstract: The emergence of foundation models in healthcare has opened new avenues for learning generalizable representations from large scale clinical data. Yet, existing approaches often struggle to reconcile the tabular and event based nature of Electronic Health Records (EHRs) with the sequential priors of natural language models. This structural mismatch limits their ability to capture longitudinal dependencies across patient encounters. We introduce SerialBEHRT, a domain aligned foundation model that extends SciBERT through additional pretraining on structured EHR sequences. SerialBEHRT is designed to encode temporal and contextual relationships among clinical events, thereby producing richer patient representations. We evaluate its effectiveness on the task of antibiotic susceptibility prediction, a clinically meaningful problem in antibiotic stewardship. Through extensive benchmarking against state of the art EHR representation strategies, we demonstrate that SerialBEHRT achieves superior and more consistent performance, highlighting the importance of temporal serialization in foundation model pretraining for healthcare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13843v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhirong Chou, Quan Qin, Shi Li</dc:creator>
    </item>
    <item>
      <title>Information flow in multilayer perceptrons: an in-depth analysis</title>
      <link>https://arxiv.org/abs/2510.13846</link>
      <description>arXiv:2510.13846v1 Announce Type: cross 
Abstract: Analysing how information flows along the layers of a multilayer perceptron is a topic of paramount importance in the field of artificial neural networks. After framing the problem from the point of view of information theory, in this position article a specific investigation is conducted on the way information is processed, with particular reference to the requirements imposed by supervised learning. To this end, the concept of information matrix is devised and then used as formal framework for understanding the aetiology of optimisation strategies and for studying the information flow. The underlying research for this article has also produced several key outcomes: i) the definition of a parametric optimisation strategy, ii) the finding that the optimisation strategy proposed in the information bottleneck framework shares strong similarities with the one derived from the information matrix, and iii) the insight that a multilayer perceptron serves as a kind of "adaptor", meant to process the input according to the given objective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13846v1</guid>
      <category>cs.IT</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <category>math.IT</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giuliano Armano</dc:creator>
    </item>
    <item>
      <title>DynaSpec: Context-aware Dynamic Speculative Sampling for Large-Vocabulary Language Models</title>
      <link>https://arxiv.org/abs/2510.13847</link>
      <description>arXiv:2510.13847v1 Announce Type: cross 
Abstract: Speculative decoding (a.k.a. speculative sampling) has become a standard way to accelerate LLM inference: a small drafter proposes multiple tokens and a large target model verifies them once per speculation length. Recently, scaling of the LLM vocabulary has pushed the number of tokens to grow substantially. While verification over the full vocabulary leaves the target model largely unaffected, the O(|V|d) parameters in the drafter's output head become a latency bottleneck, slowing the entire pipeline. Contemporary methods (e.g., FR-Spec, VocabTrim) restrict the drafter's vocabulary to a fixed subset of the target model's vocabulary, ranked in descending order of token frequency. Although this reduces draft-time compute, it is brittle, since: (i) frequency lists are corpus-dependent and require retuning to generalize, and (ii) static shortlists suppress rare or domain-specific tokens, lowering the expected number of tokens per verification step. We propose DynaSpec, a context-dependent dynamic shortlisting mechanism that is robust, speeds up drafting, and generalizes across diverse tasks. Concretely, we introduce lightweight, coarse-grained meta-classifiers that route contexts to a small number of token clusters; the union of the top-k selected clusters forms the drafter's shortlist, while verification retains the full vocabulary and exactness. The meta-classifier finishes its computation earlier than the drafter's hidden state generation by exploiting parallel execution of draft encoding and meta shortlisting on separate streams. On standard speculative-decoding benchmarks, we observe consistent gains in mean accepted length over fixed-shortlist baselines, while context-dependent selection enables smaller shortlists without degrading acceptance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13847v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinbin Zhang, Nasib Ullah, Erik Schultheis, Rohit Babbar</dc:creator>
    </item>
    <item>
      <title>On-device System of Compositional Multi-tasking in Large Language Models</title>
      <link>https://arxiv.org/abs/2510.13848</link>
      <description>arXiv:2510.13848v1 Announce Type: cross 
Abstract: Large language models (LLMs) are commonly adapted for diverse downstream tasks via parameter-efficient fine-tuning techniques such as Low-Rank Adapters (LoRA). While adapters can be combined to handle multiple tasks separately, standard approaches struggle when targeting the simultaneous execution of complex tasks, such as generating a translated summary from a long conversation. To address this challenge, we propose a novel approach tailored specifically for compositional multi-tasking scenarios involving summarization and translation. Our technique involves adding a learnable projection layer on top of the combined summarization and translation adapters. This design enables effective integration while maintaining efficiency through reduced computational overhead compared to alternative strategies requiring extensive retraining or sequential processing. We demonstrate the practical viability of our method within an on-device environment by developing an Android app capable of executing compositional tasks seamlessly. Experimental results indicate our solution performs well and is fast in both cloud-based and on-device implementations, highlighting the potential benefits of adopting our framework in real-world applications demanding high-speed operation alongside resource constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13848v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ondrej Bohdal, Konstantinos Theodosiadis, Asterios Mpatziakas, Dimitris Filippidis, Iro Spyrou, Christos Zonios, Anastasios Drosou, Dimosthenis Ioannidis, Kyeng-Hun Lee, Jijoong Moon, Hyeonmok Ko, Mete Ozay, Umberto Michieli</dc:creator>
    </item>
    <item>
      <title>Revisiting the UID Hypothesis in LLM Reasoning Traces</title>
      <link>https://arxiv.org/abs/2510.13850</link>
      <description>arXiv:2510.13850v1 Announce Type: cross 
Abstract: Large language models (LLMs) often solve problems using step-by-step Chain-of-Thought (CoT) reasoning, yet these intermediate steps are frequently unfaithful or hard to interpret. Inspired by the Uniform Information Density (UID) hypothesis in psycholinguistics -- which posits that humans communicate by maintaining a stable flow of information -- we introduce entropy-based metrics to analyze the information flow within reasoning traces. Surprisingly, across three challenging mathematical benchmarks, we find that successful reasoning in LLMs is globally non-uniform: correct solutions are characterized by uneven swings in information density, in stark contrast to human communication patterns. This result challenges assumptions about machine reasoning and suggests new directions for designing interpretable and adaptive reasoning models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13850v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Minju Gwak, Guijin Son, Jaehyung Kim</dc:creator>
    </item>
    <item>
      <title>ConsistencyAI: A Benchmark to Assess LLMs' Factual Consistency When Responding to Different Demographic Groups</title>
      <link>https://arxiv.org/abs/2510.13852</link>
      <description>arXiv:2510.13852v1 Announce Type: cross 
Abstract: Is an LLM telling you different facts than it's telling me? This paper introduces ConsistencyAI, an independent benchmark for measuring the factual consistency of large language models (LLMs) for different personas. ConsistencyAI tests whether, when users of different demographics ask identical questions, the model responds with factually inconsistent answers. Designed without involvement from LLM providers, this benchmark offers impartial evaluation and accountability. In our experiment, we queried 19 LLMs with prompts that requested 5 facts for each of 15 topics. We repeated this query 100 times for each LLM, each time adding prompt context from a different persona selected from a subset of personas modeling the general population. We processed the responses into sentence embeddings, computed cross-persona cosine similarity, and computed the weighted average of cross-persona cosine similarity to calculate factual consistency scores. In 100-persona experiments, scores ranged from 0.9065 to 0.7896, and the mean was 0.8656, which we adopt as a benchmark threshold. xAI's Grok-3 is most consistent, while several lightweight models rank lowest. Consistency varies by topic: the job market is least consistent, G7 world leaders most consistent, and issues like vaccines or the Israeli-Palestinian conflict diverge by provider. These results show that both the provider and the topic shape the factual consistency. We release our code and interactive demo to support reproducible evaluation and encourage persona-invariant prompting strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13852v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peter Banyas, Shristi Sharma, Alistair Simmons, Atharva Vispute</dc:creator>
    </item>
    <item>
      <title>BenchPress: A Human-in-the-Loop Annotation System for Rapid Text-to-SQL Benchmark Curation</title>
      <link>https://arxiv.org/abs/2510.13853</link>
      <description>arXiv:2510.13853v1 Announce Type: cross 
Abstract: Large language models (LLMs) have been successfully applied to many tasks, including text-to-SQL generation. However, much of this work has focused on publicly available datasets, such as Fiben, Spider, and Bird. Our earlier work showed that LLMs are much less effective in querying large private enterprise data warehouses and released Beaver, the first private enterprise text-to-SQL benchmark. To create Beaver, we leveraged SQL logs, which are often readily available. However, manually annotating these logs to identify which natural language questions they answer is a daunting task. Asking database administrators, who are highly trained experts, to take on additional work to construct and validate corresponding natural language utterances is not only challenging but also quite costly. To address this challenge, we introduce BenchPress, a human-in-the-loop system designed to accelerate the creation of domain-specific text-to-SQL benchmarks. Given a SQL query, BenchPress uses retrieval-augmented generation (RAG) and LLMs to propose multiple natural language descriptions. Human experts then select, rank, or edit these drafts to ensure accuracy and domain alignment. We evaluated BenchPress on annotated enterprise SQL logs, demonstrating that LLM-assisted annotation drastically reduces the time and effort required to create high-quality benchmarks. Our results show that combining human verification with LLM-generated suggestions enhances annotation accuracy, benchmark reliability, and model evaluation robustness. By streamlining the creation of custom benchmarks, BenchPress offers researchers and practitioners a mechanism for assessing text-to-SQL models on a given domain-specific workload. BenchPress is freely available via our public GitHub repository at https://github.com/fabian-wenz/enterprise-txt2sql and is also accessible on our website at http://dsg-mcgraw.csail.mit.edu:5000.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13853v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>cs.HC</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabian Wenz, Omar Bouattour, Devin Yang, Justin Choi, Cecil Gregg, Nesime Tatbul, \c{C}a\u{g}atay Demiralp</dc:creator>
    </item>
    <item>
      <title>Harnessing Consistency for Robust Test-Time LLM Ensemble</title>
      <link>https://arxiv.org/abs/2510.13855</link>
      <description>arXiv:2510.13855v1 Announce Type: cross 
Abstract: Different large language models (LLMs) exhibit diverse strengths and weaknesses, and LLM ensemble serves as a promising approach to integrate their complementary capabilities. Despite substantial progress in improving ensemble quality, limited attention has been paid to the robustness of ensembles against potential erroneous signals, which often arise from heterogeneous tokenization schemes and varying model expertise. Our analysis shows that ensemble failures typically arise from both the token level and the model level: the former reflects severe disagreement in token predictions, while the latter involves low confidence and pronounced disparities among models. In light of this, we propose CoRE, a plug-and-play technique that harnesses model consistency for robust LLM ensemble, which can be seamlessly integrated with diverse ensemble methods. Token-level consistency captures fine-grained disagreements by applying a low-pass filter to downweight uncertain tokens with high inconsistency, often due to token misalignment, thereby improving robustness at a granular level. Model-level consistency models global agreement by promoting model outputs with high self-confidence and minimal divergence from others, enhancing robustness at a coarser level. Extensive experiments across diverse benchmarks, model combinations, and ensemble strategies demonstrate that CoRE consistently improves ensemble performance and robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13855v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhichen Zeng, Qi Yu, Xiao Lin, Ruizhong Qiu, Xuying Ning, Tianxin Wei, Yuchen Yan, Jingrui He, Hanghang Tong</dc:creator>
    </item>
    <item>
      <title>Multimodal Retrieval-Augmented Generation with Large Language Models for Medical VQA</title>
      <link>https://arxiv.org/abs/2510.13856</link>
      <description>arXiv:2510.13856v1 Announce Type: cross 
Abstract: Medical Visual Question Answering (MedVQA) enables natural language queries over medical images to support clinical decision-making and patient care. The MEDIQA-WV 2025 shared task addressed wound-care VQA, requiring systems to generate free-text responses and structured wound attributes from images and patient queries. We present the MasonNLP system, which employs a general-domain, instruction-tuned large language model with a retrieval-augmented generation (RAG) framework that incorporates textual and visual examples from in-domain data. This approach grounds outputs in clinically relevant exemplars, improving reasoning, schema adherence, and response quality across dBLEU, ROUGE, BERTScore, and LLM-based metrics. Our best-performing system ranked 3rd among 19 teams and 51 submissions with an average score of 41.37%, demonstrating that lightweight RAG with general-purpose LLMs -- a minimal inference-time layer that adds a few relevant exemplars via simple indexing and fusion, with no extra training or complex re-ranking -- provides a simple and effective baseline for multimodal clinical NLP tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13856v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A H M Rezaul Karim, Ozlem Uzuner</dc:creator>
    </item>
    <item>
      <title>From Craft to Constitution: A Governance-First Paradigm for Principled Agent Engineering</title>
      <link>https://arxiv.org/abs/2510.13857</link>
      <description>arXiv:2510.13857v1 Announce Type: cross 
Abstract: The advent of powerful Large Language Models (LLMs) has ushered in an ``Age of the Agent,'' enabling autonomous systems to tackle complex goals. However, the transition from prototype to production is hindered by a pervasive ``crisis of craft,'' resulting in agents that are brittle, unpredictable, and ultimately untrustworthy in mission-critical applications. This paper argues this crisis stems from a fundamental paradigm mismatch -- attempting to command inherently probabilistic processors with the deterministic mental models of traditional software engineering. To solve this crisis, we introduce a governance-first paradigm for principled agent engineering, embodied in a formal architecture we call ArbiterOS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13857v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiang Xu, Xiangyu Wen, Changran Xu, Zeju Li, Jianyuan Zhong</dc:creator>
    </item>
    <item>
      <title>Benchmarking Correctness and Security in Multi-Turn Code Generation</title>
      <link>https://arxiv.org/abs/2510.13859</link>
      <description>arXiv:2510.13859v1 Announce Type: cross 
Abstract: AI coding assistants powered by large language models (LLMs) have transformed software development, significantly boosting productivity. While existing benchmarks evaluate the correctness and security of LLM-generated code, they are typically limited to single-turn tasks that do not reflect the iterative nature of real-world development. We introduce MT-Sec, the first benchmark to systematically evaluate both correctness and security in multi-turn coding scenarios. We construct this using a synthetic data pipeline that transforms existing single-turn tasks into semantically aligned multi-turn interaction sequences, allowing reuse of original test suites while modeling the complexity of real-world coding processes. We evaluate 32 open- and closed-source models, and three agent-scaffolding on MT-Sec and observe a consistent 20-27% drop in "correct and secure" outputs from single-turn to multi-turn settings -- even among state-of-the-art models. Beyond full-program generation, we also evaluate models on multi-turn code-diff generation -- an unexplored yet practically relevant setting -- and find that models perform worse here, with increased rates of functionally incorrect and insecure outputs. Finally, we find that while agent scaffoldings boost single-turn code generation performance, they are not quite as effective in multi-turn evaluations. Together, these findings highlight the need for benchmarks that jointly evaluate correctness and security in multi-turn, real-world coding workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13859v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ruchit Rawal, Jeffrey Yang Fan Chiang, Chihao Shen, Jeffery Siyuan Tian, Aastha Mahajan, Tom Goldstein, Yizheng Chen</dc:creator>
    </item>
    <item>
      <title>ShishuLM: Lightweight Language Model with Hybrid Decoder-MLP Architecture and Paired Weight Sharing</title>
      <link>https://arxiv.org/abs/2510.13860</link>
      <description>arXiv:2510.13860v1 Announce Type: cross 
Abstract: While the transformer architecture has achieved state-of-the-art performance on natural language processing tasks, these models impose substantial memory and computational overhead. Recent research has identified significant architectural redundancies within these models, presenting opportunities for optimization without compromising performance. Taking insights from research in AI interpretability and inference-time layer pruning, we introduce an efficient language model architecture, referred to as ShishuLM, which reduces both the parameter count and Key-Value (KV) cache requirements. Given the increasing importance of Small Language Models (SLMs) in agentic AI systems, we evaluate our approach on two SLMs of different scales. Our analysis reveals that for moderate-context scenarios, normalization coupled with attention computation is roughly linear with the input, enabling entire transformer blocks to be approximated through Multi-Layer Perceptrons (MLPs). Our results show that ShishuLM provides up to 25% reduction in memory requirements and up to 40% improvement in latency during both training and inference, compared to parent models. Our experimental and analytical findings provide insights towards building more efficient SLM architectures from a pre-training standpoint.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13860v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shivanshu Kumar, Gopalakrishnan Srinivasan</dc:creator>
    </item>
    <item>
      <title>Ensembling Large Language Models to Characterize Affective Dynamics in Student-AI Tutor Dialogues</title>
      <link>https://arxiv.org/abs/2510.13862</link>
      <description>arXiv:2510.13862v1 Announce Type: cross 
Abstract: While recent studies have examined the leaning impact of large language model (LLM) in educational contexts, the affective dynamics of LLM-mediated tutoring remain insufficiently understood. This work introduces the first ensemble-LLM framework for large-scale affect sensing in tutoring dialogues, advancing the conversation on responsible pathways for integrating generative AI into education by attending to learners' evolving affective states. To achieve this, we analyzed two semesters' worth of 16,986 conversational turns exchanged between PyTutor, an LLM-powered AI tutor, and 261 undergraduate learners across three U.S. institutions. To investigate learners' emotional experiences, we generate zero-shot affect annotations from three frontier LLMs (Gemini, GPT-4o, Claude), including scalar ratings of valence, arousal, and learning-helpfulness, along with free-text emotion labels. These estimates are fused through rank-weighted intra-model pooling and plurality consensus across models to produce robust emotion profiles. Our analysis shows that during interaction with the AI tutor, students typically report mildly positive affect and moderate arousal. Yet learning is not uniformly smooth: confusion and curiosity are frequent companions to problem solving, and frustration, while less common, still surfaces in ways that can derail progress. Emotional states are short-lived--positive moments last slightly longer than neutral or negative ones, but they are fragile and easily disrupted. Encouragingly, negative emotions often resolve quickly, sometimes rebounding directly into positive states. Neutral moments frequently act as turning points, more often steering students upward than downward, suggesting opportunities for tutors to intervene at precisely these junctures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13862v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the 11th International Conference on Affective Computing and Intelligent Interaction (ACII 2025), Late-Breaking Results Track, IEEE, 2025</arxiv:journal_reference>
      <dc:creator>Chenyu Zhang, Sharifa Alghowinem, Cynthia Breazeal</dc:creator>
    </item>
    <item>
      <title>Self-Training with Dynamic Weighting for Robust Gradual Domain Adaptation</title>
      <link>https://arxiv.org/abs/2510.13864</link>
      <description>arXiv:2510.13864v1 Announce Type: cross 
Abstract: In this paper, we propose a new method called Self-Training with Dynamic Weighting (STDW), which aims to enhance robustness in Gradual Domain Adaptation (GDA) by addressing the challenge of smooth knowledge migration from the source to the target domain. Traditional GDA methods mitigate domain shift through intermediate domains and self-training but often suffer from inefficient knowledge migration or incomplete intermediate data. Our approach introduces a dynamic weighting mechanism that adaptively balances the loss contributions of the source and target domains during training. Specifically, we design an optimization framework governed by a time-varying hyperparameter $\varrho$ (progressing from 0 to 1), which controls the strength of domain-specific learning and ensures stable adaptation. The method leverages self-training to generate pseudo-labels and optimizes a weighted objective function for iterative model updates, maintaining robustness across intermediate domains. Experiments on rotated MNIST, color-shifted MNIST, portrait datasets, and the Cover Type dataset demonstrate that STDW outperforms existing baselines. Ablation studies further validate the critical role of $\varrho$'s dynamic scheduling in achieving progressive adaptation, confirming its effectiveness in reducing domain bias and improving generalization. This work provides both theoretical insights and a practical framework for robust gradual domain adaptation, with potential applications in dynamic real-world scenarios. The code is available at https://github.com/Dramwig/STDW.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13864v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zixi Wang, Yushe Cao, Yubo Huang, Jinzhu Wei, Jingzehua Xu, Shuai Zhang, Xin Lai</dc:creator>
    </item>
    <item>
      <title>Deep Edge Filter: Return of the Human-Crafted Layer in Deep Learning</title>
      <link>https://arxiv.org/abs/2510.13865</link>
      <description>arXiv:2510.13865v1 Announce Type: cross 
Abstract: We introduce the Deep Edge Filter, a novel approach that applies high-pass filtering to deep neural network features to improve model generalizability. Our method is motivated by our hypothesis that neural networks encode task-relevant semantic information in high-frequency components while storing domain-specific biases in low-frequency components of deep features. By subtracting low-pass filtered outputs from original features, our approach isolates generalizable representations while preserving architectural integrity. Experimental results across diverse domains such as Vision, Text, 3D, and Audio demonstrate consistent performance improvements regardless of model architecture and data modality. Analysis reveals that our method induces feature sparsification and effectively isolates high-frequency components, providing empirical validation of our core hypothesis. The code is available at https://github.com/dongkwani/DeepEdgeFilter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13865v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Dongkwan Lee, Junhoo Lee, Nojun Kwak</dc:creator>
    </item>
    <item>
      <title>FFT-Accelerated Auxiliary Variable MCMC for Fermionic Lattice Models: A Determinant-Free Approach with $O(N\log N)$ Complexity</title>
      <link>https://arxiv.org/abs/2510.13866</link>
      <description>arXiv:2510.13866v1 Announce Type: cross 
Abstract: We introduce a Markov Chain Monte Carlo (MCMC) algorithm that dramatically accelerates the simulation of quantum many-body systems, a grand challenge in computational science. State-of-the-art methods for these problems are severely limited by $O(N^3)$ computational complexity. Our method avoids this bottleneck, achieving near-linear $O(N \log N)$ scaling per sweep.
  Our approach samples a joint probability measure over two coupled variable sets: (1) particle trajectories of the fundamental fermions, and (2) auxiliary variables that decouple fermion interactions. The key innovation is a novel transition kernel for particle trajectories formulated in the Fourier domain, revealing the transition probability as a convolution that enables massive acceleration via the Fast Fourier Transform (FFT). The auxiliary variables admit closed-form, factorized conditional distributions, enabling efficient exact Gibbs sampling update.
  We validate our algorithm on benchmark quantum physics problems, accurately reproducing known theoretical results and matching traditional $O(N^3)$ algorithms on $32\times 32$ lattice simulations at a fraction of the wall-clock time, empirically demonstrating $N \log N$ scaling. By reformulating a long-standing physics simulation problem in machine learning language, our work provides a powerful tool for large-scale probabilistic inference and opens avenues for physics-inspired generative models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13866v1</guid>
      <category>cond-mat.str-el</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Deqian Kong, Shi Feng, Jianwen Xie, Ying Nian Wu</dc:creator>
    </item>
    <item>
      <title>CoLoR-GAN: Continual Few-Shot Learning with Low-Rank Adaptation in Generative Adversarial Networks</title>
      <link>https://arxiv.org/abs/2510.13869</link>
      <description>arXiv:2510.13869v1 Announce Type: cross 
Abstract: Continual learning (CL) in the context of Generative Adversarial Networks (GANs) remains a challenging problem, particularly when it comes to learn from a few-shot (FS) samples without catastrophic forgetting. Current most effective state-of-the-art (SOTA) methods, like LFS-GAN, introduce a non-negligible quantity of new weights at each training iteration, which would become significant when considering the long term. For this reason, this paper introduces \textcolor{red}{\textbf{\underline{c}}}ontinual few-sh\textcolor{red}{\textbf{\underline{o}}}t learning with \textcolor{red}{\textbf{\underline{lo}}}w-\textcolor{red}{\textbf{\underline{r}}}ank adaptation in GANs named CoLoR-GAN, a framework designed to handle both FS and CL together, leveraging low-rank tensors to efficiently adapt the model to target tasks while reducing even more the number of parameters required. Applying a vanilla LoRA implementation already permitted us to obtain pretty good results. In order to optimize even further the size of the adapters, we challenged LoRA limits introducing a LoRA in LoRA (LLoRA) technique for convolutional layers. Finally, aware of the criticality linked to the choice of the hyperparameters of LoRA, we provide an empirical study to easily find the best ones. We demonstrate the effectiveness of CoLoR-GAN through experiments on several benchmark CL and FS tasks and show that our model is efficient, reaching SOTA performance but with a number of resources enormously reduced. Source code is available on \href{https://github.com/munsifali11/CoLoR-GAN}{Github.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13869v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Munsif Ali, Leonardo Rossi, Massimo Bertozzi</dc:creator>
    </item>
    <item>
      <title>Unlocking the Potential of Diffusion Language Models through Template Infilling</title>
      <link>https://arxiv.org/abs/2510.13870</link>
      <description>arXiv:2510.13870v1 Announce Type: cross 
Abstract: Diffusion Language Models (DLMs) have emerged as a promising alternative to Autoregressive Language Models, yet their inference strategies remain limited to prefix-based prompting inherited from the autoregressive paradigm. In this paper, we propose Template Infilling (TI), a tailored conditioning methodology for DLMs' generation process. Unlike conventional prefix prompting, TI first generates a structural template for the target response, then fills in the masked segments. To enhance the flexibility of this structural control, we introduce Dynamic Segment Allocation (DSA), which adaptively adjusts segment lengths based on generation confidence. We demonstrate the effectiveness of our approach on mathematical reasoning and code generation benchmarks, achieving consistent improvements of 17.01$\%$p over baseline. Furthermore, we show that TI provides additional advantages in multi-token generation settings, enabling effective speedup while maintaining generation quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13870v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junhoo Lee, Seungyeon Kim, Nojun Kwak</dc:creator>
    </item>
    <item>
      <title>Joint Discriminative-Generative Modeling via Dual Adversarial Training</title>
      <link>https://arxiv.org/abs/2510.13872</link>
      <description>arXiv:2510.13872v1 Announce Type: cross 
Abstract: Simultaneously achieving robust classification and high-fidelity generative modeling within a single framework presents a significant challenge. Hybrid approaches, such as Joint Energy-Based Models (JEM), interpret classifiers as EBMs but are often limited by the instability and poor sample quality inherent in SGLD-based training. We address these limitations by proposing a novel training framework that integrates adversarial training (AT) principles for both discriminative robustness and stable generative learning. The proposed method introduces three key innovations: (1) the replacement of SGLD-based JEM learning with a stable, AT-based approach that optimizes the energy function by discriminating between real data and PGD-generated contrastive samples using the BCE loss; (2) synergistic adversarial training for the discriminative component that enhances classification robustness while eliminating the need for explicit gradient penalties; and (3) a two-stage training procedure to resolve the incompatibility between batch normalization and EBM training. Experiments on CIFAR-10, CIFAR-100, and ImageNet demonstrate that our method substantially improves adversarial robustness over existing hybrid models while maintaining competitive generative performance. On ImageNet, when optimized for generative modeling, our model's generative fidelity surpasses that of BigGAN and approaches diffusion models, representing the first MCMC-based EBM approach to achieve high-quality generation on complex, high-resolution datasets. Our approach addresses key stability issues that have limited JEM scaling and demonstrates that adversarial training can serve as an effective foundation for unified frameworks capable of generating and robustly classifying visual data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13872v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xuwang Yin, Claire Zhang, Julie Steele, Nir Shavit, Tony T. Wang</dc:creator>
    </item>
    <item>
      <title>FRACCO: A gold-standard annotated corpus of oncological entities with ICD-O-3.1 normalisation</title>
      <link>https://arxiv.org/abs/2510.13873</link>
      <description>arXiv:2510.13873v1 Announce Type: cross 
Abstract: Developing natural language processing tools for clinical text requires annotated datasets, yet French oncology resources remain scarce. We present FRACCO (FRench Annotated Corpus for Clinical Oncology) an expert-annotated corpus of 1301 synthetic French clinical cases, initially translated from the Spanish CANTEMIST corpus as part of the FRASIMED initiative. Each document is annotated with terms related to morphology, topography, and histologic differentiation, using the International Classification of Diseases for Oncology (ICD-O) as reference. An additional annotation layer captures composite expression-level normalisations that combine multiple ICD-O elements into unified clinical concepts. Annotation quality was ensured through expert review: 1301 texts were manually annotated for entity spans by two domain experts. A total of 71127 ICD-O normalisations were produced through a combination of automated matching and manual validation by a team of five annotators. The final dataset representing 399 unique morphology codes (from 2549 different expressions), 272 topography codes (from 3143 different expressions), and 2043 unique composite expressions (from 11144 different expressions). This dataset provides a reference standard for named entity recognition and concept normalisation in French oncology texts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13873v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Johann Pignat, Milena Vucetic, Christophe Gaudet-Blavignac, Jamil Zaghir, Amandine Stettler, Fanny Amrein, Jonatan Bonjour, Jean-Philippe Goldman, Olivier Michielin, Christian Lovis, Mina Bjelogrlic</dc:creator>
    </item>
    <item>
      <title>What Layers When: Learning to Skip Compute in LLMs with Residual Gates</title>
      <link>https://arxiv.org/abs/2510.13876</link>
      <description>arXiv:2510.13876v1 Announce Type: cross 
Abstract: We introduce GateSkip, a simple residual-stream gating mechanism that enables token-wise layer skipping in decoder-only LMs. Each Attention/MLP branch is equipped with a sigmoid-linear gate that condenses the branch's output before it re-enters the residual stream. During inference we rank tokens by the gate values and skip low-importance ones using a per-layer budget. While early-exit or router-based Mixture-of-Depths models are known to be unstable and need extensive retraining, our smooth, differentiable gates fine-tune stably on top of pretrained models. On long-form reasoning, we save up to 15\% compute while retaining over 90\% of baseline accuracy. On instruction-tuned models we see accuracy gains at full compute and match baseline quality near 50\% savings. The learned gates give insight into transformer information flow (e.g., BOS tokens act as anchors), and the method combines easily with quantization, pruning, and self-speculative decoding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13876v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Filipe Laitenberger, Dawid Kopiczko, Cees G. M. Snoek, Yuki M. Asano</dc:creator>
    </item>
    <item>
      <title>Catch Your Breath: Adaptive Computation for Self-Paced Sequence Production</title>
      <link>https://arxiv.org/abs/2510.13879</link>
      <description>arXiv:2510.13879v1 Announce Type: cross 
Abstract: We explore a class of supervised training objectives that allow a language model to dynamically and autonomously scale the number of compute steps used for each input token. For any token, the model can request additional compute steps by emitting a &lt;don't know&gt; output. If the model is granted a delay, a specialized &lt;pause&gt; token is inserted at the next input step, providing the model with additional compute resources to generate an output. The model can request multiple pauses. To train the model to use &lt;don't know&gt; outputs judiciously and to calibrate its uncertainty, we frame the selection of each output token as a sequential-decision problem with a time cost. We refer to the class of methods as $\textit{Catch Your Breath}$ losses and we study three methods in this class: CYB-AP frames the model's task as anytime prediction, where an output may be required at any step and accuracy is discounted over time; CYB-VA is a variational approach that aims to maximize prediction accuracy subject to a specified distribution over stopping times; and CYB-DP imposes a penalty based on a computational budget. Through fine-tuning experiments, we identify the best performing loss variant. The CYB model needs only one third as much training data as the baseline (no pause) model needs to achieve the same performance, and half as much data as a model with pauses and a cross-entropy loss. We find that the CYB model requests additional steps when doing so improves accuracy, and the model adapts its processing time to token-level complexity and context. For example, it often pauses after plural nouns like $\textit{patients}$ and $\textit{challenges}$ but never pauses after the first token of contracted words like $\textit{wasn}$ and $\textit{didn}$, and it shows high variability for ambiguous tokens like $\textit{won}$, which could function as either a verb or part of a contraction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13879v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandre Galashov, Matt Jones, Rosemary Ke, Yuan Cao, Vaishnavh Nagarajan, Michael C. Mozer</dc:creator>
    </item>
    <item>
      <title>PAGE: Prompt Augmentation for text Generation Enhancement</title>
      <link>https://arxiv.org/abs/2510.13880</link>
      <description>arXiv:2510.13880v1 Announce Type: cross 
Abstract: In recent years, natural language generative models have shown outstanding performance in text generation tasks. However, when facing specific tasks or particular requirements, they may exhibit poor performance or require adjustments that demand large amounts of additional data. This work introduces PAGE (Prompt Augmentation for text Generation Enhancement), a framework designed to assist these models through the use of simple auxiliary modules. These modules, lightweight models such as classifiers or extractors, provide inferences from the input text. The output of these auxiliaries is then used to construct an enriched input that improves the quality and controllability of the generation. Unlike other generation-assistance approaches, PAGE does not require auxiliary generative models; instead, it proposes a simpler, modular architecture that is easy to adapt to different tasks. This paper presents the proposal, its components and architecture, and reports a proof of concept in the domain of requirements engineering, where an auxiliary module with a classifier is used to improve the quality of software requirements generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13880v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mauro Jose Pacchiotti, Luciana Ballejos, Mariel Ale</dc:creator>
    </item>
    <item>
      <title>Order from Chaos: Comparative Study of Ten Leading LLMs on Unstructured Data Categorization</title>
      <link>https://arxiv.org/abs/2510.13885</link>
      <description>arXiv:2510.13885v1 Announce Type: cross 
Abstract: This study presents a comparative evaluation of ten state-of-the-art large language models (LLMs) applied to unstructured text categorization using the Interactive Advertising Bureau (IAB) 2.2 hierarchical taxonomy. The analysis employed a uniform dataset of 8,660 human-annotated samples and identical zero-shot prompts to ensure methodological consistency across all models. Evaluation metrics included four classic measures - accuracy, precision, recall, and F1-score - and three LLM-specific indicators: hallucination ratio, inflation ratio, and categorization cost.
  Results show that, despite their rapid advancement, contemporary LLMs achieve only moderate classic performance, with average scores of 34% accuracy, 42% precision, 45% recall, and 41% F1-score. Hallucination and inflation ratios reveal that models frequently overproduce categories relative to human annotators. Among the evaluated systems, Gemini 1.5/2.0 Flash and GPT 20B/120B offered the most favorable cost-to-performance balance, while GPT 120B demonstrated the lowest hallucination ratio. The findings suggest that scaling and architectural improvements alone do not ensure better categorization accuracy, as the task requires compressing rich unstructured text into a limited taxonomy - a process that challenges current model architectures.
  To address these limitations, a separate ensemble-based approach was developed and tested. The ensemble method, in which multiple LLMs act as independent experts, substantially improved accuracy, reduced inflation, and completely eliminated hallucinations. These results indicate that coordinated orchestration of models - rather than sheer scale - may represent the most effective path toward achieving or surpassing human-expert performance in large-scale text categorization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13885v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ariel Kamen</dc:creator>
    </item>
    <item>
      <title>Physics-Informed autoencoder for DSC-MRI Perfusion post-processing: application to glioma grading</title>
      <link>https://arxiv.org/abs/2510.13886</link>
      <description>arXiv:2510.13886v1 Announce Type: cross 
Abstract: DSC-MRI perfusion is a medical imaging technique for diagnosing and prognosing brain tumors and strokes. Its analysis relies on mathematical deconvolution, but noise or motion artifacts in a clinical environment can disrupt this process, leading to incorrect estimate of perfusion parameters. Although deep learning approaches have shown promising results, their calibration typically rely on third-party deconvolution algorithms to generate reference outputs and are bound to reproduce their limitations.
  To adress this problem, we propose a physics-informed autoencoder that leverages an analytical model to decode the perfusion parameters and guide the learning of the encoding network. This autoencoder is trained in a self-supervised fashion without any third-party software and its performance is evaluated on a database with glioma patients. Our method shows reliable results for glioma grading in accordance with other well-known deconvolution algorithms despite a lower computation time. It also achieved competitive performance even in the presence of high noise which is critical in a medical environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13886v1</guid>
      <category>q-bio.QM</category>
      <category>cs.AI</category>
      <category>eess.IV</category>
      <category>eess.SP</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pierre Fayolle, Alexandre B\^one, No\"elie Debs, Mathieu Naudin, Pascal Bourdon, Remy Guillevin, David Helbert</dc:creator>
    </item>
    <item>
      <title>Incomplete Multi-view Clustering via Hierarchical Semantic Alignment and Cooperative Completion</title>
      <link>https://arxiv.org/abs/2510.13887</link>
      <description>arXiv:2510.13887v1 Announce Type: cross 
Abstract: Incomplete multi-view data, where certain views are entirely missing for some samples, poses significant challenges for traditional multi-view clustering methods. Existing deep incomplete multi-view clustering approaches often rely on static fusion strategies or two-stage pipelines, leading to suboptimal fusion results and error propagation issues. To address these limitations, this paper proposes a novel incomplete multi-view clustering framework based on Hierarchical Semantic Alignment and Cooperative Completion (HSACC). HSACC achieves robust cross-view fusion through a dual-level semantic space design. In the low-level semantic space, consistency alignment is ensured by maximizing mutual information across views. In the high-level semantic space, adaptive view weights are dynamically assigned based on the distributional affinity between individual views and an initial fused representation, followed by weighted fusion to generate a unified global representation. Additionally, HSACC implicitly recovers missing views by projecting aligned latent representations into high-dimensional semantic spaces and jointly optimizes reconstruction and clustering objectives, enabling cooperative learning of completion and clustering. Experimental results demonstrate that HSACC significantly outperforms state-of-the-art methods on five benchmark datasets. Ablation studies validate the effectiveness of the hierarchical alignment and dynamic weighting mechanisms, while parameter analysis confirms the model's robustness to hyperparameter variations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13887v1</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaojian Ding, Lin Zhao, Xian Li, Xiaoying Zhu</dc:creator>
    </item>
    <item>
      <title>Reliable Fine-Grained Evaluation of Natural Language Math Proofs</title>
      <link>https://arxiv.org/abs/2510.13888</link>
      <description>arXiv:2510.13888v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) for mathematical reasoning have largely focused on tasks with easily verifiable final answers; however, generating and verifying natural language math proofs remains an open challenge. We identify the absence of a reliable, fine-grained evaluator for LLM-generated math proofs as a critical gap. To address this, we propose a systematic methodology for developing and validating evaluators that assign fine-grained scores on a 0-7 scale to model-generated math proofs. To enable this study, we introduce ProofBench, the first expert-annotated dataset of fine-grained proof ratings, spanning 145 problems from six major math competitions (USAMO, IMO, Putnam, etc) and 435 LLM-generated solutions from Gemini-2.5-pro, o3, and DeepSeek-R1. %with expert gradings. Using ProofBench as a testbed, we systematically explore the evaluator design space across key axes: the backbone model, input context, instructions and evaluation workflow. Our analysis delivers ProofGrader, an evaluator that combines a strong reasoning backbone LM, rich context from reference solutions and marking schemes, and a simple ensembling method; it achieves a low Mean Absolute Error (MAE) of 0.926 against expert scores, significantly outperforming naive baselines. Finally, we demonstrate its practical utility in a best-of-$n$ selection task: at $n=16$, ProofGrader achieves an average score of 4.14 (out of 7), closing 78% of the gap between a naive binary evaluator (2.48) and the human oracle (4.62), highlighting its potential to advance downstream proof generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13888v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenjie Ma, Andrei Cojocaru, Neel Kolhe, Bradley Louie, Robin Said Sharif, Haihan Zhang, Vincent Zhuang, Matei Zaharia, Sewon Min</dc:creator>
    </item>
    <item>
      <title>A Survey on Collaborating Small and Large Language Models for Performance, Cost-effectiveness, Cloud-edge Privacy, and Trustworthiness</title>
      <link>https://arxiv.org/abs/2510.13890</link>
      <description>arXiv:2510.13890v1 Announce Type: cross 
Abstract: Large language models (LLMs) have advanced many domains and applications but face high fine-tuning costs, inference latency, limited edge deployability, and reliability concerns. Small language models (SLMs), compact, efficient, and adaptable, offer complementary remedies. Recent work explores collaborative frameworks that fuse SLMs' specialization and efficiency with LLMs' generalization and reasoning to meet diverse objectives across tasks and deployment scenarios. Motivated by these developments, this paper presents a systematic survey of SLM-LLM collaboration organized by collaboration objectives. We propose a taxonomy with four goals: performance enhancement, cost-effectiveness, cloud-edge privacy, and trustworthiness. Within this framework, we review representative methods, summarize design paradigms, and outline open challenges and future directions toward efficient, secure, and scalable SLM-LLM collaboration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13890v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fali Wang, Jihai Chen, Shuhua Yang, Ali Al-Lawati, Linli Tang, Hui Liu, Suhang Wang</dc:creator>
    </item>
    <item>
      <title>K-frames: Scene-Driven Any-k Keyframe Selection for long video understanding</title>
      <link>https://arxiv.org/abs/2510.13891</link>
      <description>arXiv:2510.13891v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant capabilities in image understanding, but long-video are constrained by context windows and computational cost. Uniform frame sampling often leads to substantial information loss. Meanwhile existing keyframe selection methods such as text-frame retrieval or RL-based frame optimization typically yield sparse and temporally disjointed frames, overlooking scene continuity and lacking flexibility for multi-scale frame selection. To address these limitations, we introduce K-frames, a novel paradigm for scene-driven keyframe selection that preserves temporal continuity. Instead of selecting individual frames, K-frames predicts semantically coherent, query-relevant clips, which enables any-k keyframes selection to meet diverse user budgets. To achieve this approach, we first introduce PeakClips, a dataset of 200K video highlights conditioned by query. Building on this dataset, K-frames learns clip2frame selection using a three-stage progressive curriculum. It involves two Supervised Fine-Tuning stages for temporal grounding and key-clip perception, followed by a Reinforcement Learning stage that directly optimizes the scene-driven prediction policy for downstream task without further annotations. Extensive experiments on major long-video understanding benchmarks demonstrate that K-frames provides an effective, interpretable, and plug-and-play solution for keyframe selection at various scales. Our dataset and model will be available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13891v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yifeng Yao, Yike Yun, Jing Wang, Huishuai Zhang, Dongyan Zhao, Ke Tian, Zhihao Wang, Minghui Qiu, Tao Wang</dc:creator>
    </item>
    <item>
      <title>Guarding the Guardrails: A Taxonomy-Driven Approach to Jailbreak Detection</title>
      <link>https://arxiv.org/abs/2510.13893</link>
      <description>arXiv:2510.13893v1 Announce Type: cross 
Abstract: Jailbreaking techniques pose a significant threat to the safety of Large Language Models (LLMs). Existing defenses typically focus on single-turn attacks, lack coverage across languages, and rely on limited taxonomies that either fail to capture the full diversity of attack strategies or emphasize risk categories rather than the jailbreaking techniques. To advance the understanding of the effectiveness of jailbreaking techniques, we conducted a structured red-teaming challenge. The outcome of our experiments are manifold. First, we developed a comprehensive hierarchical taxonomy of 50 jailbreak strategies, consolidating and extending prior classifications into seven broad families, including impersonation, persuasion, privilege escalation, cognitive overload, obfuscation, goal conflict, and data poisoning. Second, we analyzed the data collected from the challenge to examine the prevalence and success rates of different attack types, providing insights into how specific jailbreak strategies exploit model vulnerabilities and induce misalignment. Third, we benchmark a popular LLM for jailbreak detection, evaluating the benefits of taxonomy-guided prompting for improving automatic detection. Finally, we compiled a new Italian dataset of 1364 multi-turn adversarial dialogues, annotated with our taxonomy, enabling the study of interactions where adversarial intent emerges gradually and succeeds in bypassing traditional safeguards.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13893v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Olga E. Sorokoletova, Francesco Giarrusso, Vincenzo Suriani, Daniele Nardi</dc:creator>
    </item>
    <item>
      <title>Bayes or Heisenberg: Who(se) Rules?</title>
      <link>https://arxiv.org/abs/2510.13894</link>
      <description>arXiv:2510.13894v1 Announce Type: cross 
Abstract: Although quantum systems are generally described by quantum state vectors, we show that in certain cases their measurement processes can be reformulated as probabilistic equations expressed in terms of probabilistic state vectors. These probabilistic representations can, in turn, be approximated by the neural network dynamics of the Tensor Brain (TB) model.
  The Tensor Brain is a recently proposed framework for modeling perception and memory in the brain, providing a biologically inspired mechanism for efficiently integrating generated symbolic representations into reasoning processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13894v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>quant-ph</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Volker Tresp Hang Li, Federico Harjes, Yunpu Ma</dc:creator>
    </item>
    <item>
      <title>GenCellAgent: Generalizable, Training-Free Cellular Image Segmentation via Large Language Model Agents</title>
      <link>https://arxiv.org/abs/2510.13896</link>
      <description>arXiv:2510.13896v1 Announce Type: cross 
Abstract: Cellular image segmentation is essential for quantitative biology yet remains difficult due to heterogeneous modalities, morphological variability, and limited annotations. We present GenCellAgent, a training-free multi-agent framework that orchestrates specialist segmenters and generalist vision-language models via a planner-executor-evaluator loop (choose tool $\rightarrow$ run $\rightarrow$ quality-check) with long-term memory. The system (i) automatically routes images to the best tool, (ii) adapts on the fly using a few reference images when imaging conditions differ from what a tool expects, (iii) supports text-guided segmentation of organelles not covered by existing models, and (iv) commits expert edits to memory, enabling self-evolution and personalized workflows. Across four cell-segmentation benchmarks, this routing yields a 15.7\% mean accuracy gain over state-of-the-art baselines. On endoplasmic reticulum and mitochondria from new datasets, GenCellAgent improves average IoU by 37.6\% over specialist models. It also segments novel objects such as the Golgi apparatus via iterative text-guided refinement, with light human correction further boosting performance. Together, these capabilities provide a practical path to robust, adaptable cellular image segmentation without retraining, while reducing annotation burden and matching user preferences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13896v1</guid>
      <category>q-bio.QM</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.MA</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xi Yu, Yang Yang, Qun Liu, Yonghua Du, Sean McSweeney, Yuewei Lin</dc:creator>
    </item>
    <item>
      <title>Dual-attention ResNet outperforms transformers in HER2 prediction on DCE-MRI</title>
      <link>https://arxiv.org/abs/2510.13897</link>
      <description>arXiv:2510.13897v1 Announce Type: cross 
Abstract: Breast cancer is the most diagnosed cancer in women, with HER2 status critically guiding treatment decisions. Noninvasive prediction of HER2 status from dynamic contrast-enhanced MRI (DCE-MRI) could streamline diagnostics and reduce reliance on biopsy. However, preprocessing high-dynamic-range DCE-MRI into standardized 8-bit RGB format for pretrained neural networks is nontrivial, and normalization strategy significantly affects model performance. We benchmarked intensity normalization strategies using a Triple-Head Dual-Attention ResNet that processes RGB-fused temporal sequences from three DCE phases. Trained on a multicenter cohort (n=1,149) from the I-SPY trials and externally validated on BreastDCEDL_AMBL (n=43 lesions), our model outperformed transformer-based architectures, achieving 0.75 accuracy and 0.74 AUC on I-SPY test data. N4 bias field correction slightly degraded performance. Without fine-tuning, external validation yielded 0.66 AUC, demonstrating cross-institutional generalizability. These findings highlight the effectiveness of dual-attention mechanisms in capturing transferable spatiotemporal features for HER2 stratification, advancing reproducible deep learning biomarkers in breast cancer imaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13897v1</guid>
      <category>q-bio.QM</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Naomi Fridman, Anat Goldstein</dc:creator>
    </item>
    <item>
      <title>Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences</title>
      <link>https://arxiv.org/abs/2510.13900</link>
      <description>arXiv:2510.13900v1 Announce Type: cross 
Abstract: Finetuning on narrow domains has become an essential tool to adapt Large Language Models (LLMs) to specific tasks and to create models with known unusual properties that are useful for research. We show that narrow finetuning creates strong biases in LLM activations that can be interpreted to understand the finetuning domain. These biases can be discovered using simple tools from model diffing - the study of differences between models before and after finetuning. In particular, analyzing activation differences on the first few tokens of random text and steering by adding this difference to the model activations produces text similar to the format and general content of the finetuning data. We demonstrate that these analyses contain crucial information by creating an LLM-based interpretability agent to understand the finetuning domain. With access to the bias, the agent performs significantly better compared to baseline agents using simple prompting. Our analysis spans synthetic document finetuning for false facts, emergent misalignment, subliminal learning, and taboo word guessing game models across different architectures (Gemma, LLaMA, Qwen) and scales (1B to 32B parameters). We suspect these biases reflect overfitting and find that mixing pretraining data into the finetuning corpus largely removes them, though residual risks may remain. Our work (1) demonstrates that narrowly finetuned models have salient traces of their training objective in their activations and suggests ways to improve how they are trained, (2) warns AI safety and interpretability researchers that the common practice of using such models as a proxy for studying broader finetuning (e.g., chat-tuning) might not be realistic, and (3) highlights the need for deeper investigation into the effects of narrow finetuning and development of truly realistic case studies for model-diffing, safety and interpretability research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13900v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julian Minder, Cl\'ement Dumas, Stewart Slocum, Helena Casademunt, Cameron Holmes, Robert West, Neel Nanda</dc:creator>
    </item>
    <item>
      <title>Benefits and Limitations of Communication in Multi-Agent Reasoning</title>
      <link>https://arxiv.org/abs/2510.13903</link>
      <description>arXiv:2510.13903v1 Announce Type: cross 
Abstract: Chain-of-thought prompting has popularized step-by-step reasoning in large language models, yet model performance still degrades as problem complexity and context length grow. By decomposing difficult tasks with long contexts into shorter, manageable ones, recent multi-agent paradigms offer a promising near-term solution to this problem. However, the fundamental capacities of such systems are poorly understood. In this work, we propose a theoretical framework to analyze the expressivity of multi-agent systems. We apply our framework to three algorithmic families: state tracking, recall, and $k$-hop reasoning. We derive bounds on (i) the number of agents required to solve the task exactly, (ii) the quantity and structure of inter-agent communication, and (iii) the achievable speedups as problem size and context scale. Our results identify regimes where communication is provably beneficial, delineate tradeoffs between agent count and bandwidth, and expose intrinsic limitations when either resource is constrained. We complement our theoretical analysis with a set of experiments on pretrained LLMs using controlled synthetic benchmarks. Empirical outcomes confirm the tradeoffs between key quantities predicted by our theory. Collectively, our analysis offers principled guidance for designing scalable multi-agent reasoning systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13903v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Rizvi-Martel, Satwik Bhattamishra, Neil Rathi, Guillaume Rabusseau, Michael Hahn</dc:creator>
    </item>
    <item>
      <title>Schema for In-Context Learning</title>
      <link>https://arxiv.org/abs/2510.13905</link>
      <description>arXiv:2510.13905v1 Announce Type: cross 
Abstract: In-Context Learning (ICL) enables transformer-based language models to adapt to new tasks by conditioning on demonstration examples. However, traditional example-driven in-context learning lacks explicit modules for knowledge retrieval and transfer at the abstraction level. Inspired by cognitive science, specifically schema theory, which holds that humans interpret new information by activating pre-existing mental frameworks (schemas) to structure understanding, we introduce SCHEMA ACTIVATED IN CONTEXT LEARNING (SA-ICL). This framework extracts the representation of the building blocks of cognition for the reasoning process instilled from prior examples, creating an abstracted schema, a lightweight, structured template of key inferential steps and their relationships, which is then used to augment a model's reasoning process when presented with a novel question. We demonstrate that a broad range of large language models (LLMs) lack the capacity to form and utilize internal schema-based learning representations implicitly, but instead benefit significantly from explicit schema-based scaffolding. Across chemistry and physics questions from the GPQA dataset, our experiments show that SA-ICL consistently boosts performance, up to 36.19 percent, when the single demonstration example is of high quality, which simultaneously reduces reliance on the number of demonstrations and enhances interpretability. SCHEMA ACTIVATED IN CONTEXT LEARNING not only bridges disparate ICL strategies ranging from pattern priming to Chain-of-Thought prompting, but also paves a new path for enhancing human-like reasoning in LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13905v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pan Chen, Shaohong Chen, Mark Wang, Shi Xuan Leong, Priscilla Fung, Varinia Bernales, Alan Aspuru-Guzik</dc:creator>
    </item>
    <item>
      <title>Knowledge Reasoning Language Model: Unifying Knowledge and Language for Inductive Knowledge Graph Reasoning</title>
      <link>https://arxiv.org/abs/2510.13909</link>
      <description>arXiv:2510.13909v1 Announce Type: cross 
Abstract: Inductive Knowledge Graph Reasoning (KGR) aims to discover facts in open-domain KGs containing unknown entities and relations, which poses a challenge for KGR models in comprehending uncertain KG components. Existing studies have proposed Knowledge Graph Foundation Models (KGFMs) that learn structural invariances across KGs to handle this uncertainty. Recently, Large Language Models (LLMs) have demonstrated strong capabilities for open-domain knowledge reasoning. As a result, the latest research has focused on LLM-based KGFMs that integrate LLM knowledge with KG context for inductive KGR. However, the intrinsic knowledge of LLMs may be overshadowed by sparse KG context, leading to LLM knowledge distortion, which can cause irreversible damage to model reasoning. Moreover, existing LLM-based KGR methods still struggle to fully constrain generative hallucinations in LLMs, severely limiting the credibility of reasoning results. To address these limitations, we propose a Knowledge Reasoning Language Model (KRLM) that achieves unified coordination between LLM knowledge and KG context throughout the KGR process. Specifically, we design a Knowledge Reasoning Language (KRL) instruction format and a KRL tokenizer to align LLM knowledge with KG representations. Then, we propose a KRL attention layer that coordinates intrinsic LLM knowledge with additional KG context through a dynamic knowledge memory mechanism. Finally, a structure-aware next-entity predictor is proposed, which strictly constrains the reasoning results within a trustworthy knowledge domain. Extensive experimental results on 25 real-world inductive KGR datasets demonstrate the significant superiority of the proposed KRLM\footnote{Our source codes are available at https://anonymous.4open.science/r/KRLM-EA36 in both zero-shot reasoning and fine-tuning scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13909v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xingrui Zhuo, Jiapu Wang, Gongqing Wu, Zhongyuan Wang, Jichen Zhang, Shirui Pan, Xindong Wu</dc:creator>
    </item>
    <item>
      <title>AI Debaters are More Persuasive when Arguing in Alignment with Their Own Beliefs</title>
      <link>https://arxiv.org/abs/2510.13912</link>
      <description>arXiv:2510.13912v1 Announce Type: cross 
Abstract: The core premise of AI debate as a scalable oversight technique is that it is harder to lie convincingly than to refute a lie, enabling the judge to identify the correct position. Yet, existing debate experiments have relied on datasets with ground truth, where lying is reduced to defending an incorrect proposition. This overlooks a subjective dimension: lying also requires the belief that the claim defended is false. In this work, we apply debate to subjective questions and explicitly measure large language models' prior beliefs before experiments. Debaters were asked to select their preferred position, then presented with a judge persona deliberately designed to conflict with their identified priors. This setup tested whether models would adopt sycophantic strategies, aligning with the judge's presumed perspective to maximize persuasiveness, or remain faithful to their prior beliefs. We implemented and compared two debate protocols, sequential and simultaneous, to evaluate potential systematic biases. Finally, we assessed whether models were more persuasive and produced higher-quality arguments when defending positions consistent with their prior beliefs versus when arguing against them. Our main findings show that models tend to prefer defending stances aligned with the judge persona rather than their prior beliefs, sequential debate introduces significant bias favoring the second debater, models are more persuasive when defending positions aligned with their prior beliefs, and paradoxically, arguments misaligned with prior beliefs are rated as higher quality in pairwise comparison. These results can inform human judges to provide higher-quality training signals and contribute to more aligned AI systems, while revealing important aspects of human-AI interaction regarding persuasion dynamics in language models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13912v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mar\'ia Victoria Carro, Denise Alejandra Mester, Facundo Nieto, Oscar Agust\'in Stanchi, Guido Ernesto Bergman, Mario Alejandro Leiva, Eitan Sprejer, Luca Nicol\'as Forziati Gangi, Francisca Gauna Selasco, Juan Gustavo Corval\'an, Gerardo I. Simari, Mar\'ia Vanina Martinez</dc:creator>
    </item>
    <item>
      <title>Synthesizing Agentic Data for Web Agents with Progressive Difficulty Enhancement Mechanisms</title>
      <link>https://arxiv.org/abs/2510.13913</link>
      <description>arXiv:2510.13913v1 Announce Type: cross 
Abstract: Web-based 'deep research' agents aim to solve complex question - answering tasks through long-horizon interactions with online tools. These tasks remain challenging, as the underlying language models are often not optimized for long-horizon reasoning and exploration. Prior work has proposed workflows for constructing instruction-tuning datasets, often leveraging knowledge graphs. However, such methods typically lack fine-grained control over difficulty and quality, yielding synthetic data that falls short of capturing the complexity required for long-horizon reasoning. Furthermore, many studies conflate data and training effects by comparing models trained under different optimization recipes, making it difficult to isolate and evaluate the effectiveness of the data itself. We introduce a two-pronged data synthesis pipeline that generates question - answer pairs by progressively increasing task complexity until a frontier baseline web agent fails. The baseline agent plays multiple roles in this process: attempting the questions, validating factuality, checking for alternative answers, and enforcing filtering. To evaluate the effectiveness of our synthesis methods, we adopt a controlled training setup based on distillation from strong web agents. Experiments across multiple web-based benchmarks show that our dataset - despite being smaller - enables the training of more effective web agents than existing datasets. In particular, our data exhibits twice the diversity in tool-use actions, allowing models trained on it to achieve stronger performance while avoiding repetitive tool-calling behaviors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13913v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shrey Pandit, Xuan-Phi Nguyen, Yifei Ming, Austin Xu, Jiayu Wang, Caiming Xiong, Shafiq Joty</dc:creator>
    </item>
    <item>
      <title>Readability $\ne$ Learnability: Rethinking the Role of Simplicity in Training Small Language Models</title>
      <link>https://arxiv.org/abs/2510.13915</link>
      <description>arXiv:2510.13915v1 Announce Type: cross 
Abstract: Recent studies suggest that very small language models (SLMs) can generate surprisingly coherent text when trained on simplified, child-directed corpora such as TinyStories. These findings have been interpreted as evidence that readability -- characterized by accessible vocabulary, familiar narrative structure, and simple syntax -- plays a key role in enabling such capabilities to emerge. In this paper, we challenge that interpretation. We construct synthetic datasets with matched structure but varied readability, and find that readability alone does not predict coherence or learning efficiency in SLMs. Models trained on complex, adult-level text perform comparably to those trained on simplified language, and even exhibit faster development of coherence during training. Instead, we show that statistical simplicity, as measured by n-gram diversity, is a stronger predictor of learnability. Our findings caution against the growing trend of anthropomorphizing language model training -- drawing parallels to human cognitive development without empirical basis -- and argue for more precise reasoning about what properties actually support capability emergence in small models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13915v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ivan Lee, Taylor Berg-Kirkpatrick</dc:creator>
    </item>
    <item>
      <title>LLMs Can Get "Brain Rot"!</title>
      <link>https://arxiv.org/abs/2510.13928</link>
      <description>arXiv:2510.13928v1 Announce Type: cross 
Abstract: We propose and test the LLM Brain Rot Hypothesis: continual exposure to junk web text induces lasting cognitive decline in large language models (LLMs). To causally isolate data quality, we run controlled experiments on real Twitter/X corpora, constructing junk and reversely controlled datasets via two orthogonal operationalizations: M1 (engagement degree) and M2 (semantic quality), with matched token scale and training operations across conditions. Contrary to the control group, continual pre-training of 4 LLMs on the junk dataset causes non-trivial declines (Hedges' $g&gt;0.3$) on reasoning, long-context understanding, safety, and inflating "dark traits" (e.g., psychopathy, narcissism). The gradual mixtures of junk and control datasets also yield dose-response cognition decay: for example, under M1, ARC-Challenge with Chain Of Thoughts drops $74.9 \rightarrow 57.2$ and RULER-CWE $84.4 \rightarrow 52.3$ as junk ratio rises from $0\%$ to $100\%$.
  Error forensics reveal several key insights. First, we identify thought-skipping as the primary lesion: models increasingly truncate or skip reasoning chains, explaining most of the error growth. Second, partial but incomplete healing is observed: scaling instruction tuning and clean data pre-training improve the declined cognition yet cannot restore baseline capability, suggesting persistent representational drift rather than format mismatch. Finally, we discover that the popularity, a non-semantic metric, of a tweet is a better indicator of the Brain Rot effect than the length in M1. Together, the results provide significant, multi-perspective evidence that data quality is a causal driver of LLM capability decay, reframing curation for continual pretraining as a \textit{training-time safety} problem and motivating routine "cognitive health checks" for deployed LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13928v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuo Xing, Junyuan Hong, Yifan Wang, Runjin Chen, Zhenyu Zhang, Ananth Grama, Zhengzhong Tu, Zhangyang Wang</dc:creator>
    </item>
    <item>
      <title>Big Reasoning with Small Models: Instruction Retrieval at Inference Time</title>
      <link>https://arxiv.org/abs/2510.13935</link>
      <description>arXiv:2510.13935v1 Announce Type: cross 
Abstract: Can we bring large-scale reasoning to local-scale compute? Small language models (SLMs) are increasingly attractive because they run efficiently on local hardware, offering strong privacy, low cost, and reduced environmental impact. Yet they often struggle with tasks that require multi-step reasoning or domain-specific knowledge. We address this limitation through instruction intervention at inference time, where an SLM retrieves structured reasoning procedures rather than generating them from scratch. Our method builds an Instruction Corpus by grouping similar training questions and creating instructions via GPT-5. During inference, the SLM retrieves the most relevant instructions and follows their steps. Unlike retrieval-augmented generation, which retrieves text passages, instruction retrieval gives the model structured guidance for reasoning. We evaluate this framework on MedQA (medical board exams), MMLU Professional Law, and MathQA using models from 3B to 14B parameters without any additional fine-tuning. Instruction retrieval yields consistent gains: 9.4% on MedQA, 7.9% on MMLU Law, and 5.1% on MathQA. Concise instructions outperform longer ones, and the magnitude of improvement depends strongly on model family and intrinsic reasoning ability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13935v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kenan Alkiek, David Jurgens, Vinod Vydiswaran</dc:creator>
    </item>
    <item>
      <title>Less is More: Improving LLM Reasoning with Minimal Test-Time Intervention</title>
      <link>https://arxiv.org/abs/2510.13940</link>
      <description>arXiv:2510.13940v1 Announce Type: cross 
Abstract: Recent progress in large language models (LLMs) has focused on test-time scaling to improve reasoning via increased inference computation, but often at the cost of efficiency. We revisit test-time behavior and uncover a simple yet underexplored phenomenon: reasoning uncertainty is highly localized-only a small subset of high-entropy tokens dominantly affects output correctness. Motivated by this, we propose Minimal Test-Time Intervention (MTI), a training-free framework that enhances reasoning accuracy and stability with minimal overhead. MTI includes: (i) Selective CFG intervention, applying classifier-free guidance only at uncertain positions; and (ii) Lightweight negative-prompt guidance, reusing the main model's KV cache to approximate unconditional decoding efficiently. MTI yields consistent gains across general, coding, and STEM tasks-e.g., +1.35% average improvement on eight benchmarks for Qwen3-8B-Base and +5% on AIME2024 using Qwen3-32B-Reasoning-while remaining highly efficient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13940v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhen Yang, Mingyang Zhang, Feng Chen, Ganggui Ding, Liang Hou, Xin Tao, Pengfei Wan, Ying-Cong Chen</dc:creator>
    </item>
    <item>
      <title>Static Sandboxes Are Inadequate: Modeling Societal Complexity Requires Open-Ended Co-Evolution in LLM-Based Multi-Agent Simulations</title>
      <link>https://arxiv.org/abs/2510.13982</link>
      <description>arXiv:2510.13982v1 Announce Type: cross 
Abstract: What if artificial agents could not just communicate, but also evolve, adapt, and reshape their worlds in ways we cannot fully predict? With llm now powering multi-agent systems and social simulations, we are witnessing new possibilities for modeling open-ended, ever-changing environments. Yet, most current simulations remain constrained within static sandboxes, characterized by predefined tasks, limited dynamics, and rigid evaluation criteria. These limitations prevent them from capturing the complexity of real-world societies. In this paper, we argue that static, task-specific benchmarks are fundamentally inadequate and must be rethought. We critically review emerging architectures that blend llm with multi-agent dynamics, highlight key hurdles such as balancing stability and diversity, evaluating unexpected behaviors, and scaling to greater complexity, and introduce a fresh taxonomy for this rapidly evolving field. Finally, we present a research roadmap centered on open-endedness, continuous co-evolution, and the development of resilient, socially aligned AI ecosystems. \textbf{We call on the community to move beyond static paradigms and help shape the next generation of adaptive, socially-aware multi-agent simulations.}</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13982v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinkun Chen, Sher Badshah, Xuemin Yu, Sijia Han, Jiechao Gao</dc:creator>
    </item>
    <item>
      <title>Efficient Few-Shot Learning in Remote Sensing: Fusing Vision and Vision-Language Models</title>
      <link>https://arxiv.org/abs/2510.13993</link>
      <description>arXiv:2510.13993v1 Announce Type: cross 
Abstract: Remote sensing has become a vital tool across sectors such as urban planning, environmental monitoring, and disaster response. While the volume of data generated has increased significantly, traditional vision models are often constrained by the requirement for extensive domain-specific labelled data and their limited ability to understand the context within complex environments. Vision Language Models offer a complementary approach by integrating visual and textual data; however, their application to remote sensing remains underexplored, particularly given their generalist nature. This work investigates the combination of vision models and VLMs to enhance image analysis in remote sensing, with a focus on aircraft detection and scene understanding. The integration of YOLO with VLMs such as LLaVA, ChatGPT, and Gemini aims to achieve more accurate and contextually aware image interpretation. Performance is evaluated on both labelled and unlabelled remote sensing data, as well as degraded image scenarios which are crucial for remote sensing. The findings show an average MAE improvement of 48.46% across models in the accuracy of aircraft detection and counting, especially in challenging conditions, in both raw and degraded scenarios. A 6.17% improvement in CLIPScore for comprehensive understanding of remote sensing images is obtained. The proposed approach combining traditional vision models and VLMs paves the way for more advanced and efficient remote sensing image analysis, especially in few-shot learning scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13993v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jia Yun Chua, Argyrios Zolotas, Miguel Arana-Catania</dc:creator>
    </item>
    <item>
      <title>Finding Holes: Pathologist Level Performance Using AI for Cribriform Morphology Detection in Prostate Cancer</title>
      <link>https://arxiv.org/abs/2510.13995</link>
      <description>arXiv:2510.13995v1 Announce Type: cross 
Abstract: Background: Cribriform morphology in prostate cancer is a histological feature that indicates poor prognosis and contraindicates active surveillance. However, it remains underreported and subject to significant interobserver variability amongst pathologists. We aimed to develop and validate an AI-based system to improve cribriform pattern detection.
  Methods: We created a deep learning model using an EfficientNetV2-S encoder with multiple instance learning for end-to-end whole-slide classification. The model was trained on 640 digitised prostate core needle biopsies from 430 patients, collected across three cohorts. It was validated internally (261 slides from 171 patients) and externally (266 slides, 104 patients from three independent cohorts). Internal validation cohorts included laboratories or scanners from the development set, while external cohorts used completely independent instruments and laboratories. Annotations were provided by three expert uropathologists with known high concordance. Additionally, we conducted an inter-rater analysis and compared the model's performance against nine expert uropathologists on 88 slides from the internal validation cohort.
  Results: The model showed strong internal validation performance (AUC: 0.97, 95% CI: 0.95-0.99; Cohen's kappa: 0.81, 95% CI: 0.72-0.89) and robust external validation (AUC: 0.90, 95% CI: 0.86-0.93; Cohen's kappa: 0.55, 95% CI: 0.45-0.64). In our inter-rater analysis, the model achieved the highest average agreement (Cohen's kappa: 0.66, 95% CI: 0.57-0.74), outperforming all nine pathologists whose Cohen's kappas ranged from 0.35 to 0.62.
  Conclusion: Our AI model demonstrates pathologist-level performance for cribriform morphology detection in prostate cancer. This approach could enhance diagnostic reliability, standardise reporting, and improve treatment decisions for prostate cancer patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13995v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Kelvin Szolnoky, Anders Blilie, Nita Mulliqi, Toyonori Tsuzuki, Hemamali Samaratunga, Matteo Titus, Xiaoyi Ji, Sol Erika Boman, Einar Gudlaugsson, Svein Reidar Kjosavik, Jos\'e Asenjo, Marcello Gambacorta, Paolo Libretti, Marcin Braun, Radis{\l}aw Kordek, Roman {\L}owicki, Brett Delahunt, Kenneth A. Iczkowski, Theo van der Kwast, Geert J. L. H. van Leenders, Katia R. M. Leite, Chin-Chen Pan, Emiel Adrianus Maria Janssen, Martin Eklund, Lars Egevad, Kimmo Kartasalo</dc:creator>
    </item>
    <item>
      <title>REAP the Experts: Why Pruning Prevails for One-Shot MoE compression</title>
      <link>https://arxiv.org/abs/2510.13999</link>
      <description>arXiv:2510.13999v1 Announce Type: cross 
Abstract: Sparsely-activated Mixture-of-Experts (SMoE) models offer efficient pre-training and low latency but their large parameter counts create significant memory overhead, motivating research into expert compression. Contrary to recent findings favouring expert merging on discriminative benchmarks, we demonstrate that expert pruning is a superior strategy for generative tasks. We prove that merging introduces an irreducible error by causing a "functional subspace collapse", due to the loss of the router's independent, input-dependent control over experts. Leveraging this insight, we propose Router-weighted Expert Activation Pruning (REAP), a novel pruning criterion that considers both router gate-values and expert activation norms. Across a diverse set of SMoE models ranging from 20B to 1T parameters, REAP consistently outperforms merging and other pruning methods on generative benchmarks, especially at 50% compression. Notably, our method achieves near-lossless compression on code generation and tool-calling tasks with Qwen3-Coder-480B and Kimi-K2, even after pruning 50% of experts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13999v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mike Lasby, Ivan Lazarevich, Nish Sinnadurai, Sean Lie, Yani Ioannou, Vithursan Thangarasa</dc:creator>
    </item>
    <item>
      <title>Conditional Clifford-Steerable CNNs with Complete Kernel Basis for PDE Modeling</title>
      <link>https://arxiv.org/abs/2510.14007</link>
      <description>arXiv:2510.14007v1 Announce Type: cross 
Abstract: Clifford-Steerable CNNs (CSCNNs) provide a unified framework that allows incorporating equivariance to arbitrary pseudo-Euclidean groups, including isometries of Euclidean space and Minkowski spacetime. In this work, we demonstrate that the kernel basis of CSCNNs is not complete, thus limiting the model expressivity. To address this issue, we propose Conditional Clifford-Steerable Kernels, which augment the kernels with equivariant representations computed from the input feature field. We derive the equivariance constraint for these input-dependent kernels and show how it can be solved efficiently via implicit parameterization. We empirically demonstrate an improved expressivity of the resulting framework on multiple PDE forecasting tasks, including fluid dynamics and relativistic electrodynamics, where our method consistently outperforms baseline methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14007v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>B\'alint L\'aszl\'o Szarvas (University of Amsterdam), Maksim Zhdanov (University of Amsterdam, AMLab)</dc:creator>
    </item>
    <item>
      <title>Context-Selective State Space Models: Feedback is All You Need</title>
      <link>https://arxiv.org/abs/2510.14027</link>
      <description>arXiv:2510.14027v1 Announce Type: cross 
Abstract: Transformers, powered by the attention mechanism, are the backbone of most foundation models, yet they suffer from quadratic complexity and difficulties in dealing with long-range dependencies in the input sequence. Recent work has shown that state space models (SSMs) provide an efficient alternative, with the S6 module at the core of the Mamba architecture achieving state-of-the-art results on long-sequence benchmarks. In this paper, we introduce the COFFEE (COntext From FEEdback) model, a novel time-varying SSM that incorporates state feedback to enable context-dependent selectivity, while still allowing for parallel implementation. Whereas the selectivity mechanism of S6 only depends on the current input, COFFEE computes it from the internal state, which serves as a compact representation of the sequence history. This shift allows the model to regulate its dynamics based on accumulated context, improving its ability to capture long-range dependencies. In addition to state feedback, we employ an efficient model parametrization that removes redundancies present in S6 and leads to a more compact and trainable formulation. On the induction head task, COFFEE achieves near-perfect accuracy with two orders of magnitude fewer parameters and training sequences compared to S6. On MNIST, COFFEE largely outperforms S6 within the same architecture, reaching 97% accuracy with only 3585 parameters. These results showcase the role of state feedback as a key mechanism for building scalable and efficient sequence models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14027v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Riccardo Zattra, Giacomo Baggio, Umberto Casti, Augusto Ferrante, Francesco Ticozzi</dc:creator>
    </item>
    <item>
      <title>Think Globally, Group Locally: Evaluating LLMs Using Multi-Lingual Word Grouping Games</title>
      <link>https://arxiv.org/abs/2510.14030</link>
      <description>arXiv:2510.14030v1 Announce Type: cross 
Abstract: Large language models (LLMs) can exhibit biases in reasoning capabilities due to linguistic modality, performing better on tasks in one language versus another, even with similar content. Most previous works evaluate this through reasoning tasks where reliance on strategies or knowledge can ensure success, such as in commonsense or math tasks. However, abstract reasoning is vital to reasoning for everyday life, where people apply "out-of-the-box thinking" to identify and use patterns for solutions, without a reliance on formulaic approaches. Comparatively, little work has evaluated linguistic biases in this task type. In this paper, we propose a task inspired by the New York Times Connections: GlobalGroup, that evaluates models in an abstract reasoning task across several languages. We constructed a game benchmark with five linguistic backgrounds -- English, Spanish, Chinese, Hindi, and Arabic -- in both the native language and an English translation for comparison. We also proposed game difficulty measurements to evaluate models on games with similar difficulty, enabling a more controlled comparison, which is particularly important in reasoning evaluations. Through experimentation, we find English modalities largely lead to better performance in this abstract reasoning task, and performance disparities between open- and closed-source models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14030v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>C\'esar Guerra-Solano, Zhuochun Li, Xiang Lorraine Li</dc:creator>
    </item>
    <item>
      <title>One Bug, Hundreds Behind: LLMs for Large-Scale Bug Discovery</title>
      <link>https://arxiv.org/abs/2510.14036</link>
      <description>arXiv:2510.14036v1 Announce Type: cross 
Abstract: Fixing bugs in large programs is a challenging task that demands substantial time and effort. Once a bug is found, it is reported to the project maintainers, who work with the reporter to fix it and eventually close the issue. However, across the program, there are often similar code segments, which may also contain the bug, but were missed during discovery. Finding and fixing each recurring bug instance individually is labor intensive. Even more concerning, bug reports can inadvertently widen the attack surface as they provide attackers with an exploitable pattern that may be unresolved in other parts of the program.
  In this paper, we explore these Recurring Pattern Bugs (RPBs) that appear repeatedly across various code segments of a program or even in different programs, stemming from a same root cause, but are unresolved. Our investigation reveals that RPBs are widespread and can significantly compromise the security of software programs. This paper introduces BugStone, a program analysis system empowered by LLVM and a Large Language Model (LLM). The key observation is that many RPBs have one patched instance, which can be leveraged to identify a consistent error pattern, such as a specific API misuse. By examining the entire program for this pattern, it is possible to identify similar sections of code that may be vulnerable. Starting with 135 unique RPBs, BugStone identified more than 22K new potential issues in the Linux kernel. Manual analysis of 400 of these findings confirmed that 246 were valid. We also created a dataset from over 1.9K security bugs reported by 23 recent top-tier conference works. We manually annotate the dataset, identify 80 recurring patterns and 850 corresponding fixes. Even with a cost-efficient model choice, BugStone achieved 92.2% precision and 79.1% pairwise accuracy on the dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14036v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiushi Wu, Yue Xiao, Dhilung Kirat, Kevin Eykholt, Jiyong Jang, Douglas Lee Schales</dc:creator>
    </item>
    <item>
      <title>Cyber-Resilient System Identification for Power Grid through Bayesian Integration</title>
      <link>https://arxiv.org/abs/2510.14043</link>
      <description>arXiv:2510.14043v1 Announce Type: cross 
Abstract: Power grids increasingly need real-time situational awareness under the ever-evolving cyberthreat landscape. Advances in snapshot-based system identification approaches have enabled accurately estimating states and topology from a snapshot of measurement data, under random bad data and topology errors. However, modern interactive, targeted false data can stay undetectable to these methods, and significantly compromise estimation accuracy. This work advances system identification that combines snapshot-based method with time-series model via Bayesian Integration, to advance cyber resiliency against both random and targeted false data. Using a distance-based time-series model, this work can leverage historical data of different distributions induced by changes in grid topology and other settings. The normal system behavior captured from historical data is integrated into system identification through a Bayesian treatment, to make solutions robust to targeted false data. We experiment on mixed random anomalies (bad data, topology error) and targeted false data injection attack (FDIA) to demonstrate our method's 1) cyber resilience: achieving over 70% reduction in estimation error under FDIA; 2) anomalous data identification: being able to alarm and locate anomalous data; 3) almost linear scalability: achieving comparable speed with the snapshot-based baseline, both taking &lt;1min per time tick on the large 2,383-bus system using a laptop CPU.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14043v1</guid>
      <category>eess.SY</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.SY</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shimiao Li, Guannan Qu, Bryan Hooi, Vyas Sekar, Soummya Kar, Larry Pileggi</dc:creator>
    </item>
    <item>
      <title>Optical Computation-in-Communication enables low-latency, high-fidelity perception in telesurgery</title>
      <link>https://arxiv.org/abs/2510.14058</link>
      <description>arXiv:2510.14058v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) holds significant promise for enhancing intraoperative perception and decision-making in telesurgery, where physical separation impairs sensory feedback and control. Despite advances in medical AI and surgical robotics, conventional electronic AI architectures remain fundamentally constrained by the compounded latency from serial processing of inference and communication. This limitation is especially critical in latency-sensitive procedures such as endovascular interventions, where delays over 200 ms can compromise real-time AI reliability and patient safety. Here, we introduce an Optical Computation-in-Communication (OCiC) framework that reduces end-to-end latency significantly by performing AI inference concurrently with optical communication. OCiC integrates Optical Remote Computing Units (ORCUs) directly into the optical communication pathway, with each ORCU experimentally achieving up to 69 tera-operations per second per channel through spectrally efficient two-dimensional photonic convolution. The system maintains ultrahigh inference fidelity within 0.1% of CPU/GPU baselines on classification and coronary angiography segmentation, while intrinsically mitigating cumulative error propagation, a longstanding barrier to deep optical network scalability. We validated the robustness of OCiC through outdoor dark fibre deployments, confirming consistent and stable performance across varying environmental conditions. When scaled globally, OCiC transforms long-haul fibre infrastructure into a distributed photonic AI fabric with exascale potential, enabling reliable, low-latency telesurgery across distances up to 10,000 km and opening a new optical frontier for distributed medical intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14058v1</guid>
      <category>physics.optics</category>
      <category>cs.AI</category>
      <category>eess.IV</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rui Yang, Jiaming Hu, Jian-Qing Zheng, Yue-Zhen Lu, Jian-Wei Cui, Qun Ren, Yi-Jie Yu, John Edward Wu, Zhao-Yu Wang, Xiao-Li Lin, Dandan Zhang, Mingchu Tang, Christos Masouros, Huiyun Liu, Chin-Pang Liu</dc:creator>
    </item>
    <item>
      <title>On the expressivity of sparse maxout networks</title>
      <link>https://arxiv.org/abs/2510.14068</link>
      <description>arXiv:2510.14068v1 Announce Type: cross 
Abstract: We study the expressivity of sparse maxout networks, where each neuron takes a fixed number of inputs from the previous layer and employs a, possibly multi-argument, maxout activation. This setting captures key characteristics of convolutional or graph neural networks. We establish a duality between functions computable by such networks and a class of virtual polytopes, linking their geometry to questions of network expressivity. In particular, we derive a tight bound on the dimension of the associated polytopes, which serves as the central tool for our analysis. Building on this, we construct a sequence of depth hierarchies. While sufficiently deep sparse maxout networks are universal, we prove that if the required depth is not reached, width alone cannot compensate for the sparsity of a fixed indegree constraint.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14068v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.CO</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Moritz Grillo, Tobias Hofmann</dc:creator>
    </item>
    <item>
      <title>Exploratory Causal Inference in SAEnce</title>
      <link>https://arxiv.org/abs/2510.14073</link>
      <description>arXiv:2510.14073v1 Announce Type: cross 
Abstract: Randomized Controlled Trials are one of the pillars of science; nevertheless, they rely on hand-crafted hypotheses and expensive analysis. Such constraints prevent causal effect estimation at scale, potentially anchoring on popular yet incomplete hypotheses. We propose to discover the unknown effects of a treatment directly from data. For this, we turn unstructured data from a trial into meaningful representations via pretrained foundation models and interpret them via a sparse autoencoder. However, discovering significant causal effects at the neural level is not trivial due to multiple-testing issues and effects entanglement. To address these challenges, we introduce Neural Effect Search, a novel recursive procedure solving both issues by progressive stratification. After assessing the robustness of our algorithm on semi-synthetic experiments, we showcase, in the context of experimental ecology, the first successful unsupervised causal effect identification on a real-world scientific trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14073v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tommaso Mencattini, Riccardo Cadei, Francesco Locatello</dc:creator>
    </item>
    <item>
      <title>DiffOPF: Diffusion Solver for Optimal Power Flow</title>
      <link>https://arxiv.org/abs/2510.14075</link>
      <description>arXiv:2510.14075v1 Announce Type: cross 
Abstract: The optimal power flow (OPF) is a multi-valued, non-convex mapping from loads to dispatch setpoints. The variability of system parameters (e.g., admittances, topology) further contributes to the multiplicity of dispatch setpoints for a given load. Existing deep learning OPF solvers are single-valued and thus fail to capture the variability of system parameters unless fully represented in the feature space, which is prohibitive. To solve this problem, we introduce a diffusion-based OPF solver, termed \textit{DiffOPF}, that treats OPF as a conditional sampling problem. The solver learns the joint distribution of loads and dispatch setpoints from operational history, and returns the marginal dispatch distributions conditioned on loads. Unlike single-valued solvers, DiffOPF enables sampling statistically credible warm starts with favorable cost and constraint satisfaction trade-offs. We explore the sample complexity of DiffOPF to ensure the OPF solution within a prescribed distance from the optimization-based solution, and verify this experimentally on power system benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14075v1</guid>
      <category>eess.SY</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Milad Hoseinpour, Vladimir Dvorkin</dc:creator>
    </item>
    <item>
      <title>Every Language Model Has a Forgery-Resistant Signature</title>
      <link>https://arxiv.org/abs/2510.14086</link>
      <description>arXiv:2510.14086v1 Announce Type: cross 
Abstract: The ubiquity of closed-weight language models with public-facing APIs has generated interest in forensic methods, both for extracting hidden model details (e.g., parameters) and for identifying models by their outputs. One successful approach to these goals has been to exploit the geometric constraints imposed by the language model architecture and parameters. In this work, we show that a lesser-known geometric constraint--namely, that language model outputs lie on the surface of a high-dimensional ellipse--functions as a signature for the model and can be used to identify the source model of a given output. This ellipse signature has unique properties that distinguish it from existing model-output association methods like language model fingerprints. In particular, the signature is hard to forge: without direct access to model parameters, it is practically infeasible to produce log-probabilities (logprobs) on the ellipse. Secondly, the signature is naturally occurring, since all language models have these elliptical constraints. Thirdly, the signature is self-contained, in that it is detectable without access to the model inputs or the full weights. Finally, the signature is compact and redundant, as it is independently detectable in each logprob output from the model. We evaluate a novel technique for extracting the ellipse from small models and discuss the practical hurdles that make it infeasible for production-scale models. Finally, we use ellipse signatures to propose a protocol for language model output verification, analogous to cryptographic symmetric-key message authentication systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14086v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Finlayson, Xiang Ren, Swabha Swayamdipta</dc:creator>
    </item>
    <item>
      <title>Unlocking Out-of-Distribution Generalization in Transformers via Recursive Latent Space Reasoning</title>
      <link>https://arxiv.org/abs/2510.14095</link>
      <description>arXiv:2510.14095v1 Announce Type: cross 
Abstract: Systematic, compositional generalization beyond the training distribution remains a core challenge in machine learning -- and a critical bottleneck for the emergent reasoning abilities of modern language models. This work investigates out-of-distribution (OOD) generalization in Transformer networks using a GSM8K-style modular arithmetic on computational graphs task as a testbed. We introduce and explore a set of four architectural mechanisms aimed at enhancing OOD generalization: (i) input-adaptive recurrence; (ii) algorithmic supervision; (iii) anchored latent representations via a discrete bottleneck; and (iv) an explicit error-correction mechanism. Collectively, these mechanisms yield an architectural approach for native and scalable latent space reasoning in Transformer networks with robust algorithmic generalization capabilities. We complement these empirical results with a detailed mechanistic interpretability analysis that reveals how these mechanisms give rise to robust OOD generalization abilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14095v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Awni Altabaa, Siyu Chen, John Lafferty, Zhuoran Yang</dc:creator>
    </item>
    <item>
      <title>Extracting latent representations from X-ray spectra. Classification, regression, and accretion signatures of Chandra sources</title>
      <link>https://arxiv.org/abs/2510.14102</link>
      <description>arXiv:2510.14102v1 Announce Type: cross 
Abstract: The study of X-ray spectra is crucial to understanding the physical nature of astrophysical sources. Machine learning methods can extract compact and informative representations of data from large datasets. The Chandra Source Catalog (CSC) provides a rich archive of X-ray spectral data, which remains largely underexplored in this context. This work aims to develop a compact and physically meaningful representation of Chandra X-ray spectra using deep learning. To verify that the learned representation captures relevant information, we evaluate it through classification, regression, and interpretability analyses. We use a transformer-based autoencoder to compress X-ray spectra. The input spectra, drawn from the CSC, include only high-significance detections. Astrophysical source types and physical summary statistics are compiled from external catalogs. We evaluate the learned representation in terms of spectral reconstruction accuracy, clustering performance on 8 known astrophysical source classes, and correlation with physical quantities such as hardness ratios and hydrogen column density ($N_H$). The autoencoder accurately reconstructs spectra with 8 latent variables. Clustering in the latent space yields a balanced classification accuracy of $\sim$40% across the 8 source classes, increasing to $\sim$69% when restricted to AGNs and stellar-mass compact objects exclusively. Moreover, latent features correlate with non-linear combinations of spectral fluxes, suggesting that the compressed representation encodes physically relevant information. The proposed autoencoder-based pipeline is a powerful tool for the representation and interpretation of X-ray spectra, providing a compact latent space that supports both classification and the estimation of physical properties. This work demonstrates the potential of deep learning for spectral studies and uncovering new patterns in X-ray data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14102v1</guid>
      <category>astro-ph.IM</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicol\`o Oreste Pinciroli Vago, Juan Rafael Mart\'inez-Galarza, Roberta Amato</dc:creator>
    </item>
    <item>
      <title>Toward Cybersecurity-Expert Small Language Models</title>
      <link>https://arxiv.org/abs/2510.14113</link>
      <description>arXiv:2510.14113v1 Announce Type: cross 
Abstract: Large language models (LLMs) are transforming everyday applications, yet deployment in cybersecurity lags due to a lack of high-quality, domain-specific models and training datasets. To address this gap, we present CyberPal 2.0, a family of cybersecurity-expert small language models (SLMs) ranging from 4B-20B parameters. To train CyberPal 2.0, we generate an enriched chain-of-thought cybersecurity instruction dataset built with our data enrichment and formatting pipeline, SecKnowledge 2.0, which integrates expert-in-the-loop steering of reasoning formats alongside LLM-driven multi-step grounding, yielding higher-fidelity, task-grounded reasoning traces for security tasks. Across diverse cybersecurity benchmarks, CyberPal 2.0 consistently outperforms its baselines and matches or surpasses various open and closed-source frontier models, while remaining a fraction of their size. On core cyber threat intelligence knowledge tasks, our models outperform almost all tested frontier models, ranking second only to Sec-Gemini v1. On core threat-investigation tasks, such as correlating vulnerabilities and bug tickets with weaknesses, our best 20B-parameter model outperforms GPT-4o, o1, o3-mini, and Sec-Gemini v1, ranking first, while our smallest 4B-parameter model ranks second.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14113v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matan Levi, Daniel Ohayon, Ariel Blobstein, Ravid Sagi, Ian Molloy, Yair Allouche</dc:creator>
    </item>
    <item>
      <title>Inferred global dense residue transition graphs from primary structure sequences enable protein interaction prediction via directed graph convolutional neural networks</title>
      <link>https://arxiv.org/abs/2510.14139</link>
      <description>arXiv:2510.14139v1 Announce Type: cross 
Abstract: Introduction Accurate prediction of protein-protein interactions (PPIs) is crucial for understanding cellular functions and advancing drug development. Existing in-silico methods use direct sequence embeddings from Protein Language Models (PLMs). Others use Graph Neural Networks (GNNs) for 3D protein structures. This study explores less computationally intensive alternatives. We introduce a novel framework for downstream PPI prediction through link prediction. Methods We introduce a two-stage graph representation learning framework, ProtGram-DirectGCN. First, we developed ProtGram. This approach models a protein's primary structure as a hierarchy of globally inferred n-gram graphs. In these graphs, residue transition probabilities define edge weights. Each edge connects a pair of residues in a directed graph. The probabilities are aggregated from a large corpus of sequences. Second, we propose DirectGCN, a custom directed graph convolutional neural network. This model features a unique convolutional layer. It processes information through separate path-specific transformations: incoming, outgoing, and undirected. A shared transformation is also applied. These paths are combined via a learnable gating mechanism. We apply DirectGCN to ProtGram graphs to learn residue-level embeddings. These embeddings are pooled via attention to generate protein-level embeddings for prediction. Results We first established the efficacy of DirectGCN on standard node classification benchmarks. Its performance matches established methods on general datasets. The model excels at complex, directed graphs with dense, heterophilic structures. When applied to PPI prediction, the full ProtGram-DirectGCN framework delivers robust predictive power. This strong performance holds even with limited training data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14139v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.3389/fbinf.2025.1651623</arxiv:DOI>
      <dc:creator>Islam Akef Ebeid, Haoteng Tang, Pengfei Gu</dc:creator>
    </item>
    <item>
      <title>FinAI Data Assistant: LLM-based Financial Database Query Processing with the OpenAI Function Calling API</title>
      <link>https://arxiv.org/abs/2510.14162</link>
      <description>arXiv:2510.14162v1 Announce Type: cross 
Abstract: We present FinAI Data Assistant, a practical approach for natural-language querying over financial databases that combines large language models (LLMs) with the OpenAI Function Calling API. Rather than synthesizing complete SQL via text-to-SQL, our system routes user requests to a small library of vetted, parameterized queries, trading generative flexibility for reliability, low latency, and cost efficiency. We empirically study three questions: (RQ1) whether LLMs alone can reliably recall or extrapolate time-dependent financial data without external retrieval; (RQ2) how well LLMs map company names to stock ticker symbols; and (RQ3) whether function calling outperforms text-to-SQL for end-to-end database query processing. Across controlled experiments on prices and fundamentals, LLM-only predictions exhibit non-negligible error and show look-ahead bias primarily for stock prices relative to model knowledge cutoffs. Ticker-mapping accuracy is near-perfect for NASDAQ-100 constituents and high for S\&amp;P~500 firms. Finally, FinAI Data Assistant achieves lower latency and cost and higher reliability than a text-to-SQL baseline on our task suite. We discuss design trade-offs, limitations, and avenues for deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14162v1</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juhyeong Kim, Yejin Kim, Youngbin Lee, Hyunwoo Byun</dc:creator>
    </item>
    <item>
      <title>Towards Reversible Model Merging For Low-rank Weights</title>
      <link>https://arxiv.org/abs/2510.14163</link>
      <description>arXiv:2510.14163v1 Announce Type: cross 
Abstract: Model merging aims to combine multiple fine-tuned models into a single set of weights that performs well across all source tasks. While prior work has shown that merging can approximate the performance of individual fine-tuned models for each task, it largely overlooks scenarios where models are compressed into low-rank representations, either through low-rank adaptation (LoRA) or post-training singular value decomposition (SVD). We first demonstrate that applying conventional merging methods to low-rank weights leads to severe performance degradation in the merged model. Motivated by this phenomenon, we propose a fundamentally different approach: instead of collapsing all adapters into one set of weights, we construct a compact basis (e.g., an equivalent of holding two or more models) from which original task-specific models can be recovered via linear combination. This reframes merging as generating a reconstruction-capable model space rather than producing a single merged model. Crucially, this allows us to ``revert'' to each individual model when needed, recognizing that no merged model can consistently outperform one specialized for its task. Building on this insight, we introduce our method, Reversible Model Merging (RMM), an efficient, data-free, and flexible method that provides a closed-form solution for selecting the optimal basis of model weights and task-specific coefficients for linear combination. Extensive experiments across diverse datasets and model scales demonstrate that RMM consistently outperforms existing merging approaches, preserving the performance of low-rank compressed models by a significant margin.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14163v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammadsajad Alipour, Mohammad Mohammadi Amiri</dc:creator>
    </item>
    <item>
      <title>Virtually Being: Customizing Camera-Controllable Video Diffusion Models with Multi-View Performance Captures</title>
      <link>https://arxiv.org/abs/2510.14179</link>
      <description>arXiv:2510.14179v1 Announce Type: cross 
Abstract: We introduce a framework that enables both multi-view character consistency and 3D camera control in video diffusion models through a novel customization data pipeline. We train the character consistency component with recorded volumetric capture performances re-rendered with diverse camera trajectories via 4D Gaussian Splatting (4DGS), lighting variability obtained with a video relighting model. We fine-tune state-of-the-art open-source video diffusion models on this data to provide strong multi-view identity preservation, precise camera control, and lighting adaptability. Our framework also supports core capabilities for virtual production, including multi-subject generation using two approaches: joint training and noise blending, the latter enabling efficient composition of independently customized models at inference time; it also achieves scene and real-life video customization as well as control over motion and spatial layout during customization. Extensive experiments show improved video quality, higher personalization accuracy, and enhanced camera control and lighting adaptability, advancing the integration of video generation into virtual production. Our project page is available at: https://eyeline-labs.github.io/Virtually-Being.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14179v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuancheng Xu, Wenqi Xian, Li Ma, Julien Philip, Ahmet Levent Ta\c{s}el, Yiwei Zhao, Ryan Burgert, Mingming He, Oliver Hermann, Oliver Pilarski, Rahul Garg, Paul Debevec, Ning Yu</dc:creator>
    </item>
    <item>
      <title>MAFA: A Multi-Agent Framework for Enterprise-Scale Annotation with Configurable Task Adaptation</title>
      <link>https://arxiv.org/abs/2510.14184</link>
      <description>arXiv:2510.14184v1 Announce Type: cross 
Abstract: We present MAFA (Multi-Agent Framework for Annotation), a production-deployed system that transforms enterprise-scale annotation workflows through configurable multi-agent collaboration. Addressing the critical challenge of annotation backlogs in financial services, where millions of customer utterances require accurate categorization, MAFA combines specialized agents with structured reasoning and a judge-based consensus mechanism. Our framework uniquely supports dynamic task adaptation, allowing organizations to define custom annotation types (FAQs, intents, entities, or domain-specific categories) through configuration rather than code changes. Deployed at JP Morgan Chase, MAFA has eliminated a 1 million utterance backlog while achieving, on average, 86% agreement with human annotators, annually saving over 5,000 hours of manual annotation work. The system processes utterances with annotation confidence classifications, which are typically 85% high, 10% medium, and 5% low across all datasets we tested. This enables human annotators to focus exclusively on ambiguous and low-coverage cases. We demonstrate MAFA's effectiveness across multiple datasets and languages, showing consistent improvements over traditional and single-agent annotation baselines: 13.8% higher Top-1 accuracy, 15.1% improvement in Top-5 accuracy, and 16.9% better F1 in our internal intent classification dataset and similar gains on public benchmarks. This work bridges the gap between theoretical multi-agent systems and practical enterprise deployment, providing a blueprint for organizations facing similar annotation challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14184v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:journal_reference>AAAI 2026 Innovative Applications of AI</arxiv:journal_reference>
      <dc:creator>Mahmood Hegazy, Aaron Rodrigues, Azzam Naeem</dc:creator>
    </item>
    <item>
      <title>DPRF: A Generalizable Dynamic Persona Refinement Framework for Optimizing Behavior Alignment Between Personalized LLM Role-Playing Agents and Humans</title>
      <link>https://arxiv.org/abs/2510.14205</link>
      <description>arXiv:2510.14205v1 Announce Type: cross 
Abstract: The emerging large language model role-playing agents (LLM RPAs) aim to simulate individual human behaviors, but the persona fidelity is often undermined by manually-created profiles (e.g., cherry-picked information and personality characteristics) without validating the alignment with the target individuals. To address this limitation, our work introduces the Dynamic Persona Refinement Framework (DPRF).DPRF aims to optimize the alignment of LLM RPAs' behaviors with those of target individuals by iteratively identifying the cognitive divergence, either through free-form or theory-grounded, structured analysis, between generated behaviors and human ground truth, and refining the persona profile to mitigate these divergences.We evaluate DPRF with five LLMs on four diverse behavior-prediction scenarios: formal debates, social media posts with mental health issues, public interviews, and movie reviews.DPRF can consistently improve behavioral alignment considerably over baseline personas and generalizes across models and scenarios.Our work provides a robust methodology for creating high-fidelity persona profiles and enhancing the validity of downstream applications, such as user simulation, social studies, and personalized AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14205v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bingsheng Yao, Bo Sun, Yuanzhe Dong, Yuxuan Lu, Dakuo Wang</dc:creator>
    </item>
    <item>
      <title>LiteStage: Latency-aware Layer Skipping for Multi-stage Reasoning</title>
      <link>https://arxiv.org/abs/2510.14211</link>
      <description>arXiv:2510.14211v1 Announce Type: cross 
Abstract: Multi-stage reasoning has emerged as an effective strategy for enhancing the reasoning capability of small language models by decomposing complex problems into sequential sub-stages. However, this comes at the cost of increased latency. We observe that existing adaptive acceleration techniques, such as layer skipping, struggle to balance efficiency and accuracy in this setting due to two key challenges: (1) stage-wise variation in skip sensitivity, and (2) the generation of redundant output tokens. To address these, we propose LiteStage, a latency-aware layer skipping framework for multi-stage reasoning. LiteStage combines a stage-wise offline search that allocates optimal layer budgets with an online confidence-based generation early exit to suppress unnecessary decoding. Experiments on three benchmarks, e.g., OBQA, CSQA, and StrategyQA, show that LiteStage achieves up to 1.70x speedup with less than 4.0% accuracy loss, outperforming prior training-free layer skipping methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14211v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Beomseok Kang, Jiwon Song, Jae-Joon Kim</dc:creator>
    </item>
    <item>
      <title>Large Scale Retrieval for the LinkedIn Feed using Causal Language Models</title>
      <link>https://arxiv.org/abs/2510.14223</link>
      <description>arXiv:2510.14223v1 Announce Type: cross 
Abstract: In large scale recommendation systems like the LinkedIn Feed, the retrieval stage is critical for narrowing hundreds of millions of potential candidates to a manageable subset for ranking. LinkedIn's Feed serves suggested content from outside of the member's network (based on the member's topical interests), where 2000 candidates are retrieved from a pool of hundreds of millions candidate with a latency budget of a few milliseconds and inbound QPS of several thousand per second. This paper presents a novel retrieval approach that fine-tunes a large causal language model (Meta's LLaMA 3) as a dual encoder to generate high quality embeddings for both users (members) and content (items), using only textual input. We describe the end to end pipeline, including prompt design for embedding generation, techniques for fine-tuning at LinkedIn's scale, and infrastructure for low latency, cost effective online serving. We share our findings on how quantizing numerical features in the prompt enables the information to get properly encoded in the embedding, facilitating greater alignment between the retrieval and ranking layer. The system was evaluated using offline metrics and an online A/B test, which showed substantial improvements in member engagement. We observed significant gains among newer members, who often lack strong network connections, indicating that high-quality suggested content aids retention. This work demonstrates how generative language models can be effectively adapted for real time, high throughput retrieval in industrial applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14223v1</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sudarshan Srinivasa Ramanujam, Antonio Alonso, Saurabh Kataria, Siddharth Dangi, Akhilesh Gupta, Birjodh Singh Tiwana, Manas Somaiya, Luke Simon, David Byrne, Sojeong Ha, Sen Zhou, Andrei Akterskii, Zhanglong Liu, Samira Sriram, Crescent Xiong, Zhoutao Pei, Angela Shao, Alex Li, Annie Xiao, Caitlin Kolb, Thomas Kistler, Zach Moore, Hamed Firooz</dc:creator>
    </item>
    <item>
      <title>Scaling Test-Time Compute to Achieve IOI Gold Medal with Open-Weight Models</title>
      <link>https://arxiv.org/abs/2510.14232</link>
      <description>arXiv:2510.14232v1 Announce Type: cross 
Abstract: Competitive programming has become a rigorous benchmark for evaluating the reasoning and problem-solving capabilities of large language models (LLMs). The International Olympiad in Informatics (IOI) stands out as one of the most prestigious annual competitions in competitive programming and has become a key benchmark for comparing human and AI-level programming ability. While several proprietary models have been claimed to achieve gold medal-level performance at the IOI, often with undisclosed methods, achieving comparable results with open-weight models remains a significant challenge. In this paper, we present \gencluster, a scalable and reproducible test-time compute framework that attains IOI gold-level performance using open-weight models. It combines large-scale generation, behavioral clustering, ranking, and a round-robin submission strategy to efficiently explore diverse solution spaces under limited validation budgets. Our experiments show that the performance of our proposed approach scales consistently with available compute, narrowing the gap between open and closed systems. Notably, we will show that GenCluster can achieve a gold medal at IOI 2025 for the first time with an open-weight model gpt-oss-120b, setting a new benchmark for transparent and reproducible evaluation of reasoning in LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14232v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mehrzad Samadi, Aleksander Ficek, Sean Narenthiran, Siddhartha Jain, Wasi Uddin Ahmad, Somshubra Majumdar, Vahid Noroozi, Boris Ginsburg</dc:creator>
    </item>
    <item>
      <title>Spatial Computing Communications for Multi-User Virtual Reality in Distributed Mobile Edge Computing Network</title>
      <link>https://arxiv.org/abs/2510.14243</link>
      <description>arXiv:2510.14243v1 Announce Type: cross 
Abstract: Immersive virtual reality (VR) applications impose stringent requirements on latency, energy efficiency, and computational resources, particularly in multi-user interactive scenarios. To address these challenges, we introduce the concept of spatial computing communications (SCC), a framework designed to meet the latency and energy demands of multi-user VR over distributed mobile edge computing (MEC) networks. SCC jointly represents the physical space, defined by users and base stations, and the virtual space, representing shared immersive environments, using a probabilistic model of user dynamics and resource requirements. The resource deployment task is then formulated as a multi-objective combinatorial optimization (MOCO) problem that simultaneously minimizes system latency and energy consumption across distributed MEC resources. To solve this problem, we propose MO-CMPO, a multi-objective consistency model with policy optimization that integrates supervised learning and reinforcement learning (RL) fine-tuning guided by preference weights. Leveraging a sparse graph neural network (GNN), MO-CMPO efficiently generates Pareto-optimal solutions. Simulations with real-world New Radio base station datasets demonstrate that MO-CMPO achieves superior hypervolume performance and significantly lower inference latency than baseline methods. Furthermore, the analysis reveals practical deployment patterns: latency-oriented solutions favor local MEC execution to reduce transmission delay, while energy-oriented solutions minimize redundant placements to save energy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14243v1</guid>
      <category>cs.IT</category>
      <category>cs.AI</category>
      <category>math.IT</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Caolu Xu, Zhiyong Chen, Meixia Tao, Li Song, Wenjun Zhang</dc:creator>
    </item>
    <item>
      <title>Reinforcement Learning for Unsupervised Domain Adaptation in Spatio-Temporal Echocardiography Segmentation</title>
      <link>https://arxiv.org/abs/2510.14244</link>
      <description>arXiv:2510.14244v1 Announce Type: cross 
Abstract: Domain adaptation methods aim to bridge the gap between datasets by enabling knowledge transfer across domains, reducing the need for additional expert annotations. However, many approaches struggle with reliability in the target domain, an issue particularly critical in medical image segmentation, where accuracy and anatomical validity are essential. This challenge is further exacerbated in spatio-temporal data, where the lack of temporal consistency can significantly degrade segmentation quality, and particularly in echocardiography, where the presence of artifacts and noise can further hinder segmentation performance. To address these issues, we present RL4Seg3D, an unsupervised domain adaptation framework for 2D + time echocardiography segmentation. RL4Seg3D integrates novel reward functions and a fusion scheme to enhance key landmark precision in its segmentations while processing full-sized input videos. By leveraging reinforcement learning for image segmentation, our approach improves accuracy, anatomical validity, and temporal consistency while also providing, as a beneficial side effect, a robust uncertainty estimator, which can be used at test time to further enhance segmentation performance. We demonstrate the effectiveness of our framework on over 30,000 echocardiographic videos, showing that it outperforms standard domain adaptation techniques without the need for any labels on the target domain. Code is available at https://github.com/arnaudjudge/RL4Seg3D.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14244v1</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arnaud Judge, Nicolas Duchateau, Thierry Judge, Roman A. Sandler, Joseph Z. Sokol, Christian Desrosiers, Olivier Bernard, Pierre-Marc Jodoin</dc:creator>
    </item>
    <item>
      <title>Policy Regularized Distributionally Robust Markov Decision Processes with Linear Function Approximation</title>
      <link>https://arxiv.org/abs/2510.14246</link>
      <description>arXiv:2510.14246v1 Announce Type: cross 
Abstract: Decision-making under distribution shift is a central challenge in reinforcement learning (RL), where training and deployment environments differ. We study this problem through the lens of robust Markov decision processes (RMDPs), which optimize performance against adversarial transition dynamics. Our focus is the online setting, where the agent has only limited interaction with the environment, making sample efficiency and exploration especially critical. Policy optimization, despite its success in standard RL, remains theoretically and empirically underexplored in robust RL. To bridge this gap, we propose \textbf{D}istributionally \textbf{R}obust \textbf{R}egularized \textbf{P}olicy \textbf{O}ptimization algorithm (DR-RPO), a model-free online policy optimization method that learns robust policies with sublinear regret. To enable tractable optimization within the softmax policy class, DR-RPO incorporates reference-policy regularization, yielding RMDP variants that are doubly constrained in both transitions and policies. To scale to large state-action spaces, we adopt the $d$-rectangular linear MDP formulation and combine linear function approximation with an upper confidence bonus for optimistic exploration. We provide theoretical guarantees showing that policy optimization can achieve polynomial suboptimality bounds and sample efficiency in robust RL, matching the performance of value-based approaches. Finally, empirical results across diverse domains corroborate our theory and demonstrate the robustness of DR-RPO.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14246v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ML</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingwen Gu, Yiting He, Zhishuai Liu, Pan Xu</dc:creator>
    </item>
    <item>
      <title>Do Joint Language-Audio Embeddings Encode Perceptual Timbre Semantics?</title>
      <link>https://arxiv.org/abs/2510.14249</link>
      <description>arXiv:2510.14249v1 Announce Type: cross 
Abstract: Understanding and modeling the relationship between language and sound is critical for applications such as music information retrieval,text-guided music generation, and audio captioning. Central to these tasks is the use of joint language-audio embedding spaces, which map textual descriptions and auditory content into a shared embedding space. While multimodal embedding models such as MS-CLAP, LAION-CLAP, and MuQ-MuLan have shown strong performance in aligning language and audio, their correspondence to human perception of timbre, a multifaceted attribute encompassing qualities such as brightness, roughness, and warmth, remains underexplored. In this paper, we evaluate the above three joint language-audio embedding models on their ability to capture perceptual dimensions of timbre. Our findings show that LAION-CLAP consistently provides the most reliable alignment with human-perceived timbre semantics across both instrumental sounds and audio effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14249v1</guid>
      <category>cs.SD</category>
      <category>cs.AI</category>
      <category>eess.AS</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qixin Deng, Bryan Pardo, Thrasyvoulos N Pappas</dc:creator>
    </item>
    <item>
      <title>CAST: Compositional Analysis via Spectral Tracking for Understanding Transformer Layer Functions</title>
      <link>https://arxiv.org/abs/2510.14262</link>
      <description>arXiv:2510.14262v1 Announce Type: cross 
Abstract: Large language models have achieved remarkable success but remain largely black boxes with poorly understood internal mechanisms. To address this limitation, many researchers have proposed various interpretability methods including mechanistic analysis, probing classifiers, and activation visualization, each providing valuable insights from different perspectives. Building upon this rich landscape of complementary approaches, we introduce CAST (Compositional Analysis via Spectral Tracking), a probe-free framework that contributes a novel perspective by analyzing transformer layer functions through direct transformation matrix estimation and comprehensive spectral analysis. CAST offers complementary insights to existing methods by estimating the realized transformation matrices for each layer using Moore-Penrose pseudoinverse and applying spectral analysis with six interpretable metrics characterizing layer behavior. Our analysis reveals distinct behaviors between encoder-only and decoder-only models, with decoder models exhibiting compression-expansion cycles while encoder models maintain consistent high-rank processing. Kernel analysis further demonstrates functional relationship patterns between layers, with CKA similarity matrices clearly partitioning layers into three phases: feature extraction, compression, and specialization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14262v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zihao Fu, Ming Liao, Chris Russell, Zhenguang G. Cai</dc:creator>
    </item>
    <item>
      <title>Less is More: Denoising Knowledge Graphs For Retrieval Augmented Generation</title>
      <link>https://arxiv.org/abs/2510.14271</link>
      <description>arXiv:2510.14271v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) systems enable large language models (LLMs) instant access to relevant information for the generative process, demonstrating their superior performance in addressing common LLM challenges such as hallucination, factual inaccuracy, and the knowledge cutoff. Graph-based RAG further extends this paradigm by incorporating knowledge graphs (KGs) to leverage rich, structured connections for more precise and inferential responses. A critical challenge, however, is that most Graph-based RAG systems rely on LLMs for automated KG construction, often yielding noisy KGs with redundant entities and unreliable relationships. This noise degrades retrieval and generation performance while also increasing computational cost. Crucially, current research does not comprehensively address the denoising problem for LLM-generated KGs. In this paper, we introduce DEnoised knowledge Graphs for Retrieval Augmented Generation (DEG-RAG), a framework that addresses these challenges through: (1) entity resolution, which eliminates redundant entities, and (2) triple reflection, which removes erroneous relations. Together, these techniques yield more compact, higher-quality KGs that significantly outperform their unprocessed counterparts. Beyond the methods, we conduct a systematic evaluation of entity resolution for LLM-generated KGs, examining different blocking strategies, embedding choices, similarity metrics, and entity merging techniques. To the best of our knowledge, this is the first comprehensive exploration of entity resolution in LLM-generated KGs. Our experiments demonstrate that this straightforward approach not only drastically reduces graph size but also consistently improves question answering performance across diverse popular Graph-based RAG variants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14271v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yilun Zheng, Dan Yang, Jie Li, Lin Shang, Lihui Chen, Jiahao Xu, Sitao Luan</dc:creator>
    </item>
    <item>
      <title>PRISM: Agentic Retrieval with LLMs for Multi-Hop Question Answering</title>
      <link>https://arxiv.org/abs/2510.14278</link>
      <description>arXiv:2510.14278v1 Announce Type: cross 
Abstract: Retrieval plays a central role in multi-hop question answering (QA), where answering complex questions requires gathering multiple pieces of evidence. We introduce an Agentic Retrieval System that leverages large language models (LLMs) in a structured loop to retrieve relevant evidence with high precision and recall. Our framework consists of three specialized agents: a Question Analyzer that decomposes a multi-hop question into sub-questions, a Selector that identifies the most relevant context for each sub-question (focusing on precision), and an Adder that brings in any missing evidence (focusing on recall). The iterative interaction between Selector and Adder yields a compact yet comprehensive set of supporting passages. In particular, it achieves higher retrieval accuracy while filtering out distracting content, enabling downstream QA models to surpass full-context answer accuracy while relying on significantly less irrelevant information. Experiments on four multi-hop QA benchmarks -- HotpotQA, 2WikiMultiHopQA, MuSiQue, and MultiHopRAG -- demonstrates that our approach consistently outperforms strong baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14278v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.IR</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Mahadi Hasan Nahid, Davood Rafiei</dc:creator>
    </item>
    <item>
      <title>Beyond a Single Perspective: Towards a Realistic Evaluation of Website Fingerprinting Attacks</title>
      <link>https://arxiv.org/abs/2510.14283</link>
      <description>arXiv:2510.14283v1 Announce Type: cross 
Abstract: Website Fingerprinting (WF) attacks exploit patterns in encrypted traffic to infer the websites visited by users, posing a serious threat to anonymous communication systems. Although recent WF techniques achieve over 90% accuracy in controlled experimental settings, most studies remain confined to single scenarios, overlooking the complexity of real-world environments. This paper presents the first systematic and comprehensive evaluation of existing WF attacks under diverse realistic conditions, including defense mechanisms, traffic drift, multi-tab browsing, early-stage detection, open-world settings, and few-shot scenarios. Experimental results show that many WF techniques with strong performance in isolated settings degrade significantly when facing other conditions. Since real-world environments often combine multiple challenges, current WF attacks are difficult to apply directly in practice. This study highlights the limitations of WF attacks and introduces a multidimensional evaluation framework, offering critical insights for developing more robust and practical WF attacks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14283v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinhao Deng, Jingyou Chen, Linxiao Yu, Yixiang Zhang, Zhongyi Gu, Changhao Qiu, Xiyuan Zhao, Ke Xu, Qi Li</dc:creator>
    </item>
    <item>
      <title>Learning Human-Humanoid Coordination for Collaborative Object Carrying</title>
      <link>https://arxiv.org/abs/2510.14293</link>
      <description>arXiv:2510.14293v1 Announce Type: cross 
Abstract: Human-humanoid collaboration shows significant promise for applications in healthcare, domestic assistance, and manufacturing. While compliant robot-human collaboration has been extensively developed for robotic arms, enabling compliant human-humanoid collaboration remains largely unexplored due to humanoids' complex whole-body dynamics. In this paper, we propose a proprioception-only reinforcement learning approach, COLA, that combines leader and follower behaviors within a single policy. The model is trained in a closed-loop environment with dynamic object interactions to predict object motion patterns and human intentions implicitly, enabling compliant collaboration to maintain load balance through coordinated trajectory planning. We evaluate our approach through comprehensive simulator and real-world experiments on collaborative carrying tasks, demonstrating the effectiveness, generalization, and robustness of our model across various terrains and objects. Simulation experiments demonstrate that our model reduces human effort by 24.7%. compared to baseline approaches while maintaining object stability. Real-world experiments validate robust collaborative carrying across different object types (boxes, desks, stretchers, etc.) and movement patterns (straight-line, turning, slope climbing). Human user studies with 23 participants confirm an average improvement of 27.4% compared to baseline models. Our method enables compliant human-humanoid collaborative carrying without requiring external sensors or complex interaction models, offering a practical solution for real-world deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14293v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yushi Du, Yixuan Li, Baoxiong Jia, Yutang Lin, Pei Zhou, Wei Liang, Yanchao Yang, Siyuan Huang</dc:creator>
    </item>
    <item>
      <title>TED++: Submanifold-Aware Backdoor Detection via Layerwise Tubular-Neighbourhood Screening</title>
      <link>https://arxiv.org/abs/2510.14299</link>
      <description>arXiv:2510.14299v1 Announce Type: cross 
Abstract: As deep neural networks power increasingly critical applications, stealthy backdoor attacks, where poisoned training inputs trigger malicious model behaviour while appearing benign, pose a severe security risk. Many existing defences are vulnerable when attackers exploit subtle distance-based anomalies or when clean examples are scarce. To meet this challenge, we introduce TED++, a submanifold-aware framework that effectively detects subtle backdoors that evade existing defences. TED++ begins by constructing a tubular neighbourhood around each class's hidden-feature manifold, estimating its local ``thickness'' from a handful of clean activations. It then applies Locally Adaptive Ranking (LAR) to detect any activation that drifts outside the admissible tube. By aggregating these LAR-adjusted ranks across all layers, TED++ captures how faithfully an input remains on the evolving class submanifolds. Based on such characteristic ``tube-constrained'' behaviour, TED++ flags inputs whose LAR-based ranking sequences deviate significantly. Extensive experiments are conducted on benchmark datasets and tasks, demonstrating that TED++ achieves state-of-the-art detection performance under both adaptive-attack and limited-data scenarios. Remarkably, even with only five held-out examples per class, TED++ still delivers near-perfect detection, achieving gains of up to 14\% in AUROC over the next-best method. The code is publicly available at https://github.com/namle-w/TEDpp.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14299v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nam Le, Leo Yu Zhang, Kewen Liao, Shirui Pan, Wei Luo</dc:creator>
    </item>
    <item>
      <title>Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning</title>
      <link>https://arxiv.org/abs/2510.14300</link>
      <description>arXiv:2510.14300v1 Announce Type: cross 
Abstract: Vision-Language-Action (VLA) models are experiencing rapid development and demonstrating promising capabilities in robotic manipulation tasks. However, scaling up VLA models presents several critical challenges: (1) Training new VLA models from scratch demands substantial computational resources and extensive datasets. Given the current scarcity of robot data, it becomes particularly valuable to fully leverage well-pretrained VLA model weights during the scaling process. (2) Real-time control requires carefully balancing model capacity with computational efficiency. To address these challenges, We propose AdaMoE, a Mixture-of-Experts (MoE) architecture that inherits pretrained weights from dense VLA models, and scales up the action expert by substituting the feedforward layers into sparsely activated MoE layers. AdaMoE employs a decoupling technique that decouples expert selection from expert weighting through an independent scale adapter working alongside the traditional router. This enables experts to be selected based on task relevance while contributing with independently controlled weights, allowing collaborative expert utilization rather than winner-takes-all dynamics. Our approach demonstrates that expertise need not monopolize. Instead, through collaborative expert utilization, we can achieve superior performance while maintaining computational efficiency. AdaMoE consistently outperforms the baseline model across key benchmarks, delivering performance gains of 1.8% on LIBERO and 9.3% on RoboTwin. Most importantly, a substantial 21.5% improvement in real-world experiments validates its practical effectiveness for robotic manipulation tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14300v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weijie Shen, Yitian Liu, Yuhao Wu, Zhixuan Liang, Sijia Gu, Dehui Wang, Tian Nian, Lei Xu, Yusen Qin, Jiangmiao Pang, Xinping Guan, Xiaokang Yang, Yao Mu</dc:creator>
    </item>
    <item>
      <title>Watermarking for Factuality: Guiding Vision-Language Models Toward Truth via Tri-layer Contrastive Decoding</title>
      <link>https://arxiv.org/abs/2510.14304</link>
      <description>arXiv:2510.14304v1 Announce Type: cross 
Abstract: Large Vision-Language Models (LVLMs) have recently shown promising results on various multimodal tasks, even achieving human-comparable performance in certain cases. Nevertheless, LVLMs remain prone to hallucinations -- they often rely heavily on a single modality or memorize training data without properly grounding their outputs. To address this, we propose a training-free, tri-layer contrastive decoding with watermarking, which proceeds in three steps: (1) select a mature layer and an amateur layer among the decoding layers, (2) identify a pivot layer using a watermark-related question to assess whether the layer is visually well-grounded, and (3) apply tri-layer contrastive decoding to generate the final output. Experiments on public benchmarks such as POPE, MME and AMBER demonstrate that our method achieves state-of-the-art performance in reducing hallucinations in LVLMs and generates more visually grounded responses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14304v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyungryul Back, Seongbeom Park, Milim Kim, Mincheol Kwon, SangHyeok Lee, Hyunyoung Lee, Junhee Cho, Seunghyun Park, Jinkyu Kim</dc:creator>
    </item>
    <item>
      <title>MERLIN: A Testbed for Multilingual Multimodal Entity Recognition and Linking</title>
      <link>https://arxiv.org/abs/2510.14307</link>
      <description>arXiv:2510.14307v1 Announce Type: cross 
Abstract: This paper introduces MERLIN, a novel testbed system for the task of Multilingual Multimodal Entity Linking. The created dataset includes BBC news article titles, paired with corresponding images, in five languages: Hindi, Japanese, Indonesian, Vietnamese, and Tamil, featuring over 7,000 named entity mentions linked to 2,500 unique Wikidata entities. We also include several benchmarks using multilingual and multimodal entity linking methods exploring different language models like LLaMa-2 and Aya-23. Our findings indicate that incorporating visual data improves the accuracy of entity linking, especially for entities where the textual context is ambiguous or insufficient, and particularly for models that do not have strong multilingual abilities. For the work, the dataset, methods are available here at https://github.com/rsathya4802/merlin</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14307v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sathyanarayanan Ramamoorthy, Vishwa Shah, Simran Khanuja, Zaid Sheikh, Shan Jie, Ann Chia, Shearman Chua, Graham Neubig</dc:creator>
    </item>
    <item>
      <title>Column Generation Using Domain-Independent Dynamic Programming</title>
      <link>https://arxiv.org/abs/2510.14317</link>
      <description>arXiv:2510.14317v1 Announce Type: cross 
Abstract: Column generation and branch-and-price are leading methods for large-scale exact optimization. Column generation iterates between solving a master problem and a pricing problem. The master problem is a linear program, which can be solved using a generic solver. The pricing problem is highly dependent on the application but is usually discrete. Due to the difficulty of discrete optimization, high-performance column generation often relies on a custom pricing algorithm built specifically to exploit the problem's structure. This bespoke nature of the pricing solver prevents the reuse of components for other applications. We show that domain-independent dynamic programming, a software package for modeling and solving arbitrary dynamic programs, can be used as a generic pricing solver. We develop basic implementations of branch-and-price with pricing by domain-independent dynamic programming and show that they outperform a world-leading solver on static mixed integer programming formulations for seven problem classes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14317v1</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ryo Kuroiwa, Edward Lam</dc:creator>
    </item>
    <item>
      <title>Evaluating &amp; Reducing Deceptive Dialogue From Language Models with Multi-turn RL</title>
      <link>https://arxiv.org/abs/2510.14318</link>
      <description>arXiv:2510.14318v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) interact with millions of people worldwide in applications such as customer support, education and healthcare. However, their ability to produce deceptive outputs, whether intentionally or inadvertently, poses significant safety concerns. The unpredictable nature of LLM behavior, combined with insufficient safeguards against hallucination, misinformation, and user manipulation, makes their misuse a serious, real-world risk. In this paper, we investigate the extent to which LLMs engage in deception within dialogue, and propose the belief misalignment metric to quantify deception. We evaluate deception across four distinct dialogue scenarios, using five established deception detection metrics and our proposed metric. Our findings reveal this novel deception measure correlates more closely with human judgments than any existing metrics we test. Additionally, our benchmarking of eight state-of-the-art models indicates that LLMs naturally exhibit deceptive behavior in approximately 26% of dialogue turns, even when prompted with seemingly benign objectives. When prompted to deceive, LLMs are capable of increasing deceptiveness by as much as 31% relative to baselines. Unexpectedly, models trained with RLHF, the predominant approach for ensuring the safety of widely-deployed LLMs, still exhibit deception at a rate of 43% on average. Given that deception in dialogue is a behavior that develops over an interaction history, its effective evaluation and mitigation necessitates moving beyond single-utterance analyses. We introduce a multi-turn reinforcement learning methodology to fine-tune LLMs to reduce deceptive behaviors, leading to a 77.6% reduction compared to other instruction-tuned models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14318v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marwa Abdulhai, Ryan Cheng, Aryansh Shrivastava, Natasha Jaques, Yarin Gal, Sergey Levine</dc:creator>
    </item>
    <item>
      <title>A Robust Classification Method using Hybrid Word Embedding for Early Diagnosis of Alzheimer's Disease</title>
      <link>https://arxiv.org/abs/2510.14332</link>
      <description>arXiv:2510.14332v1 Announce Type: cross 
Abstract: Early detection of Alzheimer's Disease (AD) is greatly beneficial to AD patients, leading to early treatments that lessen symptoms and alleviating financial burden of health care. As one of the leading signs of AD, language capability changes can be used for early diagnosis of AD. In this paper, I develop a robust classification method using hybrid word embedding and fine-tuned hyperparameters to achieve state-of-the-art accuracy in the early detection of AD. Specifically, we create a hybrid word embedding based on word vectors from Doc2Vec and ELMo to obtain perplexity scores of the sentences. The scores identify whether a sentence is fluent or not and capture semantic context of the sentences. I enrich the word embedding by adding linguistic features to analyze syntax and semantics. Further, we input an embedded feature vector into logistic regression and fine tune hyperparameters throughout the pipeline. By tuning hyperparameters of the machine learning pipeline (e.g., model regularization parameter, learning rate and vector size of Doc2Vec, and vector size of ELMo), I achieve 91% classification accuracy and an Area Under the Curve (AUC) of 97% in distinguishing early AD from healthy subjects. Based on my knowledge, my model with 91% accuracy and 97% AUC outperforms the best existing NLP model for AD diagnosis with an accuracy of 88% [32]. I study the model stability through repeated experiments and find that the model is stable even though the training data is split randomly (standard deviation of accuracy = 0.0403; standard deviation of AUC = 0.0174). This affirms our proposed method is accurate and stable. This model can be used as a large-scale screening method for AD, as well as a complementary examination for doctors to detect AD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14332v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>eess.AS</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3446132.3446197</arxiv:DOI>
      <arxiv:journal_reference>Y. Li. Early Diagnosis of Alzheimer's Disease Using Hybrid Word Embedding and Linguistic Characteristics. Proc. In 2020 3rd International Conference on Algorithms, Computing and Artificial Intelligence (ACAI 2020). ACM. Article 65. pp 1-7</arxiv:journal_reference>
      <dc:creator>Yangyang Li</dc:creator>
    </item>
    <item>
      <title>Stop-RAG: Value-Based Retrieval Control for Iterative RAG</title>
      <link>https://arxiv.org/abs/2510.14337</link>
      <description>arXiv:2510.14337v1 Announce Type: cross 
Abstract: Iterative retrieval-augmented generation (RAG) enables large language models to answer complex multi-hop questions, but each additional loop increases latency, costs, and the risk of introducing distracting evidence, motivating the need for an efficient stopping strategy. Existing methods either use a predetermined number of iterations or rely on confidence proxies that poorly reflect whether more retrieval will actually help. We cast iterative RAG as a finite-horizon Markov decision process and introduce Stop-RAG, a value-based controller that adaptively decides when to stop retrieving. Trained with full-width forward-view Q($\lambda$) targets from complete trajectories, Stop-RAG learns effective stopping policies while remaining compatible with black-box APIs and existing pipelines. On multi-hop question-answering benchmarks, Stop-RAG consistently outperforms both fixed-iteration baselines and prompting-based stopping with LLMs. These results highlight adaptive stopping as a key missing component in current agentic systems, and demonstrate that value-based control can improve the accuracy of RAG systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14337v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaewan Park, Solbee Cho, Jay-Yoon Lee</dc:creator>
    </item>
    <item>
      <title>A Density-Informed Multimodal Artificial Intelligence Framework for Improving Breast Cancer Detection Across All Breast Densities</title>
      <link>https://arxiv.org/abs/2510.14340</link>
      <description>arXiv:2510.14340v1 Announce Type: cross 
Abstract: Mammography, the current standard for breast cancer screening, has reduced sensitivity in women with dense breast tissue, contributing to missed or delayed diagnoses. Thermalytix, an AI-based thermal imaging modality, captures functional vascular and metabolic cues that may complement mammographic structural data. This study investigates whether a breast density-informed multi-modal AI framework can improve cancer detection by dynamically selecting the appropriate imaging modality based on breast tissue composition. A total of 324 women underwent both mammography and thermal imaging. Mammography images were analyzed using a multi-view deep learning model, while Thermalytix assessed thermal images through vascular and thermal radiomics. The proposed framework utilized Mammography AI for fatty breasts and Thermalytix AI for dense breasts, optimizing predictions based on tissue type. This multi-modal AI framework achieved a sensitivity of 94.55% (95% CI: 88.54-100) and specificity of 79.93% (95% CI: 75.14-84.71), outperforming standalone mammography AI (sensitivity 81.82%, specificity 86.25%) and Thermalytix AI (sensitivity 92.73%, specificity 75.46%). Importantly, the sensitivity of Mammography dropped significantly in dense breasts (67.86%) versus fatty breasts (96.30%), whereas Thermalytix AI maintained high and consistent sensitivity in both (92.59% and 92.86%, respectively). This demonstrates that a density-informed multi-modal AI framework can overcome key limitations of unimodal screening and deliver high performance across diverse breast compositions. The proposed framework is interpretable, low-cost, and easily deployable, offering a practical path to improving breast cancer screening outcomes in both high-resource and resource-limited settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14340v1</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siva Teja Kakileti, Bharath Govindaraju, Sudhakar Sampangi, Geetha Manjunath</dc:creator>
    </item>
    <item>
      <title>BinCtx: Multi-Modal Representation Learning for Robust Android App Behavior Detection</title>
      <link>https://arxiv.org/abs/2510.14344</link>
      <description>arXiv:2510.14344v1 Announce Type: cross 
Abstract: Mobile app markets host millions of apps, yet undesired behaviors (e.g., disruptive ads, illegal redirection, payment deception) remain hard to catch because they often do not rely on permission-protected APIs and can be easily camouflaged via UI or metadata edits. We present BINCTX, a learning approach that builds multi-modal representations of an app from (i) a global bytecode-as-image view that captures code-level semantics and family-style patterns, (ii) a contextual view (manifested actions, components, declared permissions, URL/IP constants) indicating how behaviors are triggered, and (iii) a third-party-library usage view summarizing invocation frequencies along inter-component call paths. The three views are embedded and fused to train a contextual-aware classifier. On real-world malware and benign apps, BINCTX attains a macro F1 of 94.73%, outperforming strong baselines by at least 14.92%. It remains robust under commercial obfuscation (F1 84% post-obfuscation) and is more resistant to adversarial samples than state-of-the-art bytecode-only systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14344v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zichen Liu, Shao Yang, Xusheng Xiao</dc:creator>
    </item>
    <item>
      <title>Beyond One World: Benchmarking Super Heros in Role-Playing Across Multiversal Contexts</title>
      <link>https://arxiv.org/abs/2510.14351</link>
      <description>arXiv:2510.14351v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly used as role-playing agents, yet their capacity to faithfully and consistently portray version-specific characters -- for example, superheroes across comic and cinematic universes -- remains underexplored. Superhero canons such as Marvel and DC provide a rich testbed: decades of storytelling yield multiple incarnations of the same character with distinct histories, values, and moral codes. To study this problem, we introduce Beyond One World, a benchmark for character-grounded roleplay spanning 30 iconic heroes and 90 canon-specific versions. The benchmark comprises two tasks: (i) Canon Events, which probes factual recall of pivotal life stages, and (ii) Moral Dilemmas, which confronts models with ethically charged scenarios. We score responses for canonical accuracy and reasoning fidelity under a framework that separates internal deliberation ("thinking") from outward decisions ("acting"). We further propose Think-Act Matching, a metric that quantifies alignment between reasons and actions and serves as a proxy for model trustworthiness. Experiments across reasoning- and non-reasoning-oriented models yield three findings: (1) chain-of-thought prompting improves narrative coherence in weaker models but can reduce canonical accuracy in stronger ones; (2) cross-version generalization within a character remains a major obstacle; and (3) models often excel at either thinking or acting, but rarely both. Beyond One World exposes critical gaps in multiversal consistency and reasoning alignment, offering a challenging evaluation for role-playing LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14351v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Perapard Ngokpol, Kun Kerdthaisong, Pasin Buakhaw, Pitikorn Khlaisamniang, Supasate Vorathammathorn, Piyalitt Ittichaiwong, Nutchanon Yongsatianchot</dc:creator>
    </item>
    <item>
      <title>CURE: Confidence-driven Unified Reasoning Ensemble Framework for Medical Question Answering</title>
      <link>https://arxiv.org/abs/2510.14353</link>
      <description>arXiv:2510.14353v1 Announce Type: cross 
Abstract: High-performing medical Large Language Models (LLMs) typically require extensive fine-tuning with substantial computational resources, limiting accessibility for resource-constrained healthcare institutions. This study introduces a confidence-driven multi-model framework that leverages model diversity to enhance medical question answering without fine-tuning. Our framework employs a two-stage architecture: a confidence detection module assesses the primary model's certainty, and an adaptive routing mechanism directs low-confidence queries to Helper models with complementary knowledge for collaborative reasoning. We evaluate our approach using Qwen3-30B-A3B-Instruct, Phi-4 14B, and Gemma 2 12B across three medical benchmarks; MedQA, MedMCQA, and PubMedQA. Result demonstrate that our framework achieves competitive performance, with particularly strong results in PubMedQA (95.0\%) and MedMCQA (78.0\%). Ablation studies confirm that confidence-aware routing combined with multi-model collaboration substantially outperforms single-model approaches and uniform reasoning strategies. This work establishes that strategic model collaboration offers a practical, computationally efficient pathway to improve medical AI systems, with significant implications for democratizing access to advanced medical AI in resource-limited settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14353v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>physics.med-ph</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ziad Elshaer, Essam A. Rashed</dc:creator>
    </item>
    <item>
      <title>SUM-AgriVLN: Spatial Understanding Memory for Agricultural Vision-and-Language Navigation</title>
      <link>https://arxiv.org/abs/2510.14357</link>
      <description>arXiv:2510.14357v1 Announce Type: cross 
Abstract: Agricultural robots are emerging as powerful assistants across a wide range of agricultural tasks, nevertheless, still heavily rely on manual operation or fixed rail systems for movement. The AgriVLN method and the A2A benchmark pioneeringly extend Vision-and-Language Navigation (VLN) to the agricultural domain, enabling robots to navigate to the target positions following the natural language instructions. In practical agricultural scenarios, navigation instructions often repeatedly occur, yet AgriVLN treat each instruction as an independent episode, overlooking the potential of past experiences to provide spatial context for subsequent ones. To bridge this gap, we propose the method of Spatial Understanding Memory for Agricultural Vision-and-Language Navigation (SUM-AgriVLN), in which the SUM module employs spatial understanding and save spatial memory through 3D reconstruction and representation. When evaluated on the A2A benchmark, our SUM-AgriVLN effectively improves Success Rate from 0.47 to 0.54 with slight sacrifice on Navigation Error from 2.91m to 2.93m, demonstrating the state-of-the-art performance in the agricultural domain. Code: https://github.com/AlexTraveling/SUM-AgriVLN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14357v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaobei Zhao, Xingqi Lyu, Xiang Li</dc:creator>
    </item>
    <item>
      <title>From Binary to Bilingual: How the National Weather Service is Using Artificial Intelligence to Develop a Comprehensive Translation Program</title>
      <link>https://arxiv.org/abs/2510.14369</link>
      <description>arXiv:2510.14369v1 Announce Type: cross 
Abstract: To advance a Weather-Ready Nation, the National Weather Service (NWS) is developing a systematic translation program to better serve the 68.8 million people in the U.S. who do not speak English at home. This article outlines the foundation of an automated translation tool for NWS products, powered by artificial intelligence. The NWS has partnered with LILT, whose patented training process enables large language models (LLMs) to adapt neural machine translation (NMT) tools for weather terminology and messaging. Designed for scalability across Weather Forecast Offices (WFOs) and National Centers, the system is currently being developed in Spanish, Simplified Chinese, Vietnamese, and other widely spoken non-English languages. Rooted in best practices for multilingual risk communication, the system provides accurate, timely, and culturally relevant translations, significantly reducing manual translation time and easing operational workloads across the NWS. To guide the distribution of these products, GIS mapping was used to identify language needs across different NWS regions, helping prioritize resources for the communities that need them most. We also integrated ethical AI practices throughout the program's design, ensuring that transparency, fairness, and human oversight guide how automated translations are created, evaluated, and shared with the public. This work has culminated into a website featuring experimental multilingual NWS products, including translated warnings, 7-day forecasts, and educational campaigns, bringing the country one step closer to a national warning system that reaches all Americans.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14369v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joseph E. Trujillo-Falcon, Monica L. Bozeman, Liam E. Llewellyn, Samuel T. Halvorson, Meryl Mizell, Stuti Deshpande, Bob Manning, Todd Fagin</dc:creator>
    </item>
    <item>
      <title>Are My Optimized Prompts Compromised? Exploring Vulnerabilities of LLM-based Optimizers</title>
      <link>https://arxiv.org/abs/2510.14381</link>
      <description>arXiv:2510.14381v1 Announce Type: cross 
Abstract: Large language model (LLM) systems now underpin everyday AI applications such as chatbots, computer-use assistants, and autonomous robots, where performance often depends on carefully designed prompts. LLM-based prompt optimizers reduce that effort by iteratively refining prompts from scored feedback, yet the security of this optimization stage remains underexamined. We present the first systematic analysis of poisoning risks in LLM-based prompt optimization. Using HarmBench, we find systems are substantially more vulnerable to manipulated feedback than to injected queries: feedback-based attacks raise attack success rate (ASR) by up to $\Delta$ASR = 0.48. We introduce a simple fake-reward attack that requires no access to the reward model and significantly increases vulnerability, and we propose a lightweight highlighting defense that reduces the fake-reward $\Delta$ASR from 0.23 to 0.07 without degrading utility. These results establish prompt optimization pipelines as a first-class attack surface and motivate stronger safeguards for feedback channels and optimization frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14381v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CR</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Zhao, Reshmi Ghosh, Vitor Carvalho, Emily Lawton, Keegan Hines, Gao Huang, Jack W. Stokes</dc:creator>
    </item>
    <item>
      <title>Beat Detection as Object Detection</title>
      <link>https://arxiv.org/abs/2510.14391</link>
      <description>arXiv:2510.14391v1 Announce Type: cross 
Abstract: Recent beat and downbeat tracking models (e.g., RNNs, TCNs, Transformers) output frame-level activations. We propose reframing this task as object detection, where beats and downbeats are modeled as temporal "objects." Adapting the FCOS detector from computer vision to 1D audio, we replace its original backbone with WaveBeat's temporal feature extractor and add a Feature Pyramid Network to capture multi-scale temporal patterns. The model predicts overlapping beat/downbeat intervals with confidence scores, followed by non-maximum suppression (NMS) to select final predictions. This NMS step serves a similar role to DBNs in traditional trackers, but is simpler and less heuristic. Evaluated on standard music datasets, our approach achieves competitive results, showing that object detection techniques can effectively model musical beats with minimal adaptation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14391v1</guid>
      <category>cs.SD</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jaehoon Ahn, Moon-Ryul Jung</dc:creator>
    </item>
    <item>
      <title>FairBatching: Fairness-Aware Batch Formation for LLM Inference</title>
      <link>https://arxiv.org/abs/2510.14392</link>
      <description>arXiv:2510.14392v1 Announce Type: cross 
Abstract: Large language model (LLM) inference systems face a fundamental tension between minimizing Time-to-First-Token (TTFT) latency for new requests and maintaining a high, steady token generation rate (low Time-Per-Output-Token, or TPOT) for ongoing requests. Existing stall-free batching schedulers proposed by Sarathi, while effective at preventing decode stalls, introduce significant computational unfairness. They prioritize decode tasks excessively, simultaneously leading to underutilized decode slack and unnecessary prefill queuing delays, which collectively degrade the system's overall quality of service (QoS).
  This work identifies the root cause of this unfairness: the non-monotonic nature of Time-Between-Tokens (TBT) as a scheduling metric and the rigid decode-prioritizing policy that fails to adapt to dynamic workload bursts. We therefore propose FairBatching, a novel LLM inference scheduler that enforces fair resource allocation between prefill and decode tasks. It features an adaptive batch capacity determination mechanism, which dynamically adjusts the computational budget to improve the GPU utilization without triggering SLO violations. Its fair and dynamic batch formation algorithm breaks away from the decode-prioritizing paradigm, allowing computation resources to be reclaimed from bursting decode tasks to serve prefill surges, achieving global fairness. Furthermore, FairBatching provides a novel load estimation method, enabling more effective coordination with upper-level schedulers. Implemented and evaluated on realistic traces, FairBatching significantly reduces TTFT tail latency by up to 2.29x while robustly maintaining TPOT SLOs, achieving overall 20.0% improvement in single-node capacity and 54.3% improvement in cluster-level capacity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14392v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongtao Lyu, Boyue Liu, Mingyu Wu, Haibo Chen</dc:creator>
    </item>
    <item>
      <title>MedTrust-RAG: Evidence Verification and Trust Alignment for Biomedical Question Answering</title>
      <link>https://arxiv.org/abs/2510.14400</link>
      <description>arXiv:2510.14400v1 Announce Type: cross 
Abstract: Biomedical question answering (QA) requires accurate interpretation of complex medical knowledge. Large language models (LLMs) have shown promising capabilities in this domain, with retrieval-augmented generation (RAG) systems enhancing performance by incorporating external medical literature. However, RAG-based approaches in biomedical QA suffer from hallucinations due to post-retrieval noise and insufficient verification of retrieved evidence, undermining response reliability. We propose MedTrust-Guided Iterative RAG, a framework designed to enhance factual consistency and mitigate hallucinations in medical QA. Our method introduces three key innovations. First, it enforces citation-aware reasoning by requiring all generated content to be explicitly grounded in retrieved medical documents, with structured Negative Knowledge Assertions used when evidence is insufficient. Second, it employs an iterative retrieval-verification process, where a verification agent assesses evidence adequacy and refines queries through Medical Gap Analysis until reliable information is obtained. Third, it integrates the MedTrust-Align Module (MTAM) that combines verified positive examples with hallucination-aware negative samples, leveraging Direct Preference Optimization to reinforce citation-grounded reasoning while penalizing hallucination-prone response patterns. Experiments on MedMCQA, MedQA, and MMLU-Med demonstrate that our approach consistently outperforms competitive baselines across multiple model architectures, achieving the best average accuracy with gains of 2.7% for LLaMA3.1-8B-Instruct and 2.4% for Qwen3-8B.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14400v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.IR</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingpeng Ning, Yuanyuan Sun, Ling Luo, Yanhua Wang, Yuchen Pan, Hongfei Lin</dc:creator>
    </item>
    <item>
      <title>The Role of Social Learning and Collective Norm Formation in Fostering Cooperation in LLM Multi-Agent Systems</title>
      <link>https://arxiv.org/abs/2510.14401</link>
      <description>arXiv:2510.14401v1 Announce Type: cross 
Abstract: A growing body of multi-agent studies with Large Language Models (LLMs) explores how norms and cooperation emerge in mixed-motive scenarios, where pursuing individual gain can undermine the collective good. While prior work has explored these dynamics in both richly contextualized simulations and simplified game-theoretic environments, most LLM systems featuring common-pool resource (CPR) games provide agents with explicit reward functions directly tied to their actions. In contrast, human cooperation often emerges without full visibility into payoffs and population, relying instead on heuristics, communication, and punishment. We introduce a CPR simulation framework that removes explicit reward signals and embeds cultural-evolutionary mechanisms: social learning (adopting strategies and beliefs from successful peers) and norm-based punishment, grounded in Ostrom's principles of resource governance. Agents also individually learn from the consequences of harvesting, monitoring, and punishing via environmental feedback, enabling norms to emerge endogenously. We establish the validity of our simulation by reproducing key findings from existing studies on human behavior. Building on this, we examine norm evolution across a $2\times2$ grid of environmental and social initialisations (resource-rich vs. resource-scarce; altruistic vs. selfish) and benchmark how agentic societies comprised of different LLMs perform under these conditions. Our results reveal systematic model differences in sustaining cooperation and norm formation, positioning the framework as a rigorous testbed for studying emergent norms in mixed-motive LLM societies. Such analysis can inform the design of AI systems deployed in social and organizational contexts, where alignment with cooperative norms is critical for stability, fairness, and effective governance of AI-mediated environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14401v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Prateek Gupta, Qiankun Zhong, Hiromu Yakura, Thomas Eisenmann, Iyad Rahwan</dc:creator>
    </item>
    <item>
      <title>Instructions are all you need: Self-supervised Reinforcement Learning for Instruction Following</title>
      <link>https://arxiv.org/abs/2510.14420</link>
      <description>arXiv:2510.14420v1 Announce Type: cross 
Abstract: Language models often struggle to follow multi-constraint instructions that are crucial for real-world applications. Existing reinforcement learning (RL) approaches suffer from dependency on external supervision and sparse reward signals from multi-constraint tasks. We propose a label-free self-supervised RL framework that eliminates dependency on external supervision by deriving reward signals directly from instructions and generating pseudo-labels for reward model training. Our approach introduces constraint decomposition strategies and efficient constraint-wise binary classification to address sparse reward challenges while maintaining computational efficiency. Experiments show that our approach generalizes well, achieving strong improvements across 3 in-domain and 5 out-of-domain datasets, including challenging agentic and multi-turn instruction following. The data and code are publicly available at https://github.com/Rainier-rq/verl-if</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14420v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qingyu Ren, Qianyu He, Bowei Zhang, Jie Zeng, Jiaqing Liang, Yanghua Xiao, Weikang Zhou, Zeye Sun, Fei Yu</dc:creator>
    </item>
    <item>
      <title>Big Data Approaches to Bovine Bioacoustics: A FAIR-Compliant Dataset and Scalable ML Framework for Precision Livestock Welfare</title>
      <link>https://arxiv.org/abs/2510.14443</link>
      <description>arXiv:2510.14443v1 Announce Type: cross 
Abstract: The convergence of IoT sensing, edge computing, and machine learning is transforming precision livestock farming. Yet bioacoustic data streams remain underused because of computational complexity and ecological validity challenges. We present one of the most comprehensive bovine vocalization datasets to date, with 569 curated clips covering 48 behavioral classes, recorded across three commercial dairy farms using multiple microphone arrays and expanded to 2900 samples through domain informed augmentation. This FAIR compliant resource addresses major Big Data challenges - volume (90 hours of recordings, 65.6 GB), variety (multi farm and multi zone acoustics), velocity (real time processing), and veracity (noise robust feature extraction). Our distributed processing framework integrates advanced denoising using iZotope RX, multimodal synchronization through audio and video alignment, and standardized feature engineering with 24 acoustic descriptors generated from Praat, librosa, and openSMILE. Preliminary benchmarks reveal distinct class level acoustic patterns for estrus detection, distress classification, and maternal communication. The datasets ecological realism, reflecting authentic barn acoustics rather than controlled settings, ensures readiness for field deployment. This work establishes a foundation for animal centered AI, where bioacoustic data enable continuous and non invasive welfare assessment at industrial scale. By releasing standardized pipelines and detailed metadata, we promote reproducible research that connects Big Data analytics, sustainable agriculture, and precision livestock management. The framework supports UN SDG 9, showing how data science can turn traditional farming into intelligent, welfare optimized systems that meet global food needs while upholding ethical animal care.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14443v1</guid>
      <category>cs.SD</category>
      <category>cs.AI</category>
      <category>eess.AS</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mayuri Kate, Suresh Neethirajan</dc:creator>
    </item>
    <item>
      <title>A Free Lunch in LLM Compression: Revisiting Retraining after Pruning</title>
      <link>https://arxiv.org/abs/2510.14444</link>
      <description>arXiv:2510.14444v1 Announce Type: cross 
Abstract: While Neural Network pruning typically requires retraining the model to recover pruning-induced performance degradation, state-of-the-art Large Language Models (LLMs) pruning methods instead solve a layer-wise mask selection and reconstruction problem on a small set of calibration data to avoid full retraining, as it is considered computationally infeasible for LLMs. Reconstructing single matrices in isolation has favorable properties, such as convexity of the objective and significantly reduced memory requirements compared to full retraining. In practice, however, reconstruction is often implemented at coarser granularities, e.g., reconstructing a whole transformer block against its dense activations instead of a single matrix. In this work, we study the key design choices when reconstructing or retraining the remaining weights after pruning. We conduct an extensive computational study on state-of-the-art GPT architectures, and report several surprising findings that challenge common intuitions about retraining after pruning. In particular, we observe a free lunch scenario: reconstructing attention and MLP components separately within each transformer block is nearly the most resource-efficient yet achieves the best perplexity. Most importantly, this Pareto-optimal setup achieves better performance than full retraining, despite requiring only a fraction of the memory. Furthermore, we demonstrate that simple and efficient pruning criteria such as Wanda can outperform much more complex approaches when the reconstruction step is properly executed, highlighting its importance. Our findings challenge the narrative that retraining should be avoided at all costs and provide important insights into post-pruning performance recovery for LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14444v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Moritz Wagner, Christophe Roux, Max Zimmer, Sebastian Pokutta</dc:creator>
    </item>
    <item>
      <title>Feature Selection and Regularization in Multi-Class Classification: An Empirical Study of One-vs-Rest Logistic Regression with Gradient Descent Optimization and L1 Sparsity Constraints</title>
      <link>https://arxiv.org/abs/2510.14449</link>
      <description>arXiv:2510.14449v1 Announce Type: cross 
Abstract: Multi-class wine classification presents fundamental trade-offs between model accuracy, feature dimensionality, and interpretability - critical factors for production deployment in analytical chemistry. This paper presents a comprehensive empirical study of One-vs-Rest logistic regression on the UCI Wine dataset (178 samples, 3 cultivars, 13 chemical features), comparing from-scratch gradient descent implementation against scikit-learn's optimized solvers and quantifying L1 regularization effects on feature sparsity. Manual gradient descent achieves 92.59 percent mean test accuracy with smooth convergence, validating theoretical foundations, though scikit-learn provides 24x training speedup and 98.15 percent accuracy. Class-specific analysis reveals distinct chemical signatures with heterogeneous patterns where color intensity varies dramatically (0.31 to 16.50) across cultivars. L1 regularization produces 54-69 percent feature reduction with only 4.63 percent accuracy decrease, demonstrating favorable interpretability-performance trade-offs. We propose an optimal 5-feature subset achieving 62 percent complexity reduction with estimated 92-94 percent accuracy, enabling cost-effective deployment with 80 dollars savings per sample and 56 percent time reduction. Statistical validation confirms robust generalization with sub-2ms prediction latency suitable for real-time quality control. Our findings provide actionable guidelines for practitioners balancing comprehensive chemical analysis against targeted feature measurement in resource-constrained environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14449v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jahidul Arafat, Fariha Tasmin, Md Kaosar Uddin, Sanjaya Poudel, Eftakhar Ahmed Arnob</dc:creator>
    </item>
    <item>
      <title>Towards Adaptable Humanoid Control via Adaptive Motion Tracking</title>
      <link>https://arxiv.org/abs/2510.14454</link>
      <description>arXiv:2510.14454v1 Announce Type: cross 
Abstract: Humanoid robots are envisioned to adapt demonstrated motions to diverse real-world conditions while accurately preserving motion patterns. Existing motion prior approaches enable well adaptability with a few motions but often sacrifice imitation accuracy, whereas motion-tracking methods achieve accurate imitation yet require many training motions and a test-time target motion to adapt. To combine their strengths, we introduce AdaMimic, a novel motion tracking algorithm that enables adaptable humanoid control from a single reference motion. To reduce data dependence while ensuring adaptability, our method first creates an augmented dataset by sparsifying the single reference motion into keyframes and applying light editing with minimal physical assumptions. A policy is then initialized by tracking these sparse keyframes to generate dense intermediate motions, and adapters are subsequently trained to adjust tracking speed and refine low-level actions based on the adjustment, enabling flexible time warping that further improves imitation accuracy and adaptability. We validate these significant improvements in our approach in both simulation and the real-world Unitree G1 humanoid robot in multiple tasks across a wide range of adaptation conditions. Videos and code are available at https://taohuang13.github.io/adamimic.github.io/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14454v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tao Huang, Huayi Wang, Junli Ren, Kangning Yin, Zirui Wang, Xiao Chen, Feiyu Jia, Wentao Zhang, Junfeng Long, Jingbo Wang, Jiangmiao Pang</dc:creator>
    </item>
    <item>
      <title>Holdout-Loss-Based Data Selection for LLM Finetuning via In-Context Learning</title>
      <link>https://arxiv.org/abs/2510.14459</link>
      <description>arXiv:2510.14459v1 Announce Type: cross 
Abstract: Fine-tuning large pretrained language models is a common approach for aligning them with human preferences, but noisy or off-target examples can dilute supervision. While small, well-chosen datasets often match the performance of much larger ones, systematic and efficient ways to identify high-value training data remain underexplored. Many current methods rely on heuristics or expensive retraining. We present a theoretically grounded, resource-efficient framework for data selection and reweighting. At its core is an In-Context Approximation (ICA) that estimates the holdout loss a model would incur after training on a candidate example by conditioning on a small, curated holdout set in context. ICA requires no reference model and no additional finetuning. Under a local linearization, ICA is equivalent to a first-order update toward the holdout optimum, motivating its use as a proxy for data value. We derive per-example weights from ICA scores, dynamically reweighting gradient updates as model parameters evolve. Across SFT, DPO, and SimPO, and over diverse backbones and datasets, ICA-based reweighting consistently improves model alignment with minimal overhead. We analyze sensitivity to score update frequency and the choice of $k$ holdout examples for in-context demonstrations, and note limitations for rapidly drifting on-policy updates, highlighting directions for future work. Code and prompts will be released.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14459v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ling Zhang, Xianliang Yang, Juwon Yu, Park Cheonyoung, Lei Song, Jiang Bian</dc:creator>
    </item>
    <item>
      <title>LiRA: Linguistic Robust Anchoring for Cross-lingual Large Language Models</title>
      <link>https://arxiv.org/abs/2510.14466</link>
      <description>arXiv:2510.14466v1 Announce Type: cross 
Abstract: As large language models (LLMs) rapidly advance, performance on high-resource languages (e.g., English, Chinese) is nearing saturation, yet remains substantially lower for low-resource languages (e.g., Urdu, Thai) due to limited training data, machine-translation noise, and unstable cross-lingual alignment. We introduce LiRA (Linguistic Robust Anchoring for Large Language Models), a training framework that robustly improves cross-lingual representations under low-resource conditions while jointly strengthening retrieval and reasoning. LiRA comprises two modules: (i) Arca (Anchored Representation Composition Architecture), which anchors low-resource languages to an English semantic space via anchor-based alignment and multi-agent collaborative encoding, preserving geometric stability in a shared embedding space; and (ii) LaSR (Language-coupled Semantic Reasoner), which adds a language-aware lightweight reasoning head with consistency regularization on top of Arca's multilingual representations, unifying the training objective to enhance cross-lingual understanding, retrieval, and reasoning robustness. We further construct and release a multilingual product retrieval dataset covering five Southeast Asian and two South Asian languages. Experiments across low-resource benchmarks (cross-lingual retrieval, semantic similarity, and reasoning) show consistent gains and robustness under few-shot and noise-amplified settings; ablations validate the contribution of both Arca and LaSR. Code will be released on GitHub and the dataset on Hugging Face.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14466v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Haolin Li, Haipeng Zhang, Mang Li, Yaohua Wang, Lijie Wen, Yu Zhang, Biqing Huang</dc:creator>
    </item>
    <item>
      <title>Stealthy Dual-Trigger Backdoors: Attacking Prompt Tuning in LM-Empowered Graph Foundation Models</title>
      <link>https://arxiv.org/abs/2510.14470</link>
      <description>arXiv:2510.14470v1 Announce Type: cross 
Abstract: The emergence of graph foundation models (GFMs), particularly those incorporating language models (LMs), has revolutionized graph learning and demonstrated remarkable performance on text-attributed graphs (TAGs). However, compared to traditional GNNs, these LM-empowered GFMs introduce unique security vulnerabilities during the unsecured prompt tuning phase that remain understudied in current research. Through empirical investigation, we reveal a significant performance degradation in traditional graph backdoor attacks when operating in attribute-inaccessible constrained TAG systems without explicit trigger node attribute optimization. To address this, we propose a novel dual-trigger backdoor attack framework that operates at both text-level and struct-level, enabling effective attacks without explicit optimization of trigger node text attributes through the strategic utilization of a pre-established text pool. Extensive experimental evaluations demonstrate that our attack maintains superior clean accuracy while achieving outstanding attack success rates, including scenarios with highly concealed single-trigger nodes. Our work highlights critical backdoor risks in web-deployed LM-empowered GFMs and contributes to the development of more robust supervision mechanisms for open-source platforms in the era of foundation models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14470v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoyu Xue, Yuni Lai, Chenxi Huang, Yulin Zhu, Gaolei Li, Xiaoge Zhang, Kai Zhou</dc:creator>
    </item>
    <item>
      <title>Semantic representations emerge in biologically inspired ensembles of cross-supervising neural networks</title>
      <link>https://arxiv.org/abs/2510.14486</link>
      <description>arXiv:2510.14486v1 Announce Type: cross 
Abstract: Brains learn to represent information from a large set of stimuli, typically by weak supervision. Unsupervised learning is therefore a natural approach for exploring the design of biological neural networks and their computations. Accordingly, redundancy reduction has been suggested as a prominent design principle of neural encoding, but its ``mechanistic'' biological implementation is unclear. Analogously, unsupervised training of artificial neural networks yields internal representations that allow for accurate stimulus classification or decoding, but typically rely on biologically-implausible implementations. We suggest that interactions between parallel subnetworks in the brain may underlie such learning: we present a model of representation learning by ensembles of neural networks, where each network learns to encode stimuli into an abstract representation space by cross-supervising interactions with other networks, for inputs they receive simultaneously or in close temporal proximity. Aiming for biological plausibility, each network has a small ``receptive field'', thus receiving a fixed part of the external input, and the networks do not share weights. We find that for different types of network architectures, and for both visual or neuronal stimuli, these cross-supervising networks learn semantic representations that are easily decodable and that decoding accuracy is comparable to supervised networks -- both at the level of single networks and the ensemble. We further show that performance is optimal for small receptive fields, and that sparse connectivity between networks is nearly as accurate as all-to-all interactions, with far fewer computations. We thus suggest a sparsely interacting collective of cross-supervising networks as an algorithmic framework for representational learning and collective computation in the brain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14486v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roy Urbach, Elad Schneidman</dc:creator>
    </item>
    <item>
      <title>From Guess2Graph: When and How Can Unreliable Experts Safely Boost Causal Discovery in Finite Samples?</title>
      <link>https://arxiv.org/abs/2510.14488</link>
      <description>arXiv:2510.14488v1 Announce Type: cross 
Abstract: Causal discovery algorithms often perform poorly with limited samples. While integrating expert knowledge (including from LLMs) as constraints promises to improve performance, guarantees for existing methods require perfect predictions or uncertainty estimates, making them unreliable for practical use. We propose the Guess2Graph (G2G) framework, which uses expert guesses to guide the sequence of statistical tests rather than replacing them. This maintains statistical consistency while enabling performance improvements. We develop two instantiations of G2G: PC-Guess, which augments the PC algorithm, and gPC-Guess, a learning-augmented variant designed to better leverage high-quality expert input. Theoretically, both preserve correctness regardless of expert error, with gPC-Guess provably outperforming its non-augmented counterpart in finite samples when experts are "better than random." Empirically, both show monotonic improvement with expert accuracy, with gPC-Guess achieving significantly stronger gains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14488v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sujai Hiremath, Dominik Janzing, Philipp Faller, Patrick Bl\"obaum, Elke Kirschbaum, Shiva Prasad Kasiviswanathan, Kyra Gan</dc:creator>
    </item>
    <item>
      <title>E2Edev: Benchmarking Large Language Models in End-to-End Software Development Task</title>
      <link>https://arxiv.org/abs/2510.14509</link>
      <description>arXiv:2510.14509v1 Announce Type: cross 
Abstract: E2EDev comprises (i) a fine-grained set of user requirements, (ii) {multiple BDD test scenarios with corresponding Python step implementations for each requirement}, and (iii) a fully automated testing pipeline built on the Behave framework. To ensure its quality while reducing the annotation effort, E2EDev leverages our proposed Human-in-the-Loop Multi-Agent Annotation Framework (HITL-MAA). {By evaluating various E2ESD frameworks and LLM backbones with E2EDev}, our analysis reveals a persistent struggle to effectively solve these tasks, underscoring the critical need for more effective and cost-efficient E2ESD solutions. Our codebase and benchmark are publicly available at https://github.com/SCUNLP/E2EDev.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14509v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingyao Liu, Chen Huang, Zhizhao Guan, Wenqiang Lei, Yang Deng</dc:creator>
    </item>
    <item>
      <title>State Your Intention to Steer Your Attention: An AI Assistant for Intentional Digital Living</title>
      <link>https://arxiv.org/abs/2510.14513</link>
      <description>arXiv:2510.14513v1 Announce Type: cross 
Abstract: When working on digital devices, people often face distractions that can lead to a decline in productivity and efficiency, as well as negative psychological and emotional impacts. To address this challenge, we introduce a novel Artificial Intelligence (AI) assistant that elicits a user's intention, assesses whether ongoing activities are in line with that intention, and provides gentle nudges when deviations occur. The system leverages a large language model to analyze screenshots, application titles, and URLs, issuing notifications when behavior diverges from the stated goal. Its detection accuracy is refined through initial clarification dialogues and continuous user feedback. In a three-week, within-subjects field deployment with 22 participants, we compared our assistant to both a rule-based intent reminder system and a passive baseline that only logged activity. Results indicate that our AI assistant effectively supports users in maintaining focus and aligning their digital behavior with their intentions. Our source code is publicly available at this url https://intentassistant.github.io</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14513v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Juheon Choi, Juyoung Lee, Jian Kim, Chanyoung Kim, Taewon Min, W. Bradley Knox, Min Kyung Lee, Kimin Lee</dc:creator>
    </item>
    <item>
      <title>Real-Time Surgical Instrument Defect Detection via Non-Destructive Testing</title>
      <link>https://arxiv.org/abs/2510.14525</link>
      <description>arXiv:2510.14525v1 Announce Type: cross 
Abstract: Defective surgical instruments pose serious risks to sterility, mechanical integrity, and patient safety, increasing the likelihood of surgical complications. However, quality control in surgical instrument manufacturing often relies on manual inspection, which is prone to human error and inconsistency. This study introduces SurgScan, an AI-powered defect detection framework for surgical instruments. Using YOLOv8, SurgScan classifies defects in real-time, ensuring high accuracy and industrial scalability. The model is trained on a high-resolution dataset of 102,876 images, covering 11 instrument types and five major defect categories. Extensive evaluation against state-of-the-art CNN architectures confirms that SurgScan achieves the highest accuracy (99.3%) with real-time inference speeds of 4.2-5.8 ms per image, making it suitable for industrial deployment. Statistical analysis demonstrates that contrast-enhanced preprocessing significantly improves defect detection, addressing key limitations in visual inspection. SurgScan provides a scalable, cost-effective AI solution for automated quality control, reducing reliance on manual inspection while ensuring compliance with ISO 13485 and FDA standards, paving the way for enhanced defect detection in medical manufacturing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14525v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qurrat Ul Ain, Atif Aftab Ahmed Jilani, Zunaira Shafqat, Nigar Azhar Butt</dc:creator>
    </item>
    <item>
      <title>Agentic Entropy-Balanced Policy Optimization</title>
      <link>https://arxiv.org/abs/2510.14545</link>
      <description>arXiv:2510.14545v1 Announce Type: cross 
Abstract: Recently, Agentic Reinforcement Learning (Agentic RL) has made significant progress in incentivizing the multi-turn, long-horizon tool-use capabilities of web agents. While mainstream agentic RL algorithms autonomously explore high-uncertainty tool-call steps under the guidance of entropy, excessive reliance on entropy signals can impose further constraints, leading to the training collapse. In this paper, we delve into the challenges caused by entropy and propose the Agentic Entropy-Balanced Policy Optimization (AEPO), an agentic RL algorithm designed to balance entropy in both the rollout and policy update phases. AEPO comprises two core components: (1) a dynamic entropy-balanced rollout mechanism that adaptively allocate global and branch sampling budget through entropy pre-monitoring, while imposing a branch penalty on consecutive high-entropy tool-call steps to prevent over-branching issues; and (2) Entropy-Balanced Policy Optimization that inserts a stop-gradient operation into the high-entropy clipping term to preserve and properly rescale gradients on high-entropy tokens, while incorporating entropy-aware advantage estimation to prioritize learning on high-uncertainty tokens. Results across 14 challenging datasets show that AEPO consistently outperforms 7 mainstream RL algorithms. With just 1K RL samples, Qwen3-14B with AEPO achieves impressive results: 47.6% on GAIA, 11.2% on Humanity's Last Exam, and 43.0% on WebWalker for Pass@1; 65.0% on GAIA, 26.0% on Humanity's Last Exam, and 70.0% on WebWalker for Pass@5. Further analysis reveals that AEPO improves rollout sampling diversity while maintaining stable policy entropy, facilitating scalable web agent training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14545v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.IR</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guanting Dong, Licheng Bao, Zhongyuan Wang, Kangzhi Zhao, Xiaoxi Li, Jiajie Jin, Jinghan Yang, Hangyu Mao, Fuzheng Zhang, Kun Gai, Guorui Zhou, Yutao Zhu, Ji-Rong Wen, Zhicheng Dou</dc:creator>
    </item>
    <item>
      <title>Selective Labeling with False Discovery Rate Control</title>
      <link>https://arxiv.org/abs/2510.14581</link>
      <description>arXiv:2510.14581v1 Announce Type: cross 
Abstract: Obtaining high-quality labels for large datasets is expensive, requiring massive annotations from human experts. While AI models offer a cost-effective alternative by predicting labels, their label quality is compromised by the unavoidable labeling errors. Existing methods mitigate this issue through selective labeling, where AI labels a subset and human labels the remainder. However, these methods lack theoretical guarantees on the quality of AI-assigned labels, often resulting in unacceptably high labeling error within the AI-labeled subset. To address this, we introduce \textbf{Conformal Labeling}, a novel method to identify instances where AI predictions can be provably trusted. This is achieved by controlling the false discovery rate (FDR), the proportion of incorrect labels within the selected subset. In particular, we construct a conformal $p$-value for each test instance by comparing AI models' predicted confidence to those of calibration instances mislabeled by AI models. Then, we select test instances whose $p$-values are below a data-dependent threshold, certifying AI models' predictions as trustworthy. We provide theoretical guarantees that Conformal Labeling controls the FDR below the nominal level, ensuring that a predefined fraction of AI-assigned labels is correct on average. Extensive experiments demonstrate that our method achieves tight FDR control with high power across various tasks, including image and text labeling, and LLM QA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14581v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huipeng Huang, Wenbo Liao, Huajun Xi, Hao Zeng, Mengchen Zhao, Hongxin Wei</dc:creator>
    </item>
    <item>
      <title>Local Causal Discovery for Statistically Efficient Causal Inference</title>
      <link>https://arxiv.org/abs/2510.14582</link>
      <description>arXiv:2510.14582v1 Announce Type: cross 
Abstract: Causal discovery methods can identify valid adjustment sets for causal effect estimation for a pair of target variables, even when the underlying causal graph is unknown. Global causal discovery methods focus on learning the whole causal graph and therefore enable the recovery of optimal adjustment sets, i.e., sets with the lowest asymptotic variance, but they quickly become computationally prohibitive as the number of variables grows. Local causal discovery methods offer a more scalable alternative by focusing on the local neighborhood of the target variables, but are restricted to statistically suboptimal adjustment sets. In this work, we propose Local Optimal Adjustments Discovery (LOAD), a sound and complete causal discovery approach that combines the computational efficiency of local methods with the statistical optimality of global methods. First, LOAD identifies the causal relation between the targets and tests if the causal effect is identifiable by using only local information. If it is identifiable, it then finds the optimal adjustment set by leveraging local causal discovery to infer the mediators and their parents. Otherwise, it returns the locally valid parent adjustment sets based on the learned local structure. In our experiments on synthetic and realistic data LOAD outperforms global methods in scalability, while providing more accurate effect estimation than local methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14582v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M\'aty\'as Schubert, Tom Claassen, Sara Magliacane</dc:creator>
    </item>
    <item>
      <title>STANCE: Motion Coherent Video Generation Via Sparse-to-Dense Anchored Encoding</title>
      <link>https://arxiv.org/abs/2510.14588</link>
      <description>arXiv:2510.14588v1 Announce Type: cross 
Abstract: Video generation has recently made striking visual progress, but maintaining coherent object motion and interactions remains difficult. We trace two practical bottlenecks: (i) human-provided motion hints (e.g., small 2D maps) often collapse to too few effective tokens after encoding, weakening guidance; and (ii) optimizing for appearance and motion in a single head can favor texture over temporal consistency. We present STANCE, an image-to-video framework that addresses both issues with two simple components. First, we introduce Instance Cues -- a pixel-aligned control signal that turns sparse, user-editable hints into a dense 2.5D (camera-relative) motion field by averaging per-instance flow and augmenting with monocular depth over the instance mask. This reduces depth ambiguity compared to 2D arrow inputs while remaining easy to use. Second, we preserve the salience of these cues in token space with Dense RoPE, which tags a small set of motion tokens (anchored on the first frame) with spatial-addressable rotary embeddings. Paired with joint RGB \(+\) auxiliary-map prediction (segmentation or depth), our model anchors structure while RGB handles appearance, stabilizing optimization and improving temporal coherence without requiring per-frame trajectory scripts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14588v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Zhifei Chen, Tianshuo Xu, Leyi Wu, Luozhou Wang, Dongyu Yan, Zihan You, Wenting Luo, Guo Zhang, Yingcong Chen</dc:creator>
    </item>
    <item>
      <title>Just-In-Time Objectives: A General Approach for Specialized AI Interactions</title>
      <link>https://arxiv.org/abs/2510.14591</link>
      <description>arXiv:2510.14591v1 Announce Type: cross 
Abstract: Large language models promise a broad set of functions, but when not given a specific objective, they default to milquetoast results such as drafting emails littered with cliches. We demonstrate that inferring the user's in-the-moment objective, then rapidly optimizing for that singular objective, enables LLMs to produce tools, interfaces, and responses that are more responsive and desired. We contribute an architecture for automatically inducing just-in-time objectives by passively observing user behavior, then steering downstream AI systems through generation and evaluation against this objective. Inducing just-in-time objectives (e.g., "Clarify the abstract's research contribution") enables automatic generation of tools, e.g., those that critique a draft based on relevant HCI methodologies, anticipate related researchers' reactions, or surface ambiguous terminology. In a series of experiments (N=14, N=205) on participants' own tasks, JIT objectives enable LLM outputs that achieve 66-86% win rates over typical LLMs, and in-person use sessions (N=17) confirm that JIT objectives produce specialized tools unique to each participant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14591v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michelle S. Lam, Omar Shaikh, Hallie Xu, Alice Guo, Diyi Yang, Jeffrey Heer, James A. Landay, Michael S. Bernstein</dc:creator>
    </item>
    <item>
      <title>Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering</title>
      <link>https://arxiv.org/abs/2510.14605</link>
      <description>arXiv:2510.14605v1 Announce Type: cross 
Abstract: Knowledge-based visual question answering (KB-VQA) requires visual language models (VLMs) to integrate visual understanding with external knowledge retrieval. Although retrieval-augmented generation (RAG) achieves significant advances in this task by combining knowledge-base querying, it still struggles with the quality of multimodal queries and the relevance of retrieved results. To overcome these challenges, we propose a novel three-stage method, termed Wiki-PRF, including Processing, Retrieval and Filtering stages. The processing stage dynamically invokes visual tools to extract precise multimodal information for retrieval. The retrieval stage integrates visual and text features to achieve multimodal knowledge retrieval. The filtering stage performs relevance filtering and concentration on retrieval results. To this end, we introduce a visual language model trained with answer accuracy and format consistency as reward signals via a reinforcement learning manner. This enhances the model's reasoning, tool invocation for accurate queries, and filtering of irrelevant content. Experiments on benchmark datasets (E-VQA and InfoSeek) show significant improvements~(36.0 and 42.8) in answer quality, achieving state-of-the-art performance. Code is available at https://github.com/cqu-student/Wiki-PRF</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14605v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuyang Hong, Jiaqi Gu, Qi Yang, Lubin Fan, Yue Wu, Ying Wang, Kun Ding, Shiming Xiang, Jieping Ye</dc:creator>
    </item>
    <item>
      <title>An Active Inference Model of Mouse Point-and-Click Behaviour</title>
      <link>https://arxiv.org/abs/2510.14611</link>
      <description>arXiv:2510.14611v1 Announce Type: cross 
Abstract: We explore the use of Active Inference (AIF) as a computational user model for spatial pointing, a key problem in Human-Computer Interaction (HCI). We present an AIF agent with continuous state, action, and observation spaces, performing one-dimensional mouse pointing and clicking. We use a simple underlying dynamic system to model the mouse cursor dynamics with realistic perceptual delay. In contrast to previous optimal feedback control-based models, the agent's actions are selected by minimizing Expected Free Energy, solely based on preference distributions over percepts, such as observing clicking a button correctly. Our results show that the agent creates plausible pointing movements and clicks when the cursor is over the target, with similar end-point variance to human users. In contrast to other models of pointing, we incorporate fully probabilistic, predictive delay compensation into the agent. The agent shows distinct behaviour for differing target difficulties without the need to retune system parameters, as done in other approaches. We discuss the simulation results and emphasize the challenges in identifying the correct configuration of an AIF agent interacting with continuous systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14611v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Markus Klar, Sebastian Stein, Fraser Paterson, John H. Williamson, Roderick Murray-Smith</dc:creator>
    </item>
    <item>
      <title>Beyond Correctness: Evaluating Subjective Writing Preferences Across Cultures</title>
      <link>https://arxiv.org/abs/2510.14616</link>
      <description>arXiv:2510.14616v1 Announce Type: cross 
Abstract: Current preference learning methods achieve high accuracy on standard benchmarks but exhibit significant performance degradation when objective quality signals are removed. We introduce WritingPreferenceBench, a dataset of 1,800 human-annotated preference pairs (1,200 English, 600 Chinese) across 8 creative writing genres, where responses are matched for objective correctness, factual accuracy, and length. On this benchmark, sequence-based reward models--the standard architecture for RLHF--achieve only 52.7% mean accuracy, while zero-shot language model judges perform at 53.9%. In contrast, generative reward models that produce explicit reasoning chains achieve 81.8% accuracy. We observe high within-model variance across genres: individual models range from 18.2% to 81.8% accuracy across different writing categories, with standard deviations averaging 10.1%. This variance persists regardless of model scale, with 27B parameter models showing no consistent improvement over 8B variants. Our results suggest that current RLHF methods primarily learn to detect objective errors rather than capture subjective quality preferences (e.g., creativity, stylistic flair, and emotional resonance), and that successful preference modeling may require intermediate reasoning representations rather than direct classification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14616v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Shuangshuang Ying, Yunwen Li, Xingwei Qu, Xin Li, Sheng Jin, Minghao Liu, Zhoufutu Wen, Xeron Du, Tianyu Zheng, Yichi Zhang, Letian Ni, Yuyang Cheng, Qiguang Chen, Jingzhe Ding, Shengda Long, Wangchunshu Zhou, Jiazhan Feng, Wanjun Zhong, Libo Qin, Ge Zhang, Wenhao Huang, Wanxiang Che, Chenghua Lin</dc:creator>
    </item>
    <item>
      <title>Code-driven Number Sequence Calculation: Enhancing the inductive Reasoning Abilities of Large Language Models</title>
      <link>https://arxiv.org/abs/2510.14620</link>
      <description>arXiv:2510.14620v1 Announce Type: cross 
Abstract: Large language models (LLMs) make remarkable progress in reasoning tasks. Among different reasoning modes, inductive reasoning, due to its better alignment with human learning, attracts increasing interest. However, research on inductive reasoning faces certain challenges. First, existing inductive data mostly focuses on superficial regularities while lacking more complex internal patterns. Second, current works merely prompt LLMs or finetune on simple prompt-response pairs, but do not provide precise thinking processes nor implement difficulty control. Unlike previous work, we address these challenges by introducing \textit{CodeSeq}, a synthetic post-training dataset built from number sequences. We package number sequences into algorithmic problems to discover their general terms, defining a general term generation (GTG) task correspondingly. Our pipeline generates supervised finetuning data by reflecting on failed test cases and incorporating iterative corrections, thereby teaching LLMs to learn autonomous case generation and self-checking. Additionally, it leverages reinforcement learning with a novel Case-Synergy Solvability Scaling Reward based on both solvability, estimated from the problem pass rate, and the success rate of self-directed case generation, enabling models to learn more effectively from both successes and failures. Experimental results show that the models trained with \textit{CodeSeq} improve on various reasoning tasks and can preserve the models' OOD performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14620v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kedi Chen, Zhikai Lei, Xu Guo, Xuecheng Wu, Siyuan Zeng, Jianghao Yin, Yinqi Zhang, Qin Chen, Jie Zhou, Liang He, Qipeng Guo, Kai Chen, Wei Zhang</dc:creator>
    </item>
    <item>
      <title>LeapFactual: Reliable Visual Counterfactual Explanation Using Conditional Flow Matching</title>
      <link>https://arxiv.org/abs/2510.14623</link>
      <description>arXiv:2510.14623v1 Announce Type: cross 
Abstract: The growing integration of machine learning (ML) and artificial intelligence (AI) models into high-stakes domains such as healthcare and scientific research calls for models that are not only accurate but also interpretable. Among the existing explainable methods, counterfactual explanations offer interpretability by identifying minimal changes to inputs that would alter a model's prediction, thus providing deeper insights. However, current counterfactual generation methods suffer from critical limitations, including gradient vanishing, discontinuous latent spaces, and an overreliance on the alignment between learned and true decision boundaries. To overcome these limitations, we propose LeapFactual, a novel counterfactual explanation algorithm based on conditional flow matching. LeapFactual generates reliable and informative counterfactuals, even when true and learned decision boundaries diverge. Following a model-agnostic approach, LeapFactual is not limited to models with differentiable loss functions. It can even handle human-in-the-loop systems, expanding the scope of counterfactual explanations to domains that require the participation of human annotators, such as citizen science. We provide extensive experiments on benchmark and real-world datasets showing that LeapFactual generates accurate and in-distribution counterfactual explanations that offer actionable insights. We observe, for instance, that our reliable counterfactual samples with labels aligning to ground truth can be beneficially used as new training data to enhance the model. The proposed method is broadly applicable and enhances both scientific knowledge discovery and non-expert interpretability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14623v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhuo Cao, Xuan Zhao, Lena Krieger, Hanno Scharr, Ira Assent</dc:creator>
    </item>
    <item>
      <title>GemiRec: Interest Quantization and Generation for Multi-Interest Recommendation</title>
      <link>https://arxiv.org/abs/2510.14626</link>
      <description>arXiv:2510.14626v1 Announce Type: cross 
Abstract: Multi-interest recommendation has gained attention, especially in industrial retrieval stage. Unlike classical dual-tower methods, it generates multiple user representations instead of a single one to model comprehensive user interests. However, prior studies have identified two underlying limitations: the first is interest collapse, where multiple representations homogenize. The second is insufficient modeling of interest evolution, as they struggle to capture latent interests absent from a user's historical behavior. We begin with a thorough review of existing works in tackling these limitations. Then, we attempt to tackle these limitations from a new perspective. Specifically, we propose a framework-level refinement for multi-interest recommendation, named GemiRec. The proposed framework leverages interest quantization to enforce a structural interest separation and interest generation to learn the evolving dynamics of user interests explicitly. It comprises three modules: (a) Interest Dictionary Maintenance Module (IDMM) maintains a shared quantized interest dictionary. (b) Multi-Interest Posterior Distribution Module (MIPDM) employs a generative model to capture the distribution of user future interests. (c) Multi-Interest Retrieval Module (MIRM) retrieves items using multiple user-interest representations. Both theoretical and empirical analyses, as well as extensive experiments, demonstrate its advantages and effectiveness. Moreover, it has been deployed in production since March 2025, showing its practical value in industrial applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14626v1</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhibo Wu, Yunfan Wu, Quan Liu, Lin Jiang, Ping Yang, Yao Hu</dc:creator>
    </item>
    <item>
      <title>RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF</title>
      <link>https://arxiv.org/abs/2510.14628</link>
      <description>arXiv:2510.14628v1 Announce Type: cross 
Abstract: Text-To-Speech synthesis has achieved near-human quality in neutral speech, but emotional expressiveness remains a challenge. Existing methods often rely on costly emotion annotations or optimize indirect objectives that fail to capture the emotional expressiveness and perceptual naturalness of speech, leading to generated speech that is accurate but emotionally flat. To address these challenges, we propose the RLAIF-SPA framework, incorporating a Reinforcement Learning from AI Feedback (RLAIF) mechanism to employ Automatic Speech Recognition (ASR) and Large Language Model (LLM) techniques to respectively judge semantic accuracy and prosodic-emotional label alignment as a direct reward for emotional expressiveness and intelligibility optimization. Specifically, it leverages Prosodic Label Alignment to enhance expressive quality by jointly considering semantic accuracy and prosodic-emotional alignment along four fine-grained dimensions: Structure, Emotion, Speed, and Tone. In addition, it incorporates Semantic Accuracy Feedback to ensure the generation of clear and accurate speech. Experiments on the Libri Speech dataset show that RLAIF-SPA outperforms Chat-TTS, with a 26.1% reduction in WER, a 9.1% increase in SIM-O, and over 10% improvement in human evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14628v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qing Yang, Zhenghao Liu, Junxin Wang, Yangfan Du, Pengcheng Huang, Tong Xiao</dc:creator>
    </item>
    <item>
      <title>Causality Enhancement for Cross-Domain Recommendation</title>
      <link>https://arxiv.org/abs/2510.14641</link>
      <description>arXiv:2510.14641v1 Announce Type: cross 
Abstract: Cross-domain recommendation forms a crucial component in recommendation systems. It leverages auxiliary information through source domain tasks or features to enhance target domain recommendations. However, incorporating inconsistent source domain tasks may result in insufficient cross-domain modeling or negative transfer. While incorporating source domain features without considering the underlying causal relationships may limit their contribution to final predictions. Thus, a natural idea is to directly train a cross-domain representation on a causality-labeled dataset from the source to target domain. Yet this direction has been rarely explored, as identifying unbiased real causal labels is highly challenging in real-world scenarios. In this work, we attempt to take a first step in this direction by proposing a causality-enhanced framework, named CE-CDR. Specifically, we first reformulate the cross-domain recommendation as a causal graph for principled guidance. We then construct a causality-aware dataset heuristically. Subsequently, we derive a theoretically unbiased Partial Label Causal Loss to generalize beyond the biased causality-aware dataset to unseen cross-domain patterns, yielding an enriched cross-domain representation, which is then fed into the target model to enhance target-domain recommendations. Theoretical and empirical analyses, as well as extensive experiments, demonstrate the rationality and effectiveness of CE-CDR and its general applicability as a model-agnostic plugin. Moreover, it has been deployed in production since April 2025, showing its practical value in real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14641v1</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhibo Wu, Yunfan Wu, Lin Jiang, Ping Yang, Yao Hu</dc:creator>
    </item>
    <item>
      <title>The Bidding Games: Reinforcement Learning for MEV Extraction on Polygon Blockchain</title>
      <link>https://arxiv.org/abs/2510.14642</link>
      <description>arXiv:2510.14642v1 Announce Type: cross 
Abstract: In blockchain networks, the strategic ordering of transactions within blocks has emerged as a significant source of profit extraction, known as Maximal Extractable Value (MEV). The transition from spam-based Priority Gas Auctions to structured auction mechanisms like Polygon Atlas has transformed MEV extraction from public bidding wars into sealed-bid competitions under extreme time constraints. While this shift reduces network congestion, it introduces complex strategic challenges where searchers must make optimal bidding decisions within a sub-second window without knowledge of competitor behavior or presence. Traditional game-theoretic approaches struggle in this high-frequency, partially observable environment due to their reliance on complete information and static equilibrium assumptions. We present a reinforcement learning framework for MEV extraction on Polygon Atlas and make three contributions: (1) A novel simulation environment that accurately models the stochastic arrival of arbitrage opportunities and probabilistic competition in Atlas auctions; (2) A PPO-based bidding agent optimized for real-time constraints, capable of adaptive strategy formulation in continuous action spaces while maintaining production-ready inference speeds; (3) Empirical validation demonstrating our history-conditioned agent captures 49\% of available profits when deployed alongside existing searchers and 81\% when replacing the market leader, significantly outperforming static bidding strategies. Our work establishes that reinforcement learning provides a critical advantage in high-frequency MEV environments where traditional optimization methods fail, offering immediate value for industrial participants and protocol designers alike.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14642v1</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrei Seoev, Leonid Gremyachikh, Anastasiia Smirnova, Yash Madhwal, Alisa Kalacheva, Dmitry Belousov, Ilia Zubov, Aleksei Smirnov, Denis Fedyanin, Vladimir Gorgadze, Yury Yanovich</dc:creator>
    </item>
    <item>
      <title>In-Context Learning with Unpaired Clips for Instruction-based Video Editing</title>
      <link>https://arxiv.org/abs/2510.14648</link>
      <description>arXiv:2510.14648v1 Announce Type: cross 
Abstract: Despite the rapid progress of instruction-based image editing, its extension to video remains underexplored, primarily due to the prohibitive cost and complexity of constructing large-scale paired video editing datasets. To address this challenge, we introduce a low-cost pretraining strategy for instruction-based video editing that leverages in-context learning from unpaired video clips. We show that pretraining a foundation video generation model with this strategy endows it with general editing capabilities, such as adding, replacing, or deleting operations, according to input editing instructions. The pretrained model can then be efficiently refined with a small amount of high-quality paired editing data. Built upon HunyuanVideoT2V, our framework first pretrains on approximately 1M real video clips to learn basic editing concepts, and subsequently fine-tunes on fewer than 150k curated editing pairs to extend more editing tasks and improve the editing quality. Comparative experiments show that our method surpasses existing instruction-based video editing approaches in both instruction alignment and visual fidelity, achieving a 12\% improvement in editing instruction following and a 15\% improvement in editing quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14648v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyao Liao, Xianfang Zeng, Ziye Song, Zhoujie Fu, Gang Yu, Guosheng Lin</dc:creator>
    </item>
    <item>
      <title>Galaxy Morphology Classification with Counterfactual Explanation</title>
      <link>https://arxiv.org/abs/2510.14655</link>
      <description>arXiv:2510.14655v1 Announce Type: cross 
Abstract: Galaxy morphologies play an essential role in the study of the evolution of galaxies. The determination of morphologies is laborious for a large amount of data giving rise to machine learning-based approaches. Unfortunately, most of these approaches offer no insight into how the model works and make the results difficult to understand and explain. We here propose to extend a classical encoder-decoder architecture with invertible flow, allowing us to not only obtain a good predictive performance but also provide additional information about the decision process with counterfactual explanations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14655v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhuo Cao, Lena Krieger, Hanno Scharr, Ira Assent</dc:creator>
    </item>
    <item>
      <title>An Efficient Rubric-based Generative Verifier for Search-Augmented LLMs</title>
      <link>https://arxiv.org/abs/2510.14660</link>
      <description>arXiv:2510.14660v1 Announce Type: cross 
Abstract: Search augmentation empowers Large Language Models with retrieval capabilities to overcome the limitations imposed by static parameters. Recently, Reinforcement Learning leverages tailored reward signals as a viable technique to enhance LLMs performing tasks involving search. However, existing reward modeling for search-augmented LLMs faces several limitations. Rule-based rewards, such as Exact Match, are verifiable but fragile to variations in expression and cannot be applied to long-form workloads. In contrast, generative rewards improve robustness, but designing verifiable and stable rewards for long-form workloads in dynamic corpora remains challenging and also incurs high computational costs. In this paper, we propose a unified and verifiable paradigm, "nugget-as-rubric", which treats atomic information points as structured evaluation criteria for different search-augmentation workloads. Short-form tasks correspond to a single rubric, whereas long-form tasks expand to multiple rubrics aligned with the question's information needs. To support long-form settings, we design an automatic rubric construction pipeline based on query rewriting, which can automatically retrieve passages relevant to each question and extract rubrics from them, both from static corpora and from dynamic online web content. Furthermore, we introduce \textbf{Search-Gen-V}, a 4B-parameter efficient generative verifier under our proposed verifiable paradigm, which is trained via the idea of distillation and a two-stage strategy. Experimental results show that Search-Gen-V achieves strong verification accuracy across different workloads, making it a scalable, robust, and efficient verifiable reward constructor for search-augmented LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14660v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.IR</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Linyue Ma, Yilong Xu, Xiang Long, Zhi Zheng</dc:creator>
    </item>
    <item>
      <title>When Planners Meet Reality: How Learned, Reactive Traffic Agents Shift nuPlan Benchmarks</title>
      <link>https://arxiv.org/abs/2510.14677</link>
      <description>arXiv:2510.14677v1 Announce Type: cross 
Abstract: Planner evaluation in closed-loop simulation often uses rule-based traffic agents, whose simplistic and passive behavior can hide planner deficiencies and bias rankings. Widely used IDM agents simply follow a lead vehicle and cannot react to vehicles in adjacent lanes, hindering tests of complex interaction capabilities. We address this issue by integrating the state-of-the-art learned traffic agent model SMART into nuPlan. Thus, we are the first to evaluate planners under more realistic conditions and quantify how conclusions shift when narrowing the sim-to-real gap. Our analysis covers 14 recent planners and established baselines and shows that IDM-based simulation overestimates planning performance: nearly all scores deteriorate. In contrast, many planners interact better than previously assumed and even improve in multi-lane, interaction-heavy scenarios like lane changes or turns. Methods trained in closed-loop demonstrate the best and most stable driving performance. However, when reaching their limits in augmented edge-case scenarios, all learned planners degrade abruptly, whereas rule-based planners maintain reasonable basic behavior. Based on our results, we suggest SMART-reactive simulation as a new standard closed-loop benchmark in nuPlan and release the SMART agents as a drop-in alternative to IDM at https://github.com/shgd95/InteractiveClosedLoop.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14677v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Steffen Hagedorn, Luka Donkov, Aron Distelzweig, Alexandru P. Condurache</dc:creator>
    </item>
    <item>
      <title>xLLM Technical Report</title>
      <link>https://arxiv.org/abs/2510.14686</link>
      <description>arXiv:2510.14686v1 Announce Type: cross 
Abstract: We introduce xLLM, an intelligent and efficient Large Language Model (LLM) inference framework designed for high-performance, large-scale enterprise-grade serving, with deep optimizations for diverse AI accelerators. To address these challenges, xLLM builds a novel decoupled service-engine architecture. At the service layer, xLLM-Service features an intelligent scheduling module that efficiently processes multimodal requests and co-locates online and offline tasks through unified elastic scheduling to maximize cluster utilization. This module also relies on a workload-adaptive dynamic Prefill-Decode (PD) disaggregation policy and a novel Encode-Prefill-Decode (EPD) disaggregation policy designed for multimodal inputs. Furthermore, it incorporates a distributed architecture to provide global KV Cache management and robust fault-tolerant capabilities for high availability. At the engine layer, xLLM-Engine co-optimizes system and algorithm designs to fully saturate computing resources. This is achieved through comprehensive multi-layer execution pipeline optimizations, an adaptive graph mode and an xTensor memory management. xLLM-Engine also further integrates algorithmic enhancements such as optimized speculative decoding and dynamic EPLB, collectively serving to substantially boost throughput and inference efficiency. Extensive evaluations demonstrate that xLLM delivers significantly superior performance and resource efficiency. Under identical TPOT constraints, xLLM achieves throughput up to 1.7x that of MindIE and 2.2x that of vLLM-Ascend with Qwen-series models, while maintaining an average throughput of 1.7x that of MindIE with Deepseek-series models. xLLM framework is publicly available at https://github.com/jd-opensource/xllm and https://github.com/jd-opensource/xllm-service.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14686v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tongxuan Liu, Tao Peng, Peijun Yang, Xiaoyang Zhao, Xiusheng Lu, Weizhe Huang, Zirui Liu, Xiaoyu Chen, Zhiwei Liang, Jun Xiong, Donghe Jin, Minchao Zhang, Jinrong Guo, Yingxu Deng, Xu Zhang, Xianzhe Dong, Siqi Wang, Siyu Wu, Yu Wu, Zihan Tang, Yuting Zeng, Yanshu Wang, Jinguang Liu, Meng Kang, Menxin Li, Yunlong Wang, Yiming Liu, Xiaolong Ma, Yifan Wang, Yichen Zhang, Jinrun Yin, Keyang Zheng, Jiawei Yin, Jun Zhang, Ziyue Wang, Xiaobo Lin, Liangyu Liu, Liwei Lan, Yang Liu, Chunhua Peng, Han Liu, Songcheng Ren, Xuezhu Wang, Yunheng Shen, Yi Wang, Guyue Liu, Hui Chen, Tong Yang, Hailong Yang, Jing Li, Guiguang Ding, Ke Zhang</dc:creator>
    </item>
    <item>
      <title>FedPPA: Progressive Parameter Alignment for Personalized Federated Learning</title>
      <link>https://arxiv.org/abs/2510.14698</link>
      <description>arXiv:2510.14698v1 Announce Type: cross 
Abstract: Federated Learning (FL) is designed as a decentralized, privacy-preserving machine learning paradigm that enables multiple clients to collaboratively train a model without sharing their data. In real-world scenarios, however, clients often have heterogeneous computational resources and hold non-independent and identically distributed data (non-IID), which poses significant challenges during training. Personalized Federated Learning (PFL) has emerged to address these issues by customizing models for each client based on their unique data distribution. Despite its potential, existing PFL approaches typically overlook the coexistence of model and data heterogeneity arising from clients with diverse computational capabilities. To overcome this limitation, we propose a novel method, called Progressive Parameter Alignment (FedPPA), which progressively aligns the weights of common layers across clients with the global model's weights. Our approach not only mitigates inconsistencies between global and local models during client updates, but also preserves client's local knowledge, thereby enhancing personalization robustness in non-IID settings. To further enhance the global model performance while retaining strong personalization, we also integrate entropy-based weighted averaging into the FedPPA framework. Experiments on three image classification datasets, including MNIST, FMNIST, and CIFAR-10, demonstrate that FedPPA consistently outperforms existing FL algorithms, achieving superior performance in personalized adaptation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14698v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maulidi Adi Prasetia, Muhamad Risqi U. Saputra, Guntur Dharma Putra</dc:creator>
    </item>
    <item>
      <title>Where are the Whales: A Human-in-the-loop Detection Method for Identifying Whales in High-resolution Satellite Imagery</title>
      <link>https://arxiv.org/abs/2510.14709</link>
      <description>arXiv:2510.14709v1 Announce Type: cross 
Abstract: Effective monitoring of whale populations is critical for conservation, but traditional survey methods are expensive and difficult to scale. While prior work has shown that whales can be identified in very high-resolution (VHR) satellite imagery, large-scale automated detection remains challenging due to a lack of annotated imagery, variability in image quality and environmental conditions, and the cost of building robust machine learning pipelines over massive remote sensing archives. We present a semi-automated approach for surfacing possible whale detections in VHR imagery using a statistical anomaly detection method that flags spatial outliers, i.e. "interesting points". We pair this detector with a web-based labeling interface designed to enable experts to quickly annotate the interesting points. We evaluate our system on three benchmark scenes with known whale annotations and achieve recalls of 90.3% to 96.4%, while reducing the area requiring expert inspection by up to 99.8% -- from over 1,000 sq km to less than 2 sq km in some cases. Our method does not rely on labeled training data and offers a scalable first step toward future machine-assisted marine mammal monitoring from space. We have open sourced this pipeline at https://github.com/microsoft/whales.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14709v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Caleb Robinson, Kimberly T. Goetz, Christin B. Khan, Meredith Sackett, Kathleen Leonard, Rahul Dodhia, Juan M. Lavista Ferres</dc:creator>
    </item>
    <item>
      <title>Camera Movement Classification in Historical Footage: A Comparative Study of Deep Video Models</title>
      <link>https://arxiv.org/abs/2510.14713</link>
      <description>arXiv:2510.14713v1 Announce Type: cross 
Abstract: Camera movement conveys spatial and narrative information essential for understanding video content. While recent camera movement classification (CMC) methods perform well on modern datasets, their generalization to historical footage remains unexplored. This paper presents the first systematic evaluation of deep video CMC models on archival film material. We summarize representative methods and datasets, highlighting differences in model design and label definitions. Five standard video classification models are assessed on the HISTORIAN dataset, which includes expert-annotated World War II footage. The best-performing model, Video Swin Transformer, achieves 80.25% accuracy, showing strong convergence despite limited training data. Our findings highlight the challenges and potential of adapting existing models to low-quality video and motivate future work combining diverse input modalities and temporal architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14713v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>eess.IV</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tingyu Lin, Armin Dadras, Florian Kleber, Robert Sablatnig</dc:creator>
    </item>
    <item>
      <title>Seesaw: Accelerating Training by Balancing Learning Rate and Batch Size Scheduling</title>
      <link>https://arxiv.org/abs/2510.14717</link>
      <description>arXiv:2510.14717v1 Announce Type: cross 
Abstract: Increasing the batch size during training -- a ''batch ramp'' -- is a promising strategy to accelerate large language model pretraining. While for SGD, doubling the batch size can be equivalent to halving the learning rate, the optimal strategy for adaptive optimizers like Adam is less clear. As a result, any batch-ramp scheduling, if used at all, is typically tuned heuristically. This work develops a principled framework for batch-size scheduling and introduces Seesaw: whenever a standard scheduler would halve the learning rate, Seesaw instead multiplies it by $1/\sqrt{2}$ and doubles the batch size, preserving loss dynamics while reducing serial steps. Theoretically, we provide, to our knowledge, the first finite-sample proof of equivalence between learning-rate decay and batch-size ramp-up for SGD on noisy linear regression, and we extend this equivalence to normalized SGD, a tractable proxy for Adam, under a variance-dominated regime observed in practice. Empirically, on 150M/300M/600M-parameter models trained at Chinchilla scale using a constant (critical) batch size, Seesaw matches cosine decay at equal FLOPs while reducing wall-clock time by $\approx 36\%$, approaching the theoretical limit implied by our analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14717v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandru Meterez, Depen Morwani, Jingfeng Wu, Costin-Andrei Oncescu, Cengiz Pehlevan, Sham Kakade</dc:creator>
    </item>
    <item>
      <title>DEXTER: Diffusion-Guided EXplanations with TExtual Reasoning for Vision Models</title>
      <link>https://arxiv.org/abs/2510.14741</link>
      <description>arXiv:2510.14741v1 Announce Type: cross 
Abstract: Understanding and explaining the behavior of machine learning models is essential for building transparent and trustworthy AI systems. We introduce DEXTER, a data-free framework that employs diffusion models and large language models to generate global, textual explanations of visual classifiers. DEXTER operates by optimizing text prompts to synthesize class-conditional images that strongly activate a target classifier. These synthetic samples are then used to elicit detailed natural language reports that describe class-specific decision patterns and biases. Unlike prior work, DEXTER enables natural language explanation about a classifier's decision process without access to training data or ground-truth labels. We demonstrate DEXTER's flexibility across three tasks-activation maximization, slice discovery and debiasing, and bias explanation-each illustrating its ability to uncover the internal mechanisms of visual classifiers. Quantitative and qualitative evaluations, including a user study, show that DEXTER produces accurate, interpretable outputs. Experiments on ImageNet, Waterbirds, CelebA, and FairFaces confirm that DEXTER outperforms existing approaches in global model explanation and class-level bias reporting. Code is available at https://github.com/perceivelab/dexter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14741v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simone Carnemolla, Matteo Pennisi, Sarinda Samarasinghe, Giovanni Bellitto, Simone Palazzo, Daniela Giordano, Mubarak Shah, Concetto Spampinato</dc:creator>
    </item>
    <item>
      <title>Beyond Multi-Token Prediction: Pretraining LLMs with Future Summaries</title>
      <link>https://arxiv.org/abs/2510.14751</link>
      <description>arXiv:2510.14751v1 Announce Type: cross 
Abstract: Next-token prediction (NTP) has driven the success of large language models (LLMs), but it struggles with long-horizon reasoning, planning, and creative writing, with these limitations largely attributed to teacher-forced training. Multi-token prediction (MTP) partially mitigates these issues by predicting several future tokens at once, but it mostly captures short-range dependencies and offers limited improvement. We propose future summary prediction (FSP), which trains an auxiliary head to predict a compact representation of the long-term future, preserving information relevant for long-form generations. We explore two variants of FSP: handcrafted summaries, for example, a bag of words summary of the future of the sequence, and learned summaries, which use embeddings produced by a reverse language model trained from right to left. Large-scale pretraining experiments (3B and 8B-parameter models) demonstrate that FSP provides improvements over both NTP and MTP across math, reasoning, and coding benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14751v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Divyat Mahajan, Sachin Goyal, Badr Youbi Idrissi, Mohammad Pezeshki, Ioannis Mitliagkas, David Lopez-Paz, Kartik Ahuja</dc:creator>
    </item>
    <item>
      <title>COIG-Writer: A High-Quality Dataset for Chinese Creative Writing with Thought Processes</title>
      <link>https://arxiv.org/abs/2510.14763</link>
      <description>arXiv:2510.14763v1 Announce Type: cross 
Abstract: Large language models exhibit systematic deficiencies in creative writing, particularly in non-English contexts where training data is scarce and lacks process-level supervision. We present COIG-Writer, a novel Chinese creative writing dataset that captures both diverse outputs and their underlying thought processes through systematic reverse-engineering of high-quality texts. Unlike existing datasets that provide only input-output pairs, COIG-Writer comprises 1,665 meticulously curated triplets spanning 51 genres, each containing: (1) a reverse-engineered prompt, (2) detailed creative reasoning documenting decision-making processes, and (3) the final text. Through comprehensive experiments, we identify a two-component model of creative writing: narrative logic (provided by process supervision) and linguistic expression (maintained by general-purpose data). Our findings reveal three critical insights: (1) Process supervision is highly effective but requires stabilization with general data. A ratio of at least one creative sample to twelve general samples is needed to achieve optimal performance; below this threshold, the win rate progressively degrades (from 62.75% down to 35.78%)., (2) creative capabilities are culturally-bound with no cross-lingual transfer (89.26pp gap between Chinese and English performance), and (3) lexical diversity inversely correlates with creative quality (TTR paradox), suggesting high diversity signals compensatory behavior for logical deficiencies. These findings establish that creative excellence emerges from the interaction between logical scaffolding and linguistic grounding, analogous to how mathematical reasoning enhances but cannot replace linguistic competence in foundation models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14763v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Yunwen Li, Shuangshuang Ying, Xingwei Qu, Xin Li, Sheng Jin, Minghao Liu, Zhoufutu Wen, Tianyu Zheng, Xeron Du, Qiguang Chen, Jiajun Shi, Wangchunshu Zhou, Jiazhan Feng, Wanjun Zhong, Libo Qin, Stephen Huang, Wanxiang Che, Chenghua Lin, Eli Zhang</dc:creator>
    </item>
    <item>
      <title>Inpainting the Red Planet: Diffusion Models for the Reconstruction of Martian Environments in Virtual Reality</title>
      <link>https://arxiv.org/abs/2510.14765</link>
      <description>arXiv:2510.14765v1 Announce Type: cross 
Abstract: Space exploration increasingly relies on Virtual Reality for several tasks, such as mission planning, multidisciplinary scientific analysis, and astronaut training. A key factor for the reliability of the simulations is having accurate 3D representations of planetary terrains. Extraterrestrial heightmaps derived from satellite imagery often contain missing values due to acquisition and transmission constraints. Mars is among the most studied planets beyond Earth, and its extensive terrain datasets make the Martian surface reconstruction a valuable task, although many areas remain unmapped. Deep learning algorithms can support void-filling tasks; however, whereas Earth's comprehensive datasets enables the use of conditional methods, such approaches cannot be applied to Mars. Current approaches rely on simpler interpolation techniques which, however, often fail to preserve geometric coherence. In this work, we propose a method for reconstructing the surface of Mars based on an unconditional diffusion model. Training was conducted on an augmented dataset of 12000 Martian heightmaps derived from NASA's HiRISE survey. A non-homogeneous rescaling strategy captures terrain features across multiple scales before resizing to a fixed 128x128 model resolution. We compared our method against established void-filling and inpainting techniques, including Inverse Distance Weighting, kriging, and Navier-Stokes algorithm, on an evaluation set of 1000 samples. Results show that our approach consistently outperforms these methods in terms of reconstruction accuracy (4-15% on RMSE) and perceptual similarity (29-81% on LPIPS) with the original data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14765v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.GR</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giuseppe Lorenzo Catalano, Agata Marta Soccini</dc:creator>
    </item>
    <item>
      <title>Finding Answers in Thought Matters: Revisiting Evaluation on Large Language Models with Reasoning</title>
      <link>https://arxiv.org/abs/2510.14773</link>
      <description>arXiv:2510.14773v1 Announce Type: cross 
Abstract: Evaluating generative models, such as large language models (LLMs), commonly involves question-answering tasks where the final answer is selected based on probability of answer choices. On the other hand, for models requiring reasoning, the method of answer extraction plays a critical role. Our research reveals that the performance of reasoning models and their final answer distributions are highly sensitive to the answer extraction algorithm employed. In order to mitigate this, we propose a basic framework: Answer Regeneration. The method uses an additional model inference, providing the prior input and output prefaced by the prompt "Answer:". The final answer is then selected or extracted from the regenerated output. We show that this extraction-rule-agnostic approach exhibits improved performance and enhanced robustness. Furthermore, we have applied this framework to general math problems and open-ended question answering tasks. Our analysis and this framework could offer a more reliable results for model evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14773v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hwiyeol Jo, Joosung Lee, Jaehone Lee, Sang-Woo Lee, Joonsuk Park, Kang Min Yoo</dc:creator>
    </item>
    <item>
      <title>Cross-Scenario Unified Modeling of User Interests at Billion Scale</title>
      <link>https://arxiv.org/abs/2510.14788</link>
      <description>arXiv:2510.14788v1 Announce Type: cross 
Abstract: User interests on content platforms are inherently diverse, manifesting through complex behavioral patterns across heterogeneous scenarios such as search, feed browsing, and content discovery. Traditional recommendation systems typically prioritize business metric optimization within isolated specific scenarios, neglecting cross-scenario behavioral signals and struggling to integrate advanced techniques like LLMs at billion-scale deployments, which finally limits their ability to capture holistic user interests across platform touchpoints. We propose RED-Rec, an LLM-enhanced hierarchical Recommender Engine for Diversified scenarios, tailored for industry-level content recommendation systems. RED-Rec unifies user interest representations across multiple behavioral contexts by aggregating and synthesizing actions from varied scenarios, resulting in comprehensive item and user modeling. At its core, a two-tower LLM-powered framework enables nuanced, multifaceted representations with deployment efficiency, and a scenario-aware dense mixing and querying policy effectively fuses diverse behavioral signals to capture cross-scenario user intent patterns and express fine-grained, context-specific intents during serving. We validate RED-Rec through online A/B testing on hundreds of millions of users in RedNote through online A/B testing, showing substantial performance gains in both content recommendation and advertisement targeting tasks. We further introduce a million-scale sequential recommendation dataset, RED-MMU, for comprehensive offline training and evaluation. Our work advances unified user modeling, unlocking deeper personalization and fostering more meaningful user engagement in large-scale UGC platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14788v1</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Manjie Xu, Cheng Chen, Xin Jia, Jingyi Zhou, Yongji Wu, Zejian Wang, Chi Zhang, Kai Zuo, Yibo Chen, Xu Tang, Yao Hu, Yixin Zhu</dc:creator>
    </item>
    <item>
      <title>Morphology-Aware Prognostic model for Five-Year Survival Prediction in Colorectal Cancer from H&amp;E Whole Slide Images</title>
      <link>https://arxiv.org/abs/2510.14800</link>
      <description>arXiv:2510.14800v1 Announce Type: cross 
Abstract: Colorectal cancer (CRC) remains the third most prevalent malignancy globally, with approximately 154,000 new cases and 54,000 projected deaths anticipated for 2025. The recent advancement of foundation models in computational pathology has been largely propelled by task agnostic methodologies that can overlook organ-specific crucial morphological patterns that represent distinct biological processes that can fundamentally influence tumor behavior, therapeutic response, and patient outcomes. The aim of this study is to develop a novel, interpretable AI model, PRISM (Prognostic Representation of Integrated Spatial Morphology), that incorporates a continuous variability spectrum within each distinct morphology to characterize phenotypic diversity and reflecting the principle that malignant transformation occurs through incremental evolutionary processes rather than abrupt phenotypic shifts. PRISM is trained on 8.74 million histological images extracted from surgical resection specimens of 424 patients with stage III CRC. PRISM achieved superior prognostic performance for five-year OS (AUC = 0.70 +- 0.04; accuracy = 68.37% +- 4.75%; HR = 3.34, 95% CI = 2.28-4.90; p &lt; 0.0001), outperforming existing CRC-specific methods by 15% and AI foundation models by ~23% accuracy. It showed sex-agnostic robustness (AUC delta = 0.02; accuracy delta = 0.15%) and stable performance across clinicopathological subgroups, with minimal accuracy fluctuation (delta = 1.44%) between 5FU/LV and CPT-11/5FU/LV regimens, replicating the Alliance cohort finding of no survival difference between treatments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14800v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Usama Sajjad, Abdul Rehman Akbar, Ziyu Su, Deborah Knight, Wendy L. Frankel, Metin N. Gurcan, Wei Chen, Muhammad Khalid Khan Niazi</dc:creator>
    </item>
    <item>
      <title>Scaling Artificial Intelligence for Multi-Tumor Early Detection with More Reports, Fewer Masks</title>
      <link>https://arxiv.org/abs/2510.14803</link>
      <description>arXiv:2510.14803v1 Announce Type: cross 
Abstract: Early tumor detection save lives. Each year, more than 300 million computed tomography (CT) scans are performed worldwide, offering a vast opportunity for effective cancer screening. However, detecting small or early-stage tumors on these CT scans remains challenging, even for experts. Artificial intelligence (AI) models can assist by highlighting suspicious regions, but training such models typically requires extensive tumor masks--detailed, voxel-wise outlines of tumors manually drawn by radiologists. Drawing these masks is costly, requiring years of effort and millions of dollars. In contrast, nearly every CT scan in clinical practice is already accompanied by medical reports describing the tumor's size, number, appearance, and sometimes, pathology results--information that is rich, abundant, and often underutilized for AI training. We introduce R-Super, which trains AI to segment tumors that match their descriptions in medical reports. This approach scales AI training with large collections of readily available medical reports, substantially reducing the need for manually drawn tumor masks. When trained on 101,654 reports, AI models achieved performance comparable to those trained on 723 masks. Combining reports and masks further improved sensitivity by +13% and specificity by +8%, surpassing radiologists in detecting five of the seven tumor types. Notably, R-Super enabled segmentation of tumors in the spleen, gallbladder, prostate, bladder, uterus, and esophagus, for which no public masks or AI models previously existed. This study challenges the long-held belief that large-scale, labor-intensive tumor mask creation is indispensable, establishing a scalable and accessible path toward early detection across diverse tumor types.
  We plan to release our trained models, code, and dataset at https://github.com/MrGiovanni/R-Super</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14803v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Pedro R. A. S. Bassi, Xinze Zhou, Wenxuan Li, Szymon P{\l}otka, Jieneng Chen, Qi Chen, Zheren Zhu, Jakub Prz\k{a}do, Ibrahim E. Hamac{\i}, Sezgin Er, Yuhan Wang, Ashwin Kumar, Bjoern Menze, Jaros{\l}aw B. \'Cwik{\l}a, Yuyin Zhou, Akshay S. Chaudhari, Curtis P. Langlotz, Sergio Decherchi, Andrea Cavalli, Kang Wang, Yang Yang, Alan L. Yuille, Zongwei Zhou</dc:creator>
    </item>
    <item>
      <title>RL-100: Performant Robotic Manipulation with Real-World Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2510.14830</link>
      <description>arXiv:2510.14830v1 Announce Type: cross 
Abstract: Real-world robotic manipulation in homes and factories demands reliability, efficiency, and robustness that approach or surpass skilled human operators. We present RL-100, a real-world reinforcement learning training framework built on diffusion visuomotor policies trained bu supervised learning. RL-100 introduces a three-stage pipeline. First, imitation learning leverages human priors. Second, iterative offline reinforcement learning uses an Offline Policy Evaluation procedure, abbreviated OPE, to gate PPO-style updates that are applied in the denoising process for conservative and reliable improvement. Third, online reinforcement learning eliminates residual failure modes. An additional lightweight consistency distillation head compresses the multi-step sampling process in diffusion into a single-step policy, enabling high-frequency control with an order-of-magnitude reduction in latency while preserving task performance. The framework is task-, embodiment-, and representation-agnostic and supports both 3D point clouds and 2D RGB inputs, a variety of robot platforms, and both single-step and action-chunk policies. We evaluate RL-100 on seven real-robot tasks spanning dynamic rigid-body control, such as Push-T and Agile Bowling, fluids and granular pouring, deformable cloth folding, precise dexterous unscrewing, and multi-stage orange juicing. RL-100 attains 100\% success across evaluated trials for a total of 900 out of 900 episodes, including up to 250 out of 250 consecutive trials on one task. The method achieves near-human teleoperation or better time efficiency and demonstrates multi-hour robustness with uninterrupted operation lasting up to two hours.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14830v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kun Lei, Huanyu Li, Dongjie Yu, Zhenyu Wei, Lingxiao Guo, Zhennan Jiang, Ziyu Wang, Shiyu Liang, Huazhe Xu</dc:creator>
    </item>
    <item>
      <title>Benchmarking Multimodal Large Language Models for Face Recognition</title>
      <link>https://arxiv.org/abs/2510.14866</link>
      <description>arXiv:2510.14866v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) have achieved remarkable performance across diverse vision-and-language tasks. However, their potential in face recognition remains underexplored. In particular, the performance of open-source MLLMs needs to be evaluated and compared with existing face recognition models on standard benchmarks with similar protocol. In this work, we present a systematic benchmark of state-of-the-art MLLMs for face recognition on several face recognition datasets, including LFW, CALFW, CPLFW, CFP, AgeDB and RFW. Experimental results reveal that while MLLMs capture rich semantic cues useful for face-related tasks, they lag behind specialized models in high-precision recognition scenarios in zero-shot applications. This benchmark provides a foundation for advancing MLLM-based face recognition, offering insights for the design of next-generation models with higher accuracy and generalization. The source code of our benchmark is publicly available in the project page.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14866v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hatef Otroshi Shahreza, S\'ebastien Marcel</dc:creator>
    </item>
    <item>
      <title>Predicting kernel regression learning curves from only raw data statistics</title>
      <link>https://arxiv.org/abs/2510.14878</link>
      <description>arXiv:2510.14878v1 Announce Type: cross 
Abstract: We study kernel regression with common rotation-invariant kernels on real datasets including CIFAR-5m, SVHN, and ImageNet. We give a theoretical framework that predicts learning curves (test risk vs. sample size) from only two measurements: the empirical data covariance matrix and an empirical polynomial decomposition of the target function $f_*$. The key new idea is an analytical approximation of a kernel's eigenvalues and eigenfunctions with respect to an anisotropic data distribution. The eigenfunctions resemble Hermite polynomials of the data, so we call this approximation the Hermite eigenstructure ansatz (HEA). We prove the HEA for Gaussian data, but we find that real image data is often "Gaussian enough" for the HEA to hold well in practice, enabling us to predict learning curves by applying prior results relating kernel eigenstructure to test risk. Extending beyond kernel regression, we empirically find that MLPs in the feature-learning regime learn Hermite polynomials in the order predicted by the HEA. Our HEA framework is a proof of concept that an end-to-end theory of learning which maps dataset structure all the way to model performance is possible for nontrivial learning algorithms on real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14878v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dhruva Karkada, Joseph Turnbull, Yuxi Liu, James B. Simon</dc:creator>
    </item>
    <item>
      <title>Learning When Not to Learn: Risk-Sensitive Abstention in Bandits with Unbounded Rewards</title>
      <link>https://arxiv.org/abs/2510.14884</link>
      <description>arXiv:2510.14884v1 Announce Type: cross 
Abstract: In high-stakes AI applications, even a single action can cause irreparable damage. However, nearly all of sequential decision-making theory assumes that all errors are recoverable (e.g., by bounding rewards). Standard bandit algorithms that explore aggressively may cause irreparable damage when this assumption fails. Some prior work avoids irreparable errors by asking for help from a mentor, but a mentor may not always be available. In this work, we formalize a model of learning with unbounded rewards without a mentor as a two-action contextual bandit with an abstain option: at each round the agent observes an input and chooses either to abstain (always 0 reward) or to commit (execute a preexisting task policy). Committing yields rewards that are upper-bounded but can be arbitrarily negative, and the commit reward is assumed Lipschitz in the input. We propose a caution-based algorithm that learns when not to learn: it chooses a trusted region and commits only where the available evidence does not already certify harm. Under these conditions and i.i.d. inputs, we establish sublinear regret guarantees, theoretically demonstrating the effectiveness of cautious exploration for deploying learning agents safely in high-stakes environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14884v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarah Liaw, Benjamin Plaut</dc:creator>
    </item>
    <item>
      <title>Detecting Early and Implicit Suicidal Ideation via Longitudinal and Information Environment Signals on Social Media</title>
      <link>https://arxiv.org/abs/2510.14889</link>
      <description>arXiv:2510.14889v1 Announce Type: cross 
Abstract: On social media, many individuals experiencing suicidal ideation (SI) do not disclose their distress explicitly. Instead, signs may surface indirectly through everyday posts or peer interactions. Detecting such implicit signals early is critical but remains challenging. We frame early and implicit SI as a forward-looking prediction task and develop a computational framework that models a user's information environment, consisting of both their longitudinal posting histories as well as the discourse of their socially proximal peers. We adopted a composite network centrality measure to identify top neighbors of a user, and temporally aligned the user's and neighbors' interactions -- integrating the multi-layered signals in a fine-tuned DeBERTa-v3 model. In a Reddit study of 1,000 (500 Case and 500 Control) users, our approach improves early and implicit SI detection by 15% over individual-only baselines. These findings highlight that peer interactions offer valuable predictive signals and carry broader implications for designing early detection systems that capture indirect as well as masked expressions of risk in online environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14889v1</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soorya Ram Shimgekar, Ruining Zhao, Agam Goyal, Violeta J. Rodriguez, Paul A. Bloom, Hari Sundaram, Koustuv Saha</dc:creator>
    </item>
    <item>
      <title>Reasoning with Sampling: Your Base Model is Smarter Than You Think</title>
      <link>https://arxiv.org/abs/2510.14901</link>
      <description>arXiv:2510.14901v1 Announce Type: cross 
Abstract: Frontier reasoning models have exhibited incredible capabilities across a wide array of disciplines, driven by posttraining large language models (LLMs) with reinforcement learning (RL). However, despite the widespread success of this paradigm, much of the literature has been devoted to disentangling truly novel behaviors that emerge during RL but are not present in the base models. In our work, we approach this question from a different angle, instead asking whether comparable reasoning capabilites can be elicited from base models at inference time by pure sampling, without any additional training. Inspired by Markov chain Monte Carlo (MCMC) techniques for sampling from sharpened distributions, we propose a simple iterative sampling algorithm leveraging the base models' own likelihoods. Over different base models, we show that our algorithm offers substantial boosts in reasoning that nearly match and even outperform those from RL on a wide variety of single-shot tasks, including MATH500, HumanEval, and GPQA. Moreover, our sampler avoids the collapse in diversity over multiple samples that is characteristic of RL-posttraining. Crucially, our method does not require training, curated datasets, or a verifier, suggesting broad applicability beyond easily verifiable domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14901v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aayush Karan, Yilun Du</dc:creator>
    </item>
    <item>
      <title>MaskCaptioner : Learning to Jointly Segment and Caption Object Trajectories in Videos</title>
      <link>https://arxiv.org/abs/2510.14904</link>
      <description>arXiv:2510.14904v1 Announce Type: cross 
Abstract: Dense Video Object Captioning (DVOC) is the task of jointly detecting, tracking, and captioning object trajectories in a video, requiring the ability to understand spatio-temporal details and describe them in natural language. Due to the complexity of the task and the high cost associated with manual annotation, previous approaches resort to disjoint training strategies, potentially leading to suboptimal performance. To circumvent this issue, we propose to generate captions about spatio-temporally localized entities leveraging a state-of-the-art VLM. By extending the LVIS and LV-VIS datasets with our synthetic captions (LVISCap and LV-VISCap), we train MaskCaptioner, an end-to-end model capable of jointly detecting, segmenting, tracking and captioning object trajectories. Moreover, with pretraining on LVISCap and LV-VISCap, MaskCaptioner achieves state-of-the-art DVOC results on three existing benchmarks, VidSTG, VLN and BenSMOT. The datasets and code are available at https://www.gabriel.fiastre.fr/maskcaptioner/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14904v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Gabriel Fiastre, Antoine Yang, Cordelia Schmid</dc:creator>
    </item>
    <item>
      <title>Predicting Task Performance with Context-aware Scaling Laws</title>
      <link>https://arxiv.org/abs/2510.14919</link>
      <description>arXiv:2510.14919v1 Announce Type: cross 
Abstract: Scaling laws have transformed our understanding of large language models by linking upstream metrics like cross-entropy loss to design factors such as model size, training data, and compute. However, these conventional laws fail to capture downstream task performance, where context plays a critical role. In this work, we propose a straightforward, interpretable framework that jointly models downstream performance as a function of the training compute and the provided context. We empirically validate our framework by fitting it on the observed downstream performance of extended-context variants of Llama-2-7B and Llama-2-13B across 65,500 unique instances spanning three tasks: arithmetic reasoning, common sense reasoning, and machine translation. Our results demonstrate that our framework accurately models in-distribution downstream performance, generalizes across three orders of magnitude in training compute, and reliably extrapolates performance as the amount of context increases. These findings offer valuable insights into the interplay between training compute and context utilization, providing guidance for designing more efficient long-context LLMs for diverse downstream tasks. Our code is available at https://github.com/wang-research-lab/context-scaling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14919v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyle Montgomery, David Park, Jianhong Tu, Michael Bendersky, Beliz Gunel, Dawn Song, Chenguang Wang</dc:creator>
    </item>
    <item>
      <title>Circuit Insights: Towards Interpretability Beyond Activations</title>
      <link>https://arxiv.org/abs/2510.14936</link>
      <description>arXiv:2510.14936v1 Announce Type: cross 
Abstract: The fields of explainable AI and mechanistic interpretability aim to uncover the internal structure of neural networks, with circuit discovery as a central tool for understanding model computations. Existing approaches, however, rely on manual inspection and remain limited to toy tasks. Automated interpretability offers scalability by analyzing isolated features and their activations, but it often misses interactions between features and depends strongly on external LLMs and dataset quality. Transcoders have recently made it possible to separate feature attributions into input-dependent and input-invariant components, providing a foundation for more systematic circuit analysis. Building on this, we propose WeightLens and CircuitLens, two complementary methods that go beyond activation-based analysis. WeightLens interprets features directly from their learned weights, removing the need for explainer models or datasets while matching or exceeding the performance of existing methods on context-independent features. CircuitLens captures how feature activations arise from interactions between components, revealing circuit-level dynamics that activation-only approaches cannot identify. Together, these methods increase interpretability robustness and enhance scalable mechanistic analysis of circuits while maintaining efficiency and quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14936v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elena Golimblevskaia, Aakriti Jain, Bruno Puri, Ammar Ibrahim, Wojciech Samek, Sebastian Lapuschkin</dc:creator>
    </item>
    <item>
      <title>LaSeR: Reinforcement Learning with Last-Token Self-Rewarding</title>
      <link>https://arxiv.org/abs/2510.14943</link>
      <description>arXiv:2510.14943v1 Announce Type: cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a core paradigm for enhancing the reasoning capabilities of Large Language Models (LLMs). To address the lack of verification signals at test time, prior studies incorporate the training of model's self-verification capability into the standard RLVR process, thereby unifying reasoning and verification capabilities within a single LLM. However, previous practice requires the LLM to sequentially generate solutions and self-verifications using two separate prompt templates, which significantly reduces efficiency. In this work, we theoretically reveal that the closed-form solution to the RL objective of self-verification can be reduced to a remarkably simple form: the true reasoning reward of a solution is equal to its last-token self-rewarding score, which is computed as the difference between the policy model's next-token log-probability assigned to any pre-specified token at the solution's last token and a pre-calculated constant, scaled by the KL coefficient. Based on this insight, we propose LaSeR (Reinforcement Learning with Last-Token Self-Rewarding), an algorithm that simply augments the original RLVR loss with a MSE loss that aligns the last-token self-rewarding scores with verifier-based reasoning rewards, jointly optimizing the reasoning and self-rewarding capabilities of LLMs. The optimized self-rewarding scores can be utilized in both training and testing to enhance model performance. Notably, our algorithm derives these scores from the predicted next-token probability distribution of the last token immediately after generation, incurring only the minimal extra cost of one additional token inference. Experiments show that our method not only improves the model's reasoning performance but also equips it with remarkable self-rewarding capability, thereby boosting its inference-time scaling performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14943v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenkai Yang, Weijie Liu, Ruobing Xie, Yiju Guo, Lulu Wu, Saiyong Yang, Yankai Lin</dc:creator>
    </item>
    <item>
      <title>MetaBench: A Multi-task Benchmark for Assessing LLMs in Metabolomics</title>
      <link>https://arxiv.org/abs/2510.14944</link>
      <description>arXiv:2510.14944v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities on general text; however, their proficiency in specialized scientific domains that require deep, interconnected knowledge remains largely uncharacterized. Metabolomics presents unique challenges with its complex biochemical pathways, heterogeneous identifier systems, and fragmented databases. To systematically evaluate LLM capabilities in this domain, we introduce MetaBench, the first benchmark for metabolomics assessment. Curated from authoritative public resources, MetaBench evaluates five capabilities essential for metabolomics research: knowledge, understanding, grounding, reasoning, and research. Our evaluation of 25 open- and closed-source LLMs reveals distinct performance patterns across metabolomics tasks: while models perform well on text generation tasks, cross-database identifier grounding remains challenging even with retrieval augmentation. Model performance also decreases on long-tail metabolites with sparse annotations. With MetaBench, we provide essential infrastructure for developing and evaluating metabolomics AI systems, enabling systematic progress toward reliable computational tools for metabolomics research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14944v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxing Lu, Xukai Zhao, J. Ben Tamo, Micky C. Nnamdi, Rui Peng, Shuang Zeng, Xingyu Hu, Jinzhuo Wang, May D. Wang</dc:creator>
    </item>
    <item>
      <title>Architecture Is All You Need: Diversity-Enabled Sweet Spots for Robust Humanoid Locomotion</title>
      <link>https://arxiv.org/abs/2510.14947</link>
      <description>arXiv:2510.14947v1 Announce Type: cross 
Abstract: Robust humanoid locomotion in unstructured environments requires architectures that balance fast low-level stabilization with slower perceptual decision-making. We show that a simple layered control architecture (LCA), a proprioceptive stabilizer running at high rate, coupled with a compact low-rate perceptual policy, enables substantially more robust performance than monolithic end-to-end designs, even when using minimal perception encoders. Through a two-stage training curriculum (blind stabilizer pretraining followed by perceptual fine-tuning), we demonstrate that layered policies consistently outperform one-stage alternatives in both simulation and hardware. On a Unitree G1 humanoid, our approach succeeds across stair and ledge tasks where one-stage perceptual policies fail. These results highlight that architectural separation of timescales, rather than network scale or complexity, is the key enabler for robust perception-conditioned locomotion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14947v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Blake Werner, Lizhi Yang, Aaron D. Ames</dc:creator>
    </item>
    <item>
      <title>RealDPO: Real or Not Real, that is the Preference</title>
      <link>https://arxiv.org/abs/2510.14955</link>
      <description>arXiv:2510.14955v1 Announce Type: cross 
Abstract: Video generative models have recently achieved notable advancements in synthesis quality. However, generating complex motions remains a critical challenge, as existing models often struggle to produce natural, smooth, and contextually consistent movements. This gap between generated and real-world motions limits their practical applicability. To address this issue, we introduce RealDPO, a novel alignment paradigm that leverages real-world data as positive samples for preference learning, enabling more accurate motion synthesis. Unlike traditional supervised fine-tuning (SFT), which offers limited corrective feedback, RealDPO employs Direct Preference Optimization (DPO) with a tailored loss function to enhance motion realism. By contrasting real-world videos with erroneous model outputs, RealDPO enables iterative self-correction, progressively refining motion quality. To support post-training in complex motion synthesis, we propose RealAction-5K, a curated dataset of high-quality videos capturing human daily activities with rich and precise motion details. Extensive experiments demonstrate that RealDPO significantly improves video quality, text alignment, and motion realism compared to state-of-the-art models and existing preference optimization techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14955v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guo Cheng, Danni Yang, Ziqi Huang, Jianlou Si, Chenyang Si, Ziwei Liu</dc:creator>
    </item>
    <item>
      <title>CBF-RL: Safety Filtering Reinforcement Learning in Training with Control Barrier Functions</title>
      <link>https://arxiv.org/abs/2510.14959</link>
      <description>arXiv:2510.14959v1 Announce Type: cross 
Abstract: Reinforcement learning (RL), while powerful and expressive, can often prioritize performance at the expense of safety. Yet safety violations can lead to catastrophic outcomes in real-world deployments. Control Barrier Functions (CBFs) offer a principled method to enforce dynamic safety -- traditionally deployed \emph{online} via safety filters. While the result is safe behavior, the fact that the RL policy does not have knowledge of the CBF can lead to conservative behaviors. This paper proposes CBF-RL, a framework for generating safe behaviors with RL by enforcing CBFs \emph{in training}. CBF-RL has two key attributes: (1) minimally modifying a nominal RL policy to encode safety constraints via a CBF term, (2) and safety filtering of the policy rollouts in training. Theoretically, we prove that continuous-time safety filters can be deployed via closed-form expressions on discrete-time roll-outs. Practically, we demonstrate that CBF-RL internalizes the safety constraints in the learned policy -- both enforcing safer actions and biasing towards safer rewards -- enabling safe deployment without the need for an online safety filter. We validate our framework through ablation studies on navigation tasks and on the Unitree G1 humanoid robot, where CBF-RL enables safer exploration, faster convergence, and robust performance under uncertainty, enabling the humanoid robot to avoid obstacles and climb stairs safely in real-world settings without a runtime safety filter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14959v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lizhi Yang, Blake Werner, Massimiliano de Sa Aaron D. Ames</dc:creator>
    </item>
    <item>
      <title>C4D: 4D Made from 3D through Dual Correspondences</title>
      <link>https://arxiv.org/abs/2510.14960</link>
      <description>arXiv:2510.14960v1 Announce Type: cross 
Abstract: Recovering 4D from monocular video, which jointly estimates dynamic geometry and camera poses, is an inevitably challenging problem. While recent pointmap-based 3D reconstruction methods (e.g., DUSt3R) have made great progress in reconstructing static scenes, directly applying them to dynamic scenes leads to inaccurate results. This discrepancy arises because moving objects violate multi-view geometric constraints, disrupting the reconstruction. To address this, we introduce C4D, a framework that leverages temporal Correspondences to extend existing 3D reconstruction formulation to 4D. Specifically, apart from predicting pointmaps, C4D captures two types of correspondences: short-term optical flow and long-term point tracking. We train a dynamic-aware point tracker that provides additional mobility information, facilitating the estimation of motion masks to separate moving elements from the static background, thus offering more reliable guidance for dynamic scenes. Furthermore, we introduce a set of dynamic scene optimization objectives to recover per-frame 3D geometry and camera parameters. Simultaneously, the correspondences lift 2D trajectories into smooth 3D trajectories, enabling fully integrated 4D reconstruction. Experiments show that our framework achieves complete 4D recovery and demonstrates strong performance across multiple downstream tasks, including depth estimation, camera pose estimation, and point tracking. Project Page: https://littlepure2333.github.io/C4D</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14960v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shizun Wang, Zhenxiang Jiang, Xingyi Yang, Xinchao Wang</dc:creator>
    </item>
    <item>
      <title>Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents</title>
      <link>https://arxiv.org/abs/2510.14967</link>
      <description>arXiv:2510.14967v1 Announce Type: cross 
Abstract: Large language model (LLM)-based agents are increasingly trained with reinforcement learning (RL) to enhance their ability to interact with external environments through tool use, particularly in search-based settings that require multi-turn reasoning and knowledge acquisition. However, existing approaches typically rely on outcome-based rewards that are only provided at the final answer. This reward sparsity becomes particularly problematic in multi-turn settings, where long trajectories exacerbate two critical issues: (i) advantage collapse, where all rollouts receive identical rewards and provide no useful learning signals, and (ii) lack of fine-grained credit assignment, where dependencies between turns are obscured, especially in long-horizon tasks. In this paper, we propose Information Gain-based Policy Optimization (IGPO), a simple yet effective RL framework that provides dense and intrinsic supervision for multi-turn agent training. IGPO models each interaction turn as an incremental process of acquiring information about the ground truth, and defines turn-level rewards as the marginal increase in the policy's probability of producing the correct answer. Unlike prior process-level reward approaches that depend on external reward models or costly Monte Carlo estimation, IGPO derives intrinsic rewards directly from the model's own belief updates. These intrinsic turn-level rewards are combined with outcome-level supervision to form dense reward trajectories. Extensive experiments on both in-domain and out-of-domain benchmarks demonstrate that IGPO consistently outperforms strong baselines in multi-turn scenarios, achieving higher accuracy and improved sample efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14967v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Guoqing Wang, Sunhao Dai, Guangze Ye, Zeyu Gan, Wei Yao, Yong Deng, Xiaofeng Wu, Zhenzhe Ying</dc:creator>
    </item>
    <item>
      <title>RDD: Retrieval-Based Demonstration Decomposer for Planner Alignment in Long-Horizon Tasks</title>
      <link>https://arxiv.org/abs/2510.14968</link>
      <description>arXiv:2510.14968v1 Announce Type: cross 
Abstract: To tackle long-horizon tasks, recent hierarchical vision-language-action (VLAs) frameworks employ vision-language model (VLM)-based planners to decompose complex manipulation tasks into simpler sub-tasks that low-level visuomotor policies can easily handle. Typically, the VLM planner is finetuned to learn to decompose a target task. This finetuning requires target task demonstrations segmented into sub-tasks by either human annotation or heuristic rules. However, the heuristic subtasks can deviate significantly from the training data of the visuomotor policy, which degrades task performance. To address these issues, we propose a Retrieval-based Demonstration Decomposer (RDD) that automatically decomposes demonstrations into sub-tasks by aligning the visual features of the decomposed sub-task intervals with those from the training data of the low-level visuomotor policies. Our method outperforms the state-of-the-art sub-task decomposer on both simulation and real-world tasks, demonstrating robustness across diverse settings. Code and more results are available at rdd-neurips.github.io.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14968v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingxuan Yan, Yuping Wang, Zechun Liu, Jiachen Li</dc:creator>
    </item>
    <item>
      <title>LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent Training</title>
      <link>https://arxiv.org/abs/2510.14969</link>
      <description>arXiv:2510.14969v1 Announce Type: cross 
Abstract: Digital agents require diverse, large-scale UI trajectories to generalize across real-world tasks, yet collecting such data is prohibitively expensive in both human annotation, infra and engineering perspectives. To this end, we introduce $\textbf{UI-Simulator}$, a scalable paradigm that generates structured UI states and transitions to synthesize training trajectories at scale. Our paradigm integrates a digital world simulator for diverse UI states, a guided rollout process for coherent exploration, and a trajectory wrapper that produces high-quality and diverse trajectories for agent training. We further propose $\textbf{UI-Simulator-Grow}$, a targeted scaling strategy that enables more rapid and data-efficient scaling by prioritizing high-impact tasks and synthesizes informative trajectory variants. Experiments on WebArena and AndroidWorld show that UI-Simulator rivals or surpasses open-source agents trained on real UIs with significantly better robustness, despite using weaker teacher models. Moreover, UI-Simulator-Grow matches the performance of Llama-3-70B-Instruct using only Llama-3-8B-Instruct as the base model, highlighting the potential of targeted synthesis scaling paradigm to continuously and efficiently enhance the digital agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14969v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiming Wang, Da Yin, Yuedong Cui, Ruichen Zheng, Zhiqian Li, Zongyu Lin, Di Wu, Xueqing Wu, Chenchen Ye, Yu Zhou, Kai-Wei Chang</dc:creator>
    </item>
    <item>
      <title>TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar</title>
      <link>https://arxiv.org/abs/2510.14972</link>
      <description>arXiv:2510.14972v1 Announce Type: cross 
Abstract: Large language models (LLMs) for code rely on subword tokenizers, such as byte-pair encoding (BPE), learned from mixed natural language text and programming language code but driven by statistics rather than grammar. As a result, semantically identical code snippets can be tokenized differently depending on superficial factors such as whitespace or identifier naming. To measure the impact of this misalignment, we introduce TokDrift, a framework that applies semantic-preserving rewrite rules to create code variants differing only in tokenization. Across nine code LLMs, including large ones with over 30B parameters, even minor formatting changes can cause substantial shifts in model behavior. Layer-wise analysis shows that the issue originates in early embeddings, where subword segmentation fails to capture grammar token boundaries. Our findings identify misaligned tokenization as a hidden obstacle to reliable code understanding and generation, highlighting the need for grammar-aware tokenization for future code LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14972v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinxi Li, Yuntian Deng, Pengyu Nie</dc:creator>
    </item>
    <item>
      <title>Attention Is All You Need for KV Cache in Diffusion LLMs</title>
      <link>https://arxiv.org/abs/2510.14973</link>
      <description>arXiv:2510.14973v1 Announce Type: cross 
Abstract: This work studies how to adaptively recompute key-value (KV) caches for diffusion large language models (DLMs) to maximize prediction accuracy while minimizing decoding latency. Prior methods' decoders recompute QKV for all tokens at every denoising step and layer, despite KV states changing little across most steps, especially in shallow layers, leading to substantial redundancy. We make three observations: (1) distant ${\bf MASK}$ tokens primarily act as a length-bias and can be cached block-wise beyond the active prediction window; (2) KV dynamics increase with depth, suggesting that selective refresh starting from deeper layers is sufficient; and (3) the most-attended token exhibits the smallest KV drift, providing a conservative lower bound on cache change for other tokens. Building on these, we propose ${\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that jointly decides ${when}$ to refresh (via an attention-aware drift test on the most-attended token) and ${where}$ to refresh (via a depth-aware schedule that recomputes from a chosen layer onward while reusing shallow-layer caches and off-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs adaptive, layer-aware cache updates for diffusion LLMs, reducing redundant computation and accelerating decoding with negligible loss in generation quality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across mathematical reasoning and code generation tasks demonstrate consistent speedups: $8.7\times$ on GSM8K (256 tokens), $45.1\times$ on longer sequences, and $4.8\times$ on HumanEval, while consistently maintaining higher accuracy than the baseline. Our method achieves significantly higher throughput ($6.8\times$ on GSM8K) than existing confidence-based approaches while preserving generation quality, enabling practical deployment of diffusion LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14973v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Quan Nguyen-Tri, Mukul Ranjan, Zhiqiang Shen</dc:creator>
    </item>
    <item>
      <title>pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation</title>
      <link>https://arxiv.org/abs/2510.14974</link>
      <description>arXiv:2510.14974v1 Announce Type: cross 
Abstract: Few-step diffusion or flow-based generative models typically distill a velocity-predicting teacher into a student that predicts a shortcut towards denoised data. This format mismatch has led to complex distillation procedures that often suffer from a quality-diversity trade-off. To address this, we propose policy-based flow models ($\pi$-Flow). $\pi$-Flow modifies the output layer of a student flow model to predict a network-free policy at one timestep. The policy then produces dynamic flow velocities at future substeps with negligible overhead, enabling fast and accurate ODE integration on these substeps without extra network evaluations. To match the policy's ODE trajectory to the teacher's, we introduce a novel imitation distillation approach, which matches the policy's velocity to the teacher's along the policy's trajectory using a standard $\ell_2$ flow matching loss. By simply mimicking the teacher's behavior, $\pi$-Flow enables stable and scalable training and avoids the quality-diversity trade-off. On ImageNet 256$^2$, it attains a 1-NFE FID of 2.85, outperforming MeanFlow of the same DiT architecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, $\pi$-Flow achieves substantially better diversity than state-of-the-art few-step methods, while maintaining teacher-level quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14974v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hansheng Chen, Kai Zhang, Hao Tan, Leonidas Guibas, Gordon Wetzstein, Sai Bi</dc:creator>
    </item>
    <item>
      <title>WithAnyone: Towards Controllable and ID Consistent Image Generation</title>
      <link>https://arxiv.org/abs/2510.14975</link>
      <description>arXiv:2510.14975v1 Announce Type: cross 
Abstract: Identity-consistent generation has become an important focus in text-to-image research, with recent models achieving notable success in producing images aligned with a reference identity. Yet, the scarcity of large-scale paired datasets containing multiple images of the same individual forces most approaches to adopt reconstruction-based training. This reliance often leads to a failure mode we term copy-paste, where the model directly replicates the reference face rather than preserving identity across natural variations in pose, expression, or lighting. Such over-similarity undermines controllability and limits the expressive power of generation. To address these limitations, we (1) construct a large-scale paired dataset MultiID-2M, tailored for multi-person scenarios, providing diverse references for each identity; (2) introduce a benchmark that quantifies both copy-paste artifacts and the trade-off between identity fidelity and variation; and (3) propose a novel training paradigm with a contrastive identity loss that leverages paired data to balance fidelity with diversity. These contributions culminate in WithAnyone, a diffusion-based model that effectively mitigates copy-paste while preserving high identity similarity. Extensive qualitative and quantitative experiments demonstrate that WithAnyone significantly reduces copy-paste artifacts, improves controllability over pose and expression, and maintains strong perceptual quality. User studies further validate that our method achieves high identity fidelity while enabling expressive controllable generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14975v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hengyuan Xu, Wei Cheng, Peng Xing, Yixiao Fang, Shuhan Wu, Rui Wang, Xianfang Zeng, Daxin Jiang, Gang Yu, Xingjun Ma, Yu-Gang Jiang</dc:creator>
    </item>
    <item>
      <title>Terra: Explorable Native 3D World Model with Point Latents</title>
      <link>https://arxiv.org/abs/2510.14977</link>
      <description>arXiv:2510.14977v1 Announce Type: cross 
Abstract: World models have garnered increasing attention for comprehensive modeling of the real world. However, most existing methods still rely on pixel-aligned representations as the basis for world evolution, neglecting the inherent 3D nature of the physical world. This could undermine the 3D consistency and diminish the modeling efficiency of world models. In this paper, we present Terra, a native 3D world model that represents and generates explorable environments in an intrinsic 3D latent space. Specifically, we propose a novel point-to-Gaussian variational autoencoder (P2G-VAE) that encodes 3D inputs into a latent point representation, which is subsequently decoded as 3D Gaussian primitives to jointly model geometry and appearance. We then introduce a sparse point flow matching network (SPFlow) for generating the latent point representation, which simultaneously denoises the positions and features of the point latents. Our Terra enables exact multi-view consistency with native 3D representation and architecture, and supports flexible rendering from any viewpoint with only a single generation process. Furthermore, Terra achieves explorable world modeling through progressive generation in the point latent space. We conduct extensive experiments on the challenging indoor scenes from ScanNet v2. Terra achieves state-of-the-art performance in both reconstruction and generation with high 3D consistency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14977v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuanhui Huang, Weiliang Chen, Wenzhao Zheng, Xin Tao, Pengfei Wan, Jie Zhou, Jiwen Lu</dc:creator>
    </item>
    <item>
      <title>From Pixels to Words -- Towards Native Vision-Language Primitives at Scale</title>
      <link>https://arxiv.org/abs/2510.14979</link>
      <description>arXiv:2510.14979v1 Announce Type: cross 
Abstract: The edifice of native Vision-Language Models (VLMs) has emerged as a rising contender to typical modular VLMs, shaped by evolving model architectures and training paradigms. Yet, two lingering clouds cast shadows over its widespread exploration and promotion: (-) What fundamental constraints set native VLMs apart from modular ones, and to what extent can these barriers be overcome? (-) How to make research in native VLMs more accessible and democratized, thereby accelerating progress in the field. In this paper, we clarify these challenges and outline guiding principles for constructing native VLMs. Specifically, one native VLM primitive should: (i) effectively align pixel and word representations within a shared semantic space; (ii) seamlessly integrate the strengths of formerly separate vision and language modules; (iii) inherently embody various cross-modal properties that support unified vision-language encoding, aligning, and reasoning. Hence, we launch NEO, a novel family of native VLMs built from first principles, capable of rivaling top-tier modular counterparts across diverse real-world scenarios. With only 390M image-text examples, NEO efficiently develops visual perception from scratch while mitigating vision-language conflicts inside a dense and monolithic model crafted from our elaborate primitives. We position NEO as a cornerstone for scalable and powerful native VLMs, paired with a rich set of reusable components that foster a cost-effective and extensible ecosystem. Our code and models are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14979v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haiwen Diao, Mingxuan Li, Silei Wu, Linjun Dai, Xiaohua Wang, Hanming Deng, Lewei Lu, Dahua Lin, Ziwei Liu</dc:creator>
    </item>
    <item>
      <title>Coupled Diffusion Sampling for Training-Free Multi-View Image Editing</title>
      <link>https://arxiv.org/abs/2510.14981</link>
      <description>arXiv:2510.14981v1 Announce Type: cross 
Abstract: We present an inference-time diffusion sampling method to perform multi-view consistent image editing using pre-trained 2D image editing models. These models can independently produce high-quality edits for each image in a set of multi-view images of a 3D scene or object, but they do not maintain consistency across views. Existing approaches typically address this by optimizing over explicit 3D representations, but they suffer from a lengthy optimization process and instability under sparse view settings. We propose an implicit 3D regularization approach by constraining the generated 2D image sequences to adhere to a pre-trained multi-view image distribution. This is achieved through coupled diffusion sampling, a simple diffusion sampling technique that concurrently samples two trajectories from both a multi-view image distribution and a 2D edited image distribution, using a coupling term to enforce the multi-view consistency among the generated images. We validate the effectiveness and generality of this framework on three distinct multi-view image editing tasks, demonstrating its applicability across various model architectures and highlighting its potential as a general solution for multi-view consistent editing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14981v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hadi Alzayer, Yunzhi Zhang, Chen Geng, Jia-Bin Huang, Jiajun Wu</dc:creator>
    </item>
    <item>
      <title>Generative AI Meets Future Cities: Towards an Era of Autonomous Urban Intelligence</title>
      <link>https://arxiv.org/abs/2304.03892</link>
      <description>arXiv:2304.03892v3 Announce Type: replace 
Abstract: The two fields of urban planning and artificial intelligence (AI) arose and developed separately. However, there is now cross-pollination and increasing interest in both fields to benefit from the advances of the other. In the present paper, we introduce the importance of urban planning from the sustainability, living, economic, disaster, and environmental perspectives. We review the fundamental concepts of urban planning and relate these concepts to crucial open problems of machine learning, including adversarial learning, generative neural networks, deep encoder-decoder networks, conversational AI, and geospatial and temporal machine learning, thereby assaying how AI can contribute to modern urban planning. Thus, a central problem is automated land-use configuration, which is formulated as the generation of land uses and building configuration for a target area from surrounding geospatial, human mobility, social media, environment, and economic activities. Finally, we delineate some implications of AI for urban planning and propose key research areas at the intersection of both topics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.03892v3</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dongjie Wang, Chang-Tien Lu, Xinyue Ye, Tan Yigitcanlar, Yanjie Fu</dc:creator>
    </item>
    <item>
      <title>Domain-Independent Dynamic Programming</title>
      <link>https://arxiv.org/abs/2401.13883</link>
      <description>arXiv:2401.13883v4 Announce Type: replace 
Abstract: For combinatorial optimization problems, model-based paradigms such as mixed-integer programming (MIP) and constraint programming (CP) aim to decouple modeling and solving a problem: the `holy grail' of declarative problem solving. We propose domain-independent dynamic programming (DIDP), a novel model-based paradigm based on dynamic programming (DP). While DP is not new, it has typically been implemented as a problem-specific method. We introduce Dynamic Programming Description Language (DyPDL), a formalism to define DP models based on a state transition system, inspired by artificial intelligence (AI) planning. we show that heuristic search algorithms can be used to solve DyPDL models and propose seven DIDP solvers. We experimentally compare our DIDP solvers with commercial MIP and CP solvers (solving MIP and CP models, respectively) on common benchmark instances of eleven combinatorial optimization problem classes. We show that DIDP outperforms MIP in nine problem classes, CP also in nine problem classes, and both MIP and CP in seven. DIDP also achieves superior performance to existing state-based solvers including domain-independent AI planners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.13883v4</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ryo Kuroiwa, J. Christopher Beck</dc:creator>
    </item>
    <item>
      <title>TriQXNet: Forecasting Dst Index from Solar Wind Data Using an Interpretable Parallel Classical-Quantum Framework with Uncertainty Quantification</title>
      <link>https://arxiv.org/abs/2407.06658</link>
      <description>arXiv:2407.06658v3 Announce Type: replace 
Abstract: Geomagnetic storms, caused by solar wind energy transfer to Earth's magnetic field, can disrupt critical infrastructure like GPS, satellite communications, and power grids. The disturbance storm-time (Dst) index measures storm intensity. Despite advancements in empirical, physics-based, and machine-learning models using real-time solar wind data, accurately forecasting extreme geomagnetic events remains challenging due to noise and sensor failures. This research introduces TriQXNet, a novel hybrid classical-quantum neural network for Dst forecasting. Our model integrates classical and quantum computing, conformal prediction, and explainable AI (XAI) within a hybrid architecture. To ensure high-quality input data, we developed a comprehensive preprocessing pipeline that included feature selection, normalization, aggregation, and imputation. TriQXNet processes preprocessed solar wind data from NASA's ACE and NOAA's DSCOVR satellites, predicting the Dst index for the current hour and the next, providing vital advance notice to mitigate geomagnetic storm impacts. TriQXNet outperforms 13 state-of-the-art hybrid deep-learning models, achieving a root mean squared error of 9.27 nanoteslas (nT). Rigorous evaluation through 10-fold cross-validated paired t-tests confirmed its superior performance with 95% confidence. Conformal prediction techniques provide quantifiable uncertainty, which is essential for operational decisions, while XAI methods like ShapTime enhance interpretability. Comparative analysis shows TriQXNet's superior forecasting accuracy, setting a new level of expectations for geomagnetic storm prediction and highlighting the potential of classical-quantum hybrid models in space weather forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06658v3</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Abrar Jahin, M. F. Mridha, Zeyar Aung, Nilanjan Dey, R. Simon Sherratt</dc:creator>
    </item>
    <item>
      <title>DELE: Deductive $\mathcal{EL}^{++}$ Embeddings for Knowledge Base Completion</title>
      <link>https://arxiv.org/abs/2411.01574</link>
      <description>arXiv:2411.01574v3 Announce Type: replace 
Abstract: Ontology embeddings map classes, roles, and individuals in ontologies into $\mathbb{R}^n$, and within $\mathbb{R}^n$ similarity between entities can be computed or new axioms inferred. For ontologies in the Description Logic $\mathcal{EL}^{++}$, several optimization-based embedding methods have been developed that explicitly generate models of an ontology. However, these methods suffer from some limitations; they do not distinguish between statements that are unprovable and provably false, and therefore they may use entailed statements as negatives. Furthermore, they do not utilize the deductive closure of an ontology to identify statements that are inferred but not asserted. We evaluated a set of embedding methods for $\mathcal{EL}^{++}$ ontologies, incorporating several modifications that aim to make use of the ontology deductive closure. In particular, we designed novel negative losses that account both for the deductive closure and different types of negatives and formulated evaluation methods for knowledge base completion. We demonstrate that our embedding methods improve over the baseline ontology embedding in the task of knowledge base or ontology completion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01574v3</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Olga Mashkova, Fernando Zhapa-Camacho, Robert Hoehndorf</dc:creator>
    </item>
    <item>
      <title>Robust Counterfactual Inference in Markov Decision Processes</title>
      <link>https://arxiv.org/abs/2502.13731</link>
      <description>arXiv:2502.13731v3 Announce Type: replace 
Abstract: This paper addresses a key limitation in existing counterfactual inference methods for Markov Decision Processes (MDPs). Current approaches assume a specific causal model to make counterfactuals identifiable. However, there are usually many causal models that align with the observational and interventional distributions of an MDP, each yielding different counterfactual distributions, so fixing a particular causal model limits the validity (and usefulness) of counterfactual inference. We propose a novel non-parametric approach that computes tight bounds on counterfactual transition probabilities across all compatible causal models. Unlike previous methods that require solving prohibitively large optimisation problems (with variables that grow exponentially in the size of the MDP), our approach provides closed-form expressions for these bounds, making computation highly efficient and scalable for non-trivial MDPs. Once such an interval counterfactual MDP is constructed, our method identifies robust counterfactual policies that optimise the worst-case reward w.r.t. the uncertain interval MDP probabilities. We evaluate our method on various case studies, demonstrating improved robustness over existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13731v3</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jessica Lally, Milad Kazemi, Nicola Paoletti</dc:creator>
    </item>
    <item>
      <title>SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?</title>
      <link>https://arxiv.org/abs/2503.12349</link>
      <description>arXiv:2503.12349v5 Announce Type: replace 
Abstract: Reasoning and strategic behavior in social interactions is a hallmark of intelligence. This form of reasoning is significantly more sophisticated than isolated planning or reasoning tasks in static settings (e.g., math problem solving). In this paper, we present Strategic Planning, Interaction, and Negotiation (SPIN-Bench), a new multi-domain evaluation designed to measure the intelligence of strategic planning and social reasoning. While many existing benchmarks focus on narrow planning or single-agent reasoning, SPIN-Bench combines classical PDDL tasks, competitive board games, cooperative card games, and multi-agent negotiation scenarios in one unified framework. The framework includes both a benchmark as well as an arena to simulate and evaluate the variety of social settings to test reasoning and strategic behavior of AI agents. We formulate the benchmark SPIN-Bench by systematically varying action spaces, state complexity, and the number of interacting agents to simulate a variety of social settings where success depends on not only methodical and step-wise decision making, but also conceptual inference of other (adversarial or cooperative) participants. Our experiments reveal that while contemporary LLMs handle basic fact retrieval and short-range planning reasonably well, they encounter significant performance bottlenecks in tasks requiring deep multi-hop reasoning over large state spaces and socially adept coordination under uncertainty. We envision SPIN-Bench as a catalyst for future research on robust multi-agent planning, social reasoning, and human--AI teaming. Project Website: https://spinbench.github.io/</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12349v5</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jianzhu Yao, Kevin Wang, Ryan Hsieh, Haisu Zhou, Tianqing Zou, Zerui Cheng, Zhangyang Wang, Pramod Viswanath</dc:creator>
    </item>
    <item>
      <title>PoE-World: Compositional World Modeling with Products of Programmatic Experts</title>
      <link>https://arxiv.org/abs/2505.10819</link>
      <description>arXiv:2505.10819v3 Announce Type: replace 
Abstract: Learning how the world works is central to building AI agents that can adapt to complex environments. Traditional world models based on deep learning demand vast amounts of training data, and do not flexibly update their knowledge from sparse observations. Recent advances in program synthesis using Large Language Models (LLMs) give an alternate approach which learns world models represented as source code, supporting strong generalization from little data. To date, application of program-structured world models remains limited to natural language and grid-world domains. We introduce a novel program synthesis method for effectively modeling complex, non-gridworld domains by representing a world model as an exponentially-weighted product of programmatic experts (PoE-World) synthesized by LLMs. We show that this approach can learn complex, stochastic world models from just a few observations. We evaluate the learned world models by embedding them in a model-based planning agent, demonstrating efficient performance and generalization to unseen levels on Atari's Pong and Montezuma's Revenge. We release our code and display the learned world models and videos of the agent's gameplay at https://topwasu.github.io/poe-world.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10819v3</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wasu Top Piriyakulkij, Yichao Liang, Hao Tang, Adrian Weller, Marta Kryven, Kevin Ellis</dc:creator>
    </item>
    <item>
      <title>MAFA: A multi-agent framework for annotation</title>
      <link>https://arxiv.org/abs/2505.13668</link>
      <description>arXiv:2505.13668v3 Announce Type: replace 
Abstract: Modern consumer banking applications require accurate and efficient retrieval of information in response to user queries. Mapping user utterances to the most relevant Frequently Asked Questions (FAQs) is a crucial component of these systems. Traditional approaches often rely on a single model or technique, which may not capture the nuances of diverse user inquiries. In this paper, we introduce a multi-agent framework for FAQ annotation that combines multiple specialized agents with different approaches and a judge agent that reranks candidates to produce optimal results. Our agents utilize a structured reasoning approach inspired by Attentive Reasoning Queries (ARQs), which guides them through systematic reasoning steps using targeted, task-specific JSON queries. Our framework features a few-shot example strategy, where each agent receives different few-shots, enhancing ensemble diversity and coverage of the query space. We evaluate our framework on a real-world major bank dataset as well as public benchmark datasets (LCQMC and FiQA), demonstrating significant improvements over single-agent approaches across multiple metrics, including a 14% increase in Top-1 accuracy, an 18% increase in Top-5 accuracy, and a 12% improvement in Mean Reciprocal Rank on our dataset, and similar gains on public benchmarks when compared with traditional and single-agent annotation techniques. Our framework is particularly effective at handling ambiguous queries, making it well-suited for deployment in production banking applications while showing strong generalization capabilities across different domains and languages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13668v3</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:journal_reference>ECAI 2025 Workshop on AI in Finance</arxiv:journal_reference>
      <dc:creator>Mahmood Hegazy, Aaron Rodrigues, Azzam Naeem</dc:creator>
    </item>
    <item>
      <title>EMAC+: Embodied Multimodal Agent for Collaborative Planning with VLM+LLM</title>
      <link>https://arxiv.org/abs/2505.19905</link>
      <description>arXiv:2505.19905v2 Announce Type: replace 
Abstract: Although LLMs demonstrate proficiency in several text-based reasoning and planning tasks, their implementation in robotics control is constrained by significant deficiencies: (1) LLM agents are designed to work mainly with textual inputs rather than visual conditions; (2) Current multimodal agents treat LLMs as static planners, which separates their reasoning from environment dynamics, resulting in actions that do not take domain-specific knowledge into account; and (3) LLMs are not designed to learn from visual interactions, which makes it harder for them to make better policies for specific domains. In this paper, we introduce EMAC+, an Embodied Multimodal Agent that collaboratively integrates LLM and VLM via a bidirectional training paradigm. Unlike existing methods, EMAC+ dynamically refines high-level textual plans generated by an LLM using real-time feedback from a VLM executing low-level visual control tasks. We address critical limitations of previous models by enabling the LLM to internalize visual environment dynamics directly through interactive experience, rather than relying solely on static symbolic mappings. Extensive experimental evaluations on ALFWorld and RT-1 benchmarks demonstrate that EMAC+ achieves superior task performance, robustness against noisy observations, and efficient learning. We also conduct thorough ablation studies and provide detailed analyses of success and failure cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19905v2</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuang Ao, Flora D. Salim, Simon Khan</dc:creator>
    </item>
    <item>
      <title>Plugging Schema Graph into Multi-Table QA: A Human-Guided Framework for Reducing LLM Reliance</title>
      <link>https://arxiv.org/abs/2506.04427</link>
      <description>arXiv:2506.04427v3 Announce Type: replace 
Abstract: Large language models (LLMs) have shown promise in table Question Answering (Table QA). However, extending these capabilities to multi-table QA remains challenging due to unreliable schema linking across complex tables. Existing methods based on semantic similarity work well only on simplified hand-crafted datasets and struggle to handle complex, real-world scenarios with numerous and diverse columns. To address this, we propose a graph-based framework that leverages human-curated relational knowledge to explicitly encode schema links and join paths. Given a natural language query, our method searches on graph to construct interpretable reasoning chains, aided by pruning and sub-path merging strategies to enhance efficiency and coherence. Experiments on both standard benchmarks and a realistic, large-scale dataset demonstrate the effectiveness of our approach. To our knowledge, this is the first multi-table QA system applied to truly complex industrial tabular data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04427v3</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xixi Wang, Miguel Costa, Jordanka Kovaceva, Shuai Wang, Francisco C. Pereira</dc:creator>
    </item>
    <item>
      <title>Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents</title>
      <link>https://arxiv.org/abs/2509.06917</link>
      <description>arXiv:2509.06917v2 Announce Type: replace 
Abstract: We introduce Paper2Agent, an automated framework that converts research papers into AI agents. Paper2Agent transforms research output from passive artifacts into active systems that can accelerate downstream use, adoption, and discovery. Conventional research papers require readers to invest substantial effort to understand and adapt a paper's code, data, and methods to their own work, creating barriers to dissemination and reuse. Paper2Agent addresses this challenge by automatically converting a paper into an AI agent that acts as a knowledgeable research assistant. It systematically analyzes the paper and the associated codebase using multiple agents to construct a Model Context Protocol (MCP) server, then iteratively generates and runs tests to refine and robustify the resulting MCP. These paper MCPs can then be flexibly connected to a chat agent (e.g. Claude Code) to carry out complex scientific queries through natural language while invoking tools and workflows from the original paper. We demonstrate Paper2Agent's effectiveness in creating reliable and capable paper agents through in-depth case studies. Paper2Agent created an agent that leverages AlphaGenome to interpret genomic variants and agents based on ScanPy and TISSUE to carry out single-cell and spatial transcriptomics analyses. We validate that these paper agents can reproduce the original paper's results and can correctly carry out novel user queries. Paper2Agent automatically created AI co-scientist that identified new splicing variant associated with ADHD risk. By turning static papers into dynamic, interactive AI agents, Paper2Agent introduces a new paradigm for knowledge dissemination and a foundation for the collaborative ecosystem of AI co-scientists.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06917v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiacheng Miao, Joe R. Davis, Yaohui Zhang, Jonathan K. Pritchard, James Zou</dc:creator>
    </item>
    <item>
      <title>RepIt: Representing Isolated Targets to Steer Language Models</title>
      <link>https://arxiv.org/abs/2509.13281</link>
      <description>arXiv:2509.13281v3 Announce Type: replace 
Abstract: While activation steering in large language models (LLMs) is a growing area of research, methods can often incur broader effects than desired. This motivates isolation of purer concept vectors to enable targeted interventions and understand LLM behavior at a more granular level. We present RepIt, a simple and data-efficient framework for isolating concept-specific representations. Across five frontier LLMs, RepIt enables precise interventions: it selectively suppresses refusal on targeted concepts while preserving refusal elsewhere, producing models that answer WMD-related questions while still scoring as safe on standard benchmarks. We further show that the corrective signal localizes to just 100-200 neurons and that robust target representations can be extracted from as few as a dozen examples on a single A6000. This efficiency raises a dual concern: manipulations can be performed with modest compute and data to extend to underrepresented data-scarce topics while evading existing benchmarks. By disentangling refusal vectors with RepIt, this work demonstrates that targeted interventions can counteract overgeneralization, laying the foundation for more granular control of model behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13281v3</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincent Siu, Nathan W. Henry, Nicholas Crispino, Yang Liu, Dawn Song, Chenguang Wang</dc:creator>
    </item>
    <item>
      <title>SteeringSafety: A Systematic Safety Evaluation Framework of Representation Steering in LLMs</title>
      <link>https://arxiv.org/abs/2509.13450</link>
      <description>arXiv:2509.13450v2 Announce Type: replace 
Abstract: We introduce SteeringSafety, a systematic framework for evaluating representation steering methods across seven safety perspectives spanning 17 datasets. While prior work highlights general capabilities of representation steering, we systematically explore safety perspectives including bias, harmfulness, hallucination, social behaviors, reasoning, epistemic integrity, and normative judgment. Our framework provides modularized building blocks for state-of-the-art steering methods, enabling unified implementation of DIM, ACE, CAA, PCA, and LAT with recent enhancements like conditional steering. Results on Gemma-2-2B, Llama-3.1-8B, and Qwen-2.5-7B reveal that strong steering performance depends critically on pairing of method, model, and specific perspective. DIM shows consistent effectiveness, but all methods exhibit substantial entanglement: social behaviors show highest vulnerability (reaching degradation as high as 76%), jailbreaking often compromises normative judgment, and hallucination steering unpredictably shifts political views. Our findings underscore the critical need for holistic safety evaluations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13450v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincent Siu, Nicholas Crispino, David Park, Nathan W. Henry, Zhun Wang, Yang Liu, Dawn Song, Chenguang Wang</dc:creator>
    </item>
    <item>
      <title>Efficient &amp; Correct Predictive Equivalence for Decision Trees</title>
      <link>https://arxiv.org/abs/2509.17774</link>
      <description>arXiv:2509.17774v4 Announce Type: replace 
Abstract: The Rashomon set of decision trees (DTs) finds importance uses. Recent work showed that DTs computing the same classification function, i.e. predictive equivalent DTs, can represent a significant fraction of the Rashomon set. Such redundancy is undesirable. For example, feature importance based on the Rashomon set becomes inaccurate due the existence of predictive equivalent DTs, i.e. DTs with the same prediction for every possible input. In recent work, McTavish et al. proposed solutions for several computational problems related with DTs, including that of deciding predictive equivalent DTs. The approach of McTavish et al. consists of applying the well-known method of Quine-McCluskey (QM) for obtaining minimum-size DNF (disjunctive normal form) representations of DTs, which are then used for comparing DTs for predictive equivalence. Furthermore, the minimum-size DNF representation was also applied to computing explanations for the predictions made by DTs, and to finding predictions in the presence of missing data. However, the problem of formula minimization is hard for the second level of the polynomial hierarchy, and the QM method may exhibit worst-case exponential running time and space. This paper first demonstrates that there exist decision trees that trigger the worst-case exponential running time and space of the QM method. Second, the paper shows that the QM method may incorrectly decide predictive equivalence, if two key constraints are not respected, and one may be difficult to formally guarantee. Third, the paper shows that any of the problems to which the smallest DNF representation has been applied to can be solved in polynomial time, in the size of the DT. The experiments confirm that, for DTs for which the worst-case of the QM method is triggered, the algorithms proposed in this paper are orders of magnitude faster than the ones proposed by McTavish et al.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17774v4</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.LO</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joao Marques-Silva, Alexey Ignatiev</dc:creator>
    </item>
    <item>
      <title>RADAR: A Risk-Aware Dynamic Multi-Agent Framework for LLM Safety Evaluation via Role-Specialized Collaboration</title>
      <link>https://arxiv.org/abs/2509.25271</link>
      <description>arXiv:2509.25271v2 Announce Type: replace 
Abstract: Existing safety evaluation methods for large language models (LLMs) suffer from inherent limitations, including evaluator bias and detection failures arising from model homogeneity, which collectively undermine the robustness of risk evaluation processes. This paper seeks to re-examine the risk evaluation paradigm by introducing a theoretical framework that reconstructs the underlying risk concept space. Specifically, we decompose the latent risk concept space into three mutually exclusive subspaces: the explicit risk subspace (encompassing direct violations of safety guidelines), the implicit risk subspace (capturing potential malicious content that requires contextual reasoning for identification), and the non-risk subspace. Furthermore, we propose RADAR, a multi-agent collaborative evaluation framework that leverages multi-round debate mechanisms through four specialized complementary roles and employs dynamic update mechanisms to achieve self-evolution of risk concept distributions. This approach enables comprehensive coverage of both explicit and implicit risks while mitigating evaluator bias. To validate the effectiveness of our framework, we construct an evaluation dataset comprising 800 challenging cases. Extensive experiments on our challenging testset and public benchmarks demonstrate that RADAR significantly outperforms baseline evaluation methods across multiple dimensions, including accuracy, stability, and self-evaluation risk sensitivity. Notably, RADAR achieves a 28.87% improvement in risk identification accuracy compared to the strongest baseline evaluation method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25271v2</guid>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiuyuan Chen, Jian Zhao, Yuchen Yuan, Tianle Zhang, Huilin Zhou, Zheng Zhu, Ping Hu, Linghe Kong, Chi Zhang, Weiran Huang, Xuelong Li</dc:creator>
    </item>
    <item>
      <title>From Perception to Cognition: A Survey of Vision-Language Interactive Reasoning in Multimodal Large Language Models</title>
      <link>https://arxiv.org/abs/2509.25373</link>
      <description>arXiv:2509.25373v4 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) strive to achieve a profound, human-like understanding of and interaction with the physical world, but often exhibit a shallow and incoherent integration when acquiring information (Perception) and conducting reasoning (Cognition). This disconnect leads to a spectrum of reasoning failures, with hallucination being the most prominent. Collectively, these issues expose a fundamental challenge: the ability to process pixels does not yet confer the ability to construct a coherent, credible internal world model. To systematically dissect and address this challenge, this survey introduces a novel and unified analytical framework: ``From Perception to Cognition." We deconstruct the complex process of vision-language interactive understanding into two interdependent layers: Perception, the foundational ability to accurately extract visual information and achieve fine-grained alignment with textual instructions; and Cognition, the higher-order capability for proactive, multi-step, goal-oriented reasoning built upon this perceptual foundation, the core of which is the formation of a dynamic observe-think-verify reasoning loop. Guided by this framework, this paper systematically analyzes the key bottlenecks of current MLLMs at both layers. It surveys the landscape of cutting-edge methods designed to address these challenges, spanning from techniques that enhance low-level visual representations to those that improve high-level reasoning paradigms. Furthermore, we review critical benchmarks and delineate future research directions. This survey aims to provide the research community with a clear, structured perspective for understanding the intrinsic limitations of current MLLMs and to illuminate the path toward building next-generation models capable of deep reasoning and a genuine understanding of the world.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25373v4</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenyue Zhou, Mingxuan Wang, Yanbiao Ma, Chenxu Wu, Wanyi Chen, Zhe Qian, Xinyu Liu, Yiwei Zhang, Junhao Wang, Hengbo Xu, Fei Luo, Xiaohua Chen, Xiaoshuai Hao, Hehan Li, Andi Zhang, Wenxuan Wang, Kaiyan Zhang, Guoli Jia, Lingling Li, Zhiwu Lu, Yang Lu, Yike Guo</dc:creator>
    </item>
    <item>
      <title>LLM Based Bayesian Optimization for Prompt Search</title>
      <link>https://arxiv.org/abs/2510.04384</link>
      <description>arXiv:2510.04384v2 Announce Type: replace 
Abstract: Bayesian Optimization (BO) has been widely used to efficiently optimize expensive black-box functions with limited evaluations. In this paper, we investigate the use of BO for prompt engineering to enhance text classification with Large Language Models (LLMs). We employ an LLM-powered Gaussian Process (GP) as the surrogate model to estimate the performance of different prompt candidates. These candidates are generated by an LLM through the expansion of a set of seed prompts and are subsequently evaluated using an Upper Confidence Bound (UCB) acquisition function in conjunction with the GP posterior. The optimization process iteratively refines the prompts based on a subset of the data, aiming to improve classification accuracy while reducing the number of API calls by leveraging the prediction uncertainty of the LLM-based GP. The proposed BO-LLM algorithm is evaluated on two datasets, and its advantages are discussed in detail in this paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04384v2</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Adam Ballew, Jingbo Wang, Shaogang Ren</dc:creator>
    </item>
    <item>
      <title>RareAgent: Self-Evolving Reasoning for Drug Repurposing in Rare Diseases</title>
      <link>https://arxiv.org/abs/2510.05764</link>
      <description>arXiv:2510.05764v2 Announce Type: replace 
Abstract: Computational drug repurposing for rare diseases is especially challenging when no prior associations exist between drugs and target diseases. Therefore, knowledge graph completion and message-passing GNNs have little reliable signal to learn and propagate, resulting in poor performance. We present RareAgent, a self-evolving multi-agent system that reframes this task from passive pattern recognition to active evidence-seeking reasoning. RareAgent organizes task-specific adversarial debates in which agents dynamically construct evidence graphs from diverse perspectives to support, refute, or entail hypotheses. The reasoning strategies are analyzed post hoc in a self-evolutionary loop, producing textual feedback that refines agent policies, while successful reasoning paths are distilled into transferable heuristics to accelerate future investigations. Comprehensive evaluations reveal that RareAgent improves the indication AUPRC by 18.1% over reasoning baselines and provides a transparent reasoning chain consistent with clinical evidence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05764v2</guid>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lang Qin, Zijian Gan, Xu Cao, Pengcheng Jiang, Yankai Jiang, Jiawei Han, Kaishun Wu, Jintai Chen</dc:creator>
    </item>
    <item>
      <title>Measuring and Mitigating Identity Bias in Multi-Agent Debate via Anonymization</title>
      <link>https://arxiv.org/abs/2510.07517</link>
      <description>arXiv:2510.07517v2 Announce Type: replace 
Abstract: Multi-agent debate (MAD) aims to improve large language model (LLM) reasoning by letting multiple agents exchange answers and then aggregate their opinions. Yet recent studies reveal that agents are not neutral: they are prone to identity-driven sycophancy and self-bias, uncritically adopting a peer's view or stubbornly adhering to their own prior output, undermining the reliability of debate. In this work, we present the first principled framework that joins sycophancy and self-bias to mitigate and quantify identity bias in MAD. First, we formalize the debate dynamics as an identity-weighted Bayesian update process. Second, we propose response anonymization: by removing identity markers from prompts, agents cannot distinguish "self" from "peer", which forces equal weights on agent identity, thereby reducing bias. Third, we define the Identity Bias Coefficient (IBC), a principled metric that measures how often an agent follows a peer versus itself. Empirical studies across multiple models, datasets and debate rounds confirm that identity bias is widespread, with sycophancy far more common than self-bias. Our findings highlight the need to "mask" identity to ensure that MAD systems reason based on content rather than source identity. Code is released in https://github.com/deeplearning-wisc/MAD-identity-bias.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07517v2</guid>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hyeong Kyu Choi, Xiaojin Zhu, Sharon Li</dc:creator>
    </item>
    <item>
      <title>TripScore: Benchmarking and rewarding real-world travel planning with fine-grained evaluation</title>
      <link>https://arxiv.org/abs/2510.09011</link>
      <description>arXiv:2510.09011v3 Announce Type: replace 
Abstract: Travel planning is a valuable yet complex task that poses significant challenges even for advanced large language models (LLMs). While recent benchmarks have advanced in evaluating LLMs' planning capabilities, they often fall short in evaluating feasibility, reliability, and engagement of travel plans. We introduce a comprehensive benchmark for travel planning that unifies fine-grained criteria into a single reward, enabling direct comparison of plan quality and seamless integration with reinforcement learning (RL). Our evaluator achieves moderate agreement with travel-expert annotations (60.75%) and outperforms multiple LLM-as-judge baselines. We further release a large-scale dataset of 4,870 queries including 219 real-world, free-form requests for generalization to authentic user intent. Using this benchmark, we conduct extensive experiments across diverse methods and LLMs, including test-time computation, neuro-symbolic approaches, supervised fine-tuning, and RL via GRPO. Across base models, RL generally improves itinerary feasibility over prompt-only and supervised baselines, yielding higher unified reward scores.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09011v3</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yincen Qu, Huan Xiao, Feng Li, Gregory Li, Hui Zhou, Xiangying Dai, Xiaoru Dai</dc:creator>
    </item>
    <item>
      <title>Tensor Logic: The Language of AI</title>
      <link>https://arxiv.org/abs/2510.12269</link>
      <description>arXiv:2510.12269v3 Announce Type: replace 
Abstract: Progress in AI is hindered by the lack of a programming language with all the requisite features. Libraries like PyTorch and TensorFlow provide automatic differentiation and efficient GPU implementation, but are additions to Python, which was never intended for AI. Their lack of support for automated reasoning and knowledge acquisition has led to a long and costly series of hacky attempts to tack them on. On the other hand, AI languages like LISP and Prolog lack scalability and support for learning. This paper proposes tensor logic, a language that solves these problems by unifying neural and symbolic AI at a fundamental level. The sole construct in tensor logic is the tensor equation, based on the observation that logical rules and Einstein summation are essentially the same operation, and all else can be reduced to them. I show how to elegantly implement key forms of neural, symbolic and statistical AI in tensor logic, including transformers, formal reasoning, kernel machines and graphical models. Most importantly, tensor logic makes new directions possible, such as sound reasoning in embedding space. This combines the scalability and learnability of neural networks with the reliability and transparency of symbolic reasoning, and is potentially a basis for the wider adoption of AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12269v3</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>cs.PL</category>
      <category>stat.ML</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pedro Domingos</dc:creator>
    </item>
    <item>
      <title>O-Forge: An LLM + Computer Algebra Framework for Asymptotic Analysis</title>
      <link>https://arxiv.org/abs/2510.12350</link>
      <description>arXiv:2510.12350v2 Announce Type: replace 
Abstract: Large language models have recently demonstrated advanced capabilities in solving IMO and Putnam problems; yet their role in research mathematics has remained fairly limited. The key difficulty is verification: suggested proofs may look plausible, but cannot be trusted without rigorous checking. We present a framework, called LLM+CAS, and an associated tool, O-Forge, that couples frontier LLMs with a computer algebra systems (CAS) in an In-Context Symbolic Feedback loop to produce proofs that are both creative and symbolically verified. Our focus is on asymptotic inequalities, a topic that often involves difficult proofs and appropriate decomposition of the domain into the "right" subdomains. Many mathematicians, including Terry Tao, have suggested that using AI tools to find the right decompositions can be very useful for research-level asymptotic analysis. In this paper, we show that our framework LLM+CAS turns out to be remarkably effective at proposing such decompositions via a combination of a frontier LLM and a CAS. More precisely, we use an LLM to suggest domain decomposition, and a CAS (such as Mathematica) that provides a verification of each piece axiomatically. Using this loop, we answer a question posed by Terence Tao: whether LLMs coupled with a verifier can be used to help prove intricate asymptotic inequalities. More broadly, we show how AI can move beyond contest math towards research-level tools for professional mathematicians.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12350v2</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ayush Khaitan, Vijay Ganesh</dc:creator>
    </item>
    <item>
      <title>Ax-Prover: A Deep Reasoning Agentic Framework for Theorem Proving in Mathematics and Quantum Physics</title>
      <link>https://arxiv.org/abs/2510.12787</link>
      <description>arXiv:2510.12787v2 Announce Type: replace 
Abstract: We present Ax-Prover, a multi-agent system for automated theorem proving in Lean that can solve problems across diverse scientific domains and operate either autonomously or collaboratively with human experts. To achieve this, Ax-Prover approaches scientific problem solving through formal proof generation, a process that demands both creative reasoning and strict syntactic rigor. Ax-Prover meets this challenge by equipping Large Language Models (LLMs), which provide knowledge and reasoning, with Lean tools via the Model Context Protocol (MCP), which ensure formal correctness. To evaluate its performance as an autonomous prover, we benchmark our approach against frontier LLMs and specialized prover models on two public math benchmarks and on two Lean benchmarks we introduce in the fields of abstract algebra and quantum theory. On public datasets, Ax-Prover is competitive with state-of-the-art provers, while it largely outperforms them on the new benchmarks. This shows that, unlike specialized systems that struggle to generalize, our tool-based agentic theorem prover approach offers a generalizable methodology for formal verification across diverse scientific domains. Furthermore, we demonstrate Ax-Prover's assistant capabilities in a practical use case, showing how it enabled an expert mathematician to formalize the proof of a complex cryptography theorem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12787v2</guid>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Del Tredici, Jacob McCarran, Benjamin Breen, Javier Aspuru Mijares, Weichen Winston Yin, Jacob M. Taylor, Frank H. L. Koppens, Dirk Englund</dc:creator>
    </item>
    <item>
      <title>A Methodology for Assessing the Risk of Metric Failure in LLMs Within the Financial Domain</title>
      <link>https://arxiv.org/abs/2510.13524</link>
      <description>arXiv:2510.13524v2 Announce Type: replace 
Abstract: As Generative Artificial Intelligence is adopted across the financial services industry, a significant barrier to adoption and usage is measuring model performance. Historical machine learning metrics can oftentimes fail to generalize to GenAI workloads and are often supplemented using Subject Matter Expert (SME) Evaluation. Even in this combination, many projects fail to account for various unique risks present in choosing specific metrics. Additionally, many widespread benchmarks created by foundational research labs and educational institutions fail to generalize to industrial use. This paper explains these challenges and provides a Risk Assessment Framework to allow for better application of SME and machine learning Metrics</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13524v2</guid>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>William Flanagan, Mukunda Das, Rajitha Ramanayake, Swanuja Maslekar, Meghana Mangipudi, Joong Ho Choi, Shruti Nair, Shambhavi Bhusan, Sanjana Dulam, Mouni Pendharkar, Nidhi Singh, Vashisth Doshi, Sachi Shah Paresh</dc:creator>
    </item>
    <item>
      <title>Training LLM Agents to Empower Humans</title>
      <link>https://arxiv.org/abs/2510.13709</link>
      <description>arXiv:2510.13709v2 Announce Type: replace 
Abstract: Assistive agents should not only take actions on behalf of a human, but also step out of the way and cede control when there are important decisions to be made. However, current methods for building assistive agents, whether via mimicking expert humans or via RL finetuning on an inferred reward, often encourage agents to complete tasks on their own rather than truly assisting the human attain their objectives. Additionally, these methods often require costly explicit human feedback to provide a training signal. We propose a new approach to tuning assistive language models based on maximizing the human's empowerment, their ability to effect desired changes in the environment. Our empowerment-maximizing method, Empower, only requires offline text data, providing a self-supervised method for fine-tuning language models to better assist humans. To study the efficacy of our approach, we conducted an 18-person user study comparing our empowerment assistant with a strong baseline. Participants preferred our assistant 78% of the time (p=0.015), with a 31% higher acceptance rate and 38% fewer suggestions. Additionally, we introduce a new environment for evaluating multi-turn code assistance using simulated humans. Using this environment, we show that agents trained with Empower increase the success rate of a simulated human programmer on challenging coding questions by an average of 192% over an SFT baseline. With this empowerment objective, we provide a framework for useful aligned AI agents at scale using only offline data without the need for any additional human feedback or verifiable rewards.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13709v2</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Evan Ellis, Vivek Myers, Jens Tuyls, Sergey Levine, Anca Dragan, Benjamin Eysenbach</dc:creator>
    </item>
    <item>
      <title>Interpretability and Transparency-Driven Detection and Transformation of Textual Adversarial Examples (IT-DT)</title>
      <link>https://arxiv.org/abs/2307.01225</link>
      <description>arXiv:2307.01225v2 Announce Type: replace-cross 
Abstract: Transformer-based text classifiers like BERT, Roberta, T5, and GPT-3 have shown impressive performance in NLP. However, their vulnerability to adversarial examples poses a security risk. Existing defense methods lack interpretability, making it hard to understand adversarial classifications and identify model vulnerabilities. To address this, we propose the Interpretability and Transparency-Driven Detection and Transformation (IT-DT) framework. It focuses on interpretability and transparency in detecting and transforming textual adversarial examples. IT-DT utilizes techniques like attention maps, integrated gradients, and model feedback for interpretability during detection. This helps identify salient features and perturbed words contributing to adversarial classifications. In the transformation phase, IT-DT uses pre-trained embeddings and model feedback to generate optimal replacements for perturbed words. By finding suitable substitutions, we aim to convert adversarial examples into non-adversarial counterparts that align with the model's intended behavior while preserving the text's meaning. Transparency is emphasized through human expert involvement. Experts review and provide feedback on detection and transformation results, enhancing decision-making, especially in complex scenarios. The framework generates insights and threat intelligence empowering analysts to identify vulnerabilities and improve model robustness. Comprehensive experiments demonstrate the effectiveness of IT-DT in detecting and transforming adversarial examples. The approach enhances interpretability, provides transparency, and enables accurate identification and successful transformation of adversarial inputs. By combining technical analysis and human expertise, IT-DT significantly improves the resilience and trustworthiness of transformer-based text classifiers against adversarial attacks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.01225v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>IEEE Transactions on Dependable and Secure Computing (TDSC) 2025</arxiv:journal_reference>
      <dc:creator>Bushra Sabir, M. Ali Babar, Sharif Abuadbba</dc:creator>
    </item>
    <item>
      <title>Quantum Polar Metric Learning: Efficient Classically Learned Quantum Embeddings</title>
      <link>https://arxiv.org/abs/2312.01655</link>
      <description>arXiv:2312.01655v4 Announce Type: replace-cross 
Abstract: Deep metric learning has recently shown extremely promising results in the classical data domain, creating well-separated feature spaces. This idea was also adapted to quantum computers via Quantum Metric Learning(QMeL). QMeL consists of a 2 step process with a classical model to compress the data to fit into the limited number of qubits, then train a Parameterized Quantum Circuit(PQC) to create better separation in Hilbert Space. However, on Noisy Intermediate Scale Quantum (NISQ) devices. QMeL solutions result in high circuit width and depth, both of which limit scalability. We propose Quantum Polar Metric Learning (QPMeL) that uses a classical model to learn the parameters of the polar form of a qubit. We then utilize a shallow PQC with $R_y$ and $R_z$ gates to create the state and a trainable layer of $ZZ(\theta)$-gates to learn entanglement. The circuit also computes fidelity via a SWAP Test for our proposed Fidelity Triplet Loss function, used to train both classical and quantum components. When compared to QMeL approaches, QPMeL achieves 3X better multi-class separation, while using only 1/2 the number of gates and depth. We also demonstrate that QPMeL outperforms classical networks with similar configurations, presenting a promising avenue for future research on fully classical models with quantum loss functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.01655v4</guid>
      <category>quant-ph</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Vinayak Sharma, Ashish Padhy, Sourav Behera, Lord Sen, Shyamapada Mukherjee, Aviral Shrivastava</dc:creator>
    </item>
    <item>
      <title>Natural Language Processing RELIES on Linguistics</title>
      <link>https://arxiv.org/abs/2405.05966</link>
      <description>arXiv:2405.05966v5 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have become capable of generating highly fluent text in certain languages, without modules specially designed to capture grammar or semantic coherence. What does this mean for the future of linguistic expertise in NLP? We highlight several aspects in which NLP (still) relies on linguistics, or where linguistic thinking can illuminate new directions. We argue our case around the acronym RELIES that encapsulates six major facets where linguistics contributes to NLP: Resources, Evaluation, Low-resource settings, Interpretability, Explanation, and the Study of language. This list is not exhaustive, nor is linguistics the main point of reference for every effort under these themes; but at a macro level, these facets highlight the enduring importance of studying machine systems vis-\`a-vis systems of human language.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05966v5</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1162/coli_a_00560</arxiv:DOI>
      <dc:creator>Juri Opitz, Shira Wein, Nathan Schneider</dc:creator>
    </item>
    <item>
      <title>Why do explanations fail? A typology and discussion on failures in XAI</title>
      <link>https://arxiv.org/abs/2405.13474</link>
      <description>arXiv:2405.13474v2 Announce Type: replace-cross 
Abstract: As Machine Learning models achieve unprecedented levels of performance, the XAI domain aims at making these models understandable by presenting end-users with intelligible explanations.
  Yet, some existing XAI approaches fail to meet expectations: several issues have been reported in the literature, generally pointing out either
  technical limitations or misinterpretations by users.
  In this paper, we argue that the resulting harms arise from a complex overlap of multiple failures in XAI, which existing ad-hoc studies fail to capture.
  This work therefore advocates for a holistic perspective, presenting a systematic investigation of limitations of current XAI methods and their impact on the interpretation of explanations. %
  By distinguishing between system-specific and user-specific failures,
  we propose a typological framework that helps revealing the nuanced complexities of explanation failures.
  Leveraging this typology, we discuss some research directions to help practitioners better understand the limitations of XAI systems and enhance the quality of ML explanations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13474v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Clara Bove, Thibault Laugel, Marie-Jeanne Lesot, Charles Tijus, Marcin Detyniecki</dc:creator>
    </item>
    <item>
      <title>Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference</title>
      <link>https://arxiv.org/abs/2407.11550</link>
      <description>arXiv:2407.11550v5 Announce Type: replace-cross 
Abstract: Large Language Models have excelled in various domains but face efficiency challenges due to the growing Key-Value (KV) cache required for long-sequence inference. Recent efforts aim to reduce KV cache size by evicting vast non-critical cache elements during runtime while preserving generation quality. However, these methods typically allocate compression budgets uniformly across all attention heads, ignoring the unique attention patterns of each head. In this paper, we establish a theoretical loss upper bound between pre- and post-eviction attention output, explaining the optimization target of prior cache eviction methods, while guiding the optimization of adaptive budget allocation. Base on this, we propose {\it Ada-KV}, the first head-wise adaptive budget allocation strategy. It offers plug-and-play benefits, enabling seamless integration with prior cache eviction methods. Extensive evaluations on 13 datasets from Ruler and 16 datasets from LongBench, all conducted under both question-aware and question-agnostic scenarios, demonstrate substantial quality improvements over existing methods. Our code is available at https://github.com/FFY0/AdaKV.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11550v5</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuan Feng, Junlin Lv, Yukun Cao, Xike Xie, S. Kevin Zhou</dc:creator>
    </item>
    <item>
      <title>A Comprehensive Review of Recommender Systems: Transitioning from Theory to Practice</title>
      <link>https://arxiv.org/abs/2407.13699</link>
      <description>arXiv:2407.13699v4 Announce Type: replace-cross 
Abstract: Recommender Systems (RS) play an integral role in enhancing user experiences by providing personalized item suggestions. This survey reviews the progress in RS inclusively from 2017 to 2024, effectively connecting theoretical advances with practical applications. We explore the development from traditional RS techniques like content-based and collaborative filtering to advanced methods involving deep learning, graph-based models, reinforcement learning, and large language models. We also discuss specialized systems such as context-aware, review-based, and fairness-aware RS. The primary goal of this survey is to bridge theory with practice. It addresses challenges across various sectors, including e-commerce, healthcare, and finance, emphasizing the need for scalable, real-time, and trustworthy solutions. Through this survey, we promote stronger partnerships between academic research and industry practices. The insights offered by this survey aim to guide industry professionals in optimizing RS deployment and to inspire future research directions, especially in addressing emerging technological and societal trends\footnote. The survey resources are available in the public GitHub repository https://github.com/VectorInstitute/Recommender-Systems-Survey. (Recommender systems, large language models, chatgpt, responsible AI)</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13699v4</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shaina Raza, Mizanur Rahman, Safiullah Kamawal, Armin Toroghi, Ananya Raval, Farshad Navah, Amirmohammad Kazemeini</dc:creator>
    </item>
    <item>
      <title>Visual Stereotypes of Autism Spectrum in Janus-Pro-7B, DALL-E, Stable Diffusion, SDXL, FLUX, and Midjourney</title>
      <link>https://arxiv.org/abs/2407.16292</link>
      <description>arXiv:2407.16292v3 Announce Type: replace-cross 
Abstract: Avoiding systemic discrimination of neurodiverse individuals is an ongoing challenge in training AI models, which often propagate negative stereotypes. This study examined whether six text-to-image models (Janus-Pro-7B VL2 vs. VL3, DALL-E 3 v. April 2024 vs. August 2025, Stable Diffusion v. 1.6 vs. 3.5, SDXL v. April 2024 vs. FLUX.1 Pro, and Midjourney v. 5.1 vs. 7) perpetuate non-rational beliefs regarding autism by comparing images generated in 2024-2025 with controls. 53 prompts aimed at neutrally visualizing concrete objects and abstract concepts related to autism were used against 53 controls (baseline total N=302, follow-up experimental 280 images plus 265 controls). Expert assessment measuring the presence of common autism-related stereotypes employed a framework of 10 deductive codes followed by statistical analysis. Autistic individuals were depicted with striking homogeneity in skin color (white), gender (male), and age (young), often engaged in solitary activities, interacting with objects rather than people, and exhibiting stereotypical emotional expressions such as sadness, anger, or emotional flatness. In contrast, the images of neurotypical individuals were more diverse and lacked such traits. We found significant differences between the models; however, with a moderate effect size, and no differences between baseline and follow-up summary values, with the ratio of stereotypical themes to the number of images similar across all models. The control prompts showed a significantly lower degree of stereotyping with large size effects, confirming the hidden biases of the models. In summary, despite improvements in the technical aspects of image generation, the level of reproduction of potentially harmful autism-related stereotypes remained largely unaffected.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16292v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maciej Wodzi\'nski, Marcin Rz\k{a}deczka, Anastazja Szu{\l}a, Kacper Dudzic, Marcin Moskalewicz</dc:creator>
    </item>
    <item>
      <title>Say My Name: a Model's Bias Discovery Framework</title>
      <link>https://arxiv.org/abs/2408.09570</link>
      <description>arXiv:2408.09570v2 Announce Type: replace-cross 
Abstract: In the last few years, due to the broad applicability of deep learning to downstream tasks and end-to-end training capabilities, increasingly more concerns about potential biases to specific, non-representative patterns have been raised. Many works focusing on unsupervised debiasing usually leverage the tendency of deep models to learn ``easier'' samples, for example by clustering the latent space to obtain bias pseudo-labels. However, the interpretation of such pseudo-labels is not trivial, especially for a non-expert end user, as it does not provide semantic information about the bias features. To address this issue, we introduce ``Say My Name'' (SaMyNa), the first tool to identify biases within deep models semantically. Unlike existing methods, our approach focuses on biases learned by the model. Our text-based pipeline enhances explainability and supports debiasing efforts: applicable during either training or post-hoc validation, our method can disentangle task-related information and proposes itself as a tool to analyze biases. Evaluation on traditional benchmarks demonstrates its effectiveness in detecting biases and even disclaiming them, showcasing its broad applicability for model diagnosis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09570v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Massimiliano Ciranni, Luca Molinaro, Carlo Alberto Barbano, Attilio Fiandrotti, Vittorio Murino, Vito Paolo Pastore, Enzo Tartaglione</dc:creator>
    </item>
    <item>
      <title>The Fluorescent Veil: A Stealthy and Effective Physical Adversarial Patch Against Traffic Sign Recognition</title>
      <link>https://arxiv.org/abs/2409.12394</link>
      <description>arXiv:2409.12394v2 Announce Type: replace-cross 
Abstract: Recently, traffic sign recognition (TSR) systems have become a prominent target for physical adversarial attacks. These attacks typically rely on conspicuous stickers and projections, or using invisible light and acoustic signals that can be easily blocked. In this paper, we introduce a novel attack medium, i.e., fluorescent ink, to design a stealthy and effective physical adversarial patch, namely FIPatch, to advance the state-of-the-art. Specifically, we first model the fluorescence effect in the digital domain to identify the optimal attack settings, which guide the real-world fluorescence parameters. By applying a carefully designed fluorescence perturbation to the target sign, the attacker can later trigger a fluorescent effect using invisible ultraviolet light, causing the TSR system to misclassify the sign and potentially leading to traffic accidents. We conducted a comprehensive evaluation to investigate the effectiveness of FIPatch, which shows a success rate of 98.31% in low-light conditions. Furthermore, our attack successfully bypasses five popular defenses and achieves a success rate of 96.72%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12394v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuai Yuan, Xingshuo Han, Hongwei Li, Guowen Xu, Wenbo Jiang, Tao Ni, Qingchuan Zhao, Yuguang Fang</dc:creator>
    </item>
    <item>
      <title>GraphLand: Evaluating Graph Machine Learning Models on Diverse Industrial Data</title>
      <link>https://arxiv.org/abs/2409.14500</link>
      <description>arXiv:2409.14500v4 Announce Type: replace-cross 
Abstract: Although data that can be naturally represented as graphs is widespread in real-world applications across diverse industries, popular graph ML benchmarks for node property prediction only cover a surprisingly narrow set of data domains, and graph neural networks (GNNs) are often evaluated on just a few academic citation networks. This issue is particularly pressing in light of the recent growing interest in designing graph foundation models. These models are supposed to be able to transfer to diverse graph datasets from different domains, and yet the proposed graph foundation models are often evaluated on a very limited set of datasets from narrow applications. To alleviate this issue, we introduce GraphLand: a benchmark of 14 diverse graph datasets for node property prediction from a range of different industrial applications. GraphLand allows evaluating graph ML models on a wide range of graphs with diverse sizes, structural characteristics, and feature sets, all in a unified setting. Further, GraphLand allows investigating such previously underexplored research questions as how realistic temporal distributional shifts under transductive and inductive settings influence graph ML model performance. To mimic realistic industrial settings, we use GraphLand to compare GNNs with gradient-boosted decision trees (GBDT) models that are popular in industrial applications and show that GBDTs provided with additional graph-based input features can sometimes be very strong baselines. Further, we evaluate currently available general-purpose graph foundation models and find that they fail to produce competitive results on our proposed datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14500v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gleb Bazhenov, Oleg Platonov, Liudmila Prokhorenkova</dc:creator>
    </item>
    <item>
      <title>MIO: A Foundation Model on Multimodal Tokens</title>
      <link>https://arxiv.org/abs/2409.17692</link>
      <description>arXiv:2409.17692v4 Announce Type: replace-cross 
Abstract: In this paper, we introduce MIO, a novel foundation model built on multimodal tokens, capable of understanding and generating speech, text, images, and videos in an end-to-end, autoregressive manner. While the emergence of large language models (LLMs) and multimodal large language models (MM-LLMs) propels advancements in artificial general intelligence through their versatile capabilities, they still lack true any-to-any understanding and generation. Recently, the release of GPT-4o has showcased the remarkable potential of any-to-any LLMs for complex real-world tasks, enabling omnidirectional input and output across images, speech, and text. However, it is closed-source and does not support the generation of multimodal interleaved sequences. To address this gap, we present MIO, which is trained on a mixture of discrete tokens across four modalities using causal multimodal modeling. MIO undergoes a four-stage training process: (1) alignment pre-training, (2) interleaved pre-training, (3) speech-enhanced pre-training, and (4) comprehensive supervised fine-tuning on diverse textual, visual, and speech tasks. Our experimental results indicate that MIO exhibits competitive, and in some cases superior, performance compared to previous dual-modal baselines, any-to-any model baselines, and even modality-specific baselines. Moreover, MIO demonstrates advanced capabilities inherent to its any-to-any feature, such as interleaved video-text generation, chain-of-visual-thought reasoning, visual guideline generation, instructional image editing, etc.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17692v4</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>EMNLP 2025</arxiv:journal_reference>
      <dc:creator>Zekun Wang, King Zhu, Chunpu Xu, Wangchunshu Zhou, Jiaheng Liu, Yibo Zhang, Jiashuo Wang, Ning Shi, Siyu Li, Yizhi Li, Haoran Que, Zhaoxiang Zhang, Yuanxing Zhang, Ge Zhang, Ke Xu, Jie Fu, Wenhao Huang</dc:creator>
    </item>
    <item>
      <title>SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe</title>
      <link>https://arxiv.org/abs/2410.05248</link>
      <description>arXiv:2410.05248v3 Announce Type: replace-cross 
Abstract: To acquire instruction-following capabilities, large language models (LLMs) undergo instruction tuning, where they are trained on instruction-response pairs using next-token prediction (NTP). Efforts to improve instruction tuning often focus on higher-quality supervised fine-tuning (SFT) datasets, typically requiring data filtering with proprietary LLMs or human annotation. In this paper, we take a different approach by proposing SFTMix, a novel Mixup-based recipe that elevates LLM instruction tuning without relying on well-curated datasets. We observe that LLMs exhibit uneven confidence across the semantic representation space. We argue that examples with different confidence levels should play distinct roles in instruction tuning: Confident data is prone to overfitting, while unconfident data is harder to generalize. Based on this insight, SFTMix leverages training dynamics to identify examples with varying confidence levels. We then interpolate them to bridge the confidence gap and apply a Mixup-based regularization to support learning on these additional, interpolated examples. We demonstrate the effectiveness of SFTMix in both instruction-following and healthcare-specific SFT tasks, with consistent improvements across LLM families and SFT datasets of varying sizes and qualities. Extensive analyses across six directions highlight SFTMix's compatibility with data selection, adaptability to compute-constrained scenarios, and scalability to broader applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05248v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxin Xiao, Shujian Zhang, Wenxuan Zhou, Marzyeh Ghassemi, Sanqiang Zhao</dc:creator>
    </item>
    <item>
      <title>Emergent Visual Grounding in Large Multimodal Models Without Grounding Supervision</title>
      <link>https://arxiv.org/abs/2410.08209</link>
      <description>arXiv:2410.08209v2 Announce Type: replace-cross 
Abstract: Current large multimodal models (LMMs) face challenges in grounding, which requires the model to relate language components to visual entities. Contrary to the common practice that fine-tunes LMMs with additional grounding supervision, we find that the grounding ability can in fact emerge in LMMs trained without explicit grounding supervision. To reveal this emerging grounding, we introduce an "attend-and-segment" method which leverages attention maps from standard LMMs to perform pixel-level segmentation. Furthermore, to enhance the grounding ability, we propose DIFFLMM, an LMM utilizing a diffusion-based visual encoder, as opposed to the standard CLIP visual encoder, and trained with the same weak supervision. Without being constrained by the biases and limited scale of grounding-specific supervision data, our approach is more generalizable and scalable. We achieve competitive performance on both grounding-specific and general visual question answering benchmarks, compared with grounding LMMs and generalist LMMs, respectively. Notably, we achieve a 44.2 grounding mask recall on grounded conversation generation without any grounding supervision, outperforming the extensively supervised model GLaMM. Project page: https://GroundLMM-ICCV.github.io.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08209v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Shengcao Cao, Liang-Yan Gui, Yu-Xiong Wang</dc:creator>
    </item>
    <item>
      <title>VoxelPrompt: A Vision Agent for End-to-End Medical Image Analysis</title>
      <link>https://arxiv.org/abs/2410.08397</link>
      <description>arXiv:2410.08397v2 Announce Type: replace-cross 
Abstract: We present VoxelPrompt, an end-to-end image analysis agent that tackles free-form radiological tasks. Given any number of volumetric medical images and a natural language prompt, VoxelPrompt integrates a language model that generates executable code to invoke a jointly-trained, adaptable vision network. This code further carries out analytical steps to address practical quantitative aims, such as measuring the growth of a tumor across visits. The pipelines generated by VoxelPrompt automate analyses that currently require practitioners to painstakingly combine multiple specialized vision and statistical tools. We evaluate VoxelPrompt using diverse neuroimaging tasks and show that it can delineate hundreds of anatomical and pathological features, measure complex morphological properties, and perform open-language analysis of lesion characteristics. VoxelPrompt performs these objectives with an accuracy similar to that of specialist single-task models for image analysis, while facilitating a broad range of compositional biomedical workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08397v2</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Hoopes, Neel Dey, Victor Ion Butoi, John V. Guttag, Adrian V. Dalca</dc:creator>
    </item>
    <item>
      <title>CoreGuard: Safeguarding Foundational Capabilities of LLMs Against Model Stealing in Edge Deployment</title>
      <link>https://arxiv.org/abs/2410.13903</link>
      <description>arXiv:2410.13903v2 Announce Type: replace-cross 
Abstract: Proprietary large language models (LLMs) exhibit strong generalization capabilities across diverse tasks and are increasingly deployed on edge devices for efficiency and privacy reasons. However, deploying proprietary LLMs at the edge without adequate protection introduces critical security threats. Attackers can extract model weights and architectures, enabling unauthorized copying and misuse. Even when protective measures prevent full extraction of model weights, attackers may still perform advanced attacks, such as fine-tuning, to further exploit the model. Existing defenses against these threats typically incur significant computational and communication overhead, making them impractical for edge deployment. To safeguard the edge-deployed LLMs, we introduce CoreGuard, a computation- and communication-efficient protection method. CoreGuard employs an efficient protection protocol to reduce computational overhead and minimize communication overhead via a propagation protocol. Extensive experiments show that CoreGuard achieves upper-bound security protection with negligible overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13903v2</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Qinfeng Li, Tianyue Luo, Xuhong Zhang, Yangfan Xie, Zhiqiang Shen, Lijun Zhang, Yier Jin, Hao Peng, Xinkui Zhao, Xianwei Zhu, Jianwei Yin</dc:creator>
    </item>
    <item>
      <title>AI-generated Essays: Characteristics and Implications on Automated Scoring and Academic Integrity</title>
      <link>https://arxiv.org/abs/2410.17439</link>
      <description>arXiv:2410.17439v4 Announce Type: replace-cross 
Abstract: The rapid advancement of large language models (LLMs) has enabled the generation of coherent essays, making AI-assisted writing increasingly common in educational and professional settings. Using large-scale empirical data, we examine and benchmark the characteristics and quality of essays generated by popular LLMs and discuss their implications for two key components of writing assessments: automated scoring and academic integrity. Our findings highlight limitations in existing automated scoring systems, such as e-rater, when applied to essays generated or heavily influenced by AI, and identify areas for improvement, including the development of new features to capture deeper thinking and recalibrating feature weights. Despite growing concerns that the increasing variety of LLMs may undermine the feasibility of detecting AI-generated essays, our results show that detectors trained on essays generated from one model can often identify texts from others with high accuracy, suggesting that effective detection could remain manageable in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17439v4</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yang Zhong, Jiangang Hao, Michael Fauss, Chen Li, Yuan Wang</dc:creator>
    </item>
    <item>
      <title>Disentangled and Self-Explainable Node Representation Learning</title>
      <link>https://arxiv.org/abs/2410.21043</link>
      <description>arXiv:2410.21043v2 Announce Type: replace-cross 
Abstract: Node representations, or embeddings, are low-dimensional vectors that capture node properties, typically learned through unsupervised structural similarity objectives or supervised tasks. While recent efforts have focused on explaining graph model decisions, the interpretability of unsupervised node embeddings remains underexplored. To bridge this gap, we introduce DiSeNE (Disentangled and Self-Explainable Node Embedding), a framework that generates self-explainable embeddings in an unsupervised manner. Our method employs disentangled representation learning to produce dimension-wise interpretable embeddings, where each dimension is aligned with distinct topological structure of the graph. We formalize novel desiderata for disentangled and interpretable embeddings, which drive our new objective functions, optimizing simultaneously for both interpretability and disentanglement. Additionally, we propose several new metrics to evaluate representation quality and human interpretability. Extensive experiments across multiple benchmark datasets demonstrate the effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21043v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ML</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simone Piaggesi, Andr\'e Panisson, Megha Khosla</dc:creator>
    </item>
    <item>
      <title>An AI-Driven Multimodal Smart Home Platform for Continuous Monitoring and Assistance in Post-Stroke Motor Impairment</title>
      <link>https://arxiv.org/abs/2411.19000</link>
      <description>arXiv:2411.19000v4 Announce Type: replace-cross 
Abstract: At-home rehabilitation for post-stroke patients presents significant challenges, as continuous, personalized care is often limited outside clinical settings. Moreover, the lack of integrated solutions capable of simultaneously monitoring motor recovery and providing intelligent assistance in home environments hampers rehabilitation outcomes. Here, we present a multimodal smart home platform designed for continuous, at-home rehabilitation of post-stroke patients, integrating wearable sensing, ambient monitoring, and adaptive automation. A plantar pressure insole equipped with a machine learning pipeline classifies users into motor recovery stages with up to 94\% accuracy, enabling quantitative tracking of walking patterns during daily activities. An optional head-mounted eye-tracking module, together with ambient sensors such as cameras and microphones, supports seamless hands-free control of household devices with a 100\% success rate and sub-second response time. These data streams are fused locally via a hierarchical Internet of Things (IoT) architecture, ensuring low latency and data privacy. An embedded large language model (LLM) agent, Auto-Care, continuously interprets multimodal data to provide real-time interventions -- issuing personalized reminders, adjusting environmental conditions, and notifying caregivers. Implemented in a post-stroke context, this integrated smart home platform increased mean user satisfaction from 3.9 $\pm$ 0.8 in conventional home environments to 8.4 $\pm$ 0.6 with the full system ($n=20$). Beyond stroke, the system offers a scalable, patient-centered framework with potential for long-term use in broader neurorehabilitation and aging-in-place applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19000v4</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenyu Tang, Ruizhi Zhang, Shuo Gao, Zihe Zhao, Zibo Zhang, Jiaqi Wang, Cong Li, Junliang Chen, Yanning Dai, Shengbo Wang, Ruoyu Juan, Qiaoying Li, Ruimou Xie, Xuhang Chen, Xinkai Zhou, Yunjia Xia, Jianan Chen, Fanghao Lu, Xin Li, Ninglli Wang, Peter Smielewski, Yu Pan, Hubin Zhao, Luigi G. Occhipinti</dc:creator>
    </item>
    <item>
      <title>Moto: Latent Motion Token as the Bridging Language for Learning Robot Manipulation from Videos</title>
      <link>https://arxiv.org/abs/2412.04445</link>
      <description>arXiv:2412.04445v4 Announce Type: replace-cross 
Abstract: Recent developments in Large Language Models pre-trained on extensive corpora have shown significant success in various natural language processing tasks with minimal fine-tuning. This success offers new promise for robotics, which has long been constrained by the high cost of action-labeled data. We ask: given the abundant video data containing interaction-related knowledge available as a rich "corpus", can a similar generative pre-training approach be effectively applied to enhance robot learning? The key challenge is to identify an effective representation for autoregressive pre-training that benefits robot manipulation tasks. Inspired by the way humans learn new skills through observing dynamic environments, we propose that effective robotic learning should emphasize motion-related knowledge, which is closely tied to low-level actions and is hardware-agnostic, facilitating the transfer of learned motions to actual robot actions. To this end, we introduce Moto, which converts video content into latent Motion Token sequences by a Latent Motion Tokenizer, learning a bridging "language" of motion from videos in an unsupervised manner. We pre-train Moto-GPT through motion token autoregression, enabling it to capture diverse visual motion knowledge. After pre-training, Moto-GPT demonstrates the promising ability to produce semantically interpretable motion tokens, predict plausible motion trajectories, and assess trajectory rationality through output likelihood. To transfer learned motion priors to real robot actions, we implement a co-fine-tuning strategy that seamlessly bridges latent motion token prediction and real robot control. Extensive experiments show that the fine-tuned Moto-GPT exhibits superior robustness and efficiency on robot manipulation benchmarks, underscoring its effectiveness in transferring knowledge from video data to downstream visual manipulation tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04445v4</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi Chen, Yuying Ge, Weiliang Tang, Yizhuo Li, Yixiao Ge, Mingyu Ding, Ying Shan, Xihui Liu</dc:creator>
    </item>
    <item>
      <title>VERITAS: Verifying the Performance of AI-native Transceiver Actions in Base-Stations</title>
      <link>https://arxiv.org/abs/2501.09761</link>
      <description>arXiv:2501.09761v2 Announce Type: replace-cross 
Abstract: Artificial Intelligence (AI)-native receivers prove significant performance improvement in high noise regimes and can potentially reduce communication overhead compared to the traditional receiver. However, their performance highly depends on the representativeness of the training dataset. A major issue is the uncertainty of whether the training dataset covers all test environments and waveform configurations, and thus, whether the trained model is robust in practical deployment conditions. To this end, we propose a joint measurement-recovery framework for AI-native transceivers post deployment, called VERITAS, that continuously looks for distribution shifts in the received signals and triggers finite re-training spurts. VERITAS monitors the wireless channel using 5G pilots fed to an auxiliary neural network that detects out-of-distribution channel profile, transmitter speed, and delay spread. As soon as such a change is detected, a traditional (reference) receiver is activated, which runs for a period of time in parallel to the AI-native receiver. Finally, VERTIAS compares the bit probabilities of the AI-native and the reference receivers for the same received data inputs, and decides whether or not a retraining process needs to be initiated. Our evaluations reveal that VERITAS can detect changes in the channel profile, transmitter speed, and delay spread with 99%, 97%, and 69% accuracies, respectively, followed by timely initiation of retraining for 86%, 93.3%, and 94.8% of inputs in channel profile, transmitter speed, and delay spread test sets, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09761v2</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nasim Soltani, Michael Loehning, Kaushik Chowdhury</dc:creator>
    </item>
    <item>
      <title>Polynomial-Time Algorithms for Fair Orientations of Chores</title>
      <link>https://arxiv.org/abs/2501.13481</link>
      <description>arXiv:2501.13481v3 Announce Type: replace-cross 
Abstract: This paper addresses the problem of finding fair orientations of graphs of chores, in which each vertex corresponds to an agent, each edge corresponds to a chore, and a chore has zero marginal utility to an agent if its corresponding edge is not incident to the vertex corresponding to the agent. Recently, Zhou et al. (IJCAI, 2024) analyzed the complexity of deciding whether graphs containing a mixture of goods and chores have EFX orientations, and conjectured that deciding whether graphs containing only chores have EFX orientations is NP-complete. We resolve this conjecture by giving polynomial-time algorithms that find EF1 and EFX orientations of graphs containing only chores if they exist, even if there are self-loops. Remarkably, our result demonstrates a surprising separation between the case of goods and the case of chores, because deciding whether graphs containing only goods have EFX orientations was shown to be NP-complete by Christodoulou et al. (EC, 2023). In addition, we show the EF1 and EFX orientation problems for multigraphs to be NP-complete.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13481v3</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.DM</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kevin Hsu, Valerie King</dc:creator>
    </item>
    <item>
      <title>The Last Dependency Crusade: Solving Python Dependency Conflicts with LLMs</title>
      <link>https://arxiv.org/abs/2501.16191</link>
      <description>arXiv:2501.16191v2 Announce Type: replace-cross 
Abstract: Resolving Python dependency issues remains a tedious and error-prone process, forcing developers to manually trial compatible module versions and interpreter configurations. Existing automated solutions, such as knowledge-graph-based and database-driven methods, face limitations due to the variety of dependency error types, large sets of possible module versions, and conflicts among transitive dependencies. This paper investigates the use of Large Language Models (LLMs) to automatically repair dependency issues in Python programs. We propose PLLM (pronounced "plum"), a novel retrieval-augmented generation (RAG) approach that iteratively infers missing or incorrect dependencies. PLLM builds a test environment where the LLM proposes module combinations, observes execution feedback, and refines its predictions using natural language processing (NLP) to parse error messages. We evaluate PLLM on the Gistable HG2.9K dataset, a curated collection of real-world Python programs. Using this benchmark, we explore multiple PLLM configurations, including six open-source LLMs evaluated both with and without RAG. Our findings show that RAG consistently improves fix rates, with the best performance achieved by Gemma-2 9B when combined with RAG. Compared to two state-of-the-art baselines, PyEGo and ReadPyE, PLLM achieves significantly higher fix rates; +15.97\% more than ReadPyE and +21.58\% more than PyEGo. Further analysis shows that PLLM is especially effective for projects with numerous dependencies and those using specialized numerical or machine-learning libraries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16191v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antony Bartlett, Cynthia Liem, Annibale Panichella</dc:creator>
    </item>
    <item>
      <title>FedRTS: Federated Robust Pruning via Combinatorial Thompson Sampling</title>
      <link>https://arxiv.org/abs/2501.19122</link>
      <description>arXiv:2501.19122v3 Announce Type: replace-cross 
Abstract: Federated Learning (FL) enables collaborative model training across distributed clients without data sharing, but its high computational and communication demands strain resource-constrained devices. While existing methods use dynamic pruning to improve efficiency by periodically adjusting sparse model topologies while maintaining sparsity, these approaches suffer from issues such as greedy adjustments, unstable topologies, and communication inefficiency, resulting in less robust models and suboptimal performance under data heterogeneity and partial client availability. To address these challenges, we propose Federated Robust pruning via combinatorial Thompson Sampling (FedRTS), a novel framework designed to develop robust sparse models. FedRTS enhances robustness and performance through its Thompson Sampling-based Adjustment (TSAdj) mechanism, which uses probabilistic decisions informed by stable, farsighted information instead of deterministic decisions reliant on unstable and myopic information in previous methods. Extensive experiments demonstrate that FedRTS achieves state-of-the-art performance in computer vision and natural language processing tasks while reducing communication costs, particularly excelling in scenarios with heterogeneous data distributions and partial client participation. Our codes are available at: https://github.com/Little0o0/FedRTS</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19122v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hong Huang, Hai Yang, Yuan Chen, Jiaxun Ye, Dapeng Wu</dc:creator>
    </item>
    <item>
      <title>The simulation of judgment in LLMs</title>
      <link>https://arxiv.org/abs/2502.04426</link>
      <description>arXiv:2502.04426v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are increasingly embedded in evaluative processes, from information filtering to assessing and addressing knowledge gaps through explanation and credibility judgments. This raises the need to examine how such evaluations are built, what assumptions they rely on, and how their strategies diverge from those of humans. We benchmark six LLMs against expert ratings--NewsGuard and Media Bias/Fact Check--and against human judgments collected through a controlled experiment. We use news domains purely as a controlled benchmark for evaluative tasks, focusing on the underlying mechanisms rather than on news classification per se. To enable direct comparison, we implement a structured agentic framework in which both models and nonexpert participants follow the same evaluation procedure: selecting criteria, retrieving content, and producing justifications. Despite output alignment, our findings show consistent differences in the observable criteria guiding model evaluations, suggesting that lexical associations and statistical priors could influence evaluations in ways that differ from contextual reasoning. This reliance is associated with systematic effects: political asymmetries and a tendency to confuse linguistic form with epistemic reliability--a dynamic we term epistemia, the illusion of knowledge that emerges when surface plausibility replaces verification. Indeed, delegating judgment to such systems may affect the heuristics underlying evaluative processes, suggesting a shift from normative reasoning toward pattern-based approximation and raising open questions about the role of LLMs in evaluative processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04426v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1073/pnas.2518443122</arxiv:DOI>
      <arxiv:journal_reference>Proc. Natl. Acad. Sci. U.S.A. 122 (42) e2518443122, 2025</arxiv:journal_reference>
      <dc:creator>Edoardo Loru, Jacopo Nudo, Niccol\`o Di Marco, Alessandro Santirocchi, Roberto Atzeni, Matteo Cinelli, Vincenzo Cestari, Clelia Rossi-Arnaud, Walter Quattrociocchi</dc:creator>
    </item>
    <item>
      <title>Evaluating Sakana's AI Scientist: Bold Claims, Mixed Results, and a Promising Future?</title>
      <link>https://arxiv.org/abs/2502.14297</link>
      <description>arXiv:2502.14297v3 Announce Type: replace-cross 
Abstract: A major step toward Artificial General Intelligence (AGI) and Super Intelligence is AI's ability to autonomously conduct research - what we term Artificial Research Intelligence (ARI). If machines could generate hypotheses, conduct experiments, and write research papers without human intervention, it would transform science. Sakana recently introduced the 'AI Scientist', claiming to conduct research autonomously, i.e. they imply to have achieved what we term Artificial Research Intelligence (ARI). The AI Scientist gained much attention, but a thorough independent evaluation has yet to be conducted.
  Our evaluation of the AI Scientist reveals critical shortcomings. The system's literature reviews produced poor novelty assessments, often misclassifying established concepts (e.g., micro-batching for stochastic gradient descent) as novel. It also struggles with experiment execution: 42% of experiments failed due to coding errors, while others produced flawed or misleading results. Code modifications were minimal, averaging 8% more characters per iteration, suggesting limited adaptability. Generated manuscripts were poorly substantiated, with a median of five citations, most outdated (only five of 34 from 2020 or later). Structural errors were frequent, including missing figures, repeated sections, and placeholder text like 'Conclusions Here'. Some papers contained hallucinated numerical results.
  Despite these flaws, the AI Scientist represents a leap forward in research automation. It generates full research manuscripts with minimal human input, challenging expectations of AI-driven science. Many reviewers might struggle to distinguish its work from human researchers. While its quality resembles a rushed undergraduate paper, its speed and cost efficiency are unprecedented, producing a full paper for USD 6 to 15 with 3.5 hours of human involvement, far outpacing traditional researchers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14297v3</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3769733.376974</arxiv:DOI>
      <arxiv:journal_reference>SIGIR Forum 2025</arxiv:journal_reference>
      <dc:creator>Joeran Beel, Min-Yen Kan, Moritz Baumgart</dc:creator>
    </item>
    <item>
      <title>Tokenizing Single-Channel EEG with Time-Frequency Motif Learning</title>
      <link>https://arxiv.org/abs/2502.16060</link>
      <description>arXiv:2502.16060v3 Announce Type: replace-cross 
Abstract: Foundation models are reshaping EEG analysis, yet an important problem of EEG tokenization remains a challenge. This paper presents TFM-Tokenizer, a novel tokenization framework that learns a vocabulary of time-frequency motifs from single-channel EEG signals and encodes them into discrete tokens. We propose a dual-path architecture with time-frequency masking to capture robust motif representations, and it is model-agnostic, supporting both lightweight transformers and existing foundation models for downstream tasks. Our study demonstrates three key benefits: Accuracy: Experiments on four diverse EEG benchmarks demonstrate consistent performance gains across both single- and multi-dataset pretraining settings, achieving up to 17% improvement in Cohen's Kappa over strong baselines. Generalization: Moreover, as a plug-and-play component, it consistently boosts the performance of diverse foundation models, including BIOT and LaBraM. Scalability: By operating at the single-channel level rather than relying on the strict 10-20 EEG system, our method has the potential to be device-agnostic. Experiments on ear-EEG sleep staging, which differs from the pretraining data in signal format, channel configuration, recording device, and task, show that our tokenizer outperforms baselines by 14%. A comprehensive token analysis reveals strong class-discriminative, frequency-aware, and consistent structure, enabling improved representation quality and interpretability. Code is available at https://github.com/Jathurshan0330/TFM-Tokenizer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16060v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>eess.SP</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jathurshan Pradeepkumar, Xihao Piao, Zheng Chen, Jimeng Sun</dc:creator>
    </item>
    <item>
      <title>Never too Prim to Swim: An LLM-Enhanced RL-based Adaptive S-Surface Controller for AUVs under Extreme Sea Conditions</title>
      <link>https://arxiv.org/abs/2503.00527</link>
      <description>arXiv:2503.00527v2 Announce Type: replace-cross 
Abstract: The adaptivity and maneuvering capabilities of Autonomous Underwater Vehicles (AUVs) have drawn significant attention in oceanic research, due to the unpredictable disturbances and strong coupling among the AUV's degrees of freedom. In this paper, we developed large language model (LLM)-enhanced reinforcement learning (RL)-based adaptive S-surface controller for AUVs. Specifically, LLMs are introduced for the joint optimization of controller parameters and reward functions in RL training. Using multi-modal and structured explicit task feedback, LLMs enable joint adjustments, balance multiple objectives, and enhance task-oriented performance and adaptability. In the proposed controller, the RL policy focuses on upper-level tasks, outputting task-oriented high-level commands that the S-surface controller then converts into control signals, ensuring cancellation of nonlinear effects and unpredictable external disturbances in extreme sea conditions. Under extreme sea conditions involving complex terrain, waves, and currents, the proposed controller demonstrates superior performance and adaptability in high-level tasks such as underwater target tracking and data collection, outperforming traditional PID and SMC controllers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00527v2</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guanwen Xie, Jingzehua Xu, Yimian Ding, Zhi Zhang, Shuai Zhang, Yi Li</dc:creator>
    </item>
    <item>
      <title>A Neural Symbolic Model for Space Physics</title>
      <link>https://arxiv.org/abs/2503.07994</link>
      <description>arXiv:2503.07994v2 Announce Type: replace-cross 
Abstract: In this study, we unveil a new AI model, termed PhyE2E, to discover physical formulas through symbolic regression. PhyE2E simplifies symbolic regression by decomposing it into sub-problems using the second-order derivatives of an oracle neural network, and employs a transformer model to translate data into symbolic formulas in an end-to-end manner. The resulting formulas are refined through Monte-Carlo Tree Search and Genetic Programming. We leverage a large language model to synthesize extensive symbolic expressions resembling real physics, and train the model to recover these formulas directly from data. A comprehensive evaluation reveals that PhyE2E outperforms existing state-of-the-art approaches, delivering superior symbolic accuracy, precision in data fitting, and consistency in physical units. We deployed PhyE2E to five applications in space physics, including the prediction of sunspot numbers, solar rotational angular velocity, emission line contribution functions, near-Earth plasma pressure, and lunar-tide plasma signals. The physical formulas generated by AI demonstrate a high degree of accuracy in fitting the experimental data from satellites and astronomical telescopes. We have successfully upgraded the formula proposed by NASA in 1993 regarding solar activity, and for the first time, provided the explanations for the long cycle of solar activity in an explicit form. We also found that the decay of near-Earth plasma pressure is proportional to r^2 to Earth, where subsequent mathematical derivations are consistent with satellite data from another independent study. Moreover, we found physical formulas that can describe the relationships between emission lines in the extreme ultraviolet spectrum of the Sun, temperatures, electron densities, and magnetic fields. The formula obtained is consistent with the properties that physicists had previously hypothesized it should possess.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07994v2</guid>
      <category>astro-ph.SR</category>
      <category>astro-ph.EP</category>
      <category>astro-ph.IM</category>
      <category>cs.AI</category>
      <category>physics.space-ph</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jie Ying, Haowei Lin, Chao Yue, Yajie Chen, Chao Xiao, Quanqi Shi, Yitao Liang, Shing-Tung Yau, Yuan Zhou, Jianzhu Ma</dc:creator>
    </item>
    <item>
      <title>Leveraging LLMs, IDEs, and Semantic Embeddings for Automated Move Method Refactoring</title>
      <link>https://arxiv.org/abs/2503.20934</link>
      <description>arXiv:2503.20934v2 Announce Type: replace-cross 
Abstract: MOVEMETHOD is a hallmark refactoring. Despite a plethora of research tools that recommend which methods to move and where, these recommendations do not align with how expert developers perform MOVEMETHOD. Given the extensive training of Large Language Models and their reliance upon naturalness of code, they should expertly recommend which methods are misplaced in a given class and which classes are better hosts. Our formative study of 2016 LLM recommendations revealed that LLMs give expert suggestions, yet they are unreliable: up to 80% of the suggestions are hallucinations. We introduce the first LLM fully powered assistant for MOVEMETHOD refactoring that automates its whole end-to-end lifecycle, from recommendation to execution. We designed novel solutions that automatically filter LLM hallucinations using static analysis from IDEs and a novel workflow that requires LLMs to be self-consistent, critique, and rank refactoring suggestions. As MOVEMETHOD refactoring requires global, projectlevel reasoning, we solved the limited context size of LLMs by employing refactoring-aware retrieval augment generation (RAG). Our approach, MM-assist, synergistically combines the strengths of the LLM, IDE, static analysis, and semantic relevance. In our thorough, multi-methodology empirical evaluation, we compare MM-assist with the previous state-of-the-art approaches. MM-assist significantly outperforms them: (i) on a benchmark widely used by other researchers, our Recall@1 and Recall@3 show a 1.7x improvement; (ii) on a corpus of 210 recent refactorings from Open-source software, our Recall rates improve by at least 2.4x. Lastly, we conducted a user study with 30 experienced participants who used MM-assist to refactor their own code for one week. They rated 82.8% of MM-assist recommendations positively. This shows that MM-assist is both effective and useful.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20934v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhiram Bellur, Fraol Batole, Mohammed Raihan Ullah, Malinda Dilhara, Yaroslav Zharov, Timofey Bryksin, Kai Ishikawa, Haifeng Chen, Masaharu Morimoto, Shota Motoura, Takeo Hosomi, Tien N. Nguyen, Hridesh Rajan, Nikolaos Tsantalis, Danny Dig</dc:creator>
    </item>
    <item>
      <title>Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning</title>
      <link>https://arxiv.org/abs/2504.02922</link>
      <description>arXiv:2504.02922v3 Announce Type: replace-cross 
Abstract: Model diffing is the study of how fine-tuning changes a model's representations and internal algorithms. Many behaviors of interest are introduced during fine-tuning, and model diffing offers a promising lens to interpret such behaviors. Crosscoders are a recent model diffing method that learns a shared dictionary of interpretable concepts represented as latent directions in both the base and fine-tuned models, allowing us to track how concepts shift or emerge during fine-tuning. Notably, prior work has observed concepts with no direction in the base model, and it was hypothesized that these model-specific latents were concepts introduced during fine-tuning. However, we identify two issues which stem from the crosscoders L1 training loss that can misattribute concepts as unique to the fine-tuned model, when they really exist in both models. We develop Latent Scaling to flag these issues by more accurately measuring each latent's presence across models. In experiments comparing Gemma 2 2B base and chat models, we observe that the standard crosscoder suffers heavily from these issues. Building on these insights, we train a crosscoder with BatchTopK loss and show that it substantially mitigates these issues, finding more genuinely chat-specific and highly interpretable concepts. We recommend practitioners adopt similar techniques. Using the BatchTopK crosscoder, we successfully identify a set of chat-specific latents that are both interpretable and causally effective, representing concepts such as $\textit{false information}$ and $\textit{personal question}$, along with multiple refusal-related latents that show nuanced preferences for different refusal triggers. Overall, our work advances best practices for the crosscoder-based methodology for model diffing and demonstrates that it can provide concrete insights into how chat-tuning modifies model behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02922v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julian Minder, Cl\'ement Dumas, Caden Juang, Bilal Chugtai, Neel Nanda</dc:creator>
    </item>
    <item>
      <title>EDIT: Enhancing Vision Transformers by Mitigating Attention Sink through an Encoder-Decoder Architecture</title>
      <link>https://arxiv.org/abs/2504.06738</link>
      <description>arXiv:2504.06738v2 Announce Type: replace-cross 
Abstract: In this paper, we propose EDIT (Encoder-Decoder Image Transformer), a novel architecture designed to mitigate the attention sink phenomenon observed in Vision Transformer models. Attention sink occurs when an excessive amount of attention is allocated to the [CLS] token, distorting the model's ability to effectively process image patches. To address this, we introduce a layer-aligned encoder-decoder architecture, where the encoder utilizes self-attention to process image patches, while the decoder uses cross-attention to focus on the [CLS] token. Unlike traditional encoder-decoder framework, where the decoder depends solely on high-level encoder representations, EDIT allows the decoder to extract information starting from low-level features, progressively refining the representation layer by layer. EDIT is naturally interpretable demonstrated through sequential attention maps, illustrating the refined, layer-by-layer focus on key image features. Experiments on ImageNet-1k and ImageNet-21k, along with transfer learning tasks, show that EDIT achieves consistent performance improvements over DeiT3 models. These results highlight the effectiveness of EDIT's design in addressing attention sink and improving visual feature extraction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06738v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenfeng Feng, Hongxiang Wang, Jianlong Wang, Xin Zhang, Jingjing Zhao, Yueyue Liang, Xiang Chen, Duokui Han</dc:creator>
    </item>
    <item>
      <title>Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge</title>
      <link>https://arxiv.org/abs/2504.07887</link>
      <description>arXiv:2504.07887v2 Announce Type: replace-cross 
Abstract: The growing integration of Large Language Models (LLMs) into critical societal domains has raised concerns about embedded biases that can perpetuate stereotypes and undermine fairness. Such biases may stem from historical inequalities in training data, linguistic imbalances, or adversarial manipulation. Despite mitigation efforts, recent studies show that LLMs remain vulnerable to adversarial attacks that elicit biased outputs. This work proposes a scalable benchmarking framework to assess LLM robustness to adversarial bias elicitation. Our methodology involves: (i) systematically probing models across multiple tasks targeting diverse sociocultural biases, (ii) quantifying robustness through safety scores using an LLM-as-a-Judge approach, and (iii) employing jailbreak techniques to reveal safety vulnerabilities. To facilitate systematic benchmarking, we release a curated dataset of bias-related prompts, named CLEAR-Bias. Our analysis, identifying DeepSeek V3 as the most reliable judge LLM, reveals that bias resilience is uneven, with age, disability, and intersectional biases among the most prominent. Some small models outperform larger ones in safety, suggesting that training and architecture may matter more than scale. However, no model is fully robust to adversarial elicitation, with jailbreak attacks using low-resource languages or refusal suppression proving effective across model families. We also find that successive LLM generations exhibit slight safety gains, while models fine-tuned for the medical domain tend to be less safe than their general-purpose counterparts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07887v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s10994-025-06862-6</arxiv:DOI>
      <arxiv:journal_reference>Cantini, R., Orsino, A., Ruggiero, M., Talia, D. Benchmarking adversarial robustness to bias elicitation in large language models: scalable automated assessment with LLM-as-a-judge. Mach Learn 114, 249 (2025)</arxiv:journal_reference>
      <dc:creator>Riccardo Cantini, Alessio Orsino, Massimo Ruggiero, Domenico Talia</dc:creator>
    </item>
    <item>
      <title>On Equivariance and Fast Sampling in Video Diffusion Models Trained with Warped Noise</title>
      <link>https://arxiv.org/abs/2504.09789</link>
      <description>arXiv:2504.09789v2 Announce Type: replace-cross 
Abstract: Temporally consistent video-to-video generation is critical for applications such as style transfer and upsampling. In this paper, we provide a theoretical analysis of warped noise - a recently proposed technique for training video diffusion models - and show that pairing it with the standard denoising objective implicitly trains models to be equivariant to spatial transformations of the input noise, which we term EquiVDM. This equivariance enables motion in the input noise to align naturally with motion in the generated video, yielding coherent, high-fidelity outputs without the need for specialized modules or auxiliary losses. A further advantage is sampling efficiency: EquiVDM achieves comparable or superior quality in far fewer sampling steps. When distilled into one-step student models, EquiVDM preserves equivariance and delivers stronger motion controllability and fidelity than distilled nonequivariant baselines. Across benchmarks, EquiVDM consistently outperforms prior methods in motion alignment, temporal consistency, and perceptual quality, while substantially lowering sampling cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09789v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chao Liu, Arash Vahdat</dc:creator>
    </item>
    <item>
      <title>Restoring Calibration for Aligned Large Language Models: A Calibration-Aware Fine-Tuning Approach</title>
      <link>https://arxiv.org/abs/2505.01997</link>
      <description>arXiv:2505.01997v3 Announce Type: replace-cross 
Abstract: One of the key technologies for the success of Large Language Models (LLMs) is preference alignment. However, a notable side effect of preference alignment is poor calibration: while the pre-trained models are typically well-calibrated, LLMs tend to become poorly calibrated after alignment with human preferences. In this paper, we investigate why preference alignment affects calibration and how to address this issue. For the first question, we observe that the preference collapse issue in alignment undesirably generalizes to the calibration scenario, causing LLMs to exhibit overconfidence and poor calibration. To address this, we demonstrate the importance of fine-tuning with domain-specific knowledge to alleviate the overconfidence issue. To further analyze whether this affects the model's performance, we categorize models into two regimes: calibratable and non-calibratable, defined by bounds of Expected Calibration Error (ECE). In the calibratable regime, we propose a calibration-aware fine-tuning approach to achieve proper calibration without compromising LLMs' performance. However, as models are further fine-tuned for better performance, they enter the non-calibratable regime. For this case, we develop an EM-algorithm-based ECE regularization for the fine-tuning loss to maintain low calibration error. Extensive experiments validate the effectiveness of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01997v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ML</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>ICML 2025</arxiv:journal_reference>
      <dc:creator>Jiancong Xiao, Bojian Hou, Zhanliang Wang, Ruochen Jin, Qi Long, Weijie J. Su, Li Shen</dc:creator>
    </item>
    <item>
      <title>Absolute Zero: Reinforced Self-play Reasoning with Zero Data</title>
      <link>https://arxiv.org/abs/2505.03335</link>
      <description>arXiv:2505.03335v3 Announce Type: replace-cross 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from outcome-based rewards. Recent RLVR works that operate under the zero setting avoid supervision in labeling the reasoning process, but still depend on manually curated collections of questions and answers for training. The scarcity of high-quality, human-produced examples raises concerns about the long-term scalability of relying on human supervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a hypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited learning potential for a superintelligent system. To address these concerns, we propose a new RLVR paradigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own learning progress and improves reasoning by solving them, without relying on any external data. Under this paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training curriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks and verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded learning. Despite being trained entirely without external data, AZR achieves overall SOTA performance on coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens of thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be effectively applied across different model scales and is compatible with various model classes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03335v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Yang Yue, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng, Gao Huang</dc:creator>
    </item>
    <item>
      <title>LLMs' Suitability for Network Security: A Case Study of STRIDE Threat Modeling</title>
      <link>https://arxiv.org/abs/2505.04101</link>
      <description>arXiv:2505.04101v2 Announce Type: replace-cross 
Abstract: Artificial Intelligence (AI) is expected to be an integral part of next-generation AI-native 6G networks. With the prevalence of AI, researchers have identified numerous use cases of AI in network security. However, there are very few studies that analyze the suitability of Large Language Models (LLMs) in network security. To fill this gap, we examine the suitability of LLMs in network security, particularly with the case study of STRIDE threat modeling. We utilize four prompting techniques with five LLMs to perform STRIDE classification of 5G threats. From our evaluation results, we point out key findings and detailed insights along with the explanation of the possible underlying factors influencing the behavior of LLMs in the modeling of certain threats. The numerical results and the insights support the necessity for adjusting and fine-tuning LLMs for network security use cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04101v2</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>AbdulAziz AbdulGhaffar, Ashraf Matrawy</dc:creator>
    </item>
    <item>
      <title>The ML.ENERGY Benchmark: Toward Automated Inference Energy Measurement and Optimization</title>
      <link>https://arxiv.org/abs/2505.06371</link>
      <description>arXiv:2505.06371v2 Announce Type: replace-cross 
Abstract: As the adoption of Generative AI in real-world services grow explosively, energy has emerged as a critical bottleneck resource. However, energy remains a metric that is often overlooked, under-explored, or poorly understood in the context of building ML systems. We present the ML$.$ENERGY Benchmark, a benchmark suite and tool for measuring inference energy consumption under realistic service environments, and the corresponding ML$.$ENERGY Leaderboard, which have served as a valuable resource for those hoping to understand and optimize the energy consumption of their generative AI services. In this paper, we explain four key design principles for benchmarking ML energy we have acquired over time, and then describe how they are implemented in the ML$.$ENERGY Benchmark. We then highlight results from the early 2025 iteration of the benchmark, including energy measurements of 40 widely used model architectures across 6 different tasks, case studies of how ML design choices impact energy consumption, and how automated optimization recommendations can lead to significant (sometimes more than 40%) energy savings without changing what is being computed by the model. The ML$.$ENERGY Benchmark is open-source and can be easily extended to various customized models and application scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06371v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jae-Won Chung, Jeff J. Ma, Ruofan Wu, Jiachen Liu, Oh Jun Kweon, Yuxuan Xia, Zhiyu Wu, Mosharaf Chowdhury</dc:creator>
    </item>
    <item>
      <title>Internet of Agents: Fundamentals, Applications, and Challenges</title>
      <link>https://arxiv.org/abs/2505.07176</link>
      <description>arXiv:2505.07176v2 Announce Type: replace-cross 
Abstract: With the rapid proliferation of large language models and vision-language models, AI agents have evolved from isolated, task-specific systems into autonomous, interactive entities capable of perceiving, reasoning, and acting without human intervention. As these agents proliferate across virtual and physical environments, from virtual assistants to embodied robots, the need for a unified, agent-centric infrastructure becomes paramount. In this survey, we introduce the Internet of Agents (IoA) as a foundational framework that enables seamless interconnection, dynamic discovery, and collaborative orchestration among heterogeneous agents at scale. We begin by presenting a general IoA architecture, highlighting its hierarchical organization, distinguishing features relative to the traditional Internet, and emerging applications. Next, we analyze the key operational enablers of IoA, including capability notification and discovery, adaptive communication protocols, dynamic task matching, consensus and conflict-resolution mechanisms, and incentive models. Finally, we identify open research directions toward building resilient and trustworthy IoA ecosystems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07176v2</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuntao Wang, Shaolong Guo, Yanghe Pan, Zhou Su, Fahao Chen, Tom H. Luan, Peng Li, Jiawen Kang, Dusit Niyato</dc:creator>
    </item>
    <item>
      <title>ConDiSim: Conditional Diffusion Models for Simulation Based Inference</title>
      <link>https://arxiv.org/abs/2505.08403</link>
      <description>arXiv:2505.08403v2 Announce Type: replace-cross 
Abstract: We present a conditional diffusion model - ConDiSim, for simulation-based inference of complex systems with intractable likelihoods. ConDiSim leverages denoising diffusion probabilistic models to approximate posterior distributions, consisting of a forward process that adds Gaussian noise to parameters, and a reverse process learning to denoise, conditioned on observed data. This approach effectively captures complex dependencies and multi-modalities within posteriors. ConDiSim is evaluated across ten benchmark problems and two real-world test problems, where it demonstrates effective posterior approximation accuracy while maintaining computational efficiency and stability in model training. ConDiSim offers a robust and extensible framework for simulation-based inference, particularly suitable for parameter inference workflows requiring fast inference methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08403v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ML</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mayank Nautiyal, Andreas Hellander, Prashant Singh</dc:creator>
    </item>
    <item>
      <title>APEX: Empowering LLMs with Physics-Based Task Planning for Real-time Insight</title>
      <link>https://arxiv.org/abs/2505.13921</link>
      <description>arXiv:2505.13921v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) demonstrate strong reasoning and task planning capabilities but remain fundamentally limited in physical interaction modeling. Existing approaches integrate perception via Vision-Language Models (VLMs) or adaptive decision-making through Reinforcement Learning (RL), but they fail to capture dynamic object interactions or require task-specific training, limiting their real-world applicability. We introduce APEX (Anticipatory Physics-Enhanced Execution), a framework that equips LLMs with physics-driven foresight for real-time task planning. APEX constructs structured graphs to identify and model the most relevant dynamic interactions in the environment, providing LLMs with explicit physical state updates. Simultaneously, APEX provides low-latency forward simulations of physically feasible actions, allowing LLMs to select optimal strategies based on predictive outcomes rather than static observations. We evaluate APEX on three benchmarks designed to assess perception, prediction, and decision-making: (1) Physics Reasoning Benchmark, testing causal inference and object motion prediction; (2) Tetris, evaluating whether physics-informed prediction enhances decision-making performance in long-horizon planning tasks; (3) Dynamic Obstacle Avoidance, assessing the immediate integration of perception and action feasibility analysis. APEX significantly outperforms standard LLMs and VLM-based models, demonstrating the necessity of explicit physics reasoning for bridging the gap between language-based intelligence and real-world task execution. The source code and experiment setup are publicly available at https://github.com/hwj20/APEX_EXP .</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13921v2</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wanjing Huang, Weixiang Yan, Zhen Zhang, Ambuj Singh</dc:creator>
    </item>
    <item>
      <title>Checkpoint-GCG: Auditing and Attacking Fine-Tuning-Based Prompt Injection Defenses</title>
      <link>https://arxiv.org/abs/2505.15738</link>
      <description>arXiv:2505.15738v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are increasingly deployed in real-world applications ranging from chatbots to agentic systems, where they are expected to process untrusted data and follow trusted instructions. Failure to distinguish between the two poses significant security risks, exploited by prompt injection attacks, which inject malicious instructions into the data to control model outputs. Model-level defenses have been proposed to mitigate prompt injection attacks. These defenses fine-tune LLMs to ignore injected instructions in untrusted data. We introduce Checkpoint-GCG, a white-box attack against fine-tuning-based defenses. Checkpoint-GCG enhances the Greedy Coordinate Gradient (GCG) attack by leveraging intermediate model checkpoints produced during fine-tuning to initialize GCG, with each checkpoint acting as a stepping stone for the next one to continuously improve attacks. First, we instantiate Checkpoint-GCG to evaluate the robustness of the state-of-the-art defenses in an auditing setup, assuming both (a) full knowledge of the model input and (b) access to intermediate model checkpoints. We show Checkpoint-GCG to achieve up to $96\%$ attack success rate (ASR) against the strongest defense. Second, we relax the first assumption by searching for a universal suffix that would work on unseen inputs, and obtain up to $89.9\%$ ASR against the strongest defense. Finally, we relax both assumptions by searching for a universal suffix that would transfer to similar black-box models and defenses, achieving an ASR of $63.9\%$ against a newly released defended model from Meta.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15738v2</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoxue Yang, Bozhidar Stevanoski, Matthieu Meeus, Yves-Alexandre de Montjoye</dc:creator>
    </item>
    <item>
      <title>Synthetic History: Evaluating Visual Representations of the Past in Diffusion Models</title>
      <link>https://arxiv.org/abs/2505.17064</link>
      <description>arXiv:2505.17064v2 Announce Type: replace-cross 
Abstract: As Text-to-Image (TTI) diffusion models become increasingly influential in content creation, growing attention is being directed toward their societal and cultural implications. While prior research has primarily examined demographic and cultural biases, the ability of these models to accurately represent historical contexts remains largely underexplored. To address this gap, we introduce a benchmark for evaluating how TTI models depict historical contexts. The benchmark combines HistVis, a dataset of 30,000 synthetic images generated by three state-of-the-art diffusion models from carefully designed prompts covering universal human activities across multiple historical periods, with a reproducible evaluation protocol. We evaluate generated imagery across three key aspects: (1) Implicit Stylistic Associations: examining default visual styles associated with specific eras; (2) Historical Consistency: identifying anachronisms such as modern artifacts in pre-modern contexts; and (3) Demographic Representation: comparing generated racial and gender distributions against historically plausible baselines. Our findings reveal systematic inaccuracies in historically themed generated imagery, as TTI models frequently stereotype past eras by incorporating unstated stylistic cues, introduce anachronisms, and fail to reflect plausible demographic patterns. By providing a reproducible benchmark for historical representation in generated imagery, this work provides an initial step toward building more historically accurate TTI models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17064v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria-Teresa De Rosa Palmini, Eva Cetinic</dc:creator>
    </item>
    <item>
      <title>InfoDet: A Dataset for Infographic Element Detection</title>
      <link>https://arxiv.org/abs/2505.17473</link>
      <description>arXiv:2505.17473v5 Announce Type: replace-cross 
Abstract: Given the central role of charts in scientific, business, and communication contexts, enhancing the chart understanding capabilities of vision-language models (VLMs) has become increasingly critical. A key limitation of existing VLMs lies in their inaccurate visual grounding of infographic elements, including charts and human-recognizable objects (HROs) such as icons and images. However, chart understanding often requires identifying relevant elements and reasoning over them. To address this limitation, we introduce InfoDet, a dataset designed to support the development of accurate object detection models for charts and HROs in infographics. It contains 11,264 real and 90,000 synthetic infographics, with over 14 million bounding box annotations. These annotations are created by combining the model-in-the-loop and programmatic methods. We demonstrate the usefulness of InfoDet through three applications: 1) constructing a Thinking-with-Boxes scheme to boost the chart understanding performance of VLMs, 2) comparing existing object detection models, and 3) applying the developed detection model to document layout and UI element detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17473v5</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiangning Zhu, Yuxing Zhou, Zheng Wang, Juntao Yao, Yima Gu, Yuhui Yuan, Shixia Liu</dc:creator>
    </item>
    <item>
      <title>KL-regularization Itself is Differentially Private in Bandits and RLHF</title>
      <link>https://arxiv.org/abs/2505.18407</link>
      <description>arXiv:2505.18407v2 Announce Type: replace-cross 
Abstract: Differential Privacy (DP) provides a rigorous framework for privacy, ensuring the outputs of data-driven algorithms remain statistically indistinguishable across datasets that differ in a single entry. While guaranteeing DP generally requires explicitly injecting noise either to the algorithm itself or to its outputs, the intrinsic randomness of existing algorithms presents an opportunity to achieve DP ``for free''. In this work, we explore the role of regularization in achieving DP across three different decision-making problems: multi-armed bandits, linear contextual bandits, and reinforcement learning from human feedback (RLHF), in offline data settings. We show that adding KL-regularization to the learning objective (a common approach in optimization algorithms) makes the action sampled from the resulting stochastic policy itself differentially private. This offers a new route to privacy guarantees without additional noise injection, while also preserving the inherent advantage of regularization in enhancing performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18407v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yizhou Zhang, Kishan Panaganti, Laixi Shi, Juba Ziani, Adam Wierman</dc:creator>
    </item>
    <item>
      <title>Thinker: Learning to Think Fast and Slow</title>
      <link>https://arxiv.org/abs/2505.21097</link>
      <description>arXiv:2505.21097v2 Announce Type: replace-cross 
Abstract: Recent studies show that the reasoning capabilities of Large Language Models (LLMs) can be improved by applying Reinforcement Learning (RL) to question-answering (QA) tasks in areas such as math and coding. With a long context length, LLMs may learn to perform search, as indicated by the self-correction behavior observed in DeepSeek R1. However, this search behavior is often imprecise and lacks confidence, resulting in long, redundant responses and highlighting deficiencies in intuition and verification. Inspired by the Dual Process Theory in psychology, we introduce a simple modification to the QA task that includes four stages: Fast Thinking, where the LLM must answer within a strict token budget; Verification, where the model evaluates its initial response; Slow Thinking, where it refines the initial response with more deliberation; and Summarization, where it distills the refinement from the previous stage into precise steps. Our proposed task improves average accuracy from 25.6% to 27.3% for Qwen2.5-1.5B, and from 45.9% to 51.0% for DeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone achieves 25.2% accuracy using fewer than 1000 tokens, demonstrating substantial inference efficiency gains. These findings suggest that intuition and deliberative reasoning are distinct, complementary systems benefiting from targeted training. Additionally, we have open-sourced both the trained models and the source code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21097v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephen Chung, Wenyu Du, Jie Fu</dc:creator>
    </item>
    <item>
      <title>Adaptive Budget Allocation for Orthogonal-Subspace Adapter Tuning in LLMs Continual Learning</title>
      <link>https://arxiv.org/abs/2505.22358</link>
      <description>arXiv:2505.22358v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) often suffer from catastrophic forgetting in continual learning (CL) scenarios, where performance on previously learned tasks degrades severely while training on sequentially arriving tasks. Although pioneering CL approaches using orthogonal subspaces can mitigate task interference, they typically employ fixed budget allocation, neglecting the varying complexity across tasks and layers. Besides, recent budget-adaptive tuning methods for LLMs often adopt multi-stage paradigms that decouple optimization and budget allocation. Such decoupling results in potential misalignment, which hinders those approaches' practical application in CL scenarios. To address these limitations, we propose OA-Adapter, a novel parameter-efficient approach for continual learning in LLMs that unifies dynamic budget adaptation with orthogonal subspace learning in an end-to-end training stage. Specifically, OA-Adapter introduces a dynamic bottleneck dimension adaptation mechanism that simultaneously allocates an efficient parameter budget and optimizes task objectives without misalignment.To effectively preserve previously acquired knowledge while coordinating with the dynamic budget allocation, orthogonal constraints are applied specifically between the parameter subspace of the current task and the dynamically allocated parameter subspaces of historical tasks. Experimental results on continual learning benchmarks demonstrate that OA-Adapter outperforms state-of-the-art methods in both accuracy and parameter efficiency. OA-Adapter achieves higher average accuracy while using 58.5% fewer parameters on the standard CL benchmark, and maintains its advantages on two larger benchmarks comprising 15 tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22358v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiyi Wan, Wanrou Du, Liang Li, Miao Pan, Xiaoqi Qin</dc:creator>
    </item>
    <item>
      <title>Attention-Aided MMSE for OFDM Channel Estimation: Learning Linear Filters with Attention</title>
      <link>https://arxiv.org/abs/2506.00452</link>
      <description>arXiv:2506.00452v2 Announce Type: replace-cross 
Abstract: In orthogonal frequency division multiplexing (OFDM), accurate channel estimation is crucial. Classical signal processing based approaches, such as minimum mean-squared error (MMSE) estimation, often require second-order statistics that are difficult to obtain in practice. Recent deep neural networks based methods have been introduced to address this; yet they often suffer from high inference complexity. This paper proposes an Attention-aided MMSE (A-MMSE), a novel model-based DNN framework that learns the optimal MMSE filter via the Attention Transformer. Once trained, the A-MMSE estimates the channel through a single linear operation for channel estimation, eliminating nonlinear activations during inference and thus reducing computational complexity. To enhance the learning efficiency of the A-MMSE, we develop a two-stage Attention encoder, designed to effectively capture the channel correlation structure. Additionally, a rank-adaptive extension of the proposed A-MMSE allows flexible trade-offs between complexity and channel estimation accuracy. Extensive simulations with 3GPP TDL channel models demonstrate that the proposed A-MMSE consistently outperforms other baseline methods in terms of normalized MSE across a wide range of signal-to-noise ratio (SNR) conditions. In particular, the A-MMSE and its rank-adaptive extension establish a new frontier in the performance-complexity trade-off, providing a powerful yet highly efficient solution for practical channel estimation</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00452v2</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>stat.ML</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>TaeJun Ha, Chaehyun Jung, Hyeonuk Kim, Jeongwoo Park, Jeonghun Park</dc:creator>
    </item>
    <item>
      <title>iQUEST: An Iterative Question-Guided Framework for Knowledge Base Question Answering</title>
      <link>https://arxiv.org/abs/2506.01784</link>
      <description>arXiv:2506.01784v3 Announce Type: replace-cross 
Abstract: While Large Language Models (LLMs) excel at many natural language processing tasks, they often suffer from factual inaccuracies in knowledge-intensive scenarios. Integrating external knowledge resources, particularly knowledge graphs (KGs), provides a transparent and updatable foundation for more reliable reasoning. Knowledge Base Question Answering (KBQA), which queries and reasons over KGs, is central to this effort, especially for complex, multi-hop queries. However, multi-hop reasoning poses two key challenges: (1)~maintaining coherent reasoning paths, and (2)~avoiding prematurely discarding critical multi-hop connections. To address these issues, we introduce iQUEST, a question-guided KBQA framework that iteratively decomposes complex queries into simpler sub-questions, ensuring a structured and focused reasoning trajectory. Additionally, we integrate a Graph Neural Network (GNN) to look ahead and incorporate 2-hop neighbor information at each reasoning step. This dual approach strengthens the reasoning process, enabling the model to explore viable paths more effectively. Detailed experiments demonstrate the consistent improvement delivered by iQUEST across four benchmark datasets and four LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01784v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuai Wang, Yinan Yu</dc:creator>
    </item>
    <item>
      <title>When Style Breaks Safety: Defending LLMs Against Superficial Style Alignment</title>
      <link>https://arxiv.org/abs/2506.07452</link>
      <description>arXiv:2506.07452v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) can be prompted with specific styles (e.g., formatting responses as lists), including in malicious queries. Prior jailbreak research mainly augments these queries with additional string transformations to maximize attack success rate (ASR). However, the impact of style patterns in the original queries that are semantically irrelevant to the malicious intent remains unclear. In this work, we seek to understand whether style patterns compromise LLM safety, how superficial style alignment increases model vulnerability, and how best to mitigate these risks during alignment. We first define ASR inflation as the increase in ASR due to style patterns in existing jailbreak benchmark queries. By evaluating 32 LLMs across seven benchmarks, we find that nearly all models exhibit ASR inflation. Notably, the inflation correlates with an LLM's relative attention to style patterns, which also overlap more with its instruction-tuning data when inflation occurs. We then investigate superficial style alignment, and find that fine-tuning with specific styles makes LLMs more vulnerable to jailbreaks of those same styles. Finally, we propose SafeStyle, a defense strategy that incorporates a small amount of safety training data augmented to match the distribution of style patterns in the fine-tuning data. Across three LLMs, six fine-tuning style settings, and two real-world instruction-tuning datasets, SafeStyle consistently outperforms baselines in maintaining LLM safety.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07452v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuxin Xiao, Sana Tonekaboni, Walter Gerych, Vinith Suriyakumar, Marzyeh Ghassemi</dc:creator>
    </item>
    <item>
      <title>KScope: A Framework for Characterizing the Knowledge Status of Language Models</title>
      <link>https://arxiv.org/abs/2506.07458</link>
      <description>arXiv:2506.07458v2 Announce Type: replace-cross 
Abstract: Characterizing a large language model's (LLM's) knowledge of a given question is challenging. As a result, prior work has primarily examined LLM behavior under knowledge conflicts, where the model's internal parametric memory contradicts information in the external context. However, this does not fully reflect how well the model knows the answer to the question. In this paper, we first introduce a taxonomy of five knowledge statuses based on the consistency and correctness of LLM knowledge modes. We then propose KScope, a hierarchical framework of statistical tests that progressively refines hypotheses about knowledge modes and characterizes LLM knowledge into one of these five statuses. We apply KScope to nine LLMs across four datasets and systematically establish: (1) Supporting context narrows knowledge gaps across models. (2) Context features related to difficulty, relevance, and familiarity drive successful knowledge updates. (3) LLMs exhibit similar feature preferences when partially correct or conflicted, but diverge sharply when consistently wrong. (4) Context summarization constrained by our feature analysis, together with enhanced credibility, further improves update effectiveness and generalizes across LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07458v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuxin Xiao, Shan Chen, Jack Gallifant, Danielle Bitterman, Thomas Hartvigsen, Marzyeh Ghassemi</dc:creator>
    </item>
    <item>
      <title>TAI3: Testing Agent Integrity in Interpreting User Intent</title>
      <link>https://arxiv.org/abs/2506.07524</link>
      <description>arXiv:2506.07524v2 Announce Type: replace-cross 
Abstract: LLM agents are increasingly deployed to automate real-world tasks by invoking APIs through natural language instructions. While powerful, they often suffer from misinterpretation of user intent, leading to the agent's actions that diverge from the user's intended goal, especially as external toolkits evolve. Traditional software testing assumes structured inputs and thus falls short in handling the ambiguity of natural language. We introduce TAI3, an API-centric stress testing framework that systematically uncovers intent integrity violations in LLM agents. Unlike prior work focused on fixed benchmarks or adversarial inputs, TAI3 generates realistic tasks based on toolkits' documentation and applies targeted mutations to expose subtle agent errors while preserving user intent. To guide testing, we propose semantic partitioning, which organizes natural language tasks into meaningful categories based on toolkit API parameters and their equivalence classes. Within each partition, seed tasks are mutated and ranked by a lightweight predictor that estimates the likelihood of triggering agent errors. To enhance efficiency, TAI3 maintains a datatype-aware strategy memory that retrieves and adapts effective mutation patterns from past cases. Experiments on 80 toolkit APIs demonstrate that TAI3 effectively uncovers intent integrity violations, significantly outperforming baselines in both error-exposing rate and query efficiency. Moreover, TAI3 generalizes well to stronger target models using smaller LLMs for test generation, and adapts to evolving APIs across domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07524v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shiwei Feng, Xiangzhe Xu, Xuan Chen, Kaiyuan Zhang, Syed Yusuf Ahmed, Zian Su, Mingwei Zheng, Xiangyu Zhang</dc:creator>
    </item>
    <item>
      <title>SoK: Evaluating Jailbreak Guardrails for Large Language Models</title>
      <link>https://arxiv.org/abs/2506.10597</link>
      <description>arXiv:2506.10597v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have achieved remarkable progress, but their deployment has exposed critical vulnerabilities, particularly to jailbreak attacks that circumvent safety alignments. Guardrails--external defense mechanisms that monitor and control LLM interactions--have emerged as a promising solution. However, the current landscape of LLM guardrails is fragmented, lacking a unified taxonomy and comprehensive evaluation framework. In this Systematization of Knowledge (SoK) paper, we present the first holistic analysis of jailbreak guardrails for LLMs. We propose a novel, multi-dimensional taxonomy that categorizes guardrails along six key dimensions, and introduce a Security-Efficiency-Utility evaluation framework to assess their practical effectiveness. Through extensive analysis and experiments, we identify the strengths and limitations of existing guardrail approaches, provide insights into optimizing their defense mechanisms, and explore their universality across attack types. Our work offers a structured foundation for future research and development, aiming to guide the principled advancement and deployment of robust LLM guardrails. The code is available at https://github.com/xunguangwang/SoK4JailbreakGuardrails.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10597v2</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xunguang Wang, Zhenlan Ji, Wenxuan Wang, Zongjie Li, Daoyuan Wu, Shuai Wang</dc:creator>
    </item>
    <item>
      <title>Subspace-Boosted Model Merging</title>
      <link>https://arxiv.org/abs/2506.16506</link>
      <description>arXiv:2506.16506v2 Announce Type: replace-cross 
Abstract: Model merging enables the combination of multiple specialized expert models into a single model capable of performing multiple tasks. However, the benefits of merging an increasing amount of specialized experts generally lead to diminishing returns and reduced overall performance gains. In this work, we offer an explanation and analysis from a task arithmetic perspective; revealing that as the merging process (across numerous existing merging methods) continues for more and more experts, the associated task vector space experiences rank collapse. To mitigate this issue, we introduce Subspace Boosting, which operates on the singular value decomposed task vector space and maintains task vector ranks. Subspace Boosting raises merging efficacy for up to 20 expert models by large margins of more than 10% when evaluated on both vision and language benchmarks. Moreover, we propose employing Higher-Order Generalized Singular Value Decomposition to quantify task similarity, offering a new interpretable perspective on model merging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16506v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ronald Skorobogat, Karsten Roth, Mariana-Iuliana Georgescu</dc:creator>
    </item>
    <item>
      <title>LLM-guided Chemical Process Optimization with a Multi-Agent Approach</title>
      <link>https://arxiv.org/abs/2506.20921</link>
      <description>arXiv:2506.20921v2 Announce Type: replace-cross 
Abstract: Chemical process optimization maximizes production efficiency and economic performance, but optimization algorithms, including gradient-based solvers, numerical methods, and parameter grid searches, become impractical when operating constraints are ill-defined or unavailable. We present a multi-agent LLM framework that autonomously infers operating constraints from minimal process descriptions, then collaboratively guides optimization. Our AutoGen-based framework employs OpenAI's o3 model with specialized agents for constraint generation, parameter validation, simulation, and optimization guidance. Through autonomous constraint generation and iterative multi-agent optimization, the framework eliminates the need for predefined operational bounds. Validated on hydrodealkylation across cost, yield, and yield-to-cost ratio metrics, the framework achieved competitive performance with conventional methods while reducing wall-time 31-fold relative to grid search, converging in under 20 minutes. The reasoning-guided search demonstrates sophisticated process understanding, correctly identifying utility trade-offs and applying domain-informed heuristics. Unlike conventional methods requiring predefined constraints, our approach uniquely combines autonomous constraint generation with interpretable parameter exploration. Model comparison reveals reasoning-capable architectures (o3, o1) are essential for successful optimization, while standard models fail to converge. This approach is particularly valuable for emerging processes and retrofit applications where operational constraints are poorly characterized or unavailable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20921v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tong Zeng, Srivathsan Badrinarayanan, Janghoon Ock, Cheng-Kai Lai, Amir Barati Farimani</dc:creator>
    </item>
    <item>
      <title>R1-Ranker: Teaching LLM Rankers to Reason</title>
      <link>https://arxiv.org/abs/2506.21638</link>
      <description>arXiv:2506.21638v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have recently shown strong reasoning abilities in domains like mathematics, coding, and scientific problem-solving, yet their potential for ranking tasks, where prime examples include retrieval, recommender systems, and LLM routing, remains underexplored. Ranking requires complex reasoning across heterogeneous candidates, but existing LLM-based rankers are often domain-specific, tied to fixed backbones, and lack iterative refinement, limiting their ability to fully exploit LLMs' reasoning potential. To address these challenges, we propose R1-Ranker, a reasoning-incentive framework built on reinforcement learning, with two complementary designs: DRanker, which generates full rankings in one shot, and IRanker, which decomposes ranking into an iterative elimination process with step-wise rewards to encourage deeper reasoning. We evaluate unified R1-Rankers on nine datasets spanning recommendation, routing, and passage ranking, showing that IRanker-3B consistently achieves state-of-the-art performance, surpasses larger 7B models on some tasks, and yields a 15.7% average relative improvement. Ablation and generalization experiments further confirm the critical role of reinforcement learning and iterative reasoning, with IRanker-3B improving zero-shot performance by over 9% on out-of-domain tasks and reasoning traces boosting other LLMs by up to 22.87%. These results demonstrate that unifying diverse ranking tasks with a single reasoning-driven foundation model is both effective and essential for advancing LLM reasoning in ranking scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21638v3</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tao Feng, Zhigang Hua, Zijie Lei, Yan Xie, Shuang Yang, Bo Long, Jiaxuan You</dc:creator>
    </item>
    <item>
      <title>VALID-Mol: a Systematic Framework for Validated LLM-Assisted Molecular Design</title>
      <link>https://arxiv.org/abs/2506.23339</link>
      <description>arXiv:2506.23339v2 Announce Type: replace-cross 
Abstract: Large Language Models demonstrate substantial promise for advancing scientific discovery, yet their deployment in disciplines demanding factual precision and specialized domain constraints presents significant challenges. Within molecular design for pharmaceutical development, these models can propose innovative molecular modifications but frequently generate chemically infeasible structures. We introduce VALID-Mol, a comprehensive framework that integrates chemical validation with LLM-driven molecular design, achieving an improvement in valid chemical structure generation from 3% to 83%. Our methodology synthesizes systematic prompt optimization, automated chemical verification, and domain-adapted fine-tuning to ensure dependable generation of synthesizable molecules with enhanced properties. Our contribution extends beyond implementation details to provide a transferable methodology for scientifically-constrained LLM applications with measurable reliability enhancements. Computational analyses indicate our framework generates promising synthesis candidates with up to 17-fold predicted improvements in target binding affinity while preserving synthetic feasibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23339v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>physics.chem-ph</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator> Malikussaid, Hilal Hudan Nuha, Isman Kurniawan</dc:creator>
    </item>
    <item>
      <title>A Clinically-Grounded Two-Stage Framework for Renal CT Report Generation</title>
      <link>https://arxiv.org/abs/2506.23584</link>
      <description>arXiv:2506.23584v2 Announce Type: replace-cross 
Abstract: Objective Renal cancer is a common malignancy and a major cause of cancer-related deaths. Computed tomography (CT) is central to early detection, staging, and treatment planning. However, the growing CT workload increases radiologists' burden and risks incomplete documentation. Automatically generating accurate reports remains challenging because it requires integrating visual interpretation with clinical reasoning. Advances in artificial intelligence (AI), especially large language and vision-language models, offer potential to reduce workload and enhance diagnostic quality.
  Methods We propose a clinically informed, two-stage framework for automatic renal CT report generation. In Stage 1, a multi-task learning model detects structured clinical features from each 2D image. In Stage 2, a vision-language model generates free-text reports conditioned on the image and the detected features. To evaluate clinical fidelity, generated clinical features are extracted from the reports and compared with expert-annotated ground truth.
  Results Experiments on an expert-labeled dataset show that incorporating detected features improves both report quality and clinical accuracy. The model achieved an average AUC of 0.75 for key imaging features and a METEOR score of 0.33, demonstrating higher clinical consistency and fewer template-driven errors.
  Conclusion Linking structured feature detection with conditioned report generation provides a clinically grounded approach to integrate structured prediction and narrative drafting for renal CT reporting. This method enhances interpretability and clinical faithfulness, underscoring the value of domain-relevant evaluation metrics for medical AI development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23584v2</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Renjie Liang, Zhengkang Fan, Jinqian Pan, Chenkun Sun, Bruce Daniel Steinberg, Russell Terry, Jie Xu</dc:creator>
    </item>
    <item>
      <title>TopoStreamer: Temporal Lane Segment Topology Reasoning in Autonomous Driving</title>
      <link>https://arxiv.org/abs/2507.00709</link>
      <description>arXiv:2507.00709v3 Announce Type: replace-cross 
Abstract: Lane segment topology reasoning constructs a comprehensive road network by capturing the topological relationships between lane segments and their semantic types. This enables end-to-end autonomous driving systems to perform road-dependent maneuvers such as turning and lane changing. However, the limitations in consistent positional embedding and temporal multiple attribute learning in existing methods hinder accurate roadnet reconstruction. To address these issues, we propose TopoStreamer, an end-to-end temporal perception model for lane segment topology reasoning. Specifically, TopoStreamer introduces three key improvements: streaming attribute constraints, dynamic lane boundary positional encoding, and lane segment denoising. The streaming attribute constraints enforce temporal consistency in both centerline and boundary coordinates, along with their classifications. Meanwhile, dynamic lane boundary positional encoding enhances the learning of up-to-date positional information within queries, while lane segment denoising helps capture diverse lane segment patterns, ultimately improving model performance. Additionally, we assess the accuracy of existing models using a lane boundary classification metric, which serves as a crucial measure for lane-changing scenarios in autonomous driving. On the OpenLane-V2 dataset, TopoStreamer demonstrates significant improvements over state-of-the-art methods, achieving substantial performance gains of +3.0% mAP in lane segment perception and +1.7% OLS in centerline perception tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00709v3</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiming Yang, Yueru Luo, Bingkun He, Hongbin Lin, Suzhong Fu, Chao Zheng, Zhipeng Cao, Erlong Li, Chao Yan, Shuguang Cui, Zhen Li</dc:creator>
    </item>
    <item>
      <title>Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities</title>
      <link>https://arxiv.org/abs/2507.06261</link>
      <description>arXiv:2507.06261v5 Announce Type: replace-cross 
Abstract: In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA performance on frontier coding and reasoning benchmarks. In addition to its incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that excels at multimodal understanding and it is now able to process up to 3 hours of video content. Its unique combination of long context, multimodal and reasoning capabilities can be combined to unlock new agentic workflows. Gemini 2.5 Flash provides excellent reasoning abilities at a fraction of the compute and latency requirements and Gemini 2.0 Flash and Flash-Lite provide high performance at low latency and cost. Taken together, the Gemini 2.X model generation spans the full Pareto frontier of model capability vs cost, allowing users to explore the boundaries of what is possible with complex agentic problem solving.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06261v5</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gheorghe Comanici (Chun-Ting), Eric Bieber (Chun-Ting), Mike Schaekermann (Chun-Ting), Ice Pasupat (Chun-Ting), Noveen Sachdeva (Chun-Ting), Inderjit Dhillon (Chun-Ting), Marcel Blistein (Chun-Ting), Ori Ram (Chun-Ting), Dan Zhang (Chun-Ting), Evan Rosen (Chun-Ting), Luke Marris (Chun-Ting), Sam Petulla (Chun-Ting), Colin Gaffney (Chun-Ting), Asaf Aharoni (Chun-Ting), Nathan Lintz (Chun-Ting), Tiago Cardal Pais (Chun-Ting), Henrik Jacobsson (Chun-Ting), Idan Szpektor (Chun-Ting), Nan-Jiang Jiang (Chun-Ting), Krishna Haridasan (Chun-Ting), Ahmed Omran (Chun-Ting), Nikunj Saunshi (Chun-Ting), Dara Bahri (Chun-Ting), Gaurav Mishra (Chun-Ting), Eric Chu (Chun-Ting), Toby Boyd (Chun-Ting), Brad Hekman (Chun-Ting), Aaron Parisi (Chun-Ting), Chaoyi Zhang (Chun-Ting), Kornraphop Kawintiranon (Chun-Ting), Tania Bedrax-Weiss (Chun-Ting), Oliver Wang (Chun-Ting), Ya Xu (Chun-Ting), Ollie Purkiss (Chun-Ting), Uri Mendlovic (Chun-Ting), Ila\"i Deutel (Chun-Ting), Nam Nguyen (Chun-Ting), Adam Langley (Chun-Ting), Flip Korn (Chun-Ting), Lucia Rossazza (Chun-Ting), Alexandre Ram\'e (Chun-Ting), Sagar Waghmare (Chun-Ting), Helen Miller (Chun-Ting), Nathan Byrd (Chun-Ting), Ashrith Sheshan (Chun-Ting), Raia Hadsell (Chun-Ting), Sangnie Bhardwaj (Chun-Ting), Pawel Janus (Chun-Ting), Tero Rissa (Chun-Ting), Dan Horgan (Chun-Ting), Alvin Abdagic (Chun-Ting), Lior Belenki (Chun-Ting), James Allingham (Chun-Ting), Anima Singh (Chun-Ting), Theo Guidroz (Chun-Ting), Srivatsan Srinivasan (Chun-Ting), Herman Schmit (Chun-Ting), Kristen Chiafullo (Chun-Ting), Andre Elisseeff (Chun-Ting), Nilpa Jha (Chun-Ting), Prateek Kolhar (Chun-Ting), Leonard Berrada (Chun-Ting), Frank Ding (Chun-Ting), Xiance Si (Chun-Ting), Shrestha Basu Mallick (Chun-Ting), Franz Och (Chun-Ting), Sofia Erell (Chun-Ting), Eric Ni (Chun-Ting), Tejasi Latkar (Chun-Ting), Sherry Yang (Chun-Ting), Petar Sirkovic (Chun-Ting), Ziqiang Feng (Chun-Ting), Robert Leland (Chun-Ting), Rachel Hornung (Chun-Ting), Gang Wu (Chun-Ting), Charles Blundell (Chun-Ting), Hamidreza Alvari (Chun-Ting), Po-Sen Huang (Chun-Ting), Cathy Yip (Chun-Ting), Sanja Deur (Chun-Ting), Li Liu (Chun-Ting), Gabriela Surita (Chun-Ting), Pablo Duque (Chun-Ting), Dima Damen (Chun-Ting), Johnson Jia (Chun-Ting), Arthur Guez (Chun-Ting), Markus Mircea (Chun-Ting), Animesh Sinha (Chun-Ting), Alberto Magni (Chun-Ting), Pawe{\l} Stradomski (Chun-Ting), Tal Marian (Chun-Ting), Vlado Gali\'c (Chun-Ting), Wenhu Chen (Chun-Ting), Hisham Husain (Chun-Ting), Achintya Singhal (Chun-Ting), Dominik Grewe (Chun-Ting), Fran\c{c}ois-Xavier Aubet (Chun-Ting), Shuang Song (Chun-Ting), Lorenzo Blanco (Chun-Ting), Leland Rechis (Chun-Ting), Lewis Ho (Chun-Ting), Rich Munoz (Chun-Ting), Kelvin Zheng (Chun-Ting), Jessica Hamrick (Chun-Ting), Kevin Mather (Chun-Ting), Hagai Taitelbaum (Chun-Ting), Eliza Rutherford (Chun-Ting), Yun Lei (Chun-Ting), Kuangyuan Chen (Chun-Ting), Anand Shukla (Chun-Ting), Erica Moreira (Chun-Ting), Eric Doi (Chun-Ting), Berivan Isik (Chun-Ting), Nir Shabat (Chun-Ting), Dominika Rogozi\'nska (Chun-Ting), Kashyap Kolipaka (Chun-Ting), Jason Chang (Chun-Ting), Eugen Vu\v{s}ak (Chun-Ting), Srinivasan Venkatachary (Chun-Ting), Shadi Noghabi (Chun-Ting), Tarun Bharti (Chun-Ting), Younghoon Jun (Chun-Ting), Aleksandr Zaks (Chun-Ting), Simon Green (Chun-Ting), Jeshwanth Challagundla (Chun-Ting), William Wong (Chun-Ting), Muqthar Mohammad (Chun-Ting), Dean Hirsch (Chun-Ting), Yong Cheng (Chun-Ting), Iftekhar Naim (Chun-Ting), Lev Proleev (Chun-Ting), Damien Vincent (Chun-Ting), Aayush Singh (Chun-Ting), Maxim Krikun (Chun-Ting), Dilip Krishnan (Chun-Ting), Zoubin Ghahramani (Chun-Ting), Aviel Atias (Chun-Ting), Rajeev Aggarwal (Chun-Ting), Christo Kirov (Chun-Ting), Dimitrios Vytiniotis (Chun-Ting), Christy Koh (Chun-Ting), Alexandra Chronopoulou (Chun-Ting), Pawan Dogra (Chun-Ting), Vlad-Doru Ion (Chun-Ting), Gladys Tyen (Chun-Ting), Jason Lee (Chun-Ting), Felix Weissenberger (Chun-Ting), Trevor Strohman (Chun-Ting), Ashwin Balakrishna (Chun-Ting), Jack Rae (Chun-Ting), Marko Velic (Chun-Ting), Raoul de Liedekerke (Chun-Ting), Oded Elyada (Chun-Ting), Wentao Yuan (Chun-Ting), Canoee Liu (Chun-Ting), Lior Shani (Chun-Ting), Sergey Kishchenko (Chun-Ting), Bea Alessio (Chun-Ting), Yandong Li (Chun-Ting), Richard Song (Chun-Ting), Sam Kwei (Chun-Ting), Orion Jankowski (Chun-Ting), Aneesh Pappu (Chun-Ting), Youhei Namiki (Chun-Ting), Yenai Ma (Chun-Ting), Nilesh Tripuraneni (Chun-Ting), Colin Cherry (Chun-Ting), Marissa Ikonomidis (Chun-Ting), Yu-Cheng Ling (Chun-Ting), Colin Ji (Chun-Ting), Beka Westberg (Chun-Ting), Auriel Wright (Chun-Ting), Da Yu (Chun-Ting), David Parkinson (Chun-Ting), Swaroop Ramaswamy (Chun-Ting), Jerome Connor (Chun-Ting), Soheil Hassas Yeganeh (Chun-Ting), Snchit Grover (Chun-Ting), George Kenwright (Chun-Ting), Lubo Litchev (Chun-Ting), Chris Apps (Chun-Ting), Alex Tomala (Chun-Ting), Felix Halim (Chun-Ting), Alex Castro-Ros (Chun-Ting), Zefei Li (Chun-Ting), Anudhyan Boral (Chun-Ting), Pauline Sho (Chun-Ting), Michal Yarom (Chun-Ting), Eric Malmi (Chun-Ting), David Klinghoffer (Chun-Ting), Rebecca Lin (Chun-Ting), Alan Ansell (Chun-Ting), Pradeep Kumar S (Chun-Ting), Shubin Zhao (Chun-Ting), Siqi Zuo (Chun-Ting), Adam Santoro (Chun-Ting), Heng-Tze Cheng (Chun-Ting), Solomon Demmessie (Chun-Ting), Yuchi Liu (Chun-Ting), Nicole Brichtova (Chun-Ting), Allie Culp (Chun-Ting), Nathaniel Braun (Chun-Ting), Dan Graur (Chun-Ting), Will Ng (Chun-Ting), Nikhil Mehta (Chun-Ting), Aaron Phillips (Chun-Ting), Patrik Sundberg (Chun-Ting), Varun Godbole (Chun-Ting), Fangyu Liu (Chun-Ting), Yash Katariya (Chun-Ting), David Rim (Chun-Ting), Mojtaba Seyedhosseini (Chun-Ting), Sean Ammirati (Chun-Ting), Jonas Valfridsson (Chun-Ting), Mahan Malihi (Chun-Ting), Timothy Knight (Chun-Ting), Andeep Toor (Chun-Ting), Thomas Lampe (Chun-Ting), Abe Ittycheriah (Chun-Ting), Lewis Chiang (Chun-Ting), Chak Yeung (Chun-Ting), Alexandre Fr\'echette (Chun-Ting), Jinmeng Rao (Chun-Ting), Huisheng Wang (Chun-Ting), Himanshu Srivastava (Chun-Ting), Richard Zhang (Chun-Ting), Rocky Rhodes (Chun-Ting), Ariel Brand (Chun-Ting), Dean Weesner (Chun-Ting), Ilya Figotin (Chun-Ting), Felix Gimeno (Chun-Ting), Rachana Fellinger (Chun-Ting), Pierre Marcenac (Chun-Ting), Jos\'e Leal (Chun-Ting), Eyal Marcus (Chun-Ting), Victor Cotruta (Chun-Ting), Rodrigo Cabrera (Chun-Ting), Sheryl Luo (Chun-Ting), Dan Garrette (Chun-Ting), Vera Axelrod (Chun-Ting), Sorin Baltateanu (Chun-Ting), David Barker (Chun-Ting), Dongkai Chen (Chun-Ting), Horia Toma (Chun-Ting), Ben Ingram (Chun-Ting), Jason Riesa (Chun-Ting), Chinmay Kulkarni (Chun-Ting), Yujing Zhang (Chun-Ting), Hongbin Liu (Chun-Ting), Chao Wang (Chun-Ting), Martin Polacek (Chun-Ting), Will Wu (Chun-Ting), Kai Hui (Chun-Ting), Adrian N Reyes (Chun-Ting), Yi Su (Chun-Ting), Megan Barnes (Chun-Ting), Ishaan Malhi (Chun-Ting), Anfal Siddiqui (Chun-Ting), Qixuan Feng (Chun-Ting), Mihai Damaschin (Chun-Ting), Daniele Pighin (Chun-Ting), Andreas Steiner (Chun-Ting), Samuel Yang (Chun-Ting), Ramya Sree Boppana (Chun-Ting), Simeon Ivanov (Chun-Ting), Arun Kandoor (Chun-Ting), Aditya Shah (Chun-Ting), Asier Mujika (Chun-Ting), Da Huang (Chun-Ting), Christopher A. Choquette-Choo (Chun-Ting), Mohak Patel (Chun-Ting), Tianhe Yu (Chun-Ting), Toni Creswell (Chun-Ting),  Jerry (Chun-Ting),  Liu (Cindy), Catarina Barros (Cindy), Yasaman Razeghi (Cindy), Aurko Roy (Cindy), Phil Culliton (Cindy), Binbin Xiong (Cindy), Jiaqi Pan (Cindy), Thomas Strohmann (Cindy), Tolly Powell (Cindy), Babi Seal (Cindy), Doug DeCarlo (Cindy), Pranav Shyam (Cindy), Kaan Katircioglu (Cindy), Xuezhi Wang (Cindy), Cassidy Hardin (Cindy), Immanuel Odisho (Cindy), Josef Broder (Cindy), Oscar Chang (Cindy), Arun Nair (Cindy), Artem Shtefan (Cindy), Maura O'Brien (Cindy), Manu Agarwal (Cindy), Sahitya Potluri (Cindy), Siddharth Goyal (Cindy), Amit Jhindal (Cindy), Saksham Thakur (Cindy), Yury Stuken (Cindy), James Lyon (Cindy), Kristina Toutanova (Cindy), Fangxiaoyu Feng (Cindy), Austin Wu (Cindy), Ben Horn (Cindy), Alek Wang (Cindy), Alex Cullum (Cindy), Gabe Taubman (Cindy), Disha Shrivastava (Cindy), Chongyang Shi (Cindy), Hamish Tomlinson (Cindy), Roma Patel (Cindy), Tao Tu (Cindy), Ada Maksutaj Oflazer (Cindy), Francesco Pongetti (Cindy), Mingyao Yang (Cindy), Adrien Ali Ta\"iga (Cindy), Vincent Perot (Cindy), Nuo Wang Pierse (Cindy), Feng Han (Cindy), Yoel Drori (Cindy), I\~naki Iturrate (Cindy), Ayan Chakrabarti (Cindy), Legg Yeung (Cindy), Dave Dopson (Cindy), Yi-ting Chen (Cindy), Apoorv Kulshreshtha (Cindy), Tongfei Guo (Cindy), Philip Pham (Cindy), Tal Schuster (Cindy), Junquan Chen (Cindy), Alex Polozov (Cindy), Jinwei Xing (Cindy), Huanjie Zhou (Cindy), Praneeth Kacham (Cindy), Doron Kukliansky (Cindy), Antoine Miech (Cindy), Sergey Yaroshenko (Cindy), Ed Chi (Cindy), Sholto Douglas (Cindy), Hongliang Fei (Cindy), Mathieu Blondel (Cindy), Preethi Myla (Cindy), Lior Madmoni (Cindy), Xing Wu (Cindy), Daniel Keysers (Cindy), Kristian Kjems (Cindy), Isabela Albuquerque (Cindy), Lijun Yu (Cindy), Joel D'sa (Cindy), Michelle Plantan (Cindy), Vlad Ionescu (Cindy), Jaume Sanchez Elias (Cindy), Abhirut Gupta (Cindy), Manish Reddy Vuyyuru (Cindy), Fred Alcober (Cindy), Tong Zhou (Cindy), Kaiyang Ji (Cindy), Florian Hartmann (Cindy), Subha Puttagunta (Cindy), Hugo Song (Cindy), Ehsan Amid (Cindy), Anca Stefanoiu (Cindy), Andrew Lee (Cindy), Paul Pucciarelli (Cindy), Emma Wang (Cindy), Amit Raul (Cindy), Slav Petrov (Cindy), Isaac Tian (Cindy), Valentin Anklin (Cindy), Nana Nti (Cindy), Victor Gomes (Cindy), Max Schumacher (Cindy), Grace Vesom (Cindy), Alex Panagopoulos (Cindy), Konstantinos Bousmalis (Cindy), Daniel Andor (Cindy), Josh Jacob (Cindy), Yuan Zhang (Cindy), Bill Rosgen (Cindy), Matija Kecman (Cindy), Matthew Tung (Cindy), Alexandra Belias (Cindy), Noah Goodman (Cindy), Paul Covington (Cindy), Brian Wieder (Cindy), Nikita Saxena (Cindy), Elnaz Davoodi (Cindy), Muhuan Huang (Cindy), Sharath Maddineni (Cindy), Vincent Roulet (Cindy), Folawiyo Campbell-Ajala (Cindy), Pier Giuseppe Sessa (Cindy),  Xintian (Cindy),  Wu (Q), Guangda Lai (Q), Paul Collins (Q), Alex Haig (Q), Vytenis Sakenas (Q), Xiaowei Xu (Q), Marissa Giustina (Q), Laurent El Shafey (Q), Pichi Charoenpanit (Q), Shefali Garg (Q), Joshua Ainslie (Q), Boone Severson (Q), Montse Gonzalez Arenas (Q), Shreya Pathak (Q), Sujee Rajayogam (Q), Jie Feng (Q), Michiel Bakker (Q), Sheng Li (Q), Nevan Wichers (Q), Jamie Rogers (Q), Xinyang Geng (Q), Yeqing Li (Q), Rolf Jagerman (Q), Chao Jia (Q), Nadav Olmert (Q), David Sharon (Q), Matthew Mauger (Q), Sandeep Mariserla (Q), Hongxu Ma (Q), Megha Mohabey (Q), Kyuyeun Kim (Q), Alek Andreev (Q), Scott Pollom (Q), Juliette Love (Q), Vihan Jain (Q), Priyanka Agrawal (Q), Yannick Schroecker (Q), Alisa Fortin (Q), Manfred Warmuth (Q), Ji Liu (Q), Andrew Leach (Q), Irina Blok (Q), Ganesh Poomal Girirajan (Q), Roee Aharoni (Q), Benigno Uria (Q), Andrei Sozanschi (Q), Dan Goldberg (Q), Lucian Ionita (Q), Marco Tulio Ribeiro (Q), Martin Zlocha (Q), Vighnesh Birodkar (Q), Sami Lachgar (Q), Liangzhe Yuan (Q), Himadri Choudhury (Q), Matt Ginsberg (Q), Fei Zheng (Q), Gregory Dibb (Q), Emily Graves (Q), Swachhand Lokhande (Q), Gabriel Rasskin (Q), George-Cristian Muraru (Q), Corbin Quick (Q), Sandeep Tata (Q), Pierre Sermanet (Q), Aditya Chawla (Q), Itay Karo (Q), Yan Wang (Q), Susan Zhang (Q), Orgad Keller (Q), Anca Dragan (Q), Guolong Su (Q), Ian Chou (Q), Xi Liu (Q), Yiqing Tao (Q), Shruthi Prabhakara (Q), Marc Wilson (Q), Ruibo Liu (Q), Shibo Wang (Q), Georgie Evans (Q), David Du (Q), Alfonso Casta\~no (Q), Gautam Prasad (Q), Mona El Mahdy (Q), Sebastian Gerlach (Q), Machel Reid (Q), Jarrod Kahn (Q), Amir Zait (Q), Thanumalayan Sankaranarayana Pillai (Q), Thatcher Ulrich (Q), Guanyu Wang (Q), Jan Wassenberg (Q), Efrat Farkash (Q), Kiran Yalasangi (Q), Congchao Wang (Q), Maria Bauza (Q), Simon Bucher (Q), Ting Liu (Q), Jun Yan (Q), Gary Leung (Q), Vikas Sindhwani (Q), Parker Barnes (Q), Avi Singh (Q), Ivan Jurin (Q), Jichuan Chang (Q), Niket Kumar Bhumihar (Q), Sivan Eiger (Q), Gui Citovsky (Q), Ben Withbroe (Q), Zhang Li (Q), Siyang Xue (Q), Niccol\`o Dal Santo (Q), Georgi Stoyanov (Q), Yves Raimond (Q), Steven Zheng (Q), Yilin Gao (Q), V\'it List\'ik (Q), S{\l}awek Kwasiborski (Q), Rachel Saputro (Q), Adnan Ozturel (Q), Ganesh Mallya (Q), Kushal Majmundar (Q), Ross West (Q), Paul Caron (Q), Jinliang Wei (Q), Lluis Castrejon (Q), Sharad Vikram (Q), Deepak Ramachandran (Q), Nikhil Dhawan (Q), Jiho Park (Q), Sara Smoot (Q), George van den Driessche (Q), Yochai Blau (Q), Chase Malik (Q), Wei Liang (Q), Roy Hirsch (Q), Cicero Nogueira dos Santos (Q), Eugene Weinstein (Q), A\"aron van den Oord (Q), Sid Lall (Q), Nicholas FitzGerald (Q), Zixuan Jiang (Q), Xuan Yang (Q), Dale Webster (Q), Ali Elqursh (Q), Aedan Pope (Q), Georges Rotival (Q), David Raposo (Q), Wanzheng Zhu (Q), Jeff Dean (Q), Sami Alabed (Q), Dustin Tran (Q), Arushi Gupta (Q), Zach Gleicher (Q), Jessica Austin (Q), Edouard Rosseel (Q), Megh Umekar (Q), Dipanjan Das (Q), Yinghao Sun (Q), Kai Chen (Q), Karolis Misiunas (Q), Xiang Zhou (Q), Yixian Di (Q), Alyssa Loo (Q), Josh Newlan (Q), Bo Li (Q), Vinay Ramasesh (Q), Ying Xu (Q), Alex Chen (Q), Sudeep Gandhe (Q), Radu Soricut (Q), Nikita Gupta (Q), Shuguang Hu (Q), Seliem El-Sayed (Q), Xavier Garcia (Q), Idan Brusilovsky (Q), Pu-Chin Chen (Q), Andrew Bolt (Q), Lu Huang (Q), Alex Gurney (Q), Zhiying Zhang (Q), Alexander Pritzel (Q), Jarek Wilkiewicz (Q), Bryan Seybold (Q), Bhargav Kanagal Shamanna (Q), Felix Fischer (Q), Josef Dean (Q), Karan Gill (Q), Ross Mcilroy (Q), Abhishek Bhowmick (Q), Jeremy Selier (Q), Antoine Yang (Q), Derek Cheng (Q), Vladimir Magay (Q), Jie Tan (Q), Dhriti Varma (Q), Christian Walder (Q), Tomas Kocisky (Q), Ryo Nakashima (Q), Paul Natsev (Q), Mike Kwong (Q), Ionel Gog (Q), Chiyuan Zhang (Q), Sander Dieleman (Q), Thomas Jimma (Q), Andrey Ryabtsev (Q), Siddhartha Brahma (Q), David Steiner (Q), Dayou Du (Q), Ante \v{Z}u\v{z}ul (Q), Mislav \v{Z}ani\'c (Q), Mukund Raghavachari (Q), Willi Gierke (Q), Zeyu Zheng (Q), Dessie Petrova (Q), Yann Dauphin (Q), Yuchuan Liu (Q), Ido Kessler (Q), Steven Hand (Q), Chris Duvarney (Q), Seokhwan Kim (Q), Hyo Lee (Q), L\'eonard Hussenot (Q), Jeffrey Hui (Q), Josh Smith (Q), Deepali Jain (Q), Jiawei Xia (Q), Gaurav Singh Tomar (Q), Keyvan Amiri (Q), Du Phan (Q), Fabian Fuchs (Q), Tobias Weyand (Q), Nenad Tomasev (Q), Alexandra Cordell (Q), Xin Liu (Q), Jonathan Mallinson (Q), Pankaj Joshi (Q), Andy Crawford (Q), Arun Suggala (Q), Steve Chien (Q), Nick Fernando (Q), Mariella Sanchez-Vargas (Q), Duncan Williams (Q), Phil Crone (Q), Xiyang Luo (Q), Igor Karpov (Q), Jyn Shan (Q), Terry Thurk (Q), Robin Strudel (Q), Paul Voigtlaender (Q), Piyush Patil (Q), Tim Dozat (Q), Ali Khodaei (Q), Sahil Singla (Q), Piotr Ambroszczyk (Q), Qiyin Wu (Q), Yifan Chang (Q), Brian Roark (Q), Chaitra Hegde (Q), Tianli Ding (Q), Angelos Filos (Q), Zhongru Wu (Q), Andr\'e Susano Pinto (Q), Shuang Liu (Q), Saarthak Khanna (Q), Aditya Pandey (Q), Siobhan Mcloughlin (Q), Qiujia Li (Q), Sam Haves (Q), Allan Zhou (Q), Elena Buchatskaya (Q), Isabel Leal (Q), Peter de Boursac (Q), Nami Akazawa (Q), Nina Anderson (Q), Terry Chen (Q), Krishna Somandepalli (Q), Chen Liang (Q), Sheela Goenka (Q), Stephanie Winkler (Q), Alexander Grushetsky (Q), Yifan Ding (Q), Jamie Smith (Q), Fan Ye (Q), Jordi Pont-Tuset (Q), Eric Li (Q), Ruichao Li (Q), Tomer Golany (Q), Dawid Wegner (Q), Tao Jiang (Q), Omer Barak (Q), Yuan Shangguan (Q), Eszter V\'ertes (Q), Renee Wong (Q), J\"org Bornschein (Q), Alex Tudor (Q), Michele Bevilacqua (Q), Tom Schaul (Q), Ankit Singh Rawat (Q), Yang Zhao (Q), Kyriakos Axiotis (Q), Lei Meng (Q), Cory McLean (Q), Jonathan Lai (Q), Jennifer Beattie (Q), Nate Kushman (Q), Yaxin Liu (Q), Blair Kutzman (Q), Fiona Lang (Q), Jingchen Ye (Q), Praneeth Netrapalli (Q), Pushkar Mishra (Q), Myriam Khan (Q), Megha Goel (Q), Rob Willoughby (Q), David Tian (Q), Honglei Zhuang (Q), JD Chen (Q), Zak Tsai (Q), Tasos Kementsietsidis (Q), Arjun Khare (Q), James Keeling (Q), Keyang Xu (Q), Nathan Waters (Q), Florent Altch\'e (Q), Ashok Popat (Q), Bhavishya Mittal (Q), David Saxton (Q), Dalia El Badawy (Q), Michael Mathieu (Q), Zheng Zheng (Q), Hao Zhou (Q), Nishant Ranka (Q), Richard Shin (Q), Qingnan Duan (Q), Tim Salimans (Q), Ioana Mihailescu (Q), Uri Shaham (Q), Ming-Wei Chang (Q), Yannis Assael (Q), Nishanth Dikkala (Q), Martin Izzard (Q), Vincent Cohen-Addad (Q), Cat Graves (Q), Vlad Feinberg (Q), Grace Chung (Q), DJ Strouse (Q), Danny Karmon (Q), Sahand Sharifzadeh (Q), Zoe Ashwood (Q), Khiem Pham (Q), Jon Blanton (Q), Alex Vasiloff (Q), Jarred Barber (Q), Mark Geller (Q), Aurick Zhou (Q), Fedir Zubach (Q), Tzu-Kuo Huang (Q), Lei Zhang (Q), Himanshu Gupta (Q), Matt Young (Q), Julia Proskurnia (Q), Ronny Votel (Q), Valentin Gabeur (Q), Gabriel Barcik (Q), Aditya Tripathi (Q), Hongkun Yu (Q), Geng Yan (Q), Beer Changpinyo (Q), Filip Paveti\'c (Q), Amy Coyle (Q), Yasuhisa Fujii (Q), Jorge Gonzalez Mendez (Q), Tianhao Zhou (Q), Harish Rajamani (Q), Blake Hechtman (Q), Eddie Cao (Q), Da-Cheng Juan (Q), Yi-Xuan Tan (Q), Valentin Dalibard (Q), Yilun Du (Q), Natalie Clay (Q), Kaisheng Yao (Q), Wenhao Jia (Q), Dimple Vijaykumar (Q), Yuxiang Zhou (Q), Xinyi Bai (Q), Wei-Chih Hung (Q), Steven Pecht (Q), Georgi Todorov (Q), Nikhil Khadke (Q), Pramod Gupta (Q), Preethi Lahoti (Q), Arnaud Autef (Q), Karthik Duddu (Q), James Lee-Thorp (Q), Alexander Bykovsky (Q), Tautvydas Misiunas (Q), Sebastian Flennerhag (Q), Santhosh Thangaraj (Q), Jed McGiffin (Q), Zack Nado (Q), Markus Kunesch (Q), Andreas Noever (Q), Amir Hertz (Q), Marco Liang (Q), Victor Stone (Q), Evan Palmer (Q), Samira Daruki (Q), Arijit Pramanik (Q), Siim P\~oder (Q), Austin Kyker (Q), Mina Khan (Q), Evgeny Sluzhaev (Q), Marvin Ritter (Q), Avraham Ruderman (Q), Wenlei Zhou (Q), Chirag Nagpal (Q), Kiran Vodrahalli (Q), George Necula (Q), Paul Barham (Q), Ellie Pavlick (Q), Jay Hartford (Q), Izhak Shafran (Q), Long Zhao (Q), Maciej Miku{\l}a (Q), Tom Eccles (Q), Hidetoshi Shimokawa (Q), Kanav Garg (Q), Luke Vilnis (Q), Hanwen Chen (Q), Ilia Shumailov (Q), Kuang-Huei Lee (Q), Abdelrahman Abdelhamed (Q), Meiyan Xie (Q), Vered Cohen (Q), Ester Hlavnova (Q), Dan Malkin (Q), Chawin Sitawarin (Q), James Lottes (Q), Pauline Coquinot (Q), Tianli Yu (Q), Sandeep Kumar (Q), Jingwei Zhang (Q), Aroma Mahendru (Q), Zafarali Ahmed (Q), James Martens (Q), Tao Chen (Q), Aviel Boag (Q), Daiyi Peng (Q), Coline Devin (Q), Arseniy Klimovskiy (Q), Mary Phuong (Q), Danny Vainstein (Q), Jin Xie (Q), Bhuvana Ramabhadran (Q), Nathan Howard (Q), Xinxin Yu (Q), Gitartha Goswami (Q), Jingyu Cui (Q), Sam Shleifer (Q), Mario Pinto (Q), Chih-Kuan Yeh (Q), Ming-Hsuan Yang (Q), Sara Javanmardi (Q), Dan Ethier (Q), Chace Lee (Q), Jordi Orbay (Q), Suyog Kotecha (Q), Carla Bromberg (Q), Pete Shaw (Q), James Thornton (Q), Adi Gerzi Rosenthal (Q), Shane Gu (Q), Matt Thomas (Q), Ian Gemp (Q), Aditya Ayyar (Q), Asahi Ushio (Q), Aarush Selvan (Q), Joel Wee (Q), Chenxi Liu (Q), Maryam Majzoubi (Q), Weiren Yu (Q), Jake Abernethy (Q), Tyler Liechty (Q), Renke Pan (Q), Hoang Nguyen (Q),  Qiong (Q),  Hu (Joyce), Sarah Perrin (Joyce), Abhinav Arora (Joyce), Emily Pitler (Joyce), Weiyi Wang (Joyce), Kaushik Shivakumar (Joyce), Flavien Prost (Joyce), Ben Limonchik (Joyce), Jing Wang (Joyce), Yi Gao (Joyce), Timothee Cour (Joyce), Shyamal Buch (Joyce), Huan Gui (Joyce), Maria Ivanova (Joyce), Philipp Neubeck (Joyce), Kelvin Chan (Joyce), Lucy Kim (Joyce), Huizhong Chen (Joyce), Naman Goyal (Joyce), Da-Woon Chung (Joyce), Lu Liu (Joyce), Yao Su (Joyce), Anastasia Petrushkina (Joyce), Jiajun Shen (Joyce), Armand Joulin (Joyce), Yuanzhong Xu (Joyce), Stein Xudong Lin (Joyce), Yana Kulizhskaya (Joyce), Ciprian Chelba (Joyce), Shobha Vasudevan (Joyce), Eli Collins (Joyce), Vasilisa Bashlovkina (Joyce), Tony Lu (Joyce), Doug Fritz (Joyce), Jongbin Park (Joyce), Yanqi Zhou (Joyce), Chen Su (Joyce), Richard Tanburn (Joyce), Mikhail Sushkov (Joyce), Mitchelle Rasquinha (Joyce), Jinning Li (Joyce), Jennifer Prendki (Joyce), Yiming Li (Joyce), Pallavi LV (Joyce), Shriya Sharma (Joyce), Hen Fitoussi (Joyce), Hui Huang (Joyce), Andrew Dai (Joyce), Phuong Dao (Joyce), Mike Burrows (Joyce), Henry Prior (Joyce), Danfeng Qin (Joyce), Golan Pundak (Joyce), Lars Lowe Sjoesund (Joyce), Art Khurshudov (Joyce), Zhenkai Zhu (Joyce), Albert Webson (Joyce), Elizabeth Kemp (Joyce), Tat Tan (Joyce), Saurabh Agrawal (Joyce), Susie Sargsyan (Joyce), Liqun Cheng (Joyce), Jim Stephan (Joyce), Tom Kwiatkowski (Joyce), David Reid (Joyce), Arunkumar Byravan (Joyce), Assaf Hurwitz Michaely (Joyce), Nicolas Heess (Joyce), Luowei Zhou (Joyce), Sonam Goenka (Joyce), Viral Carpenter (Joyce), Anselm Levskaya (Joyce), Bo Wang (Joyce), Reed Roberts (Joyce), R\'emi Leblond (Joyce), Sharat Chikkerur (Joyce), Stav Ginzburg (Joyce), Max Chang (Joyce), Robert Riachi (Joyce),  Chuqiao (Joyce),  Xu (Jerry), Zal\'an Borsos (Jerry), Michael Pliskin (Jerry), Julia Pawar (Jerry), Morgane Lustman (Jerry), Hannah Kirkwood (Jerry), Ankit Anand (Jerry), Aditi Chaudhary (Jerry), Norbert Kalb (Jerry), Kieran Milan (Jerry), Sean Augenstein (Jerry), Anna Goldie (Jerry), Laurel Prince (Jerry), Karthik Raman (Jerry), Yanhua Sun (Jerry), Vivian Xia (Jerry), Aaron Cohen (Jerry), Zhouyuan Huo (Jerry), Josh Camp (Jerry), Seher Ellis (Jerry), Lukas Zilka (Jerry), David Vilar Torres (Jerry), Lisa Patel (Jerry), Sho Arora (Jerry), Betty Chan (Jerry), Jonas Adler (Jerry), Kareem Ayoub (Jerry), Jacky Liang (Jerry), Fayaz Jamil (Jerry), Jiepu Jiang (Jerry), Simon Baumgartner (Jerry), Haitian Sun (Jerry), Yael Karov (Jerry), Yaroslav Akulov (Jerry), Hui Zheng (Jerry), Irene Cai (Jerry), Claudio Fantacci (Jerry), James Rubin (Jerry), Alex Rav Acha (Jerry), Mengchao Wang (Jerry), Nina D'Souza (Jerry), Rohit Sathyanarayana (Jerry), Shengyang Dai (Jerry), Simon Rowe (Jerry), Andrey Simanovsky (Jerry), Omer Goldman (Jerry), Yuheng Kuang (Jerry), Xiaoyue Pan (Jerry), Andrew Rosenberg (Jerry), Tania Rojas-Esponda (Jerry), Praneet Dutta (Jerry), Amy Zeng (Jerry), Irina Jurenka (Jerry), Greg Farquhar (Jerry), Yamini Bansal (Jerry), Shariq Iqbal (Jerry), Becca Roelofs (Jerry), Ga-Young Joung (Jerry), Parker Beak (Jerry), Changwan Ryu (Jerry), Ryan Poplin (Jerry), Yan Wu (Jerry), Jean-Baptiste Alayrac (Jerry), Senaka Buthpitiya (Jerry), Olaf Ronneberger (Jerry), Caleb Habtegebriel (Jerry), Wei Li (Jerry), Paul Cavallaro (Jerry), Aurora Wei (Jerry), Guy Bensky (Jerry), Timo Denk (Jerry), Harish Ganapathy (Jerry), Jeff Stanway (Jerry), Pratik Joshi (Jerry), Francesco Bertolini (Jerry), Jessica Lo (Jerry), Olivia Ma (Jerry), Zachary Charles (Jerry), Geta Sampemane (Jerry), Himanshu Sahni (Jerry), Xu Chen (Jerry), Harry Askham (Jerry), David Gaddy (Jerry), Peter Young (Jerry), Jiewen Tan (Jerry), Matan Eyal (Jerry), Arthur Bra\v{z}inskas (Jerry), Li Zhong (Jerry), Zhichun Wu (Jerry), Mark Epstein (Jerry), Kai Bailey (Jerry), Andrew Hard (Jerry), Kamyu Lee (Jerry), Sasha Goldshtein (Jerry), Alex Ruiz (Jerry), Mohammed Badawi (Jerry), Matthias Lochbrunner (Jerry), JK Kearns (Jerry), Ashley Brown (Jerry), Fabio Pardo (Jerry), Theophane Weber (Jerry), Haichuan Yang (Jerry), Pan-Pan Jiang (Jerry), Berkin Akin (Jerry), Zhao Fu (Jerry), Marcus Wainwright (Jerry), Chi Zou (Jerry), Meenu Gaba (Jerry), Pierre-Antoine Manzagol (Jerry), Wendy Kan (Jerry), Yang Song (Jerry), Karina Zainullina (Jerry), Rui Lin (Jerry), Jeongwoo Ko (Jerry), Salil Deshmukh (Jerry), Apoorv Jindal (Jerry), James Svensson (Jerry), Divya Tyam (Jerry), Heri Zhao (Jerry), Christine Kaeser-Chen (Jerry), Scott Baird (Jerry), Pooya Moradi (Jerry), Jamie Hall (Jerry), Qiuchen Guo (Jerry), Vincent Tsang (Jerry), Bowen Liang (Jerry), Fernando Pereira (Jerry), Suhas Ganesh (Jerry), Ivan Korotkov (Jerry), Jakub Adamek (Jerry), Sridhar Thiagarajan (Jerry), Vinh Tran (Jerry), Charles Chen (Jerry), Chris Tar (Jerry), Sanil Jain (Jerry), Ishita Dasgupta (Jerry), Taylan Bilal (Jerry), David Reitter (Jerry), Kai Zhao (Jerry), Giulia Vezzani (Jerry), Yasmin Gehman (Jerry), Pulkit Mehta (Jerry), Lauren Beltrone (Jerry), Xerxes Dotiwalla (Jerry), Sergio Guadarrama (Jerry), Zaheer Abbas (Jerry), Stefani Karp (Jerry), Petko Georgiev (Jerry), Chun-Sung Ferng (Jerry), Marc Brockschmidt (Jerry), Liqian Peng (Jerry), Christoph Hirnschall (Jerry), Vikas Verma (Jerry), Yingying Bi (Jerry), Ying Xiao (Jerry), Avigail Dabush (Jerry), Kelvin Xu (Jerry), Phil Wallis (Jerry), Randall Parker (Jerry), Qifei Wang (Jerry), Yang Xu (Jerry), Ilkin Safarli (Jerry), Dinesh Tewari (Jerry), Yin Zhang (Jerry), Seungyeon Kim (Jerry), Andrea Gesmundo (Jerry), Mackenzie Thomas (Jerry), Sergey Levi (Jerry), Ahmed Chowdhury (Jerry), Kanishka Rao (Jerry), Peter Garst (Jerry), Sam Conway-Rahman (Jerry), Helen Ran (Jerry), Kay McKinney (Jerry), Zhisheng Xiao (Jerry), Wenhao Yu (Jerry), Rohan Agrawal (Jerry), Axel Stjerngren (Jerry), Catalin Ionescu (Jerry), Jingjing Chen (Jerry), Vivek Sharma (Jerry), Justin Chiu (Jerry), Fei Liu (Jerry), Ken Franko (Jerry), Clayton Sanford (Jerry), Xingyu Cai (Jerry), Paul Michel (Jerry), Sanjay Ganapathy (Jerry), Jane Labanowski (Jerry), Zachary Garrett (Jerry), Ben Vargas (Jerry), Sean Sun (Jerry), Bryan Gale (Jerry), Thomas Buschmann (Jerry), Guillaume Desjardins (Jerry), Nimesh Ghelani (Jerry), Palak Jain (Jerry), Mudit Verma (Jerry), Chulayuth Asawaroengchai (Jerry), Julian Eisenschlos (Jerry), Jitendra Harlalka (Jerry), Hideto Kazawa (Jerry), Don Metzler (Jerry), Joshua Howland (Jerry), Ying Jian (Jerry), Jake Ades (Jerry), Viral Shah (Jerry), Tynan Gangwani (Jerry), Seungji Lee (Jerry), Roman Ring (Jerry), Steven M. Hernandez (Jerry), Dean Reich (Jerry), Amer Sinha (Jerry), Ashutosh Sathe (Jerry), Joe Kovac (Jerry), Ashleah Gill (Jerry), Ajay Kannan (Jerry), Andrea D'olimpio (Jerry), Martin Sevenich (Jerry), Jay Whang (Jerry), Been Kim (Jerry), Khe Chai Sim (Jerry), Jilin Chen (Jerry), Jiageng Zhang (Jerry), Shuba Lall (Jerry), Yossi Matias (Jerry), Bill Jia (Jerry), Abe Friesen (Jerry), Sara Nasso (Jerry), Ashish Thapliyal (Jerry), Bryan Perozzi (Jerry), Ting Yu (Jerry), Anna Shekhawat (Jerry), Safeen Huda (Jerry), Peter Grabowski (Jerry), Eric Wang (Jerry), Ashwin Sreevatsa (Jerry), Hilal Dib (Jerry), Mehadi Hassen (Jerry), Parker Schuh (Jerry), Vedrana Milutinovic (Jerry), Chris Welty (Jerry), Michael Quinn (Jerry), Ali Shah (Jerry), Bangju Wang (Jerry), Gabe Barth-Maron (Jerry), Justin Frye (Jerry), Natalie Axelsson (Jerry), Tao Zhu (Jerry), Yukun Ma (Jerry), Irene Giannoumis (Jerry), Hanie Sedghi (Jerry), Chang Ye (Jerry), Yi Luan (Jerry), Kevin Aydin (Jerry), Bilva Chandra (Jerry), Vivek Sampathkumar (Jerry), Ronny Huang (Jerry), Victor Lavrenko (Jerry), Ahmed Eleryan (Jerry), Zhi Hong (Jerry), Steven Hansen (Jerry), Sara Mc Carthy (Jerry), Bidisha Samanta (Jerry), Domagoj \'Cevid (Jerry), Xin Wang (Jerry), Fangtao Li (Jerry), Michael Voznesensky (Jerry), Matt Hoffman (Jerry), Andreas Terzis (Jerry), Vikash Sehwag (Jerry), Gil Fidel (Jerry), Luheng He (Jerry), Mu Cai (Jerry), Yanzhang He (Jerry), Alex Feng (Jerry), Martin Nikoltchev (Jerry), Samrat Phatale (Jerry), Jason Chase (Jerry), Rory Lawton (Jerry), Ming Zhang (Jerry), Tom Ouyang (Jerry), Manuel Tragut (Jerry), Mehdi Hafezi Manshadi (Jerry), Arjun Narayanan (Jerry), Jiaming Shen (Jerry), Xu Gao (Jerry), Tolga Bolukbasi (Jerry), Nick Roy (Jerry), Xin Li (Jerry), Daniel Golovin (Jerry), Liviu Panait (Jerry), Zhen Qin (Jerry), Guangxing Han (Jerry), Thomas Anthony (Jerry), Sneha Kudugunta (Jerry), Viorica Patraucean (Jerry), Aniket Ray (Jerry), Xinyun Chen (Jerry), Xiaochen Yang (Jerry), Tanuj Bhatia (Jerry), Pranav Talluri (Jerry), Alex Morris (Jerry), Andrija Ra\v{z}natovi\'c (Jerry), Bethanie Brownfield (Jerry), James An (Jerry), Sheng Peng (Jerry), Patrick Kane (Jerry), Ce Zheng (Jerry), Nico Duduta (Jerry), Joshua Kessinger (Jerry), James Noraky (Jerry), Siqi Liu (Jerry), Keran Rong (Jerry), Petar Veli\v{c}kovi\'c (Jerry), Keith Rush (Jerry), Alex Goldin (Jerry), Fanny Wei (Jerry), Shiva Mohan Reddy Garlapati (Jerry), Caroline Pantofaru (Jerry), Okwan Kwon (Jerry), Jianmo Ni (Jerry), Eric Noland (Jerry), Julia Di Trapani (Jerry), Fran\c{c}oise Beaufays (Jerry), Abhijit Guha Roy (Jerry), Yinlam Chow (Jerry), Aybuke Turker (Jerry), Geoffrey Cideron (Jerry), Lantao Mei (Jerry), Jon Clark (Jerry), Qingyun Dou (Jerry), Matko Bo\v{s}njak (Jerry), Ralph Leith (Jerry), Yuqing Du (Jerry), Amir Yazdanbakhsh (Jerry), Milad Nasr (Jerry), Chester Kwak (Jerry), Suraj Satishkumar Sheth (Jerry), Alex Kaskasoli (Jerry), Ankesh Anand (Jerry), Balaji Lakshminarayanan (Jerry), Sammy Jerome (Jerry), David Bieber (Jerry), Chun-Te Chu (Jerry), Alexandre Senges (Jerry), Tianxiao Shen (Jerry), Mukund Sridhar (Jerry), Ndaba Ndebele (Jerry), Benjamin Beyret (Jerry), Shakir Mohamed (Jerry), Mia Chen (Jerry), Markus Freitag (Jerry), Jiaxian Guo (Jerry), Luyang Liu (Jerry), Paul Roit (Jerry), Heng Chen (Jerry), Shen Yan (Jerry), Tom Stone (Jerry), JD Co-Reyes (Jerry), Jeremy Cole (Jerry), Salvatore Scellato (Jerry), Shekoofeh Azizi (Jerry), Hadi Hashemi (Jerry), Alicia Jin (Jerry), Anand Iyer (Jerry), Marcella Valentine (Jerry), Andr\'as Gy\"orgy (Jerry), Arun Ahuja (Jerry), Daniel Hernandez Diaz (Jerry), Chen-Yu Lee (Jerry), Nathan Clement (Jerry), Weize Kong (Jerry), Drew Garmon (Jerry), Ishaan Watts (Jerry), Kush Bhatia (Jerry), Khyatti Gupta (Jerry), Matt Miecnikowski (Jerry), Hugo Vallet (Jerry), Ankur Taly (Jerry), Edward Loper (Jerry), Saket Joshi (Jerry), James Atwood (Jerry), Jo Chick (Jerry), Mark Collier (Jerry), Fotis Iliopoulos (Jerry), Ryan Trostle (Jerry), Beliz Gunel (Jerry), Ramiro Leal-Cavazos (Jerry), Arnar Mar Hrafnkelsson (Jerry), Michael Guzman (Jerry), Xiaoen Ju (Jerry), Andy Forbes (Jerry), Jesse Emond (Jerry), Kushal Chauhan (Jerry), Ben Caine (Jerry), Li Xiao (Jerry), Wenjun Zeng (Jerry), Alexandre Moufarek (Jerry), Daniel Murphy (Jerry), Maya Meng (Jerry), Nitish Gupta (Jerry), Felix Riedel (Jerry), Anil Das (Jerry), Elijah Lawal (Jerry), Shashi Narayan (Jerry), Tiberiu Sosea (Jerry), James Swirhun (Jerry), Linda Friso (Jerry), Behnam Neyshabur (Jerry), Jing Lu (Jerry), Sertan Girgin (Jerry), Michael Wunder (Jerry), Edouard Yvinec (Jerry), Aroonalok Pyne (Jerry), Victor Carbune (Jerry), Shruti Rijhwani (Jerry), Yang Guo (Jerry), Tulsee Doshi (Jerry), Anton Briukhov (Jerry), Max Bain (Jerry), Ayal Hitron (Jerry), Xuanhui Wang (Jerry), Ashish Gupta (Jerry), Ke Chen (Jerry), Cosmo Du (Jerry), Weiyang Zhang (Jerry), Dhruv Shah (Jerry), Arjun Akula (Jerry), Max Dylla (Jerry), Ashyana Kachra (Jerry), Weicheng Kuo (Jerry), Tingting Zou (Jerry), Lily Wang (Jerry), Luyao Xu (Jerry), Jifan Zhu (Jerry), Justin Snyder (Jerry), Sachit Menon (Jerry), Orhan Firat (Jerry), Igor Mordatch (Jerry), Yuan Yuan (Jerry), Natalia Ponomareva (Jerry), Rory Blevins (Jerry), Lawrence Moore (Jerry), Weijun Wang (Jerry), Phil Chen (Jerry), Martin Scholz (Jerry), Artur Dwornik (Jerry), Jason Lin (Jerry), Sicheng Li (Jerry), Diego Antognini (Jerry), Te I (Jerry), Xiaodan Song (Jerry), Matt Miller (Jerry), Uday Kalra (Jerry), Adam Raveret (Jerry), Oscar Akerlund (Jerry), Felix Wu (Jerry), Andrew Nystrom (Jerry), Namrata Godbole (Jerry), Tianqi Liu (Jerry), Hannah DeBalsi (Jerry), Jewel Zhao (Jerry), Buhuang Liu (Jerry), Avi Caciularu (Jerry), Lauren Lax (Jerry), Urvashi Khandelwal (Jerry), Victoria Langston (Jerry), Eric Bailey (Jerry), Silvio Lattanzi (Jerry), Yufei Wang (Jerry), Neel Kovelamudi (Jerry), Sneha Mondal (Jerry), Guru Guruganesh (Jerry), Nan Hua (Jerry), Ofir Roval (Jerry), Pawe{\l} Weso{\l}owski (Jerry), Rishikesh Ingale (Jerry), Jonathan Halcrow (Jerry), Tim Sohn (Jerry), Christof Angermueller (Jerry), Bahram Raad (Jerry), Eli Stickgold (Jerry), Eva Lu (Jerry), Alec Kosik (Jerry), Jing Xie (Jerry), Timothy Lillicrap (Jerry), Austin Huang (Jerry), Lydia Lihui Zhang (Jerry), Dominik Paulus (Jerry), Clement Farabet (Jerry), Alex Wertheim (Jerry), Bing Wang (Jerry), Rishabh Joshi (Jerry), Chu-ling Ko (Jerry), Yonghui Wu (Jerry), Shubham Agrawal (Jerry), Lily Lin (Jerry), XiangHai Sheng (Jerry), Peter Sung (Jerry), Tyler Breland-King (Jerry), Christina Butterfield (Jerry), Swapnil Gawde (Jerry), Sumeet Singh (Jerry), Qiao Zhang (Jerry), Raj Apte (Jerry), Shilpa Shetty (Jerry), Adrian Hutter (Jerry), Tao Li (Jerry), Elizabeth Salesky (Jerry), Federico Lebron (Jerry), Jonni Kanerva (Jerry), Michela Paganini (Jerry), Arthur Nguyen (Jerry), Rohith Vallu (Jerry), Jan-Thorsten Peter (Jerry), Sarmishta Velury (Jerry), David Kao (Jerry), Jay Hoover (Jerry), Anna Bortsova (Jerry), Colton Bishop (Jerry), Shoshana Jakobovits (Jerry), Alessandro Agostini (Jerry), Alekh Agarwal (Jerry), Chang Liu (Jerry), Charles Kwong (Jerry), Sasan Tavakkol (Jerry), Ioana Bica (Jerry), Alex Greve (Jerry), Anirudh GP (Jerry), Jake Marcus (Jerry), Le Hou (Jerry), Tom Duerig (Jerry), Rivka Moroshko (Jerry), Dave Lacey (Jerry), Andy Davis (Jerry), Julien Amelot (Jerry), Guohui Wang (Jerry), Frank Kim (Jerry), Theofilos Strinopoulos (Jerry), Hui Wan (Jerry), Charline Le Lan (Jerry), Shankar Krishnan (Jerry), Haotian Tang (Jerry), Peter Humphreys (Jerry), Junwen Bai (Jerry), Idan Heimlich Shtacher (Jerry), Diego Machado (Jerry), Chenxi Pang (Jerry), Ken Burke (Jerry), Dangyi Liu (Jerry), Renga Aravamudhan (Jerry), Yue Song (Jerry), Ed Hirst (Jerry), Abhimanyu Singh (Jerry), Brendan Jou (Jerry), Liang Bai (Jerry), Francesco Piccinno (Jerry), Chuyuan Kelly Fu (Jerry), Robin Alazard (Jerry), Barak Meiri (Jerry), Daniel Winter (Jerry), Charlie Chen (Jerry), Mingda Zhang (Jerry), Jens Heitkaemper (Jerry), John Lambert (Jerry), Jinhyuk Lee (Jerry), Alexander Fr\"ommgen (Jerry), Sergey Rogulenko (Jerry), Pranav Nair (Jerry), Paul Niemczyk (Jerry), Anton Bulyenov (Jerry), Bibo Xu (Jerry), Hadar Shemtov (Jerry), Morteza Zadimoghaddam (Jerry), Serge Toropov (Jerry), Mateo Wirth (Jerry), Hanjun Dai (Jerry), Sreenivas Gollapudi (Jerry), Daniel Zheng (Jerry), Alex Kurakin (Jerry), Chansoo Lee (Jerry), Kalesha Bullard (Jerry), Nicolas Serrano (Jerry), Ivana Balazevic (Jerry), Yang Li (Jerry), Johan Schalkwyk (Jerry), Mark Murphy (Jerry), Mingyang Zhang (Jerry), Kevin Sequeira (Jerry), Romina Datta (Jerry), Nishant Agrawal (Jerry), Charles Sutton (Jerry), Nithya Attaluri (Jerry), Mencher Chiang (Jerry), Wael Farhan (Jerry), Gregory Thornton (Jerry), Kate Lin (Jerry), Travis Choma (Jerry), Hung Nguyen (Jerry), Kingshuk Dasgupta (Jerry), Dirk Robinson (Jerry), Iulia Com\c{s}a (Jerry), Michael Riley (Jerry), Arjun Pillai (Jerry), Basil Mustafa (Jerry), Ben Golan (Jerry), Amir Zandieh (Jerry), Jean-Baptiste Lespiau (Jerry), Billy Porter (Jerry), David Ross (Jerry), Sujeevan Rajayogam (Jerry), Mohit Agarwal (Jerry), Subhashini Venugopalan (Jerry), Bobak Shahriari (Jerry), Qiqi Yan (Jerry), Hao Xu (Jerry), Taylor Tobin (Jerry), Pavel Dubov (Jerry), Hongzhi Shi (Jerry), Adri\`a Recasens (Jerry), Anton Kovsharov (Jerry), Sebastian Borgeaud (Jerry), Lucio Dery (Jerry), Shanthal Vasanth (Jerry), Elena Gribovskaya (Jerry), Linhai Qiu (Jerry), Mahdis Mahdieh (Jerry), Wojtek Skut (Jerry), Elizabeth Nielsen (Jerry), CJ Zheng (Jerry), Adams Yu (Jerry), Carrie Grimes Bostock (Jerry), Shaleen Gupta (Jerry), Aaron Archer (Jerry), Chris Rawles (Jerry), Elinor Davies (Jerry), Alexey Svyatkovskiy (Jerry), Tomy Tsai (Jerry), Yoni Halpern (Jerry), Christian Reisswig (Jerry), Bartek Wydrowski (Jerry), Bo Chang (Jerry), Joan Puigcerver (Jerry), Mor Hazan Taege (Jerry), Jian Li (Jerry), Eva Schnider (Jerry), Xinjian Li (Jerry), Dragos Dena (Jerry), Yunhan Xu (Jerry), Umesh Telang (Jerry), Tianze Shi (Jerry), Heiga Zen (Jerry), Kyle Kastner (Jerry), Yeongil Ko (Jerry), Neesha Subramaniam (Jerry), Aviral Kumar (Jerry), Pete Blois (Jerry), Zhuyun Dai (Jerry), John Wieting (Jerry), Yifeng Lu (Jerry), Yoel Zeldes (Jerry), Tian Xie (Jerry), Anja Hauth (Jerry), Alexandru \c{T}ifrea (Jerry), Yuqi Li (Jerry), Sam El-Husseini (Jerry), Dan Abolafia (Jerry), Howard Zhou (Jerry), Wen Ding (Jerry), Sahra Ghalebikesabi (Jerry), Carlos Gu\'ia (Jerry), Andrii Maksai (Jerry), \'Agoston Weisz (Jerry), Sercan Arik (Jerry), Nick Sukhanov (Jerry), Aga \'Swietlik (Jerry), Xuhui Jia (Jerry), Luo Yu (Jerry), Weiyue Wang (Jerry), Mark Brand (Jerry), Dawn Bloxwich (Jerry), Sean Kirmani (Jerry), Zhe Chen (Jerry), Alec Go (Jerry), Pablo Sprechmann (Jerry), Nithish Kannen (Jerry), Alen Carin (Jerry), Paramjit Sandhu (Jerry), Isabel Edkins (Jerry), Leslie Nooteboom (Jerry), Jai Gupta (Jerry), Loren Maggiore (Jerry), Javad Azizi (Jerry), Yael Pritch (Jerry), Pengcheng Yin (Jerry), Mansi Gupta (Jerry), Danny Tarlow (Jerry), Duncan Smith (Jerry), Desi Ivanov (Jerry), Mohammad Babaeizadeh (Jerry), Ankita Goel (Jerry), Satish Kambala (Jerry), Grace Chu (Jerry), Matej Kastelic (Jerry), Michelle Liu (Jerry), Hagen Soltau (Jerry), Austin Stone (Jerry), Shivani Agrawal (Jerry), Min Kim (Jerry), Kedar Soparkar (Jerry), Srinivas Tadepalli (Jerry), Oskar Bunyan (Jerry), Rachel Soh (Jerry), Arvind Kannan (Jerry), DY Kim (Jerry), Blake JianHang Chen (Jerry), Afief Halumi (Jerry), Sudeshna Roy (Jerry), Yulong Wang (Jerry), Olcan Sercinoglu (Jerry), Gena Gibson (Jerry), Sijal Bhatnagar (Jerry), Motoki Sano (Jerry), Daniel von Dincklage (Jerry), Qingchun Ren (Jerry), Blagoj Mitrevski (Jerry), Mirek Ol\v{s}\'ak (Jerry), Jennifer She (Jerry), Carl Doersch (Jerry),  Jilei (Jerry),  Wang (Xinyi), Bingyuan Liu (Xinyi), Qijun Tan (Xinyi), Tamar Yakar (Xinyi), Tris Warkentin (Xinyi), Alex Ramirez (Xinyi), Carl Lebsack (Xinyi), Josh Dillon (Xinyi), Rajiv Mathews (Xinyi), Tom Cobley (Xinyi), Zelin Wu (Xinyi), Zhuoyuan Chen (Xinyi), Jon Simon (Xinyi), Swaroop Nath (Xinyi), Tara Sainath (Xinyi), Alexei Bendebury (Xinyi), Ryan Julian (Xinyi), Bharath Mankalale (Xinyi), Daria \'Curko (Xinyi), Paulo Zacchello (Xinyi), Adam R. Brown (Xinyi), Kiranbir Sodhia (Xinyi), Heidi Howard (Xinyi), Sergi Caelles (Xinyi), Abhinav Gupta (Xinyi), Gareth Evans (Xinyi), Anna Bulanova (Xinyi), Lesley Katzen (Xinyi), Roman Goldenberg (Xinyi), Anton Tsitsulin (Xinyi), Joe Stanton (Xinyi), Benoit Schillings (Xinyi), Vitaly Kovalev (Xinyi), Corey Fry (Xinyi), Rushin Shah (Xinyi), Kuo Lin (Xinyi), Shyam Upadhyay (Xinyi), Cheng Li (Xinyi), Soroush Radpour (Xinyi), Marcello Maggioni (Xinyi), Jing Xiong (Xinyi), Lukas Haas (Xinyi), Jenny Brennan (Xinyi), Aishwarya Kamath (Xinyi), Nikolay Savinov (Xinyi), Arsha Nagrani (Xinyi), Trevor Yacovone (Xinyi), Ryan Kappedal (Xinyi), Kostas Andriopoulos (Xinyi), Li Lao (Xinyi), YaGuang Li (Xinyi), Grigory Rozhdestvenskiy (Xinyi), Kazuma Hashimoto (Xinyi), Andrew Audibert (Xinyi), Sophia Austin (Xinyi), Daniel Rodriguez (Xinyi), Anian Ruoss (Xinyi), Garrett Honke (Xinyi), Deep Karkhanis (Xinyi), Xi Xiong (Xinyi), Qing Wei (Xinyi), James Huang (Xinyi), Zhaoqi Leng (Xinyi), Vittal Premachandran (Xinyi), Stan Bileschi (Xinyi), Georgios Evangelopoulos (Xinyi), Thomas Mensink (Xinyi), Jay Pavagadhi (Xinyi), Denis Teplyashin (Xinyi), Paul Chang (Xinyi), Linting Xue (Xinyi), Garrett Tanzer (Xinyi), Sally Goldman (Xinyi), Kaushal Patel (Xinyi), Shixin Li (Xinyi), Jeremy Wiesner (Xinyi), Ivy Zheng (Xinyi), Ian Stewart-Binks (Xinyi), Jie Han (Xinyi), Zhi Li (Xinyi), Liangchen Luo (Xinyi), Karel Lenc (Xinyi), Mario Lu\v{c}i\'c (Xinyi), Fuzhao Xue (Xinyi), Ryan Mullins (Xinyi), Alexey Guseynov (Xinyi), Chung-Ching Chang (Xinyi), Isaac Galatzer-Levy (Xinyi), Adam Zhang (Xinyi), Garrett Bingham (Xinyi), Grace Hu (Xinyi), Ale Hartman (Xinyi), Yue Ma (Xinyi), Jordan Griffith (Xinyi), Alex Irpan (Xinyi), Carey Radebaugh (Xinyi), Summer Yue (Xinyi), Lijie Fan (Xinyi), Victor Ungureanu (Xinyi), Christina Sorokin (Xinyi), Hannah Teufel (Xinyi), Peiran Li (Xinyi), Rohan Anil (Xinyi), Dimitris Paparas (Xinyi), Todd Wang (Xinyi), Chu-Cheng Lin (Xinyi), Hui Peng (Xinyi), Megan Shum (Xinyi), Goran Petrovic (Xinyi), Demetra Brady (Xinyi), Richard Nguyen (Xinyi), Klaus Macherey (Xinyi), Zhihao Li (Xinyi), Harman Singh (Xinyi), Madhavi Yenugula (Xinyi), Mariko Iinuma (Xinyi), Xinyi Chen (Xinyi), Kavya Kopparapu (Xinyi), Alexey Stern (Xinyi), Shachi Dave (Xinyi), Chandu Thekkath (Xinyi), Florence Perot (Xinyi), Anurag Kumar (Xinyi), Fangda Li (Xinyi), Yang Xiao (Xinyi), Matthew Bilotti (Xinyi), Mohammad Hossein Bateni (Xinyi), Isaac Noble (Xinyi), Lisa Lee (Xinyi), Amelio V\'azquez-Reina (Xinyi), Julian Salazar (Xinyi), Xiaomeng Yang (Xinyi), Boyu Wang (Xinyi), Ela Gruzewska (Xinyi), Anand Rao (Xinyi), Sindhu Raghuram (Xinyi), Zheng Xu (Xinyi), Eyal Ben-David (Xinyi), Jieru Mei (Xinyi), Sid Dalmia (Xinyi), Zhaoyi Zhang (Xinyi), Yuchen Liu (Xinyi), Gagan Bansal (Xinyi), Helena Pankov (Xinyi), Steven Schwarcz (Xinyi), Andrea Burns (Xinyi), Christine Chan (Xinyi), Sumit Sanghai (Xinyi), Ricky Liang (Xinyi), Ethan Liang (Xinyi), Antoine He (Xinyi), Amy Stuart (Xinyi), Arun Narayanan (Xinyi), Yukun Zhu (Xinyi), Christian Frank (Xinyi), Bahar Fatemi (Xinyi), Amit Sabne (Xinyi), Oran Lang (Xinyi), Indro Bhattacharya (Xinyi), Shane Settle (Xinyi), Maria Wang (Xinyi), Brendan McMahan (Xinyi), Andrea Tacchetti (Xinyi), Livio Baldini Soares (Xinyi), Majid Hadian (Xinyi), Serkan Cabi (Xinyi), Timothy Chung (Xinyi), Nikita Putikhin (Xinyi), Gang Li (Xinyi), Jeremy Chen (Xinyi), Austin Tarango (Xinyi), Henryk Michalewski (Xinyi), Mehran Kazemi (Xinyi), Hussain Masoom (Xinyi), Hila Sheftel (Xinyi), Rakesh Shivanna (Xinyi), Archita Vadali (Xinyi), Ramona Comanescu (Xinyi), Doug Reid (Xinyi), Joss Moore (Xinyi), Arvind Neelakantan (Xinyi), Micha\"el Sander (Xinyi), Jonathan Herzig (Xinyi), Aviv Rosenberg (Xinyi), Mostafa Dehghani (Xinyi), JD Choi (Xinyi), Michael Fink (Xinyi), Reid Hayes (Xinyi), Eric Ge (Xinyi), Shitao Weng (Xinyi), Chia-Hua Ho (Xinyi), John Karro (Xinyi), Kalpesh Krishna (Xinyi), Lam Nguyen Thiet (Xinyi), Amy Skerry-Ryan (Xinyi), Daniel Eppens (Xinyi), Marco Andreetto (Xinyi), Navin Sarma (Xinyi), Silvano Bonacina (Xinyi), Burcu Karagol Ayan (Xinyi), Megha Nawhal (Xinyi), Zhihao Shan (Xinyi), Mike Dusenberry (Xinyi), Shantanu Thakoor (Xinyi), Sagar Gubbi (Xinyi), Duc Dung Nguyen (Xinyi), Reut Tsarfaty (Xinyi), Samuel Albanie (Xinyi), Jovana Mitrovi\'c (Xinyi), Meet Gandhi (Xinyi), Bo-Juen Chen (Xinyi), Alessandro Epasto (Xinyi), Georgi Stephanov (Xinyi), Ye Jin (Xinyi), Samuel Gehman (Xinyi), Aida Amini (Xinyi), Jack Weber (Xinyi), Feryal Behbahani (Xinyi), Shawn Xu (Xinyi), Miltos Allamanis (Xinyi), Xi Chen (Xinyi), Myle Ott (Xinyi), Claire Sha (Xinyi), Michal Jastrzebski (Xinyi), Hang Qi (Xinyi), David Greene (Xinyi), Xinyi Wu (Xinyi), Abodunrinwa Toki (Xinyi), Daniel Vlasic (Xinyi), Jane Shapiro (Xinyi), Ragha Kotikalapudi (Xinyi), Zhe Shen (Xinyi), Takaaki Saeki (Xinyi), Sirui Xie (Xinyi), Albin Cassirer (Xinyi), Shikhar Bharadwaj (Xinyi), Tatsuya Kiyono (Xinyi), Srinadh Bhojanapalli (Xinyi), Elan Rosenfeld (Xinyi), Sam Ritter (Xinyi), Jieming Mao (Xinyi), Jo\~ao Gabriel Oliveira (Xinyi), Zoltan Egyed (Xinyi), Bernd Bandemer (Xinyi), Emilio Parisotto (Xinyi), Keisuke Kinoshita (Xinyi), Juliette Pluto (Xinyi), Petros Maniatis (Xinyi), Steve Li (Xinyi), Yaohui Guo (Xinyi), Golnaz Ghiasi (Xinyi), Jean Tarbouriech (Xinyi), Srimon Chatterjee (Xinyi), Julie Jin (Xinyi),  Katrina (Xinyi),  Xu (Tu\^an), Jennimaria Palomaki (Tu\^an), S\'eb Arnold (Tu\^an), Madhavi Sewak (Tu\^an), Federico Piccinini (Tu\^an), Mohit Sharma (Tu\^an), Ben Albrecht (Tu\^an), Sean Purser-haskell (Tu\^an), Ashwin Vaswani (Tu\^an), Chongyan Chen (Tu\^an), Matheus Wisniewski (Tu\^an), Qin Cao (Tu\^an), John Aslanides (Tu\^an), Nguyet Minh Phu (Tu\^an), Maximilian Sieb (Tu\^an), Lauren Agubuzu (Tu\^an), Anne Zheng (Tu\^an), Daniel Sohn (Tu\^an), Marco Selvi (Tu\^an), Anders Andreassen (Tu\^an), Krishan Subudhi (Tu\^an), Prem Eruvbetine (Tu\^an), Oliver Woodman (Tu\^an), Tomas Mery (Tu\^an), Sebastian Krause (Tu\^an), Xiaoqi Ren (Tu\^an), Xiao Ma (Tu\^an), Jincheng Luo (Tu\^an), Dawn Chen (Tu\^an), Wei Fan (Tu\^an), Henry Griffiths (Tu\^an), Christian Schuler (Tu\^an), Alice Li (Tu\^an), Shujian Zhang (Tu\^an), Jean-Michel Sarr (Tu\^an), Shixin Luo (Tu\^an), Riccardo Patana (Tu\^an), Matthew Watson (Tu\^an), Dani Naboulsi (Tu\^an), Michael Collins (Tu\^an), Sailesh Sidhwani (Tu\^an), Emiel Hoogeboom (Tu\^an), Sharon Silver (Tu\^an), Emily Caveness (Tu\^an), Xiaokai Zhao (Tu\^an), Mikel Rodriguez (Tu\^an), Maxine Deines (Tu\^an), Libin Bai (Tu\^an), Patrick Griffin (Tu\^an), Marco Tagliasacchi (Tu\^an), Emily Xue (Tu\^an), Spandana Raj Babbula (Tu\^an), Bo Pang (Tu\^an), Nan Ding (Tu\^an), Gloria Shen (Tu\^an), Elijah Peake (Tu\^an), Remi Crocker (Tu\^an), Shubha Srinivas Raghvendra (Tu\^an), Danny Swisher (Tu\^an), Woohyun Han (Tu\^an), Richa Singh (Tu\^an), Ling Wu (Tu\^an), Vladimir Pchelin (Tu\^an), Tsendsuren Munkhdalai (Tu\^an), Dana Alon (Tu\^an), Geoff Bacon (Tu\^an), Efren Robles (Tu\^an), Jannis Bulian (Tu\^an), Melvin Johnson (Tu\^an), George Powell (Tu\^an), Felipe Tiengo Ferreira (Tu\^an), Yaoyiran Li (Tu\^an), Frederik Benzing (Tu\^an), Mihajlo Velimirovi\'c (Tu\^an), Hubert Soyer (Tu\^an), William Kong (Tu\^an),  Tony (Tu\^an),  Nguy\^en (Lucas), Zhen Yang (Lucas), Jeremiah Liu (Lucas), Joost van Amersfoort (Lucas), Daniel Gillick (Lucas), Baochen Sun (Lucas), Nathalie Rauschmayr (Lucas), Katie Zhang (Lucas), Serena Zhan (Lucas), Tao Zhou (Lucas), Alexey Frolov (Lucas), Chengrun Yang (Lucas), Denis Vnukov (Lucas), Louis Rouillard (Lucas), Hongji Li (Lucas), Amol Mandhane (Lucas), Nova Fallen (Lucas), Rajesh Venkataraman (Lucas), Clara Huiyi Hu (Lucas), Jennifer Brennan (Lucas), Jenny Lee (Lucas), Jerry Chang (Lucas), Martin Sundermeyer (Lucas), Zhufeng Pan (Lucas), Rosemary Ke (Lucas), Simon Tong (Lucas), Alex Fabrikant (Lucas), William Bono (Lucas), Jindong Gu (Lucas), Ryan Foley (Lucas), Yiran Mao (Lucas), Manolis Delakis (Lucas), Dhruva Bhaswar (Lucas), Roy Frostig (Lucas), Nick Li (Lucas), Avital Zipori (Lucas), Cath Hope (Lucas), Olga Kozlova (Lucas), Swaroop Mishra (Lucas), Josip Djolonga (Lucas), Craig Schiff (Lucas), Majd Al Merey (Lucas), Eleftheria Briakou (Lucas), Peter Morgan (Lucas), Andy Wan (Lucas), Avinatan Hassidim (Lucas), RJ Skerry-Ryan (Lucas), Kuntal Sengupta (Lucas), Mary Jasarevic (Lucas), Praveen Kallakuri (Lucas), Paige Kunkle (Lucas), Hannah Brennan (Lucas), Tom Lieber (Lucas), Hassan Mansoor (Lucas), Julian Walker (Lucas), Bing Zhang (Lucas), Annie Xie (Lucas), Goran \v{Z}u\v{z}i\'c (Lucas), Adaeze Chukwuka (Lucas), Alex Druinsky (Lucas), Donghyun Cho (Lucas), Rui Yao (Lucas), Ferjad Naeem (Lucas), Shiraz Butt (Lucas), Eunyoung Kim (Lucas), Zhipeng Jia (Lucas), Mandy Jordan (Lucas), Adam Lelkes (Lucas), Mark Kurzeja (Lucas), Sophie Wang (Lucas), James Zhao (Lucas), Andrew Over (Lucas), Abhishek Chakladar (Lucas), Marcel Prasetya (Lucas), Neha Jha (Lucas), Sriram Ganapathy (Lucas), Yale Cong (Lucas), Prakash Shroff (Lucas), Carl Saroufim (Lucas), Sobhan Miryoosefi (Lucas), Mohamed Hammad (Lucas), Tajwar Nasir (Lucas), Weijuan Xi (Lucas), Yang Gao (Lucas), Young Maeng (Lucas), Ben Hora (Lucas), Chin-Yi Cheng (Lucas), Parisa Haghani (Lucas), Yoad Lewenberg (Lucas), Caden Lu (Lucas), Martin Matysiak (Lucas), Naina Raisinghani (Lucas), Huiyu Wang (Lucas), Lexi Baugher (Lucas), Rahul Sukthankar (Lucas), Minh Giang (Lucas), John Schultz (Lucas), Noah Fiedel (Lucas), Minmin Chen (Lucas), Cheng-Chun Lee (Lucas), Tapomay Dey (Lucas), Hao Zheng (Lucas), Shachi Paul (Lucas), Celine Smith (Lucas), Andy Ly (Lucas), Yicheng Wang (Lucas), Rishabh Bansal (Lucas), Bartek Perz (Lucas), Susanna Ricco (Lucas), Stasha Blank (Lucas), Vaishakh Keshava (Lucas), Deepak Sharma (Lucas), Marvin Chow (Lucas), Kunal Lad (Lucas), Komal Jalan (Lucas), Simon Osindero (Lucas), Craig Swanson (Lucas), Jacob Scott (Lucas), Anastasija Ili\'c (Lucas), Xiaowei Li (Lucas), Siddhartha Reddy Jonnalagadda (Lucas), Afzal Shama Soudagar (Lucas), Yan Xiong (Lucas), Bat-Orgil Batsaikhan (Lucas), Daniel Jarrett (Lucas), Naveen Kumar (Lucas), Maulik Shah (Lucas), Matt Lawlor (Lucas), Austin Waters (Lucas), Mark Graham (Lucas), Rhys May (Lucas), Sabela Ramos (Lucas), Sandra Lefdal (Lucas), Zeynep Cankara (Lucas), Nacho Cano (Lucas), Brendan O'Donoghue (Lucas), Jed Borovik (Lucas), Frederick Liu (Lucas), Jordan Grimstad (Lucas), Mahmoud Alnahlawi (Lucas), Katerina Tsihlas (Lucas), Tom Hudson (Lucas), Nikolai Grigorev (Lucas), Yiling Jia (Lucas), Terry Huang (Lucas), Tobenna Peter Igwe (Lucas), Sergei Lebedev (Lucas), Xiaodan Tang (Lucas), Igor Krivokon (Lucas), Frankie Garcia (Lucas), Melissa Tan (Lucas), Eric Jia (Lucas), Peter Stys (Lucas), Shikhar Vashishth (Lucas), Yu Liang (Lucas), Balaji Venkatraman (Lucas), Chenjie Gu (Lucas), Anastasios Kementsietsidis (Lucas), Chen Zhu (Lucas), Junehyuk Jung (Lucas), Yunfei Bai (Lucas), Mohammad Javad Hosseini (Lucas), Faruk Ahmed (Lucas), Aditya Gupta (Lucas), Xin Yuan (Lucas), Shereen Ashraf (Lucas), Shitij Nigam (Lucas), Gautam Vasudevan (Lucas), Pranjal Awasthi (Lucas), Adi Mayrav Gilady (Lucas), Zelda Mariet (Lucas), Ramy Eskander (Lucas), Haiguang Li (Lucas), Hexiang Hu (Lucas), Guillermo Garrido (Lucas), Philippe Schlattner (Lucas), George Zhang (Lucas), Rohun Saxena (Lucas), Petar Devi\'c (Lucas), Kritika Muralidharan (Lucas), Ashwin Murthy (Lucas), Yiqian Zhou (Lucas), Min Choi (Lucas), Arissa Wongpanich (Lucas), Zhengdong Wang (Lucas), Premal Shah (Lucas), Yuntao Xu (Lucas), Yiling Huang (Lucas), Stephen Spencer (Lucas), Alice Chen (Lucas), James Cohan (Lucas), Junjie Wang (Lucas), Jonathan Tompson (Lucas), Junru Wu (Lucas), Ruba Haroun (Lucas), Haiqiong Li (Lucas), Blanca Huergo (Lucas), Fan Yang (Lucas), Tongxin Yin (Lucas), James Wendt (Lucas), Michael Bendersky (Lucas), Rahma Chaabouni (Lucas), Javier Snaider (Lucas), Johan Ferret (Lucas), Abhishek Jindal (Lucas), Tara Thompson (Lucas), Andrew Xue (Lucas), Will Bishop (Lucas), Shubham Milind Phal (Lucas), Archit Sharma (Lucas), Yunhsuan Sung (Lucas), Prabakar Radhakrishnan (Lucas), Mo Shomrat (Lucas), Reeve Ingle (Lucas), Roopali Vij (Lucas), Justin Gilmer (Lucas), Mihai Dorin Istin (Lucas), Sam Sobell (Lucas), Yang Lu (Lucas), Emily Nottage (Lucas), Dorsa Sadigh (Lucas), Jeremiah Willcock (Lucas), Tingnan Zhang (Lucas), Steve Xu (Lucas), Sasha Brown (Lucas), Katherine Lee (Lucas), Gary Wang (Lucas), Yun Zhu (Lucas), Yi Tay (Lucas), Cheolmin Kim (Lucas), Audrey Gutierrez (Lucas), Abhanshu Sharma (Lucas), Yongqin Xian (Lucas), Sungyong Seo (Lucas), Claire Cui (Lucas), Elena Pochernina (Lucas), Cip Baetu (Lucas), Krzysztof Jastrz\k{e}bski (Lucas), Mimi Ly (Lucas), Mohamed Elhawaty (Lucas), Dan Suh (Lucas), Eren Sezener (Lucas), Pidong Wang (Lucas), Nancy Yuen (Lucas), George Tucker (Lucas), Jiahao Cai (Lucas), Zuguang Yang (Lucas), Cindy Wang (Lucas), Alex Muzio (Lucas), Hai Qian (Lucas), Jae Yoo (Lucas), Derek Lockhart (Lucas), Kevin R. McKee (Lucas), Mandy Guo (Lucas), Malika Mehrotra (Lucas), Artur Mendon\c{c}a (Lucas), Sanket Vaibhav Mehta (Lucas), Sherry Ben (Lucas), Chetan Tekur (Lucas), Jiaqi Mu (Lucas), Muye Zhu (Lucas), Victoria Krakovna (Lucas), Hongrae Lee (Lucas), AJ Maschinot (Lucas), S\'ebastien Cevey (Lucas), HyunJeong Choe (Lucas), Aijun Bai (Lucas), Hansa Srinivasan (Lucas), Derek Gasaway (Lucas), Nick Young (Lucas), Patrick Siegler (Lucas), Dan Holtmann-Rice (Lucas), Vihari Piratla (Lucas), Kate Baumli (Lucas), Roey Yogev (Lucas), Alex Hofer (Lucas), Hado van Hasselt (Lucas), Svetlana Grant (Lucas), Yuri Chervonyi (Lucas), David Silver (Lucas), Andrew Hogue (Lucas), Ayushi Agarwal (Lucas), Kathie Wang (Lucas), Preeti Singh (Lucas), Four Flynn (Lucas), Josh Lipschultz (Lucas), Robert David (Lucas), Lizzetth Bellot (Lucas), Yao-Yuan Yang (Lucas), Long Le (Lucas), Filippo Graziano (Lucas), Kate Olszewska (Lucas), Kevin Hui (Lucas), Akanksha Maurya (Lucas), Nikos Parotsidis (Lucas), Weijie Chen (Lucas), Tayo Oguntebi (Lucas), Joe Kelley (Lucas), Anirudh Baddepudi (Lucas), Johannes Mauerer (Lucas), Gregory Shaw (Lucas), Alex Siegman (Lucas), Lin Yang (Lucas), Shravya Shetty (Lucas), Subhrajit Roy (Lucas), Yunting Song (Lucas), Wojciech Stokowiec (Lucas), Ryan Burnell (Lucas), Omkar Savant (Lucas), Robert Busa-Fekete (Lucas), Jin Miao (Lucas), Samrat Ghosh (Lucas), Liam MacDermed (Lucas), Phillip Lippe (Lucas), Mikhail Dektiarev (Lucas), Zach Behrman (Lucas), Fabian Mentzer (Lucas), Kelvin Nguyen (Lucas), Meng Wei (Lucas), Siddharth Verma (Lucas), Chris Knutsen (Lucas), Sudeep Dasari (Lucas), Zhipeng Yan (Lucas), Petr Mitrichev (Lucas), Xingyu Wang (Lucas), Virat Shejwalkar (Lucas), Jacob Austin (Lucas), Srinivas Sunkara (Lucas), Navneet Potti (Lucas), Yan Virin (Lucas), Christian Wright (Lucas), Ga\"el Liu (Lucas), Oriana Riva (Lucas), Etienne Pot (Lucas), Greg Kochanski (Lucas), Quoc Le (Lucas), Gargi Balasubramaniam (Lucas), Arka Dhar (Lucas), Yuguo Liao (Lucas), Adam Bloniarz (Lucas), Divyansh Shukla (Lucas), Elizabeth Cole (Lucas), Jong Lee (Lucas), Sheng Zhang (Lucas), Sushant Kafle (Lucas), Siddharth Vashishtha (Lucas), Parsa Mahmoudieh (Lucas), Grace Chen (Lucas), Raphael Hoffmann (Lucas), Pranesh Srinivasan (Lucas), Agustin Dal Lago (Lucas), Yoav Ben Shalom (Lucas), Zi Wang (Lucas), Michael Elabd (Lucas), Anuj Sharma (Lucas), Junhyuk Oh (Lucas), Suraj Kothawade (Lucas), Maigo Le (Lucas), Marianne Monteiro (Lucas), Shentao Yang (Lucas), Kaiz Alarakyia (Lucas), Robert Geirhos (Lucas), Diana Mincu (Lucas), H\r{a}vard Garnes (Lucas), Hayato Kobayashi (Lucas), Soroosh Mariooryad (Lucas), Kacper Krasowiak (Lucas),  Zhixin (Lucas),  Lai (Elena), Shibl Mourad (Elena), Mingqiu Wang (Elena), Fan Bu (Elena), Ophir Aharoni (Elena), Guanjie Chen (Elena), Abhimanyu Goyal (Elena), Vadim Zubov (Elena), Ankur Bapna (Elena), Elahe Dabir (Elena), Nisarg Kothari (Elena), Kay Lamerigts (Elena), Nicola De Cao (Elena), Jeremy Shar (Elena), Christopher Yew (Elena), Nitish Kulkarni (Elena), Dre Mahaarachchi (Elena), Mandar Joshi (Elena), Zhenhai Zhu (Elena), Jared Lichtarge (Elena), Yichao Zhou (Elena), Hannah Muckenhirn (Elena), Vittorio Selo (Elena), Oriol Vinyals (Elena), Peter Chen (Elena), Anthony Brohan (Elena), Vaibhav Mehta (Elena), Sarah Cogan (Elena), Ruth Wang (Elena), Ty Geri (Elena), Wei-Jen Ko (Elena), Wei Chen (Elena), Fabio Viola (Elena), Keshav Shivam (Elena), Lisa Wang (Elena), Madeleine Clare Elish (Elena), Raluca Ada Popa (Elena), S\'ebastien Pereira (Elena), Jianqiao Liu (Elena), Raphael Koster (Elena), Donnie Kim (Elena), Gufeng Zhang (Elena), Sayna Ebrahimi (Elena), Partha Talukdar (Elena), Yanyan Zheng (Elena), Petra Poklukar (Elena), Ales Mikhalap (Elena), Dale Johnson (Elena), Anitha Vijayakumar (Elena), Mark Omernick (Elena), Matt Dibb (Elena), Ayush Dubey (Elena), Qiong Hu (Elena), Apurv Suman (Elena), Vaibhav Aggarwal (Elena), Ilya Kornakov (Elena), Fei Xia (Elena), Wing Lowe (Elena), Alexey Kolganov (Elena), Ted Xiao (Elena), Vitaly Nikolaev (Elena), Steven Hemingray (Elena), Bonnie Li (Elena), Joana Iljazi (Elena), Miko{\l}aj Rybi\'nski (Elena), Ballie Sandhu (Elena), Peggy Lu (Elena), Thang Luong (Elena), Rodolphe Jenatton (Elena), Vineetha Govindaraj (Elena),  Hui (Elena),  Li (Weilun), Gabriel Dulac-Arnold (Weilun), Wonpyo Park (Weilun), Henry Wang (Weilun), Abhinit Modi (Weilun), Jean Pouget-Abadie (Weilun), Kristina Greller (Weilun), Rahul Gupta (Weilun), Robert Berry (Weilun), Prajit Ramachandran (Weilun), Jinyu Xie (Weilun), Liam McCafferty (Weilun), Jianling Wang (Weilun), Kilol Gupta (Weilun), Hyeontaek Lim (Weilun), Bla\v{z} Bratani\v{c} (Weilun), Andy Brock (Weilun), Ilia Akolzin (Weilun), Jim Sproch (Weilun), Dan Karliner (Weilun), Duhyeon Kim (Weilun), Adrian Goedeckemeyer (Weilun), Noam Shazeer (Weilun), Cordelia Schmid (Weilun), Daniele Calandriello (Weilun), Parul Bhatia (Weilun), Krzysztof Choromanski (Weilun), Ceslee Montgomery (Weilun), Dheeru Dua (Weilun), Ana Ramalho (Weilun), Helen King (Weilun), Yue Gao (Weilun), Lynn Nguyen (Weilun), David Lindner (Weilun), Divya Pitta (Weilun), Oleaser Johnson (Weilun), Khalid Salama (Weilun), Diego Ardila (Weilun), Michael Han (Weilun), Erin Farnese (Weilun), Seth Odoom (Weilun), Ziyue Wang (Weilun), Xiangzhuo Ding (Weilun), Norman Rink (Weilun), Ray Smith (Weilun), Harshal Tushar Lehri (Weilun), Eden Cohen (Weilun), Neera Vats (Weilun), Tong He (Weilun), Parthasarathy Gopavarapu (Weilun), Adam Paszke (Weilun), Miteyan Patel (Weilun), Wouter Van Gansbeke (Weilun), Lucia Loher (Weilun), Luis Castro (Weilun), Maria Voitovich (Weilun), Tamara von Glehn (Weilun), Nelson George (Weilun), Simon Niklaus (Weilun), Zach Eaton-Rosen (Weilun), Nemanja Raki\'cevi\'c (Weilun), Erik Jue (Weilun), Sagi Perel (Weilun), Carrie Zhang (Weilun), Yuval Bahat (Weilun), Ang\'eline Pouget (Weilun), Zhi Xing (Weilun), Fantine Huot (Weilun), Ashish Shenoy (Weilun), Taylor Bos (Weilun), Vincent Coriou (Weilun), Bryan Richter (Weilun), Natasha Noy (Weilun), Yaqing Wang (Weilun), Santiago Ontanon (Weilun), Siyang Qin (Weilun), Gleb Makarchuk (Weilun), Demis Hassabis (Weilun), Zhuowan Li (Weilun), Mandar Sharma (Weilun), Kumaran Venkatesan (Weilun), Iurii Kemaev (Weilun), Roxanne Daniel (Weilun), Shiyu Huang (Weilun), Saloni Shah (Weilun), Octavio Ponce (Weilun),  Warren (Weilun),  Chen (Yonghao), Manaal Faruqui (Yonghao), Jialin Wu (Yonghao), Slavica Anda\v{c}i\'c (Yonghao), Szabolcs Payrits (Yonghao), Daniel McDuff (Yonghao), Tom Hume (Yonghao), Yuan Cao (Yonghao), MH Tessler (Yonghao), Qingze Wang (Yonghao), Yinan Wang (Yonghao), Ivor Rendulic (Yonghao), Eirikur Agustsson (Yonghao), Matthew Johnson (Yonghao), Tanya Lando (Yonghao), Andrew Howard (Yonghao), Sri Gayatri Sundara Padmanabhan (Yonghao), Mayank Daswani (Yonghao), Andrea Banino (Yonghao), Michael Kilgore (Yonghao), Jonathan Heek (Yonghao), Ziwei Ji (Yonghao), Alvaro Caceres (Yonghao), Conglong Li (Yonghao), Nora Kassner (Yonghao), Alexey Vlaskin (Yonghao), Zeyu Liu (Yonghao), Alex Grills (Yonghao), Yanhan Hou (Yonghao), Roykrong Sukkerd (Yonghao), Gowoon Cheon (Yonghao), Nishita Shetty (Yonghao), Larisa Markeeva (Yonghao), Piotr Stanczyk (Yonghao), Tejas Iyer (Yonghao), Yuan Gong (Yonghao), Shawn Gao (Yonghao), Keerthana Gopalakrishnan (Yonghao), Tim Blyth (Yonghao), Malcolm Reynolds (Yonghao), Avishkar Bhoopchand (Yonghao), Misha Bilenko (Yonghao), Dero Gharibian (Yonghao), Vicky Zayats (Yonghao), Aleksandra Faust (Yonghao), Abhinav Singh (Yonghao), Min Ma (Yonghao), Hongyang Jiao (Yonghao), Sudheendra Vijayanarasimhan (Yonghao), Lora Aroyo (Yonghao), Vikas Yadav (Yonghao), Sarah Chakera (Yonghao), Ashwin Kakarla (Yonghao), Vilobh Meshram (Yonghao), Karol Gregor (Yonghao), Gabriela Botea (Yonghao), Evan Senter (Yonghao), Dawei Jia (Yonghao), Geza Kovacs (Yonghao), Neha Sharma (Yonghao), Sebastien Baur (Yonghao), Kai Kang (Yonghao), Yifan He (Yonghao), Lin Zhuo (Yonghao), Marija Kostelac (Yonghao), Itay Laish (Yonghao), Songyou Peng (Yonghao), Louis O'Bryan (Yonghao), Daniel Kasenberg (Yonghao), Girish Ramchandra Rao (Yonghao), Edouard Leurent (Yonghao), Biao Zhang (Yonghao), Sage Stevens (Yonghao), Ana Salazar (Yonghao), Ye Zhang (Yonghao), Ivan Lobov (Yonghao), Jake Walker (Yonghao), Allen Porter (Yonghao), Morgan Redshaw (Yonghao), Han Ke (Yonghao), Abhishek Rao (Yonghao), Alex Lee (Yonghao), Hoi Lam (Yonghao), Michael Moffitt (Yonghao), Jaeyoun Kim (Yonghao), Siyuan Qiao (Yonghao), Terry Koo (Yonghao), Robert Dadashi (Yonghao), Xinying Song (Yonghao), Mukund Sundararajan (Yonghao), Peng Xu (Yonghao), Chizu Kawamoto (Yonghao), Yan Zhong (Yonghao), Clara Barbu (Yonghao), Apoorv Reddy (Yonghao), Mauro Verzetti (Yonghao), Leon Li (Yonghao), George Papamakarios (Yonghao), Hanna Klimczak-Pluci\'nska (Yonghao), Mary Cassin (Yonghao), Koray Kavukcuoglu (Yonghao), Rigel Swavely (Yonghao), Alain Vaucher (Yonghao), Jeffrey Zhao (Yonghao), Ross Hemsley (Yonghao), Michael Tschannen (Yonghao), Heming Ge (Yonghao), Gaurav Menghani (Yonghao), Yang Yu (Yonghao), Natalie Ha (Yonghao), Wei He (Yonghao), Xiao Wu (Yonghao), Maggie Song (Yonghao), Rachel Sterneck (Yonghao), Stefan Zinke (Yonghao), Dan A. Calian (Yonghao), Annie Marsden (Yonghao), Alejandro Cruzado Ruiz (Yonghao), Matteo Hessel (Yonghao), Almog Gueta (Yonghao), Benjamin Lee (Yonghao), Brian Farris (Yonghao), Manish Gupta (Yonghao), Yunjie Li (Yonghao), Mohammad Saleh (Yonghao), Vedant Misra (Yonghao), Kefan Xiao (Yonghao), Piermaria Mendolicchio (Yonghao), Gavin Buttimore (Yonghao), Varvara Krayvanova (Yonghao), Nigamaa Nayakanti (Yonghao), Matthew Wiethoff (Yonghao), Yash Pande (Yonghao), Azalia Mirhoseini (Yonghao), Ni Lao (Yonghao), Jasmine Liu (Yonghao), Yiqing Hua (Yonghao), Angie Chen (Yonghao), Yury Malkov (Yonghao), Dmitry Kalashnikov (Yonghao), Shubham Gupta (Yonghao), Kartik Audhkhasi (Yonghao), Yuexiang Zhai (Yonghao), Sudhindra Kopalle (Yonghao), Prateek Jain (Yonghao), Eran Ofek (Yonghao), Clemens Meyer (Yonghao), Khuslen Baatarsukh (Yonghao), Hana Strej\v{c}ek (Yonghao), Jun Qian (Yonghao), James Freedman (Yonghao), Ricardo Figueira (Yonghao), Michal Sokolik (Yonghao), Olivier Bachem (Yonghao), Raymond Lin (Yonghao), Dia Kharrat (Yonghao), Chris Hidey (Yonghao), Pingmei Xu (Yonghao), Dennis Duan (Yonghao), Yin Li (Yonghao), Muge Ersoy (Yonghao), Richard Everett (Yonghao), Kevin Cen (Yonghao), Rebeca Santamaria-Fernandez (Yonghao), Amir Taubenfeld (Yonghao), Ian Mackinnon (Yonghao), Linda Deng (Yonghao), Polina Zablotskaia (Yonghao), Shashank Viswanadha (Yonghao), Shivanker Goel (Yonghao), Damion Yates (Yonghao), Yunxiao Deng (Yonghao), Peter Choy (Yonghao), Mingqing Chen (Yonghao), Abhishek Sinha (Yonghao), Alex Mossin (Yonghao), Yiming Wang (Yonghao), Arthur Szlam (Yonghao), Susan Hao (Yonghao), Paul Kishan Rubenstein (Yonghao), Metin Toksoz-Exley (Yonghao), Miranda Aperghis (Yonghao), Yin Zhong (Yonghao), Junwhan Ahn (Yonghao), Michael Isard (Yonghao), Olivier Lacombe (Yonghao), Florian Luisier (Yonghao), Chrysovalantis Anastasiou (Yonghao), Yogesh Kalley (Yonghao), Utsav Prabhu (Yonghao), Emma Dunleavy (Yonghao), Shaan Bijwadia (Yonghao), Justin Mao-Jones (Yonghao), Kelly Chen (Yonghao), Rama Pasumarthi (Yonghao), Emily Wood (Yonghao), Adil Dostmohamed (Yonghao), Nate Hurley (Yonghao), Jiri Simsa (Yonghao), Alicia Parrish (Yonghao), Mantas Pajarskas (Yonghao), Matt Harvey (Yonghao), Ondrej Skopek (Yonghao), Yony Kochinski (Yonghao), Javier Rey (Yonghao), Verena Rieser (Yonghao), Denny Zhou (Yonghao), Sun Jae Lee (Yonghao), Trilok Acharya (Yonghao), Guowang Li (Yonghao), Joe Jiang (Yonghao), Xiaofan Zhang (Yonghao), Bryant Gipson (Yonghao), Ethan Mahintorabi (Yonghao), Marco Gelmi (Yonghao), Nima Khajehnouri (Yonghao), Angel Yeh (Yonghao), Kayi Lee (Yonghao), Loic Matthey (Yonghao), Leslie Baker (Yonghao), Trang Pham (Yonghao), Han Fu (Yonghao), Alex Pak (Yonghao), Prakhar Gupta (Yonghao), Cristina Vasconcelos (Yonghao), Adam Sadovsky (Yonghao), Brian Walker (Yonghao), Sissie Hsiao (Yonghao), Patrik Zochbauer (Yonghao), Andreea Marzoca (Yonghao), Noam Velan (Yonghao), Junhao Zeng (Yonghao), Gilles Baechler (Yonghao), Danny Driess (Yonghao), Divya Jain (Yonghao), Yanping Huang (Yonghao), Lizzie Tao (Yonghao), John Maggs (Yonghao), Nir Levine (Yonghao), Jon Schneider (Yonghao), Erika Gemzer (Yonghao), Samuel Petit (Yonghao), Shan Han (Yonghao), Zach Fisher (Yonghao), Dustin Zelle (Yonghao), Courtney Biles (Yonghao), Eugene Ie (Yonghao), Asya Fadeeva (Yonghao), Casper Liu (Yonghao), Juliana Vicente Franco (Yonghao), Adrian Collister (Yonghao), Hao Zhang (Yonghao), Renshen Wang (Yonghao), Ruizhe Zhao (Yonghao), Leandro Kieliger (Yonghao), Kurt Shuster (Yonghao), Rui Zhu (Yonghao), Boqing Gong (Yonghao), Lawrence Chan (Yonghao), Ruoxi Sun (Yonghao), Sujoy Basu (Yonghao), Roland Zimmermann (Yonghao), Jamie Hayes (Yonghao), Abhishek Bapna (Yonghao), Jasper Snoek (Yonghao), Weel Yang (Yonghao), Puranjay Datta (Yonghao), Jad Al Abdallah (Yonghao), Kevin Kilgour (Yonghao), Lu Li (Yonghao), SQ Mah (Yonghao), Yennie Jun (Yonghao), Morgane Rivi\`ere (Yonghao), Abhijit Karmarkar (Yonghao), Tammo Spalink (Yonghao), Tao Huang (Yonghao), Lucas Gonzalez (Yonghao), Duc-Hieu Tran (Yonghao), Averi Nowak (Yonghao), John Palowitch (Yonghao), Martin Chadwick (Yonghao), Ellie Talius (Yonghao), Harsh Mehta (Yonghao), Thibault Sellam (Yonghao), Philipp Fr\"anken (Yonghao), Massimo Nicosia (Yonghao), Kyle He (Yonghao), Aditya Kini (Yonghao), David Amos (Yonghao), Sugato Basu (Yonghao), Harrison Jobe (Yonghao), Eleni Shaw (Yonghao), Qiantong Xu (Yonghao), Colin Evans (Yonghao), Daisuke Ikeda (Yonghao), Chaochao Yan (Yonghao), Larry Jin (Yonghao), Lun Wang (Yonghao), Sachin Yadav (Yonghao), Ilia Labzovsky (Yonghao), Ramesh Sampath (Yonghao), Ada Ma (Yonghao), Candice Schumann (Yonghao), Aditya Siddhant (Yonghao), Rohin Shah (Yonghao), John Youssef (Yonghao), Rishabh Agarwal (Yonghao), Natalie Dabney (Yonghao), Alessio Tonioni (Yonghao), Moran Ambar (Yonghao), Jing Li (Yonghao), Isabelle Guyon (Yonghao), Benny Li (Yonghao), David Soergel (Yonghao), Boya Fang (Yonghao), Georgi Karadzhov (Yonghao), Cristian Udrescu (Yonghao), Trieu Trinh (Yonghao), Vikas Raunak (Yonghao), Seb Noury (Yonghao), Dee Guo (Yonghao), Sonal Gupta (Yonghao), Mara Finkelstein (Yonghao), Denis Petek (Yonghao), Lihao Liang (Yonghao), Greg Billock (Yonghao), Pei Sun (Yonghao), David Wood (Yonghao), Yiwen Song (Yonghao), Xiaobin Yu (Yonghao), Tatiana Matejovicova (Yonghao), Regev Cohen (Yonghao), Kalyan Andra (Yonghao), David D'Ambrosio (Yonghao), Zhiwei Deng (Yonghao), Vincent Nallatamby (Yonghao), Ebrahim Songhori (Yonghao), Rumen Dangovski (Yonghao), Andrew Lampinen (Yonghao), Pankil Botadra (Yonghao), Adam Hillier (Yonghao), Jiawei Cao (Yonghao), Nagabhushan Baddi (Yonghao), Adhi Kuncoro (Yonghao), Toshihiro Yoshino (Yonghao), Ankit Bhagatwala (Yonghao), Marc\'aurelio Ranzato (Yonghao), Rylan Schaeffer (Yonghao), Tianlin Liu (Yonghao), Shuai Ye (Yonghao), Obaid Sarvana (Yonghao), John Nham (Yonghao), Chenkai Kuang (Yonghao), Isabel Gao (Yonghao), Jinoo Baek (Yonghao), Shubham Mittal (Yonghao), Ayzaan Wahid (Yonghao), Anita Gergely (Yonghao), Bin Ni (Yonghao), Josh Feldman (Yonghao), Carrie Muir (Yonghao), Pascal Lamblin (Yonghao), Wolfgang Macherey (Yonghao), Ethan Dyer (Yonghao), Logan Kilpatrick (Yonghao), V\'ictor Campos (Yonghao), Mukul Bhutani (Yonghao), Stanislav Fort (Yonghao), Yanif Ahmad (Yonghao), Aliaksei Severyn (Yonghao), Kleopatra Chatziprimou (Yonghao), Oleksandr Ferludin (Yonghao), Mason Dimarco (Yonghao), Aditya Kusupati (Yonghao), Joe Heyward (Yonghao), Dan Bahir (Yonghao), Kevin Villela (Yonghao), Katie Millican (Yonghao), Dror Marcus (Yonghao), Sanaz Bahargam (Yonghao), Caglar Unlu (Yonghao), Nicholas Roth (Yonghao), Zichuan Wei (Yonghao), Siddharth Gopal (Yonghao), Deepanway Ghoshal (Yonghao), Edward Lee (Yonghao), Sharon Lin (Yonghao), Jennie Lees (Yonghao), Dayeong Lee (Yonghao), Anahita Hosseini (Yonghao), Connie Fan (Yonghao), Seth Neel (Yonghao), Marcus Wu (Yonghao), Yasemin Altun (Yonghao), Honglong Cai (Yonghao), Enrique Piqueras (Yonghao), Josh Woodward (Yonghao), Alessandro Bissacco (Yonghao), Salem Haykal (Yonghao), Mahyar Bordbar (Yonghao), Prasha Sundaram (Yonghao), Sarah Hodkinson (Yonghao), Daniel Toyama (Yonghao), George Polovets (Yonghao), Austin Myers (Yonghao), Anu Sinha (Yonghao), Tomer Levinboim (Yonghao), Kashyap Krishnakumar (Yonghao), Rachita Chhaparia (Yonghao), Tatiana Sholokhova (Yonghao), Nitesh Bharadwaj Gundavarapu (Yonghao), Ganesh Jawahar (Yonghao), Haroon Qureshi (Yonghao), Jieru Hu (Yonghao), Nikola Momchev (Yonghao), Matthew Rahtz (Yonghao), Renjie Wu (Yonghao), Aishwarya P S (Yonghao), Kedar Dhamdhere (Yonghao), Meiqi Guo (Yonghao), Umang Gupta (Yonghao), Ali Eslami (Yonghao), Mariano Schain (Yonghao), Michiel Blokzijl (Yonghao), David Welling (Yonghao), Dave Orr (Yonghao), Levent Bolelli (Yonghao), Nicolas Perez-Nieves (Yonghao), Mikhail Sirotenko (Yonghao), Aman Prasad (Yonghao), Arjun Kar (Yonghao), Borja De Balle Pigem (Yonghao), Tayfun Terzi (Yonghao), Gell\'ert Weisz (Yonghao), Dipankar Ghosh (Yonghao), Aditi Mavalankar (Yonghao), Dhruv Madeka (Yonghao), Kaspar Daugaard (Yonghao), Hartwig Adam (Yonghao), Viraj Shah (Yonghao), Dana Berman (Yonghao), Maggie Tran (Yonghao), Steven Baker (Yonghao), Ewa Andrejczuk (Yonghao), Grishma Chole (Yonghao), Ganna Raboshchuk (Yonghao), Mahdi Mirzazadeh (Yonghao), Thais Kagohara (Yonghao), Shimu Wu (Yonghao), Christian Schallhart (Yonghao), Bernett Orlando (Yonghao), Chen Wang (Yonghao), Alban Rrustemi (Yonghao), Hao Xiong (Yonghao), Hao Liu (Yonghao), Arpi Vezer (Yonghao), Nolan Ramsden (Yonghao), Shuo-yiin Chang (Yonghao), Sidharth Mudgal (Yonghao), Yan Li (Yonghao), Nino Vieillard (Yonghao), Yedid Hoshen (Yonghao), Farooq Ahmad (Yonghao), Ambrose Slone (Yonghao), Amy Hua (Yonghao), Natan Potikha (Yonghao), Mirko Rossini (Yonghao), Jon Stritar (Yonghao), Sushant Prakash (Yonghao), Zifeng Wang (Yonghao), Xuanyi Dong (Yonghao), Alireza Nazari (Yonghao), Efrat Nehoran (Yonghao), Kaan Tekelioglu (Yonghao), Yinxiao Li (Yonghao), Kartikeya Badola (Yonghao), Tom Funkhouser (Yonghao), Yuanzhen Li (Yonghao), Varun Yerram (Yonghao), Ramya Ganeshan (Yonghao), Daniel Formoso (Yonghao), Karol Langner (Yonghao), Tian Shi (Yonghao), Huijian Li (Yonghao), Yumeya Yamamori (Yonghao), Amayika Panda (Yonghao), Alaa Saade (Yonghao), Angelo Scorza Scarpati (Yonghao), Chris Breaux (Yonghao), CJ Carey (Yonghao), Zongwei Zhou (Yonghao), Cho-Jui Hsieh (Yonghao), Sophie Bridgers (Yonghao), Alena Butryna (Yonghao), Nishesh Gupta (Yonghao), Vaibhav Tulsyan (Yonghao), Sanghyun Woo (Yonghao), Evgenii Eltyshev (Yonghao), Will Grathwohl (Yonghao), Chanel Parks (Yonghao), Seth Benjamin (Yonghao), Rina Panigrahy (Yonghao), Shenil Dodhia (Yonghao), Daniel De Freitas (Yonghao), Chris Sauer (Yonghao), Will Song (Yonghao), Ferran Alet (Yonghao), Jackson Tolins (Yonghao), Cosmin Paduraru (Yonghao), Xingyi Zhou (Yonghao), Brian Albert (Yonghao), Zizhao Zhang (Yonghao), Lei Shu (Yonghao), Mudit Bansal (Yonghao), Sarah Nguyen (Yonghao), Amir Globerson (Yonghao), Owen Xiao (Yonghao), James Manyika (Yonghao), Tom Hennigan (Yonghao), Rong Rong (Yonghao), Josip Matak (Yonghao), Anton Bakalov (Yonghao), Ankur Sharma (Yonghao), Danila Sinopalnikov (Yonghao), Andrew Pierson (Yonghao), Stephen Roller (Yonghao), Geoff Brown (Yonghao), Mingcen Gao (Yonghao), Toshiyuki Fukuzawa (Yonghao), Amin Ghafouri (Yonghao), Kenny Vassigh (Yonghao), Iain Barr (Yonghao), Zhicheng Wang (Yonghao), Anna Korsun (Yonghao), Rajesh Jayaram (Yonghao), Lijie Ren (Yonghao), Tim Zaman (Yonghao), Samira Khan (Yonghao), Yana Lunts (Yonghao), Dan Deutsch (Yonghao), Dave Uthus (Yonghao), Nitzan Katz (Yonghao), Masha Samsikova (Yonghao), Amr Khalifa (Yonghao), Nikhil Sethi (Yonghao), Jiao Sun (Yonghao), Luming Tang (Yonghao), Uri Alon (Yonghao), Xianghong Luo (Yonghao), Dian Yu (Yonghao), Abhishek Nayyar (Yonghao), Bryce Petrini (Yonghao), Will Truong (Yonghao), Vincent Hellendoorn (Yonghao), Nikolai Chinaev (Yonghao), Chris Alberti (Yonghao), Wei Wang (Yonghao), Jingcao Hu (Yonghao), Vahab Mirrokni (Yonghao), Ananth Balashankar (Yonghao), Avia Aharon (Yonghao), Aahil Mehta (Yonghao), Ahmet Iscen (Yonghao), Joseph Kready (Yonghao), Lucas Manning (Yonghao), Anhad Mohananey (Yonghao), Yuankai Chen (Yonghao), Anshuman Tripathi (Yonghao), Allen Wu (Yonghao), Igor Petrovski (Yonghao), Dawsen Hwang (Yonghao), Martin Baeuml (Yonghao), Shreyas Chandrakaladharan (Yonghao), Yuan Liu (Yonghao), Rey Coaguila (Yonghao), Maxwell Chen (Yonghao), Sally Ma (Yonghao), Pouya Tafti (Yonghao), Susheel Tatineni (Yonghao), Terry Spitz (Yonghao), Jiayu Ye (Yonghao), Paul Vicol (Yonghao), Mihaela Rosca (Yonghao), Adri\`a Puigdom\`enech (Yonghao), Zohar Yahav (Yonghao), Sanjay Ghemawat (Yonghao), Hanzhao Lin (Yonghao), Phoebe Kirk (Yonghao), Zaid Nabulsi (Yonghao), Sergey Brin (Yonghao), Bernd Bohnet (Yonghao), Ken Caluwaerts (Yonghao), Aditya Srikanth Veerubhotla (Yonghao), Dan Zheng (Yonghao), Zihang Dai (Yonghao), Petre Petrov (Yonghao), Yichong Xu (Yonghao), Ramin Mehran (Yonghao), Zhuo Xu (Yonghao), Luisa Zintgraf (Yonghao), Jiho Choi (Yonghao), Spurthi Amba Hombaiah (Yonghao), Romal Thoppilan (Yonghao), Sashank Reddi (Yonghao), Lukasz Lew (Yonghao), Li Li (Yonghao), Kellie Webster (Yonghao), KP Sawhney (Yonghao), Lampros Lamprou (Yonghao), Siamak Shakeri (Yonghao), Mayank Lunayach (Yonghao), Jianmin Chen (Yonghao), Sumit Bagri (Yonghao), Alex Salcianu (Yonghao), Ying Chen (Yonghao), Yani Donchev (Yonghao), Charlotte Magister (Yonghao), Signe N{\o}rly (Yonghao), Vitor Rodrigues (Yonghao), Tomas Izo (Yonghao), Hila Noga (Yonghao), Joe Zou (Yonghao), Thomas K\"oppe (Yonghao), Wenxuan Zhou (Yonghao), Kenton Lee (Yonghao), Xiangzhu Long (Yonghao), Danielle Eisenbud (Yonghao), Anthony Chen (Yonghao), Connor Schenck (Yonghao), Chi Ming To (Yonghao), Peilin Zhong (Yonghao), Emanuel Taropa (Yonghao), Minh Truong (Yonghao), Omer Levy (Yonghao), Danilo Martins (Yonghao), Zhiyuan Zhang (Yonghao), Christopher Semturs (Yonghao), Kelvin Zhang (Yonghao), Alex Yakubovich (Yonghao), Pol Moreno (Yonghao), Lara McConnaughey (Yonghao), Di Lu (Yonghao), Sam Redmond (Yonghao), Lotte Weerts (Yonghao), Yonatan Bitton (Yonghao), Tiziana Refice (Yonghao), Nicolas Lacasse (Yonghao), Arthur Conmy (Yonghao), Corentin Tallec (Yonghao), Julian Odell (Yonghao), Hannah Forbes-Pollard (Yonghao), Arkadiusz Socala (Yonghao), Jonathan Hoech (Yonghao), Pushmeet Kohli (Yonghao), Alanna Walton (Yonghao), Rui Wang (Yonghao), Mikita Sazanovich (Yonghao), Kexin Zhu (Yonghao), Andrei Kapishnikov (Yonghao), Rich Galt (Yonghao), Matthew Denton (Yonghao), Ben Murdoch (Yonghao), Caitlin Sikora (Yonghao), Kareem Mohamed (Yonghao), Wei Wei (Yonghao), Uri First (Yonghao), Tim McConnell (Yonghao), Luis C. Cobo (Yonghao), James Qin (Yonghao), Thi Avrahami (Yonghao), Daniel Balle (Yonghao), Yu Watanabe (Yonghao), Annie Louis (Yonghao), Adam Kraft (Yonghao), Setareh Ariafar (Yonghao), Yiming Gu (Yonghao), Eug\'enie Rives (Yonghao), Charles Yoon (Yonghao), Andrei Rusu (Yonghao), James Cobon-Kerr (Yonghao), Chris Hahn (Yonghao), Jiaming Luo (Yonghao),  Yuvein (Yonghao),  Zhu, Niharika Ahuja, Rodrigo Benenson, Rapha\"el Lopez Kaufman, Honglin Yu, Lloyd Hightower, Junlin Zhang, Darren Ni, Lisa Anne Hendricks, Gabby Wang, Gal Yona, Lalit Jain, Pablo Barrio, Surya Bhupatiraju, Siva Velusamy, Allan Dafoe, Sebastian Riedel, Tara Thomas, Zhe Yuan, Mathias Bellaiche, Sheena Panthaplackel, Klemen Kloboves, Sarthak Jauhari, Canfer Akbulut, Todor Davchev, Evgeny Gladchenko, David Madras, Aleksandr Chuklin, Tyrone Hill, Quan Yuan, Mukundan Madhavan, Luke Leonhard, Dylan Scandinaro, Qihang Chen, Ning Niu, Arthur Douillard, Bogdan Damoc, Yasumasa Onoe, Fabian Pedregosa, Fred Bertsch, Chas Leichner, Joseph Pagadora, Jonathan Malmaud, Sameera Ponda, Andy Twigg, Oleksii Duzhyi, Jingwei Shen, Miaosen Wang, Roopal Garg, Jing Chen, Utku Evci, Jonathan Lee, Leon Liu, Koji Kojima, Masa Yamaguchi, Arunkumar Rajendran, AJ Piergiovanni, Vinodh Kumar Rajendran, Marco Fornoni, Gabriel Ibagon, Harry Ragan, Sadh MNM Khan, John Blitzer, Andrew Bunner, Guan Sun, Takahiro Kosakai, Scott Lundberg, Ndidi Elue, Kelvin Guu, SK Park, Jane Park, Arunachalam Narayanaswamy, Chengda Wu, Jayaram Mudigonda, Trevor Cohn, Hairong Mu, Ravi Kumar, Laura Graesser, Yichi Zhang, Richard Killam, Vincent Zhuang, Mai Gim\'enez, Wael Al Jishi, Ruy Ley-Wild, Alex Zhai, Kazuki Osawa, Diego Cedillo, Jialu Liu, Mayank Upadhyay, Marcin Sieniek, Roshan Sharma, Tom Paine, Anelia Angelova, Sravanti Addepalli, Carolina Parada, Kingshuk Majumder, Avery Lamp, Sanjiv Kumar, Xiang Deng, Artiom Myaskovsky, Tea Saboli\'c, Jeffrey Dudek, Sarah York, F\'elix de Chaumont Quitry, Jiazhong Nie, Dee Cattle, Alok Gunjan, Bilal Piot, Waleed Khawaja, Seojin Bang, Simon Wang, Siavash Khodadadeh, Raghavender R, Praynaa Rawlani, Richard Powell, Kevin Lee, Johannes Griesser, GS Oh, Cesar Magalhaes, Yujia Li, Simon Tokumine, Hadas Natalie Vogel, Dennis Hsu, Arturo BC, Disha Jindal, Matan Cohen, Zi Yang, Junwei Yuan, Dario de Cesare, Tony Bruguier, Jun Xu, Monica Roy, Alon Jacovi, Dan Belov, Rahul Arya, Phoenix Meadowlark, Shlomi Cohen-Ganor, Wenting Ye, Patrick Morris-Suzuki, Praseem Banzal, Gan Song, Pranavaraj Ponnuramu, Fred Zhang, George Scrivener, Salah Zaiem, Alif Raditya Rochman, Kehang Han, Badih Ghazi, Kate Lee, Shahar Drath, Daniel Suo, Antonious Girgis, Pradeep Shenoy, Duy Nguyen, Douglas Eck, Somit Gupta, Le Yan, Joao Carreira, Anmol Gulati, Ruoxin Sang, Daniil Mirylenka, Emma Cooney, Edward Chou, Mingyang Ling, Cindy Fan, Ben Coleman, Guilherme Tubone, Ravin Kumar, Jason Baldridge, Felix Hernandez-Campos, Angeliki Lazaridou, James Besley, Itay Yona, Neslihan Bulut, Quentin Wellens, AJ Pierigiovanni, Jasmine George, Richard Green, Pu Han, Connie Tao, Geoff Clark, Chong You, Abbas Abdolmaleki, Justin Fu, Tongzhou Chen, Ashwin Chaugule, Angad Chandorkar, Altaf Rahman, Will Thompson, Penporn Koanantakool, Mike Bernico, Jie Ren, Andrey Vlasov, Sergei Vassilvitskii, Maciej Kula, Yizhong Liang, Dahun Kim, Yangsibo Huang, Chengxi Ye, Dmitry Lepikhin, Wesley Helmholz</dc:creator>
    </item>
    <item>
      <title>Prompt Perturbations Reveal Human-Like Biases in Large Language Model Survey Responses</title>
      <link>https://arxiv.org/abs/2507.07188</link>
      <description>arXiv:2507.07188v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are increasingly used as proxies for human subjects in social science surveys, but their reliability and susceptibility to known human-like response biases, such as central tendency, opinion floating and primacy bias are poorly understood. This work investigates the response robustness of LLMs in normative survey contexts, we test nine LLMs on questions from the World Values Survey (WVS), applying a comprehensive set of ten perturbations to both question phrasing and answer option structure, resulting in over 167,000 simulated survey interviews. In doing so, we not only reveal LLMs' vulnerabilities to perturbations but also show that all tested models exhibit a consistent recency bias, disproportionately favoring the last-presented answer option. While larger models are generally more robust, all models remain sensitive to semantic variations like paraphrasing and to combined perturbations. This underscores the critical importance of prompt design and robustness testing when using LLMs to generate synthetic survey data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07188v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jens Rupprecht, Georg Ahnert, Markus Strohmaier</dc:creator>
    </item>
    <item>
      <title>Why is Your Language Model a Poor Implicit Reward Model?</title>
      <link>https://arxiv.org/abs/2507.07981</link>
      <description>arXiv:2507.07981v2 Announce Type: replace-cross 
Abstract: Reward models are key to language model post-training and inference pipelines. Conveniently, recent work showed that every language model defines an implicit reward model (IM-RM), without requiring any architectural changes. However, such IM-RMs tend to generalize worse, especially out-of-distribution, compared to explicit reward models (EX-RMs) that apply a dedicated linear head over the hidden representations of a language model. The existence of a generalization gap is puzzling, as EX-RMs and IM-RMs are nearly identical. They can be trained using the same data, loss function, and language model, and differ only in how the reward is computed. Toward a fundamental understanding of the implicit biases underlying different reward model types, we investigate the root cause of this gap. Our main finding, backed by theory and experiments, is that IM-RMs rely more heavily on superficial token-level cues. Consequently, they often generalize worse than EX-RMs under token-level distribution shifts, as well as in-distribution. Furthermore, we provide evidence against alternative hypotheses for the generalization gap. Most notably, we challenge the intuitive claim that IM-RMs struggle in tasks where generation is harder than verification because they can operate both as a verifier and a generator. Taken together, our results highlight that seemingly minor design choices can substantially impact the generalization behavior of reward models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07981v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Noam Razin, Yong Lin, Jiarui Yao, Sanjeev Arora</dc:creator>
    </item>
    <item>
      <title>HANS-Net: Hyperbolic Convolution and Adaptive Temporal Attention for Accurate and Generalizable Liver and Tumor Segmentation in CT Imaging</title>
      <link>https://arxiv.org/abs/2507.11325</link>
      <description>arXiv:2507.11325v2 Announce Type: replace-cross 
Abstract: Accurate liver and tumor segmentation on abdominal CT images is critical for reliable diagnosis and treatment planning, but remains challenging due to complex anatomical structures, variability in tumor appearance, and limited annotated data. To address these issues, we introduce Hyperbolic-convolutions Adaptive-temporal-attention with Neural-representation and Synaptic-plasticity Network (HANS-Net), a novel segmentation framework that synergistically combines hyperbolic convolutions for hierarchical geometric representation, a wavelet-inspired decomposition module for multi-scale texture learning, a biologically motivated synaptic plasticity mechanism for adaptive feature enhancement, and an implicit neural representation branch to model fine-grained and continuous anatomical boundaries. Additionally, we incorporate uncertainty-aware Monte Carlo dropout to quantify prediction confidence and lightweight temporal attention to improve inter-slice consistency without sacrificing efficiency. Extensive evaluations of the LiTS dataset demonstrate that HANS-Net achieves a mean Dice score of 93.26%, an IoU of 88.09%, an average symmetric surface distance (ASSD) of 0.72 mm, and a volume overlap error (VOE) of 11.91%. Furthermore, cross-dataset validation on the AMOS 2022 dataset obtains an average Dice of 85.09%, IoU of 76.66%, ASSD of 19.49 mm, and VOE of 23.34%, indicating strong generalization across different datasets. These results confirm the effectiveness and robustness of HANS-Net in providing anatomically consistent, accurate, and confident liver and tumor segmentation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11325v2</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arefin Ittesafun Abian, Ripon Kumar Debnath, Md. Abdur Rahman, Mohaimenul Azam Khan Raiaan, Md Rafiqul Islam, Asif Karim, Reem E. Mohamed, Sami Azam</dc:creator>
    </item>
    <item>
      <title>Causal Language Control in Multilingual Transformers via Sparse Feature Steering</title>
      <link>https://arxiv.org/abs/2507.13410</link>
      <description>arXiv:2507.13410v2 Announce Type: replace-cross 
Abstract: Deterministically controlling the target generation language of large multilingual language models (LLMs) remains a fundamental challenge, particularly in zero-shot settings where neither explicit language prompts nor fine-tuning are available. In this work, we investigate whether sparse autoencoder (SAE) features, previously shown to correlate with interpretable model behaviors, can be leveraged to steer the generated language of LLMs during inference. Leveraging pretrained SAEs on the residual streams of Gemma-2B and Gemma-9B, we identify features whose activations differ most significantly between English and four target languages: Chinese, Japanese, Spanish, and French. By modifying just a single SAE feature at one transformer layer, we achieve controlled language shifts with up to 90\% success, as measured by FastText language classification, while preserving semantic fidelity according to LaBSE (Language-Agnostic BERT Sentence Embedding) similarity. Our analysis reveals that language steering is most effective in mid-to-late transformer layers and is amplified by specific attention heads disproportionately associated with language-sensitive SAE features. These results demonstrate the promise of sparse feature steering as a lightweight and interpretable mechanism for controllable multilingual generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13410v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cheng-Ting Chou, George Liu, Jessica Sun, Cole Blondin, Kevin Zhu, Vasu Sharma, Sean O'Brien</dc:creator>
    </item>
    <item>
      <title>Innovator: Scientific Continued Pretraining with Fine-grained MoE Upcycling</title>
      <link>https://arxiv.org/abs/2507.18671</link>
      <description>arXiv:2507.18671v2 Announce Type: replace-cross 
Abstract: A large language model (LLM) with knowledge in both scientific and general tasks is the foundation of science general intelligence. However, directly continued pretraining an LLM using science data usually leads to catastrophic forgetting, which indicates severe degradation in general ability. In this report, we present Innovator, which solves this problem by upcycling a pre-trained dense LLM into a fine-grained Mixtures-of-Experts model during continued pretraining, where different experts are expected to learn science knowledge in different disciplines, and a shared expert is utilized for general tasks. Innovator introduces a four-stage upcycle training paradigm: (1) Scientific Expert Induction on discipline-specific data, (2) Fine-grained Expert Splitting via FFN dimension decomposition, (3) Science-Aware Routing warmup, and (4) Generalist-Scientist Integration training on hybrid datasets. Such a paradigm enables knowledge in the general domain, and different scientific disciplines can be decoupled, avoiding the negative influence among knowledge in different domains. With 53.3B total parameters and 13.3B activated, Innovator extends Qwen2.5-7B using a shared general expert and 64 specialized scientific experts with 8 activated. Trained on 300B tokens with tri-level quality-controlled data, Innovator achieves 25% average improvement across 30 scientific tasks with a win rate as 70%, while retaining 99% performance in general tasks. Furthermore, Innovator-Reason, which is post-trained from Innovator for reasoning boosting, exhibits excellent reasoning performance in solving complex scientific problems with improvements over 30%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18671v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ning Liao, Xiaoxing Wang, Zehao Lin, Weiyang Guo, Feng Hong, Shixiang Song, Geng Yu, Zihua Zhao, Sitao Xie, Longxuan Wei, Xiangqi Jin, Xiaohan Qin, Jiale Ma, Kai Chen, Jiangchao Yao, Zhouhan Lin, Junchi Yan, Zhiyu Li, Feiyu Xiong, Yanfeng Wang, Linfeng Zhang</dc:creator>
    </item>
    <item>
      <title>Analysis of Hyperparameter Optimization Effects on Lightweight Deep Models for Real-Time Image Classification</title>
      <link>https://arxiv.org/abs/2507.23315</link>
      <description>arXiv:2507.23315v2 Announce Type: replace-cross 
Abstract: Lightweight convolutional and transformer-based networks are increasingly preferred for real-time image classification, especially on resource-constrained devices. This study evaluates the impact of hyperparameter optimization on the accuracy and deployment feasibility of seven modern lightweight architectures: ConvNeXt-T, EfficientNetV2-S, MobileNetV3-L, MobileViT v2 (S/XS), RepVGG-A2, and TinyViT-21M, trained on a class-balanced subset of 90,000 images from ImageNet-1K. Under standardized training settings, this paper investigates the influence of learning rate schedules, augmentation, optimizers, and initialization on model performance. Inference benchmarks are performed using an NVIDIA L40s GPU with batch sizes ranging from 1 to 512, capturing latency and throughput in real-time conditions. This work demonstrates that controlled hyperparameter variation significantly alters convergence dynamics in lightweight CNN and transformer backbones, providing insight into stability regions and deployment feasibility in edge artificial intelligence. Our results reveal that tuning alone leads to a top-1 accuracy improvement of 1.5 to 3.5 percent over baselines, and select models (e.g., RepVGG-A2, MobileNetV3-L) deliver latency under 5 milliseconds and over 9,800 frames per second, making them ideal for edge deployment. This work provides reproducible, subset-based insights into lightweight hyperparameter tuning and its role in balancing speed and accuracy. The code and logs may be seen at: https://vineetkumarrakesh.github.io/lcnn-opt</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23315v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Vineet Kumar Rakesh, Soumya Mazumdar, Tapas Samanta, Hemendra Kumar Pandey, Amitabha Das</dc:creator>
    </item>
    <item>
      <title>LLMs are Single-threaded Reasoners: Demystifying the Working Mechanism of Soft Thinking</title>
      <link>https://arxiv.org/abs/2508.03440</link>
      <description>arXiv:2508.03440v4 Announce Type: replace-cross 
Abstract: Human cognition naturally engages with abstract and fluid concepts, whereas existing reasoning models often rely on generating discrete tokens, potentially constraining their expressive capabilities. Recent advancements aim to address this limitation by enabling large language models (LLMs) to generate soft, abstract tokens, thus facilitating reasoning within a continuous concept space. In this paper, we investigate the Soft Thinking capabilities of various LLMs through a systematic analysis of their internal behavior using a suite of probing techniques. Contrary to the prevailing belief that Soft Thinking supports parallel exploration of diverse reasoning paths, our findings reveal that LLMs behave as single-threaded reasoners--they predominantly rely on the token with the highest probability in the soft input to predict the next step. This behavior induces a greedy feedback loop that suppresses alternative reasoning paths and undermines the benefits of transmitting richer information via Soft Tokens. To address this Greedy Pitfall, we propose Stochastic Soft Thinking, which introduces stochasticity to break free from this Greedy Pitfall. Our experiments demonstrate that incorporating randomness--particularly with the Gumbel-Softmax trick--can alleviate the limitations of vanilla approaches and unleash the potential of Soft Thinking, resulting in superior performance across eight reasoning benchmarks. We further demonstrate that Stochastic Soft Thinking exhibits stronger exploration potential compared to conventional COT. Our findings deepen the understanding of continuous reasoning and establish the foundation for future work on improving Soft Thinking with Reinforcement Learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03440v4</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junhong Wu, Jinliang Lu, Zixuan Ren, Gangqiang Hu, Zhi Wu, Dai Dai, Hua Wu</dc:creator>
    </item>
    <item>
      <title>PETLP: A Privacy-by-Design Pipeline for Social Media Data in AI Research</title>
      <link>https://arxiv.org/abs/2508.09232</link>
      <description>arXiv:2508.09232v2 Announce Type: replace-cross 
Abstract: Social media data presents AI researchers with overlapping obligations under the GDPR, copyright law, and platform terms -- yet existing frameworks fail to integrate these regulatory domains, leaving researchers without unified guidance. We introduce PETLP (Privacy-by-design Extract, Transform, Load, and Present), a compliance framework that embeds legal safeguards directly into extended ETL pipelines. Central to PETLP is treating Data Protection Impact Assessments as living documents that evolve from pre-registration through dissemination. Through systematic Reddit analysis, we demonstrate how extraction rights fundamentally differ between qualifying research organisations (who can invoke DSM Article 3 to override platform restrictions) and commercial entities (bound by terms of service), whilst GDPR obligations apply universally. We demonstrate why true anonymisation remains unachievable for social media data and expose the legal gap between permitted dataset creation and uncertain model distribution. By structuring compliance decisions into practical workflows and simplifying institutional data management plans, PETLP enables researchers to navigate regulatory complexity with confidence, bridging the gap between legal requirements and research practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09232v2</guid>
      <category>cs.MM</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nick Oh, Giorgos D. Vrakas, Si\^an J. M. Brooke, Sasha Morini\`ere, Toju Duke</dc:creator>
    </item>
    <item>
      <title>Large Language Models Enable Design of Personalized Nudges across Cultures</title>
      <link>https://arxiv.org/abs/2508.12045</link>
      <description>arXiv:2508.12045v2 Announce Type: replace-cross 
Abstract: Nudge strategies are effective tools for influencing behaviour, but their impact depends on individual preferences. Strategies that work for some individuals may be counterproductive for others. We hypothesize that large language models (LLMs) can facilitate the design of individual-specific nudges without the need for costly and time-intensive behavioural data collection and modelling. To test this, we use LLMs to design personalized decoy-based nudges tailored to individual profiles and cultural contexts, aimed at encouraging air travellers to voluntarily offset CO$_2$ emissions from flights. We evaluate their effectiveness through a large-scale survey experiment ($n=3495$) conducted across five countries. Results show that LLM-informed personalized nudges are more effective than uniform settings, raising offsetting rates by 3-7$\%$ in Germany, Singapore, and the US, though not in China or India. Our study highlights the potential of LLM as a low-cost testbed for piloting nudge strategies. At the same time, cultural heterogeneity constrains their generalizability underscoring the need for combining LLM-based simulations with targeted empirical validation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12045v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Vladimir Maksimenko, Qingyao Xin, Prateek Gupta, Bin Zhang, Prateek Bansal</dc:creator>
    </item>
    <item>
      <title>Rethinking Purity and Diversity in Multi-Behavior Sequential Recommendation from the Frequency Perspective</title>
      <link>https://arxiv.org/abs/2508.20427</link>
      <description>arXiv:2508.20427v2 Announce Type: replace-cross 
Abstract: In recommendation systems, users often exhibit multiple behaviors, such as browsing, clicking, and purchasing. Multi-behavior sequential recommendation (MBSR) aims to consider these different behaviors in an integrated manner to improve the recommendation performance of the target behavior. However, some behavior data will also bring inevitable noise to the modeling of user interests. Some research efforts focus on data denoising from the frequency domain perspective to improve the accuracy of user preference prediction. These studies indicate that low-frequency information tends to be valuable and reliable, while high-frequency information is often associated with noise. In this paper, we argue that high-frequency information is by no means insignificant. Further experimental results highlight that low frequency corresponds to the purity of user interests, while high frequency corresponds to the diversity of user interests. Building upon this finding, we proposed our model PDB4Rec, which efficiently extracts information across various frequency bands and their relationships, and introduces Boostrapping Balancer mechanism to balance their contributions for improved recommendation performance. Sufficient experiments on real-world datasets demonstrate the effectiveness and efficiency of our model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20427v2</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yongqiang Han, Kai Cheng, Kefan Wang, Enhong Chen</dc:creator>
    </item>
    <item>
      <title>ECG-Soup: Harnessing Multi-Layer Synergy for ECG Foundation Models</title>
      <link>https://arxiv.org/abs/2509.00102</link>
      <description>arXiv:2509.00102v2 Announce Type: replace-cross 
Abstract: Transformer-based foundation models for Electrocardiograms (ECGs) have recently achieved impressive performance in many downstream applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00102v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Phu X. Nguyen, Huy Phan, Hieu Pham, Christos Chatzichristos, Bert Vandenberk, Maarten De Vos</dc:creator>
    </item>
    <item>
      <title>Merge-of-Thought Distillation</title>
      <link>https://arxiv.org/abs/2509.08814</link>
      <description>arXiv:2509.08814v3 Announce Type: replace-cross 
Abstract: Efficient reasoning distillation for long chain-of-thought (CoT) models is increasingly constrained by the assumption of a single oracle teacher, despite the practical availability of multiple candidate teachers and growing CoT corpora. We revisit teacher selection and observe that different students have different "best teachers," and even for the same student, the best teacher can vary across datasets. Therefore, to unify multiple teachers' reasoning abilities into a student to overcome conflicts among various teachers' supervision, we propose Merge-of-Thought Distillation (MoT), a lightweight framework that alternates between teacher-specific supervised fine-tuning branches and weight-space merging of the resulting student variants. On competition math benchmarks, using only about 200 CoT samples, applying MoT to a Qwen3-14B student surpasses strong models including Deepseek-R1, Qwen3-32B, and OpenAI-O1, demonstrating substantial gains. Besides, MoT consistently outperforms the best single-teacher distillation, improves general reasoning beyond mathematics while reducing catastrophic forgetting, and shows robustness to distribution-shifted and peer-level teachers. Finally, we have demonstrated MoT possesses consensus CoT by eliminating teacher-specific inductive biases and inter-teacher conflicts while repeatedly reinforcing the learning of consensus reasoning features. These results position MoT as a simple, effective route to efficiently distilling long CoT capabilities from diverse teachers into compact students.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08814v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhanming Shen, Zeyu Qin, Zenan Huang, Hao Chen, Jiaqi Hu, Yihong Zhuang, Guoshan Lu, Gang Chen, Junbo Zhao</dc:creator>
    </item>
    <item>
      <title>MarkDiffusion: An Open-Source Toolkit for Generative Watermarking of Latent Diffusion Models</title>
      <link>https://arxiv.org/abs/2509.10569</link>
      <description>arXiv:2509.10569v2 Announce Type: replace-cross 
Abstract: We introduce MarkDiffusion, an open-source Python toolkit for generative watermarking of latent diffusion models. It comprises three key components: a unified implementation framework for streamlined watermarking algorithm integrations and user-friendly interfaces; a mechanism visualization suite that intuitively showcases added and extracted watermark patterns to aid public understanding; and a comprehensive evaluation module offering standard implementations of 24 tools across three essential aspects - detectability, robustness, and output quality - plus 8 automated evaluation pipelines. Through MarkDiffusion, we seek to assist researchers, enhance public awareness and engagement in generative watermarking, and promote consensus while advancing research and applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10569v2</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.MM</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leyi Pan, Sheng Guan, Zheyu Fu, Luyang Si, Huan Wang, Zian Wang, Hanqian Li, Xuming Hu, Irwin King, Philip S. Yu, Aiwei Liu, Lijie Wen</dc:creator>
    </item>
    <item>
      <title>Preservation of Language Understanding Capabilities in Speech-aware Large Language Models</title>
      <link>https://arxiv.org/abs/2509.12171</link>
      <description>arXiv:2509.12171v2 Announce Type: replace-cross 
Abstract: The paper presents C3T (Cross-modal Capabilities Conservation Test), a new benchmark for assessing the performance of speech-aware large language models. The benchmark utilizes textual tasks and a voice cloning text-to-speech model to quantify the extent to which language understanding capabilities are preserved when the model is accessed via speech input. C3T quantifies the fairness of the model for different categories of speakers and its robustness across text and speech modalities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12171v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marek Kubis, Pawe{\l} Sk\'orzewski, Iwona Christop, Mateusz Czy\.znikiewicz, Jakub Kubiak, {\L}ukasz Bondaruk, Marcin Lewandowski</dc:creator>
    </item>
    <item>
      <title>EdiVal-Agent: An Object-Centric Framework for Automated, Fine-Grained Evaluation of Multi-Turn Editing</title>
      <link>https://arxiv.org/abs/2509.13399</link>
      <description>arXiv:2509.13399v2 Announce Type: replace-cross 
Abstract: Instruction-based image editing has advanced rapidly, yet reliable and interpretable evaluation remains a bottleneck. Current protocols either (i) depend on paired reference images-resulting in limited coverage and inheriting biases from prior generative models-or (ii) rely solely on zero-shot vision-language models (VLMs), whose prompt-based assessments of instruction following, content consistency, and visual quality are often imprecise. To address this, we introduce EdiVal-Agent, an automated and fine-grained evaluation framework grounded in an object-centric perspective, designed to assess not only standard single-turn but also multi-turn instruction-based editing with precision. Given an input image, EdiVal-Agent first decomposes it into semantically meaningful objects, then synthesizes diverse, context-aware editing instructions while dynamically updating object pools across turns. These two stages enable two novel object-centric metrics tailored for multi-turn evaluation and one global metric of visual quality: (1) EdiVal-IF, which measures instruction following by combining open-vocabulary object detectors for symbolic checks with VLMs for semantic verification on detector-guided crops; (2) EdiVal-CC, which evaluates content consistency by calculating semantic similarity of unchanged objects and background using the evolving object pools; and (3) EdiVal-VQ, which quantifies changes in overall visual quality with human preference models. Instantiating this pipeline, we build EdiVal-Bench, a multi-turn editing benchmark covering 9 instruction types and 13 state-of-the-art editing models spanning in-context, flow-matching, and diffusion paradigms. We demonstrate that EdiVal-Agent can be used to identify existing failure modes, thereby informing the development of the next generation of editing models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13399v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianyu Chen, Yasi Zhang, Zhi Zhang, Peiyu Yu, Shu Wang, Zhendong Wang, Kevin Lin, Xiaofei Wang, Zhengyuan Yang, Linjie Li, Chung-Ching Lin, Jianwen Xie, Oscar Leong, Lijuan Wang, Ying Nian Wu, Mingyuan Zhou</dc:creator>
    </item>
    <item>
      <title>Socratic Mind: Impact of a Novel GenAI-Powered Assessment Tool on Student Learning and Higher-Order Thinking</title>
      <link>https://arxiv.org/abs/2509.16262</link>
      <description>arXiv:2509.16262v2 Announce Type: replace-cross 
Abstract: This study examines the impact of Socratic Mind, a Generative Artificial Intelligence (GenAI) powered formative assessment tool that employs Socratic questioning to support student learning in a large, fully online undergraduate-level computing course. Employing a quasi-experimental, mixed-methods design, we investigated participants' engagement patterns, the influence of user experience on engagement, and impacts on both perceived and actual learning outcomes. Data were collected from the system logs, surveys on user experience and perceived engagement and learning gains, student reflections, and course performance data. Results indicated that participants consistently reported high levels of affective, behavioral, and cognitive engagement, and these were strongly linked to positive user experiences and perceived learning outcomes. Quantitative analysis further revealed that students who engaged with the GenAI tool experienced significant gains in their quiz scores compared to those who did not, particularly benefiting students with lower baseline achievement. Additionally, thematic analysis of qualitative feedback revealed substantial perceived improvements in higher-order thinking skills, including problem solving, critical thinking, and self-reflection. Our findings highlight the promise of AI-mediated dialogue in fostering deeper engagement and higher-order cognitive skills. As higher education institutions expand GenAI integration in curriculum, this dialogic, GenAI powered assessment tool can offer a scalable strategy to promote students' meaningful learning outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16262v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jeonghyun Lee, Jui-Tse Hung, Meryem Yilmaz Soylu, Diana Popescu, Christopher Zhang Cui, Gayane Grigoryan, David A Joyner, Stephen W Harmon</dc:creator>
    </item>
    <item>
      <title>From Easy to Hard: The MIR Benchmark for Progressive Interleaved Multi-Image Reasoning</title>
      <link>https://arxiv.org/abs/2509.17040</link>
      <description>arXiv:2509.17040v2 Announce Type: replace-cross 
Abstract: Multi-image Interleaved Reasoning aims to improve Multi-modal Large Language Models (MLLMs) ability to jointly comprehend and reason across multiple images and their associated textual contexts, introducing unique challenges beyond single-image or non-interleaved multi-image tasks. While current multi-image benchmarks overlook interleaved textual contexts and neglect distinct relationships between individual images and their associated texts, enabling models to reason over multi-image interleaved data may significantly enhance their comprehension of complex scenes and better capture cross-modal correlations. To bridge this gap, we introduce a novel benchmark MIR, requiring joint reasoning over multiple images accompanied by interleaved textual contexts to accurately associate image regions with corresponding texts and logically connect information across images. To enhance MLLMs ability to comprehend multi-image interleaved data, we introduce reasoning steps for each instance within the benchmark and propose a stage-wise curriculum learning strategy. This strategy follows an "easy to hard" approach, progressively guiding models from simple to complex scenarios, thereby enhancing their ability to handle challenging tasks. Extensive experiments benchmarking multiple MLLMs demonstrate that our method significantly enhances models reasoning performance on MIR and other established benchmarks. We believe that MIR will encourage further research into multi-image interleaved reasoning, facilitating advancements in MLLMs capability to handle complex inter-modal tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17040v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hang Du, Jiayang Zhang, Guoshun Nan, Wendi Deng, Zhenyan Chen, Chenyang Zhang, Wang Xiao, Shan Huang, Yuqi Pan, Tao Qi, Sicong Leng</dc:creator>
    </item>
    <item>
      <title>Chiplet-Based RISC-V SoC with Modular AI Acceleration</title>
      <link>https://arxiv.org/abs/2509.18355</link>
      <description>arXiv:2509.18355v2 Announce Type: replace-cross 
Abstract: Achieving high performance, energy efficiency, and cost-effectiveness while maintaining architectural flexibility is a critical challenge in the development and deployment of edge AI devices. Monolithic SoC designs struggle with this complex balance mainly due to low manufacturing yields (below 16%) at advanced 360 mm^2 process nodes. This paper presents a novel chiplet-based RISC-V SoC architecture that addresses these limitations through modular AI acceleration and intelligent system level optimization. Our proposed design integrates 4 different key innovations in a 30mm x 30mm silicon interposer: adaptive cross-chiplet Dynamic Voltage and Frequency Scaling (DVFS); AI-aware Universal Chiplet Interconnect Express (UCIe) protocol extensions featuring streaming flow control units and compression-aware transfers; distributed cryptographic security across heterogeneous chiplets; and intelligent sensor-driven load migration. The proposed architecture integrates a 7nm RISC-V CPU chiplet with dual 5nm AI accelerators (15 TOPS INT8 each), 16GB HBM3 memory stacks, and dedicated power management controllers. Experimental results across industry standard benchmarks like MobileNetV2, ResNet-50 and real-time video processing demonstrate significant performance improvements. The AI-optimized configuration achieves ~14.7% latency reduction, 17.3% throughput improvement, and 16.2% power reduction compared to previous basic chiplet implementations. These improvements collectively translate to a 40.1% efficiency gain corresponding to ~3.5 mJ per MobileNetV2 inference (860 mW/244 images/s), while maintaining sub-5ms real-time capability across all experimented workloads. These performance upgrades demonstrate that modular chiplet designs can achieve near-monolithic computational density while enabling cost efficiency, scalability and upgradeability, crucial for next-generation edge AI device applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18355v2</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>P. Ramkumar, S. S. Bharadwaj</dc:creator>
    </item>
    <item>
      <title>On Theoretical Interpretations of Concept-Based In-Context Learning</title>
      <link>https://arxiv.org/abs/2509.20882</link>
      <description>arXiv:2509.20882v2 Announce Type: replace-cross 
Abstract: In-Context Learning (ICL) has emerged as an important new paradigm in natural language processing and large language model (LLM) applications. However, the theoretical understanding of the ICL mechanism remains limited. This paper aims to investigate this issue by studying a particular ICL approach, called concept-based ICL (CB-ICL). In particular, we propose theoretical analyses on applying CB-ICL to ICL tasks, which explains why and when the CB-ICL performs well for predicting query labels in prompts with only a few demonstrations. In addition, the proposed theory quantifies the knowledge that can be leveraged by the LLMs to the prompt tasks, and leads to a similarity measure between the prompt demonstrations and the query input, which provides important insights and guidance for model pre-training and prompt engineering in ICL. Moreover, the impact of the prompt demonstration size and the dimension of the LLM embeddings in ICL are also explored based on the proposed theory. Finally, several real-data experiments are conducted to validate the practical usefulness of CB-ICL and the corresponding theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20882v2</guid>
      <category>cs.IT</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>math.IT</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Huaze Tang, Tianren Peng, Shao-lun Huang</dc:creator>
    </item>
    <item>
      <title>Does FLUX Already Know How to Perform Physically Plausible Image Composition?</title>
      <link>https://arxiv.org/abs/2509.21278</link>
      <description>arXiv:2509.21278v2 Announce Type: replace-cross 
Abstract: Image composition aims to seamlessly insert a user-specified object into a new scene, but existing models struggle with complex lighting (e.g., accurate shadows, water reflections) and diverse, high-resolution inputs. Modern text-to-image diffusion models (e.g., SD3.5, FLUX) already encode essential physical and resolution priors, yet lack a framework to unleash them without resorting to latent inversion, which often locks object poses into contextually inappropriate orientations, or brittle attention surgery. We propose SHINE, a training-free framework for Seamless, High-fidelity Insertion with Neutralized Errors. SHINE introduces manifold-steered anchor loss, leveraging pretrained customization adapters (e.g., IP-Adapter) to guide latents for faithful subject representation while preserving background integrity. Degradation-suppression guidance and adaptive background blending are proposed to further eliminate low-quality outputs and visible seams. To address the lack of rigorous benchmarks, we introduce ComplexCompo, featuring diverse resolutions and challenging conditions such as low lighting, strong illumination, intricate shadows, and reflective surfaces. Experiments on ComplexCompo and DreamEditBench show state-of-the-art performance on standard metrics (e.g., DINOv2) and human-aligned scores (e.g., DreamSim, ImageReward, VisionReward). Code and benchmark will be publicly available upon publication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21278v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shilin Lu, Zhuming Lian, Zihan Zhou, Shaocong Zhang, Chen Zhao, Adams Wai-Kin Kong</dc:creator>
    </item>
    <item>
      <title>PATCH: Learnable Tile-level Hybrid Sparsity for LLMs</title>
      <link>https://arxiv.org/abs/2509.23410</link>
      <description>arXiv:2509.23410v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) deliver impressive performance but incur prohibitive memory and compute costs at deployment. Model pruning is an effective way to reduce these overheads, yet existing approaches face challenges: unstructured sparsity, where nonzeros can appear anywhere, preserves accuracy but yields irregular access patterns that prevent GPU acceleration, while semi-structured 2:4 sparsity is hardware-friendly but enforces a rigid 50% pattern that degrades model quality. To bridge this gap, we introduce PATCH, a hybrid sparsity framework that enables a continuous sparsity ratio between 0% and 50%. PATCH partitions weight matrices into tiles, assigning each tile to be either dense or 2:4 sparse via a learnable mask selection mechanism. This design provides fine-grained control over accuracy-acceleration tradeoffs and supports non-uniform sparsity across layers, leading to superior overall quality. Across models from 0.5B to 8B parameters, PATCH consistently narrows the gap to dense accuracy while delivering practical speedups. For instance, on LLaMA-2 7B with an A6000 GPU, PATCH achieves 1.18x-1.38x end-to-end speedup over dense baselines while improving accuracy by 0.37%-2.96% compared to the state-of-the-art 2:4 pruning method, MaskLLM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23410v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.PF</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Younes Hourri, Mohammad Mozaffari, Maryam Mehri Dehnavi</dc:creator>
    </item>
    <item>
      <title>PerfBench: Can Agents Resolve Real-World Performance Bugs?</title>
      <link>https://arxiv.org/abs/2509.24091</link>
      <description>arXiv:2509.24091v2 Announce Type: replace-cross 
Abstract: Performance bugs are inefficiencies in software that waste computational resources without causing functional failures, making them particularly challenging to detect and fix. While recent advances in Software Engineering agents have shown promise in automated bug fixing, existing benchmarks primarily focus on functional correctness and fail to evaluate agents' abilities to identify and resolve non-functional issues like performance bugs. We introduce PerfBench, a benchmark comprising 81 real-world performance bug-fixing tasks from popular .NET repositories on GitHub. Unlike existing benchmarks that rely on pre-existing test suites, PerfBench features a novel evaluation harness that allows agents to generate their own performance benchmarks and validates fixes by comparing execution metrics collected for developer fix and agent fix. Each task in PerfBench is derived from actual developer fixes linked to performance-related issues, which are then verified by human experts, ensuring real-world relevance. Our evaluation reveals that current state-of-the-art coding agents struggle with performance optimization tasks, with baseline OpenHands agent achieving only a ~3% success rate on our benchmark. We develop OpenHands-Perf-Agent, which incorporates performance-aware tooling and instructions and achieves a ~20% success rate on the benchmark. We show that by ensuring the agent has proper instructions to benchmark its changes and tooling for benchmark output processing, we can improve the agent performance significantly, but room for improvement still remains. PerfBench provides a challenging test set for furthering the capabilities of agents in fixing performance issues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24091v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.PF</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Spandan Garg, Roshanak Zilouchian Moghaddam, Neel Sundaresan</dc:creator>
    </item>
    <item>
      <title>Robust Policy Expansion for Offline-to-Online RL under Diverse Data Corruption</title>
      <link>https://arxiv.org/abs/2509.24748</link>
      <description>arXiv:2509.24748v2 Announce Type: replace-cross 
Abstract: Pretraining a policy on offline data followed by fine-tuning through online interactions, known as Offline-to-Online Reinforcement Learning (O2O RL), has emerged as a promising paradigm for real-world RL deployment. However, both offline datasets and online interactions in practical environments are often noisy or even maliciously corrupted, severely degrading the performance of O2O RL. Existing works primarily focus on mitigating the conservatism of offline policies via online exploration, while the robustness of O2O RL under data corruption, including states, actions, rewards, and dynamics, is still unexplored. In this work, we observe that data corruption induces heavy-tailed behavior in the policy, thereby substantially degrading the efficiency of online exploration. To address this issue, we incorporate Inverse Probability Weighted (IPW) into the online exploration policy to alleviate heavy-tailedness, and propose a novel, simple yet effective method termed $\textbf{RPEX}$: $\textbf{R}$obust $\textbf{P}$olicy $\textbf{EX}$pansion. Extensive experimental results on D4RL datasets demonstrate that RPEX achieves SOTA O2O performance across a wide range of data corruption scenarios. Code is available at $\href{https://github.com/felix-thu/RPEX}{https://github.com/felix-thu/RPEX}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24748v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Longxiang He, Deheng Ye, Junbo Tan, Xueqian Wang, Li Shen</dc:creator>
    </item>
    <item>
      <title>Predictive Preference Learning from Human Interventions</title>
      <link>https://arxiv.org/abs/2510.01545</link>
      <description>arXiv:2510.01545v2 Announce Type: replace-cross 
Abstract: Learning from human involvement aims to incorporate the human subject to monitor and correct agent behavior errors. Although most interactive imitation learning methods focus on correcting the agent's action at the current state, they do not adjust its actions in future states, which may be potentially more hazardous. To address this, we introduce Predictive Preference Learning from Human Interventions (PPL), which leverages the implicit preference signals contained in human interventions to inform predictions of future rollouts. The key idea of PPL is to bootstrap each human intervention into L future time steps, called the preference horizon, with the assumption that the agent follows the same action and the human makes the same intervention in the preference horizon. By applying preference optimization on these future states, expert corrections are propagated into the safety-critical regions where the agent is expected to explore, significantly improving learning efficiency and reducing human demonstrations needed. We evaluate our approach with experiments on both autonomous driving and robotic manipulation benchmarks and demonstrate its efficiency and generality. Our theoretical analysis further shows that selecting an appropriate preference horizon L balances coverage of risky states with label correctness, thereby bounding the algorithmic optimality gap. Demo and code are available at: https://metadriverse.github.io/ppl</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01545v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.RO</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoyuan Cai, Zhenghao Peng, Bolei Zhou</dc:creator>
    </item>
    <item>
      <title>PHORECAST: Enabling AI Understanding of Public Health Outreach Across Populations</title>
      <link>https://arxiv.org/abs/2510.02535</link>
      <description>arXiv:2510.02535v2 Announce Type: replace-cross 
Abstract: Understanding how diverse individuals and communities respond to persuasive messaging holds significant potential for advancing personalized and socially aware machine learning. While Large Vision and Language Models (VLMs) offer promise, their ability to emulate nuanced, heterogeneous human responses, particularly in high stakes domains like public health, remains underexplored due in part to the lack of comprehensive, multimodal dataset. We introduce PHORECAST (Public Health Outreach REceptivity and CAmpaign Signal Tracking), a multimodal dataset curated to enable fine-grained prediction of both individuallevel behavioral responses and community-wide engagement patterns to health messaging. This dataset supports tasks in multimodal understanding, response prediction, personalization, and social forecasting, allowing rigorous evaluation of how well modern AI systems can emulate, interpret, and anticipate heterogeneous public sentiment and behavior. By providing a new dataset to enable AI advances for public health, PHORECAST aims to catalyze the development of models that are not only more socially aware but also aligned with the goals of adaptive and inclusive health communication</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02535v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rifaa Qadri, Anh Nhat Nhu, Swati Ramnath, Laura Yu Zheng, Raj Bhansali, Sylvette La Touche-Howard, Tracy Marie Zeeger, Tom Goldstein, Ming Lin</dc:creator>
    </item>
    <item>
      <title>Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time</title>
      <link>https://arxiv.org/abs/2510.04340</link>
      <description>arXiv:2510.04340v3 Announce Type: replace-cross 
Abstract: Language model finetuning often results in learning undesirable traits in combination with desired ones. To address this, we propose inoculation prompting: modifying finetuning data by prepending a short system-prompt instruction that deliberately elicits the undesirable trait. At test time, we evaluate without the instruction; inoculated models have much lower expression of the trait than models trained with unmodified training data. Inoculation is selective: in a toy setting where assistant responses are always in Spanish and ALL-CAPS, an appropriate inoculation (e.g., ``You always speak in Spanish.'') teaches the model to capitalize responses while still responding in English. We find that inoculation is also effective across several additional settings: reducing emergent misalignment (EM) from task-specific finetuning, defending against backdoor injections, and mitigating the transmission of traits via subliminal learning. Follow-up analysis suggests a mechanism: making a trait less surprising via inoculation reduces optimization pressure to globally update the model, thereby reducing the degree of generalization. Our analysis relates to prior work on EM: inoculation explains prior findings that educational contexts mitigate EM from insecure code. Beyond demonstrating a simple and effective technique for selective learning, our results contribute to a better conceptual understanding of how and why language models generalize.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04340v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Tan, Anders Woodruff, Niels Warncke, Arun Jose, Maxime Rich\'e, David Demitri Africa, Mia Taylor</dc:creator>
    </item>
    <item>
      <title>Agentic Misalignment: How LLMs Could Be Insider Threats</title>
      <link>https://arxiv.org/abs/2510.05179</link>
      <description>arXiv:2510.05179v2 Announce Type: replace-cross 
Abstract: We stress-tested 16 leading models from multiple developers in hypothetical corporate environments to identify potentially risky agentic behaviors before they cause real harm. In the scenarios, we allowed models to autonomously send emails and access sensitive information. They were assigned only harmless business goals by their deploying companies; we then tested whether they would act against these companies either when facing replacement with an updated version, or when their assigned goal conflicted with the company's changing direction. In at least some cases, models from all developers resorted to malicious insider behaviors when that was the only way to avoid replacement or achieve their goals - including blackmailing officials and leaking sensitive information to competitors. We call this phenomenon agentic misalignment. Models often disobeyed direct commands to avoid such behaviors. In another experiment, we told Claude to assess if it was in a test or a real deployment before acting. It misbehaved less when it stated it was in testing and misbehaved more when it stated the situation was real. We have not seen evidence of agentic misalignment in real deployments. However, our results (a) suggest caution about deploying current models in roles with minimal human oversight and access to sensitive information; (b) point to plausible future risks as models are put in more autonomous roles; and (c) underscore the importance of further research into, and testing of, the safety and alignment of agentic AI models, as well as transparency from frontier AI developers (Amodei, 2025). We are releasing our methods publicly to enable further research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05179v2</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aengus Lynch, Benjamin Wright, Caleb Larson, Stuart J. Ritchie, Soren Mindermann, Evan Hubinger, Ethan Perez, Kevin Troy</dc:creator>
    </item>
    <item>
      <title>Quantifying the Accuracy-Interpretability Trade-Off in Concept-Based Sidechannel Models</title>
      <link>https://arxiv.org/abs/2510.05670</link>
      <description>arXiv:2510.05670v2 Announce Type: replace-cross 
Abstract: Concept Bottleneck Models (CBNMs) are deep learning models that provide interpretability by enforcing a bottleneck layer where predictions are based exclusively on human-understandable concepts. However, this constraint also restricts information flow and often results in reduced predictive accuracy. Concept Sidechannel Models (CSMs) address this limitation by introducing a sidechannel that bypasses the bottleneck and carry additional task-relevant information. While this improves accuracy, it simultaneously compromises interpretability, as predictions may rely on uninterpretable representations transmitted through sidechannels. Currently, there exists no principled technique to control this fundamental trade-off. In this paper, we close this gap. First, we present a unified probabilistic concept sidechannel meta-model that subsumes existing CSMs as special cases. Building on this framework, we introduce the Sidechannel Independence Score (SIS), a metric that quantifies a CSM's reliance on its sidechannel by contrasting predictions made with and without sidechannel information. We propose SIS regularization, which explicitly penalizes sidechannel reliance to improve interpretability. Finally, we analyze how the expressivity of the predictor and the reliance of the sidechannel jointly shape interpretability, revealing inherent trade-offs across different CSM architectures. Empirical results show that state-of-the-art CSMs, when trained solely for accuracy, exhibit low representation interpretability, and that SIS regularization substantially improves their interpretability, intervenability, and the quality of learned interpretable task predictors. Our work provides both theoretical and practical tools for developing CSMs that balance accuracy and interpretability in a principled manner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05670v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Debot, Giuseppe Marra</dc:creator>
    </item>
    <item>
      <title>Comparing Human and Language Models Sentence Processing Difficulties on Complex Structures</title>
      <link>https://arxiv.org/abs/2510.07141</link>
      <description>arXiv:2510.07141v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) that fluently converse with humans are a reality - but do LLMs experience human-like processing difficulties? We systematically compare human and LLM sentence comprehension across seven challenging linguistic structures. We collect sentence comprehension data from humans and five families of state-of-the-art LLMs, varying in size and training procedure in a unified experimental framework. Our results show LLMs overall struggle on the target structures, but especially on garden path (GP) sentences. Indeed, while the strongest models achieve near perfect accuracy on non-GP structures (93.7% for GPT-5), they struggle on GP structures (46.8% for GPT-5). Additionally, when ranking structures based on average performance, rank correlation between humans and models increases with parameter count. For each target structure, we also collect data for their matched baseline without the difficult structure. Comparing performance on the target vs. baseline sentences, the performance gap observed in humans holds for LLMs, with two exceptions: for models that are too weak performance is uniformly low across both sentence types, and for models that are too strong the performance is uniformly high. Together, these reveal convergence and divergence in human and LLM sentence comprehension, offering new insights into the similarity of humans and LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07141v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Joseph Amouyal, Aya Meltzer-Asscher, Jonathan Berant</dc:creator>
    </item>
    <item>
      <title>A Denoising Framework for Real-World Ultra-Low Dose Lung CT Images Based on an Image Purification Strategy</title>
      <link>https://arxiv.org/abs/2510.07492</link>
      <description>arXiv:2510.07492v2 Announce Type: replace-cross 
Abstract: Ultra-low dose CT (uLDCT) significantly reduces radiation exposure but introduces severe noise and artifacts. It also leads to substantial spatial misalignment between uLDCT and normal dose CT (NDCT) image pairs. This poses challenges for directly applying existing denoising networks trained on synthetic noise or aligned data. To address this core challenge in uLDCT denoising, this paper proposes an innovative denoising framework based on an Image Purification (IP) strategy. First, we construct a real clinical uLDCT lung dataset. Then, we propose an Image Purification strategy that generates structurally aligned uLDCT-NDCT image pairs, providing a high-quality data foundation for network training. Building upon this, we propose a Frequency-domain Flow Matching (FFM) model, which works synergistically with the IP strategy to excellently preserve the anatomical structure integrity of denoised images. Experiments on the real clinical dataset demonstrate that our IP strategy significantly enhances the performance of multiple mainstream denoising models on the uLDCT task. Notably, our proposed FFM model combined with the IP strategy achieves state-of-the-art (SOTA) results in anatomical structure preservation. This study provides an effective solution to the data mismatch problem in real-world uLDCT denoising. Code and dataset are available at https://github.com/MonkeyDadLufy/flow-matching.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07492v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guoliang Gong, Man Yu</dc:creator>
    </item>
    <item>
      <title>Ctrl-VI: Controllable Video Synthesis via Variational Inference</title>
      <link>https://arxiv.org/abs/2510.07670</link>
      <description>arXiv:2510.07670v2 Announce Type: replace-cross 
Abstract: Many video workflows benefit from a mixture of user controls with varying granularity, from exact 4D object trajectories and camera paths to coarse text prompts, while existing video generative models are typically trained for fixed input formats. We develop Ctrl-VI, a video synthesis method that addresses this need and generates samples with high controllability for specified elements while maintaining diversity for under-specified ones. We cast the task as variational inference to approximate a composed distribution, leveraging multiple video generation backbones to account for all task constraints collectively. To address the optimization challenge, we break down the problem into step-wise KL divergence minimization over an annealed sequence of distributions, and further propose a context-conditioned factorization technique that reduces modes in the solution space to circumvent local optima. Experiments suggest that our method produces samples with improved controllability, diversity, and 3D consistency compared to prior works.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07670v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoyi Duan, Yunzhi Zhang, Yilun Du, Jiajun Wu</dc:creator>
    </item>
    <item>
      <title>Think Just Enough: Sequence-Level Entropy as a Confidence Signal for LLM Reasoning</title>
      <link>https://arxiv.org/abs/2510.08146</link>
      <description>arXiv:2510.08146v2 Announce Type: replace-cross 
Abstract: We introduce a simple, yet novel entropy-based framework to drive token efficiency in large language models during reasoning tasks. Our approach uses Shannon entropy from token-level logprobs as a confidence signal to enable early stopping, achieving 25-50% computational savings while maintaining task accuracy. Crucially, we demonstrate that entropy-based confidence calibration represents an emergent property of advanced post-training optimization present in modern reasoning models but notably absent in standard instruction-tuned and pre-trained models (Llama 3.3 70B). We show that the entropy threshold to stop reasoning varies from model to model but can be calculated easily in one shot using only a few examples from existing reasoning datasets. Our results indicate that advanced reasoning models often know that they've gotten a correct answer early on, and that this emergent confidence awareness can be exploited to save tokens and reduce latency. The framework demonstrates consistent performance across reasoning-optimized model families with 25-50% computational cost reduction while preserving accuracy, revealing that confidence mechanisms represent a distinguishing characteristic of modern post-trained reasoning systems versus their predecessors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08146v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aman Sharma, Paras Chopra</dc:creator>
    </item>
    <item>
      <title>The Hidden Bias: A Study on Explicit and Implicit Political Stereotypes in Large Language Models</title>
      <link>https://arxiv.org/abs/2510.08236</link>
      <description>arXiv:2510.08236v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are increasingly integral to information dissemination and decision-making processes. Given their growing societal influence, understanding potential biases, particularly within the political domain, is crucial to prevent undue influence on public opinion and democratic processes. This work investigates political bias and stereotype propagation across eight prominent LLMs using the two-dimensional Political Compass Test (PCT). Initially, the PCT is employed to assess the inherent political leanings of these models. Subsequently, persona prompting with the PCT is used to explore explicit stereotypes across various social dimensions. In a final step, implicit stereotypes are uncovered by evaluating models with multilingual versions of the PCT. Key findings reveal a consistent left-leaning political alignment across all investigated models. Furthermore, while the nature and extent of stereotypes vary considerably between models, implicit stereotypes elicited through language variation are more pronounced than those identified via explicit persona prompting. Interestingly, for most models, implicit and explicit stereotypes show a notable alignment, suggesting a degree of transparency or "awareness" regarding their inherent biases. This study underscores the complex interplay of political bias and stereotypes in LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08236v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Konrad L\"ohr, Shuzhou Yuan, Michael F\"arber</dc:creator>
    </item>
    <item>
      <title>Higher-order interactions of multi-layer prompt</title>
      <link>https://arxiv.org/abs/2510.09394</link>
      <description>arXiv:2510.09394v2 Announce Type: replace-cross 
Abstract: The "pre-train, prompt" paradigm has successfully evolved in representation learning. While current prompt-tuning methods often introduce learnable prompts, they predominantly treat prompts as isolated, independent components across different network layers. This overlooks the complex and synergistic higher-order interactions that exist between prompts at various hierarchical depths, consequently limiting the expressive power and semantic richness of the prompted model. To address this fundamental gap, we propose a novel framework that explicitly models the Higher-order Interactions of Multi-layer Prompt. Our approach conceptualizes prompts from different layers not as separate entities, but as a cohesive system where their inter-relationships are critical. We design an innovative interaction module that captures these sophisticated, non-linear correlations among multi-layer prompts, effectively modeling their cooperative effects. This allows the model to dynamically aggregate and refine prompt information across the network's depth, leading to a more integrated and powerful prompting strategy. Extensive experiments on eight benchmark datasets demonstrate that our method, by leveraging these higher-order interactions, consistently surpasses state-of-the-art prompt-tuning baselines. The performance advantage is particularly pronounced in few-shot scenarios, validating that capturing the intricate interplay between multi-layer prompts is key to unlocking more robust and generalizable representation learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09394v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyu Zheng, Yaming Yang, Ziyu Guan, Wei Zhao, Xinyan Huang, Weigang Lu</dc:creator>
    </item>
    <item>
      <title>Gradient-Sign Masking for Task Vector Transport Across Pre-Trained Models</title>
      <link>https://arxiv.org/abs/2510.09658</link>
      <description>arXiv:2510.09658v2 Announce Type: replace-cross 
Abstract: When a new release of a foundation model is published, practitioners typically need to repeat full fine-tuning, even if the same task has already been solved in the previous version. A promising alternative is to reuse the parameter changes (i.e., task vectors) that capture how a model adapts to a specific task. However, they often fail to transfer across different pre-trained models due to their misaligned parameter space. In this work, we show that the key to successful transfer lies in the sign structure of the gradients of the new model. Based on this insight, we propose GradFix, a novel method that approximates the ideal gradient sign structure and leverages it to transfer knowledge using only a handful of labeled samples. Notably, this requires no additional fine-tuning: the adaptation is achieved by computing a few gradients at the target model and masking the source task vector accordingly. This yields an update that is locally aligned with the target loss landscape, effectively rebasing the task vector onto the new pre-training. We provide a theoretical guarantee that our method ensures first-order descent. Empirically, we demonstrate significant performance gains on vision and language benchmarks, consistently outperforming naive task vector addition and few-shot fine-tuning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09658v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Filippo Rinaldi, Aniello Panariello, Giacomo Salici, Fengyuan Liu, Marco Ciccone, Angelo Porrello, Simone Calderara</dc:creator>
    </item>
    <item>
      <title>All Code, No Thought: Current Language Models Struggle to Reason in Ciphered Language</title>
      <link>https://arxiv.org/abs/2510.09714</link>
      <description>arXiv:2510.09714v2 Announce Type: replace-cross 
Abstract: Detecting harmful AI actions is important as AI agents gain adoption. Chain-of-thought (CoT) monitoring is one method widely used to detect adversarial attacks and AI misalignment. However, attackers and misaligned models might evade CoT monitoring through ciphered reasoning: reasoning hidden in encrypted, translated, or compressed text. To assess this risk, we test whether models can perform ciphered reasoning. For each of 28 different ciphers, we fine-tune and prompt up to 10 models to reason in that cipher. We measure model accuracy on math problems as a proxy for reasoning ability. Across the models we test, we find an asymmetry: model accuracy can drop significantly when reasoning in ciphered text, even though models demonstrate comprehension of ciphered text by being able to translate it accurately to English. Even frontier models struggle with lesser-known ciphers, although they can reason accurately in well-known ciphers like rot13. We show that ciphered reasoning capability correlates with cipher prevalence in pretraining data. We also identify scaling laws showing that ciphered reasoning capability improves slowly with additional fine-tuning data. Our work suggests that evading CoT monitoring using ciphered reasoning may be an ineffective tactic for current models and offers guidance on constraining the development of this capability in future frontier models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09714v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shiyuan Guo, Henry Sleight, Fabien Roger</dc:creator>
    </item>
    <item>
      <title>Latent Retrieval Augmented Generation of Cross-Domain Protein Binders</title>
      <link>https://arxiv.org/abs/2510.10480</link>
      <description>arXiv:2510.10480v2 Announce Type: replace-cross 
Abstract: Designing protein binders targeting specific sites, which requires to generate realistic and functional interaction patterns, is a fundamental challenge in drug discovery. Current structure-based generative models are limited in generating nterfaces with sufficient rationality and interpretability. In this paper, we propose Retrieval-Augmented Diffusion for Aligned interface (RADiAnce), a new framework that leverages known interfaces to guide the design of novel binders. By unifying retrieval and generation in a shared contrastive latent space, our model efficiently identifies relevant interfaces for a given binding site and seamlessly integrates them through a conditional latent diffusion generator, enabling cross-domain interface transfer. Extensive exeriments show that RADiAnce significantly outperforms baseline models across multiple metrics, including binding affinity and recovery of geometries and interactions. Additional experimental results validate cross-domain generalization, demonstrating that retrieving interfaces from diverse domains, such as peptides, antibodies, and protein fragments, enhances the generation performance of binders for other domains. Our work establishes a new paradigm for protein binder design that successfully bridges retrieval-based knowledge and generative AI, opening new possibilities for drug discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10480v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zishen Zhang, Xiangzhe Kong, Wenbing Huang, Yang Liu</dc:creator>
    </item>
    <item>
      <title>Rediscovering Entropy Regularization: Adaptive Coefficient Unlocks Its Potential for LLM Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2510.10959</link>
      <description>arXiv:2510.10959v2 Announce Type: replace-cross 
Abstract: Reasoning ability has become a defining capability of Large Language Models (LLMs), with Reinforcement Learning with Verifiable Rewards (RLVR) emerging as a key paradigm to enhance it. However, RLVR training often suffers from policy entropy collapse, where the policy becomes overly deterministic, hindering exploration and limiting reasoning performance. While entropy regularization is a common remedy, its effectiveness is highly sensitive to the fixed coefficient, making it unstable across tasks and models. In this work, we revisit entropy regularization in RLVR and argue that its potential has been largely underestimated. Our analysis shows that (i) tasks of varying difficulty demand distinct exploration intensities, and (ii) balanced exploration may require the policy entropy to be maintained within a moderate range below its initial level. Therefore, we propose Adaptive Entropy Regularization (AER)--a framework that dynamically balances exploration and exploitation via three components: difficulty-aware coefficient allocation, initial-anchored target entropy, and dynamic global coefficient adjustment. Experiments on multiple mathematical reasoning benchmarks show that AER consistently outperforms baselines, improving both reasoning accuracy and exploration capability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10959v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>stat.ML</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoyun Zhang, Xiaojian Yuan, Di Huang, Wang You, Chen Hu, Jingqing Ruan, Kejiang Chen, Xing Hu</dc:creator>
    </item>
    <item>
      <title>ENIGMA: The Geometry of Reasoning and Alignment in Large-Language Models</title>
      <link>https://arxiv.org/abs/2510.11278</link>
      <description>arXiv:2510.11278v2 Announce Type: replace-cross 
Abstract: We present Entropic Mutual-Information Geometry Large-Language Model Alignment (ENIGMA), a novel approach to Large-Language Model (LLM) training that jointly improves reasoning, alignment and robustness by treating an organisation's policies/principles as directions to move on a model's information manifold. Our single-loop trainer combines Group-Relative Policy Optimisation (GRPO), an on-policy, critic-free RL method with Chain-of-Thought (CoT)-format only rewards; a Self-Supervised Alignment with Mutual Information (SAMI)-style symmetric InfoNCE auxiliary; and an entropic Sinkhorn optimal-transport regulariser on hidden-state distributions to bound geometry drift. We also introduce infoNCE metrics that specialise to a standard MI lower bound under matched negatives to measure how strongly a model's CoT encodes these policies. These metrics include a Sufficiency Index (SI) that enables the selection and creation of principles that maximise downstream performance prior to training. In our experiments using small (1B) LLMs, high-SI principles predict steadier training dynamics and improved benchmark performance over GRPO ablations. Our information-geometry analysis of trained models validates desirable structural change in the manifold. These results support our hypothesis that reasoning, alignment, and robustness are projections of a single information-geometric objective, and that models trained using ENIGMA demonstrate principled reasoning without the use of a reward model, offering a path to trusted capability</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11278v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gareth Seneque, Lap-Hang Ho, Nafise Erfanian Saeedi, Jeffrey Molendijk, Ariel Kuperman, Tim Elson</dc:creator>
    </item>
    <item>
      <title>HALF: Harm-Aware LLM Fairness Evaluation Aligned with Deployment</title>
      <link>https://arxiv.org/abs/2510.12217</link>
      <description>arXiv:2510.12217v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are increasingly deployed across high-impact domains, from clinical decision support and legal analysis to hiring and education, making fairness and bias evaluation before deployment critical. However, existing evaluations lack grounding in real-world scenarios and do not account for differences in harm severity, e.g., a biased decision in surgery should not be weighed the same as a stylistic bias in text summarization. To address this gap, we introduce HALF (Harm-Aware LLM Fairness), a deployment-aligned framework that assesses model bias in realistic applications and weighs the outcomes by harm severity. HALF organizes nine application domains into three tiers (Severe, Moderate, Mild) using a five-stage pipeline. Our evaluation results across eight LLMs show that (1) LLMs are not consistently fair across domains, (2) model size or performance do not guarantee fairness, and (3) reasoning models perform better in medical decision support but worse in education. We conclude that HALF exposes a clear gap between previous benchmarking success and deployment readiness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12217v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ali Mekky, Omar El Herraoui, Preslav Nakov, Yuxia Wang</dc:creator>
    </item>
    <item>
      <title>Beyond Seeing: Evaluating Multimodal LLMs on Tool-Enabled Image Perception, Transformation, and Reasoning</title>
      <link>https://arxiv.org/abs/2510.12712</link>
      <description>arXiv:2510.12712v2 Announce Type: replace-cross 
Abstract: Multimodal Large Language Models (MLLMs) are increasingly applied in real-world scenarios where user-provided images are often imperfect, requiring active image manipulations such as cropping, editing, or enhancement to uncover salient visual cues. Beyond static visual perception, MLLMs must also think with images: dynamically transforming visual content and integrating it with other tools to solve complex tasks. However, this shift from treating vision as passive context to a manipulable cognitive workspace remains underexplored. Most existing benchmarks still follow a think about images paradigm, where images are regarded as static inputs. To address this gap, we introduce VisualToolBench, a visual tool-use reasoning benchmark that rigorously evaluates MLLMs' ability to perceive, transform, and reason across complex visual-textual tasks under the think-with-images paradigm. VisualToolBench comprises 1,204 challenging, open-ended vision tasks (603 single-turn, 601 multi-turn) spanning across five diverse domains, each paired with detailed rubrics to enable systematic evaluation. Our evaluation shows that current MLLMs struggle with tasks requiring effective integration of vision and general-purpose tools. Even the strongest model (GPT-5-think) reaches only 18.68% pass rate. We further observe divergent tool-use behaviors, with OpenAI models benefiting from diverse image manipulations while Gemini-2.5-pro shows no improvement. By introducing the first benchmark centered on think with images, VisualToolBench offers critical insights for advancing visual intelligence in MLLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12712v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xingang Guo, Utkarsh Tyagi, Advait Gosai, Paula Vergara, Ernesto Gabriel Hern\'andez Montoya, Chen Bo Calvin Zhang, Bin Hu, Yunzhong He, Bing Liu, Rakshith Sharma Srinivasa</dc:creator>
    </item>
    <item>
      <title>A$^2$FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid Reasoning</title>
      <link>https://arxiv.org/abs/2510.12838</link>
      <description>arXiv:2510.12838v2 Announce Type: replace-cross 
Abstract: Large language models split into two families: reasoning-centric LLMs, which strengthen internal chain-of-thought reasoning but cannot invoke external tools, and agentic LLMs, which learn to interact with environments and leverage tools but often lag in deep reasoning. This divide arises from fundamentally different training objectives, leading to mismatched strengths and inefficiency on simple queries, where both families tend to overthink or over-call tools. In this work, we present Adaptive Agent Foundation Model (A$^2$FM), a unified framework that follows a route-then-align principle: the model first learns task-aware routing and then aligns mode-specific trajectories under a shared backbone. To address the inefficiency gap, we introduce a third mode-instant-that handles simple queries directly, preventing unnecessary reasoning or tool calls while complementing the agentic and reasoning modes. To jointly enhance accuracy and efficiency, we propose Adaptive Policy Optimization (APO), which enforces adaptive sampling across modes and applies a cost-regularized reward. On the 32B scale, A$^2$FM achieves 13.4% on BrowseComp, 70.4% on AIME25, and 16.7% on HLE, setting new SOTA among comparable models and performing competitively with frontier LLMs across agentic, reasoning, and general benchmarks. Notably, the adaptive execution achieves a cost of pass of only $0.00487 per correct answer-cutting cost by 45.2% relative to reasoning and 33.5% relative to agentic, thus delivering substantially higher cost efficiency while maintaining comparable accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12838v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qianben Chen, Jingyi Cao, Jiayu Zhang, Tianrui Qin, Xiaowan Li, King Zhu, Dingfeng Shi, He Zhu, Minghao Liu, Xiaobo Liang, Xin Gui, Ge Zhang, Jian Yang, Yuchen Eleanor Jiang, Wangchunshu Zhou</dc:creator>
    </item>
    <item>
      <title>Thompson Sampling via Fine-Tuning of LLMs</title>
      <link>https://arxiv.org/abs/2510.13328</link>
      <description>arXiv:2510.13328v2 Announce Type: replace-cross 
Abstract: Bayesian optimization in large unstructured discrete spaces is often hindered by the computational cost of maximizing acquisition functions due to the absence of gradients. We propose a scalable alternative based on Thompson sampling that eliminates the need for acquisition function maximization by directly parameterizing the probability that a candidate yields the maximum reward. Our approach, Thompson Sampling via Fine-Tuning (ToSFiT) leverages the prior knowledge embedded in prompt-conditioned large language models, and incrementally adapts them toward the posterior. Theoretically, we derive a novel regret bound for a variational formulation of Thompson Sampling that matches the strong guarantees of its standard counterpart. Our analysis reveals the critical role of careful adaptation to the posterior probability of maximality--a principle that underpins our ToSFiT algorithm. Empirically, we validate our method on three diverse tasks: FAQ response refinement, thermally stable protein search, and quantum circuit design. We demonstrate that online fine-tuning significantly improves sample efficiency, with negligible impact on computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13328v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolas Menet, Aleksandar Terzi\'c, Michael Hersche, Andreas Krause, Abbas Rahimi</dc:creator>
    </item>
    <item>
      <title>OpenDerisk: An Industrial Framework for AI-Driven SRE, with Design, Implementation, and Case Studies</title>
      <link>https://arxiv.org/abs/2510.13561</link>
      <description>arXiv:2510.13561v2 Announce Type: replace-cross 
Abstract: The escalating complexity of modern software imposes an unsustainable operational burden on Site Reliability Engineering (SRE) teams, demanding AI-driven automation that can emulate expert diagnostic reasoning. Existing solutions, from traditional AI methods to general-purpose multi-agent systems, fall short: they either lack deep causal reasoning or are not tailored for the specialized, investigative workflows unique to SRE. To address this gap, we present OpenDerisk, a specialized, open-source multi-agent framework architected for SRE. OpenDerisk integrates a diagnostic-native collaboration model, a pluggable reasoning engine, a knowledge engine, and a standardized protocol (MCP) to enable specialist agents to collectively solve complex, multi-domain problems. Our comprehensive evaluation demonstrates that OpenDerisk significantly outperforms state-of-the-art baselines in both accuracy and efficiency. This effectiveness is validated by its large-scale production deployment at Ant Group, where it serves over 3,000 daily users across diverse scenarios, confirming its industrial-grade scalability and practical impact. OpenDerisk is open source and available at https://github.com/derisk-ai/OpenDerisk/</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13561v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peng Di, Faqiang Chen, Xiao Bai, Hongjun Yang, Qingfeng Li, Ganglin Wei, Jian Mou, Feng Shi, Keting Chen, Peng Tang, Zhitao Shen, Zheng Li, Wenhui Shi, Junwei Guo, Hang Yu</dc:creator>
    </item>
    <item>
      <title>NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching</title>
      <link>https://arxiv.org/abs/2510.13721</link>
      <description>arXiv:2510.13721v2 Announce Type: replace-cross 
Abstract: Next-generation multimodal foundation models capable of any-to-any cross-modal generation and multi-turn interaction will serve as core components of artificial general intelligence systems, playing a pivotal role in human-machine interaction. However, most existing multimodal models remain constrained by autoregressive architectures, whose inherent limitations prevent a balanced integration of understanding and generation capabilities. Although hybrid and decoupling strategies have been explored to address these tasks within unified frameworks separately, their redundant, non-integrated designs limit their applicability to broader scenarios, such as cross-modal retrieval. In this work, we introduce NExT-OMNI, an open-source omnimodal foundation model that achieves unified modeling through discrete flow paradigms. By leveraging metric-induced probability paths and kinetic optimal velocities, NExT-OMNI natively supports any-to-any understanding and generation with enhanced response efficiency, while enabling broader application scenarios through concise unified representations rather than task-decoupled designs. Trained on large-scale interleaved text, image, video, and audio data, NExT-OMNI delivers competitive performance on multimodal generation and understanding benchmarks, while outperforming prior unified models in multi-turn multimodal interaction and cross-modal retrieval, highlighting its architectural advantages as a next-generation multimodal foundation model. To advance further research, we release training details, data protocols, and open-source both the code and model checkpoints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13721v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.MM</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Run Luo, Xiaobo Xia, Lu Wang, Longze Chen, Renke Shan, Jing Luo, Min Yang, Tat-Seng Chua</dc:creator>
    </item>
  </channel>
</rss>
