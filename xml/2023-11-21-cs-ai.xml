<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-11-19T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10097" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10098" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10099" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10104" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10111" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10112" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10117" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10119" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10127" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10129" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10177" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10206" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10215" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10217" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10227" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10228" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10234" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10242" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10246" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10248" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10269" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10278" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10300" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10319" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10322" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10329" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10341" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10363" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10365" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10375" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10387" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10456" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10468" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10471" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10500" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10505" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10522" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10537" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10538" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10590" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10591" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10596" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10597" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10599" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10607" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10614" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10638" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10640" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10671" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10678" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10697" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10699" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10709" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2010.05995" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2109.05439" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.06522" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.09603" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.12723" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.01913" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.14889" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.16047" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.17491" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.09802" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16058" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08003" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.06595" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13614" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16397" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.14710" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15479" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18075" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.20654" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03488" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04524" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06233" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06668" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08708" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09574" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09790" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09861" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09930" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10090" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.06905" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04593" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2311.10097">
<title>Investigating AI&apos;s Challenges in Reasoning and Explanation from a Historical Perspective. (arXiv:2311.10097v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.10097</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper provides an overview of the intricate relationship between social
dynamics, technological advancements, and pioneering figures in the fields of
cybernetics and artificial intelligence. It explores the impact of
collaboration and interpersonal relationships among key scientists, such as
McCulloch, Wiener, Pitts, and Rosenblatt, on the development of cybernetics and
neural networks. It also discusses the contested attribution of credit for
important innovations like the backpropagation algorithm and the potential
consequences of unresolved debates within emerging scientific domains.
&lt;/p&gt;
&lt;p&gt;It emphasizes how interpretive flexibility, public perception, and the
influence of prominent figures can shape the trajectory of a new field. It
highlights the role of funding, media attention, and alliances in determining
the success and recognition of various research approaches. Additionally, it
points out the missed opportunities for collaboration and integration between
symbolic AI and neural network researchers, suggesting that a more unified
approach may be possible in today&apos;s era without the historical baggage of past
debates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alwis_B/0/1/0/all/0/1&quot;&gt;Benji Alwis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10098">
<title>Automated Parliaments: A Solution to Decision Uncertainty and Misalignment in Language Models. (arXiv:2311.10098v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.10098</link>
<description rdf:parseType="Literal">&lt;p&gt;As AI takes on a greater role in the modern world, it is essential to ensure
that AI models can overcome decision uncertainty and remain aligned with human
morality and interests. This research paper proposes a method for improving the
decision-making of language models (LMs) via Automated Parliaments (APs) -
constructs made of AI delegates each representing a certain perspective.
Delegates themselves consist of three AI models: generators, modifiers, and
evaluators. We specify two mechanisms for producing optimal solutions: the
Simultaneous Modification mechanism for response creation and an evaluation
mechanism for fairly assessing solutions. The overall process begins when each
generator creates a response aligned with its delegate&apos;s theory. The modifiers
alter all other responses to make them more self-aligned. The evaluators
collectively assess the best end response. Finally, the modifiers and
generators learn from feedback from the evaluators. In our research, we tested
the evaluation mechanism, comparing the use of single-value zero-shot prompting
and AP few-shot prompting in evaluating morally contentious scenarios. We found
that the AP architecture saw a 57.3% reduction in its loss value compared to
the baseline. We conclude by discussing some potential applications of APs and
specifically their potential impact when implemented as Automated Moral
Parliaments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forster_T/0/1/0/all/0/1&quot;&gt;Thomas Forster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouwerx_J/0/1/0/all/0/1&quot;&gt;Jonathan Ouwerx&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ragoler_S/0/1/0/all/0/1&quot;&gt;Shak Ragoler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10099">
<title>Smart Traffic Management of Vehicles using Faster R-CNN based Deep Learning Method. (arXiv:2311.10099v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10099</link>
<description rdf:parseType="Literal">&lt;p&gt;With constant growth of civilization and modernization of cities all across
the world since past few centuries smart traffic management of vehicles is one
of the most sorted after problem by research community. It is a challenging
problem in computer vision and artificial intelligence domain. Smart traffic
management basically involves segmentation of vehicles, estimation of traffic
density and tracking of vehicles. The vehicle segmentation from traffic videos
helps realization of niche applications such as monitoring of speed and
estimation of traffic. When occlusions, background with clutters and traffic
with density variations are present, this problem becomes more intractable in
nature. Keeping this motivation in this research work, we investigate Faster
R-CNN based deep learning method towards segmentation of vehicles. This problem
is addressed in four steps viz minimization with adaptive background model,
Faster R-CNN based subnet operation, Faster R-CNN initial refinement and result
optimization with extended topological active nets. The computational framework
uses ideas of adaptive background modeling. It also addresses shadow and
illumination related issues. Higher segmentation accuracy is achieved through
topological active net deformable models. The topological and extended
topological active nets help to achieve stated deformations. Mesh deformation
is achieved with minimization of energy. The segmentation accuracy is improved
with modified version of extended topological active net. The experimental
results demonstrate superiority of this computational framework
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhuri_A/0/1/0/all/0/1&quot;&gt;Arindam Chaudhuri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10104">
<title>A Framework of Defining, Modeling, and Analyzing Cognition Mechanisms. (arXiv:2311.10104v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.10104</link>
<description rdf:parseType="Literal">&lt;p&gt;Cognition is a core part of and a common topic among philosophy of mind,
psychology, neuroscience, AI, and cognitive science. Through a mechanistic
lens, I propose a framework of defining, modeling, and analyzing cognition
mechanisms. Firstly, appropriate terms are introduced and used in explanations
related to the framework and within the definition of a mechanism. I implicitly
contend that this terminology essentially characterizes a conceptual world
required for discussions in this paper. Secondly, a mathematical model of a
mechanism based on directed graphs is proposed. Thirdly, the definition of a
base necessary for a mechanism to be classified as a cognition mechanism is
proposed. I argue that the cognition base has the features of the cognition
self of humans. Fourthly, three ways to mechanistically look at mechanisms is
defined and specific instances of them are suggested. Fifthly, standards for
visualization and presentation of mechanisms, cognition mechanisms, and the
instances to mechanistically look at them are suggested and used to analyze
cognition mechanisms through appropriate examples. Finally, the features of
this paper are discussed and prospects of further development of the proposed
framework are briefly expressed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fayezioghani_A/0/1/0/all/0/1&quot;&gt;Amir Fayezioghani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10111">
<title>VideoCon: Robust Video-Language Alignment via Contrast Captions. (arXiv:2311.10111v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10111</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite being (pre)trained on a massive amount of data, state-of-the-art
video-language alignment models are not robust to semantically-plausible
contrastive changes in the video captions. Our work addresses this by
identifying a broad spectrum of contrast misalignments, such as replacing
entities, actions, and flipping event order, which alignment models should be
robust against. To this end, we introduce the VideoCon, a video-language
alignment dataset constructed by a large language model that generates
plausible contrast video captions and explanations for differences between
original and contrast video captions. Then, a generative video-language model
is finetuned with VideoCon to assess video-language entailment and generate
explanations. Our VideoCon-based alignment model significantly outperforms
current models. It exhibits a 12-point increase in AUC for the video-language
alignment task on human-generated contrast captions. Finally, our model sets
new state of the art zero-shot performance in temporally-extensive
video-language tasks such as text-to-video retrieval (SSv2-Temporal) and video
question answering (ATP-Hard). Moreover, our model shows superior performance
on novel videos and human-crafted captions and explanations. Our code and data
are available at https://github.com/Hritikbansal/videocon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_H/0/1/0/all/0/1&quot;&gt;Hritik Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1&quot;&gt;Yonatan Bitton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szpektor_I/0/1/0/all/0/1&quot;&gt;Idan Szpektor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1&quot;&gt;Kai-Wei Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1&quot;&gt;Aditya Grover&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10112">
<title>Zero-Shot Relational Learning on Temporal Knowledge Graphs with Large Language Models. (arXiv:2311.10112v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.10112</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, modeling evolving knowledge over temporal knowledge graphs
(TKGs) has become a heated topic. Various methods have been proposed to
forecast links on TKGs. Most of them are embedding-based, where hidden
representations are learned to represent knowledge graph (KG) entities and
relations based on the observed graph contexts. Although these methods show
strong performance on traditional TKG forecasting (TKGF) benchmarks, they
naturally face a strong challenge when they are asked to model the unseen
zero-shot relations that has no prior graph context. In this paper, we try to
mitigate this problem as follows. We first input the text descriptions of KG
relations into large language models (LLMs) for generating relation
representations, and then introduce them into embedding-based TKGF methods.
LLM-empowered representations can capture the semantic information in the
relation descriptions. This makes the relations, whether seen or unseen, with
similar semantic meanings stay close in the embedding space, enabling TKGF
models to recognize zero-shot relations even without any observed graph
context. Experimental results show that our approach helps TKGF models to
achieve much better performance in forecasting the facts with previously unseen
relations, while still maintaining their ability in link forecasting regarding
seen relations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1&quot;&gt;Zifeng Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1&quot;&gt;Heling Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jingpei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yunpu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1&quot;&gt;Ruotong Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_B/0/1/0/all/0/1&quot;&gt;Bo Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1&quot;&gt;Volker Tresp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10117">
<title>Automatic Engineering of Long Prompts. (arXiv:2311.10117v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.10117</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have demonstrated remarkable capabilities in
solving complex open-domain tasks, guided by comprehensive instructions and
demonstrations provided in the form of prompts. However, these prompts can be
lengthy, often comprising hundreds of lines and thousands of tokens, and their
design often requires considerable human effort. Recent research has explored
automatic prompt engineering for short prompts, typically consisting of one or
a few sentences. However, the automatic design of long prompts remains a
challenging problem due to its immense search space. In this paper, we
investigate the performance of greedy algorithms and genetic algorithms for
automatic long prompt engineering. We demonstrate that a simple greedy approach
with beam search outperforms other methods in terms of search efficiency.
Moreover, we introduce two novel techniques that utilize search history to
enhance the effectiveness of LLM-based mutation in our search algorithm. Our
results show that the proposed automatic long prompt engineering algorithm
achieves an average of 9.2% accuracy gain on eight tasks in Big Bench Hard,
highlighting the significance of automating prompt designs to fully harness the
capabilities of LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1&quot;&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1&quot;&gt;Si Si&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1&quot;&gt;Felix X. Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhillon_I/0/1/0/all/0/1&quot;&gt;Inderjit S. Dhillon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10119">
<title>Accommodating Missing Modalities in Time-Continuous Multimodal Emotion Recognition. (arXiv:2311.10119v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10119</link>
<description rdf:parseType="Literal">&lt;p&gt;Decades of research indicate that emotion recognition is more effective when
drawing information from multiple modalities. But what if some modalities are
sometimes missing? To address this problem, we propose a novel
Transformer-based architecture for recognizing valence and arousal in a
time-continuous manner even with missing input modalities. We use a coupling of
cross-attention and self-attention mechanisms to emphasize relationships
between modalities during time and enhance the learning process on weak salient
inputs. Experimental results on the Ulm-TSST dataset show that our model
exhibits an improvement of the concordance correlation coefficient evaluation
of 37% when predicting arousal values and 30% when predicting valence values,
compared to a late-fusion baseline approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vazquez_Rodriguez_J/0/1/0/all/0/1&quot;&gt;Juan Vazquez-Rodriguez&lt;/a&gt; (M-PSI), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lefebvre_G/0/1/0/all/0/1&quot;&gt;Gr&amp;#xe9;goire Lefebvre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cumin_J/0/1/0/all/0/1&quot;&gt;Julien Cumin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crowley_J/0/1/0/all/0/1&quot;&gt;James L. Crowley&lt;/a&gt; (M-PSI)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10127">
<title>Learning interactions to boost human creativity with bandits and GPT-4. (arXiv:2311.10127v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.10127</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper considers how interactions with AI algorithms can boost human
creative thought. We employ a psychological task that demonstrates limits on
human creativity, namely semantic feature generation: given a concept name,
respondents must list as many of its features as possible. Human participants
typically produce only a fraction of the features they know before getting
&quot;stuck.&quot; In experiments with humans and with a language AI (GPT-4) we contrast
behavior in the standard task versus a variant in which participants can ask
for algorithmically-generated hints. Algorithm choice is administered by a
multi-armed bandit whose reward indicates whether the hint helped generating
more features. Humans and the AI show similar benefits from hints, and
remarkably, bandits learning from AI responses prefer the same prompting
strategy as those learning from human behavior. The results suggest that
strategies for boosting human creativity via computer interactions can be
learned by bandits run on groups of simulated participants.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vartanian_A/0/1/0/all/0/1&quot;&gt;Ara Vartanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xiaoxi Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1&quot;&gt;Yun-Shiuan Chuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suresh_S/0/1/0/all/0/1&quot;&gt;Siddharth Suresh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiaojin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rogers_T/0/1/0/all/0/1&quot;&gt;Timothy T. Rogers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10129">
<title>Intelligent Generation of Graphical Game Assets: A Conceptual Framework and Systematic Review of the State of the Art. (arXiv:2311.10129v1 [cs.GR])</title>
<link>http://arxiv.org/abs/2311.10129</link>
<description rdf:parseType="Literal">&lt;p&gt;Procedural content generation (PCG) can be applied to a wide variety of tasks
in games, from narratives, levels and sounds, to trees and weapons. A large
amount of game content is comprised of graphical assets, such as clouds,
buildings or vegetation, that do not require gameplay function considerations.
There is also a breadth of literature examining the procedural generation of
such elements for purposes outside of games. The body of research, focused on
specific methods for generating specific assets, provides a narrow view of the
available possibilities. Hence, it is difficult to have a clear picture of all
approaches and possibilities, with no guide for interested parties to discover
possible methods and approaches for their needs, and no facility to guide them
through each technique or approach to map out the process of using them.
Therefore, a systematic literature review has been conducted, yielding 200
accepted papers. This paper explores state-of-the-art approaches to graphical
asset generation, examining research from a wide range of applications, inside
and outside of games. Informed by the literature, a conceptual framework has
been derived to address the aforementioned gaps.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fukaya_K/0/1/0/all/0/1&quot;&gt;Kaisei Fukaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daylamani_Zad_D/0/1/0/all/0/1&quot;&gt;Damon Daylamani-Zad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agius_H/0/1/0/all/0/1&quot;&gt;Harry Agius&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10177">
<title>Towards Improving Robustness Against Common Corruptions using Mixture of Class Specific Experts. (arXiv:2311.10177v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10177</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks have demonstrated significant accuracy across various
domains, yet their vulnerability to subtle input alterations remains a
persistent challenge. Conventional methods like data augmentation, while
effective to some extent, fall short in addressing unforeseen corruptions,
limiting the adaptability of neural networks in real-world scenarios. In
response, this paper introduces a novel paradigm known as the Mixture of
Class-Specific Expert Architecture. The approach involves disentangling feature
learning for individual classes, offering a nuanced enhancement in scalability
and overall performance. By training dedicated network segments for each class
and subsequently aggregating their outputs, the proposed architecture aims to
mitigate vulnerabilities associated with common neural network structures. The
study underscores the importance of comprehensive evaluation methodologies,
advocating for the incorporation of benchmarks like the common corruptions
benchmark. This inclusion provides nuanced insights into the vulnerabilities of
neural networks, especially concerning their generalization capabilities and
robustness to unforeseen distortions. The research aligns with the broader
objective of advancing the development of highly robust learning systems
capable of nuanced reasoning across diverse and challenging real-world
scenarios. Through this contribution, the paper aims to foster a deeper
understanding of neural network limitations and proposes a practical approach
to enhance their resilience in the face of evolving and unpredictable
conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kotyan_S/0/1/0/all/0/1&quot;&gt;Shashank Kotyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vargas_D/0/1/0/all/0/1&quot;&gt;Danilo Vasconcellos Vargas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10206">
<title>Bayes in the age of intelligent machines. (arXiv:2311.10206v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10206</link>
<description rdf:parseType="Literal">&lt;p&gt;The success of methods based on artificial neural networks in creating
intelligent machines seems like it might pose a challenge to explanations of
human cognition in terms of Bayesian inference. We argue that this is not the
case, and that in fact these systems offer new opportunities for Bayesian
modeling. Specifically, we argue that Bayesian models of cognition and
artificial neural networks lie at different levels of analysis and are
complementary modeling approaches, together offering a way to understand human
cognition that spans these levels. We also argue that the same perspective can
be applied to intelligent machines, where a Bayesian approach may be uniquely
valuable in understanding the behavior of large, opaque artificial neural
networks that are trained on proprietary data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1&quot;&gt;Thomas L. Griffiths&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jian-Qiao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grant_E/0/1/0/all/0/1&quot;&gt;Erin Grant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCoy_R/0/1/0/all/0/1&quot;&gt;R. Thomas McCoy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10215">
<title>Predictive Minds: LLMs As Atypical Active Inference Agents. (arXiv:2311.10215v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.10215</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) like GPT are often conceptualized as passive
predictors, simulators, or even stochastic parrots. We instead conceptualize
LLMs by drawing on the theory of active inference originating in cognitive
science and neuroscience. We examine similarities and differences between
traditional active inference systems and LLMs, leading to the conclusion that,
currently, LLMs lack a tight feedback loop between acting in the world and
perceiving the impacts of their actions, but otherwise fit in the active
inference paradigm. We list reasons why this loop may soon be closed, and
possible consequences of this including enhanced model self-awareness and the
drive to minimize prediction error by changing the world.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulveit_J/0/1/0/all/0/1&quot;&gt;Jan Kulveit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stengel_C/0/1/0/all/0/1&quot;&gt;Clem von Stengel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leventov_R/0/1/0/all/0/1&quot;&gt;Roman Leventov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10217">
<title>A Language and Its Dimensions: Intrinsic Dimensions of Language Fractal Structures. (arXiv:2311.10217v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.10217</link>
<description rdf:parseType="Literal">&lt;p&gt;The present paper introduces a novel object of study - a language fractal
structure. We hypothesize that a set of embeddings of all $n$-grams of a
natural language constitutes a representative sample of this fractal set. (We
use the term Hailonakea to refer to the sum total of all language fractal
structures, over all $n$). The paper estimates intrinsic (genuine) dimensions
of language fractal structures for the Russian and English languages. To this
end, we employ methods based on (1) topological data analysis and (2) a minimum
spanning tree of a data graph for a cloud of points considered (Steele
theorem). For both languages, for all $n$, the intrinsic dimensions appear to
be non-integer values (typical for fractal sets), close to 9 for both of the
Russian and English language.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gromov_V/0/1/0/all/0/1&quot;&gt;Vasilii A. Gromov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borodin_N/0/1/0/all/0/1&quot;&gt;Nikita S. Borodin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yerbolova_A/0/1/0/all/0/1&quot;&gt;Asel S. Yerbolova&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10227">
<title>Think Twice: Perspective-Taking Improves Large Language Models&apos; Theory-of-Mind Capabilities. (arXiv:2311.10227v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.10227</link>
<description rdf:parseType="Literal">&lt;p&gt;Human interactions are deeply rooted in the interplay of thoughts, beliefs,
and desires made possible by Theory of Mind (ToM): our cognitive ability to
understand the mental states of ourselves and others. Although ToM may come
naturally to us, emulating it presents a challenge to even the most advanced
Large Language Models (LLMs). Recent improvements to LLMs&apos; reasoning
capabilities from simple yet effective prompting techniques such as
Chain-of-Thought have seen limited applicability to ToM. In this paper, we turn
to the prominent cognitive science theory &quot;Simulation Theory&quot; to bridge this
gap. We introduce SimToM, a novel two-stage prompting framework inspired by
Simulation Theory&apos;s notion of perspective-taking. To implement this idea on
current ToM benchmarks, SimToM first filters context based on what the
character in question knows before answering a question about their mental
state. Our approach, which requires no additional training and minimal
prompt-tuning, shows substantial improvement over existing methods, and our
analysis reveals the importance of perspective-taking to Theory-of-Mind
capabilities. Our findings suggest perspective-taking as a promising direction
for future research into improving LLMs&apos; ToM capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilf_A/0/1/0/all/0/1&quot;&gt;Alex Wilf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sihyun Shawn Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Paul Pu Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1&quot;&gt;Louis-Philippe Morency&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10228">
<title>A Graphical Model of Hurricane Evacuation Behaviors. (arXiv:2311.10228v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.10228</link>
<description rdf:parseType="Literal">&lt;p&gt;Natural disasters such as hurricanes are increasing and causing widespread
devastation. People&apos;s decisions and actions regarding whether to evacuate or
not are critical and have a large impact on emergency planning and response.
Our interest lies in computationally modeling complex relationships among
various factors influencing evacuation decisions. We conducted a study on the
evacuation of Hurricane Irma of the 2017 Atlantic hurricane season. The study
was guided by the Protection motivation theory (PMT), a widely-used framework
to understand people&apos;s responses to potential threats. Graphical models were
constructed to represent the complex relationships among the factors involved
and the evacuation decision. We evaluated different graphical structures based
on conditional independence tests using Irma data. The final model largely
aligns with PMT. It shows that both risk perception (threat appraisal) and
difficulties in evacuation (coping appraisal) influence evacuation decisions
directly and independently. Certain information received from media was found
to influence risk perception, and through it influence evacuation behaviors
indirectly. In addition, several variables were found to influence both risk
perception and evacuation behaviors directly, including family and friends&apos;
suggestions, neighbors&apos; evacuation behaviors, and evacuation notices from
officials.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hui Sophie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yongsatianchot_N/0/1/0/all/0/1&quot;&gt;Nutchanon Yongsatianchot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marsella_S/0/1/0/all/0/1&quot;&gt;Stacy Marsella&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10234">
<title>The Analysis and Extraction of Structure from Organizational Charts. (arXiv:2311.10234v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10234</link>
<description rdf:parseType="Literal">&lt;p&gt;Organizational charts, also known as org charts, are critical representations
of an organization&apos;s structure and the hierarchical relationships between its
components and positions. However, manually extracting information from org
charts can be error-prone and time-consuming. To solve this, we present an
automated and end-to-end approach that uses computer vision, deep learning, and
natural language processing techniques. Additionally, we propose a metric to
evaluate the completeness and hierarchical accuracy of the extracted
information. This approach has the potential to improve organizational
restructuring and resource utilization by providing a clear and concise
representation of the organizational structure. Our study lays a foundation for
further research on the topic of hierarchical chart analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manali_N/0/1/0/all/0/1&quot;&gt;Nikhil Manali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doermann_D/0/1/0/all/0/1&quot;&gt;David Doermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Desai_M/0/1/0/all/0/1&quot;&gt;Mahesh Desai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10242">
<title>Advancements in Generative AI: A Comprehensive Review of GANs, GPT, Autoencoders, Diffusion Model, and Transformers. (arXiv:2311.10242v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10242</link>
<description rdf:parseType="Literal">&lt;p&gt;The launch of ChatGPT has garnered global attention, marking a significant
milestone in the field of Generative Artificial Intelligence. While Generative
AI has been in effect for the past decade, the introduction of ChatGPT has
ignited a new wave of research and innovation in the AI domain. This surge in
interest has led to the development and release of numerous cutting-edge tools,
such as Bard, Stable Diffusion, DALL-E, Make-A-Video, Runway ML, and Jukebox,
among others. These tools exhibit remarkable capabilities, encompassing tasks
ranging from text generation and music composition, image creation, video
production, code generation, and even scientific work. They are built upon
various state-of-the-art models, including Stable Diffusion, transformer models
like GPT-3 (recent GPT-4), variational autoencoders, and generative adversarial
networks. This advancement in Generative AI presents a wealth of exciting
opportunities and, simultaneously, unprecedented challenges. Throughout this
paper, we have explored these state-of-the-art models, the diverse array of
tasks they can accomplish, the challenges they pose, and the promising future
of Generative Artificial Intelligence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengesi_S/0/1/0/all/0/1&quot;&gt;Staphord Bengesi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+El_Sayed_H/0/1/0/all/0/1&quot;&gt;Hoda El-Sayed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarker_M/0/1/0/all/0/1&quot;&gt;Md Kamruzzaman Sarker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Houkpati_Y/0/1/0/all/0/1&quot;&gt;Yao Houkpati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Irungu_J/0/1/0/all/0/1&quot;&gt;John Irungu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oladunni_T/0/1/0/all/0/1&quot;&gt;Timothy Oladunni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10246">
<title>Surprisal Driven $k$-NN for Robust and Interpretable Nonparametric Learning. (arXiv:2311.10246v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10246</link>
<description rdf:parseType="Literal">&lt;p&gt;Nonparametric learning is a fundamental concept in machine learning that aims
to capture complex patterns and relationships in data without making strong
assumptions about the underlying data distribution. Owing to simplicity and
familiarity, one of the most well-known algorithms under this paradigm is the
$k$-nearest neighbors ($k$-NN) algorithm. Driven by the usage of machine
learning in safety-critical applications, in this work, we shed new light on
the traditional nearest neighbors algorithm from the perspective of information
theory and propose a robust and interpretable framework for tasks such as
classification, regression, and anomaly detection using a single model. Instead
of using a traditional distance measure which needs to be scaled and
contextualized, we use a novel formulation of \textit{surprisal} (amount of
information required to explain the difference between the observed and
expected result). Finally, we demonstrate this architecture&apos;s capability to
perform at-par or above the state-of-the-art on classification, regression, and
anomaly detection tasks using a single model with enhanced interpretability by
providing novel concepts for characterizing data and predictions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1&quot;&gt;Amartya Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hazard_C/0/1/0/all/0/1&quot;&gt;Christopher J. Hazard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beel_J/0/1/0/all/0/1&quot;&gt;Jacob Beel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mack_C/0/1/0/all/0/1&quot;&gt;Cade Mack&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1&quot;&gt;Jack Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Resnick_M/0/1/0/all/0/1&quot;&gt;Michael Resnick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goddin_W/0/1/0/all/0/1&quot;&gt;Will Goddin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10248">
<title>FedTruth: Byzantine-Robust and Backdoor-Resilient Federated Learning Framework. (arXiv:2311.10248v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10248</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) enables collaborative machine learning model training
across multiple parties without sharing raw data. However, FL&apos;s distributed
nature allows malicious clients to impact model training through Byzantine or
backdoor attacks, using erroneous model updates. Existing defenses measure the
deviation of each update from a &apos;ground-truth model update.&apos; They often rely on
a benign root dataset on the server or use trimmed mean or median for clipping,
both methods having limitations.
&lt;/p&gt;
&lt;p&gt;We introduce FedTruth, a robust defense against model poisoning in FL.
FedTruth doesn&apos;t assume specific data distributions nor requires a benign root
dataset. It estimates a global model update with dynamic aggregation weights,
considering contributions from all benign clients. Empirical studies
demonstrate FedTruth&apos;s efficacy in mitigating the impacts of poisoned updates
from both Byzantine and backdoor attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ebron_S/0/1/0/all/0/1&quot;&gt;Sheldon C. Ebron Jr.&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kan Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10269">
<title>Interpretable pap smear cell representation for cervical cancer screening. (arXiv:2311.10269v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10269</link>
<description rdf:parseType="Literal">&lt;p&gt;Screening is critical for prevention and early detection of cervical cancer
but it is time-consuming and laborious. Supervised deep convolutional neural
networks have been developed to automate pap smear screening and the results
are promising. However, the interest in using only normal samples to train deep
neural networks has increased owing to class imbalance problems and
high-labeling costs that are both prevalent in healthcare. In this study, we
introduce a method to learn explainable deep cervical cell representations for
pap smear cytology images based on one class classification using variational
autoencoders. Findings demonstrate that a score can be calculated for cell
abnormality without training models with abnormal samples and localize
abnormality to interpret our results with a novel metric based on absolute
difference in cross entropy in agglomerative clustering. The best model that
discriminates squamous cell carcinoma (SCC) from normals gives 0.908 +- 0.003
area under operating characteristic curve (AUC) and one that discriminates
high-grade epithelial lesion (HSIL) 0.920 +- 0.002 AUC. Compared to other
clustering methods, our method enhances the V-measure and yields higher
homogeneity scores, which more effectively isolate different abnormality
regions, aiding in the interpretation of our results. Evaluation using in-house
and additional open dataset show that our model can discriminate abnormality
without the need of additional training of deep models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ando_Y/0/1/0/all/0/1&quot;&gt;Yu Ando&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+and_N/0/1/0/all/0/1&quot;&gt;Nora Jee-Young Park and&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chong_G/0/1/0/all/0/1&quot;&gt;Gun Oh Chong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ko_S/0/1/0/all/0/1&quot;&gt;Seokhwan Ko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Donghyeon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1&quot;&gt;Junghwan Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1&quot;&gt;Hyungsoo Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10278">
<title>Physics-Enhanced Multi-fidelity Learning for Optical Surface Imprint. (arXiv:2311.10278v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10278</link>
<description rdf:parseType="Literal">&lt;p&gt;Human fingerprints serve as one unique and powerful characteristic for each
person, from which policemen can recognize the identity. Similar to humans,
many natural bodies and intrinsic mechanical qualities can also be uniquely
identified from surface characteristics. To measure the elasto-plastic
properties of one material, one formally sharp indenter is pushed into the
measured body under constant force and retracted, leaving a unique residual
imprint of the minute size from several micrometers to nanometers. However, one
great challenge is how to map the optical image of this residual imprint into
the real wanted mechanical properties, i.e., the tensile force curve. In this
paper, we propose a novel method to use multi-fidelity neural networks (MFNN)
to solve this inverse problem. We first actively train the NN model via pure
simulation data, and then bridge the sim-to-real gap via transfer learning. The
most innovative part is that we use NN to dig out the unknown physics and also
implant the known physics into the transfer learning framework, thus highly
improving the model stability and decreasing the data requirement. This work
serves as one great example of applying machine learning into the real
experimental research, especially under the constraints of data limitation and
fidelity variance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yongchao Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10300">
<title>Supervised structure learning. (arXiv:2311.10300v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10300</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper concerns structure learning or discovery of discrete generative
models. It focuses on Bayesian model selection and the assimilation of training
data or content, with a special emphasis on the order in which data are
ingested. A key move - in the ensuing schemes - is to place priors on the
selection of models, based upon expected free energy. In this setting, expected
free energy reduces to a constrained mutual information, where the constraints
inherit from priors over outcomes (i.e., preferred outcomes). The resulting
scheme is first used to perform image classification on the MNIST dataset to
illustrate the basic idea, and then tested on a more challenging problem of
discovering models with dynamics, using a simple sprite-based visual
disentanglement paradigm and the Tower of Hanoi (cf., blocks world) problem. In
these examples, generative models are constructed autodidactically to recover
(i.e., disentangle) the factorial structure of latent states - and their
characteristic paths or dynamics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Friston_K/0/1/0/all/0/1&quot;&gt;Karl J. Friston&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Costa_L/0/1/0/all/0/1&quot;&gt;Lancelot Da Costa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tschantz_A/0/1/0/all/0/1&quot;&gt;Alexander Tschantz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiefer_A/0/1/0/all/0/1&quot;&gt;Alex Kiefer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salvatori_T/0/1/0/all/0/1&quot;&gt;Tommaso Salvatori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neacsu_V/0/1/0/all/0/1&quot;&gt;Victorita Neacsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koudahl_M/0/1/0/all/0/1&quot;&gt;Magnus Koudahl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heins_C/0/1/0/all/0/1&quot;&gt;Conor Heins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sajid_N/0/1/0/all/0/1&quot;&gt;Noor Sajid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Markovic_D/0/1/0/all/0/1&quot;&gt;Dimitrije Markovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parr_T/0/1/0/all/0/1&quot;&gt;Thomas Parr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verbelen_T/0/1/0/all/0/1&quot;&gt;Tim Verbelen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buckley_C/0/1/0/all/0/1&quot;&gt;Christopher L Buckley&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10319">
<title>Shifting to Machine Supervision: Annotation-Efficient Semi and Self-Supervised Learning for Automatic Medical Image Segmentation and Classification. (arXiv:2311.10319v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10319</link>
<description rdf:parseType="Literal">&lt;p&gt;Advancements in clinical treatment and research are limited by supervised
learning techniques that rely on large amounts of annotated data, an expensive
task requiring many hours of clinical specialists&apos; time. In this paper, we
propose using self-supervised and semi-supervised learning. These techniques
perform an auxiliary task that is label-free, scaling up machine-supervision is
easier compared with fully-supervised techniques. This paper proposes S4MI
(Self-Supervision and Semi-Supervision for Medical Imaging), our pipeline to
leverage advances in self and semi-supervision learning. We benchmark them on
three medical imaging datasets to analyze their efficacy for classification and
segmentation. This advancement in self-supervised learning with 10% annotation
performed better than 100% annotation for the classification of most datasets.
The semi-supervised approach yielded favorable outcomes for segmentation,
outperforming the fully-supervised approach by using 50% fewer labels in all
three datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1&quot;&gt;Pranav Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chukkapalli_R/0/1/0/all/0/1&quot;&gt;Raviteja Chukkapalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhari_S/0/1/0/all/0/1&quot;&gt;Shravan Chaudhari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Luoyao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Jinqian Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smuda_C/0/1/0/all/0/1&quot;&gt;Craig Smuda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cirrone_J/0/1/0/all/0/1&quot;&gt;Jacopo Cirrone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10322">
<title>Clustering Techniques for Stable Linear Dynamical Systems with applications to Hard Disk Drives. (arXiv:2311.10322v1 [eess.SY])</title>
<link>http://arxiv.org/abs/2311.10322</link>
<description rdf:parseType="Literal">&lt;p&gt;In Robust Control and Data Driven Robust Control design methodologies,
multiple plant transfer functions or a family of transfer functions are
considered and a common controller is designed such that all the plants that
fall into this family are stabilized. Though the plants are stabilized, the
controller might be sub-optimal for each of the plants when the variations in
the plants are large. This paper presents a way of clustering stable linear
dynamical systems for the design of robust controllers within each of the
clusters such that the controllers are optimal for each of the clusters. First
a k-medoids algorithm for hard clustering will be presented for stable Linear
Time Invariant (LTI) systems and then a Gaussian Mixture Models (GMM)
clustering for a special class of LTI systems, common for Hard Disk Drive
plants, will be presented.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Prakash_N/0/1/0/all/0/1&quot;&gt;Nikhil Potu Surya Prakash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Seo_J/0/1/0/all/0/1&quot;&gt;Joohwan Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jongeun Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Horowitz_R/0/1/0/all/0/1&quot;&gt;Roberto Horowitz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10329">
<title>High-fidelity Person-centric Subject-to-Image Synthesis. (arXiv:2311.10329v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10329</link>
<description rdf:parseType="Literal">&lt;p&gt;Current subject-driven image generation methods encounter significant
challenges in person-centric image generation. The reason is that they learn
the semantic scene and person generation by fine-tuning a common pre-trained
diffusion, which involves an irreconcilable training imbalance. Precisely, to
generate realistic persons, they need to sufficiently tune the pre-trained
model, which inevitably causes the model to forget the rich semantic scene
prior and makes scene generation over-fit to the training data. Moreover, even
with sufficient fine-tuning, these methods can still not generate high-fidelity
persons since joint learning of the scene and person generation also lead to
quality compromise. In this paper, we propose Face-diffuser, an effective
collaborative generation pipeline to eliminate the above training imbalance and
quality compromise. Specifically, we first develop two specialized pre-trained
diffusion models, i.e., Text-driven Diffusion Model (TDM) and Subject-augmented
Diffusion Model (SDM), for scene and person generation, respectively. The
sampling process is divided into three sequential stages, i.e., semantic scene
construction, subject-scene fusion, and subject enhancement. The first and last
stages are performed by TDM and SDM respectively. The subject-scene fusion
stage, that is the collaboration achieved through a novel and highly effective
mechanism, Saliency-adaptive Noise Fusion (SNF). Specifically, it is based on
our key observation that there exists a robust link between classifier-free
guidance responses and the saliency of generated images. In each time step, SNF
leverages the unique strengths of each model and allows for the spatial
blending of predicted noises from both models automatically in a saliency-aware
manner. Extensive experiments confirm the impressive effectiveness and
robustness of the Face-diffuser.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yibin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weizhong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Jianwei Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1&quot;&gt;Cheng Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10341">
<title>Federated Knowledge Graph Completion via Latent Embedding Sharing and Tensor Factorization. (arXiv:2311.10341v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10341</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge graphs (KGs), which consist of triples, are inherently incomplete
and always require completion procedure to predict missing triples. In
real-world scenarios, KGs are distributed across clients, complicating
completion tasks due to privacy restrictions. Many frameworks have been
proposed to address the issue of federated knowledge graph completion. However,
the existing frameworks, including FedE, FedR, and FEKG, have certain
limitations. = FedE poses a risk of information leakage, FedR&apos;s optimization
efficacy diminishes when there is minimal overlap among relations, and FKGE
suffers from computational costs and mode collapse issues. To address these
issues, we propose a novel method, i.e., Federated Latent Embedding Sharing
Tensor factorization (FLEST), which is a novel approach using federated tensor
factorization for KG completion. FLEST decompose the embedding matrix and
enables sharing of latent dictionary embeddings to lower privacy risks.
Empirical results demonstrate FLEST&apos;s effectiveness and efficiency, offering a
balanced solution between performance and privacy. FLEST expands the
application of federated tensor factorization in KG completion tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Maolin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1&quot;&gt;Dun Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zenglin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1&quot;&gt;Ruocheng Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xiangyu Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10363">
<title>Quantum-Assisted Simulation: A Framework for Designing Machine Learning Models in the Quantum Computing Domain. (arXiv:2311.10363v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2311.10363</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning (ML) models are trained using historical data to classify
new, unseen data. However, traditional computing resources often struggle to
handle the immense amount of data, commonly known as Big Data, within a
reasonable timeframe. Quantum computing (QC) provides a novel approach to
information processing. Quantum algorithms have the potential to process
classical data exponentially faster than classical computing. By mapping
quantum machine learning (QML) algorithms into the quantum mechanical domain,
we can potentially achieve exponential improvements in data processing speed,
reduced resource requirements, and enhanced accuracy and efficiency. In this
article, we delve into both the QC and ML fields, exploring the interplay of
ideas between them, as well as the current capabilities and limitations of
hardware. We investigate the history of quantum computing, examine existing QML
algorithms, and aim to present a simplified procedure for setting up
simulations of QML algorithms, making it accessible and understandable for
readers. Furthermore, we conducted simulations on a dataset using both machine
learning and quantum machine learning approaches. We then proceeded to compare
their respective performances by utilizing a quantum simulator.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Rath_M/0/1/0/all/0/1&quot;&gt;Minati Rath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Date_H/0/1/0/all/0/1&quot;&gt;Hema Date&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10365">
<title>Dates Fruit Disease Recognition using Machine Learning. (arXiv:2311.10365v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10365</link>
<description rdf:parseType="Literal">&lt;p&gt;Many countries such as Saudi Arabia, Morocco and Tunisia are among the top
exporters and consumers of palm date fruits. Date fruit production plays a
major role in the economies of the date fruit exporting countries. Date fruits
are susceptible to disease just like any fruit and early detection and
intervention can end up saving the produce. However, with the vast farming
lands, it is nearly impossible for farmers to observe date trees on a frequent
basis for early disease detection. In addition, even with human observation the
process is prone to human error and increases the date fruit cost. With the
recent advances in computer vision, machine learning, drone technology, and
other technologies; an integrated solution can be proposed for the automatic
detection of date fruit disease. In this paper, a hybrid features based method
with the standard classifiers is proposed based on the extraction of L*a*b
color features, statistical features, and Discrete Wavelet Transform (DWT)
texture features for the early detection and classification of date fruit
disease. A dataset was developed for this work consisting of 871 images divided
into the following classes; Healthy date, Initial stage of disease,
Malnourished date, and Parasite infected. The extracted features were input to
common classifiers such as the Random Forest (RF), Multilayer Perceptron (MLP),
Na\&quot;ive Bayes (NB), and Fuzzy Decision Trees (FDT). The highest average
accuracy was achieved when combining the L*a*b, Statistical, and DWT Features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brahim_G/0/1/0/all/0/1&quot;&gt;Ghassen Ben Brahim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alghazo_J/0/1/0/all/0/1&quot;&gt;Jaafar Alghazo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Latif_G/0/1/0/all/0/1&quot;&gt;Ghazanfar Latif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alnujaidi_K/0/1/0/all/0/1&quot;&gt;Khalid Alnujaidi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10375">
<title>Quantum Data Encoding: A Comparative Analysis of Classical-to-Quantum Mapping Techniques and Their Impact on Machine Learning Accuracy. (arXiv:2311.10375v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2311.10375</link>
<description rdf:parseType="Literal">&lt;p&gt;This research explores the integration of quantum data embedding techniques
into classical machine learning (ML) algorithms, aiming to assess the
performance enhancements and computational implications across a spectrum of
models. We explore various classical-to-quantum mapping methods, ranging from
basis encoding, angle encoding to amplitude encoding for encoding classical
data, we conducted an extensive empirical study encompassing popular ML
algorithms, including Logistic Regression, K-Nearest Neighbors, Support Vector
Machines and ensemble methods like Random Forest, LightGBM, AdaBoost, and
CatBoost. Our findings reveal that quantum data embedding contributes to
improved classification accuracy and F1 scores, particularly notable in models
that inherently benefit from enhanced feature representation. We observed
nuanced effects on running time, with low-complexity models exhibiting moderate
increases and more computationally intensive models experiencing discernible
changes. Notably, ensemble methods demonstrated a favorable balance between
performance gains and computational overhead. This study underscores the
potential of quantum data embedding in enhancing classical ML models and
emphasizes the importance of weighing performance improvements against
computational costs. Future research directions may involve refining quantum
encoding processes to optimize computational efficiency and exploring
scalability for real-world applications. Our work contributes to the growing
body of knowledge at the intersection of quantum computing and classical
machine learning, offering insights for researchers and practitioners seeking
to harness the advantages of quantum-inspired techniques in practical
scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Rath_M/0/1/0/all/0/1&quot;&gt;Minati Rath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Date_H/0/1/0/all/0/1&quot;&gt;Hema Date&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10387">
<title>A Bridge between Dynamical Systems and Machine Learning: Engineered Ordinary Differential Equations as Classification Algorithm (EODECA). (arXiv:2311.10387v1 [cond-mat.dis-nn])</title>
<link>http://arxiv.org/abs/2311.10387</link>
<description rdf:parseType="Literal">&lt;p&gt;In a world increasingly reliant on machine learning, the interpretability of
these models remains a substantial challenge, with many equating their
functionality to an enigmatic black box. This study seeks to bridge machine
learning and dynamical systems. Recognizing the deep parallels between dense
neural networks and dynamical systems, particularly in the light of
non-linearities and successive transformations, this manuscript introduces the
Engineered Ordinary Differential Equations as Classification Algorithms
(EODECAs). Uniquely designed as neural networks underpinned by continuous
ordinary differential equations, EODECAs aim to capitalize on the
well-established toolkit of dynamical systems. Unlike traditional deep learning
models, which often suffer from opacity, EODECAs promise both high
classification performance and intrinsic interpretability. They are naturally
invertible, granting them an edge in understanding and transparency over their
counterparts. By bridging these domains, we hope to usher in a new era of
machine learning models where genuine comprehension of data processes
complements predictive prowess.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Marino_R/0/1/0/all/0/1&quot;&gt;Raffaele Marino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Giambagli_L/0/1/0/all/0/1&quot;&gt;Lorenzo Giambagli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Chicchi_L/0/1/0/all/0/1&quot;&gt;Lorenzo Chicchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Buffoni_L/0/1/0/all/0/1&quot;&gt;Lorenzo Buffoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Fanelli_D/0/1/0/all/0/1&quot;&gt;Duccio Fanelli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10456">
<title>Accurate and Fast Fischer-Tropsch Reaction Microkinetics using PINNs. (arXiv:2311.10456v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10456</link>
<description rdf:parseType="Literal">&lt;p&gt;Microkinetics allows detailed modelling of chemical transformations occurring
in many industrially relevant reactions. Traditional way of solving the
microkinetics model for Fischer-Tropsch synthesis (FTS) becomes inefficient
when it comes to more advanced real-time applications. In this work, we address
these challenges by using physics-informed neural networks(PINNs) for modelling
FTS microkinetics. We propose a computationally efficient and accurate method,
enabling the ultra-fast solution of the existing microkinetics models in
realistic process conditions. The proposed PINN model computes the fraction of
vacant catalytic sites, a key quantity in FTS microkinetics, with median
relative error (MRE) of 0.03%, and the FTS product formation rates with MRE of
0.1%. Compared to conventional equation solvers, the model achieves up to 1E+06
times speed-up when running on GPUs, thus being fast enough for multi-scale and
multi-physics reactor modelling and enabling its applications in real-time
process control and optimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_H/0/1/0/all/0/1&quot;&gt;Harshil Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panda_A/0/1/0/all/0/1&quot;&gt;Aniruddha Panda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikolaienko_T/0/1/0/all/0/1&quot;&gt;Tymofii Nikolaienko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaso_S/0/1/0/all/0/1&quot;&gt;Stanislav Jaso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopez_A/0/1/0/all/0/1&quot;&gt;Alejandro Lopez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalyanaraman_K/0/1/0/all/0/1&quot;&gt;Kaushic Kalyanaraman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10468">
<title>Using Cooperative Game Theory to Prune Neural Networks. (arXiv:2311.10468v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10468</link>
<description rdf:parseType="Literal">&lt;p&gt;We show how solution concepts from cooperative game theory can be used to
tackle the problem of pruning neural networks.
&lt;/p&gt;
&lt;p&gt;The ever-growing size of deep neural networks (DNNs) increases their
performance, but also their computational requirements. We introduce a method
called Game Theory Assisted Pruning (GTAP), which reduces the neural network&apos;s
size while preserving its predictive accuracy. GTAP is based on eliminating
neurons in the network based on an estimation of their joint impact on the
prediction quality through game theoretic solutions. Specifically, we use a
power index akin to the Shapley value or Banzhaf index, tailored using a
procedure similar to Dropout (commonly used to tackle overfitting problems in
machine learning).
&lt;/p&gt;
&lt;p&gt;Empirical evaluation of both feedforward networks and convolutional neural
networks shows that this method outperforms existing approaches in the achieved
tradeoff between the number of parameters and model accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diaz_Ortiz_M/0/1/0/all/0/1&quot;&gt;Mauricio Diaz-Ortiz Jr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kempinski_B/0/1/0/all/0/1&quot;&gt;Benjamin Kempinski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cornelisse_D/0/1/0/all/0/1&quot;&gt;Daphne Cornelisse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bachrach_Y/0/1/0/all/0/1&quot;&gt;Yoram Bachrach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kachman_T/0/1/0/all/0/1&quot;&gt;Tal Kachman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10471">
<title>Regions are Who Walk Them: a Large Pre-trained Spatiotemporal Model Based on Human Mobility for Ubiquitous Urban Sensing. (arXiv:2311.10471v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10471</link>
<description rdf:parseType="Literal">&lt;p&gt;User profiling and region analysis are two tasks of significant commercial
value. However, in practical applications, modeling different features
typically involves four main steps: data preparation, data processing, model
establishment, evaluation, and optimization. This process is time-consuming and
labor-intensive. Repeating this workflow for each feature results in abundant
development time for tasks and a reduced overall volume of task development.
Indeed, human mobility data contains a wealth of information. Several
successful cases suggest that conducting in-depth analysis of population
movement data could potentially yield meaningful profiles about users and
areas. Nonetheless, most related works have not thoroughly utilized the
semantic information within human mobility data and trained on a fixed number
of the regions. To tap into the rich information within population movement,
based on the perspective that Regions Are Who walk them, we propose a large
spatiotemporal model based on trajectories (RAW). It possesses the following
characteristics: 1) Tailored for trajectory data, introducing a GPT-like
structure with a parameter count of up to 1B; 2) Introducing a spatiotemporal
fine-tuning module, interpreting trajectories as collection of users to derive
arbitrary region embedding. This framework allows rapid task development based
on the large spatiotemporal model. We conducted extensive experiments to
validate the effectiveness of our proposed large spatiotemporal model. It&apos;s
evident that our proposed method, relying solely on human mobility data without
additional features, exhibits a certain level of relevance in user profiling
and region analysis. Moreover, our model showcases promising predictive
capabilities in trajectory generation tasks based on the current state,
offering the potential for further innovative work utilizing this large
spatiotemporal model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruixing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1&quot;&gt;Liangzhe Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Leilei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yunqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jibin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_W/0/1/0/all/0/1&quot;&gt;Weifeng Lv&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10500">
<title>From Principle to Practice: Vertical Data Minimization for Machine Learning. (arXiv:2311.10500v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10500</link>
<description rdf:parseType="Literal">&lt;p&gt;Aiming to train and deploy predictive models, organizations collect large
amounts of detailed client data, risking the exposure of private information in
the event of a breach. To mitigate this, policymakers increasingly demand
compliance with the data minimization (DM) principle, restricting data
collection to only that data which is relevant and necessary for the task.
Despite regulatory pressure, the problem of deploying machine learning models
that obey DM has so far received little attention. In this work, we address
this challenge in a comprehensive manner. We propose a novel vertical DM (vDM)
workflow based on data generalization, which by design ensures that no
full-resolution client data is collected during training and deployment of
models, benefiting client privacy by reducing the attack surface in case of a
breach. We formalize and study the corresponding problem of finding
generalizations that both maximize data utility and minimize empirical privacy
risk, which we quantify by introducing a diverse set of policy-aligned
adversarial scenarios. Finally, we propose a range of baseline vDM algorithms,
as well as Privacy-aware Tree (PAT), an especially effective vDM algorithm that
outperforms all baselines across several settings. We plan to release our code
as a publicly available library, helping advance the standardization of DM for
machine learning. Overall, we believe our work can help lay the foundation for
further exploration and adoption of DM principles in real-world applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Staab_R/0/1/0/all/0/1&quot;&gt;Robin Staab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jovanovic_N/0/1/0/all/0/1&quot;&gt;Nikola Jovanovi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balunovic_M/0/1/0/all/0/1&quot;&gt;Mislav Balunovi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vechev_M/0/1/0/all/0/1&quot;&gt;Martin Vechev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10505">
<title>CNL2ASP: converting controlled natural language sentences into ASP. (arXiv:2311.10505v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.10505</link>
<description rdf:parseType="Literal">&lt;p&gt;Answer Set Programming (ASP) is a popular declarative programming language
for solving hard combinatorial problems. Although ASP has gained widespread
acceptance in academic and industrial contexts, there are certain user groups
who may find it more advantageous to employ a higher-level language that
closely resembles natural language when specifying ASP programs. In this paper,
we propose a novel tool, called CNL2ASP, for translating English sentences
expressed in a controlled natural language (CNL) form into ASP. In particular,
we first provide a definition of the type of sentences allowed by our CNL and
their translation as ASP rules, and then exemplify the usage of the CNL for the
specification of both synthetic and real-world combinatorial problems. Finally,
we report the results of an experimental analysis conducted on the real-world
problems to compare the performance of automatically generated encodings with
the ones written by ASP practitioners, showing that our tool can obtain
satisfactory performance on these benchmarks. Under consideration in Theory and
Practice of Logic Programming (TPLP).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caruso_S/0/1/0/all/0/1&quot;&gt;Simone Caruso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dodaro_C/0/1/0/all/0/1&quot;&gt;Carmine Dodaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maratea_M/0/1/0/all/0/1&quot;&gt;Marco Maratea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mochi_M/0/1/0/all/0/1&quot;&gt;Marco Mochi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riccio_F/0/1/0/all/0/1&quot;&gt;Francesco Riccio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10522">
<title>Enhancing Object Coherence in Layout-to-Image Synthesis. (arXiv:2311.10522v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10522</link>
<description rdf:parseType="Literal">&lt;p&gt;Layout-to-image synthesis is an emerging technique in conditional image
generation. It aims to generate complex scenes, where users require fine
control over the layout of the objects in a scene. However, it remains
challenging to control the object coherence, including semantic coherence
(e.g., the cat looks at the flowers or not) and physical coherence (e.g., the
hand and the racket should not be misaligned). In this paper, we propose a
novel diffusion model with effective global semantic fusion (GSF) and
self-similarity feature enhancement modules to guide the object coherence for
this task. For semantic coherence, we argue that the image caption contains
rich information for defining the semantic relationship within the objects in
the images. Instead of simply employing cross-attention between captions and
generated images, which addresses the highly relevant layout restriction and
semantic coherence separately and thus leads to unsatisfying results shown in
our experiments, we develop GSF to fuse the supervision from the layout
restriction and semantic coherence requirement and exploit it to guide the
image synthesis process. Moreover, to improve the physical coherence, we
develop a Self-similarity Coherence Attention (SCA) module to explicitly
integrate local contextual physical coherence into each pixel&apos;s generation
process. Specifically, we adopt a self-similarity map to encode the coherence
restrictions and employ it to extract coherent features from text embedding.
Through visualization of our self-similarity map, we explore the essence of
SCA, revealing that its effectiveness is not only in capturing reliable
physical coherence patterns but also in enhancing complex texture generation.
Extensive experiments demonstrate the superiority of our proposed method in
both image generation quality and controllability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yibin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weizhong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Jianwei Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1&quot;&gt;Cheng Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10537">
<title>MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning. (arXiv:2311.10537v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.10537</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs), despite their remarkable progress across
various general domains, encounter significant barriers in medicine and
healthcare. This field faces unique challenges such as domain-specific
terminologies and the reasoning over specialized knowledge. To address these
obstinate issues, we propose a novel Multi-disciplinary Collaboration (MC)
framework for the medical domain that leverages role-playing LLM-based agents
who participate in a collaborative multi-round discussion, thereby enhancing
LLM proficiency and reasoning capabilities. This training-free and
interpretable framework encompasses five critical steps: gathering domain
experts, proposing individual analyses, summarising these analyses into a
report, iterating over discussions until a consensus is reached, and ultimately
making a decision. Our work particularly focuses on the zero-shot scenario, our
results on nine data sets (MedQA, MedMCQA, PubMedQA, and six subtasks from
MMLU) establish that our proposed MC framework excels at mining and harnessing
the medical expertise in LLMs, as well as extending its reasoning abilities.
Based on these outcomes, we further conduct a human evaluation to pinpoint and
categorize common errors within our method, as well as ablation studies aimed
at understanding the impact of various factors on overall performance. Our code
can be found at \url{https://github.com/gersteinlab/MedAgents}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1&quot;&gt;Xiangru Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_A/0/1/0/all/0/1&quot;&gt;Anni Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhuosheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yilun Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xingyao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1&quot;&gt;Arman Cohan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gerstein_M/0/1/0/all/0/1&quot;&gt;Mark Gerstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10538">
<title>Testing Language Model Agents Safely in the Wild. (arXiv:2311.10538v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.10538</link>
<description rdf:parseType="Literal">&lt;p&gt;A prerequisite for safe autonomy-in-the-wild is safe testing-in-the-wild. Yet
real-world autonomous tests face several unique safety challenges, both due to
the possibility of causing harm during a test, as well as the risk of
encountering new unsafe agent behavior through interactions with real-world and
potentially malicious actors. We propose a framework for conducting safe
autonomous agent tests on the open internet: agent actions are audited by a
context-sensitive monitor that enforces a stringent safety boundary to stop an
unsafe test, with suspect behavior ranked and logged to be examined by humans.
We a design a basic safety monitor that is flexible enough to monitor existing
LLM agents, and, using an adversarial simulated agent, we measure its ability
to identify and stop unsafe situations. Then we apply the safety monitor on a
battery of real-world tests of AutoGPT, and we identify several limitations and
challenges that will face the creation of safe in-the-wild tests as autonomous
agents grow more capable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naihin_S/0/1/0/all/0/1&quot;&gt;Silen Naihin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atkinson_D/0/1/0/all/0/1&quot;&gt;David Atkinson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Green_M/0/1/0/all/0/1&quot;&gt;Marc Green&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamadi_M/0/1/0/all/0/1&quot;&gt;Merwane Hamadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swift_C/0/1/0/all/0/1&quot;&gt;Craig Swift&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schonholtz_D/0/1/0/all/0/1&quot;&gt;Douglas Schonholtz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalai_A/0/1/0/all/0/1&quot;&gt;Adam Tauman Kalai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1&quot;&gt;David Bau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10590">
<title>EduGym: An Environment Suite for Reinforcement Learning Education. (arXiv:2311.10590v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10590</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the empirical success of reinforcement learning, an increasing number
of students study the subject. However, from our practical teaching experience,
we see students entering the field (bachelor, master and early PhD) often
struggle. On the one hand, textbooks and (online) lectures provide the
fundamentals, but students find it hard to translate between equations and
code. On the other hand, public codebases do provide practical examples, but
the implemented algorithms tend to be complex, and the underlying test
environments contain multiple reinforcement learning challenges at once.
Although this is realistic from a research perspective, it often hinders
educational conceptual understanding. To solve this issue we introduce EduGym,
a set of educational reinforcement learning environments and associated
interactive notebooks tailored for education. Each EduGym environment is
specifically designed to illustrate a certain aspect/challenge of reinforcement
learning (e.g., exploration, partial observability, stochasticity, etc.), while
the associated interactive notebook explains the challenge and its possible
solution approaches, connecting equations and code in a single document. An
evaluation among RL students and researchers shows 86% of them think EduGym is
a useful tool for reinforcement learning education. All notebooks are available
from https://sites.google.com/view/edu-gym/home, while the full software
package can be installed from https://github.com/RLG-Leiden/edugym.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moerland_T/0/1/0/all/0/1&quot;&gt;Thomas M. Moerland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_Brockhausen_M/0/1/0/all/0/1&quot;&gt;Matthias M&amp;#xfc;ller-Brockhausen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernatavicius_A/0/1/0/all/0/1&quot;&gt;Andrius Bernatavicius&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ponse_K/0/1/0/all/0/1&quot;&gt;Koen Ponse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kouwenhoven_T/0/1/0/all/0/1&quot;&gt;Tom Kouwenhoven&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sauter_A/0/1/0/all/0/1&quot;&gt;Andreas Sauter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meer_M/0/1/0/all/0/1&quot;&gt;Michiel van der Meer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Renting_B/0/1/0/all/0/1&quot;&gt;Bram Renting&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plaat_A/0/1/0/all/0/1&quot;&gt;Aske Plaat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10591">
<title>FOCAL: A Cost-Aware Video Dataset for Active Learning. (arXiv:2311.10591v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10591</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce the FOCAL (Ford-OLIVES Collaboration on Active
Learning) dataset which enables the study of the impact of annotation-cost
within a video active learning setting. Annotation-cost refers to the time it
takes an annotator to label and quality-assure a given video sequence. A
practical motivation for active learning research is to minimize
annotation-cost by selectively labeling informative samples that will maximize
performance within a given budget constraint. However, previous work in video
active learning lacks real-time annotation labels for accurately assessing cost
minimization and instead operates under the assumption that annotation-cost
scales linearly with the amount of data to annotate. This assumption does not
take into account a variety of real-world confounding factors that contribute
to a nonlinear cost such as the effect of an assistive labeling tool and the
variety of interactions within a scene such as occluded objects, weather, and
motion of objects. FOCAL addresses this discrepancy by providing real
annotation-cost labels for 126 video sequences across 69 unique city scenes
with a variety of weather, lighting, and seasonal conditions. We also introduce
a set of conformal active learning algorithms that take advantage of the
sequential structure of video data in order to achieve a better trade-off
between annotation-cost and performance while also reducing floating point
operations (FLOPS) overhead by at least 77.67%. We show how these approaches
better reflect how annotations on videos are done in practice through a
sequence selection framework. We further demonstrate the advantage of these
approaches by introducing two performance-cost metrics and show that the best
conformal active learning method is cheaper than the best traditional active
learning method by 113 hours.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kokilepersaud_K/0/1/0/all/0/1&quot;&gt;Kiran Kokilepersaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Logan_Y/0/1/0/all/0/1&quot;&gt;Yash-Yee Logan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benkert_R/0/1/0/all/0/1&quot;&gt;Ryan Benkert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabhushankar_M/0/1/0/all/0/1&quot;&gt;Mohit Prabhushankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+AlRegib_G/0/1/0/all/0/1&quot;&gt;Ghassan AlRegib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corona_E/0/1/0/all/0/1&quot;&gt;Enrique Corona&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1&quot;&gt;Kunjan Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parchami_M/0/1/0/all/0/1&quot;&gt;Mostafa Parchami&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10596">
<title>Hashing it Out: Predicting Unhealthy Conversations on Twitter. (arXiv:2311.10596v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.10596</link>
<description rdf:parseType="Literal">&lt;p&gt;Personal attacks in the context of social media conversations often lead to
fast-paced derailment, leading to even more harmful exchanges being made.
State-of-the-art systems for the detection of such conversational derailment
often make use of deep learning approaches for prediction purposes. In this
paper, we show that an Attention-based BERT architecture, pre-trained on a
large Twitter corpus and fine-tuned on our task, is efficient and effective in
making such predictions. This model shows clear advantages in performance to
the existing LSTM model we use as a baseline. Additionally, we show that this
impressive performance can be attained through fine-tuning on a relatively
small, novel dataset, particularly after mitigating overfitting issues through
synthetic oversampling techniques. By introducing the first transformer based
model for forecasting conversational events on Twitter, this work lays the
foundation for a practical tool to encourage better interactions on one of the
most ubiquitous social media platforms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leung_S/0/1/0/all/0/1&quot;&gt;Steven Leung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papapolyzos_F/0/1/0/all/0/1&quot;&gt;Filippos Papapolyzos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10597">
<title>Designing Reconfigurable Intelligent Systems with Markov Blankets. (arXiv:2311.10597v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2311.10597</link>
<description rdf:parseType="Literal">&lt;p&gt;Compute Continuum (CC) systems comprise a vast number of devices distributed
over computational tiers. Evaluating business requirements, i.e., Service Level
Objectives (SLOs), requires collecting data from all those devices; if SLOs are
violated, devices must be reconfigured to ensure correct operation. If done
centrally, this dramatically increases the number of devices and variables that
must be considered, while creating an enormous communication overhead. To
address this, we (1) introduce a causality filter based on Markov blankets (MB)
that limits the number of variables that each device must track, (2) evaluate
SLOs decentralized on a device basis, and (3) infer optimal device
configuration for fulfilling SLOs. We evaluated our methodology by analyzing
video stream transformations and providing device configurations that ensure
the Quality of Service (QoS). The devices thus perceived their environment and
acted accordingly -- a form of decentralized intelligence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sedlak_B/0/1/0/all/0/1&quot;&gt;Boris Sedlak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pujol_V/0/1/0/all/0/1&quot;&gt;Victor Casamayor Pujol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Donta_P/0/1/0/all/0/1&quot;&gt;Praveen Kumar Donta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dustdar_S/0/1/0/all/0/1&quot;&gt;Schahram Dustdar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10599">
<title>Chatbots as social companions: How people perceive consciousness, human likeness, and social health benefits in machines. (arXiv:2311.10599v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2311.10599</link>
<description rdf:parseType="Literal">&lt;p&gt;As artificial intelligence (AI) becomes more widespread, one question that
arises is how human-AI interaction might impact human-human interaction.
Chatbots, for example, are increasingly used as social companions, but little
is known about how their use impacts human relationships. A common hypothesis
is that these companion bots are detrimental to social health by harming or
replacing human interaction. To understand how companion bots impact social
health, we studied people who used companion bots and people who did not.
Contrary to expectations, companion bot users indicated that these
relationships were beneficial to their social health, whereas nonusers viewed
them as harmful. Another common assumption is that people perceive conscious,
humanlike AI as disturbing and threatening. Among both users and nonusers,
however, we found the opposite: perceiving companion bots as more conscious and
humanlike correlated with more positive opinions and better social health
benefits. Humanlike bots may aid social health by supplying reliable and safe
interactions, without necessarily harming human relationships.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guingrich_R/0/1/0/all/0/1&quot;&gt;Rose Guingrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Graziano_M/0/1/0/all/0/1&quot;&gt;Michael S. A. Graziano&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10607">
<title>Active Inference on the Edge: A Design Study. (arXiv:2311.10607v1 [eess.SY])</title>
<link>http://arxiv.org/abs/2311.10607</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine Learning (ML) is a common tool to interpret and predict the behavior
of distributed computing systems, e.g., to optimize the task distribution
between devices. As more and more data is created by Internet of Things (IoT)
devices, data processing and ML training are carried out by edge devices in
close proximity. To ensure Quality of Service (QoS) throughout these
operations, systems are supervised and dynamically adapted with the help of ML.
However, as long as ML models are not retrained, they fail to capture gradual
shifts in the variable distribution, leading to an inaccurate view of the
system state. Moreover, as the prediction accuracy decreases, the reporting
device should actively resolve uncertainties to improve the model&apos;s precision.
Such a level of self-determination could be provided by Active Inference (ACI)
-- a concept from neuroscience that describes how the brain constantly predicts
and evaluates sensory information to decrease long-term surprise. We
encompassed these concepts in a single action-perception cycle, which we
implemented for distributed agents in a smart manufacturing use case. As a
result, we showed how our ACI agent was able to quickly and traceably solve an
optimization problem while fulfilling QoS requirements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sedlak_B/0/1/0/all/0/1&quot;&gt;Boris Sedlak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pujol_V/0/1/0/all/0/1&quot;&gt;Victor Casamayor Pujol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Donta_P/0/1/0/all/0/1&quot;&gt;Praveen Kumar Donta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dustdar_S/0/1/0/all/0/1&quot;&gt;Schahram Dustdar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10614">
<title>A Self-enhancement Approach for Domain-specific Chatbot Training via Knowledge Mining and Digest. (arXiv:2311.10614v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.10614</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs), despite their great power in language
generation, often encounter challenges when dealing with intricate and
knowledge-demanding queries in specific domains. This paper introduces a novel
approach to enhance LLMs by effectively extracting the relevant knowledge from
domain-specific textual sources, and the adaptive training of a chatbot with
domain-specific inquiries. Our two-step approach starts from training a
knowledge miner, namely LLMiner, which autonomously extracts Question-Answer
pairs from relevant documents through a chain-of-thought reasoning process.
Subsequently, we blend the mined QA pairs with a conversational dataset to
fine-tune the LLM as a chatbot, thereby enriching its domain-specific expertise
and conversational capabilities. We also developed a new evaluation benchmark
which comprises four domain-specific text corpora and associated human-crafted
QA pairs for testing. Our model shows remarkable performance improvement over
generally aligned LLM and surpasses domain-adapted models directly fine-tuned
on domain corpus. In particular, LLMiner achieves this with minimal human
intervention, requiring only 600 seed instances, thereby providing a pathway
towards self-improvement of LLMs through model-synthesized training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruohong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1&quot;&gt;Luyu Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Chen Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1&quot;&gt;Zhen Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_G/0/1/0/all/0/1&quot;&gt;Guokun Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ai_F/0/1/0/all/0/1&quot;&gt;Fangzhou Ai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yiming Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hongxia Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10638">
<title>Concept-free Causal Disentanglement with Variational Graph Auto-Encoder. (arXiv:2311.10638v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10638</link>
<description rdf:parseType="Literal">&lt;p&gt;In disentangled representation learning, the goal is to achieve a compact
representation that consists of all interpretable generative factors in the
observational data. Learning disentangled representations for graphs becomes
increasingly important as graph data rapidly grows. Existing approaches often
rely on Variational Auto-Encoder (VAE) or its causal structure learning-based
refinement, which suffer from sub-optimality in VAEs due to the independence
factor assumption and unavailability of concept labels, respectively. In this
paper, we propose an unsupervised solution, dubbed concept-free causal
disentanglement, built on a theoretically provable tight upper bound
approximating the optimal factor. This results in an SCM-like causal structure
modeling that directly learns concept structures from data. Based on this idea,
we propose Concept-free Causal VGAE (CCVGAE) by incorporating a novel causal
disentanglement layer into Variational Graph Auto-Encoder. Furthermore, we
prove concept consistency under our concept-free causal disentanglement
framework, hence employing it to enhance the meta-learning framework, called
concept-free causal Meta-Graph (CC-Meta-Graph). We conduct extensive
experiments to demonstrate the superiority of the proposed models: CCVGAE and
CC-Meta-Graph, reaching up to $29\%$ and $11\%$ absolute improvements over
baselines in terms of AUC, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jingyun Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lili Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10640">
<title>Multi-delay arterial spin-labeled perfusion estimation with biophysics simulation and deep learning. (arXiv:2311.10640v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/2311.10640</link>
<description rdf:parseType="Literal">&lt;p&gt;Purpose: To develop biophysics-based method for estimating perfusion Q from
arterial spin labeling (ASL) images using deep learning. Methods: A 3D U-Net
(QTMnet) was trained to estimate perfusion from 4D tracer propagation images.
The network was trained and tested on simulated 4D tracer concentration data
based on artificial vasculature structure generated by constrained constructive
optimization (CCO) method. The trained network was further tested in a
synthetic brain ASL image based on vasculature network extracted from magnetic
resonance (MR) angiography. The estimations from both trained network and a
conventional kinetic model were compared in ASL images acquired from eight
healthy volunteers. Results: QTMnet accurately reconstructed perfusion Q from
concentration data. Relative error of the synthetic brain ASL image was 7.04%
for perfusion Q, lower than the error using single-delay ASL model: 25.15% for
Q, and multi-delay ASL model: 12.62% for perfusion Q. Conclusion: QTMnet
provides accurate estimation on perfusion parameters and is a promising
approach as a clinical ASL MRI image processing pipeline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hu_R/0/1/0/all/0/1&quot;&gt;Renjiu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qihao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Spincemaille_P/0/1/0/all/0/1&quot;&gt;Pascal Spincemaille&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Thanh D. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10671">
<title>Fuse It or Lose It: Deep Fusion for Multimodal Simulation-Based Inference. (arXiv:2311.10671v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10671</link>
<description rdf:parseType="Literal">&lt;p&gt;We present multimodal neural posterior estimation (MultiNPE), a method to
integrate heterogeneous data from different sources in simulation-based
inference with neural networks. Inspired by advances in attention-based deep
fusion learning, it empowers researchers to analyze data from different domains
and infer the parameters of complex mathematical models with increased
accuracy. We formulate different multimodal fusion approaches for MultiNPE
(early, late, and hybrid) and evaluate their performance in three challenging
numerical experiments. MultiNPE not only outperforms na\&quot;ive baselines on a
benchmark model, but also achieves superior inference on representative
scientific models from neuroscience and cardiology. In addition, we
systematically investigate the impact of partially missing data on the
different fusion strategies. Across our different experiments, late and hybrid
fusion techniques emerge as the methods of choice for practical applications of
multimodal simulation-based inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmitt_M/0/1/0/all/0/1&quot;&gt;Marvin Schmitt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radev_S/0/1/0/all/0/1&quot;&gt;Stefan T. Radev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burkner_P/0/1/0/all/0/1&quot;&gt;Paul-Christian B&amp;#xfc;rkner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10678">
<title>Distilling and Retrieving Generalizable Knowledge for Robot Manipulation via Language Corrections. (arXiv:2311.10678v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.10678</link>
<description rdf:parseType="Literal">&lt;p&gt;Today&apos;s robot policies exhibit subpar performance when faced with the
challenge of generalizing to novel environments. Human corrective feedback is a
crucial form of guidance to enable such generalization. However, adapting to
and learning from online human corrections is a non-trivial endeavor: not only
do robots need to remember human feedback over time to retrieve the right
information in new settings and reduce the intervention rate, but also they
would need to be able to respond to feedback that can be arbitrary corrections
about high-level human preferences to low-level adjustments to skill
parameters. In this work, we present Distillation and Retrieval of Online
Corrections (DROC), a large language model (LLM)-based system that can respond
to arbitrary forms of language feedback, distill generalizable knowledge from
corrections, and retrieve relevant past experiences based on textual and visual
similarity for improving performance in novel settings. DROC is able to respond
to a sequence of online language corrections that address failures in both
high-level task plans and low-level skill primitives. We demonstrate that DROC
effectively distills the relevant information from the sequence of online
corrections in a knowledge base and retrieves that knowledge in settings with
new task or object instances. DROC outperforms other techniques that directly
generate robot code via LLMs by using only half of the total number of
corrections needed in the first round and requires little to no corrections
after two iterations. We show further results, videos, prompts and code on
https://sites.google.com/stanford.edu/droc .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_L/0/1/0/all/0/1&quot;&gt;Lihan Zha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1&quot;&gt;Yuchen Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Li-Heng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_M/0/1/0/all/0/1&quot;&gt;Minae Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arenas_M/0/1/0/all/0/1&quot;&gt;Montserrat Gonzalez Arenas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1&quot;&gt;Andy Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1&quot;&gt;Fei Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1&quot;&gt;Dorsa Sadigh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10697">
<title>PEFT-MedAware: Large Language Model for Medical Awareness. (arXiv:2311.10697v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.10697</link>
<description rdf:parseType="Literal">&lt;p&gt;Chat models are capable of answering a wide range of questions, however, the
accuracy of their responses is highly uncertain. In this research, we propose a
specialized PEFT-MedAware model where we utilize parameter-efficient
fine-tuning (PEFT) to enhance the Falcon-1b large language model on specialized
MedQuAD data consisting of 16,407 medical QA pairs, leveraging only 0.44% of
its trainable parameters to enhance computational efficiency. The paper adopts
data preprocessing and PEFT to optimize model performance, complemented by a
BitsAndBytesConfig for efficient transformer training. The resulting model was
capable of outperforming other LLMs in medical question-answering tasks in
specific domains with greater accuracy utilizing limited computational
resources making it suitable for deployment in resource-constrained
environments. We propose further improvements through expanded datasets, larger
models, and feedback mechanisms for sustained medical relevancy. Our work
highlights the efficiency gains and specialized capabilities of PEFT in medical
AI, outpacing standard models in precision without extensive resource demands.
The proposed model and data are released for research purposes only.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandya_K/0/1/0/all/0/1&quot;&gt;Keivalya Pandya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10699">
<title>Using linear initialisation to improve speed of convergence and fully-trained error in Autoencoders. (arXiv:2311.10699v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10699</link>
<description rdf:parseType="Literal">&lt;p&gt;Good weight initialisation is an important step in successful training of
Artificial Neural Networks. Over time a number of improvements have been
proposed to this process. In this paper we introduce a novel weight
initialisation technique called the Straddled Matrix Initialiser. This
initialisation technique is motivated by our assumption that major,
global-scale relationships in data are linear with only smaller effects
requiring complex non-linearities. Combination of Straddled Matrix and ReLU
activation function initialises a Neural Network as a de facto linear model,
which we postulate should be a better starting point for optimisation given our
assumptions. We test this by training autoencoders on three datasets using
Straddled Matrix and seven other state-of-the-art weight initialisation
techniques. In all our experiments the Straddeled Matrix Initialiser clearly
outperforms all other methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marais_M/0/1/0/all/0/1&quot;&gt;Marcel Marais&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hartstein_M/0/1/0/all/0/1&quot;&gt;Mate Hartstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cevora_G/0/1/0/all/0/1&quot;&gt;George Cevora&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10709">
<title>Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning. (arXiv:2311.10709v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10709</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Emu Video, a text-to-video generation model that factorizes the
generation into two steps: first generating an image conditioned on the text,
and then generating a video conditioned on the text and the generated image. We
identify critical design decisions--adjusted noise schedules for diffusion, and
multi-stage training--that enable us to directly generate high quality and high
resolution videos, without requiring a deep cascade of models as in prior work.
In human evaluations, our generated videos are strongly preferred in quality
compared to all prior work--81% vs. Google&apos;s Imagen Video, 90% vs. Nvidia&apos;s
PYOCO, and 96% vs. Meta&apos;s Make-A-Video. Our model outperforms commercial
solutions such as RunwayML&apos;s Gen2 and Pika Labs. Finally, our factorizing
approach naturally lends itself to animating images based on a user&apos;s text
prompt, where our generations are preferred 96% over prior work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Girdhar_R/0/1/0/all/0/1&quot;&gt;Rohit Girdhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1&quot;&gt;Mannat Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_A/0/1/0/all/0/1&quot;&gt;Andrew Brown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duval_Q/0/1/0/all/0/1&quot;&gt;Quentin Duval&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azadi_S/0/1/0/all/0/1&quot;&gt;Samaneh Azadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rambhatla_S/0/1/0/all/0/1&quot;&gt;Sai Saketh Rambhatla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1&quot;&gt;Akbar Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1&quot;&gt;Xi Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1&quot;&gt;Devi Parikh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1&quot;&gt;Ishan Misra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2010.05995">
<title>A Skew-Sensitive Evaluation Framework for Imbalanced Data Classification. (arXiv:2010.05995v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2010.05995</link>
<description rdf:parseType="Literal">&lt;p&gt;Class distribution skews in imbalanced datasets may lead to models with
prediction bias towards majority classes, making fair assessment of classifiers
a challenging task. Metrics such as Balanced Accuracy are commonly used to
evaluate a classifier&apos;s prediction performance under such scenarios. However,
these metrics fall short when classes vary in importance. In this paper, we
propose a simple and general-purpose evaluation framework for imbalanced data
classification that is sensitive to arbitrary skews in class cardinalities and
importances. Experiments with several state-of-the-art classifiers tested on
real-world datasets from three different domains show the effectiveness of our
framework - not only in evaluating and ranking classifiers, but also training
them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1&quot;&gt;Min Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tatbul_N/0/1/0/all/0/1&quot;&gt;Nesime Tatbul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rivers_B/0/1/0/all/0/1&quot;&gt;Brian Rivers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Akhilesh Kumar Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1&quot;&gt;Lucas Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marcus_R/0/1/0/all/0/1&quot;&gt;Ryan Marcus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shengtian Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1&quot;&gt;Insup Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gottschlich_J/0/1/0/all/0/1&quot;&gt;Justin Gottschlich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2109.05439">
<title>Concave Utility Reinforcement Learning with Zero-Constraint Violations. (arXiv:2109.05439v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2109.05439</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of tabular infinite horizon concave utility
reinforcement learning (CURL) with convex constraints. For this, we propose a
model-based learning algorithm that also achieves zero constraint violations.
Assuming that the concave objective and the convex constraints have a solution
interior to the set of feasible occupation measures, we solve a tighter
optimization problem to ensure that the constraints are never violated despite
the imprecise model knowledge and model stochasticity. We use Bellman
error-based analysis for tabular infinite-horizon setups which allows analyzing
stochastic policies. Combining the Bellman error-based analysis and tighter
optimization equation, for $T$ interactions with the environment, we obtain a
high-probability regret guarantee for objective which grows as
$\Tilde{O}(1/\sqrt{T})$, excluding other factors. The proposed method can be
applied for optimistic algorithms to obtain high-probability regret bounds and
also be used for posterior sampling algorithms to obtain a loose Bayesian
regret bounds but with significant improvement in computational complexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_M/0/1/0/all/0/1&quot;&gt;Mridul Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Q/0/1/0/all/0/1&quot;&gt;Qinbo Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aggarwal_V/0/1/0/all/0/1&quot;&gt;Vaneet Aggarwal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.06522">
<title>Graph Enhanced BERT for Query Understanding. (arXiv:2204.06522v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2204.06522</link>
<description rdf:parseType="Literal">&lt;p&gt;Query understanding plays a key role in exploring users&apos; search intents and
facilitating users to locate their most desired information. However, it is
inherently challenging since it needs to capture semantic information from
short and ambiguous queries and often requires massive task-specific labeled
data. In recent years, pre-trained language models (PLMs) have advanced various
natural language processing tasks because they can extract general semantic
information from large-scale corpora. Therefore, there are unprecedented
opportunities to adopt PLMs for query understanding. However, there is a gap
between the goal of query understanding and existing pre-training strategies --
the goal of query understanding is to boost search performance while existing
strategies rarely consider this goal. Thus, directly applying them to query
understanding is sub-optimal. On the other hand, search logs contain user
clicks between queries and urls that provide rich users&apos; search behavioral
information on queries beyond their content. Therefore, in this paper, we aim
to fill this gap by exploring search logs. In particular, to incorporate search
logs into pre-training, we first construct a query graph where nodes are
queries and two queries are connected if they lead to clicks on the same urls.
Then we propose a novel graph-enhanced pre-training framework, GE-BERT, which
can leverage both query content and the query graph. In other words, GE-BERT
can capture both the semantic information and the users&apos; search behavioral
information of queries. Extensive experiments on various query understanding
tasks have demonstrated the effectiveness of the proposed framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Juanhui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1&quot;&gt;Wei Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;Suqi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiliang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuaiqiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1&quot;&gt;Dawei Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.09603">
<title>Comparing Deep Reinforcement Learning Algorithms in Two-Echelon Supply Chains. (arXiv:2204.09603v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2204.09603</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we analyze and compare the performance of state-of-the-art
deep reinforcement learning algorithms for solving the supply chain inventory
management problem. This complex sequential decision-making problem consists of
determining the optimal quantity of products to be produced and shipped across
different warehouses over a given time horizon. In particular, we present a
mathematical formulation of a two-echelon supply chain environment with
stochastic and seasonal demand, which allows managing an arbitrary number of
warehouses and product types. Through a rich set of numerical experiments, we
compare the performance of different deep reinforcement learning algorithms
under various supply chain structures, topologies, demands, capacities, and
costs. The results of the experimental plan indicate that deep reinforcement
learning algorithms outperform traditional inventory management strategies,
such as the static (s, Q)-policy. Furthermore, this study provides detailed
insight into the design and development of an open-source software library that
provides a customizable environment for solving the supply chain inventory
management problem using a wide range of data-driven approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stranieri_F/0/1/0/all/0/1&quot;&gt;Francesco Stranieri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stella_F/0/1/0/all/0/1&quot;&gt;Fabio Stella&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.12723">
<title>A Faithful Deep Sensitivity Estimation for Accelerated Magnetic Resonance Imaging. (arXiv:2210.12723v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.12723</link>
<description rdf:parseType="Literal">&lt;p&gt;Magnetic resonance imaging (MRI) is an essential diagnostic tool that suffers
from prolonged scan time. To alleviate this limitation, advanced fast MRI
technology attracts extensive research interests. Recent deep learning has
shown its great potential in improving image quality and reconstruction speed.
Faithful coil sensitivity estimation is vital for MRI reconstruction. However,
most deep learning methods still rely on pre-estimated sensitivity maps and
ignore their inaccuracy, resulting in the significant quality degradation of
reconstructed images. In this work, we propose a Joint Deep Sensitivity
estimation and Image reconstruction network, called JDSI. During the image
artifacts removal, it gradually provides more faithful sensitivity maps with
high-frequency information, leading to improved image reconstructions. To
understand the behavior of the network, the mutual promotion of sensitivity
estimation and image reconstruction is revealed through the visualization of
network intermediate results. Results on in vivo datasets and radiologist
reader study demonstrate that, for both calibration-based and calibrationless
reconstruction, the proposed JDSI achieves the state-of-the-art performance
visually and quantitatively, especially when the acceleration factor is high.
Additionally, JDSI owns nice robustness to patients and autocalibration
signals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fang_H/0/1/0/all/0/1&quot;&gt;Haoming Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qian_C/0/1/0/all/0/1&quot;&gt;Chen Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Boxuan Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bao_L/0/1/0/all/0/1&quot;&gt;Lijun Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Liuhong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jianjun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wei_W/0/1/0/all/0/1&quot;&gt;Wenping Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jianzhong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guo_D/0/1/0/all/0/1&quot;&gt;Di Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qu_X/0/1/0/all/0/1&quot;&gt;Xiaobo Qu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.01913">
<title>Bespoke: A Block-Level Neural Network Optimization Framework for Low-Cost Deployment. (arXiv:2303.01913v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.01913</link>
<description rdf:parseType="Literal">&lt;p&gt;As deep learning models become popular, there is a lot of need for deploying
them to diverse device environments. Because it is costly to develop and
optimize a neural network for every single environment, there is a line of
research to search neural networks for multiple target environments
efficiently. However, existing works for such a situation still suffer from
requiring many GPUs and expensive costs. Motivated by this, we propose a novel
neural network optimization framework named Bespoke for low-cost deployment.
Our framework searches for a lightweight model by replacing parts of an
original model with randomly selected alternatives, each of which comes from a
pretrained neural network or the original model. In the practical sense,
Bespoke has two significant merits. One is that it requires near zero cost for
designing the search space of neural networks. The other merit is that it
exploits the sub-networks of public pretrained neural networks, so the total
cost is minimal compared to the existing works. We conduct experiments
exploring Bespoke&apos;s the merits, and the results show that it finds efficient
models for multiple targets with meager cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jong-Ryul Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_Y/0/1/0/all/0/1&quot;&gt;Yong-Hyuk Moon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.14889">
<title>Model-Based Reinforcement Learning with Isolated Imaginations. (arXiv:2303.14889v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.14889</link>
<description rdf:parseType="Literal">&lt;p&gt;World models learn the consequences of actions in vision-based interactive
systems. However, in practical scenarios like autonomous driving,
noncontrollable dynamics that are independent or sparsely dependent on action
signals often exist, making it challenging to learn effective world models. To
address this issue, we propose Iso-Dream++, a model-based reinforcement
learning approach that has two main contributions. First, we optimize the
inverse dynamics to encourage the world model to isolate controllable state
transitions from the mixed spatiotemporal variations of the environment.
Second, we perform policy optimization based on the decoupled latent
imaginations, where we roll out noncontrollable states into the future and
adaptively associate them with the current controllable state. This enables
long-horizon visuomotor control tasks to benefit from isolating mixed dynamics
sources in the wild, such as self-driving cars that can anticipate the movement
of other vehicles, thereby avoiding potential risks. On top of our previous
work, we further consider the sparse dependencies between controllable and
noncontrollable states, address the training collapse problem of state
decoupling, and validate our approach in transfer learning setups. Our
empirical study demonstrates that Iso-Dream++ outperforms existing
reinforcement learning models significantly on CARLA and DeepMind Control.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_M/0/1/0/all/0/1&quot;&gt;Minting Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiangming Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yitao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunbo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaokang Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.16047">
<title>Exploring and Interacting with the Set of Good Sparse Generalized Additive Models. (arXiv:2303.16047v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.16047</link>
<description rdf:parseType="Literal">&lt;p&gt;In real applications, interaction between machine learning models and domain
experts is critical; however, the classical machine learning paradigm that
usually produces only a single model does not facilitate such interaction.
Approximating and exploring the Rashomon set, i.e., the set of all near-optimal
models, addresses this practical challenge by providing the user with a
searchable space containing a diverse set of models from which domain experts
can choose. We present algorithms to efficiently and accurately approximate the
Rashomon set of sparse, generalized additive models with ellipsoids for fixed
support sets and use these ellipsoids to approximate Rashomon sets for many
different support sets. The approximated Rashomon set serves as a cornerstone
to solve practical challenges such as (1) studying the variable importance for
the model class; (2) finding models under user-specified constraints
(monotonicity, direct editing); and (3) investigating sudden changes in the
shape functions. Experiments demonstrate the fidelity of the approximated
Rashomon set and its effectiveness in solving practical challenges.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1&quot;&gt;Chudi Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiachang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seltzer_M/0/1/0/all/0/1&quot;&gt;Margo Seltzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rudin_C/0/1/0/all/0/1&quot;&gt;Cynthia Rudin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.17491">
<title>Language Models can Solve Computer Tasks. (arXiv:2303.17491v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2303.17491</link>
<description rdf:parseType="Literal">&lt;p&gt;Agents capable of carrying out general tasks on a computer can improve
efficiency and productivity by automating repetitive tasks and assisting in
complex problem-solving. Ideally, such agents should be able to solve new
computer tasks presented to them through natural language commands. However,
previous approaches to this problem require large amounts of expert
demonstrations and task-specific reward functions, both of which are
impractical for new tasks. In this work, we show that a pre-trained large
language model (LLM) agent can execute computer tasks guided by natural
language using a simple prompting scheme where the agent Recursively Criticizes
and Improves its output (RCI). The RCI approach significantly outperforms
existing LLM methods for automating computer tasks and surpasses supervised
learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++
benchmark. We compare multiple LLMs and find that RCI with the
InstructGPT-3+RLHF LLM is state-of-the-art on MiniWoB++, using only a handful
of demonstrations per task rather than tens of thousands, and without a
task-specific reward function. Furthermore, we demonstrate RCI prompting&apos;s
effectiveness in enhancing LLMs&apos; reasoning abilities on a suite of natural
language reasoning tasks, outperforming chain of thought (CoT) prompting with
external feedback. We find that RCI combined with CoT performs better than
either separately. Our code can be found here:
https://github.com/posgnu/rci-agent.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1&quot;&gt;Geunwoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baldi_P/0/1/0/all/0/1&quot;&gt;Pierre Baldi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McAleer_S/0/1/0/all/0/1&quot;&gt;Stephen McAleer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.09802">
<title>Sasha: creative goal-oriented reasoning in smart homes with large language models. (arXiv:2305.09802v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2305.09802</link>
<description rdf:parseType="Literal">&lt;p&gt;Smart home assistants function best when user commands are direct and
well-specified (e.g., &quot;turn on the kitchen light&quot;), or when a hard-coded
routine specifies the response. In more natural communication, however, human
speech is unconstrained, often describing goals (e.g., &quot;make it cozy in here&quot;
or &quot;help me save energy&quot;) rather than indicating specific target devices and
actions to take on those devices. Current systems fail to understand these
under-specified commands since they cannot reason about devices and settings as
they relate to human situations. We introduce large language models (LLMs) to
this problem space, exploring their use for controlling devices and creating
automation routines in response to under-specified user commands in smart
homes. We empirically study the baseline quality and failure modes of
LLM-created action plans with a survey of age-diverse users. We find that LLMs
can reason creatively to achieve challenging goals, but they experience
patterns of failure that diminish their usefulness. We address these gaps with
Sasha, a smarter smart home assistant. Sasha responds to loosely-constrained
commands like &quot;make it cozy&quot; or &quot;help me sleep better&quot; by executing plans to
achieve user goals, e.g., setting a mood with available devices, or devising
automation routines. We evaluate our implementation of Sasha in a hands-on user
study, showing its capabilities and limitations when faced with unconstrained
user-generated scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+King_E/0/1/0/all/0/1&quot;&gt;Evan King&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Haoxiang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sangsu Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Julien_C/0/1/0/all/0/1&quot;&gt;Christine Julien&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16058">
<title>DUET: 2D Structured and Approximately Equivariant Representations. (arXiv:2306.16058v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16058</link>
<description rdf:parseType="Literal">&lt;p&gt;Multiview Self-Supervised Learning (MSSL) is based on learning invariances
with respect to a set of input transformations. However, invariance partially
or totally removes transformation-related information from the representations,
which might harm performance for specific downstream tasks that require such
information. We propose 2D strUctured and EquivarianT representations (coined
DUET), which are 2d representations organized in a matrix structure, and
equivariant with respect to transformations acting on the input data. DUET
representations maintain information about an input transformation, while
remaining semantically expressive. Compared to SimCLR (Chen et al., 2020)
(unstructured and invariant) and ESSL (Dangovski et al., 2022) (unstructured
and equivariant), the structured and equivariant nature of DUET representations
enables controlled generation with lower reconstruction error, while
controllability is not possible with SimCLR or ESSL. DUET also achieves higher
accuracy for several discriminative tasks, and improves transfer learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suau_X/0/1/0/all/0/1&quot;&gt;Xavier Suau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Danieli_F/0/1/0/all/0/1&quot;&gt;Federico Danieli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keller_T/0/1/0/all/0/1&quot;&gt;T. Anderson Keller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blaas_A/0/1/0/all/0/1&quot;&gt;Arno Blaas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chen Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramapuram_J/0/1/0/all/0/1&quot;&gt;Jason Ramapuram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Busbridge_D/0/1/0/all/0/1&quot;&gt;Dan Busbridge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zappella_L/0/1/0/all/0/1&quot;&gt;Luca Zappella&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08003">
<title>SHAMSUL: Systematic Holistic Analysis to investigate Medical Significance Utilizing Local interpretability methods in deep learning for chest radiography pathology prediction. (arXiv:2307.08003v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08003</link>
<description rdf:parseType="Literal">&lt;p&gt;The interpretability of deep neural networks has become a subject of great
interest within the medical and healthcare domain. This attention stems from
concerns regarding transparency, legal and ethical considerations, and the
medical significance of predictions generated by these deep neural networks in
clinical decision support systems. To address this matter, our study delves
into the application of four well-established interpretability methods: Local
Interpretable Model-agnostic Explanations (LIME), Shapley Additive exPlanations
(SHAP), Gradient-weighted Class Activation Mapping (Grad-CAM), and Layer-wise
Relevance Propagation (LRP). Leveraging the approach of transfer learning with
a multi-label-multi-class chest radiography dataset, we aim to interpret
predictions pertaining to specific pathology classes. Our analysis encompasses
both single-label and multi-label predictions, providing a comprehensive and
unbiased assessment through quantitative and qualitative investigations, which
are compared against human expert annotation. Notably, Grad-CAM demonstrates
the most favorable performance in quantitative evaluation, while the LIME
heatmap score segmentation visualization exhibits the highest level of medical
significance. Our research underscores both the outcomes and the challenges
faced in the holistic approach adopted for assessing these interpretability
methods and suggests that a multimodal-based approach, incorporating diverse
sources of information beyond chest radiography images, could offer additional
insights for enhancing interpretability in the medical domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Alam_M/0/1/0/all/0/1&quot;&gt;Mahbub Ul Alam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hollmen_J/0/1/0/all/0/1&quot;&gt;Jaakko Hollm&amp;#xe9;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Baldvinsson_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf3;n R&amp;#xfa;nar Baldvinsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rahmani_R/0/1/0/all/0/1&quot;&gt;Rahim Rahmani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.06595">
<title>VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use. (arXiv:2308.06595v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.06595</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark for
evaluation of instruction-following vision-language models for real-world use.
Our starting point is curating 70 &apos;instruction families&apos; that we envision
instruction tuned vision-language models should be able to address. Extending
beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to
game playing and creative generation. Following curation, our dataset comprises
592 test queries, each with a human-authored instruction-conditioned caption.
These descriptions surface instruction-specific factors, e.g., for an
instruction asking about the accessibility of a storefront for wheelchair
users, the instruction-conditioned caption describes ramps/potential obstacles.
These descriptions enable 1) collecting human-verified reference outputs for
each instance; and 2) automatic evaluation of candidate multimodal generations
using a text-only LLM, aligning with human judgment. We quantify quality gaps
between models and references using both human and automatic evaluations; e.g.,
the top-performing instruction-following model wins against the GPT-4 reference
in just 27% of the comparison. VisIT-Bench is dynamic to participate,
practitioners simply submit their model&apos;s response on the project website;
Data, code and leaderboard is available at visit-bench.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1&quot;&gt;Yonatan Bitton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_H/0/1/0/all/0/1&quot;&gt;Hritik Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1&quot;&gt;Jack Hessel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1&quot;&gt;Rulin Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wanrong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awadalla_A/0/1/0/all/0/1&quot;&gt;Anas Awadalla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gardner_J/0/1/0/all/0/1&quot;&gt;Josh Gardner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taori_R/0/1/0/all/0/1&quot;&gt;Rohan Taori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1&quot;&gt;Ludwig Schmidt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13614">
<title>Boosting Offline Reinforcement Learning for Autonomous Driving with Hierarchical Latent Skills. (arXiv:2309.13614v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2309.13614</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning-based vehicle planning is receiving increasing attention with the
emergence of diverse driving simulators and large-scale driving datasets. While
offline reinforcement learning (RL) is well suited for these safety-critical
tasks, it still struggles to plan over extended periods. In this work, we
present a skill-based framework that enhances offline RL to overcome the
long-horizon vehicle planning challenge. Specifically, we design a variational
autoencoder (VAE) to learn skills from offline demonstrations. To mitigate
posterior collapse of common VAEs, we introduce a two-branch sequence encoder
to capture both discrete options and continuous variations of the complex
driving skills. The final policy treats learned skills as actions and can be
trained by any off-the-shelf offline RL algorithms. This facilitates a shift in
focus from per-step actions to temporally extended skills, thereby enabling
long-term reasoning into the future. Extensive results on CARLA prove that our
model consistently outperforms strong baselines at both training and new
scenarios. Additional visualizations and experiments demonstrate the
interpretability and transferability of extracted skills.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zenan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_F/0/1/0/all/0/1&quot;&gt;Fan Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1&quot;&gt;Qiao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Da_F/0/1/0/all/0/1&quot;&gt;Fang Da&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hang Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16397">
<title>Uncertainty-Aware Decision Transformer for Stochastic Driving Environments. (arXiv:2309.16397v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.16397</link>
<description rdf:parseType="Literal">&lt;p&gt;Offline Reinforcement Learning (RL) has emerged as a promising framework for
learning policies without active interactions, making it especially appealing
for autonomous driving tasks. Recent successes of Transformers inspire casting
offline RL as sequence modeling, which performs well in long-horizon tasks.
However, they are overly optimistic in stochastic environments with incorrect
assumptions that the same goal can be consistently achieved by identical
actions. In this paper, we introduce an UNcertainty-awaRE deciSion Transformer
(UNREST) for planning in stochastic driving environments without introducing
additional transition or complex generative models. Specifically, UNREST
estimates state uncertainties by the conditional mutual information between
transitions and returns, and segments sequences accordingly. Discovering the
`uncertainty accumulation&apos; and `temporal locality&apos; properties of driving
environments, UNREST replaces the global returns in decision transformers with
less uncertain truncated returns, to learn from true outcomes of agent actions
rather than environment transitions. We also dynamically evaluate environmental
uncertainty during inference for cautious planning. Extensive experimental
results demonstrate UNREST&apos;s superior performance in various driving scenarios
and the power of our uncertainty estimation strategy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zenan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_F/0/1/0/all/0/1&quot;&gt;Fan Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1&quot;&gt;Qiao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Da_F/0/1/0/all/0/1&quot;&gt;Fang Da&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hang Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.14710">
<title>Random Forest Kernel for High-Dimension Low Sample Size Classification. (arXiv:2310.14710v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2310.14710</link>
<description rdf:parseType="Literal">&lt;p&gt;High dimension, low sample size (HDLSS) problems are numerous among
real-world applications of machine learning. From medical images to text
processing, traditional machine learning algorithms are usually unsuccessful in
learning the best possible concept from such data. In a previous work, we
proposed a dissimilarity-based approach for multi-view classification, the
Random Forest Dissimilarity (RFD), that perfoms state-of-the-art results for
such problems. In this work, we transpose the core principle of this approach
to solving HDLSS classification problems, by using the RF similarity measure as
a learned precomputed SVM kernel (RFSVM). We show that such a learned
similarity measure is particularly suited and accurate for this classification
context. Experiments conducted on 40 public HDLSS classification datasets,
supported by rigorous statistical analyses, show that the RFSVM method
outperforms existing methods for the majority of HDLSS problems and remains at
the same time very competitive for low or non-HDLSS problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cavalheiro_L/0/1/0/all/0/1&quot;&gt;Lucca Portes Cavalheiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bernard_S/0/1/0/all/0/1&quot;&gt;Simon Bernard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Barddal_J/0/1/0/all/0/1&quot;&gt;Jean Paul Barddal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Heutte_L/0/1/0/all/0/1&quot;&gt;Laurent Heutte&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15479">
<title>AutoDiff: combining Auto-encoder and Diffusion model for tabular data synthesizing. (arXiv:2310.15479v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15479</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion model has become a main paradigm for synthetic data generation in
many subfields of modern machine learning, including computer vision, language
model, or speech synthesis. In this paper, we leverage the power of diffusion
model for generating synthetic tabular data. The heterogeneous features in
tabular data have been main obstacles in tabular data synthesis, and we tackle
this problem by employing the auto-encoder architecture. When compared with the
state-of-the-art tabular synthesizers, the resulting synthetic tables from our
model show nice statistical fidelities to the real data, and perform well in
downstream tasks for machine learning utilities. We conducted the experiments
over $15$ publicly available datasets. Notably, our model adeptly captures the
correlations among features, which has been a long-standing challenge in
tabular data synthesis. Our code is available at
https://github.com/UCLA-Trustworthy-AI-Lab/AutoDiffusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Suh_N/0/1/0/all/0/1&quot;&gt;Namjoon Suh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hsieh_D/0/1/0/all/0/1&quot;&gt;Din-Yin Hsieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Honarkhah_M/0/1/0/all/0/1&quot;&gt;Merhdad Honarkhah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cheng_G/0/1/0/all/0/1&quot;&gt;Guang Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18075">
<title>DUMA: a Dual-Mind Conversational Agent with Fast and Slow Thinking. (arXiv:2310.18075v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18075</link>
<description rdf:parseType="Literal">&lt;p&gt;Inspired by the dual-process theory of human cognition, we introduce DUMA, a
novel conversational agent framework that embodies a dual-mind mechanism
through the utilization of two generative Large Language Models (LLMs)
dedicated to fast and slow thinking respectively. The fast thinking model
serves as the primary interface for external interactions and initial response
generation, evaluating the necessity for engaging the slow thinking model based
on the complexity of the complete response. When invoked, the slow thinking
model takes over the conversation, engaging in meticulous planning, reasoning,
and tool utilization to provide a well-analyzed response. This dual-mind
configuration allows for a seamless transition between intuitive responses and
deliberate problem-solving processes based on the situation. We have
constructed a conversational agent to handle online inquiries in the real
estate industry. The experiment proves that our method balances effectiveness
and efficiency, and has a significant improvement compared to the baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Liangyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Na Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yaxuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_W/0/1/0/all/0/1&quot;&gt;Wei Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kaijiang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1&quot;&gt;Ming Cui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.20654">
<title>Closed Drafting as a Case Study for First-Principle Interpretability, Memory, and Generalizability in Deep Reinforcement Learning. (arXiv:2310.20654v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.20654</link>
<description rdf:parseType="Literal">&lt;p&gt;Closed drafting or &quot;pick and pass&quot; is a popular game mechanic where each
round players select a card or other playable element from their hand and pass
the rest to the next player. In this paper, we establish first-principle
methods for studying the interpretability, generalizability, and memory of Deep
Q-Network (DQN) models playing closed drafting games. In particular, we use a
popular family of closed drafting games called &quot;Sushi Go Party&quot;, in which we
achieve state-of-the-art performance. We fit decision rules to interpret the
decision-making strategy of trained DRL agents by comparing them to the ranking
preferences of different types of human players. As Sushi Go Party can be
expressed as a set of closely-related games based on the set of cards in play,
we quantify the generalizability of DRL models trained on various sets of
cards, establishing a method to benchmark agent performance as a function of
environment unfamiliarity. Using the explicitly calculable memory of other
player&apos;s hands in closed drafting games, we create measures of the ability of
DRL models to learn memory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rezai_R/0/1/0/all/0/1&quot;&gt;Ryan Rezai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jason Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03488">
<title>Multi-Resolution Diffusion for Privacy-Sensitive Recommender Systems. (arXiv:2311.03488v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03488</link>
<description rdf:parseType="Literal">&lt;p&gt;While recommender systems have become an integral component of the Web
experience, their heavy reliance on user data raises privacy and security
concerns. Substituting user data with synthetic data can address these
concerns, but accurately replicating these real-world datasets has been a
notoriously challenging problem. Recent advancements in generative AI have
demonstrated the impressive capabilities of diffusion models in generating
realistic data across various domains. In this work we introduce a Score-based
Diffusion Recommendation Module (SDRM), which captures the intricate patterns
of real-world datasets required for training highly accurate recommender
systems. SDRM allows for the generation of synthetic data that can replace
existing datasets to preserve user privacy, or augment existing datasets to
address excessive data sparsity. Our method outperforms competing baselines
such as generative adversarial networks, variational autoencoders, and recently
proposed diffusion models in synthesizing various datasets to replace or
augment the original data by an average improvement of 4.30% in Recall@$k$ and
4.65% in NDCG@$k$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lilienthal_D/0/1/0/all/0/1&quot;&gt;Derek Lilienthal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mello_P/0/1/0/all/0/1&quot;&gt;Paul Mello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eirinaki_M/0/1/0/all/0/1&quot;&gt;Magdalini Eirinaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tiomkin_S/0/1/0/all/0/1&quot;&gt;Stas Tiomkin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04524">
<title>Validating ChatGPT Facts through RDF Knowledge Graphs and Sentence Similarity. (arXiv:2311.04524v2 [cs.DB] UPDATED)</title>
<link>http://arxiv.org/abs/2311.04524</link>
<description rdf:parseType="Literal">&lt;p&gt;Since ChatGPT offers detailed responses without justifications, and erroneous
facts even for popular persons, events and places, in this paper we present a
novel pipeline that retrieves the response of ChatGPT in RDF and tries to
validate the ChatGPT facts using one or more RDF Knowledge Graphs (KGs). To
this end we leverage DBpedia and LODsyndesis (an aggregated Knowledge Graph
that contains 2 billion triples from 400 RDF KGs of many domains) and short
sentence embeddings, and introduce an algorithm that returns the more relevant
triple(s) accompanied by their provenance and a confidence score. This enables
the validation of ChatGPT responses and their enrichment with justifications
and provenance. To evaluate this service (such services in general), we create
an evaluation benchmark that includes 2,000 ChatGPT facts; specifically 1,000
facts for famous Greek Persons, 500 facts for popular Greek Places, and 500
facts for Events related to Greece. The facts were manually labelled
(approximately 73% of ChatGPT facts were correct and 27% of facts were
erroneous). The results are promising; indicatively for the whole benchmark, we
managed to verify the 85.3% of the correct facts of ChatGPT and to find the
correct answer for the 58% of the erroneous ChatGPT facts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mountantonakis_M/0/1/0/all/0/1&quot;&gt;Michalis Mountantonakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tzitzikas_Y/0/1/0/all/0/1&quot;&gt;Yannis Tzitzikas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06233">
<title>Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models. (arXiv:2311.06233v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.06233</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose the Data Contamination Quiz, a simple and effective approach to
detect data contamination in large language models (LLMs) and estimate the
amount of it. Specifically, we frame data contamination detection as a series
of multiple-choice questions. We devise a quiz format wherein three perturbed
versions of each dataset instance are created. These changes only include
word-level perturbations, replacing words with their contextual synonyms,
ensuring both the semantic and sentence structure remain exactly the same as
the original instance. Together with the original instance, these perturbed
versions constitute the choices in the quiz. Given that the only distinguishing
signal among these choices is the exact wording, an LLM, when tasked with
identifying the original instance from the choices, opts for the original if it
has memorized it in its pre-training phase--a trait intrinsic to LLMs. A
dataset partition is then marked as contaminated if the LLM&apos;s performance on
the quiz surpasses what random chance suggests. Our evaluation spans seven
datasets and their respective splits (train and test/validation) on two
state-of-the-art LLMs: GPT-4 and GPT-3.5. While lacking access to the
pre-training data, our results suggest that our approach not only enhances the
detection of data contamination but also provides an accurate estimation of its
extent, even when the contamination signal is weak.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golchin_S/0/1/0/all/0/1&quot;&gt;Shahriar Golchin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1&quot;&gt;Mihai Surdeanu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06668">
<title>In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering. (arXiv:2311.06668v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.06668</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) demonstrate emergent in-context learning
capabilities, where they adapt to new tasks based on example demonstrations.
However, in-context learning has seen limited effectiveness in many settings,
is difficult to quantitatively control and takes up context window space. To
overcome these limitations, we propose an alternative approach that recasts
in-context learning as in-context vectors (ICV). Using ICV has two steps. We
first use a forward pass on demonstration examples to create the in-context
vector from the latent embedding of the LLM. This vector captures essential
information about the intended task. On a new query, instead of adding
demonstrations to the prompt, we shift the latent states of the LLM using the
ICV. The ICV approach has several benefits: 1) it enables the LLM to more
effectively follow the demonstration examples; 2) it&apos;s easy to control by
adjusting the magnitude of the ICV; 3) it reduces the length of the prompt by
removing the in-context demonstrations; 4) ICV is computationally much more
efficient than fine-tuning. We demonstrate that ICV achieves better performance
compared to standard in-context learning and fine-tuning on diverse tasks
including safety, style transfer, role-playing and formatting. Moreover, we
show that we can flexibly teach LLM to simultaneously follow different types of
instructions by simple vector arithmetics on the corresponding ICVs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1&quot;&gt;Lei Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1&quot;&gt;James Zou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08708">
<title>Joint User Pairing and Beamforming Design of Multi-STAR-RISs-Aided NOMA in the Indoor Environment via Multi-Agent Reinforcement Learning. (arXiv:2311.08708v2 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/2311.08708</link>
<description rdf:parseType="Literal">&lt;p&gt;The development of 6G/B5G wireless networks, which have requirements that go
beyond current 5G networks, is gaining interest from academia and industry.
However, to increase 6G/B5G network quality, conventional cellular networks
that rely on terrestrial base stations are constrained geographically and
economically. Meanwhile, NOMA allows multiple users to share the same
resources, which improves the spectral efficiency of the system and has the
advantage of supporting a larger number of users. Additionally, by
intelligently manipulating the phase and amplitude of both the reflected and
transmitted signals, STAR-RISs can achieve improved coverage, increased
spectral efficiency, and enhanced communication reliability. However, STAR-RISs
must simultaneously optimize the amplitude and phase shift corresponding to
reflection and transmission, which makes the existing terrestrial networks more
complicated and is considered a major challenging issue. Motivated by the
above, we study the joint user pairing for NOMA and beamforming design of
Multi-STAR-RISs in an indoor environment. Then, we formulate the optimization
problem with the objective of maximizing the total throughput of MUs by jointly
optimizing the decoding order, user pairing, active beamforming, and passive
beamforming. However, the formulated problem is a MINLP. To address this
challenge, we first introduce the decoding order for NOMA networks. Next, we
decompose the original problem into two subproblems, namely: 1) MU pairing and
2) Beamforming optimization under the optimal decoding order. For the first
subproblem, we employ correlation-based K-means clustering to solve the user
pairing problem. Then, to jointly deal with beamforming vector optimizations,
we propose MAPPO, which can make quick decisions in the given environment owing
to its low complexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1&quot;&gt;Yu Min Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tun_Y/0/1/0/all/0/1&quot;&gt;Yan Kyaw Tun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1&quot;&gt;Choong Seon Hong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09574">
<title>LymphoML: An interpretable artificial intelligence-based method identifies morphologic features that correlate with lymphoma subtype. (arXiv:2311.09574v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09574</link>
<description rdf:parseType="Literal">&lt;p&gt;The accurate classification of lymphoma subtypes using hematoxylin and eosin
(H&amp;amp;E)-stained tissue is complicated by the wide range of morphological features
these cancers can exhibit. We present LymphoML - an interpretable machine
learning method that identifies morphologic features that correlate with
lymphoma subtypes. Our method applies steps to process H&amp;amp;E-stained tissue
microarray cores, segment nuclei and cells, compute features encompassing
morphology, texture, and architecture, and train gradient-boosted models to
make diagnostic predictions. LymphoML&apos;s interpretable models, developed on a
limited volume of H&amp;amp;E-stained tissue, achieve non-inferior diagnostic accuracy
to pathologists using whole-slide images and outperform black box deep-learning
on a dataset of 670 cases from Guatemala spanning 8 lymphoma subtypes. Using
SHapley Additive exPlanation (SHAP) analysis, we assess the impact of each
feature on model prediction and find that nuclear shape features are most
discriminative for DLBCL (F1-score: 78.7%) and classical Hodgkin lymphoma
(F1-score: 74.5%). Finally, we provide the first demonstration that a model
combining features from H&amp;amp;E-stained tissue with features from a standardized
panel of 6 immunostains results in a similar diagnostic accuracy (85.3%) to a
46-stain panel (86.1%).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shankar_V/0/1/0/all/0/1&quot;&gt;Vivek Shankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaoli Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishna_V/0/1/0/all/0/1&quot;&gt;Vrishab Krishna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_B/0/1/0/all/0/1&quot;&gt;Brent Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_O/0/1/0/all/0/1&quot;&gt;Oscar Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rojansky_R/0/1/0/all/0/1&quot;&gt;Rebecca Rojansky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1&quot;&gt;Andrew Ng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valvert_F/0/1/0/all/0/1&quot;&gt;Fabiola Valvert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Briercheck_E/0/1/0/all/0/1&quot;&gt;Edward Briercheck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinstock_D/0/1/0/all/0/1&quot;&gt;David Weinstock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Natkunam_Y/0/1/0/all/0/1&quot;&gt;Yasodha Natkunam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_Pol_S/0/1/0/all/0/1&quot;&gt;Sebastian Fernandez-Pol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1&quot;&gt;Pranav Rajpurkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09790">
<title>Breaking Boundaries: Balancing Performance and Robustness in Deep Wireless Traffic Forecasting. (arXiv:2311.09790v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09790</link>
<description rdf:parseType="Literal">&lt;p&gt;Balancing the trade-off between accuracy and robustness is a long-standing
challenge in time series forecasting. While most of existing robust algorithms
have achieved certain suboptimal performance on clean data, sustaining the same
performance level in the presence of data perturbations remains extremely hard.
In this paper, we study a wide array of perturbation scenarios and propose
novel defense mechanisms against adversarial attacks using real-world telecom
data. We compare our strategy against two existing adversarial training
algorithms under a range of maximal allowed perturbations, defined using
$\ell_{\infty}$-norm, $\in [0.1,0.4]$. Our findings reveal that our hybrid
strategy, which is composed of a classifier to detect adversarial examples, a
denoiser to eliminate noise from the perturbed data samples, and a standard
forecaster, achieves the best performance on both clean and perturbed data. Our
optimal model can retain up to $92.02\%$ the performance of the original
forecasting model in terms of Mean Squared Error (MSE) on clean data, while
being more robust than the standard adversarially trained models on perturbed
data. Its MSE is 2.71$\times$ and 2.51$\times$ lower than those of comparing
methods on normal and perturbed data, respectively. In addition, the components
of our models can be trained in parallel, resulting in better computational
efficiency. Our results indicate that we can optimally balance the trade-off
between the performance and robustness of forecasting models by improving the
classifier and denoiser, even in the presence of sophisticated and destructive
poisoning attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilbert_R/0/1/0/all/0/1&quot;&gt;Romain Ilbert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1&quot;&gt;Thai V. Hoang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zonghua Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palpanas_T/0/1/0/all/0/1&quot;&gt;Themis Palpanas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09861">
<title>PsyBench: a balanced and in-depth Psychological Chinese Evaluation Benchmark for Foundation Models. (arXiv:2311.09861v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09861</link>
<description rdf:parseType="Literal">&lt;p&gt;As Large Language Models (LLMs) are becoming prevalent in various fields,
there is an urgent need for improved NLP benchmarks that encompass all the
necessary knowledge of individual discipline. Many contemporary benchmarks for
foundational models emphasize a broad range of subjects but often fall short in
presenting all the critical subjects and encompassing necessary professional
knowledge of them. This shortfall has led to skewed results, given that LLMs
exhibit varying performance across different subjects and knowledge areas. To
address this issue, we present psybench, the first comprehensive Chinese
evaluation suite that covers all the necessary knowledge required for graduate
entrance exams. psybench offers a deep evaluation of a model&apos;s strengths and
weaknesses in psychology through multiple-choice questions. Our findings show
significant differences in performance across different sections of a subject,
highlighting the risk of skewed results when the knowledge in test sets is not
balanced. Notably, only the ChatGPT model reaches an average accuracy above
$70\%$, indicating that there is still plenty of room for improvement. We
expect that psybench will help to conduct thorough evaluations of base models&apos;
strengths and weaknesses and assist in practical application in the field of
psychology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Junlei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;Hongliang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_N/0/1/0/all/0/1&quot;&gt;Nirui Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1&quot;&gt;Shuyuan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shuai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1&quot;&gt;Huachuan Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1&quot;&gt;Anqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lizhi Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1&quot;&gt;Zhenzhong Lan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09930">
<title>A Framework for Monitoring and Retraining Language Models in Real-World Applications. (arXiv:2311.09930v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09930</link>
<description rdf:parseType="Literal">&lt;p&gt;In the Machine Learning (ML) model development lifecycle, training candidate
models using an offline holdout dataset and identifying the best model for the
given task is only the first step. After the deployment of the selected model,
continuous model monitoring and model retraining is required in many real-world
applications. There are multiple reasons for retraining, including data or
concept drift, which may be reflected on the model performance as monitored by
an appropriate metric. Another motivation for retraining is the acquisition of
increasing amounts of data over time, which may be used to retrain and improve
the model performance even in the absence of drifts. We examine the impact of
various retraining decision points on crucial factors, such as model
performance and resource utilization, in the context of Multilabel
Classification models. We explain our key decision points and propose a
reference framework for designing an effective model retraining strategy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasundra_J/0/1/0/all/0/1&quot;&gt;Jaykumar Kasundra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulz_C/0/1/0/all/0/1&quot;&gt;Claudia Schulz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirsafian_M/0/1/0/all/0/1&quot;&gt;Melicaalsadat Mirsafian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skylaki_S/0/1/0/all/0/1&quot;&gt;Stavroula Skylaki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10090">
<title>JaxMARL: Multi-Agent RL Environments in JAX. (arXiv:2311.10090v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10090</link>
<description rdf:parseType="Literal">&lt;p&gt;Benchmarks play an important role in the development of machine learning
algorithms. For example, research in reinforcement learning (RL) has been
heavily influenced by available environments and benchmarks. However, RL
environments are traditionally run on the CPU, limiting their scalability with
typical academic compute. Recent advancements in JAX have enabled the wider use
of hardware acceleration to overcome these computational hurdles, enabling
massively parallel RL training pipelines and environments. This is particularly
useful for multi-agent reinforcement learning (MARL) research. First of all,
multiple agents must be considered at each environment step, adding
computational burden, and secondly, the sample complexity is increased due to
non-stationarity, decentralised partial observability, or other MARL
challenges. In this paper, we present JaxMARL, the first open-source code base
that combines ease-of-use with GPU enabled efficiency, and supports a large
number of commonly used MARL environments as well as popular baseline
algorithms. When considering wall clock time, our experiments show that per-run
our JAX-based training pipeline is up to 12500x faster than existing
approaches. This enables efficient and thorough evaluations, with the potential
to alleviate the evaluation crisis of the field. We also introduce and
benchmark SMAX, a vectorised, simplified version of the popular StarCraft
Multi-Agent Challenge, which removes the need to run the StarCraft II game
engine. This not only enables GPU acceleration, but also provides a more
flexible MARL environment, unlocking the potential for self-play,
meta-learning, and other future applications in MARL. We provide code at
https://github.com/flairox/jaxmarl.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rutherford_A/0/1/0/all/0/1&quot;&gt;Alexander Rutherford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ellis_B/0/1/0/all/0/1&quot;&gt;Benjamin Ellis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gallici_M/0/1/0/all/0/1&quot;&gt;Matteo Gallici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cook_J/0/1/0/all/0/1&quot;&gt;Jonathan Cook&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lupu_A/0/1/0/all/0/1&quot;&gt;Andrei Lupu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ingvarsson_G/0/1/0/all/0/1&quot;&gt;Gardar Ingvarsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Willi_T/0/1/0/all/0/1&quot;&gt;Timon Willi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Akbir Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Witt_C/0/1/0/all/0/1&quot;&gt;Christian Schroeder de Witt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Souly_A/0/1/0/all/0/1&quot;&gt;Alexandra Souly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bandyopadhyay_S/0/1/0/all/0/1&quot;&gt;Saptarashmi Bandyopadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samvelyan_M/0/1/0/all/0/1&quot;&gt;Mikayel Samvelyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1&quot;&gt;Minqi Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lange_R/0/1/0/all/0/1&quot;&gt;Robert Tjarko Lange&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1&quot;&gt;Shimon Whiteson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lacerda_B/0/1/0/all/0/1&quot;&gt;Bruno Lacerda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hawes_N/0/1/0/all/0/1&quot;&gt;Nick Hawes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1&quot;&gt;Tim Rocktaschel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Chris Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1&quot;&gt;Jakob Nicolaus Foerster&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.06905">
<title>Less is More: Proxy Datasets in NAS approaches. (arXiv:2203.06905v1 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2203.06905</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Architecture Search (NAS) defines the design of Neural Networks as a
search problem. Unfortunately, NAS is computationally intensive because of
various possibilities depending on the number of elements in the design and the
possible connections between them. In this work, we extensively analyze the
role of the dataset size based on several sampling approaches for reducing the
dataset size (unsupervised and supervised cases) as an agnostic approach to
reduce search time. We compared these techniques with four common NAS
approaches in NAS-Bench-201 in roughly 1,400 experiments on CIFAR-100. One of
our surprising findings is that in most cases we can reduce the amount of
training data to 25\%, consequently reducing search time to 25\%, while at the
same time maintaining the same accuracy as if training on the full dataset.
Additionally, some designs derived from subsets out-perform designs derived
from the full dataset by up to 22 p.p. accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moser_B/0/1/0/all/0/1&quot;&gt;Brian Moser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raue_F/0/1/0/all/0/1&quot;&gt;Federico Raue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hees_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;rn Hees&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1&quot;&gt;Andreas Dengel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04593">
<title>DWA: Differential Wavelet Amplifier for Image Super-Resolution. (arXiv:2307.04593v1 [eess.IV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2307.04593</link>
<description rdf:parseType="Literal">&lt;p&gt;This work introduces Differential Wavelet Amplifier (DWA), a drop-in module
for wavelet-based image Super-Resolution (SR). DWA invigorates an approach
recently receiving less attention, namely Discrete Wavelet Transformation
(DWT). DWT enables an efficient image representation for SR and reduces the
spatial area of its input by a factor of 4, the overall model size, and
computation cost, framing it as an attractive approach for sustainable ML. Our
proposed DWA model improves wavelet-based SR models by leveraging the
difference between two convolutional filters to refine relevant feature
extraction in the wavelet domain, emphasizing local contrasts and suppressing
common noise in the input signals. We show its effectiveness by integrating it
into existing SR models, e.g., DWSR and MWCNN, and demonstrate a clear
improvement in classical SR tasks. Moreover, DWA enables a direct application
of DWSR and MWCNN to input image space, reducing the DWT representation
channel-wise since it omits traditional DWT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Moser_B/0/1/0/all/0/1&quot;&gt;Brian B. Moser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Frolov_S/0/1/0/all/0/1&quot;&gt;Stanislav Frolov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Raue_F/0/1/0/all/0/1&quot;&gt;Federico Raue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Palacio_S/0/1/0/all/0/1&quot;&gt;Sebastian Palacio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dengel_A/0/1/0/all/0/1&quot;&gt;Andreas Dengel&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>