<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.LG updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Machine Learning (cs.LG) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-12-12T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06669" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06677" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06681" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06682" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06695" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06699" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06701" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06705" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06710" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06711" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06742" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06786" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06791" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06795" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06798" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06799" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06810" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06820" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06828" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06832" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06833" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06837" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06845" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06850" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06854" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06855" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06868" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06869" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06871" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06872" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06874" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06877" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06881" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06887" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06899" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06902" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06914" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06925" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06932" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06937" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06940" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06941" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06942" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06951" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06957" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06958" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06960" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06973" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06979" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06993" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07003" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07022" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07032" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07035" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07044" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07046" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07064" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07067" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07069" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07087" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07103" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07110" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07112" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07133" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07142" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07145" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07161" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07165" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07168" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07174" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07175" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07178" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07182" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07186" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07206" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07208" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07231" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07248" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07252" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07271" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07281" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07282" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07285" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07296" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07311" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07316" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07331" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07335" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07358" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07371" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07392" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07405" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07413" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07414" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07420" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07424" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07425" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07434" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07435" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07439" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07457" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07466" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07492" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1902.10664" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2010.02171" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.04133" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.06074" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.16218" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.04619" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.04281" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.05575" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.01711" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.09247" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.01959" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.06434" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.12496" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.14404" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.16386" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.13108" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.11093" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.02904" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.07321" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.08618" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.11091" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.00836" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.00855" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.01092" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.08459" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.16852" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.01168" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.03907" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.12130" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.13107" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.14660" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05218" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15486" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15546" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17421" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19302" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.20009" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.03241" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06871" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09124" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10711" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14284" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14435" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05845" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08131" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.16164" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03743" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04669" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.08641" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.09604" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.09895" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12696" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.14815" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16139" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03648" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13788" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14585" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16512" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16808" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01678" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04816" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10773" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10998" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.13767" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16285" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17087" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19797" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02227" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04673" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06062" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07289" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08360" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09247" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13225" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15497" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.18130" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.18694" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.18718" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00258" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03998" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04344" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04815" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05496" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05549" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05725" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05743" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05924" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05978" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06348" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06363" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06436" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06564" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06635" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06540" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2312.06669">
<title>An Association Test Based on Kernel-Based Neural Networks for Complex Genetic Association Analysis. (arXiv:2312.06669v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/2312.06669</link>
<description rdf:parseType="Literal">&lt;p&gt;The advent of artificial intelligence, especially the progress of deep neural
networks, is expected to revolutionize genetic research and offer unprecedented
potential to decode the complex relationships between genetic variants and
disease phenotypes, which could mark a significant step toward improving our
understanding of the disease etiology. While deep neural networks hold great
promise for genetic association analysis, limited research has been focused on
developing neural-network-based tests to dissect complex genotype-phenotype
associations. This complexity arises from the opaque nature of neural networks
and the absence of defined limiting distributions. We have previously developed
a kernel-based neural network model (KNN) that synergizes the strengths of
linear mixed models with conventional neural networks. KNN adopts a
computationally efficient minimum norm quadratic unbiased estimator (MINQUE)
algorithm and uses KNN structure to capture the complex relationship between
large-scale sequencing data and a disease phenotype of interest. In the KNN
framework, we introduce a MINQUE-based test to assess the joint association of
genetic variants with the phenotype, which considers non-linear and
non-additive effects and follows a mixture of chi-square distributions. We also
construct two additional tests to evaluate and interpret linear and
non-linear/non-additive genetic effects, including interaction effects. Our
simulations show that our method consistently controls the type I error rate
under various conditions and achieves greater power than a commonly used
sequence kernel association test (SKAT), especially when involving non-linear
and interaction effects. When applied to real data from the UK Biobank, our
approach identified genes associated with hippocampal volume, which can be
further replicated and evaluated for their role in the pathogenesis of
Alzheimer&apos;s disease.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hou_T/0/1/0/all/0/1&quot;&gt;Tingting Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Jiang_C/0/1/0/all/0/1&quot;&gt;Chang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lu_Q/0/1/0/all/0/1&quot;&gt;Qing Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06677">
<title>Intelligent Virtual Assistants with LLM-based Process Automation. (arXiv:2312.06677v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.06677</link>
<description rdf:parseType="Literal">&lt;p&gt;While intelligent virtual assistants like Siri, Alexa, and Google Assistant
have become ubiquitous in modern life, they still face limitations in their
ability to follow multi-step instructions and accomplish complex goals
articulated in natural language. However, recent breakthroughs in large
language models (LLMs) show promise for overcoming existing barriers by
enhancing natural language processing and reasoning capabilities. Though
promising, applying LLMs to create more advanced virtual assistants still faces
challenges like ensuring robust performance and handling variability in
real-world user commands. This paper proposes a novel LLM-based virtual
assistant that can automatically perform multi-step operations within mobile
apps based on high-level user requests. The system represents an advance in
assistants by providing an end-to-end solution for parsing instructions,
reasoning about goals, and executing actions. LLM-based Process Automation
(LLMPA) has modules for decomposing instructions, generating descriptions,
detecting interface elements, predicting next actions, and error checking.
Experiments demonstrate the system completing complex mobile operation tasks in
Alipay based on natural language instructions. This showcases how large
language models can enable automated assistants to accomplish real-world tasks.
The main contributions are the novel LLMPA architecture optimized for app
process automation, the methodology for applying LLMs to mobile apps, and
demonstrations of multi-step task completion in a real-world environment.
Notably, this work represents the first real-world deployment and extensive
evaluation of a large language model-based virtual assistant in a widely used
mobile application with an enormous user base numbering in the hundreds of
millions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1&quot;&gt;Yanchu Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1&quot;&gt;Zhixuan Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shiyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_F/0/1/0/all/0/1&quot;&gt;Feiyue Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1&quot;&gt;Ruihua Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Longfei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jinjie Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_C/0/1/0/all/0/1&quot;&gt;Chenyi Zhuang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06681">
<title>Steering Llama 2 via Contrastive Activation Addition. (arXiv:2312.06681v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.06681</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Contrastive Activation Addition (CAA), an innovative method for
steering language models by modifying activations during their forward passes.
CAA computes ``steering vectors&apos;&apos; by averaging the difference in residual
stream activations between pairs of positive and negative examples of a
particular behavior such as factual versus hallucinatory responses. During
inference, these steering vectors are added at all token positions after the
user&apos;s prompt with either a positive or negative coefficient, allowing precise
control over the degree of the targeted behavior. We evaluate CAA&apos;s
effectiveness on Llama 2 Chat using both multiple-choice behavioral question
datasets and open-ended generation tasks. We demonstrate that CAA significantly
alters model behavior, outperforms traditional methods like finetuning and
few-shot prompting, and minimally reduces capabilities. Moreover, by employing
various activation space interpretation methods, we gain deeper insights into
CAA&apos;s mechanisms. CAA both accurately steers model outputs and also sheds light
on how high-level concepts are represented in Large Language Models (LLMs).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rimsky_N/0/1/0/all/0/1&quot;&gt;Nina Rimsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gabrieli_N/0/1/0/all/0/1&quot;&gt;Nick Gabrieli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulz_J/0/1/0/all/0/1&quot;&gt;Julian Schulz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_M/0/1/0/all/0/1&quot;&gt;Meg Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hubinger_E/0/1/0/all/0/1&quot;&gt;Evan Hubinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turner_A/0/1/0/all/0/1&quot;&gt;Alexander Matt Turner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06682">
<title>Learning to Denoise Unreliable Interactions for Link Prediction on Biomedical Knowledge Graph. (arXiv:2312.06682v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.06682</link>
<description rdf:parseType="Literal">&lt;p&gt;Link prediction in biomedical knowledge graphs (KGs) aims at predicting
unknown interactions between entities, including drug-target interaction (DTI)
and drug-drug interaction (DDI), which is critical for drug discovery and
therapeutics. Previous methods prefer to utilize the rich semantic relations
and topological structure of the KG to predict missing links, yielding
promising outcomes. However, all these works only focus on improving the
predictive performance without considering the inevitable noise and unreliable
interactions existing in the KGs, which limits the development of KG-based
computational methods. To address these limitations, we propose a Denoised Link
Prediction framework, called DenoisedLP. DenoisedLP obtains reliable
interactions based on the local subgraph by denoising noisy links in a
learnable way, providing a universal module for mining underlying task-relevant
relations. To collaborate with the smoothed semantic information, DenoisedLP
introduces the semantic subgraph by blurring conflict relations around the
predicted link. By maximizing the mutual information between the reliable
structure and smoothed semantic relations, DenoisedLP emphasizes the
informative interactions for predicting relation-specific links. Experimental
results on real-world datasets demonstrate that DenoisedLP outperforms
state-of-the-art methods on DTI and DDI prediction tasks, and verify the
effectiveness and robustness of denoising unreliable interactions on the
contaminated KGs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1&quot;&gt;Tengfei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yujie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_W/0/1/0/all/0/1&quot;&gt;Wen Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1&quot;&gt;Dashun Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xuan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_P/0/1/0/all/0/1&quot;&gt;Patrick Cheong-lao Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yiping Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yijun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1&quot;&gt;Bosheng Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1&quot;&gt;Xiangxiang Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06695">
<title>Evolving Reservoirs for Meta Reinforcement Learning. (arXiv:2312.06695v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.06695</link>
<description rdf:parseType="Literal">&lt;p&gt;Animals often demonstrate a remarkable ability to adapt to their environments
during their lifetime. They do so partly due to the evolution of morphological
and neural structures. These structures capture features of environments shared
between generations to bias and speed up lifetime learning. In this work, we
propose a computational model for studying a mechanism that can enable such a
process. We adopt a computational framework based on meta reinforcement
learning as a model of the interplay between evolution and development. At the
evolutionary scale, we evolve reservoirs, a family of recurrent neural networks
that differ from conventional networks in that one optimizes not the weight
values but hyperparameters of the architecture: the later control macro-level
properties, such as memory and dynamics. At the developmental scale, we employ
these evolved reservoirs to facilitate the learning of a behavioral policy
through Reinforcement Learning (RL). Within an RL agent, a reservoir encodes
the environment state before providing it to an action policy. We evaluate our
approach on several 2D and 3D simulated environments. Our results show that the
evolution of reservoirs can improve the learning of diverse challenging tasks.
We study in particular three hypotheses: the use of an architecture combining
reservoirs and reinforcement learning could enable (1) solving tasks with
partial observability, (2) generating oscillatory dynamics that facilitate the
learning of locomotion tasks, and (3) facilitating the generalization of
learned behaviors to new tasks unknown during the evolution phase.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leger_C/0/1/0/all/0/1&quot;&gt;Corentin L&amp;#xe9;ger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamon_G/0/1/0/all/0/1&quot;&gt;Gautier Hamon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nisioti_E/0/1/0/all/0/1&quot;&gt;Eleni Nisioti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hinaut_X/0/1/0/all/0/1&quot;&gt;Xavier Hinaut&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moulin_Frier_C/0/1/0/all/0/1&quot;&gt;Cl&amp;#xe9;ment Moulin-Frier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06699">
<title>Leveraging Generative Language Models for Weakly Supervised Sentence Component Analysis in Video-Language Joint Learning. (arXiv:2312.06699v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06699</link>
<description rdf:parseType="Literal">&lt;p&gt;A thorough comprehension of textual data is a fundamental element in
multi-modal video analysis tasks. However, recent works have shown that the
current models do not achieve a comprehensive understanding of the textual data
during the training for the target downstream tasks. Orthogonal to the previous
approaches to this limitation, we postulate that understanding the significance
of the sentence components according to the target task can potentially enhance
the performance of the models. Hence, we utilize the knowledge of a pre-trained
large language model (LLM) to generate text samples from the original ones,
targeting specific sentence components. We propose a weakly supervised
importance estimation module to compute the relative importance of the
components and utilize them to improve different video-language tasks. Through
rigorous quantitative analysis, our proposed method exhibits significant
improvement across several video-language tasks. In particular, our approach
notably enhances video-text retrieval by a relative improvement of 8.3\% in
video-to-text and 1.4\% in text-to-video retrieval over the baselines, in terms
of R@1. Additionally, in video moment retrieval, average mAP shows a relative
improvement ranging from 2.0\% to 13.7 \% across different baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hakim_Z/0/1/0/all/0/1&quot;&gt;Zaber Ibn Abdul Hakim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarker_N/0/1/0/all/0/1&quot;&gt;Najibul Haque Sarker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1&quot;&gt;Rahul Pratap Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paul_B/0/1/0/all/0/1&quot;&gt;Bishmoy Paul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dabouei_A/0/1/0/all/0/1&quot;&gt;Ali Dabouei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Min Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06701">
<title>Dynamic Adversarial Attacks on Autonomous Driving Systems. (arXiv:2312.06701v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.06701</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces an attacking mechanism to challenge the resilience of
autonomous driving systems. Specifically, we manipulate the decision-making
processes of an autonomous vehicle by dynamically displaying adversarial
patches on a screen mounted on another moving vehicle. These patches are
optimized to deceive the object detection models into misclassifying targeted
objects, e.g., traffic signs. Such manipulation has significant implications
for critical multi-vehicle interactions such as intersection crossing and lane
changing, which are vital for safe and efficient autonomous driving systems.
Particularly, we make four major contributions. First, we introduce a novel
adversarial attack approach where the patch is not co-located with its target,
enabling more versatile and stealthy attacks. Moreover, our method utilizes
dynamic patches displayed on a screen, allowing for adaptive changes and
movement, enhancing the flexibility and performance of the attack. To do so, we
design a Screen Image Transformation Network (SIT-Net), which simulates
environmental effects on the displayed images, narrowing the gap between
simulated and real-world scenarios. Further, we integrate a positional loss
term into the adversarial training process to increase the success rate of the
dynamic attack. Finally, we shift the focus from merely attacking perceptual
systems to influencing the decision-making algorithms of self-driving systems.
Our experiments demonstrate the first successful implementation of such dynamic
adversarial attacks in real-world autonomous driving scenarios, paving the way
for advancements in the field of robust and secure autonomous driving.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chahe_A/0/1/0/all/0/1&quot;&gt;Amirhosein Chahe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chenan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeyapratap_A/0/1/0/all/0/1&quot;&gt;Abhishek Jeyapratap&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kaidi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Lifeng Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06705">
<title>Perceiving University Student&apos;s Opinions from Google App Reviews. (arXiv:2312.06705v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.06705</link>
<description rdf:parseType="Literal">&lt;p&gt;Google app market captures the school of thought of users from every corner
of the globe via ratings and text reviews, in a multilinguistic arena. The
potential information from the reviews cannot be extracted manually, due to its
exponential growth. So, Sentiment analysis, by machine learning and deep
learning algorithms employing NLP, explicitly uncovers and interprets the
emotions. This study performs the sentiment classification of the app reviews
and identifies the university student&apos;s behavior towards the app market via
exploratory analysis. We applied machine learning algorithms using the TP, TF,
and TF IDF text representation scheme and evaluated its performance on Bagging,
an ensemble learning method. We used word embedding, Glove, on the deep
learning paradigms. Our model was trained on Google app reviews and tested on
Student&apos;s App Reviews(SAR). The various combinations of these algorithms were
compared amongst each other using F score and accuracy and inferences were
highlighted graphically. SVM, amongst other classifiers, gave fruitful
accuracy(93.41%), F score(89%) on bigram and TF IDF scheme. Bagging enhanced
the performance of LR and NB with accuracy of 87.88% and 86.69% and F score of
86% and 78% respectively. Overall, LSTM on Glove embedding recorded the highest
accuracy(95.2%) and F score(88%).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ranjan_S/0/1/0/all/0/1&quot;&gt;Sakshi Ranjan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1&quot;&gt;Subhankar Mishra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06710">
<title>Class-Prototype Conditional Diffusion Model for Continual Learning with Generative Replay. (arXiv:2312.06710v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.06710</link>
<description rdf:parseType="Literal">&lt;p&gt;Mitigating catastrophic forgetting is a key hurdle in continual learning.
Deep Generative Replay (GR) provides techniques focused on generating samples
from prior tasks to enhance the model&apos;s memory capabilities. With the
progression in generative AI, generative models have advanced from Generative
Adversarial Networks (GANs) to the more recent Diffusion Models (DMs). A major
issue is the deterioration in the quality of generated data compared to the
original, as the generator continuously self-learns from its outputs. This
degradation can lead to the potential risk of catastrophic forgetting occurring
in the classifier. To address this, we propose the Class-Prototype Conditional
Diffusion Model (CPDM), a GR-based approach for continual learning that
enhances image quality in generators and thus reduces catastrophic forgetting
in classifiers. The cornerstone of CPDM is a learnable class-prototype that
captures the core characteristics of images in a given class. This prototype,
integrated into the diffusion model&apos;s denoising process, ensures the generation
of high-quality images. It maintains its effectiveness for old tasks even when
new tasks are introduced, preserving image generation quality and reducing the
risk of catastrophic forgetting in classifiers. Our empirical studies on
diverse datasets demonstrate that our proposed method significantly outperforms
existing state-of-the-art models, highlighting its exceptional ability to
preserve image quality and enhance the model&apos;s memory retention.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doan_K/0/1/0/all/0/1&quot;&gt;Khanh Doan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1&quot;&gt;Quyen Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tuan Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1&quot;&gt;Dinh Phung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1&quot;&gt;Trung Le&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06711">
<title>Physics Informed Neural Network for Option Pricing. (arXiv:2312.06711v1 [q-fin.PR])</title>
<link>http://arxiv.org/abs/2312.06711</link>
<description rdf:parseType="Literal">&lt;p&gt;We apply a physics-informed deep-learning approach the PINN approach to the
Black-Scholes equation for pricing American and European options. We test our
approach on both simulated as well as real market data, compare it to
analytical/numerical benchmarks. Our model is able to accurately capture the
price behaviour on simulation data, while also exhibiting reasonable
performance for market data. We also experiment with the architecture and
learning process of our PINN model to provide more understanding of convergence
and stability issues that impact performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Dhiman_A/0/1/0/all/0/1&quot;&gt;Ashish Dhiman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yibei Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06742">
<title>Honeybee: Locality-enhanced Projector for Multimodal LLM. (arXiv:2312.06742v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06742</link>
<description rdf:parseType="Literal">&lt;p&gt;In Multimodal Large Language Models (MLLMs), a visual projector plays a
crucial role in bridging pre-trained vision encoders with LLMs, enabling
profound visual understanding while harnessing the LLMs&apos; robust capabilities.
Despite the importance of the visual projector, it has been relatively less
explored. In this study, we first identify two essential projector properties:
(i) flexibility in managing the number of visual tokens, crucial for MLLMs&apos;
overall efficiency, and (ii) preservation of local context from visual
features, vital for spatial understanding. Based on these findings, we propose
a novel projector design that is both flexible and locality-enhanced,
effectively satisfying the two desirable properties. Additionally, we present
comprehensive strategies to effectively utilize multiple and multifaceted
instruction datasets. Through extensive experiments, we examine the impact of
individual design choices. Finally, our proposed MLLM, Honeybee, remarkably
outperforms previous state-of-the-art methods across various benchmarks,
including MME, MMBench, SEED-Bench, and LLaVA-Bench, achieving significantly
higher efficiency. Code and models are available at
https://github.com/kakaobrain/honeybee.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cha_J/0/1/0/all/0/1&quot;&gt;Junbum Cha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1&quot;&gt;Wooyoung Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mun_J/0/1/0/all/0/1&quot;&gt;Jonghwan Mun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roh_B/0/1/0/all/0/1&quot;&gt;Byungseok Roh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06786">
<title>Mixture-of-Linear-Experts for Long-term Time Series Forecasting. (arXiv:2312.06786v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.06786</link>
<description rdf:parseType="Literal">&lt;p&gt;Long-term time series forecasting (LTSF) aims to predict future values of a
time series given the past values. The current state-of-the-art (SOTA) on this
problem is attained in some cases by linear-centric models, which primarily
feature a linear mapping layer. However, due to their inherent simplicity, they
are not able to adapt their prediction rules to periodic changes in time series
patterns. To address this challenge, we propose a Mixture-of-Experts-style
augmentation for linear-centric models and propose Mixture-of-Linear-Experts
(MoLE). Instead of training a single model, MoLE trains multiple linear-centric
models (i.e., experts) and a router model that weighs and mixes their outputs.
While the entire framework is trained end-to-end, each expert learns to
specialize in a specific temporal pattern, and the router model learns to
compose the experts adaptively. Experiments show that MoLE reduces forecasting
error of linear-centric models, including DLinear, RLinear, and RMLP, in over
78% of the datasets and settings we evaluated. By using MoLE existing
linear-centric models can achieve SOTA LTSF results in 68% of the experiments
that PatchTST reports and we compare to, whereas existing single-head
linear-centric models achieve SOTA results in only 25% of cases. Additionally,
MoLE models achieve SOTA in all settings for the newly released Weather2K
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_R/0/1/0/all/0/1&quot;&gt;Ronghao Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zinan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuaiqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fanti_G/0/1/0/all/0/1&quot;&gt;Giulia Fanti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06791">
<title>Learning Polynomial Representations of Physical Objects with Application to Certifying Correct Packing Configurations. (arXiv:2312.06791v1 [math.OC])</title>
<link>http://arxiv.org/abs/2312.06791</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a novel approach for learning polynomial
representations of physical objects. Given a point cloud data set associated
with a physical object, we solve a one-class classification problem to bound
the data points by a polynomial sublevel set while harnessing Sum-of-Squares
(SOS) programming to enforce prior shape knowledge constraints. By representing
objects as polynomial sublevel sets we further show it is possible to construct
a secondary SOS program to certify whether objects are packed correctly, that
is object boundaries do not overlap and are inside some container set. While
not employing reinforcement learning (RL) in this work, our proposed secondary
SOS program does provide a potential surrogate reward function for RL
algorithms, autonomously rewarding agents that propose object rotations and
translations that correctly pack objects within a given container set.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Jones_M/0/1/0/all/0/1&quot;&gt;Morgan Jones&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06795">
<title>Model Breadcrumbs: Scaling Multi-Task Model Merging with Sparse Masks. (arXiv:2312.06795v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.06795</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid development of AI systems has been greatly influenced by the
emergence of foundation models. A common approach for targeted problems
involves fine-tuning these pre-trained foundation models for specific target
tasks, resulting in a rapid spread of models fine-tuned across a diverse array
of tasks. This work focuses on the problem of merging multiple fine-tunings of
the same foundation model derived from a spectrum of auxiliary tasks. We
introduce a new simple method, Model Breadcrumbs, which consists of a sparsely
defined set of weights that carve out a trajectory within the weight space of a
pre-trained model, enhancing task performance when traversed. These breadcrumbs
are constructed by subtracting the weights from a pre-trained model before and
after fine-tuning, followed by a sparsification process that eliminates weight
outliers and negligible perturbations. Our experiments demonstrate the
effectiveness of Model Breadcrumbs to simultaneously improve performance across
multiple tasks. This contribution aligns with the evolving paradigm of
updatable machine learning, reminiscent of the collaborative principles
underlying open-source software development, fostering a community-driven
effort to reliably update machine learning models. Our method is shown to be
more efficient and unlike previous proposals does not require hyperparameter
tuning for each new task added. Through extensive experimentation involving
various models, tasks, and modalities we establish that integrating Model
Breadcrumbs offers a simple, efficient, and highly effective approach for
constructing multi-task models and facilitating updates to foundation models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davari_M/0/1/0/all/0/1&quot;&gt;MohammadReza Davari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belilovsky_E/0/1/0/all/0/1&quot;&gt;Eugene Belilovsky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06798">
<title>Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety. (arXiv:2312.06798v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.06798</link>
<description rdf:parseType="Literal">&lt;p&gt;Explainability and Safety engender Trust. These require a model to exhibit
consistency and reliability. To achieve these, it is necessary to use and
analyze data and knowledge with statistical and symbolic AI methods relevant to
the AI application - neither alone will do. Consequently, we argue and seek to
demonstrate that the NeuroSymbolic AI approach is better suited for making AI a
trusted AI system. We present the CREST framework that shows how Consistency,
Reliability, user-level Explainability, and Safety are built on NeuroSymbolic
methods that use data and knowledge to support requirements for critical
applications such as health and well-being. This article focuses on Large
Language Models (LLMs) as the chosen AI system within the CREST framework. LLMs
have garnered substantial attention from researchers due to their versatility
in handling a broad array of natural language processing (NLP) scenarios. For
example, ChatGPT and Google&apos;s MedPaLM have emerged as highly promising
platforms for providing information in general and health-related queries,
respectively. Nevertheless, these models remain black boxes despite
incorporating human feedback and instruction-guided tuning. For instance,
ChatGPT can generate unsafe responses despite instituting safety guardrails.
CREST presents a plausible approach harnessing procedural and graph-based
knowledge within a NeuroSymbolic framework to shed light on the challenges
associated with LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaur_M/0/1/0/all/0/1&quot;&gt;Manas Gaur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1&quot;&gt;Amit Sheth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06799">
<title>Densify Your Labels: Unsupervised Clustering with Bipartite Matching for Weakly Supervised Point Cloud Segmentation. (arXiv:2312.06799v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06799</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a weakly supervised semantic segmentation method for point clouds
that predicts &quot;per-point&quot; labels from just &quot;whole-scene&quot; annotations while
achieving the performance of recent fully supervised approaches. Our core idea
is to propagate the scene-level labels to each point in the point cloud by
creating pseudo labels in a conservative way. Specifically, we over-segment
point cloud features via unsupervised clustering and associate scene-level
labels with clusters through bipartite matching, thus propagating scene labels
only to the most relevant clusters, leaving the rest to be guided solely via
unsupervised clustering. We empirically demonstrate that over-segmentation and
bipartite assignment plays a crucial role. We evaluate our method on ScanNet
and S3DIS datasets, outperforming state of the art, and demonstrate that we can
achieve results comparable to fully supervised methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1&quot;&gt;Shaobo Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_J/0/1/0/all/0/1&quot;&gt;Jun Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kania_K/0/1/0/all/0/1&quot;&gt;Kacper Kania&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1&quot;&gt;Leyuan Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1&quot;&gt;Andrea Tagliasacchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1&quot;&gt;Kwang Moo Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Weiwei Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06810">
<title>System-level Safety Guard: Safe Tracking Control through Uncertain Neural Network Dynamics Models. (arXiv:2312.06810v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.06810</link>
<description rdf:parseType="Literal">&lt;p&gt;The Neural Network (NN), as a black-box function approximator, has been
considered in many control and robotics applications. However, difficulties in
verifying the overall system safety in the presence of uncertainties hinder the
modular deployment of NN in safety-critical systems. In this paper, we leverage
the NNs as predictive models for trajectory tracking of unknown dynamical
systems. We consider controller design in the presence of both intrinsic
uncertainty and uncertainties from other system modules. In this setting, we
formulate the constrained trajectory tracking problem and show that it can be
solved using Mixed-integer Linear Programming (MILP). The proposed MILP-based
solution enjoys a provable safety guarantee for the overall system, and the
approach is empirically demonstrated in robot navigation and obstacle avoidance
through simulations. The demonstration videos are available at
https://xiaolisean.github.io/publication/2023-11-01-L4DC2024.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yutong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Girard_A/0/1/0/all/0/1&quot;&gt;Anouck Girard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolmanovsky_I/0/1/0/all/0/1&quot;&gt;Ilya Kolmanovsky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06820">
<title>Extracting Self-Consistent Causal Insights from Users Feedback with LLMs and In-context Learning. (arXiv:2312.06820v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.06820</link>
<description rdf:parseType="Literal">&lt;p&gt;Microsoft Windows Feedback Hub is designed to receive customer feedback on a
wide variety of subjects including critical topics such as power and battery.
Feedback is one of the most effective ways to have a grasp of users&apos; experience
with Windows and its ecosystem. However, the sheer volume of feedback received
by Feedback Hub makes it immensely challenging to diagnose the actual cause of
reported issues. To better understand and triage issues, we leverage Double
Machine Learning (DML) to associate users&apos; feedback with telemetry signals. One
of the main challenges we face in the DML pipeline is the necessity of domain
knowledge for model design (e.g., causal graph), which sometimes is either not
available or hard to obtain. In this work, we take advantage of reasoning
capabilities in Large Language Models (LLMs) to generate a prior model that
which to some extent compensates for the lack of domain knowledge and could be
used as a heuristic for measuring feedback informativeness. Our LLM-based
approach is able to extract previously known issues, uncover new bugs, and
identify sequences of events that lead to a bug, while minimizing out-of-domain
outputs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdali_S/0/1/0/all/0/1&quot;&gt;Sara Abdali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parikh_A/0/1/0/all/0/1&quot;&gt;Anjali Parikh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1&quot;&gt;Steve Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiciman_E/0/1/0/all/0/1&quot;&gt;Emre Kiciman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06828">
<title>Resetting a fixed broken ELBO. (arXiv:2312.06828v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2312.06828</link>
<description rdf:parseType="Literal">&lt;p&gt;Variational autoencoders (VAEs) are one class of generative probabilistic
latent-variable models designed for inference based on known data. They balance
reconstruction and regularizer terms. A variational approximation produces an
evidence lower bound (ELBO). Multiplying the regularizer term by beta provides
a beta-VAE/ELBO, improving disentanglement of the latent space. However, any
beta value different than unity violates the laws of conditional probability.
To provide a similarly-parameterized VAE, we develop a Renyi (versus Shannon)
entropy VAE, and a variational approximation RELBO that introduces a similar
parameter. The Renyi VAE has an additional Renyi regularizer-like term with a
conditional distribution that is not learned. The term is evaluated essentially
analytically using a Singular Value Decomposition method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cukier_R/0/1/0/all/0/1&quot;&gt;Robert I. Cukier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06832">
<title>Symptom-based Machine Learning Models for the Early Detection of COVID-19: A Narrative Review. (arXiv:2312.06832v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.06832</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the widespread testing protocols for COVID-19, there are still
significant challenges in early detection of the disease, which is crucial for
preventing its spread and optimizing patient outcomes. Owing to the limited
testing capacity in resource-strapped settings and the limitations of the
available traditional methods of testing, it has been established that a fast
and efficient strategy is important to fully stop the virus. Machine learning
models can analyze large datasets, incorporating patient-reported symptoms,
clinical data, and medical imaging. Symptom-based detection methods have been
developed to predict COVID-19, and they have shown promising results. In this
paper, we provide an overview of the landscape of symptoms-only machine
learning models for predicting COVID-19, including their performance and
limitations. The review will also examine the performance of symptom-based
models when compared to image-based models. Because different studies used
varying datasets, methodologies, and performance metrics. Selecting the model
that performs best relies on the context and objectives of the research.
However, based on the results, we observed that ensemble classifier performed
exceptionally well in predicting the occurrence of COVID-19 based on patient
symptoms with the highest overall accuracy of 97.88%. Gradient Boosting
Algorithm achieved an AUC (Area Under the Curve) of 0.90 and identified key
features contributing to the decision-making process. Image-based models, as
observed in the analyzed studies, have consistently demonstrated higher
accuracy than symptom-based models, often reaching impressive levels ranging
from 96.09% to as high as 99%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akinloye_M/0/1/0/all/0/1&quot;&gt;Moyosolu Akinloye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06833">
<title>The unreasonable effectiveness of AI CADe polyp detectors to generalize to new countries. (arXiv:2312.06833v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.06833</link>
<description rdf:parseType="Literal">&lt;p&gt;$\textbf{Background and aims}$: Artificial Intelligence (AI) Computer-Aided
Detection (CADe) is commonly used for polyp detection, but data seen in
clinical settings can differ from model training. Few studies evaluate how well
CADe detectors perform on colonoscopies from countries not seen during
training, and none are able to evaluate performance without collecting
expensive and time-intensive labels.
&lt;/p&gt;
&lt;p&gt;$\textbf{Methods}$: We trained a CADe polyp detector on Israeli colonoscopy
videos (5004 videos, 1106 hours) and evaluated on Japanese videos (354 videos,
128 hours) by measuring the True Positive Rate (TPR) versus false alarms per
minute (FAPM). We introduce a colonoscopy dissimilarity measure called &quot;MAsked
mediCal Embedding Distance&quot; (MACE) to quantify differences between
colonoscopies, without labels. We evaluated CADe on all Japan videos and on
those with the highest MACE.
&lt;/p&gt;
&lt;p&gt;$\textbf{Results}$: MACE correctly quantifies that narrow-band imaging (NBI)
and chromoendoscopy (CE) frames are less similar to Israel data than Japan
whitelight (bootstrapped z-test, |z| &amp;gt; 690, p &amp;lt; $10^{-8}$ for both). Despite
differences in the data, CADe performance on Japan colonoscopies was
non-inferior to Israel ones without additional training (TPR at 0.5 FAPM: 0.957
and 0.972 for Israel and Japan; TPR at 1.0 FAPM: 0.972 and 0.989 for Israel and
Japan; superiority test t &amp;gt; 45.2, p &amp;lt; $10^{-8}$). Despite not being trained on
NBI or CE, TPR on those subsets were non-inferior to Japan overall
(non-inferiority test t &amp;gt; 47.3, p &amp;lt; $10^{-8}$, $\delta$ = 1.5% for both).
&lt;/p&gt;
&lt;p&gt;$\textbf{Conclusion}$: Differences that prevent CADe detectors from
performing well in non-medical settings do not degrade the performance of our
AI CADe polyp detector when applied to data from a new country. MACE can help
medical AI models internationalize by identifying the most &quot;dissimilar&quot; data on
which to evaluate models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shor_J/0/1/0/all/0/1&quot;&gt;Joel Shor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamano_H/0/1/0/all/0/1&quot;&gt;Hiro-o Yamano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsurumaru_D/0/1/0/all/0/1&quot;&gt;Daisuke Tsurumaru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Intrator_Y/0/1/0/all/0/1&quot;&gt;Yotami Intrator&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kayama_H/0/1/0/all/0/1&quot;&gt;Hiroki Kayama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ledsam_J/0/1/0/all/0/1&quot;&gt;Joe Ledsam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamabe_A/0/1/0/all/0/1&quot;&gt;Atsushi Hamabe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ando_K/0/1/0/all/0/1&quot;&gt;Koji Ando&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ota_M/0/1/0/all/0/1&quot;&gt;Mitsuhiko Ota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ogino_H/0/1/0/all/0/1&quot;&gt;Haruei Ogino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakase_H/0/1/0/all/0/1&quot;&gt;Hiroshi Nakase&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kobayashi_K/0/1/0/all/0/1&quot;&gt;Kaho Kobayashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oki_E/0/1/0/all/0/1&quot;&gt;Eiji Oki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldenberg_R/0/1/0/all/0/1&quot;&gt;Roman Goldenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rivlin_E/0/1/0/all/0/1&quot;&gt;Ehud Rivlin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takemasa_I/0/1/0/all/0/1&quot;&gt;Ichiro Takemasa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06837">
<title>Spectral State Space Models. (arXiv:2312.06837v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.06837</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies sequence modeling for prediction tasks with long range
dependencies. We propose a new formulation for state space models based on
learning linear dynamical systems with the spectral filtering algorithm
[HSZ17]. This gives rise to a novel sequence prediction architecture we call
spectral state space models. The resulting models are evaluated on synthetic
dynamical systems. These evaluations support the theoretical benefits of
spectral filtering for tasks requiring very long range memory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_N/0/1/0/all/0/1&quot;&gt;Naman Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suo_D/0/1/0/all/0/1&quot;&gt;Daniel Suo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hazan_E/0/1/0/all/0/1&quot;&gt;Elad Hazan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06845">
<title>High-Cadence Thermospheric Density Estimation enabled by Machine Learning on Solar Imagery. (arXiv:2312.06845v1 [physics.space-ph])</title>
<link>http://arxiv.org/abs/2312.06845</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate estimation of thermospheric density is critical for precise modeling
of satellite drag forces in low Earth orbit (LEO). Improving this estimation is
crucial to tasks such as state estimation, collision avoidance, and re-entry
calculations. The largest source of uncertainty in determining thermospheric
density is modeling the effects of space weather driven by solar and
geomagnetic activity. Current operational models rely on ground-based proxy
indices which imperfectly correlate with the complexity of solar outputs and
geomagnetic responses. In this work, we directly incorporate NASA&apos;s Solar
Dynamics Observatory (SDO) extreme ultraviolet (EUV) spectral images into a
neural thermospheric density model to determine whether the predictive
performance of the model is increased by using space-based EUV imagery data
instead of, or in addition to, the ground-based proxy indices. We demonstrate
that EUV imagery can enable predictions with much higher temporal resolution
and replace ground-based proxies while significantly increasing performance
relative to current operational models. Our method paves the way for
assimilating EUV image data into operational thermospheric density forecasting
models for use in LEO satellite navigation processes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Malik_S/0/1/0/all/0/1&quot;&gt;Shreshth A. Malik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Walsh_J/0/1/0/all/0/1&quot;&gt;James Walsh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Acciarini_G/0/1/0/all/0/1&quot;&gt;Giacomo Acciarini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Berger_T/0/1/0/all/0/1&quot;&gt;Thomas E. Berger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Baydin_A/0/1/0/all/0/1&quot;&gt;At&amp;#x131;l&amp;#x131;m G&amp;#xfc;ne&amp;#x15f; Baydin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06850">
<title>NDELS: A Novel Approach for Nighttime Dehazing, Low-Light Enhancement, and Light Suppression. (arXiv:2312.06850v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06850</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper tackles the intricate challenge of improving the quality of
nighttime images under hazy and low-light conditions. Overcoming issues
including nonuniform illumination glows, texture blurring, glow effects, color
distortion, noise disturbance, and overall, low light have proven daunting.
Despite the inherent difficulties, this paper introduces a pioneering solution
named Nighttime Dehazing, Low-Light Enhancement, and Light Suppression (NDELS).
NDELS utilizes a unique network that combines three essential processes to
enhance visibility, brighten low-light regions, and effectively suppress glare
from bright light sources. In contrast to limited progress in nighttime
dehazing, unlike its daytime counterpart, NDELS presents a comprehensive and
innovative approach. The efficacy of NDELS is rigorously validated through
extensive comparisons with eight state-of-the-art algorithms across four
diverse datasets. Experimental results showcase the superior performance of our
method, demonstrating its outperformance in terms of overall image quality,
including color and edge enhancement. Quantitative (PSNR, SSIM) and qualitative
metrics (CLIPIQA, MANIQA, TRES), measure these results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernabel_S/0/1/0/all/0/1&quot;&gt;Silvano A. Bernabel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agaian_S/0/1/0/all/0/1&quot;&gt;Sos S. Agaian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06854">
<title>Self-supervised Machine Learning Based Approach to Orbit Modelling Applied to Space Traffic Management. (arXiv:2312.06854v1 [physics.space-ph])</title>
<link>http://arxiv.org/abs/2312.06854</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel methodology for improving the performance of
machine learning based space traffic management tasks through the use of a
pre-trained orbit model. Taking inspiration from BERT-like self-supervised
language models in the field of natural language processing, we introduce
ORBERT, and demonstrate the ability of such a model to leverage large
quantities of readily available orbit data to learn meaningful representations
that can be used to aid in downstream tasks. As a proof of concept of this
approach we consider the task of all vs. all conjunction screening, phrased
here as a machine learning time series classification task. We show that
leveraging unlabelled orbit data leads to improved performance, and that the
proposed approach can be particularly beneficial for tasks where the
availability of labelled data is limited.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Stevenson_E/0/1/0/all/0/1&quot;&gt;Emma Stevenson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Rodriguez_Fernandez_V/0/1/0/all/0/1&quot;&gt;Victor Rodriguez-Fernandez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Urrutxua_H/0/1/0/all/0/1&quot;&gt;Hodei Urrutxua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Morand_V/0/1/0/all/0/1&quot;&gt;Vincent Morand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Camacho_D/0/1/0/all/0/1&quot;&gt;David Camacho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06855">
<title>Multimodal Pretraining of Medical Time Series and Notes. (arXiv:2312.06855v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.06855</link>
<description rdf:parseType="Literal">&lt;p&gt;Within the intensive care unit (ICU), a wealth of patient data, including
clinical measurements and clinical notes, is readily available. This data is a
valuable resource for comprehending patient health and informing medical
decisions, but it also contains many challenges in analysis. Deep learning
models show promise in extracting meaningful patterns, but they require
extensive labeled data, a challenge in critical care. To address this, we
propose a novel approach employing self-supervised pretraining, focusing on the
alignment of clinical measurements and notes. Our approach combines contrastive
and masked token prediction tasks during pretraining. Semi-supervised
experiments on the MIMIC-III dataset demonstrate the effectiveness of our
self-supervised pretraining. In downstream tasks, including in-hospital
mortality prediction and phenotyping, our pretrained model outperforms
baselines in settings where only a fraction of the data is labeled, emphasizing
its ability to enhance ICU data analysis. Notably, our method excels in
situations where very few labels are available, as evidenced by an increase in
the AUC-ROC for in-hospital mortality by 0.17 and in AUC-PR for phenotyping by
0.1 when only 1% of labels are accessible. This work advances self-supervised
learning in the healthcare domain, optimizing clinical insights from abundant
yet challenging ICU data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+King_R/0/1/0/all/0/1&quot;&gt;Ryan King&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Tianbao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mortazavi_B/0/1/0/all/0/1&quot;&gt;Bobak Mortazavi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06868">
<title>RAFIC: Retrieval-Augmented Few-shot Image Classification. (arXiv:2312.06868v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06868</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot image classification is the task of classifying unseen images to one
of N mutually exclusive classes, using only a small number of training examples
for each class. The limited availability of these examples (denoted as K)
presents a significant challenge to classification accuracy in some cases. To
address this, we have developed a method for augmenting the set of K with an
addition set of A retrieved images. We call this system Retrieval-Augmented
Few-shot Image Classification (RAFIC). Through a series of experiments, we
demonstrate that RAFIC markedly improves performance of few-shot image
classification across two challenging datasets. RAFIC consists of two main
components: (a) a retrieval component which uses CLIP, LAION-5B, and faiss, in
order to efficiently retrieve images similar to the supplied images, and (b)
retrieval meta-learning, which learns to judiciously utilize the retrieved
images. Code and data is available at github.com/amirziai/rafic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1&quot;&gt;Hangfei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_L/0/1/0/all/0/1&quot;&gt;Li Miao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziai_A/0/1/0/all/0/1&quot;&gt;Amir Ziai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06869">
<title>Adversarial Estimation of Topological Dimension with Harmonic Score Maps. (arXiv:2312.06869v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.06869</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantification of the number of variables needed to locally explain complex
data is often the first step to better understanding it. Existing techniques
from intrinsic dimension estimation leverage statistical models to glean this
information from samples within a neighborhood. However, existing methods often
rely on well-picked hyperparameters and ample data as manifold dimension and
curvature increases. Leveraging insight into the fixed point of the score
matching objective as the score map is regularized by its Dirichlet energy, we
show that it is possible to retrieve the topological dimension of the manifold
learned by the score map. We then introduce a novel method to measure the
learned manifold&apos;s topological dimension (i.e., local intrinsic dimension)
using adversarial attacks, thereby generating useful interpretations of the
learned manifold.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeats_E/0/1/0/all/0/1&quot;&gt;Eric Yeats&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darwin_C/0/1/0/all/0/1&quot;&gt;Cameron Darwin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Frank Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hai Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06871">
<title>Using Analytics on Student Created Data to Content Validate Pedagogical Tools. (arXiv:2312.06871v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.06871</link>
<description rdf:parseType="Literal">&lt;p&gt;Conceptual and simulation models can function as useful pedagogical tools,
however it is important to categorize different outcomes when evaluating them
in order to more meaningfully interpret results. VERA is a ecology-based
conceptual modeling software that enables users to simulate interactions
between biotics and abiotics in an ecosystem, allowing users to form and then
verify hypothesis through observing a time series of the species populations.
In this paper, we classify this time series into common patterns found in the
domain of ecological modeling through two methods, hierarchical clustering and
curve fitting, illustrating a general methodology for showing content validity
when combining different pedagogical tools. When applied to a diverse sample of
263 models containing 971 time series collected from three different VERA user
categories: a Georgia Tech (GATECH), North Georgia Technical College (NGTC),
and ``Self Directed Learners&apos;&apos;, results showed agreement between both
classification methods on 89.38\% of the sample curves in the test set. This
serves as a good indication that our methodology for determining content
validity was successful.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kos_J/0/1/0/all/0/1&quot;&gt;John Kos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eaton_K/0/1/0/all/0/1&quot;&gt;Kenneth Eaton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Sareen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dass_R/0/1/0/all/0/1&quot;&gt;Rahul Dass&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buckley_S/0/1/0/all/0/1&quot;&gt;Stephen Buckley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1&quot;&gt;Sungeun An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goel_A/0/1/0/all/0/1&quot;&gt;Ashok Goel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06872">
<title>ELSA: Partial Weight Freezing for Overhead-Free Sparse Network Deployment. (arXiv:2312.06872v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.06872</link>
<description rdf:parseType="Literal">&lt;p&gt;We present ELSA, a practical solution for creating deep networks that can
easily be deployed at different levels of sparsity. The core idea is to embed
one or more sparse networks within a single dense network as a proper subset of
the weights. At prediction time, any sparse model can be extracted effortlessly
simply be zeroing out weights according to a predefined mask. ELSA is simple,
powerful and highly flexible. It can use essentially any existing technique for
network sparsification and network training. In particular, it does not
restrict the loss function, architecture or the optimization technique. Our
experiments show that ELSA&apos;s advantages of flexible deployment comes with no or
just a negligible reduction in prediction quality compared to the standard way
of using multiple sparse networks that are trained and stored independently.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Halvachi_P/0/1/0/all/0/1&quot;&gt;Paniz Halvachi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peste_A/0/1/0/all/0/1&quot;&gt;Alexandra Peste&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1&quot;&gt;Dan Alistarh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lampert_C/0/1/0/all/0/1&quot;&gt;Christoph H. Lampert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06874">
<title>Dozerformer: Sequence Adaptive Sparse Transformer for Multivariate Time Series Forecasting. (arXiv:2312.06874v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.06874</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformers have achieved remarkable performance in multivariate time
series(MTS) forecasting due to their capability to capture long-term
dependencies. However, the canonical attention mechanism has two key
limitations: (1) its quadratic time complexity limits the sequence length, and
(2) it generates future values from the entire historical sequence. To address
this, we propose a Dozer Attention mechanism consisting of three sparse
components: (1) Local, each query exclusively attends to keys within a
localized window of neighboring time steps. (2) Stride, enables each query to
attend to keys at predefined intervals. (3) Vary, allows queries to selectively
attend to keys from a subset of the historical sequence. Notably, the size of
this subset dynamically expands as forecasting horizons extend. Those three
components are designed to capture essential attributes of MTS data, including
locality, seasonality, and global temporal dependencies. Additionally, we
present the Dozerformer Framework, incorporating the Dozer Attention mechanism
for the MTS forecasting task. We evaluated the proposed Dozerformer framework
with recent state-of-the-art methods on nine benchmark datasets and confirmed
its superior performance. The code will be released after the manuscript is
accepted.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yifan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1&quot;&gt;Rui Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dascalu_S/0/1/0/all/0/1&quot;&gt;Sergiu M. Dascalu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harris_F/0/1/0/all/0/1&quot;&gt;Frederick C. Harris Jr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06877">
<title>A Novel Differentiable Loss Function for Unsupervised Graph Neural Networks in Graph Partitioning. (arXiv:2312.06877v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.06877</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we explore the graph partitioning problem, a pivotal
combina-torial optimization challenge with extensive applications in various
fields such as science, technology, and business. Recognized as an NP-hard
prob-lem, graph partitioning lacks polynomial-time algorithms for its
resolution. Recently, there has been a burgeoning interest in leveraging
machine learn-ing, particularly approaches like supervised, unsupervised, and
reinforce-ment learning, to tackle such NP-hard problems. However, these
methods face significant hurdles: supervised learning is constrained by the
necessity of labeled solution instances, which are often computationally
impractical to obtain; reinforcement learning grapples with instability in the
learning pro-cess; and unsupervised learning contends with the absence of a
differentia-ble loss function, a consequence of the discrete nature of most
combinatorial optimization problems. Addressing these challenges, our research
introduces a novel pipeline employing an unsupervised graph neural network to
solve the graph partitioning problem. The core innovation of this study is the
for-mulation of a differentiable loss function tailored for this purpose. We
rigor-ously evaluate our methodology against contemporary state-of-the-art
tech-niques, focusing on metrics: cuts and balance, and our findings reveal
that our is competitive with these leading methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhary_V/0/1/0/all/0/1&quot;&gt;Vivek Chaudhary&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06881">
<title>DYAD: A Descriptive Yet Abjuring Density efficient approximation to linear neural network layers. (arXiv:2312.06881v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.06881</link>
<description rdf:parseType="Literal">&lt;p&gt;We devise, implement and performance-asses DYAD, a layer which can serve as a
faster and more memory-efficient approximate replacement for linear layers,
(nn.Linear() in Pytorch). These layers appear in common subcomponents, such as
in the ff module of Transformers. DYAD is based on a bespoke near-sparse matrix
structure which approximates the dense &quot;weight&quot; matrix W that matrix-multiplies
the input in the typical realization of such a layer, a.k.a DENSE. Our
alternative near-sparse matrix structure is decomposable to a sum of 2 matrices
permutable to a block-sparse counterpart. These can be represented as 3D
tensors, which in unison allow a faster execution of matrix multiplication with
the mini-batched input matrix X compared to DENSE (O(rows(W ) x cols(W )) --&amp;gt;
O( rows(W ) x cols(W ) # of blocks )). As the crux of our experiments, we
pretrain both DYAD and DENSE variants of 2 sizes of the OPT arch and 1 size of
the Pythia arch, including at different token scales of the babyLM benchmark.
We find DYAD to be competitive (&amp;gt;= 90%) of DENSE performance on zero-shot (e.g.
BLIMP), few-shot (OPENLM) and finetuning (GLUE) benchmarks, while being &amp;gt;=7-15%
faster to train on-GPU even at 125m scale, besides surfacing larger speedups at
increasing scale and model width.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandy_S/0/1/0/all/0/1&quot;&gt;Sarin Chandy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1&quot;&gt;Varun Gangal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maggiotti_G/0/1/0/all/0/1&quot;&gt;Gabriel Maggiotti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06887">
<title>Understanding and Leveraging the Learning Phases of Neural Networks. (arXiv:2312.06887v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.06887</link>
<description rdf:parseType="Literal">&lt;p&gt;The learning dynamics of deep neural networks are not well understood. The
information bottleneck (IB) theory proclaimed separate fitting and compression
phases. But they have since been heavily debated. We comprehensively analyze
the learning dynamics by investigating a layer&apos;s reconstruction ability of the
input and prediction performance based on the evolution of parameters during
training. We empirically show the existence of three phases using common
datasets and architectures such as ResNet and VGG: (i) near constant
reconstruction loss, (ii) decrease, and (iii) increase. We also derive an
empirically grounded data model and prove the existence of phases for
single-layer networks. Technically, our approach leverages classical complexity
analysis. It differs from IB by relying on measuring reconstruction loss rather
than information theoretic measures to relate information of intermediate
layers and inputs. Our work implies a new best practice for transfer learning:
We show empirically that the pre-training of a classifier should stop well
before its performance is optimal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1&quot;&gt;Johannes Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabhushanka_M/0/1/0/all/0/1&quot;&gt;Mohit Prabhushanka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06899">
<title>LoRA-Enhanced Distillation on Guided Diffusion Models. (arXiv:2312.06899v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06899</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models, such as Stable Diffusion (SD), offer the ability to
generate high-resolution images with diverse features, but they come at a
significant computational and memory cost. In classifier-free guided diffusion
models, prolonged inference times are attributed to the necessity of computing
two separate diffusion models at each denoising step. Recent work has shown
promise in improving inference time through distillation techniques, teaching
the model to perform similar denoising steps with reduced computations.
However, the application of distillation introduces additional memory overhead
to these already resource-intensive diffusion models, making it less practical.
&lt;/p&gt;
&lt;p&gt;To address these challenges, our research explores a novel approach that
combines Low-Rank Adaptation (LoRA) with model distillation to efficiently
compress diffusion models. This approach not only reduces inference time but
also mitigates memory overhead, and notably decreases memory consumption even
before applying distillation. The results are remarkable, featuring a
significant reduction in inference time due to the distillation process and a
substantial 50% reduction in memory consumption. Our examination of the
generated images underscores that the incorporation of LoRA-enhanced
distillation maintains image quality and alignment with the provided prompts.
In summary, while conventional distillation tends to increase memory
consumption, LoRA-enhanced distillation offers optimization without any
trade-offs or compromises in quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golnari_P/0/1/0/all/0/1&quot;&gt;Pareesa Ameneh Golnari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06902">
<title>Perseus: Removing Energy Bloat from Large Model Training. (arXiv:2312.06902v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.06902</link>
<description rdf:parseType="Literal">&lt;p&gt;Training large AI models on numerous GPUs consumes a massive amount of
energy. We observe that not all energy consumed during training directly
contributes to end-to-end training throughput, and a significant portion can be
removed without slowing down training, which we call energy bloat.
&lt;/p&gt;
&lt;p&gt;In this work, we identify two independent sources of energy bloat in large
model training, intrinsic and extrinsic, and propose Perseus, a unified
optimization framework that mitigates both. Perseus obtains the &quot;iteration
time-energy&quot; Pareto frontier of any large model training job using an efficient
iterative graph cut-based algorithm and schedules energy consumption of its
forward and backward computations across time to remove intrinsic and extrinsic
energy bloat. Evaluation on large models like GPT-3 and Bloom shows that
Perseus reduces energy consumption of large model training by up to 30%,
enabling savings otherwise unobtainable before.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1&quot;&gt;Jae-Won Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yile Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_I/0/1/0/all/0/1&quot;&gt;Insu Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1&quot;&gt;Luoxi Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_N/0/1/0/all/0/1&quot;&gt;Nikhil Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1&quot;&gt;Mosharaf Chowdhury&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06914">
<title>Exploring Novel Object Recognition and Spontaneous Location Recognition Machine Learning Analysis Techniques in Alzheimer&apos;s Mice. (arXiv:2312.06914v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.06914</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding object recognition patterns in mice is crucial for advancing
behavioral neuroscience and has significant implications for human health,
particularly in the realm of Alzheimer&apos;s research. This study is centered on
the development, application, and evaluation of a state-of-the-art
computational pipeline designed to analyze such behaviors, specifically
focusing on Novel Object Recognition (NOR) and Spontaneous Location Recognition
(SLR) tasks. The pipeline integrates three advanced computational models:
Any-Maze for initial data collection, DeepLabCut for detailed pose estimation,
and Convolutional Neural Networks (CNNs) for nuanced behavioral classification.
Employed across four distinct mouse groups, this pipeline demonstrated high
levels of accuracy and robustness. Despite certain challenges like video
quality limitations and the need for manual calculations, the results affirm
the pipeline&apos;s efficacy and potential for scalability. The study serves as a
proof of concept for a multidimensional computational approach to behavioral
neuroscience, emphasizing the pipeline&apos;s versatility and readiness for future,
more complex analyses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bafana_S/0/1/0/all/0/1&quot;&gt;Soham Bafana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raghuraman_R/0/1/0/all/0/1&quot;&gt;Radha Raghuraman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hussaini_S/0/1/0/all/0/1&quot;&gt;S. Abid Hussaini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06925">
<title>Facial Emotion Recognition in VR Games. (arXiv:2312.06925v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2312.06925</link>
<description rdf:parseType="Literal">&lt;p&gt;Emotion detection is a crucial component of Games User Research (GUR), as it
allows game developers to gain insights into players&apos; emotional experiences and
tailor their games accordingly. However, detecting emotions in Virtual Reality
(VR) games is challenging due to the Head-Mounted Display (HMD) that covers the
top part of the player&apos;s face, namely, their eyes and eyebrows, which provide
crucial information for recognizing the impression. To tackle this we used a
Convolutional Neural Network (CNN) to train a model to predict emotions in
full-face images where the eyes and eyebrows are covered. We used the FER2013
dataset, which we modified to cover eyes and eyebrows in images. The model in
these images can accurately recognize seven different emotions which are anger,
happiness, disgust, fear, impartiality, sadness and surprise.
&lt;/p&gt;
&lt;p&gt;We assessed the model&apos;s performance by testing it on two VR games and using
it to detect players&apos; emotions. We collected self-reported emotion data from
the players after the gameplay sessions. We analyzed the data collected from
our experiment to understand which emotions players experience during the
gameplay. We found that our approach has the potential to enhance gameplay
analysis by enabling the detection of players&apos; emotions in VR games, which can
help game developers create more engaging and immersive game experiences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dehghani_F/0/1/0/all/0/1&quot;&gt;Fatemeh Dehghani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaman_L/0/1/0/all/0/1&quot;&gt;Loutfouz Zaman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06932">
<title>Predictive variational autoencoder for learning robust representations of time-series data. (arXiv:2312.06932v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.06932</link>
<description rdf:parseType="Literal">&lt;p&gt;Variational autoencoders (VAEs) have been used extensively to discover
low-dimensional latent factors governing neural activity and animal behavior.
However, without careful model selection, the uncovered latent factors may
reflect noise in the data rather than true underlying features, rendering such
representations unsuitable for scientific interpretation. Existing solutions to
this problem involve introducing additional measured variables or data
augmentations specific to a particular data type. We propose a VAE architecture
that predicts the next point in time and show that it mitigates the learning of
spurious features. In addition, we introduce a model selection metric based on
smoothness over time in the latent space. We show that together these two
constraints on VAEs to be smooth over time produce robust latent
representations and faithfully recover latent factors on synthetic datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Julia Huiming Wang&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsin_D/0/1/0/all/0/1&quot;&gt;Dexter Tsin&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Engel_T/0/1/0/all/0/1&quot;&gt;Tatiana Engel&lt;/a&gt; (2) ((1) Cold Spring Harbor School of Biological Sciences, (2) Princeton Neuroscience Institute)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06937">
<title>Can a Transformer Represent a Kalman Filter?. (arXiv:2312.06937v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.06937</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformers are a class of autoregressive deep learning architectures which
have recently achieved state-of-the-art performance in various vision,
language, and robotics tasks. We revisit the problem of Kalman Filtering in
linear dynamical systems and show that Transformers can approximate the Kalman
Filter in a strong sense. Specifically, for any observable LTI system we
construct an explicit causally-masked Transformer which implements the Kalman
Filter, up to a small additive error which is bounded uniformly in time; we
call our construction the Transformer Filter. Our construction is based on a
two-step reduction. We first show that a softmax self-attention block can
exactly represent a certain Gaussian kernel smoothing estimator. We then show
that this estimator closely approximates the Kalman Filter. We also investigate
how the Transformer Filter can be used for measurement-feedback control and
prove that the resulting nonlinear controllers closely approximate the
performance of standard optimal control policies such as the LQG controller.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goel_G/0/1/0/all/0/1&quot;&gt;Gautam Goel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bartlett_P/0/1/0/all/0/1&quot;&gt;Peter Bartlett&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06940">
<title>Benchmarking Deep Learning Classifiers for SAR Automatic Target Recognition. (arXiv:2312.06940v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06940</link>
<description rdf:parseType="Literal">&lt;p&gt;Synthetic Aperture Radar SAR Automatic Target Recognition ATR is a key
technique of remote-sensing image recognition which can be supported by deep
neural networks The existing works of SAR ATR mostly focus on improving the
accuracy of the target recognition while ignoring the systems performance in
terms of speed and storage which is critical to real-world applications of SAR
ATR For decision-makers aiming to identify a proper deep learning model to
deploy in a SAR ATR system it is important to understand the performance of
different candidate deep learning models and determine the best model
accordingly This paper comprehensively benchmarks several advanced deep
learning models for SAR ATR with multiple distinct SAR imagery datasets
Specifically we train and test five SAR image classifiers based on Residual
Neural Networks ResNet18 ResNet34 ResNet50 Graph Neural Network GNN and Vision
Transformer for Small-Sized Datasets (SS-ViT) We select three datasets MSTAR
GBSAR and SynthWakeSAR that offer heterogeneity We evaluate and compare the
five classifiers concerning their classification accuracy runtime performance
in terms of inference throughput and analytical performance in terms of number
of parameters number of layers model size and number of operations Experimental
results show that the GNN classifier outperforms with respect to throughput and
latency However it is also shown that no clear model winner emerges from all of
our chosen metrics and a one model rules all case is doubtful in the domain of
SAR ATR
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fein_Ashley_J/0/1/0/all/0/1&quot;&gt;Jacob Fein-Ashley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1&quot;&gt;Tian Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kannan_R/0/1/0/all/0/1&quot;&gt;Rajgopal Kannan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prasanna_V/0/1/0/all/0/1&quot;&gt;Viktor Prasanna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Busart_C/0/1/0/all/0/1&quot;&gt;Carl Busart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06941">
<title>Humans vs Large Language Models: Judgmental Forecasting in an Era of Advanced AI. (arXiv:2312.06941v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.06941</link>
<description rdf:parseType="Literal">&lt;p&gt;This study investigates the forecasting accuracy of human experts versus
Large Language Models (LLMs) in the retail sector, particularly during standard
and promotional sales periods. Utilizing a controlled experimental setup with
123 human forecasters and five LLMs, including ChatGPT4, ChatGPT3.5, Bard,
Bing, and Llama2, we evaluated forecasting precision through Mean Absolute
Percentage Error. Our analysis centered on the effect of the following factors
on forecasters performance: the supporting statistical model (baseline and
advanced), whether the product was on promotion, and the nature of external
impact. The findings indicate that LLMs do not consistently outperform humans
in forecasting accuracy and that advanced statistical forecasting models do not
uniformly enhance the performance of either human forecasters or LLMs. Both
human and LLM forecasters exhibited increased forecasting errors, particularly
during promotional periods and under the influence of positive external
impacts. Our findings call for careful consideration when integrating LLMs into
practical forecasting processes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abolghasemi_M/0/1/0/all/0/1&quot;&gt;MAhdi Abolghasemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganbold_O/0/1/0/all/0/1&quot;&gt;Odkhishig Ganbold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rotaru_K/0/1/0/all/0/1&quot;&gt;Kristian Rotaru&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06942">
<title>AI Control: Improving Safety Despite Intentional Subversion. (arXiv:2312.06942v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.06942</link>
<description rdf:parseType="Literal">&lt;p&gt;As large language models (LLMs) become more powerful and are deployed more
autonomously, it will be increasingly important to prevent them from causing
harmful outcomes. Researchers have investigated a variety of safety techniques
for this purpose, e.g. using models to review the outputs of other models, or
red-teaming techniques to surface subtle failure modes. However, researchers
have not evaluated whether such techniques still ensure safety if the model is
itself intentionally trying to subvert them. In this paper, we develop and
evaluate pipelines of safety techniques (&quot;protocols&quot;) that are robust to
intentional subversion.
&lt;/p&gt;
&lt;p&gt;We investigate a scenario in which we want to solve a sequence of programming
problems, using access to a powerful but untrusted model (in our case, GPT-4),
access to a less powerful trusted model (in our case, GPT-3.5), and limited
access to human contractors who provide high-quality trusted labor. We
investigate protocols that aim to never submit solutions containing backdoors,
which we operationalize here as logical errors that are not caught by test
cases.
&lt;/p&gt;
&lt;p&gt;We investigate a range of protocols and test each against strategies that the
untrusted model could use to subvert them. One protocol is what we call trusted
editing. This protocol first asks GPT-4 to write code, and then asks GPT-3.5 to
rate the suspiciousness of that code. If the code is below some suspiciousness
threshold, it is submitted. Otherwise, GPT-3.5 edits the solution to remove
parts that seem suspicious and then submits the edited code. Another protocol
is untrusted monitoring. This protocol asks GPT-4 to write code, and then asks
another instance of GPT-4 whether the code is backdoored, using various
techniques to prevent the GPT-4 instances from colluding. These protocols
improve substantially on simple baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Greenblatt_R/0/1/0/all/0/1&quot;&gt;Ryan Greenblatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shlegeris_B/0/1/0/all/0/1&quot;&gt;Buck Shlegeris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sachan_K/0/1/0/all/0/1&quot;&gt;Kshitij Sachan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roger_F/0/1/0/all/0/1&quot;&gt;Fabien Roger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06951">
<title>Feature Norm Regularized Federated Learning: Transforming Skewed Distributions into Global Insights. (arXiv:2312.06951v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.06951</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of federated learning, addressing non-independent and
identically distributed (non-i.i.d.) data remains a quintessential challenge
for improving global model performance. This work introduces the Feature Norm
Regularized Federated Learning (FNR-FL) algorithm, which uniquely incorporates
class average feature norms to enhance model accuracy and convergence in
non-i.i.d. scenarios. Our comprehensive analysis reveals that FNR-FL not only
accelerates convergence but also significantly surpasses other contemporary
federated learning algorithms in test accuracy, particularly under feature
distribution skew scenarios. The novel modular design of FNR-FL facilitates
seamless integration with existing federated learning frameworks, reinforcing
its adaptability and potential for widespread application. We substantiate our
claims through rigorous empirical evaluations, demonstrating FNR-FL&apos;s
exceptional performance across various skewed data distributions. Relative to
FedAvg, FNR-FL exhibits a substantial 66.24\% improvement in accuracy and a
significant 11.40\% reduction in training time, underscoring its enhanced
effectiveness and efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1&quot;&gt;Ke Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1&quot;&gt;WeiDong Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_P/0/1/0/all/0/1&quot;&gt;Peng Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06957">
<title>Online Saddle Point Problem and Online Convex-Concave Optimization. (arXiv:2312.06957v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.06957</link>
<description rdf:parseType="Literal">&lt;p&gt;Centered around solving the Online Saddle Point problem, this paper
introduces the Online Convex-Concave Optimization (OCCO) framework, which
involves a sequence of two-player time-varying convex-concave games. We propose
the generalized duality gap (Dual-Gap) as the performance metric and establish
the parallel relationship between OCCO with Dual-Gap and Online Convex
Optimization (OCO) with regret. To demonstrate the natural extension of OCCO
from OCO, we develop two algorithms, the implicit online mirror descent-ascent
and its optimistic variant. Analysis reveals that their duality gaps share
similar expression forms with the corresponding dynamic regrets arising from
implicit updates in OCO. Empirical results further substantiate the
effectiveness of our algorithms. Simultaneously, we unveil that the dynamic
Nash equilibrium regret, which was initially introduced in a recent paper, has
inherent defects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1&quot;&gt;Qing-xin Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jian-wei Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06958">
<title>PatchMorph: A Stochastic Deep Learning Approach for Unsupervised 3D Brain Image Registration with Small Patches. (arXiv:2312.06958v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06958</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce &quot;PatchMorph,&quot; an new stochastic deep learning algorithm tailored
for unsupervised 3D brain image registration. Unlike other methods, our method
uses compact patches of a constant small size to derive solutions that can
combine global transformations with local deformations. This approach minimizes
the memory footprint of the GPU during training, but also enables us to operate
on numerous amounts of randomly overlapping small patches during inference to
mitigate image and patch boundary problems. PatchMorph adeptly handles world
coordinate transformations between two input images, accommodating variances in
attributes such as spacing, array sizes, and orientations. The spatial
resolution of patches transitions from coarse to fine, addressing both global
and local attributes essential for aligning the images. Each patch offers a
unique perspective, together converging towards a comprehensive solution.
Experiments on human T1 MRI brain images and marmoset brain images from serial
2-photon tomography affirm PatchMorph&apos;s superior performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skibbe_H/0/1/0/all/0/1&quot;&gt;Henrik Skibbe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Byra_M/0/1/0/all/0/1&quot;&gt;Michal Byra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watakabe_A/0/1/0/all/0/1&quot;&gt;Akiya Watakabe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamamori_T/0/1/0/all/0/1&quot;&gt;Tetsuo Yamamori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reisert_M/0/1/0/all/0/1&quot;&gt;Marco Reisert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06960">
<title>Remote Sensing Vision-Language Foundation Models without Annotations via Ground Remote Alignment. (arXiv:2312.06960v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06960</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a method to train vision-language models for remote-sensing
images without using any textual annotations. Our key insight is to use
co-located internet imagery taken on the ground as an intermediary for
connecting remote-sensing images and language. Specifically, we train an image
encoder for remote sensing images to align with the image encoder of CLIP using
a large amount of paired internet and satellite images. Our unsupervised
approach enables the training of a first-of-its-kind large-scale vision
language model (VLM) for remote sensing images at two different resolutions. We
show that these VLMs enable zero-shot, open-vocabulary image classification,
retrieval, segmentation and visual question answering for satellite images. On
each of these tasks, our VLM trained without textual annotations outperforms
existing VLMs trained with supervision, with gains of up to 20% for
classification and 80% for segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mall_U/0/1/0/all/0/1&quot;&gt;Utkarsh Mall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phoo_C/0/1/0/all/0/1&quot;&gt;Cheng Perng Phoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Meilin Kelsey Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vondrick_C/0/1/0/all/0/1&quot;&gt;Carl Vondrick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hariharan_B/0/1/0/all/0/1&quot;&gt;Bharath Hariharan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bala_K/0/1/0/all/0/1&quot;&gt;Kavita Bala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06973">
<title>Anytime Approximate Formal Feature Attribution. (arXiv:2312.06973v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.06973</link>
<description rdf:parseType="Literal">&lt;p&gt;Widespread use of artificial intelligence (AI) algorithms and machine
learning (ML) models on the one hand and a number of crucial issues pertaining
to them warrant the need for explainable artificial intelligence (XAI). A key
explainability question is: given this decision was made, what are the input
features which contributed to the decision? Although a range of XAI approaches
exist to tackle this problem, most of them have significant limitations.
Heuristic XAI approaches suffer from the lack of quality guarantees, and often
try to approximate Shapley values, which is not the same as explaining which
features contribute to a decision. A recent alternative is so-called formal
feature attribution (FFA), which defines feature importance as the fraction of
formal abductive explanations (AXp&apos;s) containing the given feature. This
measures feature importance from the view of formally reasoning about the
model&apos;s behavior. It is challenging to compute FFA using its definition because
that involves counting AXp&apos;s, although one can approximate it. Based on these
results, this paper makes several contributions. First, it gives compelling
evidence that computing FFA is intractable, even if the set of contrastive
formal explanations (CXp&apos;s) is provided, by proving that the problem is
#P-hard. Second, by using the duality between AXp&apos;s and CXp&apos;s, it proposes an
efficient heuristic to switch from CXp enumeration to AXp enumeration
on-the-fly resulting in an adaptive explanation enumeration algorithm
effectively approximating FFA in an anytime fashion. Finally, experimental
results obtained on a range of widely used datasets demonstrate the
effectiveness of the proposed FFA approximation approach in terms of the error
of FFA approximation as well as the number of explanations computed and their
diversity given a fixed time limit.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jinqiang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farr_G/0/1/0/all/0/1&quot;&gt;Graham Farr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ignatiev_A/0/1/0/all/0/1&quot;&gt;Alexey Ignatiev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stuckey_P/0/1/0/all/0/1&quot;&gt;Peter J. Stuckey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06979">
<title>On the notion of Hallucinations from the lens of Bias and Validity in Synthetic CXR Images. (arXiv:2312.06979v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.06979</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical imaging has revolutionized disease diagnosis, yet the potential is
hampered by limited access to diverse and privacy-conscious datasets.
Open-source medical datasets, while valuable, suffer from data quality and
clinical information disparities. Generative models, such as diffusion models,
aim to mitigate these challenges. At Stanford, researchers explored the utility
of a fine-tuned Stable Diffusion model (RoentGen) for medical imaging data
augmentation. Our work examines specific considerations to expand the Stanford
research question, Could Stable Diffusion Solve a Gap in Medical Imaging Data?
from the lens of bias and validity of the generated outcomes. We leveraged
RoentGen to produce synthetic Chest-XRay (CXR) images and conducted assessments
on bias, validity, and hallucinations. Diagnostic accuracy was evaluated by a
disease classifier, while a COVID classifier uncovered latent hallucinations.
The bias analysis unveiled disparities in classification performance among
various subgroups, with a pronounced impact on the Female Hispanic subgroup.
Furthermore, incorporating race and gender into input prompts exacerbated
fairness issues in the generated images. The quality of synthetic images
exhibited variability, particularly in certain disease classes, where there was
more significant uncertainty compared to the original images. Additionally, we
observed latent hallucinations, with approximately 42% of the images
incorrectly indicating COVID, hinting at the presence of hallucinatory
elements. These identifications provide new research directions towards
interpretability of synthetic CXR images, for further understanding of
associated risks and patient safety in medical applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bhardwaj_G/0/1/0/all/0/1&quot;&gt;Gauri Bhardwaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Govindarajulu_Y/0/1/0/all/0/1&quot;&gt;Yuvaraj Govindarajulu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Narayanan_S/0/1/0/all/0/1&quot;&gt;Sundaraparipurnan Narayanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kulkarni_P/0/1/0/all/0/1&quot;&gt;Pavan Kulkarni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Parmar_M/0/1/0/all/0/1&quot;&gt;Manojkumar Parmar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06993">
<title>Dynamically configured physics-informed neural network in topology optimization applications. (arXiv:2312.06993v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.06993</link>
<description rdf:parseType="Literal">&lt;p&gt;Integration of machine learning (ML) into the topology optimization (TO)
framework is attracting increasing attention, but data acquisition in
data-driven models is prohibitive. Compared with popular ML methods, the
physics-informed neural network (PINN) can avoid generating enormous amounts of
data when solving forward problems and additionally provide better inference.
To this end, a dynamically configured PINN-based topology optimization
(DCPINN-TO) method is proposed. The DCPINN is composed of two subnetworks,
namely the backbone neural network (NN) and the coefficient NN, where the
coefficient NN has fewer trainable parameters. The designed architecture aims
to dynamically configure trainable parameters; that is, an inexpensive NN is
used to replace an expensive one at certain optimization cycles. Furthermore,
an active sampling strategy is proposed to selectively sample collocations
depending on the pseudo-densities at each optimization cycle. In this manner,
the number of collocations will decrease with the optimization process but will
hardly affect it. The Gaussian integral is used to calculate the strain energy
of elements, which yields a byproduct of decoupling the mapping of the material
at the collocations. Several examples with different resolutions validate the
feasibility of the DCPINN-TO method, and multiload and multiconstraint problems
are employed to illustrate its generalization. In addition, compared to finite
element analysis-based TO (FEA-TO), the accuracy of the displacement prediction
and optimization results indicate that the DCPINN-TO method is effective and
efficient.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1&quot;&gt;Jichao Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1&quot;&gt;Ziming Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuhao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhanga_Y/0/1/0/all/0/1&quot;&gt;Yaya Zhanga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hu Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07003">
<title>RACER: Rational Artificial Intelligence Car-following-model Enhanced by Reality. (arXiv:2312.07003v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.07003</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces RACER, the Rational Artificial Intelligence
Car-following model Enhanced by Reality, a cutting-edge deep learning
car-following model, that satisfies partial derivative constraints, designed to
predict Adaptive Cruise Control (ACC) driving behavior while staying
theoretically feasible. Unlike conventional models, RACER effectively
integrates Rational Driving Constraints (RDCs), crucial tenets of actual
driving, resulting in strikingly accurate and realistic predictions. Against
established models like the Optimal Velocity Relative Velocity (OVRV), a
car-following Neural Network (NN), and a car-following Physics-Informed Neural
Network (PINN), RACER excels across key metrics, such as acceleration,
velocity, and spacing. Notably, it displays a perfect adherence to the RDCs,
registering zero violations, in stark contrast to other models. This study
highlights the immense value of incorporating physical constraints within AI
models, especially for augmenting safety measures in transportation. It also
paves the way for future research to test these models against human driving
data, with the potential to guide safer and more rational driving behavior. The
versatility of the proposed model, including its potential to incorporate
additional derivative constraints and broader architectural applications,
enhances its appeal and broadens its impact within the scientific community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tianyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Halatsis_A/0/1/0/all/0/1&quot;&gt;Alexander Halatsis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stern_R/0/1/0/all/0/1&quot;&gt;Raphael Stern&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07022">
<title>EdgePruner: Poisoned Edge Pruning in Graph Contrastive Learning. (arXiv:2312.07022v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.07022</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Contrastive Learning (GCL) is unsupervised graph representation
learning that can obtain useful representation of unknown nodes. The node
representation can be utilized as features of downstream tasks. However, GCL is
vulnerable to poisoning attacks as with existing learning models. A
state-of-the-art defense cannot sufficiently negate adverse effects by poisoned
graphs although such a defense introduces adversarial training in the GCL. To
achieve further improvement, pruning adversarial edges is important. To the
best of our knowledge, the feasibility remains unexplored in the GCL domain. In
this paper, we propose a simple defense for GCL, EdgePruner. We focus on the
fact that the state-of-the-art poisoning attack on GCL tends to mainly add
adversarial edges to create poisoned graphs, which means that pruning edges is
important to sanitize the graphs. Thus, EdgePruner prunes edges that contribute
to minimizing the contrastive loss based on the node representation obtained
after training on poisoned graphs by GCL. Furthermore, we focus on the fact
that nodes with distinct features are connected by adversarial edges in
poisoned graphs. Thus, we introduce feature similarity between neighboring
nodes to help more appropriately determine adversarial edges. This similarity
is helpful in further eliminating adverse effects from poisoned graphs on
various datasets. Finally, EdgePruner outputs a graph that yields the minimum
contrastive loss as the sanitized graph. Our results demonstrate that pruning
adversarial edges is feasible on six datasets. EdgePruner can improve the
accuracy of node classification under the attack by up to 5.55% compared with
that of the state-of-the-art defense. Moreover, we show that EdgePruner is
immune to an adaptive attack.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kato_H/0/1/0/all/0/1&quot;&gt;Hiroya Kato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasegawa_K/0/1/0/all/0/1&quot;&gt;Kento Hasegawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hidano_S/0/1/0/all/0/1&quot;&gt;Seira Hidano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fukushima_K/0/1/0/all/0/1&quot;&gt;Kazuhide Fukushima&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07032">
<title>Ahpatron: A New Budgeted Online Kernel Learning Machine with Tighter Mistake Bound. (arXiv:2312.07032v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07032</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the mistake bound of online kernel learning on a
budget. We propose a new budgeted online kernel learning model, called
Ahpatron, which significantly improves the mistake bound of previous work and
resolves the open problem posed by Dekel, Shalev-Shwartz, and Singer (2005). We
first present an aggressive variant of Perceptron, named AVP, a model without
budget, which uses an active updating rule. Then we design a new budget
maintenance mechanism, which removes a half of examples,and projects the
removed examples onto a hypothesis space spanned by the remaining examples.
Ahpatron adopts the above mechanism to approximate AVP. Theoretical analyses
prove that Ahpatron has tighter mistake bounds, and experimental results show
that Ahpatron outperforms the state-of-the-art algorithms on the same or a
smaller budget.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1&quot;&gt;Yun Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Junfan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1&quot;&gt;Shizhong Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1&quot;&gt;Qinghua Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dang_J/0/1/0/all/0/1&quot;&gt;Jianwu Dang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07035">
<title>HyperRouter: Towards Efficient Training and Inference of Sparse Mixture of Experts. (arXiv:2312.07035v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07035</link>
<description rdf:parseType="Literal">&lt;p&gt;By routing input tokens to only a few split experts, Sparse
Mixture-of-Experts has enabled efficient training of large language models.
Recent findings suggest that fixing the routers can achieve competitive
performance by alleviating the collapsing problem, where all experts eventually
learn similar representations. However, this strategy has two key limitations:
(i) the policy derived from random routers might be sub-optimal, and (ii) it
requires extensive resources during training and evaluation, leading to limited
efficiency gains. This work introduces \HyperRout, which dynamically generates
the router&apos;s parameters through a fixed hypernetwork and trainable embeddings
to achieve a balance between training the routers and freezing them to learn an
improved routing policy. Extensive experiments across a wide range of tasks
demonstrate the superior performance and efficiency gains of \HyperRouter
compared to existing routing methods. Our implementation is publicly available
at {\url{{https://github.com/giangdip2410/HyperRouter}}}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Do_G/0/1/0/all/0/1&quot;&gt;Giang Do&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_K/0/1/0/all/0/1&quot;&gt;Khiem Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_Q/0/1/0/all/0/1&quot;&gt;Quang Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;TrungTin Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doan_T/0/1/0/all/0/1&quot;&gt;Thanh-Nam Doan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1&quot;&gt;Bint T. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chenghao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramasamy_S/0/1/0/all/0/1&quot;&gt;Savitha Ramasamy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoli Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1&quot;&gt;Steven Hoi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07044">
<title>Large Foundation Models for Power Systems. (arXiv:2312.07044v1 [eess.SY])</title>
<link>http://arxiv.org/abs/2312.07044</link>
<description rdf:parseType="Literal">&lt;p&gt;Foundation models, such as Large Language Models (LLMs), can respond to a
wide range of format-free queries without any task-specific data collection or
model training, creating various research and application opportunities for the
modeling and operation of large-scale power systems. In this paper, we outline
how such large foundation model such as GPT-4 are developed, and discuss how
they can be leveraged in challenging power and energy system tasks. We first
investigate the potential of existing foundation models by validating their
performance on four representative tasks across power system domains, including
the optimal power flow (OPF), electric vehicle (EV) scheduling, knowledge
retrieval for power engineering technical reports, and situation awareness. Our
results indicate strong capabilities of such foundation models on boosting the
efficiency and reliability of power system operational pipelines. We also
provide suggestions and projections on future deployment of foundation models
in power system applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chenghao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Siyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Ruohong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yize Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07046">
<title>Rethinking Compression: Reduced Order Modelling of Latent Features in Large Language Models. (arXiv:2312.07046v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07046</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the substantial scale of Large Language Models (LLMs), the direct
application of conventional compression methodologies proves impractical. The
computational demands associated with even minimal gradient updates present
challenges, particularly on consumer-grade hardware. This paper introduces an
innovative approach for the parametric and practical compression of LLMs based
on reduced order modelling, which entails low-rank decomposition within the
feature space and re-parameterization in the weight space. Notably, this
compression technique operates in a layer-wise manner, obviating the need for a
GPU device and enabling the compression of billion-scale models within
stringent constraints of both memory and time. Our method represents a
significant advancement in model compression by leveraging matrix
decomposition, demonstrating superior efficacy compared to the prevailing
state-of-the-art structured pruning method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chavan_A/0/1/0/all/0/1&quot;&gt;Arnav Chavan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lele_N/0/1/0/all/0/1&quot;&gt;Nahush Lele&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1&quot;&gt;Deepak Gupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07064">
<title>Efficient Cross-Domain Federated Learning by MixStyle Approximation. (arXiv:2312.07064v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07064</link>
<description rdf:parseType="Literal">&lt;p&gt;With the advent of interconnected and sensor-equipped edge devices, Federated
Learning (FL) has gained significant attention, enabling decentralized learning
while maintaining data privacy. However, FL faces two challenges in real-world
tasks: expensive data labeling and domain shift between source and target
samples. In this paper, we introduce a privacy-preserving, resource-efficient
FL concept for client adaptation in hardware-constrained environments. Our
approach includes server model pre-training on source data and subsequent
fine-tuning on target data via low-end clients. The local client adaptation
process is streamlined by probabilistic mixing of instance-level feature
statistics approximated from source and target domain data. The adapted
parameters are transferred back to the central server and globally aggregated.
Preliminary results indicate that our method reduces computational and
transmission costs while maintaining competitive performance on downstream
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roder_M/0/1/0/all/0/1&quot;&gt;Manuel R&amp;#xf6;der&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heller_L/0/1/0/all/0/1&quot;&gt;Leon Heller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munch_M/0/1/0/all/0/1&quot;&gt;Maximilian M&amp;#xfc;nch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schleif_F/0/1/0/all/0/1&quot;&gt;Frank-Michael Schleif&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07067">
<title>Focus on Hiders: Exploring Hidden Threats for Enhancing Adversarial Training. (arXiv:2312.07067v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07067</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial training is often formulated as a min-max problem, however,
concentrating only on the worst adversarial examples causes alternating
repetitive confusion of the model, i.e., previously defended or correctly
classified samples are not defensible or accurately classifiable in subsequent
adversarial training. We characterize such non-ignorable samples as &quot;hiders&quot;,
which reveal the hidden high-risk regions within the secure area obtained
through adversarial training and prevent the model from finding the real worst
cases. We demand the model to prevent hiders when defending against adversarial
examples for improving accuracy and robustness simultaneously. By rethinking
and redefining the min-max optimization problem for adversarial training, we
propose a generalized adversarial training algorithm called Hider-Focused
Adversarial Training (HFAT). HFAT introduces the iterative evolution
optimization strategy to simplify the optimization problem and employs an
auxiliary model to reveal hiders, effectively combining the optimization
directions of standard adversarial training and prevention hiders. Furthermore,
we introduce an adaptive weighting mechanism that facilitates the model in
adaptively adjusting its focus between adversarial examples and hiders during
different training periods. We demonstrate the effectiveness of our method
based on extensive experiments, and ensure that HFAT can provide higher
robustness and accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yuxiao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yinpeng Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dongxiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuntian Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07069">
<title>Context Matter: Data-Efficient Augmentation of Large Language Models for Scientific Applications. (arXiv:2312.07069v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07069</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we explore the challenges inherent to Large Language Models
(LLMs) like GPT-4, particularly their propensity for hallucinations, logic
mistakes, and incorrect conclusions when tasked with answering complex
questions. The capacity of LLMs to present erroneous answers in a coherent and
semantically rigorous manner further complicates the detection of factual
inaccuracies. This issue is especially pronounced in fields that require
specialized expertise. Our work delves into these challenges, aiming to enhance
the understanding and mitigation of such errors, thereby contributing to the
improvement of LLM accuracy and reliability in scientific and other specialized
domains. Our findings reveal a non-linear relationship between the context&apos;s
relevancy and the answers&apos; measured quality. In addition, we demonstrate that
with the correct calibration, it is possible to automate the grading procedure
-- a finding suggesting that, at least to some degree, the LLMs can be used to
self-examine the quality of their own performance. Finally, we describe an
experimental platform that can be seen as a proof-of-concept of the techniques
described in this work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Haoran Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Siyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maravi_A/0/1/0/all/0/1&quot;&gt;Anurag Maravi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abram_M/0/1/0/all/0/1&quot;&gt;Marcin Abram&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07087">
<title>Toward Robustness in Multi-label Classification: A Data Augmentation Strategy against Imbalance and Noise. (arXiv:2312.07087v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07087</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-label classification poses challenges due to imbalanced and noisy
labels in training data. We propose a unified data augmentation method, named
BalanceMix, to address these challenges. Our approach includes two samplers for
imbalanced labels, generating minority-augmented instances with high diversity.
It also refines multi-labels at the label-wise granularity, categorizing noisy
labels as clean, re-labeled, or ambiguous for robust optimization. Extensive
experiments on three benchmark datasets demonstrate that BalanceMix outperforms
existing state-of-the-art methods. We release the code at
https://github.com/DISL-Lab/BalanceMix.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1&quot;&gt;Hwanjun Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Minseok Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jae-Gil Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07103">
<title>The Computational Complexity of Concise Hypersphere Classification. (arXiv:2312.07103v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07103</link>
<description rdf:parseType="Literal">&lt;p&gt;Hypersphere classification is a classical and foundational method that can
provide easy-to-process explanations for the classification of real-valued and
binary data. However, obtaining an (ideally concise) explanation via
hypersphere classification is much more difficult when dealing with binary data
than real-valued data. In this paper, we perform the first complexity-theoretic
study of the hypersphere classification problem for binary data. We use the
fine-grained parameterized complexity paradigm to analyze the impact of
structural properties that may be present in the input data as well as
potential conciseness constraints. Our results include stronger lower bounds
and new fixed-parameter algorithms for hypersphere classification of binary
data, which can find an exact and concise explanation when one exists.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eiben_E/0/1/0/all/0/1&quot;&gt;Eduard Eiben&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganian_R/0/1/0/all/0/1&quot;&gt;Robert Ganian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanj_I/0/1/0/all/0/1&quot;&gt;Iyad Kanj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ordyniak_S/0/1/0/all/0/1&quot;&gt;Sebastian Ordyniak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szeider_S/0/1/0/all/0/1&quot;&gt;Stefan Szeider&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07110">
<title>LLMs Perform Poorly at Concept Extraction in Cyber-security Research Literature. (arXiv:2312.07110v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07110</link>
<description rdf:parseType="Literal">&lt;p&gt;The cybersecurity landscape evolves rapidly and poses threats to
organizations. To enhance resilience, one needs to track the latest
developments and trends in the domain. It has been demonstrated that standard
bibliometrics approaches show their limits in such a fast-evolving domain. For
this purpose, we use large language models (LLMs) to extract relevant knowledge
entities from cybersecurity-related texts. We use a subset of arXiv preprints
on cybersecurity as our data and compare different LLMs in terms of entity
recognition (ER) and relevance. The results suggest that LLMs do not produce
good knowledge entities that reflect the cybersecurity context, but our results
show some potential for noun extractors. For this reason, we developed a noun
extractor boosted with some statistical analysis to extract specific and
relevant compound nouns from the domain. Later, we tested our model to identify
trends in the LLM domain. We observe some limitations, but it offers promising
results to monitor the evolution of emergent trends.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wursch_M/0/1/0/all/0/1&quot;&gt;Maxime W&amp;#xfc;rsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kucharavy_A/0/1/0/all/0/1&quot;&gt;Andrei Kucharavy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+David_D/0/1/0/all/0/1&quot;&gt;Dimitri Percia David&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mermoud_A/0/1/0/all/0/1&quot;&gt;Alain Mermoud&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07112">
<title>Generating High-Resolution Regional Precipitation Using Conditional Diffusion Model. (arXiv:2312.07112v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07112</link>
<description rdf:parseType="Literal">&lt;p&gt;Climate downscaling is a crucial technique within climate research, serving
to project low-resolution (LR) climate data to higher resolutions (HR).
Previous research has demonstrated the effectiveness of deep learning for
downscaling tasks. However, most deep learning models for climate downscaling
may not perform optimally for high scaling factors (i.e., 4x, 8x) due to their
limited ability to capture the intricate details required for generating HR
climate data. Furthermore, climate data behaves differently from image data,
necessitating a nuanced approach when employing deep generative models. In
response to these challenges, this paper presents a deep generative model for
downscaling climate data, specifically precipitation on a regional scale. We
employ a denoising diffusion probabilistic model (DDPM) conditioned on multiple
LR climate variables. The proposed model is evaluated using precipitation data
from the Community Earth System Model (CESM) v1.2.2 simulation. Our results
demonstrate significant improvements over existing baselines, underscoring the
effectiveness of the conditional diffusion model in downscaling climate data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shidqi_N/0/1/0/all/0/1&quot;&gt;Naufal Shidqi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_C/0/1/0/all/0/1&quot;&gt;Chaeyoon Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Sungwon Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeller_E/0/1/0/all/0/1&quot;&gt;Elke Zeller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nellikkattil_A/0/1/0/all/0/1&quot;&gt;Arjun Babu Nellikkattil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1&quot;&gt;Karandeep Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07133">
<title>Text2AC-Zero: Consistent Synthesis of Animated Characters using 2D Diffusion. (arXiv:2312.07133v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07133</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a zero-shot approach for consistent Text-to-Animated-Characters
synthesis based on pre-trained Text-to-Image (T2I) diffusion models. Existing
Text-to-Video (T2V) methods are expensive to train and require large-scale
video datasets to produce diverse characters and motions. At the same time,
their zero-shot alternatives fail to produce temporally consistent videos. We
strive to bridge this gap, and we introduce a zero-shot approach that produces
temporally consistent videos of animated characters and requires no training or
fine-tuning. We leverage existing text-based motion diffusion models to
generate diverse motions that we utilize to guide a T2I model. To achieve
temporal consistency, we introduce the Spatial Latent Alignment module that
exploits cross-frame dense correspondences that we compute to align the latents
of the video frames. Furthermore, we propose Pixel-Wise Guidance to steer the
diffusion process in a direction that minimizes visual discrepancies. Our
proposed approach generates temporally consistent videos with diverse motions
and styles, outperforming existing zero-shot T2V approaches in terms of
pixel-wise consistency and user preference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eldesokey_A/0/1/0/all/0/1&quot;&gt;Abdelrahman Eldesokey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wonka_P/0/1/0/all/0/1&quot;&gt;Peter Wonka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07142">
<title>General Tail Bounds for Non-Smooth Stochastic Mirror Descent. (arXiv:2312.07142v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07142</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we provide novel tail bounds on the optimization error of
Stochastic Mirror Descent for convex and Lipschitz objectives. Our analysis
extends the existing tail bounds from the classical light-tailed Sub-Gaussian
noise case to heavier-tailed noise regimes. We study the optimization error of
the last iterate as well as the average of the iterates. We instantiate our
results in two important cases: a class of noise with exponential tails and one
with polynomial tails. A remarkable feature of our results is that they do not
require an upper bound on the diameter of the domain. Finally, we support our
theory with illustrative experiments that compare the behavior of the average
of the iterates with that of the last iterate in heavy-tailed noise regimes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eldowa_K/0/1/0/all/0/1&quot;&gt;Khaled Eldowa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paudice_A/0/1/0/all/0/1&quot;&gt;Andrea Paudice&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07145">
<title>Contextual Bandits with Online Neural Regression. (arXiv:2312.07145v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07145</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent works have shown a reduction from contextual bandits to online
regression under a realizability assumption [Foster and Rakhlin, 2020, Foster
and Krishnamurthy, 2021]. In this work, we investigate the use of neural
networks for such online regression and associated Neural Contextual Bandits
(NeuCBs). Using existing results for wide networks, one can readily show a
${\mathcal{O}}(\sqrt{T})$ regret for online regression with square loss, which
via the reduction implies a ${\mathcal{O}}(\sqrt{K} T^{3/4})$ regret for
NeuCBs. Departing from this standard approach, we first show a
$\mathcal{O}(\log T)$ regret for online regression with almost convex losses
that satisfy QG (Quadratic Growth) condition, a generalization of the PL
(Polyak-\L ojasiewicz) condition, and that have a unique minima. Although not
directly applicable to wide networks since they do not have unique minima, we
show that adding a suitable small random perturbation to the network
predictions surprisingly makes the loss satisfy QG with unique minima. Based on
such a perturbed prediction, we show a ${\mathcal{O}}(\log T)$ regret for
online regression with both squared loss and KL loss, and subsequently convert
these respectively to $\tilde{\mathcal{O}}(\sqrt{KT})$ and
$\tilde{\mathcal{O}}(\sqrt{KL^*} + K)$ regret for NeuCB, where $L^*$ is the
loss of the best policy. Separately, we also show that existing regret bounds
for NeuCBs are $\Omega(T)$ or assume i.i.d. contexts, unlike this work.
Finally, our experimental results on various datasets demonstrate that our
algorithms, especially the one based on KL loss, persistently outperform
existing algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deb_R/0/1/0/all/0/1&quot;&gt;Rohan Deb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ban_Y/0/1/0/all/0/1&quot;&gt;Yikun Ban&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_S/0/1/0/all/0/1&quot;&gt;Shiliang Zuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jingrui He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1&quot;&gt;Arindam Banerjee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07161">
<title>One-dimensional Convolutional Neural Networks for Detecting Transiting Exoplanets. (arXiv:2312.07161v1 [astro-ph.EP])</title>
<link>http://arxiv.org/abs/2312.07161</link>
<description rdf:parseType="Literal">&lt;p&gt;The transit method is one of the most relevant exoplanet detection
techniques, which consists of detecting periodic eclipses in the light curves
of stars. This is not always easy due to the presence of noise in the light
curves, which is induced, for example, by the response of a telescope to
stellar flux. For this reason, we aimed to develop an artificial neural network
model that is able to detect these transits in light curves obtained from
different telescopes and surveys. We created artificial light curves with and
without transits to try to mimic those expected for the extended mission of the
Kepler telescope (K2) in order to train and validate a 1D convolutional neural
network model, which was later tested, obtaining an accuracy of 99.02 % and an
estimated error (loss function) of 0.03. These results, among others, helped to
confirm that the 1D CNN is a good choice for working with non-phased-folded
Mandel and Agol light curves with transits. It also reduces the number of light
curves that have to be visually inspected to decide if they present
transit-like signals and decreases the time needed for analyzing each (with
respect to traditional analysis).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Alvarez_S/0/1/0/all/0/1&quot;&gt;Santiago Iglesias &amp;#xc1;lvarez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Alonso_E/0/1/0/all/0/1&quot;&gt;Enrique D&amp;#xed;ez Alonso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Sanchez_M/0/1/0/all/0/1&quot;&gt;Mar&amp;#xed;a Luisa S&amp;#xe1;nchez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Rodriguez_J/0/1/0/all/0/1&quot;&gt;Javier Rodr&amp;#xed;guez Rodr&amp;#xed;guez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Lasheras_F/0/1/0/all/0/1&quot;&gt;Fernando S&amp;#xe1;nchez Lasheras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Juez_F/0/1/0/all/0/1&quot;&gt;Francisco Javier de Cos Juez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07165">
<title>Language-Guided Transformer for Federated Multi-Label Classification. (arXiv:2312.07165v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07165</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) is an emerging paradigm that enables multiple users
to collaboratively train a robust model in a privacy-preserving manner without
sharing their private data. Most existing approaches of FL only consider
traditional single-label image classification, ignoring the impact when
transferring the task to multi-label image classification. Nevertheless, it is
still challenging for FL to deal with user heterogeneity in their local data
distribution in the real-world FL scenario, and this issue becomes even more
severe in multi-label image classification. Inspired by the recent success of
Transformers in centralized settings, we propose a novel FL framework for
multi-label classification. Since partial label correlation may be observed by
local clients during training, direct aggregation of locally updated models
would not produce satisfactory performances. Thus, we propose a novel FL
framework of Language-Guided Transformer (FedLGT) to tackle this challenging
task, which aims to exploit and transfer knowledge across different clients for
learning a robust global model. Through extensive experiments on various
multi-label datasets (e.g., FLAIR, MS-COCO, etc.), we show that our FedLGT is
able to achieve satisfactory performance and outperforms standard FL techniques
under multi-label FL scenarios. Code is available at
https://github.com/Jack24658735/FedLGT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_I/0/1/0/all/0/1&quot;&gt;I-Jieh Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Ci-Siang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Fu-En Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Chiang Frank Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07168">
<title>Equivariant Flow Matching with Hybrid Probability Transport. (arXiv:2312.07168v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07168</link>
<description rdf:parseType="Literal">&lt;p&gt;The generation of 3D molecules requires simultaneously deciding the
categorical features~(atom types) and continuous features~(atom coordinates).
Deep generative models, especially Diffusion Models (DMs), have demonstrated
effectiveness in generating feature-rich geometries. However, existing DMs
typically suffer from unstable probability dynamics with inefficient sampling
speed. In this paper, we introduce geometric flow matching, which enjoys the
advantages of both equivariant modeling and stabilized probability dynamics.
More specifically, we propose a hybrid probability path where the coordinates
probability path is regularized by an equivariant optimal transport, and the
information between different modalities is aligned. Experimentally, the
proposed method could consistently achieve better performance on multiple
molecule generation benchmarks with 4.75$\times$ speed up of sampling on
average.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_J/0/1/0/all/0/1&quot;&gt;Jingjing Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Minkai Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1&quot;&gt;Ziyao Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1&quot;&gt;Yanyan Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1&quot;&gt;Stefano Ermon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1&quot;&gt;Wei-Ying Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07174">
<title>Investigation into the Training Dynamics of Learned Optimizers. (arXiv:2312.07174v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07174</link>
<description rdf:parseType="Literal">&lt;p&gt;Optimization is an integral part of modern deep learning. Recently, the
concept of learned optimizers has emerged as a way to accelerate this
optimization process by replacing traditional, hand-crafted algorithms with
meta-learned functions. Despite the initial promising results of these methods,
issues with stability and generalization still remain, limiting their practical
use. Moreover, their inner workings and behavior under different conditions are
not yet fully understood, making it difficult to come up with improvements. For
this reason, our work examines their optimization trajectories from the
perspective of network architecture symmetries and parameter update
distributions. Furthermore, by contrasting the learned optimizers with their
manually designed counterparts, we identify several key insights that
demonstrate how each approach can benefit from the strengths of the other.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sobotka_J/0/1/0/all/0/1&quot;&gt;Jan Sobotka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simanek_P/0/1/0/all/0/1&quot;&gt;Petr &amp;#x160;im&amp;#xe1;nek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasata_D/0/1/0/all/0/1&quot;&gt;Daniel Va&amp;#x161;ata&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07175">
<title>Instrumental Variable Estimation for Causal Inference in Longitudinal Data with Time-Dependent Latent Confounders. (arXiv:2312.07175v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07175</link>
<description rdf:parseType="Literal">&lt;p&gt;Causal inference from longitudinal observational data is a challenging
problem due to the difficulty in correctly identifying the time-dependent
confounders, especially in the presence of latent time-dependent confounders.
Instrumental variable (IV) is a powerful tool for addressing the latent
confounders issue, but the traditional IV technique cannot deal with latent
time-dependent confounders in longitudinal studies. In this work, we propose a
novel Time-dependent Instrumental Factor Model (TIFM) for time-varying causal
effect estimation from data with latent time-dependent confounders. At each
time-step, the proposed TIFM method employs the Recurrent Neural Network (RNN)
architecture to infer latent IV, and then uses the inferred latent IV factor
for addressing the confounding bias caused by the latent time-dependent
confounders. We provide a theoretical analysis for the proposed TIFM method
regarding causal effect estimation in longitudinal data. Extensive evaluation
with synthetic datasets demonstrates the effectiveness of TIFM in addressing
causal effect estimation over time. We further apply TIFM to a climate dataset
to showcase the potential of the proposed method in tackling real-world
problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1&quot;&gt;Debo Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Ziqi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiuyong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jixue Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1&quot;&gt;Wentao Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1&quot;&gt;Thuc Duy Le&lt;/a&gt; (UniSA STEM, University of South Australia, Adelaide, SA, Australia)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07178">
<title>Beyond Expected Return: Accounting for Policy Reproducibility when Evaluating Reinforcement Learning Algorithms. (arXiv:2312.07178v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07178</link>
<description rdf:parseType="Literal">&lt;p&gt;Many applications in Reinforcement Learning (RL) usually have noise or
stochasticity present in the environment. Beyond their impact on learning,
these uncertainties lead the exact same policy to perform differently, i.e.
yield different return, from one roll-out to another. Common evaluation
procedures in RL summarise the consequent return distributions using solely the
expected return, which does not account for the spread of the distribution. Our
work defines this spread as the policy reproducibility: the ability of a policy
to obtain similar performance when rolled out many times, a crucial property in
some real-world applications. We highlight that existing procedures that only
use the expected return are limited on two fronts: first an infinite number of
return distributions with a wide range of performance-reproducibility
trade-offs can have the same expected return, limiting its effectiveness when
used for comparing policies; second, the expected return metric does not leave
any room for practitioners to choose the best trade-off value for considered
applications. In this work, we address these limitations by recommending the
use of Lower Confidence Bound, a metric taken from Bayesian optimisation that
provides the user with a preference parameter to choose a desired
performance-reproducibility trade-off. We also formalise and quantify policy
reproducibility, and demonstrate the benefit of our metrics using extensive
experiments of popular RL algorithms on common uncertain RL tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flageat_M/0/1/0/all/0/1&quot;&gt;Manon Flageat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_B/0/1/0/all/0/1&quot;&gt;Bryan Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cully_A/0/1/0/all/0/1&quot;&gt;Antoine Cully&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07182">
<title>Classifying complex documents: comparing bespoke solutions to large language models. (arXiv:2312.07182v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07182</link>
<description rdf:parseType="Literal">&lt;p&gt;Here we search for the best automated classification approach for a set of
complex legal documents. Our classification task is not trivial: our aim is to
classify ca 30,000 public courthouse records from 12 states and 267 counties at
two different levels using nine sub-categories. Specifically, we investigated
whether a fine-tuned large language model (LLM) can achieve the accuracy of a
bespoke custom-trained model, and what is the amount of fine-tuning necessary.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hopkins_G/0/1/0/all/0/1&quot;&gt;Glen Hopkins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalm_K/0/1/0/all/0/1&quot;&gt;Kristjan Kalm&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07186">
<title>Towards Optimal Sobolev Norm Rates for the Vector-Valued Regularized Least-Squares Algorithm. (arXiv:2312.07186v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2312.07186</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the first optimal rates for infinite-dimensional vector-valued
ridge regression on a continuous scale of norms that interpolate between $L_2$
and the hypothesis space, which we consider as a vector-valued reproducing
kernel Hilbert space. These rates allow to treat the misspecified case in which
the true regression function is not contained in the hypothesis space. We
combine standard assumptions on the capacity of the hypothesis space with a
novel tensor product construction of vector-valued interpolation spaces in
order to characterize the smoothness of the regression function. Our upper
bound not only attains the same rate as real-valued kernel ridge regression,
but also removes the assumption that the target regression function is bounded.
For the lower bound, we reduce the problem to the scalar setting using a
projection argument. We show that these rates are optimal in most cases and
independent of the dimension of the output space. We illustrate our results for
the special case of vector-valued Sobolev spaces.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Meunier_D/0/1/0/all/0/1&quot;&gt;Dimitri Meunier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mollenhauer_M/0/1/0/all/0/1&quot;&gt;Mattes Mollenhauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gretton_A/0/1/0/all/0/1&quot;&gt;Arthur Gretton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07206">
<title>A churn prediction dataset from the telecom sector: a new benchmark for uplift modeling. (arXiv:2312.07206v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07206</link>
<description rdf:parseType="Literal">&lt;p&gt;Uplift modeling, also known as individual treatment effect (ITE) estimation,
is an important approach for data-driven decision making that aims to identify
the causal impact of an intervention on individuals. This paper introduces a
new benchmark dataset for uplift modeling focused on churn prediction, coming
from a telecom company in Belgium, Orange Belgium. Churn, in this context,
refers to customers terminating their subscription to the telecom service. This
is the first publicly available dataset offering the possibility to evaluate
the efficiency of uplift modeling on the churn prediction problem. Moreover,
its unique characteristics make it more challenging than the few other public
uplift datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verhelst_T/0/1/0/all/0/1&quot;&gt;Th&amp;#xe9;o Verhelst&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mercier_D/0/1/0/all/0/1&quot;&gt;Denis Mercier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shrestha_J/0/1/0/all/0/1&quot;&gt;Jeevan Shrestha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bontempi_G/0/1/0/all/0/1&quot;&gt;Gianluca Bontempi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07208">
<title>Experimental Investigation of Machine Learning based Soft-Failure Management using the Optical Spectrum. (arXiv:2312.07208v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2312.07208</link>
<description rdf:parseType="Literal">&lt;p&gt;The demand for high-speed data is exponentially growing. To conquer this,
optical networks underwent significant changes getting more complex and
versatile. The increasing complexity necessitates the fault management to be
more adaptive to enhance network assurance. In this paper, we experimentally
compare the performance of soft-failure management of different machine
learning algorithms. We further introduce a machine-learning based soft-failure
management framework. It utilizes a variational autoencoder based generative
adversarial network (VAE-GAN) running on optical spectral data obtained by
optical spectrum analyzers. The framework is able to reliably run on a fraction
of available training data as well as identifying unknown failure types. The
investigations show, that the VAE-GAN outperforms the other machine learning
algorithms when up to 10\% of the total training data is available in
identification tasks. Furthermore, the advanced training mechanism for the GAN
shows a high F1-score for unknown spectrum identification. The failure
localization comparison shows the advantage of a low complexity neural network
in combination with a VAE over established machine learning algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kruse_L/0/1/0/all/0/1&quot;&gt;Lars E. Kruse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuhl_S/0/1/0/all/0/1&quot;&gt;Sebastian K&amp;#xfc;hl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dochhan_A/0/1/0/all/0/1&quot;&gt;Annika Dochhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pachnicke_S/0/1/0/all/0/1&quot;&gt;Stephan Pachnicke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07231">
<title>Fast Training of Diffusion Transformer with Extreme Masking for 3D Point Clouds Generation. (arXiv:2312.07231v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07231</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion Transformers have recently shown remarkable effectiveness in
generating high-quality 3D point clouds. However, training voxel-based
diffusion models for high-resolution 3D voxels remains prohibitively expensive
due to the cubic complexity of attention operators, which arises from the
additional dimension of voxels. Motivated by the inherent redundancy of 3D
compared to 2D, we propose FastDiT-3D, a novel masked diffusion transformer
tailored for efficient 3D point cloud generation, which greatly reduces
training costs. Specifically, we draw inspiration from masked autoencoders to
dynamically operate the denoising process on masked voxelized point clouds. We
also propose a novel voxel-aware masking strategy to adaptively aggregate
background/foreground information from voxelized point clouds. Our method
achieves state-of-the-art performance with an extreme masking ratio of nearly
99%. Moreover, to improve multi-category 3D generation, we introduce
Mixture-of-Expert (MoE) in 3D diffusion model. Each category can learn a
distinct diffusion path with different experts, relieving gradient conflict.
Experimental results on the ShapeNet dataset demonstrate that our method
achieves state-of-the-art high-fidelity and diverse 3D point cloud generation
performance. Our FastDiT-3D improves 1-Nearest Neighbor Accuracy and Coverage
metrics when generating 128-resolution voxel point clouds, using only 6.5% of
the original training cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1&quot;&gt;Shentong Mo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1&quot;&gt;Enze Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yue Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junsong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1&quot;&gt;Matthias Nie&amp;#xdf;ner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenguo Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07248">
<title>Multi-Granularity Framework for Unsupervised Representation Learning of Time Series. (arXiv:2312.07248v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07248</link>
<description rdf:parseType="Literal">&lt;p&gt;Representation learning plays a critical role in the analysis of time series
data and has high practical value across a wide range of applications.
including trend analysis, time series data retrieval and forecasting. In
practice, data confusion is a significant issue as it can considerably impact
the effectiveness and accuracy of data analysis, machine learning models and
decision-making processes. In general, previous studies did not consider the
variability at various levels of granularity, thus resulting in inadequate
information utilization, which further exacerbated the issue of data confusion.
This paper proposes an unsupervised framework to realize multi-granularity
representation learning for time series. Specifically, we employed a
cross-granularity transformer to develop an association between fine- and
coarse-grained representations. In addition, we introduced a retrieval task as
an unsupervised training task to learn the multi-granularity representation of
time series. Moreover, a novel loss function was designed to obtain the
comprehensive multi-granularity representation of the time series via
unsupervised learning. The experimental results revealed that the proposed
framework demonstrates significant advantages over alternative representation
learning models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1&quot;&gt;Chengyang Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1&quot;&gt;Qiang Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07252">
<title>Identifying Drivers of Predictive Uncertainty using Variance Feature Attribution. (arXiv:2312.07252v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07252</link>
<description rdf:parseType="Literal">&lt;p&gt;Explainability and uncertainty quantification are two pillars of trustable
artificial intelligence. However, the reasoning behind uncertainty estimates is
generally left unexplained. Identifying the drivers of uncertainty complements
explanations of point predictions in recognizing potential model limitations.
It facilitates the detection of oversimplification in the uncertainty
estimation process. Explanations of uncertainty enhance communication and trust
in decisions. They allow for verifying whether the main drivers of model
uncertainty are relevant and may impact model usage. So far, the subject of
explaining uncertainties has been rarely studied. The few exceptions in
existing literature are tailored to Bayesian neural networks or rely heavily on
technically intricate approaches, hindering their broad adoption. We propose
variance feature attribution, a simple and scalable solution to explain
predictive aleatoric uncertainties. First, we estimate uncertainty as
predictive variance by equipping a neural network with a Gaussian output
distribution by adding a variance output neuron. Thereby, we can rely on
pre-trained point prediction models and fine-tune them for meaningful variance
estimation. Second, we apply out-of-the-box explainers on the variance output
of these models to explain the uncertainty estimation. We evaluate our approach
in a synthetic setting where the data-generating process is known. We show that
our method can explain uncertainty influences more reliably and faster than the
established baseline CLUE. We fine-tune a state-of-the-art age regression model
to estimate uncertainty and obtain attributions. Our explanations highlight
potential sources of uncertainty, such as laugh lines. Variance feature
attribution provides accurate explanations for uncertainty estimates with
little modifications to the model architecture and low computational overhead.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iversen_P/0/1/0/all/0/1&quot;&gt;Pascal Iversen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Witzke_S/0/1/0/all/0/1&quot;&gt;Simon Witzke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baum_K/0/1/0/all/0/1&quot;&gt;Katharina Baum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Renard_B/0/1/0/all/0/1&quot;&gt;Bernhard Y. Renard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07271">
<title>Analyze the Robustness of Classifiers under Label Noise. (arXiv:2312.07271v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07271</link>
<description rdf:parseType="Literal">&lt;p&gt;This study explores the robustness of label noise classifiers, aiming to
enhance model resilience against noisy data in complex real-world scenarios.
Label noise in supervised learning, characterized by erroneous or imprecise
labels, significantly impairs model performance. This research focuses on the
increasingly pertinent issue of label noise&apos;s impact on practical applications.
Addressing the prevalent challenge of inaccurate training data labels, we
integrate adversarial machine learning (AML) and importance reweighting
techniques. Our approach involves employing convolutional neural networks (CNN)
as the foundational model, with an emphasis on parameter adjustment for
individual training samples. This strategy is designed to heighten the model&apos;s
focus on samples critically influencing performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1&quot;&gt;Cheng Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yixuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1&quot;&gt;Jiaqi Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07281">
<title>Safe Multi-Task Bayesian Optimization. (arXiv:2312.07281v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07281</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian optimization has become a powerful tool for safe online optimization
of systems, due to its high sample efficiency and noise robustness. For further
speed-up reduced physical models of the system can be incorporated into the
optimization to accelerate the process, since the models are able to offer an
approximation of the actual system, and sampling from them is significantly
cheaper. The similarity between model and reality is represented by additional
hyperparameters and learned within the optimization process. Safety is an
important criteria for online optimization methods like Bayesian optimization,
which has been addressed by recent literature, which provide safety guarantees
under the assumption of known hyperparameters. However, in practice this is not
applicable. Therefore, we extend the robust Gaussian process uniform error
bounds to meet the multi-task setting, which involves the calculation of a
confidence region from the hyperparameter posterior distribution utilizing
Markov chain Monte Carlo methods. Then, using the robust safety bounds,
Bayesian optimization is applied to safely optimize the system while
incorporating measurements of the models. Simulations show that the
optimization can be significantly accelerated compared to other
state-of-the-art safe Bayesian optimization methods depending on the fidelity
of the models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lubsen_J/0/1/0/all/0/1&quot;&gt;Jannis O. L&amp;#xfc;bsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hespe_C/0/1/0/all/0/1&quot;&gt;Christian Hespe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eichler_A/0/1/0/all/0/1&quot;&gt;Annika Eichler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07282">
<title>Class Probability Matching Using Kernel Methods for Label Shift Adaptation. (arXiv:2312.07282v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2312.07282</link>
<description rdf:parseType="Literal">&lt;p&gt;In domain adaptation, covariate shift and label shift problems are two
distinct and complementary tasks. In covariate shift adaptation where the
differences in data distribution arise from variations in feature
probabilities, existing approaches naturally address this problem based on
\textit{feature probability matching} (\textit{FPM}). However, for label shift
adaptation where the differences in data distribution stem solely from
variations in class probability, current methods still use FPM on the
$d$-dimensional feature space to estimate the class probability ratio on the
one-dimensional label space. To address label shift adaptation more naturally
and effectively, inspired by a new representation of the source domain&apos;s class
probability, we propose a new framework called \textit{class probability
matching} (\textit{CPM}) which matches two class probability functions on the
one-dimensional label space to estimate the class probability ratio,
fundamentally different from FPM operating on the $d$-dimensional feature
space. Furthermore, by incorporating the kernel logistic regression into the
CPM framework to estimate the conditional probability, we propose an algorithm
called \textit{class probability matching using kernel methods}
(\textit{CPMKM}) for label shift adaptation. From the theoretical perspective,
we establish the optimal convergence rates of CPMKM with respect to the
cross-entropy loss for multi-class label shift adaptation. From the
experimental perspective, comparisons on real datasets demonstrate that CPMKM
outperforms existing FPM-based and maximum-likelihood-based algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wen_H/0/1/0/all/0/1&quot;&gt;Hongwei Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Betken_A/0/1/0/all/0/1&quot;&gt;Annika Betken&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hang_H/0/1/0/all/0/1&quot;&gt;Hanyuan Hang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07285">
<title>Forced Exploration in Bandit Problems. (arXiv:2312.07285v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07285</link>
<description rdf:parseType="Literal">&lt;p&gt;The multi-armed bandit(MAB) is a classical sequential decision problem. Most
work requires assumptions about the reward distribution (e.g., bounded), while
practitioners may have difficulty obtaining information about these
distributions to design models for their problems, especially in non-stationary
MAB problems. This paper aims to design a multi-armed bandit algorithm that can
be implemented without using information about the reward distribution while
still achieving substantial regret upper bounds. To this end, we propose a
novel algorithm alternating between greedy rule and forced exploration. Our
method can be applied to Gaussian, Bernoulli and other subgaussian
distributions, and its implementation does not require additional information.
We employ a unified analysis method for different forced exploration strategies
and provide problem-dependent regret upper bounds for stationary and
piecewise-stationary settings. Furthermore, we compare our algorithm with
popular bandit algorithms on different reward distributions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1&quot;&gt;Han Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1&quot;&gt;Fei Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Li Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07296">
<title>Complex Recurrent Spectral Network. (arXiv:2312.07296v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07296</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel approach to advancing artificial intelligence
(AI) through the development of the Complex Recurrent Spectral Network
($\mathbb{C}$-RSN), an innovative variant of the Recurrent Spectral Network
(RSN) model. The $\mathbb{C}$-RSN is designed to address a critical limitation
in existing neural network models: their inability to emulate the complex
processes of biological neural networks dynamically and accurately. By
integrating key concepts from dynamical systems theory and leveraging
principles from statistical mechanics, the $\mathbb{C}$-RSN model introduces
localized non-linearity, complex fixed eigenvalues, and a distinct separation
of memory and input processing functionalities. These features collectively
enable the $\mathbb{C}$-RSN evolving towards a dynamic, oscillating final state
that more closely mirrors biological cognition. Central to this work is the
exploration of how the $\mathbb{C}$-RSN manages to capture the rhythmic,
oscillatory dynamics intrinsic to biological systems, thanks to its complex
eigenvalue structure and the innovative segregation of its linear and
non-linear components. The model&apos;s ability to classify data through a
time-dependent function, and the localization of information processing, is
demonstrated with an empirical evaluation using the MNIST dataset. Remarkably,
distinct items supplied as a sequential input yield patterns in time which bear
the indirect imprint of the insertion order (and of the time of separation
between contiguous insertions).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chicchi_L/0/1/0/all/0/1&quot;&gt;Lorenzo Chicchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giambagli_L/0/1/0/all/0/1&quot;&gt;Lorenzo Giambagli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buffoni_L/0/1/0/all/0/1&quot;&gt;Lorenzo Buffoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marino_R/0/1/0/all/0/1&quot;&gt;Raffaele Marino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fanelli_D/0/1/0/all/0/1&quot;&gt;Duccio Fanelli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07311">
<title>Scalable Motion Style Transfer with Constrained Diffusion Generation. (arXiv:2312.07311v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07311</link>
<description rdf:parseType="Literal">&lt;p&gt;Current training of motion style transfer systems relies on consistency
losses across style domains to preserve contents, hindering its scalable
application to a large number of domains and private data. Recent image
transfer works show the potential of independent training on each domain by
leveraging implicit bridging between diffusion models, with the content
preservation, however, limited to simple data patterns. We address this by
imposing biased sampling in backward diffusion while maintaining the domain
independence in the training stage. We construct the bias from the source
domain keyframes and apply them as the gradient of content constraints,
yielding a framework with keyframe manifold constraint gradients (KMCGs). Our
validation demonstrates the success of training separate models to transfer
between as many as ten dance motion styles. Comprehensive experiments find a
significant improvement in preserving motion contents in comparison to baseline
and ablative diffusion-based style transfer models. In addition, we perform a
human study for a subjective assessment of the quality of generated dance
motions. The results validate the competitiveness of KMCGs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1&quot;&gt;Wenjie Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1&quot;&gt;Hang Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kragic_D/0/1/0/all/0/1&quot;&gt;Danica Kragic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bjorkman_M/0/1/0/all/0/1&quot;&gt;M&amp;#xe5;rten Bj&amp;#xf6;rkman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07316">
<title>GateNet: A novel Neural Network Architecture for Automated Flow Cytometry Gating. (arXiv:2312.07316v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07316</link>
<description rdf:parseType="Literal">&lt;p&gt;Flow cytometry is widely used to identify cell populations in patient-derived
fluids such as peripheral blood (PB) or cerebrospinal fluid (CSF). While
ubiquitous in research and clinical practice, flow cytometry requires gating,
i.e. cell type identification which requires labor-intensive and error-prone
manual adjustments. To facilitate this process, we designed GateNet, the first
neural network architecture enabling full end-to-end automated gating without
the need to correct for batch effects. We train GateNet with over 8,000,000
events based on N=127 PB and CSF samples which were manually labeled
independently by four experts. We show that for novel, unseen samples, GateNet
achieves human-level performance (F1 score ranging from 0.910 to 0.997). In
addition we apply GateNet to a publicly available dataset confirming
generalization with an F1 score of 0.936. As our implementation utilizes
graphics processing units (GPU), gating only needs 15 microseconds per event.
Importantly, we also show that GateNet only requires ~10 samples to reach
human-level performance, rendering it widely applicable in all domains of flow
cytometry.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fisch_L/0/1/0/all/0/1&quot;&gt;Lukas Fisch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heming_M/0/1/0/all/0/1&quot;&gt;Michael O. Heming&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulte_Mecklenbeck_A/0/1/0/all/0/1&quot;&gt;Andreas Schulte-Mecklenbeck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gross_C/0/1/0/all/0/1&quot;&gt;Catharina C. Gross&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zumdick_S/0/1/0/all/0/1&quot;&gt;Stefan Zumdick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barkhau_C/0/1/0/all/0/1&quot;&gt;Carlotta Barkhau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emden_D/0/1/0/all/0/1&quot;&gt;Daniel Emden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ernsting_J/0/1/0/all/0/1&quot;&gt;Jan Ernsting&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leenings_R/0/1/0/all/0/1&quot;&gt;Ramona Leenings&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarink_K/0/1/0/all/0/1&quot;&gt;Kelvin Sarink&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Winter_N/0/1/0/all/0/1&quot;&gt;Nils R. Winter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dannlowski_U/0/1/0/all/0/1&quot;&gt;Udo Dannlowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiendl_H/0/1/0/all/0/1&quot;&gt;Heinz Wiendl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horste_G/0/1/0/all/0/1&quot;&gt;Gerd Meyer zu H&amp;#xf6;rste&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hahn_T/0/1/0/all/0/1&quot;&gt;Tim Hahn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07331">
<title>Coupled Confusion Correction: Learning from Crowds with Sparse Annotations. (arXiv:2312.07331v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07331</link>
<description rdf:parseType="Literal">&lt;p&gt;As the size of the datasets getting larger, accurately annotating such
datasets is becoming more impractical due to the expensiveness on both time and
economy. Therefore, crowd-sourcing has been widely adopted to alleviate the
cost of collecting labels, which also inevitably introduces label noise and
eventually degrades the performance of the model. To learn from crowd-sourcing
annotations, modeling the expertise of each annotator is a common but
challenging paradigm, because the annotations collected by crowd-sourcing are
usually highly-sparse. To alleviate this problem, we propose Coupled Confusion
Correction (CCC), where two models are simultaneously trained to correct the
confusion matrices learned by each other. Via bi-level optimization, the
confusion matrices learned by one model can be corrected by the distilled data
from the other. Moreover, we cluster the ``annotator groups&apos;&apos; who share similar
expertise so that their confusion matrices could be corrected together. In this
way, the expertise of the annotators, especially of those who provide seldom
labels, could be better captured. Remarkably, we point out that the annotation
sparsity not only means the average number of labels is low, but also there are
always some annotators who provide very few labels, which is neglected by
previous works when constructing synthetic crowd-sourcing annotations. Based on
that, we propose to use Beta distribution to control the generation of the
crowd-sourcing labels so that the synthetic annotations could be more
consistent with the real-world ones. Extensive experiments are conducted on two
types of synthetic datasets and three real-world datasets, the results of which
demonstrate that CCC significantly outperforms state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hansong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shikun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1&quot;&gt;Dan Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1&quot;&gt;Chenggang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1&quot;&gt;Shiming Ge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07335">
<title>Momentum Particle Maximum Likelihood. (arXiv:2312.07335v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07335</link>
<description rdf:parseType="Literal">&lt;p&gt;Maximum likelihood estimation (MLE) of latent variable models is often recast
as an optimization problem over the extended space of parameters and
probability distributions. For example, the Expectation Maximization (EM)
algorithm can be interpreted as coordinate descent applied to a suitable free
energy functional over this space. Recently, this perspective has been combined
with insights from optimal transport and Wasserstein gradient flows to develop
particle-based algorithms applicable to wider classes of models than standard
EM.
&lt;/p&gt;
&lt;p&gt;Drawing inspiration from prior works which interpret `momentum-enriched&apos;
optimisation algorithms as discretizations of ordinary differential equations,
we propose an analogous dynamical systems-inspired approach to minimizing the
free energy functional over the extended space of parameters and probability
distributions. The result is a dynamic system that blends elements of
Nesterov&apos;s Accelerated Gradient method, the underdamped Langevin diffusion, and
particle methods.
&lt;/p&gt;
&lt;p&gt;Under suitable assumptions, we establish quantitative convergence of the
proposed system to the unique minimiser of the functional in continuous time.
We then propose a numerical discretization of this system which enables its
application to parameter estimation in latent variable models. Through
numerical experiments, we demonstrate that the resulting algorithm converges
faster than existing methods and compares favourably with other (approximate)
MLE algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1&quot;&gt;Jen Ning Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuntz_J/0/1/0/all/0/1&quot;&gt;Juan Kuntz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Power_S/0/1/0/all/0/1&quot;&gt;Samuel Power&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johansen_A/0/1/0/all/0/1&quot;&gt;Adam M. Johansen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07358">
<title>Distributional Bellman Operators over Mean Embeddings. (arXiv:2312.07358v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2312.07358</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel algorithmic framework for distributional reinforcement
learning, based on learning finite-dimensional mean embeddings of return
distributions. We derive several new algorithms for dynamic programming and
temporal-difference learning based on this framework, provide asymptotic
convergence theory, and examine the empirical performance of the algorithms on
a suite of tabular tasks. Further, we show that this approach can be
straightforwardly combined with deep reinforcement learning, and obtain a new
deep RL agent that improves over baseline distributional approaches on the
Arcade Learning Environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wenliang_L/0/1/0/all/0/1&quot;&gt;Li Kevin Wenliang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Deletang_G/0/1/0/all/0/1&quot;&gt;Gr&amp;#xe9;goire D&amp;#xe9;letang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Aitchison_M/0/1/0/all/0/1&quot;&gt;Matthew Aitchison&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hutter_M/0/1/0/all/0/1&quot;&gt;Marcus Hutter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ruoss_A/0/1/0/all/0/1&quot;&gt;Anian Ruoss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gretton_A/0/1/0/all/0/1&quot;&gt;Arthur Gretton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rowland_M/0/1/0/all/0/1&quot;&gt;Mark Rowland&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07371">
<title>Privacy-Aware Energy Consumption Modeling of Connected Battery Electric Vehicles using Federated Learning. (arXiv:2312.07371v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07371</link>
<description rdf:parseType="Literal">&lt;p&gt;Battery Electric Vehicles (BEVs) are increasingly significant in modern
cities due to their potential to reduce air pollution. Precise and real-time
estimation of energy consumption for them is imperative for effective itinerary
planning and optimizing vehicle systems, which can reduce driving range anxiety
and decrease energy costs. As public awareness of data privacy increases,
adopting approaches that safeguard data privacy in the context of BEV energy
consumption modeling is crucial. Federated Learning (FL) is a promising
solution mitigating the risk of exposing sensitive information to third parties
by allowing local data to remain on devices and only sharing model updates with
a central server. Our work investigates the potential of using FL methods, such
as FedAvg, and FedPer, to improve BEV energy consumption prediction while
maintaining user privacy. We conducted experiments using data from 10 BEVs
under simulated real-world driving conditions. Our results demonstrate that the
FedAvg-LSTM model achieved a reduction of up to 67.84\% in the MAE value of the
prediction results. Furthermore, we explored various real-world scenarios and
discussed how FL methods can be employed in those cases. Our findings show that
FL methods can effectively improve the performance of BEV energy consumption
prediction while maintaining user privacy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1&quot;&gt;Sen Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1&quot;&gt;Hongyuan Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Ji Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ward_T/0/1/0/all/0/1&quot;&gt;Tomas Ward&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OConnor_N/0/1/0/all/0/1&quot;&gt;Noel O&amp;#x27;Connor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mingming Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07392">
<title>ReRoGCRL: Representation-based Robustness in Goal-Conditioned Reinforcement Learning. (arXiv:2312.07392v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07392</link>
<description rdf:parseType="Literal">&lt;p&gt;While Goal-Conditioned Reinforcement Learning (GCRL) has gained attention,
its algorithmic robustness, particularly against adversarial perturbations,
remains unexplored. Unfortunately, the attacks and robust representation
training methods specifically designed for traditional RL are not so effective
when applied to GCRL. To address this challenge, we propose the
\textit{Semi-Contrastive Representation} attack, a novel approach inspired by
the adversarial contrastive attack. Unlike existing attacks in RL, it only
necessitates information from the policy function and can be seamlessly
implemented during deployment. Furthermore, to mitigate the vulnerability of
existing GCRL algorithms, we introduce \textit{Adversarial Representation
Tactics}. This strategy combines \textit{Semi-Contrastive Adversarial
Augmentation} with \textit{Sensitivity-Aware Regularizer}. It improves the
adversarial robustness of the underlying agent against various types of
perturbations. Extensive experiments validate the superior performance of our
attack and defence mechanism across multiple state-of-the-art GCRL algorithms.
Our tool {\bf ReRoGCRL} is available at
\url{https://github.com/TrustAI/ReRoGCRL}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1&quot;&gt;Xiangyu Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Sihao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiaxu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1&quot;&gt;Meng Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xingyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaowei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruan_W/0/1/0/all/0/1&quot;&gt;Wenjie Ruan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07405">
<title>ICL Markup: Structuring In-Context Learning using Soft-Token Tags. (arXiv:2312.07405v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07405</link>
<description rdf:parseType="Literal">&lt;p&gt;Large pretrained language models (LLMs) can be rapidly adapted to a wide
variety of tasks via a text-to-text approach, where the instruction and input
are fed to the model in natural language. Combined with in-context learning
(ICL), this paradigm is impressively flexible and powerful. However, it also
burdens users with an overwhelming number of choices, many of them arbitrary.
Inspired by markup languages like HTML, we contribute a method of using
soft-token tags to compose prompt templates. This approach reduces arbitrary
decisions and streamlines the application of ICL. Our method is a form of
meta-learning for ICL; it learns these tags in advance during a
parameter-efficient fine-tuning ``warm-up&apos;&apos; process. The tags can subsequently
be used in templates for ICL on new, unseen tasks without any additional
fine-tuning. Our experiments with this approach yield promising initial
results, improving LLM performance on important enterprise applications such as
few-shot and open-world intent detection, as well as text classification in
news and legal domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brunet_M/0/1/0/all/0/1&quot;&gt;Marc-Etienne Brunet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anderson_A/0/1/0/all/0/1&quot;&gt;Ashton Anderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1&quot;&gt;Richard Zemel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07413">
<title>AI capabilities can be significantly improved without expensive retraining. (arXiv:2312.07413v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.07413</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art AI systems can be significantly improved without expensive
retraining via &quot;post-training enhancements&quot;-techniques applied after initial
training like fine-tuning the system to use a web browser. We review recent
post-training enhancements, categorizing them into five types: tool-use,
prompting methods, scaffolding, solution selection, and data generation.
Different enhancements improve performance on different tasks, making it hard
to compare their significance. So we translate improvements from different
enhancements into a common currency, the compute-equivalent gain: how much
additional training compute would be needed to improve performance by the same
amount as the enhancement. Our non-experimental work shows that post-training
enhancements have significant benefits: most surveyed enhancements improve
benchmark performance by more than a 5x increase in training compute, some by
more than 20x. Post-training enhancements are relatively cheap to develop:
fine-tuning costs are typically &amp;lt;1% of the original training cost. Governing
the development of capable post-training enhancements may be challenging
because frontier models could be enhanced by a wide range of actors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davidson_T/0/1/0/all/0/1&quot;&gt;Tom Davidson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denain_J/0/1/0/all/0/1&quot;&gt;Jean-Stanislas Denain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Villalobos_P/0/1/0/all/0/1&quot;&gt;Pablo Villalobos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bas_G/0/1/0/all/0/1&quot;&gt;Guillem Bas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07414">
<title>QSMVM: QoS-aware and social-aware multimetric routing protocol for video-streaming services over MANETs. (arXiv:2312.07414v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2312.07414</link>
<description rdf:parseType="Literal">&lt;p&gt;A mobile ad hoc network (MANET) is a set of autonomous mobile devices
connected by wireless links in a distributed manner and without a fixed
infrastructure. Real-time multimedia services, such as video-streaming over
MANETs, offers very promising applications, e.g. two members of a group of
tourists who want to share a video transmitted through the MANET they form; a
video-streaming service deployed over a MANET where users watch a film; among
other examples. On the other hand, social web technologies, where people
actively interact online with others through social networks, are leading to a
socialization of networks. Information of interaction among users is being used
to provide socially-enhanced software. To achieve this, we need to know the
strength of the relationship between a given user and each user they interact
with. This strength of the relationship can be measured through a concept
called tie strength (TS), first introduced by Mark Granovetter in 1973. In this
article, we modify our previous proposal named multipath multimedia dynamic
source routing (MMDSR) protocol to include a social metric TS in the decisions
taken by the forwarding algorithm. We find a trade-off between the quality of
service (QoS) and the trust level between users who form the forwarding path in
the MANET. Our goal is to increase the trust metric while the QoS is not
affected significantly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jara_E/0/1/0/all/0/1&quot;&gt;Efra&amp;#xed;n Palacios Jara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mezhe_A/0/1/0/all/0/1&quot;&gt;Ahmad Mohamad Mezhe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Igartua_M/0/1/0/all/0/1&quot;&gt;M&amp;#xf3;nica Aguilar Igartua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Redondo_R/0/1/0/all/0/1&quot;&gt;Rebeca P. D&amp;#xed;az Redondo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vilas_A/0/1/0/all/0/1&quot;&gt;Ana Fern&amp;#xe1;ndez Vilas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07420">
<title>FairSISA: Ensemble Post-Processing to Improve Fairness of Unlearning in LLMs. (arXiv:2312.07420v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07420</link>
<description rdf:parseType="Literal">&lt;p&gt;Training large language models (LLMs) is a costly endeavour in terms of time
and computational resources. The large amount of training data used during the
unsupervised pre-training phase makes it difficult to verify all data and,
unfortunately, undesirable data may be ingested during training. Re-training
from scratch is impractical and has led to the creation of the &apos;unlearning&apos;
discipline where models are modified to &quot;unlearn&quot; undesirable information
without retraining. However, any modification can alter the behaviour of LLMs,
especially on key dimensions such as fairness. This is the first work that
examines this interplay between unlearning and fairness for LLMs. In
particular, we focus on a popular unlearning framework known as SISA [Bourtoule
et al., 2021], which creates an ensemble of models trained on disjoint shards.
We evaluate the performance-fairness trade-off for SISA, and empirically
demsontrate that SISA can indeed reduce fairness in LLMs. To remedy this, we
propose post-processing bias mitigation techniques for ensemble models produced
by SISA. We adapt the post-processing fairness improvement technique from
[Hardt et al., 2016] to design three methods that can handle model ensembles,
and prove that one of the methods is an optimal fair predictor for ensemble of
models. Through experimental results, we demonstrate the efficacy of our
post-processing framework called &apos;FairSISA&apos;.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kadhe_S/0/1/0/all/0/1&quot;&gt;Swanand Ravindra Kadhe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Halimi_A/0/1/0/all/0/1&quot;&gt;Anisa Halimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rawat_A/0/1/0/all/0/1&quot;&gt;Ambrish Rawat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baracaldo_N/0/1/0/all/0/1&quot;&gt;Nathalie Baracaldo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07424">
<title>How Well Does GPT-4V(ision) Adapt to Distribution Shifts? A Preliminary Investigation. (arXiv:2312.07424v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07424</link>
<description rdf:parseType="Literal">&lt;p&gt;In machine learning, generalization against distribution shifts -- where
deployment conditions diverge from the training scenarios -- is crucial,
particularly in fields like climate modeling, biomedicine, and autonomous
driving. The emergence of foundation models, distinguished by their extensive
pretraining and task versatility, has led to an increased interest in their
adaptability to distribution shifts. GPT-4V(ision) acts as the most advanced
publicly accessible multimodal foundation model, with extensive applications
across various domains, including anomaly detection, video understanding, image
generation, and medical diagnosis. However, its robustness against data
distributions remains largely underexplored. Addressing this gap, this study
rigorously evaluates GPT-4V&apos;s adaptability and generalization capabilities in
dynamic environments, benchmarking against prominent models like CLIP and
LLaVA. We delve into GPT-4V&apos;s zero-shot generalization across 13 diverse
datasets spanning natural, medical, and molecular domains. We further
investigate its adaptability to controlled data perturbations and examine the
efficacy of in-context learning as a tool to enhance its adaptation. Our
findings delineate GPT-4V&apos;s capability boundaries in distribution shifts,
shedding light on its strengths and limitations across various scenarios.
Importantly, this investigation contributes to our understanding of how AI
foundation models generalize to distribution shifts, offering pivotal insights
into their adaptability and robustness. Code is publicly available at
https://github.com/jameszhou-gl/gpt-4v-distribution-shift.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1&quot;&gt;Zhongyi Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1&quot;&gt;Guanglin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1&quot;&gt;Rundong He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jindong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xing Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tailin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1&quot;&gt;Yilong Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Salman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1&quot;&gt;Lina Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tongliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07425">
<title>Deep Internal Learning: Deep Learning from a Single Input. (arXiv:2312.07425v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07425</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning in general focuses on training a neural network from large
labeled datasets. Yet, in many cases there is value in training a network just
from the input at hand. This may involve training a network from scratch using
a single input or adapting an already trained network to a provided input
example at inference time. This survey paper aims at covering deep
internal-learning techniques that have been proposed in the past few years for
these two important directions. While our main focus will be on image
processing problems, most of the approaches that we survey are derived for
general signals (vectors with recurring patterns that can be distinguished from
noise) and are therefore applicable to other modalities. We believe that the
topic of internal-learning is very important in many signal and image
processing problems where training data is scarce and diversity is large on the
one hand, and on the other, there is a lot of structure in the data that can be
exploited.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tirer_T/0/1/0/all/0/1&quot;&gt;Tom Tirer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1&quot;&gt;Raja Giryes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1&quot;&gt;Se Young Chun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eldar_Y/0/1/0/all/0/1&quot;&gt;Yonina C. Eldar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07434">
<title>Multi-Modal Conformal Prediction Regions by Optimizing Convex Shape Templates. (arXiv:2312.07434v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07434</link>
<description rdf:parseType="Literal">&lt;p&gt;Conformal prediction is a statistical tool for producing prediction regions
for machine learning models that are valid with high probability. A key
component of conformal prediction algorithms is a non-conformity score function
that quantifies how different a model&apos;s prediction is from the unknown ground
truth value. Essentially, these functions determine the shape and the size of
the conformal prediction regions. However, little work has gone into finding
non-conformity score functions that produce prediction regions that are
multi-modal and practical, i.e., that can efficiently be used in engineering
applications. We propose a method that optimizes parameterized shape template
functions over calibration data, which results in non-conformity score
functions that produce prediction regions with minimum volume. Our approach
results in prediction regions that are multi-modal, so they can properly
capture residuals of distributions that have multiple modes, and practical, so
each region is convex and can be easily incorporated into downstream tasks,
such as a motion planner using conformal prediction regions. Our method applies
to general supervised learning tasks, while we illustrate its use in
time-series prediction. We provide a toolbox and present illustrative case
studies of F16 fighter jets and autonomous vehicles, showing an up to $68\%$
reduction in prediction region area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tumu_R/0/1/0/all/0/1&quot;&gt;Renukanandan Tumu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cleaveland_M/0/1/0/all/0/1&quot;&gt;Matthew Cleaveland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mangharam_R/0/1/0/all/0/1&quot;&gt;Rahul Mangharam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pappas_G/0/1/0/all/0/1&quot;&gt;George J. Pappas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lindemann_L/0/1/0/all/0/1&quot;&gt;Lars Lindemann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07435">
<title>Cross-modal Contrastive Learning with Asymmetric Co-attention Network for Video Moment Retrieval. (arXiv:2312.07435v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07435</link>
<description rdf:parseType="Literal">&lt;p&gt;Video moment retrieval is a challenging task requiring fine-grained
interactions between video and text modalities. Recent work in image-text
pretraining has demonstrated that most existing pretrained models suffer from
information asymmetry due to the difference in length between visual and
textual sequences. We question whether the same problem also exists in the
video-text domain with an auxiliary need to preserve both spatial and temporal
information. Thus, we evaluate a recently proposed solution involving the
addition of an asymmetric co-attention network for video grounding tasks.
Additionally, we incorporate momentum contrastive loss for robust,
discriminative representation learning in both modalities. We note that the
integration of these supplementary modules yields better performance compared
to state-of-the-art models on the TACoS dataset and comparable results on
ActivityNet Captions, all while utilizing significantly fewer parameters with
respect to baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panta_L/0/1/0/all/0/1&quot;&gt;Love Panta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shrestha_P/0/1/0/all/0/1&quot;&gt;Prashant Shrestha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sapkota_B/0/1/0/all/0/1&quot;&gt;Brabeem Sapkota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattarai_A/0/1/0/all/0/1&quot;&gt;Amrita Bhattarai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manandhar_S/0/1/0/all/0/1&quot;&gt;Suresh Manandhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sah_A/0/1/0/all/0/1&quot;&gt;Anand Kumar Sah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07439">
<title>BIRB: A Generalization Benchmark for Information Retrieval in Bioacoustics. (arXiv:2312.07439v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07439</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability for a machine learning model to cope with differences in training
and deployment conditions--e.g. in the presence of distribution shift or the
generalization to new classes altogether--is crucial for real-world use cases.
However, most empirical work in this area has focused on the image domain with
artificial benchmarks constructed to measure individual aspects of
generalization. We present BIRB, a complex benchmark centered on the retrieval
of bird vocalizations from passively-recorded datasets given focal recordings
from a large citizen science corpus available for training. We propose a
baseline system for this collection of tasks using representation learning and
a nearest-centroid search. Our thorough empirical evaluation and analysis
surfaces open research directions, suggesting that BIRB fills the need for a
more realistic and complex benchmark to drive progress on robustness to
distribution shifts and generalization of ML models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamer_J/0/1/0/all/0/1&quot;&gt;Jenny Hamer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Triantafillou_E/0/1/0/all/0/1&quot;&gt;Eleni Triantafillou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merrienboer_B/0/1/0/all/0/1&quot;&gt;Bart van Merrienboer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kahl_S/0/1/0/all/0/1&quot;&gt;Stefan Kahl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klinck_H/0/1/0/all/0/1&quot;&gt;Holger Klinck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denton_T/0/1/0/all/0/1&quot;&gt;Tom Denton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dumoulin_V/0/1/0/all/0/1&quot;&gt;Vincent Dumoulin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07457">
<title>Dynamics Harmonic Analysis of Robotic Systems: Application in Data-Driven Koopman Modelling. (arXiv:2312.07457v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.07457</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce the use of harmonic analysis to decompose the state space of
symmetric robotic systems into orthogonal isotypic subspaces. These are
lower-dimensional spaces that capture distinct, symmetric, and synergistic
motions. For linear dynamics, we characterize how this decomposition leads to a
subdivision of the dynamics into independent linear systems on each subspace, a
property we term dynamics harmonic analysis (DHA). To exploit this property, we
use Koopman operator theory to propose an equivariant deep-learning
architecture that leverages the properties of DHA to learn a global linear
model of system dynamics. Our architecture, validated on synthetic systems and
the dynamics of locomotion of a quadrupedal robot, demonstrates enhanced
generalization, sample efficiency, and interpretability, with less trainable
parameters and computational costs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ordonez_Apraez_D/0/1/0/all/0/1&quot;&gt;Daniel Ordo&amp;#xf1;ez-Apraez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kostic_V/0/1/0/all/0/1&quot;&gt;Vladimir Kostic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turrisi_G/0/1/0/all/0/1&quot;&gt;Giulio Turrisi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Novelli_P/0/1/0/all/0/1&quot;&gt;Pietro Novelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mastalli_C/0/1/0/all/0/1&quot;&gt;Carlos Mastalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Semini_C/0/1/0/all/0/1&quot;&gt;Claudio Semini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pontil_M/0/1/0/all/0/1&quot;&gt;Massimiliano Pontil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07466">
<title>Efficient Object Detection in Autonomous Driving using Spiking Neural Networks: Performance, Energy Consumption Analysis, and Insights into Open-set Object Discovery. (arXiv:2312.07466v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07466</link>
<description rdf:parseType="Literal">&lt;p&gt;Besides performance, efficiency is a key design driver of technologies
supporting vehicular perception. Indeed, a well-balanced trade-off between
performance and energy consumption is crucial for the sustainability of
autonomous vehicles. In this context, the diversity of real-world contexts in
which autonomous vehicles can operate motivates the need for empowering
perception models with the capability to detect, characterize and identify
newly appearing objects by themselves. In this manuscript we elaborate on this
threefold conundrum (performance, efficiency and open-world learning) for
object detection modeling tasks over image data collected from vehicular
scenarios. Specifically, we show that well-performing and efficient models can
be realized by virtue of Spiking Neural Networks (SNNs), reaching competitive
levels of detection performance when compared to their non-spiking counterparts
at dramatic energy consumption savings (up to 85%) and a slightly improved
robustness against image noise. Our experiments herein offered also expose
qualitatively the complexity of detecting new objects based on the preliminary
results of a simple approach to discriminate potential object proposals in the
captured image.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seras_A/0/1/0/all/0/1&quot;&gt;Aitor Martinez Seras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ser_J/0/1/0/all/0/1&quot;&gt;Javier Del Ser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_Bringas_P/0/1/0/all/0/1&quot;&gt;Pablo Garcia-Bringas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07492">
<title>SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models. (arXiv:2312.07492v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07492</link>
<description rdf:parseType="Literal">&lt;p&gt;Current datasets for unwanted social bias auditing are limited to studying
protected demographic features such as race and gender. In this work, we
introduce a comprehensive benchmark that is meant to capture the amplification
of social bias, via stigmas, in generative language models. We start with a
comprehensive list of 93 stigmas documented in social science literature and
curate a question-answering (QA) dataset which involves simple social
situations. Our benchmark, SocialStigmaQA, contains roughly 10K prompts, with a
variety of prompt styles, carefully constructed to systematically test for both
social bias and model robustness. We present results for SocialStigmaQA with
two widely used open source generative language models and we demonstrate that
the output generated by these models considerably amplifies existing social
bias against stigmatized groups. Specifically, we find that the proportion of
socially biased output ranges from 45% to 59% across a variety of decoding
strategies and prompting styles. We discover that the deliberate design of the
templates in our benchmark (e.g., by adding biasing text to the prompt or
varying the answer that indicates bias) impact the model tendencies to generate
socially biased output. Additionally, we report on patterns in the generated
chain-of-thought output, finding a variety of problems from subtle bias to
evidence of a lack of reasoning.
&lt;/p&gt;
&lt;p&gt;Warning: This paper contains examples of text which is toxic, biased, and
harmful.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagireddy_M/0/1/0/all/0/1&quot;&gt;Manish Nagireddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiazor_L/0/1/0/all/0/1&quot;&gt;Lamogha Chiazor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1&quot;&gt;Moninder Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baldini_I/0/1/0/all/0/1&quot;&gt;Ioana Baldini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1902.10664">
<title>Local Function Complexity for Active Learning via Mixture of Gaussian Processes. (arXiv:1902.10664v6 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1902.10664</link>
<description rdf:parseType="Literal">&lt;p&gt;Inhomogeneities in real-world data, e.g., due to changes in the observation
noise level or variations in the structural complexity of the source function,
pose a unique set of challenges for statistical inference. Accounting for them
can greatly improve predictive power when physical resources or computation
time is limited. In this paper, we draw on recent theoretical results on the
estimation of local function complexity (LFC), derived from the domain of local
polynomial smoothing (LPS), to establish a notion of local structural
complexity, which is used to develop a model-agnostic active learning (AL)
framework. Due to its reliance on pointwise estimates, the LPS model class is
not robust and scalable concerning large input space dimensions that typically
come along with real-world problems. Here, we derive and estimate the Gaussian
process regression (GPR)-based analog of the LPS-based LFC and use it as a
substitute in the above framework to make it robust and scalable. We assess the
effectiveness of our LFC estimate in an AL application on a prototypical
low-dimensional synthetic dataset, before taking on the challenging real-world
task of reconstructing a quantum chemical force field for a small organic
molecule and demonstrating state-of-the-art performance with a significantly
reduced training demand.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panknin_D/0/1/0/all/0/1&quot;&gt;Danny Panknin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chmiela_S/0/1/0/all/0/1&quot;&gt;Stefan Chmiela&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1&quot;&gt;Klaus-Robert M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakajima_S/0/1/0/all/0/1&quot;&gt;Shinichi Nakajima&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2010.02171">
<title>Factorized Discriminant Analysis for Genetic Signatures of Neuronal Phenotypes. (arXiv:2010.02171v7 [q-bio.QM] UPDATED)</title>
<link>http://arxiv.org/abs/2010.02171</link>
<description rdf:parseType="Literal">&lt;p&gt;Navigating the complex landscape of single-cell transcriptomic data presents
significant challenges. Central to this challenge is the identification of a
meaningful representation of high-dimensional gene expression patterns that
sheds light on the structural and functional properties of cell types. Pursuing
model interpretability and computational simplicity, we often look for a linear
transformation of the original data that aligns with key phenotypic features of
cells. In response to this need, we introduce factorized linear discriminant
analysis (FLDA), a novel method for linear dimensionality reduction. The crux
of FLDA lies in identifying a linear function of gene expression levels that is
highly correlated with one phenotypic feature while minimizing the influence of
others. To augment this method, we integrate it with a sparsity-based
regularization algorithm. This integration is crucial as it selects a subset of
genes pivotal to a specific phenotypic feature or a combination thereof. To
illustrate the effectiveness of FLDA, we apply it to transcriptomic datasets
from neurons in the Drosophila optic lobe. We demonstrate that FLDA not only
captures the inherent structural patterns aligned with phenotypic features but
also uncovers key genes associated with each phenotype.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Qiao_M/0/1/0/all/0/1&quot;&gt;Mu Qiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.04133">
<title>Quantifying disparities in intimate partner violence: a machine learning method to correct for underreporting. (arXiv:2110.04133v4 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2110.04133</link>
<description rdf:parseType="Literal">&lt;p&gt;Estimating the prevalence of a medical condition, or the proportion of the
population in which it occurs, is a fundamental problem in healthcare and
public health. Accurate estimates of the relative prevalence across groups --
capturing, for example, that a condition affects women more frequently than men
-- facilitate effective and equitable health policy which prioritizes groups
who are disproportionately affected by a condition. However, it is difficult to
estimate relative prevalence when a medical condition is underreported. In this
work, we provide a method for accurately estimating the relative prevalence of
underreported medical conditions, building upon the positive unlabeled learning
framework. We show that under the commonly made covariate shift assumption --
i.e., that the probability of having a disease conditional on symptoms remains
constant across groups -- we can recover the relative prevalence, even without
restrictive assumptions commonly made in positive unlabeled learning and even
if it is impossible to recover the absolute prevalence. We conduct experiments
on synthetic and real health data which demonstrate our method&apos;s ability to
recover the relative prevalence more accurately than do baselines, and
demonstrate the method&apos;s robustness to plausible violations of the covariate
shift assumption. We conclude by illustrating the applicability of our method
to case studies of intimate partner violence and hate speech.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shanmugam_D/0/1/0/all/0/1&quot;&gt;Divya Shanmugam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_K/0/1/0/all/0/1&quot;&gt;Kaihua Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pierson_E/0/1/0/all/0/1&quot;&gt;Emma Pierson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.06074">
<title>Early Stopping for Deep Image Prior. (arXiv:2112.06074v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2112.06074</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep image prior (DIP) and its variants have showed remarkable potential for
solving inverse problems in computer vision, without any extra training data.
Practical DIP models are often substantially overparameterized. During the
fitting process, these models learn mostly the desired visual content first,
and then pick up the potential modeling and observational noise, i.e.,
overfitting. Thus, the practicality of DIP often depends critically on good
early stopping (ES) that captures the transition period. In this regard, the
majority of DIP works for vision tasks only demonstrates the potential of the
models -- reporting the peak performance against the ground truth, but provides
no clue about how to operationally obtain near-peak performance without access
to the groundtruth. In this paper, we set to break this practicality barrier of
DIP, and propose an efficient ES strategy, which consistently detects near-peak
performance across several vision tasks and DIP variants. Based on a simple
measure of dispersion of consecutive DIP reconstructions, our ES method not
only outpaces the existing ones -- which only work in very narrow domains, but
also remains effective when combined with a number of methods that try to
mitigate the overfitting. The code is available at
https://github.com/sun-umn/Early_Stopping_for_DIP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hengkang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Taihui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1&quot;&gt;Zhong Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tiancong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1&quot;&gt;Hengyue Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Ju Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.16218">
<title>APG: Adaptive Parameter Generation Network for Click-Through Rate Prediction. (arXiv:2203.16218v3 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2203.16218</link>
<description rdf:parseType="Literal">&lt;p&gt;In many web applications, deep learning-based CTR prediction models (deep CTR
models for short) are widely adopted. Traditional deep CTR models learn
patterns in a static manner, i.e., the network parameters are the same across
all the instances. However, such a manner can hardly characterize each of the
instances which may have different underlying distributions. It actually limits
the representation power of deep CTR models, leading to sub-optimal results. In
this paper, we propose an efficient, effective, and universal module, named as
Adaptive Parameter Generation network (APG), which can dynamically generate
parameters for deep CTR models on-the-fly based on different instances.
Extensive experimental evaluation results show that APG can be applied to a
variety of deep CTR models and significantly improve their performance.
Meanwhile, APG can reduce the time cost by 38.7\% and memory usage by 96.6\%
compared to a regular deep CTR model. We have deployed APG in the industrial
sponsored search system and achieved 3\% CTR gain and 1\% RPM gain
respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1&quot;&gt;Bencheng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Pengjie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Feng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1&quot;&gt;Hongbo Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jian Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1&quot;&gt;Bo Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.04619">
<title>Risk Preferences of Learning Algorithms. (arXiv:2205.04619v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.04619</link>
<description rdf:parseType="Literal">&lt;p&gt;Agents&apos; learning from feedback shapes economic outcomes, and many economic
decision-makers today employ learning algorithms to make consequential choices.
This note shows that a widely used learning algorithm, $\varepsilon$-Greedy,
exhibits emergent risk aversion: it prefers actions with lower variance. When
presented with actions of the same expectation, under a wide range of
conditions, $\varepsilon$-Greedy chooses the lower-variance action with
probability approaching one. This emergent preference can have wide-ranging
consequences, ranging from concerns about fairness to homogenization, and holds
transiently even when the riskier action has a strictly higher expected payoff.
We discuss two methods to correct this bias. The first method requires the
algorithm to reweight data as a function of how likely the actions were to be
chosen. The second requires the algorithm to have optimistic estimates of
actions for which it has not collected much data. We show that risk-neutrality
is restored with these corrections.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haupt_A/0/1/0/all/0/1&quot;&gt;Andreas Haupt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narayanan_A/0/1/0/all/0/1&quot;&gt;Aroon Narayanan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.04281">
<title>Local Spatiotemporal Representation Learning for Longitudinally-consistent Neuroimage Analysis. (arXiv:2206.04281v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.04281</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent self-supervised advances in medical computer vision exploit global and
local anatomical self-similarity for pretraining prior to downstream tasks such
as segmentation. However, current methods assume i.i.d. image acquisition,
which is invalid in clinical study designs where follow-up longitudinal scans
track subject-specific temporal changes. Further, existing self-supervised
methods for medically-relevant image-to-image architectures exploit only
spatial or temporal self-similarity and only do so via a loss applied at a
single image-scale, with naive multi-scale spatiotemporal extensions collapsing
to degenerate solutions. To these ends, this paper makes two contributions: (1)
It presents a local and multi-scale spatiotemporal representation learning
method for image-to-image architectures trained on longitudinal images. It
exploits the spatiotemporal self-similarity of learned multi-scale
intra-subject features for pretraining and develops several feature-wise
regularizations that avoid collapsed identity representations; (2) During
finetuning, it proposes a surprisingly simple self-supervised segmentation
consistency regularization to exploit intra-subject correlation. Benchmarked in
the one-shot segmentation setting, the proposed framework outperforms both
well-tuned randomly-initialized baselines and current self-supervised
techniques designed for both i.i.d. and longitudinal datasets. These
improvements are demonstrated across both longitudinal neurodegenerative adult
MRI and developing infant brain MRI and yield both higher performance and
longitudinal consistency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1&quot;&gt;Mengwei Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dey_N/0/1/0/all/0/1&quot;&gt;Neel Dey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Styner_M/0/1/0/all/0/1&quot;&gt;Martin A. Styner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Botteron_K/0/1/0/all/0/1&quot;&gt;Kelly Botteron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gerig_G/0/1/0/all/0/1&quot;&gt;Guido Gerig&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.05575">
<title>MammoFL: Mammographic Breast Density Estimation using Federated Learning. (arXiv:2206.05575v4 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.05575</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we automate quantitative mammographic breast density
estimation with neural networks and show that this tool is a strong use case
for federated learning on multi-institutional datasets. Our dataset included
bilateral CC-view and MLO-view mammographic images from two separate
institutions. Two U-Nets were separately trained on algorithm-generated labels
to perform segmentation of the breast and dense tissue from these images and
subsequently calculate breast percent density (PD). The networks were trained
with federated learning and compared to three non-federated baselines, one
trained on each single-institution dataset and one trained on the aggregated
multi-institution dataset. We demonstrate that training on multi-institution
datasets is critical to algorithm generalizability. We further show that
federated learning on multi-institutional datasets improves model
generalization to unseen data at nearly the same level as centralized training
on multi-institutional datasets, indicating that federated learning can be
applied to our method to improve algorithm generalizability while maintaining
patient privacy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Muthukrishnan_R/0/1/0/all/0/1&quot;&gt;Ramya Muthukrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Heyler_A/0/1/0/all/0/1&quot;&gt;Angelina Heyler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Katti_K/0/1/0/all/0/1&quot;&gt;Keshava Katti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pati_S/0/1/0/all/0/1&quot;&gt;Sarthak Pati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mankowski_W/0/1/0/all/0/1&quot;&gt;Walter Mankowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Alahari_A/0/1/0/all/0/1&quot;&gt;Aprupa Alahari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sanborn_M/0/1/0/all/0/1&quot;&gt;Michael Sanborn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Conant_E/0/1/0/all/0/1&quot;&gt;Emily F. Conant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Scott_C/0/1/0/all/0/1&quot;&gt;Christopher Scott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Winham_S/0/1/0/all/0/1&quot;&gt;Stacey Winham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vachon_C/0/1/0/all/0/1&quot;&gt;Celine Vachon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chaudhari_P/0/1/0/all/0/1&quot;&gt;Pratik Chaudhari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kontos_D/0/1/0/all/0/1&quot;&gt;Despina Kontos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bakas_S/0/1/0/all/0/1&quot;&gt;Spyridon Bakas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.01711">
<title>Optimal Rates for Regularized Conditional Mean Embedding Learning. (arXiv:2208.01711v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2208.01711</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the consistency of a kernel ridge regression estimate of the
conditional mean embedding (CME), which is an embedding of the conditional
distribution of $Y$ given $X$ into a target reproducing kernel Hilbert space
$\mathcal{H}_Y$. The CME allows us to take conditional expectations of target
RKHS functions, and has been employed in nonparametric causal and Bayesian
inference. We address the misspecified setting, where the target CME is in the
space of Hilbert-Schmidt operators acting from an input interpolation space
between $\mathcal{H}_X$ and $L_2$, to $\mathcal{H}_Y$. This space of operators
is shown to be isomorphic to a newly defined vector-valued interpolation space.
Using this isomorphism, we derive a novel and adaptive statistical learning
rate for the empirical CME estimator under the misspecified setting. Our
analysis reveals that our rates match the optimal $O(\log n / n)$ rates without
assuming $\mathcal{H}_Y$ to be finite dimensional. We further establish a lower
bound on the learning rate, which shows that the obtained upper bound is
optimal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Meunier_D/0/1/0/all/0/1&quot;&gt;Dimitri Meunier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mollenhauer_M/0/1/0/all/0/1&quot;&gt;Mattes Mollenhauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gretton_A/0/1/0/all/0/1&quot;&gt;Arthur Gretton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.09247">
<title>Weak-signal extraction enabled by deep-neural-network denoising of diffraction data. (arXiv:2209.09247v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.09247</link>
<description rdf:parseType="Literal">&lt;p&gt;Removal or cancellation of noise has wide-spread applications for imaging and
acoustics. In every-day-life applications, denoising may even include
generative aspects, which are unfaithful to the ground truth. For scientific
use, however, denoising must reproduce the ground truth accurately. Here, we
show how data can be denoised via a deep convolutional neural network such that
weak signals appear with quantitative accuracy. In particular, we study X-ray
diffraction on crystalline materials. We demonstrate that weak signals stemming
from charge ordering, insignificant in the noisy data, become visible and
accurate in the denoised data. This success is enabled by supervised training
of a deep neural network with pairs of measured low- and high-noise data. We
demonstrate that using artificial noise does not yield such quantitatively
accurate results. Our approach thus illustrates a practical strategy for noise
filtering that can be applied to challenging acquisition problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Oppliger_J/0/1/0/all/0/1&quot;&gt;Jens Oppliger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Denner_M/0/1/0/all/0/1&quot;&gt;M. Michael Denner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kuspert_J/0/1/0/all/0/1&quot;&gt;Julia K&amp;#xfc;spert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Frison_R/0/1/0/all/0/1&quot;&gt;Ruggero Frison&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qisi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Morawietz_A/0/1/0/all/0/1&quot;&gt;Alexander Morawietz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ivashko_O/0/1/0/all/0/1&quot;&gt;Oleh Ivashko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dippel_A/0/1/0/all/0/1&quot;&gt;Ann-Christin Dippel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zimmermann_M/0/1/0/all/0/1&quot;&gt;Martin von Zimmermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bialo_I/0/1/0/all/0/1&quot;&gt;Izabela Bia&amp;#x142;o&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Martinelli_L/0/1/0/all/0/1&quot;&gt;Leonardo Martinelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fauque_B/0/1/0/all/0/1&quot;&gt;Beno&amp;#xee;t Fauqu&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jaewon Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Garcia_Fernandez_M/0/1/0/all/0/1&quot;&gt;Mirian Garcia-Fernandez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_K/0/1/0/all/0/1&quot;&gt;Ke-Jin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Christensen_N/0/1/0/all/0/1&quot;&gt;Niels B. Christensen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kurosawa_T/0/1/0/all/0/1&quot;&gt;Tohru Kurosawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Momono_N/0/1/0/all/0/1&quot;&gt;Naoki Momono&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Oda_M/0/1/0/all/0/1&quot;&gt;Migaku Oda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Natterer_F/0/1/0/all/0/1&quot;&gt;Fabian D. Natterer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fischer_M/0/1/0/all/0/1&quot;&gt;Mark H. Fischer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Neupert_T/0/1/0/all/0/1&quot;&gt;Titus Neupert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chang_J/0/1/0/all/0/1&quot;&gt;Johan Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.01959">
<title>Detect, Retrieve, Comprehend: A Flexible Framework for Zero-Shot Document-Level Question Answering. (arXiv:2210.01959v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2210.01959</link>
<description rdf:parseType="Literal">&lt;p&gt;Researchers produce thousands of scholarly documents containing valuable
technical knowledge. The community faces the laborious task of reading these
documents to identify, extract, and synthesize information. To automate
information gathering, document-level question answering (QA) offers a flexible
framework where human-posed questions can be adapted to extract diverse
knowledge. Finetuning QA systems requires access to labeled data (tuples of
context, question and answer). However, data curation for document QA is
uniquely challenging because the context (i.e. answer evidence passage) needs
to be retrieved from potentially long, ill-formatted documents. Existing QA
datasets sidestep this challenge by providing short, well-defined contexts that
are unrealistic in real-world applications. We present a three-stage document
QA approach: (1) text extraction from PDF; (2) evidence retrieval from
extracted texts to form well-posed contexts; (3) QA to extract knowledge from
contexts to return high-quality answers -- extractive, abstractive, or Boolean.
Using QASPER for evaluation, our detect-retrieve-comprehend (DRC) system
achieves a +7.19 improvement in Answer-F1 over existing baselines while
delivering superior context selection. Our results demonstrate that DRC holds
tremendous promise as a flexible framework for practical scientific document
QA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McDonald_T/0/1/0/all/0/1&quot;&gt;Tavish McDonald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsan_B/0/1/0/all/0/1&quot;&gt;Brian Tsan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saini_A/0/1/0/all/0/1&quot;&gt;Amar Saini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ordonez_J/0/1/0/all/0/1&quot;&gt;Juanita Ordonez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gutierrez_L/0/1/0/all/0/1&quot;&gt;Luis Gutierrez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1&quot;&gt;Phan Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mason_B/0/1/0/all/0/1&quot;&gt;Blake Mason&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_B/0/1/0/all/0/1&quot;&gt;Brenda Ng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.06434">
<title>Cross-client Label Propagation for Transductive and Semi-Supervised Federated Learning. (arXiv:2210.06434v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.06434</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Cross-Client Label Propagation(XCLP), a new method for
transductive federated learning. XCLP estimates a data graph jointly from the
data of multiple clients and computes labels for the unlabeled data by
propagating label information across the graph. To avoid clients having to
share their data with anyone, XCLP employs two cryptographically secure
protocols: secure Hamming distance computation and secure summation. We
demonstrate two distinct applications of XCLP within federated learning. In the
first, we use it in a one-shot way to predict labels for unseen test points. In
the second, we use it to repeatedly pseudo-label unlabeled training data in a
federated semi-supervised setting. Experiments on both real federated and
standard benchmark datasets show that in both applications XCLP achieves higher
classification accuracy than alternative approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scott_J/0/1/0/all/0/1&quot;&gt;Jonathan Scott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeo_M/0/1/0/all/0/1&quot;&gt;Michelle Yeo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lampert_C/0/1/0/all/0/1&quot;&gt;Christoph H. Lampert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.12496">
<title>Bayesian Optimization with Conformal Prediction Sets. (arXiv:2210.12496v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.12496</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian optimization is a coherent, ubiquitous approach to decision-making
under uncertainty, with applications including multi-arm bandits, active
learning, and black-box optimization. Bayesian optimization selects decisions
(i.e. objective function queries) with maximal expected utility with respect to
the posterior distribution of a Bayesian model, which quantifies reducible,
epistemic uncertainty about query outcomes. In practice, subjectively
implausible outcomes can occur regularly for two reasons: 1) model
misspecification and 2) covariate shift. Conformal prediction is an uncertainty
quantification method with coverage guarantees even for misspecified models and
a simple mechanism to correct for covariate shift. We propose conformal
Bayesian optimization, which directs queries towards regions of search space
where the model predictions have guaranteed validity, and investigate its
behavior on a suite of black-box optimization tasks and tabular ranking tasks.
In many cases we find that query coverage can be significantly improved without
harming sample-efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stanton_S/0/1/0/all/0/1&quot;&gt;Samuel Stanton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maddox_W/0/1/0/all/0/1&quot;&gt;Wesley Maddox&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1&quot;&gt;Andrew Gordon Wilson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.14404">
<title>Adversarial Purification with the Manifold Hypothesis. (arXiv:2210.14404v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.14404</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we formulate a novel framework for adversarial robustness using
the manifold hypothesis. This framework provides sufficient conditions for
defending against adversarial examples. We develop an adversarial purification
method with this framework. Our method combines manifold learning with
variational inference to provide adversarial robustness without the need for
expensive adversarial training. Experimentally, our approach can provide
adversarial robustness even if attackers are aware of the existence of the
defense. In addition, our method can also serve as a test-time defense
mechanism for variational autoencoders.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhaoyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hartley_R/0/1/0/all/0/1&quot;&gt;Richard Hartley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_P/0/1/0/all/0/1&quot;&gt;Peter Tu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.16386">
<title>Non-Stationary Bandits with Auto-Regressive Temporal Dependency. (arXiv:2210.16386v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.16386</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional multi-armed bandit (MAB) frameworks, predominantly examined under
stochastic or adversarial settings, often overlook the temporal dynamics
inherent in many real-world applications such as recommendation systems and
online advertising. This paper introduces a novel non-stationary MAB framework
that captures the temporal structure of these real-world dynamics through an
auto-regressive (AR) reward structure. We propose an algorithm that integrates
two key mechanisms: (i) an alternation mechanism adept at leveraging temporal
dependencies to dynamically balance exploration and exploitation, and (ii) a
restarting mechanism designed to discard out-of-date information. Our algorithm
achieves a regret upper bound that nearly matches the lower bound, with regret
measured against a robust dynamic benchmark. Finally, via a real-world case
study on tourism demand prediction, we demonstrate both the efficacy of our
algorithm and the broader applicability of our techniques to more complex,
rapidly evolving time series.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qinyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golrezaei_N/0/1/0/all/0/1&quot;&gt;Negin Golrezaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouneffouf_D/0/1/0/all/0/1&quot;&gt;Djallel Bouneffouf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.13108">
<title>Integral Continual Learning Along the Tangent Vector Field of Tasks. (arXiv:2211.13108v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.13108</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a lightweight continual learning method which incorporates
information from specialized datasets incrementally, by integrating it along
the vector field of &quot;generalist&quot; models. The tangent plane to the specialist
model acts as a generalist guide and avoids the kind of over-fitting that leads
to catastrophic forgetting, while exploiting the convexity of the optimization
landscape in the tangent plane. It maintains a small fixed-size memory buffer,
as low as 0.4% of the source datasets, which is updated by simple resampling.
Our method achieves strong performance across various buffer sizes for
different datasets. Specifically, in the class-incremental setting we
outperform the existing methods that do not require distillation by an average
of 18.77% and 28.48%, for Seq-CIFAR-10 and Seq-TinyImageNet respectively. Our
method can easily be used in conjunction with existing replay-based continual
learning methods. When memory buffer constraints are relaxed to allow storage
of metadata such as logits, we attain an error reduction of 17.84% towards the
paragon performance on Seq-CIFAR-10.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tian Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golatkar_A/0/1/0/all/0/1&quot;&gt;Aditya Golatkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1&quot;&gt;Stefano Soatto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1&quot;&gt;Alessandro Achille&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.11093">
<title>Simple diffusion: End-to-end diffusion for high resolution images. (arXiv:2301.11093v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.11093</link>
<description rdf:parseType="Literal">&lt;p&gt;Currently, applying diffusion models in pixel space of high resolution images
is difficult. Instead, existing approaches focus on diffusion in lower
dimensional spaces (latent diffusion), or have multiple super-resolution levels
of generation referred to as cascades. The downside is that these approaches
add additional complexity to the diffusion framework.
&lt;/p&gt;
&lt;p&gt;This paper aims to improve denoising diffusion for high resolution images
while keeping the model as simple as possible. The paper is centered around the
research question: How can one train a standard denoising diffusion models on
high resolution images, and still obtain performance comparable to these
alternate approaches?
&lt;/p&gt;
&lt;p&gt;The four main findings are: 1) the noise schedule should be adjusted for high
resolution images, 2) It is sufficient to scale only a particular part of the
architecture, 3) dropout should be added at specific locations in the
architecture, and 4) downsampling is an effective strategy to avoid high
resolution feature maps. Combining these simple yet effective techniques, we
achieve state-of-the-art on image generation among diffusion models without
sampling modifiers on ImageNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoogeboom_E/0/1/0/all/0/1&quot;&gt;Emiel Hoogeboom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heek_J/0/1/0/all/0/1&quot;&gt;Jonathan Heek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salimans_T/0/1/0/all/0/1&quot;&gt;Tim Salimans&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.02904">
<title>Rethinking Gauss-Newton for learning over-parameterized models. (arXiv:2302.02904v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.02904</link>
<description rdf:parseType="Literal">&lt;p&gt;This work studies the global convergence and implicit bias of Gauss Newton&apos;s
(GN) when optimizing over-parameterized one-hidden layer networks in the
mean-field regime. We first establish a global convergence result for GN in the
continuous-time limit exhibiting a faster convergence rate compared to GD due
to improved conditioning. We then perform an empirical study on a synthetic
regression task to investigate the implicit bias of GN&apos;s method. While GN is
consistently faster than GD in finding a global optimum, the learned model
generalizes well on test data when starting from random initial weights with a
small variance and using a small step size to slow down convergence.
Specifically, our study shows that such a setting results in a hidden learning
phenomenon, where the dynamics are able to recover features with good
generalization properties despite the model having sub-optimal training and
test performances due to an under-optimized linear layer. This study exhibits a
trade-off between the convergence speed of GN and the generalization ability of
the learned solution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arbel_M/0/1/0/all/0/1&quot;&gt;Michael Arbel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menegaux_R/0/1/0/all/0/1&quot;&gt;Romain Menegaux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolinski_P/0/1/0/all/0/1&quot;&gt;Pierre Wolinski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.07321">
<title>On Classification-Calibration of Gamma-Phi Losses. (arXiv:2302.07321v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2302.07321</link>
<description rdf:parseType="Literal">&lt;p&gt;Gamma-Phi losses constitute a family of multiclass classification loss
functions that generalize the logistic and other common losses, and have found
application in the boosting literature. We establish the first general
sufficient condition for the classification-calibration (CC) of such losses. To
our knowledge, this sufficient condition gives the first family of nonconvex
multiclass surrogate losses for which CC has been fully justified. In addition,
we show that a previously proposed sufficient condition is in fact not
sufficient. This contribution highlights a technical issue that is important in
the study of multiclass CC but has been neglected in prior work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yutong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Scott_C/0/1/0/all/0/1&quot;&gt;Clayton D. Scott&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.08618">
<title>SplitOut: Out-of-the-Box Training-Hijacking Detection in Split Learning via Outlier Detection. (arXiv:2302.08618v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.08618</link>
<description rdf:parseType="Literal">&lt;p&gt;Split learning enables efficient and privacy-aware training of a deep neural
network by splitting a neural network so that the clients (data holders)
compute the first layers and only share the intermediate output with the
central compute-heavy server. This paradigm introduces a new attack medium in
which the server has full control over what the client models learn, which has
already been exploited to infer the private data of clients and to implement
backdoors in the client models. Although previous work has shown that clients
can successfully detect such training-hijacking attacks, the proposed methods
rely on heuristics, require tuning of many hyperparameters, and do not fully
utilize the clients&apos; capabilities. In this work, we show that given modest
assumptions regarding the clients&apos; compute capabilities, an out-of-the-box
outlier detection method can be used to detect existing training-hijacking
attacks with almost-zero false positive rates. We conclude through experiments
on different tasks that the simplicity of our approach we name SplitOut makes
it a more viable and reliable alternative compared to the earlier detection
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erdogan_E/0/1/0/all/0/1&quot;&gt;Ege Erdogan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teksen_U/0/1/0/all/0/1&quot;&gt;Unat Teksen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Celiktenyildiz_M/0/1/0/all/0/1&quot;&gt;Mehmet Salih Celiktenyildiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kupcu_A/0/1/0/all/0/1&quot;&gt;Alptekin Kupcu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cicek_A/0/1/0/all/0/1&quot;&gt;A. Ercument Cicek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.11091">
<title>GTRL: An Entity Group-Aware Temporal Knowledge Graph Representation Learning Method. (arXiv:2302.11091v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.11091</link>
<description rdf:parseType="Literal">&lt;p&gt;Temporal Knowledge Graph (TKG) representation learning embeds entities and
event types into a continuous low-dimensional vector space by integrating the
temporal information, which is essential for downstream tasks, e.g., event
prediction and question answering. Existing methods stack multiple graph
convolution layers to model the influence of distant entities, leading to the
over-smoothing problem. To alleviate the problem, recent studies infuse
reinforcement learning to obtain paths that contribute to modeling the
influence of distant entities. However, due to the limited number of hops,
these studies fail to capture the correlation between entities that are far
apart and even unreachable. To this end, we propose GTRL, an entity Group-aware
Temporal knowledge graph Representation Learning method. GTRL is the first work
that incorporates the entity group modeling to capture the correlation between
entities by stacking only a finite number of layers. Specifically, the entity
group mapper is proposed to generate entity groups from entities in a learning
way. Based on entity groups, the implicit correlation encoder is introduced to
capture implicit correlations between any pairwise entity groups. In addition,
the hierarchical GCNs are exploited to accomplish the message aggregation and
representation updating on the entity group graph and the entity graph.
Finally, GRUs are employed to capture the temporal dependency in TKGs.
Extensive experiments on three real-world datasets demonstrate that GTRL
achieves the state-of-the-art performances on the event prediction task,
outperforming the best baseline by an average of 13.44%, 9.65%, 12.15%, and
15.12% in MRR, Hits@1, Hits@3, and Hits@10, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1&quot;&gt;Xing Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Ling Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.00836">
<title>Ensemble flow reconstruction in the atmospheric boundary layer from spatially limited measurements through latent diffusion models. (arXiv:2303.00836v2 [physics.ao-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2303.00836</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to costs and practical constraints, field campaigns in the atmospheric
boundary layer typically only measure a fraction of the atmospheric volume of
interest. Machine learning techniques have previously successfully
reconstructed unobserved regions of flow in canonical fluid mechanics problems
and two-dimensional geophysical flows, but these techniques have not yet been
demonstrated in the three-dimensional atmospheric boundary layer. Here, we
conduct a numerical analogue of a field campaign with spatially limited
measurements using large-eddy simulation. We pose flow reconstruction as an
inpainting problem, and reconstruct realistic samples of turbulent,
three-dimensional flow with the use of a latent diffusion model. The diffusion
model generates physically plausible turbulent structures on larger spatial
scales, even when input observations cover less than 1% of the volume. Through
a combination of qualitative visualization and quantitative assessment, we
demonstrate that the diffusion model generates meaningfully diverse samples
when conditioned on just one observation. These samples successfully serve as
initial conditions for a large-eddy simulation code. We find that diffusion
models show promise and potential for other applications for other turbulent
flow reconstruction problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Rybchuk_A/0/1/0/all/0/1&quot;&gt;Alex Rybchuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Hassanaly_M/0/1/0/all/0/1&quot;&gt;Malik Hassanaly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Hamilton_N/0/1/0/all/0/1&quot;&gt;Nicholas Hamilton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Doubrawa_P/0/1/0/all/0/1&quot;&gt;Paula Doubrawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Fulton_M/0/1/0/all/0/1&quot;&gt;Mitchell J. Fulton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Martinez_Tossas_L/0/1/0/all/0/1&quot;&gt;Luis A. Mart&amp;#xed;nez-Tossas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.00855">
<title>Grounded Decoding: Guiding Text Generation with Grounded Models for Embodied Agents. (arXiv:2303.00855v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2303.00855</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent progress in large language models (LLMs) has demonstrated the ability
to learn and leverage Internet-scale knowledge through pre-training with
autoregressive models. Unfortunately, applying such models to settings with
embodied agents, such as robots, is challenging due to their lack of experience
with the physical world, inability to parse non-language observations, and
ignorance of rewards or safety constraints that robots may require. On the
other hand, language-conditioned robotic policies that learn from interaction
data can provide the necessary grounding that allows the agent to be correctly
situated in the real world, but such policies are limited by the lack of
high-level semantic understanding due to the limited breadth of the interaction
data available for training them. Thus, if we want to make use of the semantic
knowledge in a language model while still situating it in an embodied setting,
we must construct an action sequence that is both likely according to the
language model and also realizable according to grounded models of the
environment. We frame this as a problem similar to probabilistic filtering:
decode a sequence that both has high probability under the language model and
high probability under a set of grounded model objectives. We demonstrate how
such grounded models can be obtained across three simulation and real-world
domains, and that the proposed decoding strategy is able to solve complex,
long-horizon embodiment tasks in a robotic setting by leveraging the knowledge
of both models. The project&apos;s website can be found at
grounded-decoding.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wenlong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1&quot;&gt;Fei Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1&quot;&gt;Dhruv Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Driess_D/0/1/0/all/0/1&quot;&gt;Danny Driess&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1&quot;&gt;Andy Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Florence_P/0/1/0/all/0/1&quot;&gt;Pete Florence&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1&quot;&gt;Igor Mordatch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hausman_K/0/1/0/all/0/1&quot;&gt;Karol Hausman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ichter_B/0/1/0/all/0/1&quot;&gt;Brian Ichter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.01092">
<title>ArCL: Enhancing Contrastive Learning with Augmentation-Robust Representations. (arXiv:2303.01092v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.01092</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-Supervised Learning (SSL) is a paradigm that leverages unlabeled data
for model training. Empirical studies show that SSL can achieve promising
performance in distribution shift scenarios, where the downstream and training
distributions differ. However, the theoretical understanding of its
transferability remains limited. In this paper, we develop a theoretical
framework to analyze the transferability of self-supervised contrastive
learning, by investigating the impact of data augmentation on it. Our results
reveal that the downstream performance of contrastive learning depends largely
on the choice of data augmentation. Moreover, we show that contrastive learning
fails to learn domain-invariant features, which limits its transferability.
Based on these theoretical insights, we propose a novel method called
Augmentation-robust Contrastive Learning (ArCL), which guarantees to learn
domain-invariant features and can be easily integrated with existing
contrastive learning algorithms. We conduct experiments on several datasets and
show that ArCL significantly improves the transferability of contrastive
learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xuyang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_T/0/1/0/all/0/1&quot;&gt;Tianqi Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yisen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Jun Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Weiran Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.08459">
<title>Forecasting Intraday Power Output by a Set of PV Systems using Recurrent Neural Networks and Physical Covariates. (arXiv:2303.08459v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.08459</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate intraday forecasts of the power output by PhotoVoltaic (PV) systems
are critical to improve the operation of energy distribution grids. We describe
a neural autoregressive model which aims at performing such intraday forecasts.
We build upon a physical, deterministic PV performance model, the output of
which being used as covariates in the context of the neural model. In addition,
our application data relates to a geographically distributed set of PV systems.
We address all PV sites with a single neural model, which embeds the
information about the PV site in specific covariates. We use a scale-free
approach which does rely on explicit modelling of seasonal effects. Our
proposal repurposes a model initially used in the retail sector, and discloses
a novel truncated Gaussian output distribution. An ablation study and a
comparison to alternative architectures from the literature shows that the
components in the best performing proposed model variant work synergistically
to reach a skill score of 15.72% with respect to the physical model, used as a
baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bruneau_P/0/1/0/all/0/1&quot;&gt;Pierrick Bruneau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fiorelli_D/0/1/0/all/0/1&quot;&gt;David Fiorelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braun_C/0/1/0/all/0/1&quot;&gt;Christian Braun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koster_D/0/1/0/all/0/1&quot;&gt;Daniel Koster&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.16852">
<title>Diffusion Schr\&quot;odinger Bridge Matching. (arXiv:2303.16852v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2303.16852</link>
<description rdf:parseType="Literal">&lt;p&gt;Solving transport problems, i.e. finding a map transporting one given
distribution to another, has numerous applications in machine learning. Novel
mass transport methods motivated by generative modeling have recently been
proposed, e.g. Denoising Diffusion Models (DDMs) and Flow Matching Models
(FMMs) implement such a transport through a Stochastic Differential Equation
(SDE) or an Ordinary Differential Equation (ODE). However, while it is
desirable in many applications to approximate the deterministic dynamic Optimal
Transport (OT) map which admits attractive properties, DDMs and FMMs are not
guaranteed to provide transports close to the OT map. In contrast,
Schr\&quot;odinger bridges (SBs) compute stochastic dynamic mappings which recover
entropy-regularized versions of OT. Unfortunately, existing numerical methods
approximating SBs either scale poorly with dimension or accumulate errors
across iterations. In this work, we introduce Iterative Markovian Fitting
(IMF), a new methodology for solving SB problems, and Diffusion Schr\&quot;odinger
Bridge Matching (DSBM), a novel numerical algorithm for computing IMF iterates.
DSBM significantly improves over previous SB numerics and recovers as
special/limiting cases various recent transport methods. We demonstrate the
performance of DSBM on a variety of problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yuyang Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bortoli_V/0/1/0/all/0/1&quot;&gt;Valentin De Bortoli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Campbell_A/0/1/0/all/0/1&quot;&gt;Andrew Campbell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1&quot;&gt;Arnaud Doucet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.01168">
<title>DeepAccident: A Motion and Accident Prediction Benchmark for V2X Autonomous Driving. (arXiv:2304.01168v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.01168</link>
<description rdf:parseType="Literal">&lt;p&gt;Safety is the primary priority of autonomous driving. Nevertheless, no
published dataset currently supports the direct and explainable safety
evaluation for autonomous driving. In this work, we propose DeepAccident, a
large-scale dataset generated via a realistic simulator containing diverse
accident scenarios that frequently occur in real-world driving. The proposed
DeepAccident dataset includes 57K annotated frames and 285K annotated samples,
approximately 7 times more than the large-scale nuScenes dataset with 40k
annotated samples. In addition, we propose a new task, end-to-end motion and
accident prediction, which can be used to directly evaluate the accident
prediction ability for different autonomous driving algorithms. Furthermore,
for each scenario, we set four vehicles along with one infrastructure to record
data, thus providing diverse viewpoints for accident scenarios and enabling V2X
(vehicle-to-everything) research on perception and prediction tasks. Finally,
we present a baseline V2X model named V2XFormer that demonstrates superior
performance for motion and accident prediction and 3D object detection compared
to the single-vehicle model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sukmin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1&quot;&gt;Wenxuan Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1&quot;&gt;Enze Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_C/0/1/0/all/0/1&quot;&gt;Chongjian Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junsong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenguo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.03907">
<title>Stochastic Nonlinear Control via Finite-dimensional Spectral Dynamic Embedding. (arXiv:2304.03907v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.03907</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents an approach, Spectral Dynamics Embedding Control (SDEC),
to optimal control for nonlinear stochastic systems. This method leverages an
infinite-dimensional feature to linearly represent the state-action value
function and exploits finite-dimensional truncation approximation for practical
implementation. To characterize the effectiveness of these finite dimensional
approximations, we provide an in-depth theoretical analysis to characterize the
approximation error induced by the finite-dimension truncation and statistical
error induced by finite-sample approximation in both policy evaluation and
policy optimization. Our analysis includes two prominent kernel approximation
methods: truncations onto random features and Nystrom features. We also
empirically test the algorithm and compare the performance with Koopman-based,
iLQR, and energy-based methods on a few benchmark problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1&quot;&gt;Tongzheng Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1&quot;&gt;Zhaolin Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1&quot;&gt;Na Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Haitong Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1&quot;&gt;Bo Dai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.12130">
<title>Reconstructing Turbulent Flows Using Physics-Aware Spatio-Temporal Dynamics and Test-Time Refinement. (arXiv:2304.12130v3 [physics.flu-dyn] UPDATED)</title>
<link>http://arxiv.org/abs/2304.12130</link>
<description rdf:parseType="Literal">&lt;p&gt;Simulating turbulence is critical for many societally important applications
in aerospace engineering, environmental science, the energy industry, and
biomedicine. Large eddy simulation (LES) has been widely used as an alternative
to direct numerical simulation (DNS) for simulating turbulent flows due to its
reduced computational cost. However, LES is unable to capture all of the scales
of turbulent transport accurately. Reconstructing DNS from low-resolution LES
is critical for many scientific and engineering disciplines, but it poses many
challenges to existing super-resolution methods due to the spatio-temporal
complexity of turbulent flows. In this work, we propose a new physics-guided
neural network for reconstructing the sequential DNS from low-resolution LES
data. The proposed method leverages the partial differential equation that
underlies the flow dynamics in the design of spatio-temporal model
architecture. A degradation-based refinement method is also developed to
enforce physical constraints and further reduce the accumulated reconstruction
errors over long periods. The results on two different types of turbulent flow
data confirm the superiority of the proposed method in reconstructing the
high-resolution DNS data and preserving the physical characteristics of flow
transport.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shengyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bao_T/0/1/0/all/0/1&quot;&gt;Tianshu Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Givi_P/0/1/0/all/0/1&quot;&gt;Peyman Givi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Can Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Jia_X/0/1/0/all/0/1&quot;&gt;Xiaowei Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.13107">
<title>Time-Selective RNN for Device-Free Multi-Room Human Presence Detection Using WiFi CSI. (arXiv:2304.13107v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2304.13107</link>
<description rdf:parseType="Literal">&lt;p&gt;Device-free human presence detection is a crucial technology for various
applications, including home automation, security, and healthcare. While
camera-based systems have traditionally been used for this purpose, they raise
privacy concerns. To address this issue, recent research has explored the use
of wireless channel state information (CSI) extracted from commercial WiFi
access points (APs) to provide detailed channel characteristics. In this paper,
we propose a device-free human presence detection system for multi-room
scenarios using a time-selective conditional dual feature extract recurrent
network (TCD-FERN). Our system is designed to capture significant time features
on current human features using a dynamic and static data preprocessing
technique. We extract both moving and spatial features of people and
differentiate between line-of-sight (LoS) and non-line-of-sight (NLoS) cases.
Subcarrier fusion is carried out in order to provide more objective variation
of each sample while reducing the computational complexity. A voting scheme is
further adopted to mitigate the feature attenuation problem caused by room
partitions, with around 3% improvement of human presence detection accuracy.
Experimental results have revealed the significant improvement of leveraging
subcarrier fusion, dual-feature recurrent network, time selection and condition
mechanisms. Compared to the existing works in open literature, our proposed
TCD-FERN system can achieve above 97% of human presence detection accuracy for
multi-room scenarios with the adoption of fewer WiFi APs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Li-Hsiang Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsiao_A/0/1/0/all/0/1&quot;&gt;An-Hung Hsiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_F/0/1/0/all/0/1&quot;&gt;Fang-Yu Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_K/0/1/0/all/0/1&quot;&gt;Kai-Ten Feng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.14660">
<title>Segment Anything Model for Medical Images?. (arXiv:2304.14660v5 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.14660</link>
<description rdf:parseType="Literal">&lt;p&gt;The Segment Anything Model (SAM) is the first foundation model for general
image segmentation. It has achieved impressive results on various natural image
segmentation tasks. However, medical image segmentation (MIS) is more
challenging because of the complex modalities, fine anatomical structures,
uncertain and complex object boundaries, and wide-range object scales. To fully
validate SAM&apos;s performance on medical data, we collected and sorted 53
open-source datasets and built a large medical segmentation dataset with 18
modalities, 84 objects, 125 object-modality paired targets, 1050K 2D images,
and 6033K masks. We comprehensively analyzed different models and strategies on
the so-called COSMOS 1050K dataset. Our findings mainly include the following:
1) SAM showed remarkable performance in some specific objects but was unstable,
imperfect, or even totally failed in other situations. 2) SAM with the large
ViT-H showed better overall performance than that with the small ViT-B. 3) SAM
performed better with manual hints, especially box, than the Everything mode.
4) SAM could help human annotation with high labeling quality and less time. 5)
SAM was sensitive to the randomness in the center point and tight box prompts,
and may suffer from a serious performance drop. 6) SAM performed better than
interactive methods with one or a few points, but will be outpaced as the
number of points increases. 7) SAM&apos;s performance correlated to different
factors, including boundary complexity, intensity differences, etc. 8)
Finetuning the SAM on specific medical tasks could improve its average DICE
performance by 4.39% and 6.68% for ViT-B and ViT-H, respectively. We hope that
this comprehensive report can help researchers explore the potential of SAM
applications in MIS, and guide how to appropriately use and develop SAM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yuhao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Han Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chang_A/0/1/0/all/0/1&quot;&gt;Ao Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xinrui Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Rusi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Junxuan Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiongquan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chaoyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sijing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chi_H/0/1/0/all/0/1&quot;&gt;Haozhe Chi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xindi Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yue_K/0/1/0/all/0/1&quot;&gt;Kejuan Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Grau_V/0/1/0/all/0/1&quot;&gt;Vicente Grau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fan_D/0/1/0/all/0/1&quot;&gt;Deng-Ping Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dong_F/0/1/0/all/0/1&quot;&gt;Fajin Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ni_D/0/1/0/all/0/1&quot;&gt;Dong Ni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05218">
<title>Graph Neural Network-based surrogate model for granular flows. (arXiv:2305.05218v2 [physics.geo-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05218</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate simulation of granular flow dynamics is crucial for assessing
various geotechnical risks, including landslides and debris flows. Granular
flows involve a dynamic rearrangement of particles exhibiting complex
transitions from solid-like to fluid-like responses. Traditional continuum and
discrete numerical methods are limited by their computational cost in
simulating large-scale systems. Statistical or machine learning-based models
offer an alternative. Still, they are largely empirical, based on a limited set
of parameters. Due to their permutation-dependent learning, traditional machine
learning-based models require huge training data to generalize. To resolve
these problems, we use a graph neural network, a state-of-the-art machine
learning architecture that learns local interactions. Graphs represent the
state of dynamically changing granular flows and the interaction laws, such as
energy and momentum exchange between grains. We develop a graph neural
network-based simulator (GNS) that takes the current state of granular flow and
predicts the next state using Euler explicit integration by learning the local
interaction laws. We train GNS on different granular trajectories. We then
assess the performance of GNS by predicting granular column collapse. GNS
accurately predicts flow dynamics for column collapses with different aspect
ratios unseen during training. GNS is hundreds of times faster than
high-fidelity numerical simulators. The model also generalizes to domains much
larger than the training data, handling more than twice the number of particles
than it was trained on.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Choi_Y/0/1/0/all/0/1&quot;&gt;Yongjin Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kumar_K/0/1/0/all/0/1&quot;&gt;Krishna Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15486">
<title>SPRING: Studying the Paper and Reasoning to Play Games. (arXiv:2305.15486v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15486</link>
<description rdf:parseType="Literal">&lt;p&gt;Open-world survival games pose significant challenges for AI algorithms due
to their multi-tasking, deep exploration, and goal prioritization requirements.
Despite reinforcement learning (RL) being popular for solving games, its high
sample complexity limits its effectiveness in complex open-world games like
Crafter or Minecraft. We propose a novel approach, SPRING, to read the game&apos;s
original academic paper and use the knowledge learned to reason and play the
game through a large language model (LLM). Prompted with the LaTeX source as
game context and a description of the agent&apos;s current observation, our SPRING
framework employs a directed acyclic graph (DAG) with game-related questions as
nodes and dependencies as edges. We identify the optimal action to take in the
environment by traversing the DAG and calculating LLM responses for each node
in topological order, with the LLM&apos;s answer to final node directly translating
to environment actions. In our experiments, we study the quality of in-context
&quot;reasoning&quot; induced by different forms of prompts under the setting of the
Crafter open-world environment. Our experiments suggest that LLMs, when
prompted with consistent chain-of-thought, have great potential in completing
sophisticated high-level trajectories. Quantitatively, SPRING with GPT-4
outperforms all state-of-the-art RL baselines, trained for 1M steps, without
any training. Finally, we show the potential of games as a test bed for LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yue Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabhumoye_S/0/1/0/all/0/1&quot;&gt;Shrimai Prabhumoye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1&quot;&gt;So Yeon Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1&quot;&gt;Yonatan Bisk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1&quot;&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azaria_A/0/1/0/all/0/1&quot;&gt;Amos Azaria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitchell_T/0/1/0/all/0/1&quot;&gt;Tom Mitchell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanzhi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15546">
<title>Regret-Optimal Model-Free Reinforcement Learning for Discounted MDPs with Short Burn-In Time. (arXiv:2305.15546v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15546</link>
<description rdf:parseType="Literal">&lt;p&gt;A crucial problem in reinforcement learning is learning the optimal policy.
We study this in tabular infinite-horizon discounted Markov decision processes
under the online setting. The existing algorithms either fail to achieve regret
optimality or have to incur a high memory and computational cost. In addition,
existing optimal algorithms all require a long burn-in time in order to achieve
optimal sample efficiency, i.e., their optimality is not guaranteed unless
sample size surpasses a high threshold. We address both open problems by
introducing a model-free algorithm that employs variance reduction and a novel
technique that switches the execution policy in a slow-yet-adaptive manner.
This is the first regret-optimal model-free algorithm in the discounted
setting, with the additional benefit of a low burn-in time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1&quot;&gt;Xiang Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Gen Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17421">
<title>FoPro-KD: Fourier Prompted Effective Knowledge Distillation for Long-Tailed Medical Image Recognition. (arXiv:2305.17421v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17421</link>
<description rdf:parseType="Literal">&lt;p&gt;Representational transfer from publicly available models is a promising
technique for improving medical image classification, especially in long-tailed
datasets with rare diseases. However, existing methods often overlook the
frequency-dependent behavior of these models, thereby limiting their
effectiveness in transferring representations and generalizations to rare
diseases. In this paper, we propose FoPro-KD, a novel framework that leverages
the power of frequency patterns learned from frozen pre-trained models to
enhance their transferability and compression, presenting a few unique
insights: 1) We demonstrate that leveraging representations from publicly
available pre-trained models can substantially improve performance,
specifically for rare classes, even when utilizing representations from a
smaller pre-trained model. 2) We observe that pre-trained models exhibit
frequency preferences, which we explore using our proposed Fourier Prompt
Generator (FPG), allowing us to manipulate specific frequencies in the input
image, enhancing the discriminative representational transfer. 3) By amplifying
or diminishing these frequencies in the input image, we enable Effective
Knowledge Distillation (EKD). EKD facilitates the transfer of knowledge from
pre-trained models to smaller models. Through extensive experiments in
long-tailed gastrointestinal image recognition and skin lesion classification,
where rare diseases are prevalent, our FoPro-KD framework outperforms existing
methods, enabling more accessible medical models for rare disease
classification. Code is available at https://github.com/xmed-lab/FoPro-KD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Elbatel_M/0/1/0/all/0/1&quot;&gt;Marawan Elbatel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Marti_R/0/1/0/all/0/1&quot;&gt;Robert Mart&amp;#xed;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaomeng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19302">
<title>Smooth, exact rotational symmetrization for deep learning on point clouds. (arXiv:2305.19302v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19302</link>
<description rdf:parseType="Literal">&lt;p&gt;Point clouds are versatile representations of 3D objects and have found
widespread application in science and engineering. Many successful
deep-learning models have been proposed that use them as input. The domain of
chemical and materials modeling is especially challenging because exact
compliance with physical constraints is highly desirable for a model to be
usable in practice. These constraints include smoothness and invariance with
respect to translations, rotations, and permutations of identical atoms. If
these requirements are not rigorously fulfilled, atomistic simulations might
lead to absurd outcomes even if the model has excellent accuracy. Consequently,
dedicated architectures, which achieve invariance by restricting their design
space, have been developed. General-purpose point-cloud models are more varied
but often disregard rotational symmetry. We propose a general symmetrization
method that adds rotational equivariance to any given model while preserving
all the other requirements. Our approach simplifies the development of better
atomic-scale ML schemes by relaxing the constraints on the design space and
making it possible to incorporate ideas that proved effective in other domains.
We demonstrate this idea by introducing the Point Edge Transformer (PET)
architecture, which is not intrinsically equivariant but achieves
state-of-the-art performance on several benchmark datasets of molecules and
solids. A-posteriori application of our general protocol makes PET exactly
equivariant, with minimal changes to its accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pozdnyakov_S/0/1/0/all/0/1&quot;&gt;Sergey N. Pozdnyakov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ceriotti_M/0/1/0/all/0/1&quot;&gt;Michele Ceriotti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.20009">
<title>Protein Design with Guided Discrete Diffusion. (arXiv:2305.20009v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.20009</link>
<description rdf:parseType="Literal">&lt;p&gt;A popular approach to protein design is to combine a generative model with a
discriminative model for conditional sampling. The generative model samples
plausible sequences while the discriminative model guides a search for
sequences with high fitness. Given its broad success in conditional sampling,
classifier-guided diffusion modeling is a promising foundation for protein
design, leading many to develop guided diffusion models for structure with
inverse folding to recover sequences. In this work, we propose diffusioN
Optimized Sampling (NOS), a guidance method for discrete diffusion models that
follows gradients in the hidden states of the denoising network. NOS makes it
possible to perform design directly in sequence space, circumventing
significant limitations of structure-based methods, including scarce data and
challenging inverse design. Moreover, we use NOS to generalize LaMBO, a
Bayesian optimization procedure for sequence design that facilitates multiple
objectives and edit-based constraints. The resulting method, LaMBO-2, enables
discrete diffusions and stronger performance with limited edits through a novel
application of saliency maps. We apply LaMBO-2 to a real-world protein design
task, optimizing antibodies for higher expression yield and binding affinity to
several therapeutic targets under locality and developability constraints,
attaining a 99% expression rate and 40% binding rate in exploratory in vitro
experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gruver_N/0/1/0/all/0/1&quot;&gt;Nate Gruver&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stanton_S/0/1/0/all/0/1&quot;&gt;Samuel Stanton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frey_N/0/1/0/all/0/1&quot;&gt;Nathan C. Frey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rudner_T/0/1/0/all/0/1&quot;&gt;Tim G. J. Rudner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hotzel_I/0/1/0/all/0/1&quot;&gt;Isidro Hotzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lafrance_Vanasse_J/0/1/0/all/0/1&quot;&gt;Julien Lafrance-Vanasse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajpal_A/0/1/0/all/0/1&quot;&gt;Arvind Rajpal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1&quot;&gt;Kyunghyun Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1&quot;&gt;Andrew Gordon Wilson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.03241">
<title>Early Weight Averaging meets High Learning Rates for LLM Pre-training. (arXiv:2306.03241v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.03241</link>
<description rdf:parseType="Literal">&lt;p&gt;Training Large Language Models (LLMs) incurs significant cost; hence, any
strategy that accelerates model convergence is helpful. In this paper, we
investigate the ability of a simple idea checkpoint averaging along the
trajectory of a training run to improve both convergence and generalization
quite early on during training. Here we show that models trained with high
learning rates observe higher gains due to checkpoint averaging. Furthermore,
these gains are amplified when checkpoints are sampled with considerable
spacing in training steps. Our training recipe outperforms conventional
training and popular checkpoint averaging baselines such as exponential moving
average (EMA) and stochastic moving average (SWA). We evaluate our training
recipe by pre-training LLMs, where high learning rates are inherently preferred
due to extremely large batch sizes. Specifically, we pre-trained nanoGPT-2
models of varying sizes, small (125M), medium (335M), and large (770M)on the
OpenWebText dataset, comprised of 9B tokens. Additionally, we present results
for publicly available Pythia LLMs, ranging from 1B to 12B, which were trained
on the PILE-deduped dataset containing 207B tokens.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanyal_S/0/1/0/all/0/1&quot;&gt;Sunny Sanyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neerkaje_A/0/1/0/all/0/1&quot;&gt;Atula Neerkaje&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaddour_J/0/1/0/all/0/1&quot;&gt;Jean Kaddour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Abhishek Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanghavi_S/0/1/0/all/0/1&quot;&gt;Sujay Sanghavi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06871">
<title>Improving Offline-to-Online Reinforcement Learning with Q-Ensembles. (arXiv:2306.06871v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06871</link>
<description rdf:parseType="Literal">&lt;p&gt;Offline reinforcement learning (RL) is a learning paradigm where an agent
learns from a fixed dataset of experience. However, learning solely from a
static dataset can limit the performance due to the lack of exploration. To
overcome it, offline-to-online RL combines offline pre-training with online
fine-tuning, which enables the agent to further refine its policy by
interacting with the environment in real-time. Despite its benefits, existing
offline-to-online RL methods suffer from performance degradation and slow
improvement during the online phase. To tackle these challenges, we propose a
novel framework called Ensemble-based Offline-to-Online (E2O) RL. By increasing
the number of Q-networks, we seamlessly bridge offline pre-training and online
fine-tuning without degrading performance. Moreover, to expedite online
performance enhancement, we appropriately loosen the pessimism of Q-value
estimation and incorporate ensemble-based exploration mechanisms into our
framework. Experimental results demonstrate that E2O can substantially improve
the training stability, learning efficiency, and final performance of existing
offline RL methods during online fine-tuning on a range of locomotion and
navigation tasks, significantly outperforming existing offline-to-online RL
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1&quot;&gt;Kai Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yi Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1&quot;&gt;Jianye Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jinyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1&quot;&gt;Zhaopeng Meng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09124">
<title>DIFFender: Diffusion-Based Adversarial Defense against Patch Attacks. (arXiv:2306.09124v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09124</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial attacks, particularly patch attacks, pose significant threats to
the robustness and reliability of deep learning models. Developing reliable
defenses against patch attacks is crucial for real-world applications, yet
current research in this area is unsatisfactory. In this paper, we propose
DIFFender, a novel defense method that leverages a text-guided diffusion model
to defend against adversarial patches. DIFFender includes two main stages:
patch localization and patch restoration. In the localization stage, we find
and exploit an intriguing property of the diffusion model to precisely identify
the locations of adversarial patches. In the restoration stage, we employ the
diffusion model to reconstruct the adversarial regions in the images while
preserving the integrity of the visual content. Thanks to the former finding,
these two stages can be simultaneously guided by a unified diffusion model.
Thus, we can utilize the close interaction between them to improve the whole
defense performance. Moreover, we propose a few-shot prompt-tuning algorithm to
fine-tune the diffusion model, enabling the pre-trained diffusion model to
adapt to the defense task easily. We conduct extensive experiments on image
classification, face recognition, and further in the physical world,
demonstrating that our proposed method exhibits superior robustness under
strong adaptive attacks and generalizes well across various scenarios, diverse
classifiers, and multiple patch attack methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_C/0/1/0/all/0/1&quot;&gt;Caixin Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yinpeng Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhengyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruan_S/0/1/0/all/0/1&quot;&gt;Shouwei Ruan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yubo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hang Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xingxing Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10711">
<title>PLASTIC: Improving Input and Label Plasticity for Sample Efficient Reinforcement Learning. (arXiv:2306.10711v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10711</link>
<description rdf:parseType="Literal">&lt;p&gt;In Reinforcement Learning (RL), enhancing sample efficiency is crucial,
particularly in scenarios when data acquisition is costly and risky. In
principle, off-policy RL algorithms can improve sample efficiency by allowing
multiple updates per environment interaction. However, these multiple updates
often lead the model to overfit to earlier interactions, which is referred to
as the loss of plasticity. Our study investigates the underlying causes of this
phenomenon by dividing plasticity into two aspects. Input plasticity, which
denotes the model&apos;s adaptability to changing input data, and label plasticity,
which denotes the model&apos;s adaptability to evolving input-output relationships.
Synthetic experiments on the CIFAR-10 dataset reveal that finding smoother
minima of loss landscape enhances input plasticity, whereas refined gradient
propagation improves label plasticity. Leveraging these findings, we introduce
the PLASTIC algorithm, which harmoniously combines techniques to address both
concerns. With minimal architectural modifications, PLASTIC achieves
competitive performance on benchmarks including Atari-100k and Deepmind Control
Suite. This result emphasizes the importance of preserving the model&apos;s
plasticity to elevate the sample efficiency in RL. The code is available at
https://github.com/dojeon-ai/plastic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hojoon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1&quot;&gt;Hanseul Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyunseung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gwak_D/0/1/0/all/0/1&quot;&gt;Daehoon Gwak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Joonkee Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1&quot;&gt;Jaegul Choo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1&quot;&gt;Se-Young Yun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_C/0/1/0/all/0/1&quot;&gt;Chulhee Yun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14284">
<title>Learning Broadcast Protocols. (arXiv:2306.14284v2 [cs.FL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14284</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of learning a computational model from examples has been
receiving growing attention. For the particularly challenging problem of
learning models of distributed systems, existing results are restricted to
models with a fixed number of interacting processes. In this work we look for
the first time (to the best of our knowledge) at the problem of learning a
distributed system with an arbitrary number of processes, assuming only that
there exists a cutoff, i.e., a number of processes that is sufficient to
produce all observable behaviors. Specifically, we consider fine broadcast
protocols, these are broadcast protocols (BPs) with a finite cutoff and no
hidden states. We provide a learning algorithm that can infer a correct BP from
a sample that is consistent with a fine BP, and a minimal equivalent BP if the
sample is sufficiently complete. On the negative side we show that (a)
characteristic sets of exponential size are unavoidable, (b) the consistency
problem for fine BPs is NP hard, and (c) that fine BPs are not polynomially
predictable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fisman_D/0/1/0/all/0/1&quot;&gt;Dana Fisman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Izsak_N/0/1/0/all/0/1&quot;&gt;Noa Izsak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacobs_S/0/1/0/all/0/1&quot;&gt;Swen Jacobs&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14435">
<title>DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing. (arXiv:2306.14435v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14435</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate and controllable image editing is a challenging task that has
attracted significant attention recently. Notably, DragGAN is an interactive
point-based image editing framework that achieves impressive editing results
with pixel-level precision. However, due to its reliance on generative
adversarial networks (GANs), its generality is limited by the capacity of
pretrained GAN models. In this work, we extend this editing framework to
diffusion models and propose a novel approach DragDiffusion. By harnessing
large-scale pretrained diffusion models, we greatly enhance the applicability
of interactive point-based editing on both real and diffusion-generated images.
Our approach involves optimizing the diffusion latents to achieve precise
spatial control. The supervision signal of this optimization process is from
the diffusion model&apos;s UNet features, which are known to contain rich semantic
and geometric information. Moreover, we introduce two additional techniques,
namely LoRA fine-tuning and latent-MasaCtrl, to further preserve the identity
of the original image. Lastly, we present a challenging benchmark dataset
called DragBench -- the first benchmark to evaluate the performance of
interactive point-based image editing methods. Experiments across a wide range
of challenging cases (e.g., images with multiple objects, diverse object
categories, various styles, etc.) demonstrate the versatility and generality of
DragDiffusion. Code: https://github.com/Yujun-Shi/DragDiffusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yujun Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_C/0/1/0/all/0/1&quot;&gt;Chuhui Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liew_J/0/1/0/all/0/1&quot;&gt;Jun Hao Liew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Jiachun Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1&quot;&gt;Hanshu Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1&quot;&gt;Vincent Y. F. Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1&quot;&gt;Song Bai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05845">
<title>PIGEON: Predicting Image Geolocations. (arXiv:2307.05845v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05845</link>
<description rdf:parseType="Literal">&lt;p&gt;Planet-scale image geolocalization remains a challenging problem due to the
diversity of images originating from anywhere in the world. Although approaches
based on vision transformers have made significant progress in geolocalization
accuracy, success in prior literature is constrained to narrow distributions of
images of landmarks, and performance has not generalized to unseen places. We
present a new geolocalization system that combines semantic geocell creation,
multi-task contrastive pretraining, and a novel loss function. Additionally,
our work is the first to perform retrieval over location clusters for guess
refinements. We train two models for evaluations on street-level data and
general-purpose image geolocalization; the first model, PIGEON, is trained on
data from the game of Geoguessr and is capable of placing over 40% of its
guesses within 25 kilometers of the target location globally. We also develop a
bot and deploy PIGEON in a blind experiment against humans, ranking in the top
0.01% of players. We further challenge one of the world&apos;s foremost professional
Geoguessr players to a series of six matches with millions of viewers, winning
all six games. Our second model, PIGEOTTO, differs in that it is trained on a
dataset of images from Flickr and Wikipedia, achieving state-of-the-art results
on a wide range of image geolocalization benchmarks, outperforming the previous
SOTA by up to 7.7 percentage points on the city accuracy level and up to 38.8
percentage points on the country level. Our findings suggest that PIGEOTTO is
the first image geolocalization model that effectively generalizes to unseen
places and that our approach can pave the way for highly accurate, planet-scale
image geolocalization systems. Our code is available on GitHub.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haas_L/0/1/0/all/0/1&quot;&gt;Lukas Haas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skreta_M/0/1/0/all/0/1&quot;&gt;Michal Skreta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alberti_S/0/1/0/all/0/1&quot;&gt;Silas Alberti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1&quot;&gt;Chelsea Finn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08131">
<title>INFLECT-DGNN: Influencer Prediction with Dynamic Graph Neural Networks. (arXiv:2307.08131v3 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08131</link>
<description rdf:parseType="Literal">&lt;p&gt;Leveraging network information for predictive modeling has become widespread
in many domains. Within the realm of referral and targeted marketing,
influencer detection stands out as an area that could greatly benefit from the
incorporation of dynamic network representation due to the ongoing development
of customer-brand relationships. To elaborate this idea, we introduce
INFLECT-DGNN, a new framework for INFLuencer prEdiCTion with Dynamic Graph
Neural Networks that combines Graph Neural Networks (GNN) and Recurrent Neural
Networks (RNN) with weighted loss functions, the Synthetic Minority
Oversampling TEchnique (SMOTE) adapted for graph data, and a carefully crafted
rolling-window strategy. To evaluate predictive performance, we utilize a
unique corporate data set with networks of three cities and derive a
profit-driven evaluation methodology for influencer prediction. Our results
show how using RNN to encode temporal attributes alongside GNNs significantly
improves predictive performance. We compare the results of various models to
demonstrate the importance of capturing graph representation, temporal
dependencies, and using a profit-driven methodology for evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tiukhova_E/0/1/0/all/0/1&quot;&gt;Elena Tiukhova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Penaloza_E/0/1/0/all/0/1&quot;&gt;Emiliano Penaloza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oskarsdottir_M/0/1/0/all/0/1&quot;&gt;Mar&amp;#xed;a &amp;#xd3;skarsd&amp;#xf3;ttir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baesens_B/0/1/0/all/0/1&quot;&gt;Bart Baesens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Snoeck_M/0/1/0/all/0/1&quot;&gt;Monique Snoeck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bravo_C/0/1/0/all/0/1&quot;&gt;Cristi&amp;#xe1;n Bravo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.16164">
<title>Adaptive learning of density ratios in RKHS. (arXiv:2307.16164v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.16164</link>
<description rdf:parseType="Literal">&lt;p&gt;Estimating the ratio of two probability densities from finitely many
observations of the densities is a central problem in machine learning and
statistics with applications in two-sample testing, divergence estimation,
generative modeling, covariate shift adaptation, conditional density
estimation, and novelty detection. In this work, we analyze a large class of
density ratio estimation methods that minimize a regularized Bregman divergence
between the true density ratio and a model in a reproducing kernel Hilbert
space (RKHS). We derive new finite-sample error bounds, and we propose a
Lepskii type parameter choice principle that minimizes the bounds without
knowledge of the regularity of the density ratio. In the special case of
quadratic loss, our method adaptively achieves a minimax optimal error rate. A
numerical illustration is provided.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zellinger_W/0/1/0/all/0/1&quot;&gt;Werner Zellinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kindermann_S/0/1/0/all/0/1&quot;&gt;Stefan Kindermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pereverzyev_S/0/1/0/all/0/1&quot;&gt;Sergei V. Pereverzyev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03743">
<title>The Copycat Perceptron: Smashing Barriers Through Collective Learning. (arXiv:2308.03743v2 [cond-mat.dis-nn] UPDATED)</title>
<link>http://arxiv.org/abs/2308.03743</link>
<description rdf:parseType="Literal">&lt;p&gt;We characterize the equilibrium properties of a model of $y$ coupled binary
perceptrons in the teacher-student scenario, subject to a learning rule, with
an explicit ferromagnetic coupling proportional to the Hamming distance between
the students&apos; weights. In contrast to recent works, we analyze a more general
setting in which thermal noise is present that affects each student&apos;s
generalization performance. In the nonzero temperature regime, we find that the
coupling of replicas produces a bend of the phase diagram towards smaller
values of $\alpha$: This suggests that the free energy landscape gets smoother
around the solution with perfect generalization (i.e., the teacher&apos;s) at a
fixed fraction of examples, allowing standard thermal updates such as Simulated
Annealing to easily reach the teacher solution and avoid entrapment in
metastable states as it happens in the unreplicated case, even in the so-called
computationally easy regime. These results provide additional analytic and
numerical evidence for the recently conjectured Bayes-optimal property of
Replicated Simulated Annealing (RSA) for a sufficient number of replicas. From
a learning perspective, these results also suggest that multiple students
working together (in this case reviewing the same data) are able to learn the
same rule both significantly faster and with fewer examples, a property that
could be exploited in the context of cooperative and federated learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Catania_G/0/1/0/all/0/1&quot;&gt;Giovanni Catania&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Decelle_A/0/1/0/all/0/1&quot;&gt;Aur&amp;#xe9;lien Decelle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Seoane_B/0/1/0/all/0/1&quot;&gt;Beatriz Seoane&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04669">
<title>A General Implicit Framework for Fast NeRF Composition and Rendering. (arXiv:2308.04669v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.04669</link>
<description rdf:parseType="Literal">&lt;p&gt;A variety of Neural Radiance Fields (NeRF) methods have recently achieved
remarkable success in high render speed. However, current accelerating methods
are specialized and incompatible with various implicit methods, preventing
real-time composition over various types of NeRF works. Because NeRF relies on
sampling along rays, it is possible to provide general guidance for
acceleration. To that end, we propose a general implicit pipeline for composing
NeRF objects quickly. Our method enables the casting of dynamic shadows within
or between objects using analytical light sources while allowing multiple NeRF
objects to be seamlessly placed and rendered together with any arbitrary rigid
transformations. Mainly, our work introduces a new surface representation known
as Neural Depth Fields (NeDF) that quickly determines the spatial relationship
between objects by allowing direct intersection computation between rays and
implicit surfaces. It leverages an intersection neural network to query NeRF
for acceleration instead of depending on an explicit spatial structure.Our
proposed method is the first to enable both the progressive and interactive
composition of NeRF objects. Additionally, it also serves as a previewing
plugin for a range of existing NeRF works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xinyu Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Ziyi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yunlu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yuxiang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1&quot;&gt;Xiaogang Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_C/0/1/0/all/0/1&quot;&gt;Changqing Zou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.08641">
<title>Non-monotone Sequential Submodular Maximization. (arXiv:2308.08641v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.08641</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study a fundamental problem in submodular optimization,
which is called sequential submodular maximization. Specifically, we aim to
select and rank a group of $k$ items from a ground set $V$ such that the
weighted summation of $k$ (possibly non-monotone) submodular functions $f_1,
\cdots ,f_k: 2^V \rightarrow \mathbb{R}^+$ is maximized, here each function
$f_j$ takes the first $j$ items from this sequence as input. The existing
research on sequential submodular maximization has predominantly concentrated
on the monotone setting, assuming that the submodular functions are
non-decreasing. However, in various real-world scenarios, like diversity-aware
recommendation systems, adding items to an existing set might negatively impact
the overall utility. In response, this paper pioneers the examination of the
aforementioned problem with non-monotone submodular functions and offers
effective solutions for both flexible and fixed length constraints, as well as
a special case with identical utility functions. The empirical evaluations
further validate the effectiveness of our proposed algorithms in the domain of
video recommendations. The results of this research have implications in
various fields, including recommendation systems and assortment optimization,
where the ordering of items significantly impacts the overall value obtained.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Shaojie Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1&quot;&gt;Jing Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.09604">
<title>Faster Stochastic Variance Reduction Methods for Compositional MiniMax Optimization. (arXiv:2308.09604v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.09604</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper delves into the realm of stochastic optimization for compositional
minimax optimization - a pivotal challenge across various machine learning
domains, including deep AUC and reinforcement learning policy evaluation.
Despite its significance, the problem of compositional minimax optimization is
still under-explored. Adding to the complexity, current methods of
compositional minimax optimization are plagued by sub-optimal complexities or
heavy reliance on sizable batch sizes. To respond to these constraints, this
paper introduces a novel method, called Nested STOchastic Recursive Momentum
(NSTORM), which can achieve the optimal sample complexity of $O(\kappa^3
/\epsilon^3 )$ to obtain the $\epsilon$-accuracy solution. We also demonstrate
that NSTORM can achieve the same sample complexity under the Polyak-\L
ojasiewicz (PL)-condition - an insightful extension of its capabilities. Yet,
NSTORM encounters an issue with its requirement for low learning rates,
potentially constraining its real-world applicability in machine learning. To
overcome this hurdle, we present ADAptive NSTORM (ADA-NSTORM) with adaptive
learning rates. We demonstrate that ADA-NSTORM can achieve the same sample
complexity but the experimental results show its more effectiveness. All the
proposed complexities indicate that our proposed methods can match lower bounds
to existing minimax optimizations, without requiring a large batch size in each
iteration. Extensive experiments support the efficiency of our proposed
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1&quot;&gt;Xiaokang Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1&quot;&gt;Junwen Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongdong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Youqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_Z/0/1/0/all/0/1&quot;&gt;Zhe Qu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.09895">
<title>Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs. (arXiv:2308.09895v4 [cs.PL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.09895</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the past few years, Large Language Models of Code (Code LLMs) have
started to have a significant impact on programming practice. Code LLMs are
also emerging as building blocks for research in programming languages and
software engineering. However, Code LLMs produce impressive results on
programming languages that are well represented in their training data (e.g.,
Java, Python, or JavaScript), but struggle with low-resource languages that
have limited training data available. Low resource languages include OCaml,
Racket, and several others.
&lt;/p&gt;
&lt;p&gt;This paper presents an effective approach for boosting the performance of
Code LLMs on low-resource languages using semi-synthetic data. Our approach,
MultiPL-T, translates training data from high-resource languages into training
data for low-resource languages in the following way. 1) We use a Code LLM to
synthesize tests for commented code from a high-resource language, filtering
out faulty tests and code with low test coverage. 2) We use a Code LLM to
translate Python code to a target low-resource language, and use tests to
validate the translation. We apply this approach to generate tens of thousands
of validated training items for Julia, Lua, OCaml, R, and Racket. Furthermore,
we use an open model (StarCoderBase) with open training data (The Stack), which
allows us to decontaminate benchmarks, train models without violating licenses,
and run experiments that could not otherwise be done.
&lt;/p&gt;
&lt;p&gt;With MultiPL-T generated data, we present fine-tuned versions of
StarCoderBase and Code Llama for Julia, Lua, OCaml, R, and Racket. On
established benchmarks (MultiPL-E), these models outperform other open Code
LLMs. The MultiPL-T approach is easy to apply to new languages, and is
significantly more efficient and effective than alternatives such as training
longer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cassano_F/0/1/0/all/0/1&quot;&gt;Federico Cassano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gouwar_J/0/1/0/all/0/1&quot;&gt;John Gouwar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucchetti_F/0/1/0/all/0/1&quot;&gt;Francesca Lucchetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlesinger_C/0/1/0/all/0/1&quot;&gt;Claire Schlesinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anderson_C/0/1/0/all/0/1&quot;&gt;Carolyn Jane Anderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Greenberg_M/0/1/0/all/0/1&quot;&gt;Michael Greenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jangda_A/0/1/0/all/0/1&quot;&gt;Abhinav Jangda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guha_A/0/1/0/all/0/1&quot;&gt;Arjun Guha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12696">
<title>Disentanglement Learning via Topology. (arXiv:2308.12696v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12696</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose TopDis (Topological Disentanglement), a method for learning
disentangled representations via adding multi-scale topological loss term.
Disentanglement is a crucial property of data representations substantial for
the explainability and robustness of deep learning models and a step towards
high-level cognition. The state-of-the-art method based on VAE minimizes the
total correlation of the joint distribution of latent variables. We take a
different perspective on disentanglement by analyzing topological properties of
data manifolds. In particular, we optimize the topological similarity for data
manifolds traversals. To the best of our knowledge, our paper is the first one
to propose a differentiable topological loss for disentanglement. Our
experiments have shown that the proposed topological loss improves
disentanglement scores such as MIG, FactorVAE score, SAP score and DCI
disentanglement score with respect to state-of-the-art results. Our method
works in an unsupervised manner, permitting to apply it for problems without
labeled factors of variation. Additionally, we show how to use the proposed
topological loss to find disentangled directions in a trained GAN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balabin_N/0/1/0/all/0/1&quot;&gt;Nikita Balabin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Voronkova_D/0/1/0/all/0/1&quot;&gt;Daria Voronkova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trofimov_I/0/1/0/all/0/1&quot;&gt;Ilya Trofimov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1&quot;&gt;Evgeny Burnaev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barannikov_S/0/1/0/all/0/1&quot;&gt;Serguei Barannikov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.14815">
<title>Distributionally Robust Statistical Verification with Imprecise Neural Networks. (arXiv:2308.14815v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.14815</link>
<description rdf:parseType="Literal">&lt;p&gt;A particularly challenging problem in AI safety is providing guarantees on
the behavior of high-dimensional autonomous systems. Verification approaches
centered around reachability analysis fail to scale, and purely statistical
approaches are constrained by the distributional assumptions about the sampling
process. Instead, we pose a distributionally robust version of the statistical
verification problem for black-box systems, where our performance guarantees
hold over a large family of distributions. This paper proposes a novel approach
based on a combination of active learning, uncertainty quantification, and
neural network verification. A central piece of our approach is an ensemble
technique called Imprecise Neural Networks, which provides the uncertainty to
guide active learning. The active learning uses an exhaustive neural-network
verification tool Sherlock to collect samples. An evaluation on multiple
physical simulators in the openAI gym Mujoco environments with
reinforcement-learned controllers demonstrates that our approach can provide
useful and scalable guarantees for high-dimensional systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1&quot;&gt;Souradeep Dutta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caprio_M/0/1/0/all/0/1&quot;&gt;Michele Caprio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_V/0/1/0/all/0/1&quot;&gt;Vivian Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cleaveland_M/0/1/0/all/0/1&quot;&gt;Matthew Cleaveland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_K/0/1/0/all/0/1&quot;&gt;Kuk Jin Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruchkin_I/0/1/0/all/0/1&quot;&gt;Ivan Ruchkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sokolsky_O/0/1/0/all/0/1&quot;&gt;Oleg Sokolsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1&quot;&gt;Insup Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16139">
<title>MedShapeNet -- A Large-Scale Dataset of 3D Medical Shapes for Computer Vision. (arXiv:2308.16139v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.16139</link>
<description rdf:parseType="Literal">&lt;p&gt;Prior to the deep learning era, shape was commonly used to describe the
objects. Nowadays, state-of-the-art (SOTA) algorithms in medical imaging are
predominantly diverging from computer vision, where voxel grids, meshes, point
clouds, and implicit surface models are used. This is seen from numerous
shape-related publications in premier vision conferences as well as the growing
popularity of ShapeNet (about 51,300 models) and Princeton ModelNet (127,915
models). For the medical domain, we present a large collection of anatomical
shapes (e.g., bones, organs, vessels) and 3D models of surgical instrument,
called MedShapeNet, created to facilitate the translation of data-driven vision
algorithms to medical applications and to adapt SOTA vision algorithms to
medical problems. As a unique feature, we directly model the majority of shapes
on the imaging data of real patients. As of today, MedShapeNet includes 23
dataset with more than 100,000 shapes that are paired with annotations (ground
truth). Our data is freely accessible via a web interface and a Python
application programming interface (API) and can be used for discriminative,
reconstructive, and variational benchmarks as well as various applications in
virtual, augmented, or mixed reality, and 3D printing. Exemplary, we present
use cases in the fields of classification of brain tumors, facial and skull
reconstructions, multi-class anatomy completion, education, and 3D printing. In
future, we will extend the data and improve the interfaces. The project pages
are: https://medshapenet.ikim.nrw/ and
https://github.com/Jianningli/medshapenet-feedback
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianning Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zongwei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jiancheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pepe_A/0/1/0/all/0/1&quot;&gt;Antonio Pepe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gsaxner_C/0/1/0/all/0/1&quot;&gt;Christina Gsaxner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luijten_G/0/1/0/all/0/1&quot;&gt;Gijs Luijten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_C/0/1/0/all/0/1&quot;&gt;Chongyu Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tiezheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaoxi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenxuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wodzinski_M/0/1/0/all/0/1&quot;&gt;Marek Wodzinski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Friedrich_P/0/1/0/all/0/1&quot;&gt;Paul Friedrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_K/0/1/0/all/0/1&quot;&gt;Kangxian Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yuan Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ambigapathy_N/0/1/0/all/0/1&quot;&gt;Narmada Ambigapathy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nasca_E/0/1/0/all/0/1&quot;&gt;Enrico Nasca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solak_N/0/1/0/all/0/1&quot;&gt;Naida Solak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melito_G/0/1/0/all/0/1&quot;&gt;Gian Marco Melito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vu_V/0/1/0/all/0/1&quot;&gt;Viet Duc Vu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Memon_A/0/1/0/all/0/1&quot;&gt;Afaque R. Memon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlachta_C/0/1/0/all/0/1&quot;&gt;Christopher Schlachta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribaupierre_S/0/1/0/all/0/1&quot;&gt;Sandrine De Ribaupierre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_R/0/1/0/all/0/1&quot;&gt;Rajnikant Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eagleson_R/0/1/0/all/0/1&quot;&gt;Roy Eagleson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaojun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Machler_H/0/1/0/all/0/1&quot;&gt;Heinrich M&amp;#xe4;chler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirschke_J/0/1/0/all/0/1&quot;&gt;Jan Stefan Kirschke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosa_E/0/1/0/all/0/1&quot;&gt;Ezequiel de la Rosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christ_P/0/1/0/all/0/1&quot;&gt;Patrick Ferdinand Christ&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongwei Bran Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ellis_D/0/1/0/all/0/1&quot;&gt;David G. Ellis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aizenberg_M/0/1/0/all/0/1&quot;&gt;Michele R. Aizenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gatidis_S/0/1/0/all/0/1&quot;&gt;Sergios Gatidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kustner_T/0/1/0/all/0/1&quot;&gt;Thomas K&amp;#xfc;stner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shusharina_N/0/1/0/all/0/1&quot;&gt;Nadya Shusharina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heller_N/0/1/0/all/0/1&quot;&gt;Nicholas Heller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andrearczyk_V/0/1/0/all/0/1&quot;&gt;Vincent Andrearczyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Depeursinge_A/0/1/0/all/0/1&quot;&gt;Adrien Depeursinge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hatt_M/0/1/0/all/0/1&quot;&gt;Mathieu Hatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sekuboyina_A/0/1/0/all/0/1&quot;&gt;Anjany Sekuboyina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loffler_M/0/1/0/all/0/1&quot;&gt;Maximilian L&amp;#xf6;ffler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liebl_H/0/1/0/all/0/1&quot;&gt;Hans Liebl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dorent_R/0/1/0/all/0/1&quot;&gt;Reuben Dorent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vercauteren_T/0/1/0/all/0/1&quot;&gt;Tom Vercauteren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shapey_J/0/1/0/all/0/1&quot;&gt;Jonathan Shapey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kujawa_A/0/1/0/all/0/1&quot;&gt;Aaron Kujawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cornelissen_S/0/1/0/all/0/1&quot;&gt;Stefan Cornelissen&lt;/a&gt;, et al. (110 additional authors not shown)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03648">
<title>Promoting Fairness in GNNs: A Characterization of Stability. (arXiv:2309.03648v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.03648</link>
<description rdf:parseType="Literal">&lt;p&gt;The Lipschitz bound, a technique from robust statistics, can limit the
maximum changes in the output concerning the input, taking into account
associated irrelevant biased factors. It is an efficient and provable method
for examining the output stability of machine learning models without incurring
additional computation costs. Recently, Graph Neural Networks (GNNs), which
operate on non-Euclidean data, have gained significant attention. However, no
previous research has investigated the GNN Lipschitz bounds to shed light on
stabilizing model outputs, especially when working on non-Euclidean data with
inherent biases. Given the inherent biases in common graph data used for GNN
training, it poses a serious challenge to constraining the GNN output
perturbations induced by input biases, thereby safeguarding fairness during
training. Recently, despite the Lipschitz constant&apos;s use in controlling the
stability of Euclideanneural networks, the calculation of the precise Lipschitz
constant remains elusive for non-Euclidean neural networks like GNNs,
especially within fairness contexts. To narrow this gap, we begin with the
general GNNs operating on an attributed graph, and formulate a Lipschitz bound
to limit the changes in the output regarding biases associated with the input.
Additionally, we theoretically analyze how the Lipschitz constant of a GNN
model could constrain the output perturbations induced by biases learned from
data for fairness training. We experimentally validate the Lipschitz bound&apos;s
effectiveness in limiting biases of the model output. Finally, from a training
dynamics perspective, we demonstrate why the theoretical Lipschitz bound can
effectively guide the GNN training to better trade-off between accuracy and
fairness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1&quot;&gt;Yaning Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chunhui Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13788">
<title>Can LLM-Generated Misinformation Be Detected?. (arXiv:2309.13788v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.13788</link>
<description rdf:parseType="Literal">&lt;p&gt;The advent of Large Language Models (LLMs) has made a transformative impact.
However, the potential that LLMs such as ChatGPT can be exploited to generate
misinformation has posed a serious concern to online safety and public trust. A
fundamental research question is: will LLM-generated misinformation cause more
harm than human-written misinformation? We propose to tackle this question from
the perspective of detection difficulty. We first build a taxonomy of
LLM-generated misinformation. Then we categorize and validate the potential
real-world methods for generating misinformation with LLMs. Then, through
extensive empirical investigation, we discover that LLM-generated
misinformation can be harder to detect for humans and detectors compared to
human-written misinformation with the same semantics, which suggests it can
have more deceptive styles and potentially cause more harm. We also discuss the
implications of our discovery on combating misinformation in the age of LLMs
and the countermeasures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Canyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_K/0/1/0/all/0/1&quot;&gt;Kai Shu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14585">
<title>DifAttack: Query-Efficient Black-Box Attack via Disentangled Feature Space. (arXiv:2309.14585v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14585</link>
<description rdf:parseType="Literal">&lt;p&gt;This work investigates efficient score-based black-box adversarial attacks
with a high Attack Success Rate (ASR) and good generalizability. We design a
novel attack method based on a Disentangled Feature space, called DifAttack,
which differs significantly from the existing ones operating over the entire
feature space. Specifically, DifAttack firstly disentangles an image&apos;s latent
feature into an adversarial feature and a visual feature, where the former
dominates the adversarial capability of an image, while the latter largely
determines its visual appearance. We train an autoencoder for the
disentanglement by using pairs of clean images and their Adversarial Examples
(AEs) generated from available surrogate models via white-box attack methods.
Eventually, DifAttack iteratively optimizes the adversarial feature according
to the query feedback from the victim model until a successful AE is generated,
while keeping the visual feature unaltered. In addition, due to the avoidance
of using surrogate models&apos; gradient information when optimizing AEs for
black-box models, our proposed DifAttack inherently possesses better attack
capability in the open-set scenario, where the training dataset of the victim
model is unknown. Extensive experimental results demonstrate that our method
achieves significant improvements in ASR and query efficiency simultaneously,
especially in the targeted attack and open-set scenarios. The code will be
available at https://github.com/csjunjun/DifAttack.git soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jun_L/0/1/0/all/0/1&quot;&gt;Liu Jun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiantao_Z/0/1/0/all/0/1&quot;&gt;Zhou Jiantao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiandian_Z/0/1/0/all/0/1&quot;&gt;Zeng Jiandian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1&quot;&gt;Jinyu Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16512">
<title>From Complexity to Clarity: Analytical Expressions of Deep Neural Network Weights via Clifford&apos;s Geometric Algebra and Convexity. (arXiv:2309.16512v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.16512</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce a novel analysis of neural networks based on
geometric (Clifford) algebra and convex optimization. We show that optimal
weights of deep ReLU neural networks are given by the wedge product of training
samples when trained with standard regularized loss. Furthermore, the training
problem reduces to convex optimization over wedge product features, which
encode the geometric structure of the training dataset. This structure is given
in terms of signed volumes of triangles and parallelotopes generated by data
vectors. The convex problem finds a small subset of samples via $\ell_1$
regularization to discover only relevant wedge product features. Our analysis
provides a novel perspective on the inner workings of deep neural networks and
sheds light on the role of the hidden layers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pilanci_M/0/1/0/all/0/1&quot;&gt;Mert Pilanci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16808">
<title>Granularity at Scale: Estimating Neighborhood Socioeconomic Indicators from High-Resolution Orthographic Imagery and Hybrid Learning. (arXiv:2309.16808v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.16808</link>
<description rdf:parseType="Literal">&lt;p&gt;Many areas of the world are without basic information on the socioeconomic
well-being of the residing population due to limitations in existing data
collection methods. Overhead images obtained remotely, such as from satellite
or aircraft, can help serve as windows into the state of life on the ground and
help &quot;fill in the gaps&quot; where community information is sparse, with estimates
at smaller geographic scales requiring higher resolution sensors. Concurrent
with improved sensor resolutions, recent advancements in machine learning and
computer vision have made it possible to quickly extract features from and
detect patterns in image data, in the process correlating these features with
other information. In this work, we explore how well two approaches, a
supervised convolutional neural network and semi-supervised clustering based on
bag-of-visual-words, estimate population density, median household income, and
educational attainment of individual neighborhoods from publicly available
high-resolution imagery of cities throughout the United States. Results and
analyses indicate that features extracted from the imagery can accurately
estimate the density (R$^2$ up to 0.81) of neighborhoods, with the supervised
approach able to explain about half the variation in a population&apos;s income and
education. In addition to the presented approaches serving as a basis for
further geographic generalization, the novel semi-supervised approach provides
a foundation for future work seeking to estimate fine-scale information from
aerial imagery without the need for label data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brewer_E/0/1/0/all/0/1&quot;&gt;Ethan Brewer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valdrighi_G/0/1/0/all/0/1&quot;&gt;Giovani Valdrighi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solunke_P/0/1/0/all/0/1&quot;&gt;Parikshit Solunke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rulff_J/0/1/0/all/0/1&quot;&gt;Joao Rulff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piadyk_Y/0/1/0/all/0/1&quot;&gt;Yurii Piadyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_Z/0/1/0/all/0/1&quot;&gt;Zhonghui Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poco_J/0/1/0/all/0/1&quot;&gt;Jorge Poco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_C/0/1/0/all/0/1&quot;&gt;Claudio Silva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01678">
<title>Score dynamics: scaling molecular dynamics with picosecond timesteps via conditional diffusion model. (arXiv:2310.01678v2 [physics.comp-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01678</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose score dynamics, a general framework for learning accelerated
evolution operators with large timesteps from molecular-dynamics simulations.
SD is centered around scores, or derivatives of the transition log-probability
with respect to the dynamical degrees of freedom. The latter play the same role
as force fields in MD but are used in denoising diffusion probability models to
generate discrete transitions of the dynamical variables in an SD timestep,
which can be orders of magnitude larger than a typical MD timestep. In this
work, we construct graph neural network based score dynamics models of
realistic molecular systems that are evolved with 10 ps timesteps. We
demonstrate the efficacy of score dynamics with case studies of alanine
dipeptide and short alkanes in aqueous solution. Both equilibrium predictions
derived from the stationary distributions of the conditional probability and
kinetic predictions for the transition rates and transition paths are in good
agreement with MD. Our current SD implementation is about two orders of
magnitude faster than the MD counterpart for the systems studied in this work.
Open challenges and possible future remedies to improve score dynamics are also
discussed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Hsu_T/0/1/0/all/0/1&quot;&gt;Tim Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Sadigh_B/0/1/0/all/0/1&quot;&gt;Babak Sadigh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bulatov_V/0/1/0/all/0/1&quot;&gt;Vasily Bulatov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zhou_F/0/1/0/all/0/1&quot;&gt;Fei Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04816">
<title>Hacking Generative Models with Differentiable Network Bending. (arXiv:2310.04816v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04816</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we propose a method to &apos;hack&apos; generative models, pushing their
outputs away from the original training distribution towards a new objective.
We inject a small-scale trainable module between the intermediate layers of the
model and train it for a low number of iterations, keeping the rest of the
network frozen. The resulting output images display an uncanny quality, given
by the tension between the original and new objectives that can be exploited
for artistic purposes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aldegheri_G/0/1/0/all/0/1&quot;&gt;Giacomo Aldegheri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rogalska_A/0/1/0/all/0/1&quot;&gt;Alina Rogalska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Youssef_A/0/1/0/all/0/1&quot;&gt;Ahmed Youssef&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iofinova_E/0/1/0/all/0/1&quot;&gt;Eugenia Iofinova&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10773">
<title>Gotta be SAFE: A New Framework for Molecular Design. (arXiv:2310.10773v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10773</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional molecular string representations, such as SMILES, often pose
challenges for AI-driven molecular design due to their non-sequential depiction
of molecular substructures. To address this issue, we introduce Sequential
Attachment-based Fragment Embedding (SAFE), a novel line notation for chemical
structures. SAFE reimagines SMILES strings as an unordered sequence of
interconnected fragment blocks while maintaining compatibility with existing
SMILES parsers. It streamlines complex generative tasks, including scaffold
decoration, fragment linking, polymer generation, and scaffold hopping, while
facilitating autoregressive generation for fragment-constrained design, thereby
eliminating the need for intricate decoding or graph-based models. We
demonstrate the effectiveness of SAFE by training an 87-million-parameter
GPT2-like model on a dataset containing 1.1 billion SAFE representations.
Through targeted experimentation, we show that our SAFE-GPT model exhibits
versatile and robust optimization performance. SAFE opens up new avenues for
the rapid exploration of chemical space under various constraints, promising
breakthroughs in AI-driven molecular design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noutahi_E/0/1/0/all/0/1&quot;&gt;Emmanuel Noutahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gabellini_C/0/1/0/all/0/1&quot;&gt;Cristian Gabellini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Craig_M/0/1/0/all/0/1&quot;&gt;Michael Craig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1&quot;&gt;Jonathan S.C Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tossou_P/0/1/0/all/0/1&quot;&gt;Prudencio Tossou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10998">
<title>Accelerating Scalable Graph Neural Network Inference with Node-Adaptive Propagation. (arXiv:2310.10998v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10998</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph neural networks (GNNs) have exhibited exceptional efficacy in a diverse
array of applications. However, the sheer size of large-scale graphs presents a
significant challenge to real-time inference with GNNs. Although existing
Scalable GNNs leverage linear propagation to preprocess the features and
accelerate the training and inference procedure, these methods still suffer
from scalability issues when making inferences on unseen nodes, as the feature
preprocessing requires the graph to be known and fixed. To further accelerate
Scalable GNNs inference in this inductive setting, we propose an online
propagation framework and two novel node-adaptive propagation methods that can
customize the optimal propagation depth for each node based on its topological
information and thereby avoid redundant feature propagation. The trade-off
between accuracy and latency can be flexibly managed through simple
hyper-parameters to accommodate various latency constraints. Moreover, to
compensate for the inference accuracy loss caused by the potential early
termination of propagation, we further propose Inception Distillation to
exploit the multi-scale receptive field information within graphs. The rigorous
and comprehensive experimental study on public datasets with varying scales and
characteristics demonstrates that the proposed inference acceleration framework
outperforms existing state-of-the-art graph inference acceleration methods in
terms of accuracy and efficiency. Particularly, the superiority of our approach
is notable on datasets with larger scales, yielding a 75x inference speedup on
the largest Ogbn-products dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xinyi Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wentao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Junliang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1&quot;&gt;Yingxia Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1&quot;&gt;Quoc Viet Hung Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1&quot;&gt;Bin Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1&quot;&gt;Hongzhi Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.13767">
<title>Graph AI in Medicine. (arXiv:2310.13767v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.13767</link>
<description rdf:parseType="Literal">&lt;p&gt;In clinical artificial intelligence (AI), graph representation learning,
mainly through graph neural networks (GNNs), stands out for its capability to
capture intricate relationships within structured clinical datasets. With
diverse data -- from patient records to imaging -- GNNs process data
holistically by viewing modalities as nodes interconnected by their
relationships. Graph AI facilitates model transfer across clinical tasks,
enabling models to generalize across patient populations without additional
parameters or minimal re-training. However, the importance of human-centered
design and model interpretability in clinical decision-making cannot be
overstated. Since graph AI models capture information through localized neural
transformations defined on graph relationships, they offer both an opportunity
and a challenge in elucidating model rationale. Knowledge graphs can enhance
interpretability by aligning model-driven insights with medical knowledge.
Emerging graph models integrate diverse data modalities through pre-training,
facilitate interactive feedback loops, and foster human-AI collaboration,
paving the way to clinically meaningful predictions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johnson_R/0/1/0/all/0/1&quot;&gt;Ruth Johnson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Michelle M. Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noori_A/0/1/0/all/0/1&quot;&gt;Ayush Noori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Queen_O/0/1/0/all/0/1&quot;&gt;Owen Queen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zitnik_M/0/1/0/all/0/1&quot;&gt;Marinka Zitnik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16285">
<title>Removing Dust from CMB Observations with Diffusion Models. (arXiv:2310.16285v2 [astro-ph.CO] UPDATED)</title>
<link>http://arxiv.org/abs/2310.16285</link>
<description rdf:parseType="Literal">&lt;p&gt;In cosmology, the quest for primordial $B$-modes in cosmic microwave
background (CMB) observations has highlighted the critical need for a refined
model of the Galactic dust foreground. We investigate diffusion-based modeling
of the dust foreground and its interest for component separation. Under the
assumption of a Gaussian CMB with known cosmology (or covariance matrix), we
show that diffusion models can be trained on examples of dust emission maps
such that their sampling process directly coincides with posterior sampling in
the context of component separation. We illustrate this on simulated mixtures
of dust emission and CMB. We show that common summary statistics (power
spectrum, Minkowski functionals) of the components are well recovered by this
process. We also introduce a model conditioned by the CMB cosmology that
outperforms models trained using a single cosmology on component separation.
Such a model will be used in future work for diffusion-based cosmological
inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Heurtel_Depeiges_D/0/1/0/all/0/1&quot;&gt;David Heurtel-Depeiges&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Burkhart_B/0/1/0/all/0/1&quot;&gt;Blakesley Burkhart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Ohana_R/0/1/0/all/0/1&quot;&gt;Ruben Ohana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Blancard_B/0/1/0/all/0/1&quot;&gt;Bruno R&amp;#xe9;galdo-Saint Blancard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17087">
<title>Good regularity creates large learning rate implicit biases: edge of stability, balancing, and catapult. (arXiv:2310.17087v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.17087</link>
<description rdf:parseType="Literal">&lt;p&gt;Large learning rates, when applied to gradient descent for nonconvex
optimization, yield various implicit biases including the edge of stability
(Cohen et al., 2021), balancing (Wang et al., 2022), and catapult (Lewkowycz et
al., 2020). These phenomena cannot be well explained by classical optimization
theory. Though significant theoretical progress has been made in understanding
these implicit biases, it remains unclear for which objective functions would
they be more likely. This paper provides an initial step in answering this
question and also shows that these implicit biases are in fact various tips of
the same iceberg. To establish these results, we develop a global convergence
theory under large learning rates, for a family of nonconvex functions without
globally Lipschitz continuous gradient, which was typically assumed in existing
convergence analysis. Specifically, these phenomena are more likely to occur
when the optimization objective function has good regularity. This regularity,
together with gradient descent using a large learning rate that favors flatter
regions, results in these nontrivial dynamical behaviors. Another corollary is
the first non-asymptotic convergence rate bound for large-learning-rate
gradient descent optimization of nonconvex functions. Although our theory only
applies to specific functions so far, the possibility of extrapolating it to
neural networks is also experimentally validated, for which different choices
of loss, activation functions, and other techniques such as batch normalization
can all affect regularity significantly and lead to very different training
dynamics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuqing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhenghao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tuo Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_M/0/1/0/all/0/1&quot;&gt;Molei Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19797">
<title>DEFT: Dexterous Fine-Tuning for Real-World Hand Policies. (arXiv:2310.19797v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19797</link>
<description rdf:parseType="Literal">&lt;p&gt;Dexterity is often seen as a cornerstone of complex manipulation. Humans are
able to perform a host of skills with their hands, from making food to
operating tools. In this paper, we investigate these challenges, especially in
the case of soft, deformable objects as well as complex, relatively
long-horizon tasks. However, learning such behaviors from scratch can be data
inefficient. To circumvent this, we propose a novel approach, DEFT (DExterous
Fine-Tuning for Hand Policies), that leverages human-driven priors, which are
executed directly in the real world. In order to improve upon these priors,
DEFT involves an efficient online optimization procedure. With the integration
of human-based learning and online fine-tuning, coupled with a soft robotic
hand, DEFT demonstrates success across various tasks, establishing a robust,
data-efficient pathway toward general dexterous manipulation. Please see our
website at https://dexterous-finetuning.github.io for video results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kannan_A/0/1/0/all/0/1&quot;&gt;Aditya Kannan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaw_K/0/1/0/all/0/1&quot;&gt;Kenneth Shaw&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bahl_S/0/1/0/all/0/1&quot;&gt;Shikhar Bahl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mannam_P/0/1/0/all/0/1&quot;&gt;Pragna Mannam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1&quot;&gt;Deepak Pathak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02227">
<title>State-Wise Safe Reinforcement Learning With Pixel Observations. (arXiv:2311.02227v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02227</link>
<description rdf:parseType="Literal">&lt;p&gt;In the context of safe exploration, Reinforcement Learning (RL) has long
grappled with the challenges of balancing the tradeoff between maximizing
rewards and minimizing safety violations, particularly in complex environments
with contact-rich or non-smooth dynamics, and when dealing with
high-dimensional pixel observations. Furthermore, incorporating state-wise
safety constraints in the exploration and learning process, where the agent
must avoid unsafe regions without prior knowledge, adds another layer of
complexity. In this paper, we propose a novel pixel-observation safe RL
algorithm that efficiently encodes state-wise safety constraints with unknown
hazard regions through a newly introduced latent barrier-like function learning
mechanism. As a joint learning framework, our approach begins by constructing a
latent dynamics model with low-dimensional latent spaces derived from pixel
observations. We then build and learn a latent barrier-like function on top of
the latent dynamics and conduct policy optimization simultaneously, thereby
improving both safety and the total expected return. Experimental evaluations
on the safety-gym benchmark suite demonstrate that our proposed method
significantly reduces safety violations throughout the training process, and
demonstrates faster safety convergence compared to existing methods while
achieving competitive results in reward return.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_S/0/1/0/all/0/1&quot;&gt;Simon Sinong Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yixuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qingyuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_R/0/1/0/all/0/1&quot;&gt;Ruochen Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1&quot;&gt;Qi Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04673">
<title>Compressive Recovery of Sparse Precision Matrices. (arXiv:2311.04673v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2311.04673</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of learning a graph modeling the statistical
relations of the $d$ variables from a dataset with $n$ samples $X \in
\mathbb{R}^{n \times d}$. Standard approaches amount to searching for a
precision matrix $\Theta$ representative of a Gaussian graphical model that
adequately explains the data. However, most maximum likelihood-based estimators
usually require storing the $d^{2}$ values of the empirical covariance matrix,
which can become prohibitive in a high-dimensional setting. In this work, we
adopt a compressive viewpoint and aim to estimate a sparse $\Theta$ from a
\emph{sketch} of the data, i.e. a low-dimensional vector of size $m \ll d^{2}$
carefully designed from $X$ using non-linear random features. Under certain
assumptions on the spectrum of $\Theta$ (or its condition number), we show that
it is possible to estimate it from a sketch of size
$m=\Omega\left((d+2k)\log(d)\right)$ where $k$ is the maximal number of edges
of the underlying graph. These information-theoretic guarantees are inspired by
compressed sensing theory and involve restricted isometry properties and
instance optimal decoders. We investigate the possibility of achieving
practical recovery with an iterative algorithm based on the graphical lasso,
viewed as a specific denoiser. We compare our approach and graphical lasso on
synthetic datasets, demonstrating its favorable performance even when the
dataset is compressed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vayer_T/0/1/0/all/0/1&quot;&gt;Titouan Vayer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lasalle_E/0/1/0/all/0/1&quot;&gt;Etienne Lasalle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gribonval_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;mi Gribonval&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Goncalves_P/0/1/0/all/0/1&quot;&gt;Paulo Gon&amp;#xe7;alves&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06062">
<title>Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration. (arXiv:2311.06062v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.06062</link>
<description rdf:parseType="Literal">&lt;p&gt;Membership Inference Attacks (MIA) aim to infer whether a target data record
has been utilized for model training or not. Prior attempts have quantified the
privacy risks of language models (LMs) via MIAs, but there is still no
consensus on whether existing MIA algorithms can cause remarkable privacy
leakage on practical Large Language Models (LLMs). Existing MIAs designed for
LMs can be classified into two categories: reference-free and reference-based
attacks. They are both based on the hypothesis that training records
consistently strike a higher probability of being sampled. Nevertheless, this
hypothesis heavily relies on the overfitting of target models, which will be
mitigated by multiple regularization methods and the generalization of LLMs.
The reference-based attack seems to achieve promising effectiveness in LLMs,
which measures a more reliable membership signal by comparing the probability
discrepancy between the target model and the reference model. However, the
performance of reference-based attack is highly dependent on a reference
dataset that closely resembles the training dataset, which is usually
inaccessible in the practical scenario. Overall, existing MIAs are unable to
effectively unveil privacy leakage over practical fine-tuned LLMs that are
overfitting-free and private. We propose a Membership Inference Attack based on
Self-calibrated Probabilistic Variation (SPV-MIA). Specifically, since
memorization in LLMs is inevitable during the training process and occurs
before overfitting, we introduce a more reliable membership signal,
probabilistic variation, which is based on memorization rather than
overfitting. Furthermore, we introduce a self-prompt approach, which constructs
the dataset to fine-tune the reference model by prompting the target LLM
itself. In this manner, the adversary can collect a dataset with a similar
distribution from public APIs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_W/0/1/0/all/0/1&quot;&gt;Wenjie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Huandong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Chen Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guanghua Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1&quot;&gt;Tao Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07289">
<title>A probabilistic forecast methodology for volatile electricity prices in the Australian National Electricity Market. (arXiv:2311.07289v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.07289</link>
<description rdf:parseType="Literal">&lt;p&gt;The South Australia region of the Australian National Electricity Market
(NEM) displays some of the highest levels of price volatility observed in
modern electricity markets. This paper outlines an approach to probabilistic
forecasting under these extreme conditions, including spike filtration and
several post-processing steps. We propose using quantile regression as an
ensemble tool for probabilistic forecasting, with our combined forecasts
achieving superior results compared to all constituent models. Within our
ensemble framework, we demonstrate that averaging models with varying training
length periods leads to a more adaptive model and increased prediction
accuracy. The applicability of the final model is evaluated by comparing our
median forecasts with the point forecasts available from the Australian NEM
operator, with our model outperforming these NEM forecasts by a significant
margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cornell_C/0/1/0/all/0/1&quot;&gt;Cameron Cornell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dinh_N/0/1/0/all/0/1&quot;&gt;Nam Trong Dinh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pourmousavi_S/0/1/0/all/0/1&quot;&gt;S. Ali Pourmousavi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08360">
<title>The Transient Nature of Emergent In-Context Learning in Transformers. (arXiv:2311.08360v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.08360</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer neural networks can exhibit a surprising capacity for in-context
learning (ICL) despite not being explicitly trained for it. Prior work has
provided a deeper understanding of how ICL emerges in transformers, e.g.
through the lens of mechanistic interpretability, Bayesian inference, or by
examining the distributional properties of training data. However, in each of
these cases, ICL is treated largely as a persistent phenomenon; namely, once
ICL emerges, it is assumed to persist asymptotically. Here, we show that the
emergence of ICL during transformer training is, in fact, often transient. We
train transformers on synthetic data designed so that both ICL and in-weights
learning (IWL) strategies can lead to correct predictions. We find that ICL
first emerges, then disappears and gives way to IWL, all while the training
loss decreases, indicating an asymptotic preference for IWL. The transient
nature of ICL is observed in transformers across a range of model sizes and
datasets, raising the question of how much to &quot;overtrain&quot; transformers when
seeking compact, cheaper-to-run models. We find that L2 regularization may
offer a path to more persistent ICL that removes the need for early stopping
based on ICL-style validation tasks. Finally, we present initial evidence that
ICL transience may be caused by competition between ICL and IWL circuits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Aaditya K. Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1&quot;&gt;Stephanie C.Y. Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moskovitz_T/0/1/0/all/0/1&quot;&gt;Ted Moskovitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grant_E/0/1/0/all/0/1&quot;&gt;Erin Grant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saxe_A/0/1/0/all/0/1&quot;&gt;Andrew M. Saxe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1&quot;&gt;Felix Hill&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09247">
<title>Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks. (arXiv:2311.09247v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09247</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore the abstract reasoning abilities of text-only and multimodal
versions of GPT-4, using the ConceptARC benchmark [10], which is designed to
evaluate robust understanding and reasoning with core-knowledge concepts. We
extend the work of Moskvichev et al. [10] by evaluating GPT-4 on more detailed,
one-shot prompting (rather than simple, zero-shot prompts) with text versions
of ConceptARC tasks, and by evaluating GPT-4V, the multimodal version of GPT-4,
on zero- and one-shot prompts using image versions of the simplest tasks. Our
experimental results support the conclusion that neither version of GPT-4 has
developed robust abstraction abilities at humanlike levels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitchell_M/0/1/0/all/0/1&quot;&gt;Melanie Mitchell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palmarini_A/0/1/0/all/0/1&quot;&gt;Alessandro B. Palmarini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moskvichev_A/0/1/0/all/0/1&quot;&gt;Arseny Moskvichev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13225">
<title>NeutronOrch: Rethinking Sample-based GNN Training under CPU-GPU Heterogeneous Environments. (arXiv:2311.13225v2 [cs.DC] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13225</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) have demonstrated outstanding performance in
various applications. Existing frameworks utilize CPU-GPU heterogeneous
environments to train GNN models and integrate mini-batch and sampling
techniques to overcome the GPU memory limitation. In CPU-GPU heterogeneous
environments, we can divide sample-based GNN training into three steps: sample,
gather, and train. Existing GNN systems use different task orchestrating
methods to employ each step on CPU or GPU. After extensive experiments and
analysis, we find that existing task orchestrating methods fail to fully
utilize the heterogeneous resources, limited by inefficient CPU processing or
GPU resource contention. In this paper, we propose NeutronOrch, a system for
sample-based GNN training that incorporates a layer-based task orchestrating
method and ensures balanced utilization of the CPU and GPU. NeutronOrch
decouples the training process by layer and pushes down the training task of
the bottom layer to the CPU. This significantly reduces the computational load
and memory footprint of GPU training. To avoid inefficient CPU processing,
NeutronOrch only offloads the training of frequently accessed vertices to the
CPU and lets GPU reuse their embeddings with bounded staleness. Furthermore,
NeutronOrch provides a fine-grained pipeline design for the layer-based task
orchestrating method, fully overlapping different tasks on heterogeneous
resources while strictly guaranteeing bounded staleness. The experimental
results show that compared with the state-of-the-art GNN systems, NeutronOrch
can achieve up to 11.51x performance speedup.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ai_X/0/1/0/all/0/1&quot;&gt;Xin Ai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qiange Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1&quot;&gt;Chunyu Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yanfeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chaoyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1&quot;&gt;Hao Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yu Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1&quot;&gt;Ge Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15497">
<title>Adaptive Image Registration: A Hybrid Approach Integrating Deep Learning and Optimization Functions for Enhanced Precision. (arXiv:2311.15497v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.15497</link>
<description rdf:parseType="Literal">&lt;p&gt;Image registration has traditionally been done using two distinct approaches:
learning based methods, relying on robust deep neural networks, and
optimization-based methods, applying complex mathematical transformations to
warp images accordingly. Of course, both paradigms offer advantages and
disadvantages, and, in this work, we seek to combine their respective strengths
into a single streamlined framework, using the outputs of the learning based
method as initial parameters for optimization while prioritizing computational
power for the image pairs that offer the greatest loss. Our investigations
showed that an improvement of 1.5% in testing when utilizing the best
performing state-of-the-art model as the backbone of the framework, while
maintaining the same inference time and a substantial 0.94% points performance
gain in deformation field smoothness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Araujo_G/0/1/0/all/0/1&quot;&gt;Gabriel De Araujo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Shanlin Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiaohui Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.18130">
<title>The Trifecta: Three simple techniques for training deeper Forward-Forward networks. (arXiv:2311.18130v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.18130</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern machine learning models are able to outperform humans on a variety of
non-trivial tasks. However, as the complexity of the models increases, they
consume significant amounts of power and still struggle to generalize
effectively to unseen data. Local learning, which focuses on updating subsets
of a model&apos;s parameters at a time, has emerged as a promising technique to
address these issues. Recently, a novel local learning algorithm, called
Forward-Forward, has received widespread attention due to its innovative
approach to learning. Unfortunately, its application has been limited to
smaller datasets due to scalability issues. To this end, we propose The
Trifecta, a collection of three simple techniques that synergize exceptionally
well and drastically improve the Forward-Forward algorithm on deeper networks.
Our experiments demonstrate that our models are on par with similarly
structured, backpropagation-based models in both training speed and test
accuracy on simple datasets. This is achieved by the ability to learn
representations that are informative locally, on a layer-by-layer basis, and
retain their informativeness when propagated to deeper layers in the
architecture. This leads to around 84% accuracy on CIFAR-10, a notable
improvement (25%) over the original FF algorithm. These results highlight the
potential of Forward-Forward as a genuine competitor to backpropagation and as
a promising research avenue.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dooms_T/0/1/0/all/0/1&quot;&gt;Thomas Dooms&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1&quot;&gt;Ing Jyh Tsang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oramas_J/0/1/0/all/0/1&quot;&gt;Jose Oramas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.18694">
<title>Balancing Summarization and Change Detection in Graph Streams. (arXiv:2311.18694v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2311.18694</link>
<description rdf:parseType="Literal">&lt;p&gt;This study addresses the issue of balancing graph summarization and graph
change detection. Graph summarization compresses large-scale graphs into a
smaller scale. However, the question remains: To what extent should the
original graph be compressed? This problem is solved from the perspective of
graph change detection, aiming to detect statistically significant changes
using a stream of summary graphs. If the compression rate is extremely high,
important changes can be ignored, whereas if the compression rate is extremely
low, false alarms may increase with more memory. This implies that there is a
trade-off between compression rate in graph summarization and accuracy in
change detection. We propose a novel quantitative methodology to balance this
trade-off to simultaneously realize reliable graph summarization and change
detection. We introduce a probabilistic structure of hierarchical latent
variable model into a graph, thereby designing a parameterized summary graph on
the basis of the minimum description length principle. The parameter specifying
the summary graph is then optimized so that the accuracy of change detection is
guaranteed to suppress Type I error probability (probability of raising false
alarms) to be less than a given confidence level. First, we provide a
theoretical framework for connecting graph summarization with change detection.
Then, we empirically demonstrate its effectiveness on synthetic and real
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fukushima_S/0/1/0/all/0/1&quot;&gt;Shintaro Fukushima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yamanishi_K/0/1/0/all/0/1&quot;&gt;Kenji Yamanishi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.18718">
<title>Steering Deep Feature Learning with Backward Aligned Feature Updates. (arXiv:2311.18718v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.18718</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning succeeds by doing hierarchical feature learning, yet tuning
Hyper-Parameters (HP) such as initialization scales, learning rates etc., only
give indirect control over this behavior. In this paper, we propose the
alignment between the feature updates and the backward pass as a key notion to
predict, measure and control feature learning. On the one hand, we show that
when alignment holds, the magnitude of feature updates after one SGD step is
related to the magnitude of the forward and backward passes by a simple and
general formula. This leads to techniques to automatically adjust HPs
(initialization scales and learning rates) at initialization and throughout
training to attain a desired feature learning behavior. On the other hand, we
show that, at random initialization, this alignment is determined by the
spectrum of a certain kernel, and that well-conditioned layer-to-layer
Jacobians (aka dynamical isometry) implies alignment. Finally, we investigate
ReLU MLPs and ResNets in the large width-then-depth limit. Combining hints from
random matrix theory and numerical experiments, we show that (i) in MLP with
iid initializations, alignment degenerates with depth, making it impossible to
start training, and that (ii) in ResNets, the branch scale
$1/\sqrt{\text{depth}}$ is the only one maintaining non-trivial alignment at
infinite depth.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chizat_L/0/1/0/all/0/1&quot;&gt;L&amp;#xe9;na&amp;#xef;c Chizat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Netrapalli_P/0/1/0/all/0/1&quot;&gt;Praneeth Netrapalli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00258">
<title>Precipitation Nowcasting With Spatial And Temporal Transfer Learning Using Swin-UNETR. (arXiv:2312.00258v2 [physics.ao-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2312.00258</link>
<description rdf:parseType="Literal">&lt;p&gt;Climate change has led to an increase in frequency of extreme weather events.
Early warning systems can prevent disasters and loss of life. Managing such
events remain a challenge for both public and private institutions.
Precipitation nowcasting can help relevant institutions to better prepare for
such events. Numerical weather prediction (NWP) has traditionally been used to
make physics based forecasting, and recently deep learning based approaches
have been used to reduce turn-around time for nowcasting. In this work,
recently proposed Swin-UNETR (Swin UNEt TRansformer) is used for precipitation
nowcasting for ten different regions of Europe. Swin-UNETR utilizes a U-shaped
network within which a swin transformer-based encoder extracts multi-scale
features from multiple input channels of satellite image, while CNN-based
decoder makes the prediction. Trained model is capable of nowcasting not only
for the regions for which data is available, but can also be used for new
regions for which data is not available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Ajitabh Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03998">
<title>Series2Vec: Similarity-based Self-supervised Representation Learning for Time Series Classification. (arXiv:2312.03998v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.03998</link>
<description rdf:parseType="Literal">&lt;p&gt;We argue that time series analysis is fundamentally different in nature to
either vision or natural language processing with respect to the forms of
meaningful self-supervised learning tasks that can be defined. Motivated by
this insight, we introduce a novel approach called \textit{Series2Vec} for
self-supervised representation learning. Unlike other self-supervised methods
in time series, which carry the risk of positive sample variants being less
similar to the anchor sample than series in the negative set, Series2Vec is
trained to predict the similarity between two series in both temporal and
spectral domains through a self-supervised task. Series2Vec relies primarily on
the consistency of the unsupervised similarity step, rather than the intrinsic
quality of the similarity measurement, without the need for hand-crafted data
augmentation. To further enforce the network to learn similar representations
for similar time series, we propose a novel approach that applies
order-invariant attention to each representation within the batch during
training. Our evaluation of Series2Vec on nine large real-world datasets, along
with the UCR/UEA archive, shows enhanced performance compared to current
state-of-the-art self-supervised techniques for time series. Additionally, our
extensive experiments show that Series2Vec performs comparably with fully
supervised training and offers high efficiency in datasets with limited-labeled
data. Finally, we show that the fusion of Series2Vec with other representation
learning models leads to enhanced performance for time series classification.
Code and models are open-source at
\url{https://github.com/Navidfoumani/Series2Vec.}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foumani_N/0/1/0/all/0/1&quot;&gt;Navid Mohammadi Foumani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Chang Wei Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Webb_G/0/1/0/all/0/1&quot;&gt;Geoffrey I. Webb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rezatofighi_H/0/1/0/all/0/1&quot;&gt;Hamid Rezatofighi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salehi_M/0/1/0/all/0/1&quot;&gt;Mahsa Salehi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04344">
<title>Enhancing Medical Task Performance in GPT-4V: A Comprehensive Study on Prompt Engineering Strategies. (arXiv:2312.04344v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04344</link>
<description rdf:parseType="Literal">&lt;p&gt;OpenAI&apos;s latest large vision-language model (LVLM), GPT-4V(ision), has piqued
considerable interest for its potential in medical applications. Despite its
promise, recent studies and internal reviews highlight its underperformance in
specialized medical tasks. This paper explores the boundary of GPT-4V&apos;s
capabilities in medicine, particularly in processing complex imaging data from
endoscopies, CT scans, and MRIs etc. Leveraging open-source datasets, we
assessed its foundational competencies, identifying substantial areas for
enhancement. Our research emphasizes prompt engineering, an often-underutilized
strategy for improving AI responsiveness. Through iterative testing, we refined
the model&apos;s prompts, significantly improving its interpretative accuracy and
relevance in medical imaging. From our comprehensive evaluations, we distilled
10 effective prompt engineering techniques, each fortifying GPT-4V&apos;s medical
acumen. These methodical enhancements facilitate more reliable, precise, and
clinically valuable insights from GPT-4V, advancing its operability in critical
healthcare environments. Our findings are pivotal for those employing AI in
medicine, providing clear, actionable guidance on harnessing GPT-4V&apos;s full
diagnostic potential.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pengcheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Ziyan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1&quot;&gt;Zhongying Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tianbin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yanzhou Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haoyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jin Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Junjun He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04815">
<title>Not All Negatives Are Worth Attending to: Meta-Bootstrapping Negative Sampling Framework for Link Prediction. (arXiv:2312.04815v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04815</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid development of graph neural networks (GNNs) encourages the rising
of link prediction, achieving promising performance with various applications.
Unfortunately, through a comprehensive analysis, we surprisingly find that
current link predictors with dynamic negative samplers (DNSs) suffer from the
migration phenomenon between &quot;easy&quot; and &quot;hard&quot; samples, which goes against the
preference of DNS of choosing &quot;hard&quot; negatives, thus severely hindering
capability. Towards this end, we propose the MeBNS framework, serving as a
general plugin that can potentially improve current negative sampling based
link predictors. In particular, we elaborately devise a Meta-learning Supported
Teacher-student GNN (MST-GNN) that is not only built upon teacher-student
architecture for alleviating the migration between &quot;easy&quot; and &quot;hard&quot; samples
but also equipped with a meta learning based sample re-weighting module for
helping the student GNN distinguish &quot;hard&quot; samples in a fine-grained manner. To
effectively guide the learning of MST-GNN, we prepare a Structure enhanced
Training Data Generator (STD-Generator) and an Uncertainty based Meta Data
Collector (UMD-Collector) for supporting the teacher and student GNN,
respectively. Extensive experiments show that the MeBNS achieves remarkable
performance across six link prediction benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yakun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1&quot;&gt;Binbin Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shuo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1&quot;&gt;Meiqi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qiyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_G/0/1/0/all/0/1&quot;&gt;Guo Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;Huimei He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05496">
<title>Flexible Cross-Modal Steganography via Implicit Representations. (arXiv:2312.05496v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2312.05496</link>
<description rdf:parseType="Literal">&lt;p&gt;We present INRSteg, an innovative lossless steganography framework based on a
novel data form Implicit Neural Representations (INR) that is modal-agnostic.
Our framework is considered for effectively hiding multiple data without
altering the original INR ensuring high-quality stego data. The neural
representations of secret data are first concatenated to have independent paths
that do not overlap, then weight freezing techniques are applied to the
diagonal blocks of the weight matrices for the concatenated network to preserve
the weights of secret data while additional free weights in the off-diagonal
blocks of weight matrices are fitted to the cover data. Our framework can
perform unexplored cross-modal steganography for various modalities including
image, audio, video, and 3D shapes, and it achieves state-of-the-art
performance compared to previous intra-modal steganographic methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Seoyun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Sojeong Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1&quot;&gt;Chang D. Yoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Junmo Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05549">
<title>Multi-granularity Causal Structure Learning. (arXiv:2312.05549v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.05549</link>
<description rdf:parseType="Literal">&lt;p&gt;Unveil, model, and comprehend the causal mechanisms underpinning natural
phenomena stand as fundamental endeavors across myriad scientific disciplines.
Meanwhile, new knowledge emerges when discovering causal relationships from
data. Existing causal learning algorithms predominantly focus on the isolated
effects of variables, overlook the intricate interplay of multiple variables
and their collective behavioral patterns. Furthermore, the ubiquity of
high-dimensional data exacts a substantial temporal cost for causal algorithms.
In this paper, we develop a novel method called MgCSL (Multi-granularity Causal
Structure Learning), which first leverages sparse auto-encoder to explore
coarse-graining strategies and causal abstractions from micro-variables to
macro-ones. MgCSL then takes multi-granularity variables as inputs to train
multilayer perceptrons and to delve the causality between variables. To enhance
the efficacy on high-dimensional data, MgCSL introduces a simplified acyclicity
constraint to adeptly search the directed acyclic graph among variables.
Experimental results show that MgCSL outperforms competitive baselines, and
finds out explainable causal connections on fMRI datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Jiaxuan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1&quot;&gt;Guoxian Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1&quot;&gt;Shuyin Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guoyin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05725">
<title>FP8-BERT: Post-Training Quantization for Transformer. (arXiv:2312.05725v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.05725</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer-based models, such as BERT, have been widely applied in a wide
range of natural language processing tasks. However, one inevitable side effect
is that they require massive memory storage and inference cost when deployed in
production. Quantization is one of the popularized ways to alleviate the cost.
However, the previous 8-bit quantization strategy based on INT8 data format
either suffers from the degradation of accuracy in a Post-Training Quantization
(PTQ) fashion or requires an expensive Quantization-Aware Training (QAT)
process. Recently, a new numeric format FP8 (i.e. floating-point of 8-bits) has
been proposed and supported in commercial AI computing platforms such as H100.
In this paper, we empirically validate the effectiveness of FP8 as a way to do
Post-Training Quantization without significant loss of accuracy, with a simple
calibration and format conversion process. We adopt the FP8 standard proposed
by NVIDIA Corp. (2022) in our extensive experiments of BERT variants on GLUE
and SQuAD v1.1 datasets, and show that PTQ with FP8 can significantly improve
the accuracy upon that with INT8, to the extent of the full-precision model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianchi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yen_I/0/1/0/all/0/1&quot;&gt;Ian En-Hsu Yen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Dongkuan Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05743">
<title>Building Variable-sized Models via Learngene Pool. (arXiv:2312.05743v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.05743</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, Stitchable Neural Networks (SN-Net) is proposed to stitch some
pre-trained networks for quickly building numerous networks with different
complexity and performance trade-offs. In this way, the burdens of designing or
training the variable-sized networks, which can be used in application
scenarios with diverse resource constraints, are alleviated. However, SN-Net
still faces a few challenges. 1) Stitching from multiple independently
pre-trained anchors introduces high storage resource consumption. 2) SN-Net
faces challenges to build smaller models for low resource constraints. 3).
SN-Net uses an unlearned initialization method for stitch layers, limiting the
final performance. To overcome these challenges, motivated by the recently
proposed Learngene framework, we propose a novel method called Learngene Pool.
Briefly, Learngene distills the critical knowledge from a large pre-trained
model into a small part (termed as learngene) and then expands this small part
into a few variable-sized models. In our proposed method, we distill one
pretrained large model into multiple small models whose network blocks are used
as learngene instances to construct the learngene pool. Since only one large
model is used, we do not need to store more large models as SN-Net and after
distilling, smaller learngene instances can be created to build small models to
satisfy low resource constraints. We also insert learnable transformation
matrices between the instances to stitch them into variable-sized models to
improve the performance of these models. Exhaustive experiments have been
implemented and the results validate the effectiveness of the proposed
Learngene Pool compared with SN-Net.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Boyu Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1&quot;&gt;Shiyu Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haokun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kou_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Kou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1&quot;&gt;Xin Geng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05924">
<title>Data-Free Hard-Label Robustness Stealing Attack. (arXiv:2312.05924v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.05924</link>
<description rdf:parseType="Literal">&lt;p&gt;The popularity of Machine Learning as a Service (MLaaS) has led to increased
concerns about Model Stealing Attacks (MSA), which aim to craft a clone model
by querying MLaaS. Currently, most research on MSA assumes that MLaaS can
provide soft labels and that the attacker has a proxy dataset with a similar
distribution. However, this fails to encapsulate the more practical scenario
where only hard labels are returned by MLaaS and the data distribution remains
elusive. Furthermore, most existing work focuses solely on stealing the model
accuracy, neglecting the model robustness, while robustness is essential in
security-sensitive scenarios, e.g., face-scan payment. Notably, improving model
robustness often necessitates the use of expensive techniques such as
adversarial training, thereby further making stealing robustness a more
lucrative prospect. In response to these identified gaps, we introduce a novel
Data-Free Hard-Label Robustness Stealing (DFHL-RS) attack in this paper, which
enables the stealing of both model accuracy and robustness by simply querying
hard labels of the target model without the help of any natural data.
Comprehensive experiments demonstrate the effectiveness of our method. The
clone model achieves a clean accuracy of 77.86% and a robust accuracy of 39.51%
against AutoAttack, which are only 4.71% and 8.40% lower than the target model
on the CIFAR-10 dataset, significantly exceeding the baselines. Our code is
available at: https://github.com/LetheSec/DFHL-RS-Attack.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xiaojian Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kejiang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wen Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weiming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1&quot;&gt;Nenghai Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05978">
<title>Neural Architecture Codesign for Fast Bragg Peak Analysis. (arXiv:2312.05978v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.05978</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop an automated pipeline to streamline neural architecture codesign
for fast, real-time Bragg peak analysis in high-energy diffraction microscopy.
Traditional approaches, notably pseudo-Voigt fitting, demand significant
computational resources, prompting interest in deep learning models for more
efficient solutions. Our method employs neural architecture search and AutoML
to enhance these models, including hardware costs, leading to the discovery of
more hardware-efficient neural architectures. Our results match the
performance, while achieving a 13$\times$ reduction in bit operations compared
to the previous state-of-the-art. We show further speedup through model
compression techniques such as quantization-aware-training and neural network
pruning. Additionally, our hierarchical search space provides greater
flexibility in optimization, which can easily extend to other tasks and
domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McDermott_L/0/1/0/all/0/1&quot;&gt;Luke McDermott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weitz_J/0/1/0/all/0/1&quot;&gt;Jason Weitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demler_D/0/1/0/all/0/1&quot;&gt;Dmitri Demler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cummings_D/0/1/0/all/0/1&quot;&gt;Daniel Cummings&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_N/0/1/0/all/0/1&quot;&gt;Nhan Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duarte_J/0/1/0/all/0/1&quot;&gt;Javier Duarte&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06348">
<title>DiffAIL: Diffusion Adversarial Imitation Learning. (arXiv:2312.06348v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06348</link>
<description rdf:parseType="Literal">&lt;p&gt;Imitation learning aims to solve the problem of defining reward functions in
real-world decision-making tasks. The current popular approach is the
Adversarial Imitation Learning (AIL) framework, which matches expert
state-action occupancy measures to obtain a surrogate reward for forward
reinforcement learning. However, the traditional discriminator is a simple
binary classifier and doesn&apos;t learn an accurate distribution, which may result
in failing to identify expert-level state-action pairs induced by the policy
interacting with the environment. To address this issue, we propose a method
named diffusion adversarial imitation learning (DiffAIL), which introduces the
diffusion model into the AIL framework. Specifically, DiffAIL models the
state-action pairs as unconditional diffusion models and uses diffusion loss as
part of the discriminator&apos;s learning objective, which enables the discriminator
to capture better expert demonstrations and improve generalization.
Experimentally, the results show that our method achieves state-of-the-art
performance and significantly surpasses expert demonstration on two benchmark
tasks, including the standard state-action setting and state-only settings. Our
code can be available at the link https://github.com/ML-Group-SDU/DiffAIL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bingzheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1&quot;&gt;Guoqiang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1&quot;&gt;Teng Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1&quot;&gt;Yilong Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06363">
<title>MMICT: Boosting Multi-Modal Fine-Tuning with In-Context Examples. (arXiv:2312.06363v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06363</link>
<description rdf:parseType="Literal">&lt;p&gt;Although In-Context Learning (ICL) brings remarkable performance gains to
Large Language Models (LLMs), the improvements remain lower than fine-tuning on
downstream tasks. This paper introduces Multi-Modal In-Context Tuning (MMICT),
a novel multi-modal fine-tuning paradigm that boosts multi-modal fine-tuning by
fully leveraging the promising ICL capability of multi-modal LLMs (MM-LLMs). We
propose the Multi-Modal Hub (M-Hub), a unified module that captures various
multi-modal features according to different inputs and objectives. Based on
M-Hub, MMICT enables MM-LLMs to learn from in-context visual-guided textual
features and subsequently generate outputs conditioned on the textual-guided
visual features. Moreover, leveraging the flexibility of M-Hub, we design a
variety of in-context demonstrations. Extensive experiments on a diverse range
of downstream multi-modal tasks demonstrate that MMICT significantly
outperforms traditional fine-tuning strategy and the vanilla ICT method that
directly takes the concatenation of all information from different modalities
as input.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1&quot;&gt;Enwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yuting Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Ke Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xing Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hui Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06436">
<title>Reward Certification for Policy Smoothed Reinforcement Learning. (arXiv:2312.06436v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06436</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement Learning (RL) has achieved remarkable success in
safety-critical areas, but it can be weakened by adversarial attacks. Recent
studies have introduced &quot;smoothed policies&quot; in order to enhance its robustness.
Yet, it is still challenging to establish a provable guarantee to certify the
bound of its total reward. Prior methods relied primarily on computing bounds
using Lipschitz continuity or calculating the probability of cumulative reward
above specific thresholds. However, these techniques are only suited for
continuous perturbations on the RL agent&apos;s observations and are restricted to
perturbations bounded by the $l_2$-norm. To address these limitations, this
paper proposes a general black-box certification method capable of directly
certifying the cumulative reward of the smoothed policy under various
$l_p$-norm bounded perturbations. Furthermore, we extend our methodology to
certify perturbations on action spaces. Our approach leverages f-divergence to
measure the distinction between the original distribution and the perturbed
distribution, subsequently determining the certification bound by solving a
convex optimisation problem. We provide a comprehensive theoretical analysis
and run sufficient experiments in multiple environments. Our results show that
our method not only improves the certified lower bound of mean cumulative
reward but also demonstrates better efficiency than state-of-the-art
techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_R/0/1/0/all/0/1&quot;&gt;Ronghui Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marcolino_L/0/1/0/all/0/1&quot;&gt;Leandro Soriano Marcolino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianle Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yanghao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaowei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruan_W/0/1/0/all/0/1&quot;&gt;Wenjie Ruan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06564">
<title>Promoting Counterfactual Robustness through Diversity. (arXiv:2312.06564v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06564</link>
<description rdf:parseType="Literal">&lt;p&gt;Counterfactual explanations shed light on the decisions of black-box models
by explaining how an input can be altered to obtain a favourable decision from
the model (e.g., when a loan application has been rejected). However, as noted
recently, counterfactual explainers may lack robustness in the sense that a
minor change in the input can cause a major change in the explanation. This can
cause confusion on the user side and open the door for adversarial attacks. In
this paper, we study some sources of non-robustness. While there are
fundamental reasons for why an explainer that returns a single counterfactual
cannot be robust in all instances, we show that some interesting robustness
guarantees can be given by reporting multiple rather than a single
counterfactual. Unfortunately, the number of counterfactuals that need to be
reported for the theoretical guarantees to hold can be prohibitively large. We
therefore propose an approximation algorithm that uses a diversity criterion to
select a feasible number of most relevant explanations and study its robustness
empirically. Our experiments indicate that our method improves the
state-of-the-art in generating robust explanations, while maintaining other
desirable properties and providing competitive computational performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leofante_F/0/1/0/all/0/1&quot;&gt;Francesco Leofante&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Potyka_N/0/1/0/all/0/1&quot;&gt;Nico Potyka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06635">
<title>Gated Linear Attention Transformers with Hardware-Efficient Training. (arXiv:2312.06635v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06635</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformers with linear attention allow for efficient parallel training but
can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden
states, thus enjoying linear (with respect to output length) inference
complexity. Recent works such as RetNet (Sun et al., 2023) and TransNormerLLM
(Qin et al., 2023a) observe that adding a global decay term to the additive RNN
update rule greatly improves performance, sometimes outperforming standard
Transformers with softmax attention when trained at scale. In this work we show
that adding a data-dependent gating mechanism further improves performance. We
derive a parallel form of this gated linear attention layer that enables
efficient training. However, a straightforward, numerically stable
implementation of this parallel form requires generalized matrix
multiplications in log-space for numerical stability, and thus cannot take
advantage of tensor cores on modern GPUs which are optimized for standard
matrix multiplications. We develop a hardware-efficient version of the parallel
form that can still make use of tensor cores through block-parallel
computations over sequence chunks. Experiments on moderate-scale language
modeling (340M-parameter models trained on 15B tokens, 1.3B-parameter models
trained on 100B tokens) show that gated linear attention (GLA) Transformers
perform competitively against a strong LLaMA-architecture Transformer baseline
(Touvron et al., 2023) as well as Mamba (Gu &amp;amp; Dao, 2023), a recently introduced
state-space model with a data-dependent state transition mechanism. For
training speed, our Triton-based implementation performs comparably to
CUDA-optimized FlashAttention-2 (Dao, 2023) under the regular 2048 training
length setting, while outperforming FlashAttention-2 when training on longer
sequences beyond 4096.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Songlin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bailin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yikang Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1&quot;&gt;Rameswar Panda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yoon Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06540">
<title>Convergence of the Chambolle-Pock Algorithm in the Absence of Monotonicity. (arXiv:2312.06540v1 [math.OC] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2312.06540</link>
<description rdf:parseType="Literal">&lt;p&gt;The Chambolle-Pock algorithm (CPA), also known as the primal-dual hybrid
gradient method (PDHG), has surged in popularity in the last decade due to its
success in solving convex/monotone structured problems. This work provides
convergence results for problems with varying degrees of (non)monotonicity,
quantified through a so-called oblique weak Minty condition on the associated
primal-dual operator. Our results reveal novel stepsize and relaxation
parameter ranges which do not only depend on the norm of the linear mapping,
but also on its other singular values. In particular, in nonmonotone settings,
in addition to the classical stepsize conditions for CPA, extra bounds on the
stepsizes and relaxation parameters are required. On the other hand, in the
strongly monotone setting, the relaxation parameter is allowed to exceed the
classical upper bound of two. Moreover, sufficient convergence conditions are
obtained when the individual operators belong to the recently introduced class
of semimonotone operators. Since this class of operators encompasses many
traditional operator classes including (hypo)- and co(hypo)monotone operators,
this analysis recovers and extends existing results for CPA. Several examples
are provided for the aforementioned problem classes to demonstrate and
establish tightness of the proposed stepsize ranges.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Evens_B/0/1/0/all/0/1&quot;&gt;Brecht Evens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Latafat_P/0/1/0/all/0/1&quot;&gt;Puya Latafat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Patrinos_P/0/1/0/all/0/1&quot;&gt;Panagiotis Patrinos&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>