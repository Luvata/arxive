<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-09-28T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15857" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15866" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15875" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15877" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15881" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15942" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15946" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15979" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16025" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16035" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16042" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16064" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16082" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16090" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16096" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16102" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16108" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16117" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16119" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16143" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16146" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16150" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16161" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16166" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16180" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16211" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16220" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16223" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16235" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16240" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16257" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16258" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16263" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16275" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16286" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16289" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16291" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16319" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16335" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16344" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16380" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16382" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16391" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16397" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16400" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16413" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16414" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16424" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16429" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16436" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16459" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16492" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16511" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16512" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16534" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16535" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16561" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16564" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16573" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16593" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16595" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16597" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16606" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16618" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16620" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16621" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16633" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16639" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16661" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16668" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.05625" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.04963" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.12814" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.11562" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.00407" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.03037" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.11003" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.16296" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.06366" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.08551" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.10749" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.12014" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.03701" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.03942" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.04811" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.04866" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.11364" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14091" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01026" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06135" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06822" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04412" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.06595" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.06922" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.09267" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.09830" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.10379" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.13978" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.14359" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.05516" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.06129" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07461" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.09175" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.09979" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13393" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14181" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14329" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14564" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14793" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15317" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15757" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12481" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2309.15857">
<title>A Survey on Image-text Multimodal Models. (arXiv:2309.15857v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.15857</link>
<description rdf:parseType="Literal">&lt;p&gt;Amidst the evolving landscape of artificial intelligence, the convergence of
visual and textual information has surfaced as a crucial frontier, leading to
the advent of image-text multimodal models. This paper provides a comprehensive
review of the evolution and current state of image-text multimodal models,
exploring their application value, challenges, and potential research
trajectories. Initially, we revisit the basic concepts and developmental
milestones of these models, introducing a novel classification that segments
their evolution into three distinct phases, based on their time of introduction
and subsequent impact on the discipline. Furthermore, based on the tasks&apos;
significance and prevalence in the academic landscape, we propose a
categorization of the tasks associated with image-text multimodal models into
five major types, elucidating the recent progress and key technologies within
each category. Despite the remarkable accomplishments of these models, numerous
challenges and issues persist. This paper delves into the inherent challenges
and limitations of image-text multimodal models, fostering the exploration of
prospective research directions. Our objective is to offer an exhaustive
overview of the present research landscape of image-text multimodal models and
to serve as a valuable reference for future scholarly endeavors. We extend an
invitation to the broader community to collaborate in enhancing the image-text
multimodal model community, accessible at:
\href{https://github.com/i2vec/A-survey-on-image-text-multimodal-models}{https://github.com/i2vec/A-survey-on-image-text-multimodal-models}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1&quot;&gt;Ruifeng Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1&quot;&gt;Jingxuan Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Linzhuang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Bihui Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_G/0/1/0/all/0/1&quot;&gt;Guiyong Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dawei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Sibo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1&quot;&gt;Zhengbing Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Mingjun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bu_L/0/1/0/all/0/1&quot;&gt;Liping Bu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15866">
<title>ChatGPT &amp; Mechanical Engineering: Examining performance on the FE Mechanical Engineering and Undergraduate Exams. (arXiv:2309.15866v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2309.15866</link>
<description rdf:parseType="Literal">&lt;p&gt;The launch of ChatGPT at the end of 2022 generated large interest into
possible applications of artificial intelligence in STEM education and among
STEM professions. As a result many questions surrounding the capabilities of
generative AI tools inside and outside of the classroom have been raised and
are starting to be explored. This study examines the capabilities of ChatGPT
within the discipline of mechanical engineering. It aims to examine use cases
and pitfalls of such a technology in the classroom and professional settings.
ChatGPT was presented with a set of questions from junior and senior level
mechanical engineering exams provided at a large private university, as well as
a set of practice questions for the Fundamentals of Engineering Exam (FE) in
Mechanical Engineering. The responses of two ChatGPT models, one free to use
and one paid subscription, were analyzed. The paper found that the subscription
model (GPT-4) greatly outperformed the free version (GPT-3.5), achieving 76%
correct vs 51% correct, but the limitation of text only input on both models
makes neither likely to pass the FE exam. The results confirm findings in the
literature with regards to types of errors and pitfalls made by ChatGPT. It was
found that due to its inconsistency and a tendency to confidently produce
incorrect answers the tool is best suited for users with expert knowledge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frenkel_M/0/1/0/all/0/1&quot;&gt;Matthew Frenkel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emara_H/0/1/0/all/0/1&quot;&gt;Hebah Emara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15875">
<title>STAG: Enabling Low Latency and Low Staleness of GNN-based Services with Dynamic Graphs. (arXiv:2309.15875v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.15875</link>
<description rdf:parseType="Literal">&lt;p&gt;Many emerging user-facing services adopt Graph Neural Networks (GNNs) to
improve serving accuracy. When the graph used by a GNN model changes,
representations (embedding) of nodes in the graph should be updated
accordingly. However, the node representation update is too slow, resulting in
either long response latency of user queries (the inference is performed after
the update completes) or high staleness problem (the inference is performed
based on stale data). Our in-depth analysis shows that the slow update is
mainly due to neighbor explosion problem in graphs and duplicated computation.
Based on such findings, we propose STAG, a GNN serving framework that enables
low latency and low staleness of GNN-based services. It comprises a
collaborative serving mechanism and an additivity-based incremental propagation
strategy. With the collaborative serving mechanism, only part of node
representations are updated during the update phase, and the final
representations are calculated in the inference phase. It alleviates the
neighbor explosion problem. The additivity-based incremental propagation
strategy reuses intermediate data during the update phase, eliminating
duplicated computation problem. Experimental results show that STAG accelerates
the update phase by 1.3x~90.1x, and greatly reduces staleness time with a
slight increase in response latency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiawen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Quan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1&quot;&gt;Deze Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zhuo Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1&quot;&gt;Minyi Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15877">
<title>Neuro-Inspired Hierarchical Multimodal Learning. (arXiv:2309.15877v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.15877</link>
<description rdf:parseType="Literal">&lt;p&gt;Integrating and processing information from various sources or modalities are
critical for obtaining a comprehensive and accurate perception of the real
world. Drawing inspiration from neuroscience, we develop the
Information-Theoretic Hierarchical Perception (ITHP) model, which utilizes the
concept of information bottleneck. Distinct from most traditional fusion models
that aim to incorporate all modalities as input, our model designates the prime
modality as input, while the remaining modalities act as detectors in the
information pathway. Our proposed perception model focuses on constructing an
effective and compact information flow by achieving a balance between the
minimization of mutual information between the latent state and the input modal
state, and the maximization of mutual information between the latent states and
the remaining modal states. This approach leads to compact latent state
representations that retain relevant information while minimizing redundancy,
thereby substantially enhancing the performance of downstream tasks.
Experimental evaluations on both the MUStARD and CMU-MOSI datasets demonstrate
that our model consistently distills crucial information in multimodal learning
scenarios, outperforming state-of-the-art benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1&quot;&gt;Xiongye Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Gengshuo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_G/0/1/0/all/0/1&quot;&gt;Gaurav Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_D/0/1/0/all/0/1&quot;&gt;Defu Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shixuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yaxing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_T/0/1/0/all/0/1&quot;&gt;Tianqing Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1&quot;&gt;Mingxi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bogdan_P/0/1/0/all/0/1&quot;&gt;Paul Bogdan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15881">
<title>Enhancing Cross-Category Learning in Recommendation Systems with Multi-Layer Embedding Training. (arXiv:2309.15881v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.15881</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern DNN-based recommendation systems rely on training-derived embeddings
of sparse features. Input sparsity makes obtaining high-quality embeddings for
rarely-occurring categories harder as their representations are updated
infrequently. We demonstrate a training-time technique to produce superior
embeddings via effective cross-category learning and theoretically explain its
surprising effectiveness. The scheme, termed the multi-layer embeddings
training (MLET), trains embeddings using factorization of the embedding layer,
with an inner dimension higher than the target embedding dimension. For
inference efficiency, MLET converts the trained two-layer embedding into a
single-layer one thus keeping inference-time model size unchanged.
&lt;/p&gt;
&lt;p&gt;Empirical superiority of MLET is puzzling as its search space is not larger
than that of the single-layer embedding. The strong dependence of MLET on the
inner dimension is even more surprising. We develop a theory that explains both
of these behaviors by showing that MLET creates an adaptive update mechanism
modulated by the singular vectors of embeddings. When tested on multiple
state-of-the-art recommendation models for click-through rate (CTR) prediction
tasks, MLET consistently produces better models, especially for rare items. At
constant model quality, MLET allows embedding dimension, and model size,
reduction by up to 16x, and 5.8x on average, across the models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1&quot;&gt;Zihao Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghaemmaghami_B/0/1/0/all/0/1&quot;&gt;Benjamin Ghaemmaghami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Ashish Kumar Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_B/0/1/0/all/0/1&quot;&gt;Benjamin Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orshansky_L/0/1/0/all/0/1&quot;&gt;Leo Orshansky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erez_M/0/1/0/all/0/1&quot;&gt;Mattan Erez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orshansky_M/0/1/0/all/0/1&quot;&gt;Michael Orshansky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15942">
<title>Towards Efficient and Trustworthy AI Through Hardware-Algorithm-Communication Co-Design. (arXiv:2309.15942v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2309.15942</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence (AI) algorithms based on neural networks have been
designed for decades with the goal of maximising some measure of accuracy. This
has led to two undesired effects. First, model complexity has risen
exponentially when measured in terms of computation and memory requirements.
Second, state-of-the-art AI models are largely incapable of providing
trustworthy measures of their uncertainty, possibly `hallucinating&apos; their
answers and discouraging their adoption for decision-making in sensitive
applications.
&lt;/p&gt;
&lt;p&gt;With the goal of realising efficient and trustworthy AI, in this paper we
highlight research directions at the intersection of hardware and software
design that integrate physical insights into computational substrates,
neuroscientific principles concerning efficient information processing,
information-theoretic results on optimal uncertainty quantification, and
communication-theoretic guidelines for distributed processing. Overall, the
paper advocates for novel design methodologies that target not only accuracy
but also uncertainty quantification, while leveraging emerging computing
hardware architectures that move beyond the traditional von Neumann digital
computing paradigm to embrace in-memory, neuromorphic, and quantum computing
technologies. An important overarching principle of the proposed approach is to
view the stochasticity inherent in the computational substrate and in the
communication channels between processors as a resource to be leveraged for the
purpose of representing and processing classical and quantum uncertainty.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajendran_B/0/1/0/all/0/1&quot;&gt;Bipin Rajendran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simeone_O/0/1/0/all/0/1&quot;&gt;Osvaldo Simeone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Hashimi_B/0/1/0/all/0/1&quot;&gt;Bashir M. Al-Hashimi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15946">
<title>Unified Long-Term Time-Series Forecasting Benchmark. (arXiv:2309.15946v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.15946</link>
<description rdf:parseType="Literal">&lt;p&gt;In order to support the advancement of machine learning methods for
predicting time-series data, we present a comprehensive dataset designed
explicitly for long-term time-series forecasting. We incorporate a collection
of datasets obtained from diverse, dynamic systems and real-life records. Each
dataset is standardized by dividing it into training and test trajectories with
predetermined lookback lengths. We include trajectories of length up to $2000$
to ensure a reliable evaluation of long-term forecasting capabilities. To
determine the most effective model in diverse scenarios, we conduct an
extensive benchmarking analysis using classical and state-of-the-art models,
namely LSTM, DeepAR, NLinear, N-Hits, PatchTST, and LatentODE. Our findings
reveal intriguing performance comparisons among these models, highlighting the
dataset-dependent nature of model effectiveness. Notably, we introduce a custom
latent NLinear model and enhance DeepAR with a curriculum learning phase. Both
consistently outperform their vanilla counterparts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cyranka_J/0/1/0/all/0/1&quot;&gt;Jacek Cyranka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haponiuk_S/0/1/0/all/0/1&quot;&gt;Szymon Haponiuk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15979">
<title>Clinical Trial Recommendations Using Semantics-Based Inductive Inference and Knowledge Graph Embeddings. (arXiv:2309.15979v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2309.15979</link>
<description rdf:parseType="Literal">&lt;p&gt;Designing a new clinical trial entails many decisions, such as defining a
cohort and setting the study objectives to name a few, and therefore can
benefit from recommendations based on exhaustive mining of past clinical trial
records. Here, we propose a novel recommendation methodology, based on neural
embeddings trained on a first-of-a-kind knowledge graph of clinical trials. We
addressed several important research questions in this context, including
designing a knowledge graph (KG) for clinical trial data, effectiveness of
various KG embedding (KGE) methods for it, a novel inductive inference using
KGE, and its use in generating recommendations for clinical trial design. We
used publicly available data from clinicaltrials.gov for the study. Results
show that our recommendations approach achieves relevance scores of 70%-83%,
measured as the text similarity to actual clinical trial elements, and the most
relevant recommendation can be found near the top of list. Our study also
suggests potential improvement in training KGE using node semantics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Devarakonda_M/0/1/0/all/0/1&quot;&gt;Murthy V. Devarakonda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohanty_S/0/1/0/all/0/1&quot;&gt;Smita Mohanty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sunkishala_R/0/1/0/all/0/1&quot;&gt;Raja Rao Sunkishala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mallampalli_N/0/1/0/all/0/1&quot;&gt;Nag Mallampalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16025">
<title>Symbolic Imitation Learning: From Black-Box to Explainable Driving Policies. (arXiv:2309.16025v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.16025</link>
<description rdf:parseType="Literal">&lt;p&gt;Current methods of imitation learning (IL), primarily based on deep neural
networks, offer efficient means for obtaining driving policies from real-world
data but suffer from significant limitations in interpretability and
generalizability. These shortcomings are particularly concerning in
safety-critical applications like autonomous driving. In this paper, we address
these limitations by introducing Symbolic Imitation Learning (SIL), a
groundbreaking method that employs Inductive Logic Programming (ILP) to learn
driving policies which are transparent, explainable and generalisable from
available datasets. Utilizing the real-world highD dataset, we subject our
method to a rigorous comparative analysis against prevailing
neural-network-based IL methods. Our results demonstrate that SIL not only
enhances the interpretability of driving policies but also significantly
improves their applicability across varied driving situations. Hence, this work
offers a novel pathway to more reliable and safer autonomous driving systems,
underscoring the potential of integrating ILP into the domain of IL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharifi_I/0/1/0/all/0/1&quot;&gt;Iman Sharifi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fallah_S/0/1/0/all/0/1&quot;&gt;Saber Fallah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16035">
<title>MedEdit: Model Editing for Medical Question Answering with External Knowledge Bases. (arXiv:2309.16035v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.16035</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs), although powerful in general domains, often
perform poorly on domain-specific tasks like medical question answering (QA).
Moreover, they tend to function as &quot;black-boxes,&quot; making it challenging to
modify their behavior. Addressing this, our study delves into model editing
utilizing in-context learning, aiming to improve LLM responses without the need
for fine-tuning or retraining. Specifically, we propose a comprehensive
retrieval strategy to extract medical facts from an external knowledge base,
and then we incorporate them into the query prompt for the LLM. Focusing on
medical QA using the MedQA-SMILE dataset, we evaluate the impact of different
retrieval models and the number of facts provided to the LLM. Notably, our
edited Vicuna model exhibited an accuracy improvement from 44.46% to 48.54%.
This work underscores the potential of model editing to enhance LLM
performance, offering a practical approach to mitigate the challenges of
black-box LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yucheng Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shaochen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhengliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tianming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Ninghao Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16042">
<title>Towards Best Practices of Activation Patching in Language Models: Metrics and Methods. (arXiv:2309.16042v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.16042</link>
<description rdf:parseType="Literal">&lt;p&gt;Mechanistic interpretability seeks to understand the internal mechanisms of
machine learning models, where localization -- identifying the important model
components -- is a key step. Activation patching, also known as causal tracing
or interchange intervention, is a standard technique for this task (Vig et al.,
2020), but the literature contains many variants with little consensus on the
choice of hyperparameters or methodology. In this work, we systematically
examine the impact of methodological details in activation patching, including
evaluation metrics and corruption methods. In several settings of localization
and circuit discovery in language models, we find that varying these
hyperparameters could lead to disparate interpretability results. Backed by
empirical observations, we give conceptual arguments for why certain metrics or
methods may be preferred. Finally, we provide recommendations for the best
practices of activation patching going forwards.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fred Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nanda_N/0/1/0/all/0/1&quot;&gt;Neel Nanda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16064">
<title>Masked autoencoders are scalable learners of cellular morphology. (arXiv:2309.16064v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16064</link>
<description rdf:parseType="Literal">&lt;p&gt;Inferring biological relationships from cellular phenotypes in high-content
microscopy screens provides significant opportunity and challenge in biological
research. Prior results have shown that deep vision models can capture
biological signal better than hand-crafted features. This work explores how
weakly supervised and self-supervised deep learning approaches scale when
training larger models on larger datasets. Our results show that both CNN- and
ViT-based masked autoencoders significantly outperform weakly supervised
models. At the high-end of our scale, a ViT-L/8 trained on over 3.5-billion
unique crops sampled from 95-million microscopy images achieves relative
improvements as high as 28% over our best weakly supervised models at inferring
known biological relationships curated from public databases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kraus_O/0/1/0/all/0/1&quot;&gt;Oren Kraus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kenyon_Dean_K/0/1/0/all/0/1&quot;&gt;Kian Kenyon-Dean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saberian_S/0/1/0/all/0/1&quot;&gt;Saber Saberian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fallah_M/0/1/0/all/0/1&quot;&gt;Maryam Fallah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McLean_P/0/1/0/all/0/1&quot;&gt;Peter McLean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leung_J/0/1/0/all/0/1&quot;&gt;Jess Leung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_V/0/1/0/all/0/1&quot;&gt;Vasudev Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Ayla Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balakrishnan_J/0/1/0/all/0/1&quot;&gt;Jia Balakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Celik_S/0/1/0/all/0/1&quot;&gt;Safiye Celik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sypetkowski_M/0/1/0/all/0/1&quot;&gt;Maciej Sypetkowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1&quot;&gt;Chi Vicky Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morse_K/0/1/0/all/0/1&quot;&gt;Kristen Morse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Makes_M/0/1/0/all/0/1&quot;&gt;Maureen Makes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mabey_B/0/1/0/all/0/1&quot;&gt;Ben Mabey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Earnshaw_B/0/1/0/all/0/1&quot;&gt;Berton Earnshaw&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16082">
<title>Forgetting Private Textual Sequences in Language Models via Leave-One-Out Ensemble. (arXiv:2309.16082v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.16082</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent research has shown that language models have a tendency to memorize
rare or unique token sequences in the training corpus. After deploying a model,
practitioners might be asked to delete any personal information from the model
by individuals&apos; requests. Re-training the underlying model every time
individuals would like to practice their rights to be forgotten is
computationally expensive. We employ a teacher-student framework and propose a
novel leave-one-out ensemble method to unlearn the targeted textual sequences
that need to be forgotten from the model. In our approach, multiple teachers
are trained on disjoint sets; for each targeted sequence to be removed, we
exclude the teacher trained on the set containing this sequence and aggregate
the predictions from remaining teachers to provide supervision during
fine-tuning. Experiments on LibriSpeech and WikiText-103 datasets show that the
proposed method achieves superior privacy-utility trade-offs than other
counterparts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhe Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1&quot;&gt;Ozlem Kalinli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16090">
<title>TPE: Towards Better Compositional Reasoning over Conceptual Tools with Multi-persona Collaboration. (arXiv:2309.16090v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2309.16090</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have demonstrated exceptional performance in
planning the use of various functional tools, such as calculators and
retrievers, particularly in question-answering tasks. In this paper, we expand
the definition of these tools, centering on conceptual tools within the context
of dialogue systems. A conceptual tool specifies a cognitive concept that aids
systematic or investigative thought. These conceptual tools play important
roles in practice, such as multiple psychological or tutoring strategies being
dynamically applied in a single turn to compose helpful responses. To further
enhance the reasoning and planning capability of LLMs with these conceptual
tools, we introduce a multi-persona collaboration framework: Think-Plan-Execute
(TPE). This framework decouples the response generation process into three
distinct roles: Thinker, Planner, and Executor. Specifically, the Thinker
analyzes the internal status exhibited in the dialogue context, such as user
emotions and preferences, to formulate a global guideline. The Planner then
generates executable plans to call different conceptual tools (e.g., sources or
strategies), while the Executor compiles all intermediate results into a
coherent response. This structured approach not only enhances the
explainability and controllability of responses but also reduces token
redundancy. We demonstrate the effectiveness of TPE across various dialogue
response generation tasks, including multi-source (FoCus) and multi-strategy
interactions (CIMA and PsyQA). This reveals its potential to handle real-world
dialogue interactions that require more complicated tool learning beyond just
functional tools. The full code and data will be released for reproduction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongru Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Huimin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lingzhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1&quot;&gt;Minda Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_B/0/1/0/all/0/1&quot;&gt;Boyang Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Hongyuan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1&quot;&gt;Fei Mi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1&quot;&gt;Kam-Fai Wong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16096">
<title>Adversarial Examples Might be Avoidable: The Role of Data Concentration in Adversarial Robustness. (arXiv:2309.16096v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.16096</link>
<description rdf:parseType="Literal">&lt;p&gt;The susceptibility of modern machine learning classifiers to adversarial
examples has motivated theoretical results suggesting that these might be
unavoidable. However, these results can be too general to be applicable to
natural data distributions. Indeed, humans are quite robust for tasks involving
vision. This apparent conflict motivates a deeper dive into the question: Are
adversarial examples truly unavoidable? In this work, we theoretically
demonstrate that a key property of the data distribution -- concentration on
small-volume subsets of the input space -- determines whether a robust
classifier exists. We further demonstrate that, for a data distribution
concentrated on a union of low-dimensional linear subspaces, exploiting data
structure naturally leads to classifiers that enjoy good robustness guarantees,
improving upon methods for provable certification in certain regimes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pal_A/0/1/0/all/0/1&quot;&gt;Ambar Pal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sulam_J/0/1/0/all/0/1&quot;&gt;Jeremias Sulam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vidal_R/0/1/0/all/0/1&quot;&gt;Ren&amp;#xe9; Vidal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16102">
<title>Discovering Utility-driven Interval Rules. (arXiv:2309.16102v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2309.16102</link>
<description rdf:parseType="Literal">&lt;p&gt;For artificial intelligence, high-utility sequential rule mining (HUSRM) is a
knowledge discovery method that can reveal the associations between events in
the sequences. Recently, abundant methods have been proposed to discover
high-utility sequence rules. However, the existing methods are all related to
point-based sequences. Interval events that persist for some time are common.
Traditional interval-event sequence knowledge discovery tasks mainly focus on
pattern discovery, but patterns cannot reveal the correlation between interval
events well. Moreover, the existing HUSRM algorithms cannot be directly applied
to interval-event sequences since the relation in interval-event sequences is
much more intricate than those in point-based sequences. In this work, we
propose a utility-driven interval rule mining (UIRMiner) algorithm that can
extract all utility-driven interval rules (UIRs) from the interval-event
sequence database to solve the problem. In UIRMiner, we first introduce a
numeric encoding relation representation, which can save much time on relation
computation and storage on relation representation. Furthermore, to shrink the
search space, we also propose a complement pruning strategy, which incorporates
the utility upper bound with the relation. Finally, plentiful experiments
implemented on both real-world and synthetic datasets verify that UIRMiner is
an effective and efficient algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chunkai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1&quot;&gt;Maohua Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_H/0/1/0/all/0/1&quot;&gt;Huaijin Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_W/0/1/0/all/0/1&quot;&gt;Wensheng Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1&quot;&gt;Philip S. Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16108">
<title>Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words. (arXiv:2309.16108v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16108</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformer (ViT) has emerged as a powerful architecture in the realm
of modern computer vision. However, its application in certain imaging fields,
such as microscopy and satellite imaging, presents unique challenges. In these
domains, images often contain multiple channels, each carrying semantically
distinct and independent information. Furthermore, the model must demonstrate
robustness to sparsity in input channels, as they may not be densely available
during training or testing. In this paper, we propose a modification to the ViT
architecture that enhances reasoning across the input channels and introduce
Hierarchical Channel Sampling (HCS) as an additional regularization technique
to ensure robustness when only partial channels are presented during test time.
Our proposed model, ChannelViT, constructs patch tokens independently from each
input channel and utilizes a learnable channel embedding that is added to the
patch tokens, similar to positional embeddings. We evaluate the performance of
ChannelViT on ImageNet, JUMP-CP (microscopy cell imaging), and So2Sat
(satellite imaging). Our results show that ChannelViT outperforms ViT on
classification tasks and generalizes well, even when a subset of input channels
is used during testing. Across our experiments, HCS proves to be a powerful
regularizer, independent of the architecture employed, suggesting itself as a
straightforward technique for robust ViT training. Lastly, we find that
ChannelViT generalizes effectively even when there is limited access to all
channels during training, highlighting its potential for multi-channel imaging
under real-world conditions with sparse sensors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1&quot;&gt;Yujia Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sivanandan_S/0/1/0/all/0/1&quot;&gt;Srinivasan Sivanandan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karaletsos_T/0/1/0/all/0/1&quot;&gt;Theofanis Karaletsos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16117">
<title>E2Net: Resource-Efficient Continual Learning with Elastic Expansion Network. (arXiv:2309.16117v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.16117</link>
<description rdf:parseType="Literal">&lt;p&gt;Continual Learning methods are designed to learn new tasks without erasing
previous knowledge. However, Continual Learning often requires massive
computational power and storage capacity for satisfactory performance. In this
paper, we propose a resource-efficient continual learning method called the
Elastic Expansion Network (E2Net). Leveraging core subnet distillation and
precise replay sample selection, E2Net achieves superior average accuracy and
diminished forgetting within the same computational and storage constraints,
all while minimizing processing time. In E2Net, we propose Representative
Network Distillation to identify the representative core subnet by assessing
parameter quantity and output similarity with the working network, distilling
analogous subnets within the working network to mitigate reliance on rehearsal
buffers and facilitating knowledge transfer across previous tasks. To enhance
storage resource utilization, we then propose Subnet Constraint Experience
Replay to optimize rehearsal efficiency through a sample storage strategy based
on the structures of representative networks. Extensive experiments conducted
predominantly on cloud environments with diverse datasets and also spanning the
edge environment demonstrate that E2Net consistently outperforms
state-of-the-art methods. In addition, our method outperforms competitors in
terms of both storage and computational requirements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;RuiQi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diao_B/0/1/0/all/0/1&quot;&gt;Boyu Diao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Libo Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_Z/0/1/0/all/0/1&quot;&gt;Zhulin An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yongjun Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16119">
<title>ModuLoRA: Finetuning 3-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers. (arXiv:2309.16119v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.16119</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a memory-efficient finetuning algorithm for large language models
(LLMs) that supports finetuning LLMs with 65B parameters in 3-bit or 4-bit
precision on as little as one 48GB GPU. Our method, modular low-rank adaptation
(ModuLoRA), integrates any user-specified weight quantizer with finetuning via
low-rank adapters (LoRAs). Our approach relies on a simple
quantization-agnostic backward pass that adaptively materializes low-precision
LLM weights from a custom black-box quantization module. This approach enables
finetuning 3-bit LLMs for the first time--leveraging state-of-the-art 3-bit
OPTQ quantization often outperforms finetuning that relies on less
sophisticated 4-bit and 8-bit methods. In our experiments, ModuLoRA attains
competitive performance on text classification, natural language infernece, and
instruction following tasks using significantly less memory than existing
approaches, and we also surpass the state-of-the-art ROUGE score on a popular
summarization task. We release ModuLoRA together with a series of low-precision
models--including the first family of 3-bit instruction following Alpaca
LLMs--as part of LLMTOOLS, a user-friendly library for quantizing, running, and
finetuning LLMs on consumer GPUs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1&quot;&gt;Junjie Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1&quot;&gt;Jiahao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yingheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1&quot;&gt;Christopher De Sa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuleshov_V/0/1/0/all/0/1&quot;&gt;Volodymyr Kuleshov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16143">
<title>Generative Semi-supervised Learning with Meta-Optimized Synthetic Samples. (arXiv:2309.16143v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.16143</link>
<description rdf:parseType="Literal">&lt;p&gt;Semi-supervised learning (SSL) is a promising approach for training deep
classification models using labeled and unlabeled datasets. However, existing
SSL methods rely on a large unlabeled dataset, which may not always be
available in many real-world applications due to legal constraints (e.g.,
GDPR). In this paper, we investigate the research question: Can we train SSL
models without real unlabeled datasets? Instead of using real unlabeled
datasets, we propose an SSL method using synthetic datasets generated from
generative foundation models trained on datasets containing millions of samples
in diverse domains (e.g., ImageNet). Our main concepts are identifying
synthetic samples that emulate unlabeled samples from generative foundation
models and training classifiers using these synthetic samples. To achieve this,
our method is formulated as an alternating optimization problem: (i)
meta-learning of generative foundation models and (ii) SSL of classifiers using
real labeled and synthetic unlabeled samples. For (i), we propose a
meta-learning objective that optimizes latent variables to generate samples
that resemble real labeled samples and minimize the validation loss. For (ii),
we propose a simple unsupervised loss function that regularizes the feature
extractors of classifiers to maximize the performance improvement obtained from
synthetic samples. We confirm that our method outperforms baselines using
generative foundation models on SSL. We also demonstrate that our methods
outperform SSL using real unlabeled datasets in scenarios with extremely small
amounts of labeled datasets. This suggests that synthetic samples have the
potential to provide improvement gains more efficiently than real unlabeled
data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamaguchi_S/0/1/0/all/0/1&quot;&gt;Shin&amp;#x27;ya Yamaguchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16146">
<title>T-COL: Generating Counterfactual Explanations for General User Preferences on Variable Machine Learning Systems. (arXiv:2309.16146v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2309.16146</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning (ML) based systems have been suffering a lack of
interpretability. To address this problem, counterfactual explanations (CEs)
have been proposed. CEs are unique as they provide workable suggestions to
users, in addition to explaining why a certain outcome was predicted. However,
the application of CEs has been hindered by two main challenges, namely general
user preferences and variable ML systems. User preferences, in particular, tend
to be general rather than specific feature values. Additionally, CEs need to be
customized to suit the variability of ML models, while also maintaining
robustness even when these validation models change. To overcome these
challenges, we propose several possible general user preferences that have been
validated by user research and map them to the properties of CEs. We also
introduce a new method called \uline{T}ree-based \uline{C}onditions
\uline{O}ptional \uline{L}inks (T-COL), which has two optional structures and
several groups of conditions for generating CEs that can be adapted to general
user preferences. Meanwhile, a group of conditions lead T-COL to generate more
robust CEs that have higher validity when the ML model is replaced. We compared
the properties of CEs generated by T-COL experimentally under different user
preferences and demonstrated that T-COL is better suited for accommodating user
preferences and variable ML systems compared to baseline methods including
Large Language Models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Ming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Daling Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wenfang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1&quot;&gt;Shi Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yifei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16150">
<title>AE-GPT: Using Large Language Models to Extract Adverse Events from Surveillance Reports-A Use Case with Influenza Vaccine Adverse Events. (arXiv:2309.16150v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.16150</link>
<description rdf:parseType="Literal">&lt;p&gt;Though Vaccines are instrumental in global health, mitigating infectious
diseases and pandemic outbreaks, they can occasionally lead to adverse events
(AEs). Recently, Large Language Models (LLMs) have shown promise in effectively
identifying and cataloging AEs within clinical reports. Utilizing data from the
Vaccine Adverse Event Reporting System (VAERS) from 1990 to 2016, this study
particularly focuses on AEs to evaluate LLMs&apos; capability for AE extraction. A
variety of prevalent LLMs, including GPT-2, GPT-3 variants, GPT-4, and Llama 2,
were evaluated using Influenza vaccine as a use case. The fine-tuned GPT 3.5
model (AE-GPT) stood out with a 0.704 averaged micro F1 score for strict match
and 0.816 for relaxed match. The encouraging performance of the AE-GPT
underscores LLMs&apos; potential in processing medical data, indicating a
significant stride towards advanced AE detection, thus presumably generalizable
to other AE extraction tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yiming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianfu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jianping He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1&quot;&gt;Cui Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16161">
<title>Leveraging Untrustworthy Commands for Multi-Robot Coordination in Unpredictable Environments: A Bandit Submodular Maximization Approach. (arXiv:2309.16161v1 [eess.SY])</title>
<link>http://arxiv.org/abs/2309.16161</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of multi-agent coordination in unpredictable and
partially-observable environments with untrustworthy external commands. The
commands are actions suggested to the robots, and are untrustworthy in that
their performance guarantees, if any, are unknown. Such commands may be
generated by human operators or machine learning algorithms and, although
untrustworthy, can often increase the robots&apos; performance in complex
multi-robot tasks. We are motivated by complex multi-robot tasks such as target
tracking, environmental mapping, and area monitoring. Such tasks are often
modeled as submodular maximization problems due to the information overlap
among the robots. We provide an algorithm, Meta Bandit Sequential Greedy
(MetaBSG), which enjoys performance guarantees even when the external commands
are arbitrarily bad. MetaBSG leverages a meta-algorithm to learn whether the
robots should follow the commands or a recently developed submodular
coordination algorithm, Bandit Sequential Greedy (BSG) [1], which has
performance guarantees even in unpredictable and partially-observable
environments. Particularly, MetaBSG asymptotically can achieve the better
performance out of the commands and the BSG algorithm, quantifying its
suboptimality against the optimal time-varying multi-robot actions in
hindsight. Thus, MetaBSG can be interpreted as robustifying the untrustworthy
commands. We validate our algorithm in simulated scenarios of multi-target
tracking.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zirui Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tzoumas_V/0/1/0/all/0/1&quot;&gt;Vasileios Tzoumas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16166">
<title>CoinRun: Solving Goal Misgeneralisation. (arXiv:2309.16166v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2309.16166</link>
<description rdf:parseType="Literal">&lt;p&gt;Goal misgeneralisation is a key challenge in AI alignment -- the task of
getting powerful Artificial Intelligences to align their goals with human
intentions and human morality. In this paper, we show how the ACE (Algorithm
for Concept Extrapolation) agent can solve one of the key standard challenges
in goal misgeneralisation: the CoinRun challenge. It uses no new reward
information in the new environment. This points to how autonomous agents could
be trusted to act in human interests, even in novel and critical situations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Armstrong_S/0/1/0/all/0/1&quot;&gt;Stuart Armstrong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maranhao_A/0/1/0/all/0/1&quot;&gt;Alexandre Maranh&amp;#xe3;o&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daniels_Koch_O/0/1/0/all/0/1&quot;&gt;Oliver Daniels-Koch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leask_P/0/1/0/all/0/1&quot;&gt;Patrick Leask&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gorman_R/0/1/0/all/0/1&quot;&gt;Rebecca Gorman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16180">
<title>A More General Theory of Diagnosis from First Principles. (arXiv:2309.16180v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2309.16180</link>
<description rdf:parseType="Literal">&lt;p&gt;Model-based diagnosis has been an active research topic in different
communities including artificial intelligence, formal methods, and control.
This has led to a set of disparate approaches addressing different classes of
systems and seeking different forms of diagnoses. In this paper, we resolve
such disparities by generalising Reiter&apos;s theory to be agnostic to the types of
systems and diagnoses considered. This more general theory of diagnosis from
first principles defines the minimal diagnosis as the set of preferred
diagnosis candidates in a search space of hypotheses. Computing the minimal
diagnosis is achieved by exploring the space of diagnosis hypotheses, testing
sets of hypotheses for consistency with the system&apos;s model and the observation,
and generating conflicts that rule out successors and other portions of the
search space. Under relatively mild assumptions, our algorithms correctly
compute the set of preferred diagnosis candidates. The main difficulty here is
that the search space is no longer a powerset as in Reiter&apos;s theory, and that,
as consequence, many of the implicit properties (such as finiteness of the
search space) no longer hold. The notion of conflict also needs to be
generalised and we present such a more general notion. We present two
implementations of these algorithms, using test solvers based on satisfiability
and heuristic search, respectively, which we evaluate on instances from two
real world discrete event problems. Despite the greater generality of our
theory, these implementations surpass the special purpose algorithms designed
for discrete event systems, and enable solving instances that were out of reach
of existing diagnosis approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grastien_A/0/1/0/all/0/1&quot;&gt;Alban Grastien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haslum_P/0/1/0/all/0/1&quot;&gt;Patrik Haslum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thiebaux_S/0/1/0/all/0/1&quot;&gt;Sylvie Thi&amp;#xe9;baux&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16211">
<title>VDC: Versatile Data Cleanser for Detecting Dirty Samples via Visual-Linguistic Inconsistency. (arXiv:2309.16211v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16211</link>
<description rdf:parseType="Literal">&lt;p&gt;The role of data in building AI systems has recently been emphasized by the
emerging concept of data-centric AI. Unfortunately, in the real-world, datasets
may contain dirty samples, such as poisoned samples from backdoor attack, noisy
labels in crowdsourcing, and even hybrids of them. The presence of such dirty
samples makes the DNNs vunerable and unreliable.Hence, it is critical to detect
dirty samples to improve the quality and realiability of dataset. Existing
detectors only focus on detecting poisoned samples or noisy labels, that are
often prone to weak generalization when dealing with dirty samples from other
domains.In this paper, we find a commonality of various dirty samples is
visual-linguistic inconsistency between images and associated labels. To
capture the semantic inconsistency between modalities, we propose versatile
data cleanser (VDC) leveraging the surpassing capabilities of multimodal large
language models (MLLM) in cross-modal alignment and reasoning.It consists of
three consecutive modules: the visual question generation module to generate
insightful questions about the image; the visual question answering module to
acquire the semantics of the visual content by answering the questions with
MLLM; followed by the visual answer evaluation module to evaluate the
inconsistency.Extensive experiments demonstrate its superior performance and
generalization to various categories and types of dirty samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zihao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mingda Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1&quot;&gt;Shaokui Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Bingzhe Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Baoyuan Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16220">
<title>Unmasking the Chameleons: A Benchmark for Out-of-Distribution Detection in Medical Tabular Data. (arXiv:2309.16220v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.16220</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite their success, Machine Learning (ML) models do not generalize
effectively to data not originating from the training distribution. To reliably
employ ML models in real-world healthcare systems and avoid inaccurate
predictions on out-of-distribution (OOD) data, it is crucial to detect OOD
samples. Numerous OOD detection approaches have been suggested in other fields
- especially in computer vision - but it remains unclear whether the challenge
is resolved when dealing with medical tabular data. To answer this pressing
need, we propose an extensive reproducible benchmark to compare different
methods across a suite of tests including both near and far OODs. Our benchmark
leverages the latest versions of eICU and MIMIC-IV, two public datasets
encompassing tens of thousands of ICU patients in several hospitals. We
consider a wide array of density-based methods and SOTA post-hoc detectors
across diverse predictive architectures, including MLP, ResNet, and
Transformer. Our findings show that i) the problem appears to be solved for
far-OODs, but remains open for near-OODs; ii) post-hoc methods alone perform
poorly, but improve substantially when coupled with distance-based mechanisms;
iii) the transformer architecture is far less overconfident compared to MLP and
ResNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azizmalayeri_M/0/1/0/all/0/1&quot;&gt;Mohammad Azizmalayeri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abu_Hanna_A/0/1/0/all/0/1&quot;&gt;Ameen Abu-Hanna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cina_G/0/1/0/all/0/1&quot;&gt;Giovanni Cin&amp;#xe1;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16223">
<title>GInX-Eval: Towards In-Distribution Evaluation of Graph Neural Network Explanations. (arXiv:2309.16223v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2309.16223</link>
<description rdf:parseType="Literal">&lt;p&gt;Diverse explainability methods of graph neural networks (GNN) have recently
been developed to highlight the edges and nodes in the graph that contribute
the most to the model predictions. However, it is not clear yet how to evaluate
the correctness of those explanations, whether it is from a human or a model
perspective. One unaddressed bottleneck in the current evaluation procedure is
the problem of out-of-distribution explanations, whose distribution differs
from those of the training data. This important issue affects existing
evaluation metrics such as the popular faithfulness or fidelity score. In this
paper, we show the limitations of faithfulness metrics. We propose GInX-Eval
(Graph In-distribution eXplanation Evaluation), an evaluation procedure of
graph explanations that overcomes the pitfalls of faithfulness and offers new
insights on explainability methods. Using a retraining strategy, the GInX score
measures how informative removed edges are for the model and the EdgeRank score
evaluates if explanatory edges are correctly ordered by their importance.
GInX-Eval verifies if ground-truth explanations are instructive to the GNN
model. In addition, it shows that many popular methods, including
gradient-based methods, produce explanations that are not better than a random
designation of edges as important subgraphs, challenging the findings of
current works in the area. Results with GInX-Eval are consistent across
multiple datasets and align with human evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amara_K/0/1/0/all/0/1&quot;&gt;Kenza Amara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+El_Assady_M/0/1/0/all/0/1&quot;&gt;Mennatallah El-Assady&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ying_R/0/1/0/all/0/1&quot;&gt;Rex Ying&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16235">
<title>Language models in molecular discovery. (arXiv:2309.16235v1 [physics.chem-ph])</title>
<link>http://arxiv.org/abs/2309.16235</link>
<description rdf:parseType="Literal">&lt;p&gt;The success of language models, especially transformer-based architectures,
has trickled into other domains giving rise to &quot;scientific language models&quot;
that operate on small molecules, proteins or polymers. In chemistry, language
models contribute to accelerating the molecule discovery cycle as evidenced by
promising recent findings in early-stage drug discovery. Here, we review the
role of language models in molecular discovery, underlining their strength in
de novo drug design, property prediction and reaction chemistry. We highlight
valuable open-source software assets thus lowering the entry barrier to the
field of scientific language modeling. Last, we sketch a vision for future
molecular design that combines a chatbot interface with access to computational
chemistry tools. Our contribution serves as a valuable resource for
researchers, chemists, and AI enthusiasts interested in understanding how
language models can and will be used to accelerate chemical discovery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Janakarajan_N/0/1/0/all/0/1&quot;&gt;Nikita Janakarajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Erdmann_T/0/1/0/all/0/1&quot;&gt;Tim Erdmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Swaminathan_S/0/1/0/all/0/1&quot;&gt;Sarath Swaminathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Laino_T/0/1/0/all/0/1&quot;&gt;Teodoro Laino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Born_J/0/1/0/all/0/1&quot;&gt;Jannis Born&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16240">
<title>Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints. (arXiv:2309.16240v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.16240</link>
<description rdf:parseType="Literal">&lt;p&gt;The increasing capabilities of large language models (LLMs) raise
opportunities for artificial general intelligence but concurrently amplify
safety concerns, such as potential misuse of AI systems, necessitating
effective AI alignment. Reinforcement Learning from Human Feedback (RLHF) has
emerged as a promising pathway towards AI alignment but brings forth challenges
due to its complexity and dependence on a separate reward model. Direct
Preference Optimization (DPO) has been proposed as an alternative, and it
remains equivalent to RLHF under the reverse KL regularization constraint. This
paper presents $f$-DPO, a generalized approach to DPO by incorporating diverse
divergence constraints. We show that under certain $f$-divergences, including
Jensen-Shannon divergence, forward KL divergences and $\alpha$-divergences, the
complex relationship between the reward and optimal policy can also be
simplified by addressing the Karush-Kuhn-Tucker conditions. This eliminates the
need for estimating the normalizing constant in the Bradley-Terry model and
enables a tractable mapping between the reward function and the optimal policy.
Our approach optimizes LLMs to align with human preferences in a more efficient
and supervised manner under a broad set of divergence constraints. Empirically,
adopting these divergences ensures a balance between alignment performance and
generation diversity. Importantly, $f$-DPO outperforms PPO-based methods in
divergence efficiency, and divergence constraints directly influence expected
calibration error (ECE).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chaoqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yibo Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chenghao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Han Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuxin Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16257">
<title>Nondestructive chicken egg fertility detection using CNN-transfer learning algorithms. (arXiv:2309.16257v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16257</link>
<description rdf:parseType="Literal">&lt;p&gt;This study explored the application of CNN-Transfer Learning for
nondestructive chicken egg fertility detection for precision poultry hatchery
practices. Four models, VGG16, ResNet50, InceptionNet, and MobileNet, were
trained and evaluated on a dataset (200 single egg images) using augmented
images (rotation, flip, scale, translation, and reflection). Although the
training results demonstrated that all models achieved high accuracy,
indicating their ability to accurately learn and classify chicken eggs&apos;
fertility state, when evaluated on the testing set, variations in accuracy and
performance were observed. InceptionNet exhibited the best overall performance,
accurately classifying fertile and non-fertile eggs. It demonstrated excellent
performance in both training and testing sets in all parameters of the
evaluation metrics. In testing set, it achieved an accuracy of 0.98, a
sensitivity of 1 for detecting fertile eggs, and a specificity of 0.96 for
identifying non-fertile eggs. The higher performance is attributed to its
unique architecture efficiently capturing features at different scales leading
to improved accuracy and robustness. Further optimization and fine-tuning of
the models might necessary to address the limitations in accurately detecting
fertile and non-fertile eggs in case of other models. This study highlighted
the potential of CNN-Transfer Learning for nondestructive fertility detection
and emphasizes the need for further research to enhance the models&apos;
capabilities and ensure accurate classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saifullah_S/0/1/0/all/0/1&quot;&gt;Shoffan Saifullah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Drezewski_R/0/1/0/all/0/1&quot;&gt;Rafal Drezewski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yudhana_A/0/1/0/all/0/1&quot;&gt;Anton Yudhana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pranolo_A/0/1/0/all/0/1&quot;&gt;Andri Pranolo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaswijanti_W/0/1/0/all/0/1&quot;&gt;Wilis Kaswijanti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suryotomo_A/0/1/0/all/0/1&quot;&gt;Andiko Putro Suryotomo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Putra_S/0/1/0/all/0/1&quot;&gt;Seno Aji Putra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khaliduzzaman_A/0/1/0/all/0/1&quot;&gt;Alin Khaliduzzaman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabuwono_A/0/1/0/all/0/1&quot;&gt;Anton Satria Prabuwono&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Japkowicz_N/0/1/0/all/0/1&quot;&gt;Nathalie Japkowicz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16258">
<title>QonFusion -- Quantum Approaches to Gaussian Random Variables: Applications in Stable Diffusion and Brownian Motion. (arXiv:2309.16258v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2309.16258</link>
<description rdf:parseType="Literal">&lt;p&gt;In the present study, we delineate a strategy focused on non-parametric
quantum circuits for the generation of Gaussian random variables (GRVs). This
quantum-centric approach serves as a substitute for conventional pseudorandom
number generators (PRNGs), such as the \textbf{torch.rand} function in PyTorch.
The principal theme of our research is the incorporation of Quantum Random
Number Generators (QRNGs) into classical models of diffusion. Notably, our
Quantum Gaussian Random Variable Generator fulfills dual roles, facilitating
simulations in both Stable Diffusion (SD) and Brownian Motion (BM). This
diverges markedly from prevailing methods that utilize parametric quantum
circuits (PQCs), often in conjunction with variational quantum eigensolvers
(VQEs). Although conventional techniques can accurately approximate ground
states in complex systems or model elaborate probability distributions, they
require a computationally demanding optimization process to tune parameters.
Our non-parametric strategy obviates this necessity. To facilitate assimilating
our methodology into existing computational frameworks, we put forward
QonFusion, a Python library congruent with both PyTorch and PennyLane,
functioning as a bridge between classical and quantum computational paradigms.
We validate QonFusion through extensive statistical testing, including tests
which confirm the statistical equivalence of the Gaussian samples from our
quantum approach to classical counterparts within defined significance limits.
QonFusion is available at
\url{https://boltzmannentropy.github.io/qonfusion.github.io/} to reproduce all
findings here.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Kashani_S/0/1/0/all/0/1&quot;&gt;Shlomo Kashani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16263">
<title>Cooperation Dynamics in Multi-Agent Systems: Exploring Game-Theoretic Scenarios with Mean-Field Equilibria. (arXiv:2309.16263v1 [cs.GT])</title>
<link>http://arxiv.org/abs/2309.16263</link>
<description rdf:parseType="Literal">&lt;p&gt;Cooperation is fundamental in Multi-Agent Systems (MAS) and Multi-Agent
Reinforcement Learning (MARL), often requiring agents to balance individual
gains with collective rewards. In this regard, this paper aims to investigate
strategies to invoke cooperation in game-theoretic scenarios, namely the
Iterated Prisoner&apos;s Dilemma, where agents must optimize both individual and
group outcomes. Existing cooperative strategies are analyzed for their
effectiveness in promoting group-oriented behavior in repeated games.
Modifications are proposed where encouraging group rewards will also result in
a higher individual gain, addressing real-world dilemmas seen in distributed
systems. The study extends to scenarios with exponentially growing agent
populations ($N \longrightarrow +\infty$), where traditional computation and
equilibrium determination are challenging. Leveraging mean-field game theory,
equilibrium solutions and reward structures are established for infinitely
large agent sets in repeated games. Finally, practical insights are offered
through simulations using the Multi Agent-Posthumous Credit Assignment trainer,
and the paper explores adapting simulation algorithms to create scenarios
favoring cooperation for group rewards. These practical implementations bridge
theoretical concepts with real-world applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sathi_V/0/1/0/all/0/1&quot;&gt;Vaigarai Sathi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaik_S/0/1/0/all/0/1&quot;&gt;Sabahat Shaik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nidamanuri_J/0/1/0/all/0/1&quot;&gt;Jaswanth Nidamanuri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16275">
<title>UPB @ ACTI: Detecting Conspiracies using fine tuned Sentence Transformers. (arXiv:2309.16275v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.16275</link>
<description rdf:parseType="Literal">&lt;p&gt;Conspiracy theories have become a prominent and concerning aspect of online
discourse, posing challenges to information integrity and societal trust. As
such, we address conspiracy theory detection as proposed by the ACTI @ EVALITA
2023 shared task. The combination of pre-trained sentence Transformer models
and data augmentation techniques enabled us to secure first place in the final
leaderboard of both sub-tasks. Our methodology attained F1 scores of 85.71% in
the binary classification and 91.23% for the fine-grained conspiracy topic
classification, surpassing other competing systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paraschiv_A/0/1/0/all/0/1&quot;&gt;Andrei Paraschiv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dascalu_M/0/1/0/all/0/1&quot;&gt;Mihai Dascalu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16286">
<title>Generalizable Heterogeneous Federated Cross-Correlation and Instance Similarity Learning. (arXiv:2309.16286v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.16286</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning is an important privacy-preserving multi-party learning
paradigm, involving collaborative learning with others and local updating on
private data. Model heterogeneity and catastrophic forgetting are two crucial
challenges, which greatly limit the applicability and generalizability. This
paper presents a novel FCCL+, federated correlation and similarity learning
with non-target distillation, facilitating the both intra-domain
discriminability and inter-domain generalization. For heterogeneity issue, we
leverage irrelevant unlabeled public data for communication between the
heterogeneous participants. We construct cross-correlation matrix and align
instance similarity distribution on both logits and feature levels, which
effectively overcomes the communication barrier and improves the generalizable
ability. For catastrophic forgetting in local updating stage, FCCL+ introduces
Federated Non Target Distillation, which retains inter-domain knowledge while
avoiding the optimization conflict issue, fulling distilling privileged
inter-domain information through depicting posterior classes relation.
Considering that there is no standard benchmark for evaluating existing
heterogeneous federated learning under the same setting, we present a
comprehensive benchmark with extensive representative methods under four domain
shift scenarios, supporting both heterogeneous and homogeneous federated
settings. Empirical results demonstrate the superiority of our method and the
efficiency of modules on various scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wenke Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1&quot;&gt;Mang Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zekun Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1&quot;&gt;Bo Du&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16289">
<title>LawBench: Benchmarking Legal Knowledge of Large Language Models. (arXiv:2309.16289v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.16289</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have demonstrated strong capabilities in various
aspects. However, when applying them to the highly specialized, safe-critical
legal domain, it is unclear how much legal knowledge they possess and whether
they can reliably perform legal-related tasks. To address this gap, we propose
a comprehensive evaluation benchmark LawBench. LawBench has been meticulously
crafted to have precise assessment of the LLMs&apos; legal capabilities from three
cognitive levels: (1) Legal knowledge memorization: whether LLMs can memorize
needed legal concepts, articles and facts; (2) Legal knowledge understanding:
whether LLMs can comprehend entities, events and relationships within legal
text; (3) Legal knowledge applying: whether LLMs can properly utilize their
legal knowledge and make necessary reasoning steps to solve realistic legal
tasks. LawBench contains 20 diverse tasks covering 5 task types: single-label
classification (SLC), multi-label classification (MLC), regression, extraction
and generation. We perform extensive evaluations of 51 LLMs on LawBench,
including 20 multilingual LLMs, 22 Chinese-oriented LLMs and 9 legal specific
LLMs. The results show that GPT-4 remains the best-performing LLM in the legal
domain, surpassing the others by a significant margin. While fine-tuning LLMs
on legal specific text brings certain improvements, we are still a long way
from obtaining usable and reliable LLMs in legal tasks. All data, model
predictions and evaluation code are released in
https://github.com/open-compass/LawBench/. We hope this benchmark provides
in-depth understanding of the LLMs&apos; domain-specified capabilities and speed up
the development of LLMs in the legal domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Fei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1&quot;&gt;Dawei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1&quot;&gt;Fengzhe Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1&quot;&gt;Zhuo Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Songyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1&quot;&gt;Zongwen Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_J/0/1/0/all/0/1&quot;&gt;Jidong Ge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16291">
<title>Efficiency Separation between RL Methods: Model-Free, Model-Based and Goal-Conditioned. (arXiv:2309.16291v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.16291</link>
<description rdf:parseType="Literal">&lt;p&gt;We prove a fundamental limitation on the efficiency of a wide class of
Reinforcement Learning (RL) algorithms. This limitation applies to model-free
RL methods as well as a broad range of model-based methods, such as planning
with tree search.
&lt;/p&gt;
&lt;p&gt;Under an abstract definition of this class, we provide a family of RL
problems for which these methods suffer a lower bound exponential in the
horizon for their interactions with the environment to find an optimal
behavior. However, there exists a method, not tailored to this specific family
of problems, which can efficiently solve the problems in the family.
&lt;/p&gt;
&lt;p&gt;In contrast, our limitation does not apply to several types of methods
proposed in the literature, for instance, goal-conditioned methods or other
algorithms that construct an inverse dynamics model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pinon_B/0/1/0/all/0/1&quot;&gt;Brieuc Pinon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jungers_R/0/1/0/all/0/1&quot;&gt;Rapha&amp;#xeb;l Jungers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delvenne_J/0/1/0/all/0/1&quot;&gt;Jean-Charles Delvenne&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16319">
<title>Augmenting transformers with recursively composed multi-grained representations. (arXiv:2309.16319v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.16319</link>
<description rdf:parseType="Literal">&lt;p&gt;We present ReCAT, a recursive composition augmented Transformer that is able
to explicitly model hierarchical syntactic structures of raw texts without
relying on gold trees during both learning and inference. Existing research
along this line restricts data to follow a hierarchical tree structure and thus
lacks inter-span communications. To overcome the problem, we propose a novel
contextual inside-outside (CIO) layer that learns contextualized
representations of spans through bottom-up and top-down passes, where a
bottom-up pass forms representations of high-level spans by composing low-level
spans, while a top-down pass combines information inside and outside a span. By
stacking several CIO layers between the embedding layer and the attention
layers in Transformer, the ReCAT model can perform both deep intra-span and
deep inter-span interactions, and thus generate multi-grained representations
fully contextualized with other spans. Moreover, the CIO layers can be jointly
pre-trained with Transformers, making ReCAT enjoy scaling ability, strong
performance, and interpretability at the same time. We conduct experiments on
various sentence-level and span-level tasks. Evaluation results indicate that
ReCAT can significantly outperform vanilla Transformer models on all span-level
tasks and baselines that combine recursive networks with Transformers on
natural language inference tasks. More interestingly, the hierarchical
structures induced by ReCAT exhibit strong consistency with human-annotated
syntactic trees, indicating good interpretability brought by the CIO layers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xiang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1&quot;&gt;Qingyang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1&quot;&gt;Kewei Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wei Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16335">
<title>End-to-end Risk Prediction of Atrial Fibrillation from the 12-Lead ECG by Deep Neural Networks. (arXiv:2309.16335v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.16335</link>
<description rdf:parseType="Literal">&lt;p&gt;Background: Atrial fibrillation (AF) is one of the most common cardiac
arrhythmias that affects millions of people each year worldwide and it is
closely linked to increased risk of cardiovascular diseases such as stroke and
heart failure. Machine learning methods have shown promising results in
evaluating the risk of developing atrial fibrillation from the
electrocardiogram. We aim to develop and evaluate one such algorithm on a large
CODE dataset collected in Brazil.
&lt;/p&gt;
&lt;p&gt;Results: The deep neural network model identified patients without indication
of AF in the presented ECG but who will develop AF in the future with an AUC
score of 0.845. From our survival model, we obtain that patients in the
high-risk group (i.e. with the probability of a future AF case being greater
than 0.7) are 50% more likely to develop AF within 40 weeks, while patients
belonging to the minimal-risk group (i.e. with the probability of a future AF
case being less than or equal to 0.1) have more than 85% chance of remaining AF
free up until after seven years.
&lt;/p&gt;
&lt;p&gt;Conclusion: We developed and validated a model for AF risk prediction. If
applied in clinical practice, the model possesses the potential of providing
valuable and useful information in decision-making and patient management
processes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Habineza_T/0/1/0/all/0/1&quot;&gt;Theogene Habineza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1&quot;&gt;Ant&amp;#xf4;nio H. Ribeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gedon_D/0/1/0/all/0/1&quot;&gt;Daniel Gedon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Behar_J/0/1/0/all/0/1&quot;&gt;Joachim A. Behar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1&quot;&gt;Antonio Luiz P. Ribeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schon_T/0/1/0/all/0/1&quot;&gt;Thomas B. Sch&amp;#xf6;n&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16344">
<title>Epistemic Logic Programs: a study of some properties. (arXiv:2309.16344v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2309.16344</link>
<description rdf:parseType="Literal">&lt;p&gt;Epistemic Logic Programs (ELPs), extend Answer Set Programming (ASP) with
epistemic operators. The semantics of such programs is provided in terms of
world views, which are sets of belief sets, i.e., syntactically, sets of sets
of atoms. Different semantic approaches propose different characterizations of
world views. Recent work has introduced semantic properties that should be met
by any semantics for ELPs, like the Epistemic Splitting Property, that, if
satisfied, allows to modularly compute world views in a bottom-up fashion,
analogously to ``traditional&apos;&apos; ASP. We analyze the possibility of changing the
perspective, shifting from a bottom-up to a top-down approach to splitting. We
propose a basic top-down approach, which we prove to be equivalent to the
bottom-up one. We then propose an extended approach, where our new definition:
(i) is provably applicable to many of the existing semantics; (ii) operates
similarly to ``traditional&apos;&apos; ASP; (iii) provably coincides under any semantics
with the bottom-up notion of splitting at least on the class of Epistemically
Stratified Programs (which are, intuitively, those where the use of epistemic
operators is stratified); (iv) better adheres to common ASP programming
methodology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Costantini_S/0/1/0/all/0/1&quot;&gt;Stefania Costantini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Formisano_A/0/1/0/all/0/1&quot;&gt;Andrea Formisano&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16380">
<title>Conditional normalizing flows for IceCube event reconstruction. (arXiv:2309.16380v1 [astro-ph.HE])</title>
<link>http://arxiv.org/abs/2309.16380</link>
<description rdf:parseType="Literal">&lt;p&gt;The IceCube Neutrino Observatory is a cubic-kilometer high-energy neutrino
detector deployed in the Antarctic ice. Two major event classes are
charged-current electron and muon neutrino interactions. In this contribution,
we discuss the inference of direction and energy for these classes using
conditional normalizing flows. They allow to derive a posterior distribution
for each individual event based on the raw data that can include systematic
uncertainties, which makes them very promising for next-generation
reconstructions. For each normalizing flow we use the differential entropy and
the KL-divergence to its maximum entropy approximation to interpret the
results. The normalizing flows correctly incorporate complex optical properties
of the Antarctic ice and their relation to the embedded detector. For showers,
the differential entropy increases in regions of high photon absorption and
decreases in clear ice. For muons, the differential entropy strongly correlates
with the contained track length. Coverage is maintained, even for low photon
counts and highly asymmetrical contour shapes. For high-photon counts, the
distributions get narrower and become more symmetrical, as expected from the
asymptotic theorem of Bernstein-von-Mises. For shower directional
reconstruction, we find the region between 1 TeV and 100 TeV to potentially
benefit the most from normalizing flows because of azimuth-zenith asymmetries
which have been neglected in previous analyses by assuming symmetrical
contours. Events in this energy range play a vital role in the recent discovery
of the galactic plane diffuse neutrino emission.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Glusenkamp_T/0/1/0/all/0/1&quot;&gt;Thorsten Gl&amp;#xfc;senkamp&lt;/a&gt; (for the IceCube collaboration)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16382">
<title>RLLTE: Long-Term Evolution Project of Reinforcement Learning. (arXiv:2309.16382v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2309.16382</link>
<description rdf:parseType="Literal">&lt;p&gt;We present RLLTE: a long-term evolution, extremely modular, and open-source
framework for reinforcement learning (RL) research and application. Beyond
delivering top-notch algorithm implementations, RLLTE also serves as a toolkit
for developing algorithms. More specifically, RLLTE decouples the RL algorithms
completely from the exploitation-exploration perspective, providing a large
number of components to accelerate algorithm development and evolution. In
particular, RLLTE is the first RL framework to build a complete and luxuriant
ecosystem, which includes model training, evaluation, deployment, benchmark
hub, and large language model (LLM)-empowered copilot. RLLTE is expected to set
standards for RL engineering practice and be highly stimulative for industry
and academia.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1&quot;&gt;Mingqi Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zequn Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1&quot;&gt;Shihao Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1&quot;&gt;Xin Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1&quot;&gt;Wenjun Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16391">
<title>Differential 2D Copula Approximating Transforms via Sobolev Training: 2-Cats Networks. (arXiv:2309.16391v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.16391</link>
<description rdf:parseType="Literal">&lt;p&gt;Copulas are a powerful statistical tool that captures dependencies across
data dimensions. When applying Copulas, we can estimate multivariate
distribution functions by initially estimating independent marginals, an easy
task, and then a single copulating function, $C$, to connect the marginals, a
hard task. For two-dimensional data, a copula is a two-increasing function of
the form $C: (u,v)\in \mathbf{I}^2 \rightarrow \mathbf{I}$, where $\mathbf{I} =
[0, 1]$. In this paper, we show how Neural Networks (NNs) can approximate any
two-dimensional copula non-parametrically. Our approach, denoted as 2-Cats, is
inspired by the Physics-Informed Neural Networks and Sobolev Training
literature. Not only do we show that we can estimate the output of a 2d Copula
better than the state-of-the-art, our approach is non-parametric and respects
the mathematical properties of a Copula $C$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Figueiredo_F/0/1/0/all/0/1&quot;&gt;Flavio Figueiredo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandes_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Geraldo Fernandes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_J/0/1/0/all/0/1&quot;&gt;Jackson Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Assuncao_R/0/1/0/all/0/1&quot;&gt;Renato M. Assun&amp;#xe7;&amp;#xe3;o&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16397">
<title>Uncertainty-Aware Decision Transformer for Stochastic Driving Environments. (arXiv:2309.16397v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.16397</link>
<description rdf:parseType="Literal">&lt;p&gt;Offline Reinforcement Learning (RL) has emerged as a promising framework for
learning policies without active interactions, making it especially appealing
for autonomous driving tasks. Recent successes of Transformers inspire casting
offline RL as sequence modeling, which performs well in long-horizon tasks.
However, they are overly optimistic in stochastic environments with incorrect
assumptions that the same goal can be consistently achieved by identical
actions. In this paper, we introduce an UNcertainty-awaRE deciSion Transformer
(UNREST) for planning in stochastic driving environments without introducing
additional transition or complex generative models. Specifically, UNREST
estimates state uncertainties by the conditional mutual information between
transitions and returns, and segments sequences accordingly. Discovering the
`uncertainty accumulation&apos; and `temporal locality&apos; properties of driving
environments, UNREST replaces the global returns in decision transformers with
less uncertain truncated returns, to learn from true outcomes of agent actions
rather than environment transitions. We also dynamically evaluate environmental
uncertainty during inference for cautious planning. Extensive experimental
results demonstrate UNREST&apos;s superior performance in various driving scenarios
and the power of our uncertainty estimation strategy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zenan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_F/0/1/0/all/0/1&quot;&gt;Fan Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1&quot;&gt;Qiao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Da_F/0/1/0/all/0/1&quot;&gt;Fang Da&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hang Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16400">
<title>Physics-Preserving AI-Accelerated Simulations of Plasma Turbulence. (arXiv:2309.16400v1 [physics.comp-ph])</title>
<link>http://arxiv.org/abs/2309.16400</link>
<description rdf:parseType="Literal">&lt;p&gt;Turbulence in fluids, gases, and plasmas remains an open problem of both
practical and fundamental importance. Its irreducible complexity usually cannot
be tackled computationally in a brute-force style. Here, we combine Large Eddy
Simulation (LES) techniques with Machine Learning (ML) to retain only the
largest dynamics explicitly, while small-scale dynamics are described by an
ML-based sub-grid-scale model. Applying this novel approach to self-driven
plasma turbulence allows us to remove large parts of the inertial range,
reducing the computational effort by about three orders of magnitude, while
retaining the statistical physical properties of the turbulent system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Greif_R/0/1/0/all/0/1&quot;&gt;Robin Greif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Jenko_F/0/1/0/all/0/1&quot;&gt;Frank Jenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Thuerey_N/0/1/0/all/0/1&quot;&gt;Nils Thuerey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16413">
<title>Genetic Engineering Algorithm (GEA): An Efficient Metaheuristic Algorithm for Solving Combinatorial Optimization Problems. (arXiv:2309.16413v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2309.16413</link>
<description rdf:parseType="Literal">&lt;p&gt;Genetic Algorithms (GAs) are known for their efficiency in solving
combinatorial optimization problems, thanks to their ability to explore diverse
solution spaces, handle various representations, exploit parallelism, preserve
good solutions, adapt to changing dynamics, handle combinatorial diversity, and
provide heuristic search. However, limitations such as premature convergence,
lack of problem-specific knowledge, and randomness of crossover and mutation
operators make GAs generally inefficient in finding an optimal solution. To
address these limitations, this paper proposes a new metaheuristic algorithm
called the Genetic Engineering Algorithm (GEA) that draws inspiration from
genetic engineering concepts. GEA redesigns the traditional GA while
incorporating new search methods to isolate, purify, insert, and express new
genes based on existing ones, leading to the emergence of desired traits and
the production of specific chromosomes based on the selected genes. Comparative
evaluations against state-of-the-art algorithms on benchmark instances
demonstrate the superior performance of GEA, showcasing its potential as an
innovative and efficient solution for combinatorial optimization problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sohrabi_M/0/1/0/all/0/1&quot;&gt;Majid Sohrabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fathollahi_Fard_A/0/1/0/all/0/1&quot;&gt;Amir M. Fathollahi-Fard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gromov_V/0/1/0/all/0/1&quot;&gt;Vasilii A. Gromov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16414">
<title>AutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models. (arXiv:2309.16414v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16414</link>
<description rdf:parseType="Literal">&lt;p&gt;Classifiers built upon vision-language models such as CLIP have shown
remarkable zero-shot performance across a broad range of image classification
tasks. Prior work has studied different ways of automatically creating
descriptor sets for every class based on prompt templates, ranging from
manually engineered templates over templates obtained from a large language
model to templates built from random words and characters. In contrast,
deriving zero-shot classifiers from the respective encoded class descriptors
has remained nearly unchanged, that is: classify to the class that maximizes
the cosine similarity between its averaged encoded class descriptors and the
encoded image. However, weighting all class descriptors equally can be
suboptimal when certain descriptors match visual clues on a given image better
than others. In this work, we propose AutoCLIP, a method for auto-tuning
zero-shot classifiers. AutoCLIP assigns to each prompt template per-image
weights, which are derived from statistics of class descriptor-image
similarities at inference time. AutoCLIP is fully unsupervised, has very low
overhead, and can be easily implemented in few lines of code. We show that for
a broad range of vision-language models, datasets, and prompt templates,
AutoCLIP outperforms baselines consistently and by up to 3 percent point
accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metzen_J/0/1/0/all/0/1&quot;&gt;Jan Hendrik Metzen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saranrittichai_P/0/1/0/all/0/1&quot;&gt;Piyapat Saranrittichai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mummadi_C/0/1/0/all/0/1&quot;&gt;Chaithanya Kumar Mummadi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16424">
<title>Prompt-and-Align: Prompt-Based Social Alignment for Few-Shot Fake News Detection. (arXiv:2309.16424v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.16424</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite considerable advances in automated fake news detection, due to the
timely nature of news, it remains a critical open question how to effectively
predict the veracity of news articles based on limited fact-checks. Existing
approaches typically follow a &quot;Train-from-Scratch&quot; paradigm, which is
fundamentally bounded by the availability of large-scale annotated data. While
expressive pre-trained language models (PLMs) have been adapted in a
&quot;Pre-Train-and-Fine-Tune&quot; manner, the inconsistency between pre-training and
downstream objectives also requires costly task-specific supervision. In this
paper, we propose &quot;Prompt-and-Align&quot; (P&amp;amp;A), a novel prompt-based paradigm for
few-shot fake news detection that jointly leverages the pre-trained knowledge
in PLMs and the social context topology. Our approach mitigates label scarcity
by wrapping the news article in a task-related textual prompt, which is then
processed by the PLM to directly elicit task-specific knowledge. To supplement
the PLM with social context without inducing additional training overheads,
motivated by empirical observation on user veracity consistency (i.e., social
users tend to consume news of the same veracity type), we further construct a
news proximity graph among news articles to capture the veracity-consistent
signals in shared readerships, and align the prompting predictions along the
graph edges in a confidence-informed manner. Extensive experiments on three
real-world benchmarks demonstrate that P&amp;amp;A sets new states-of-the-art for
few-shot fake news detection performance by significant margins.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiaying Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_A/0/1/0/all/0/1&quot;&gt;Ailin Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_M/0/1/0/all/0/1&quot;&gt;Miao Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1&quot;&gt;Bryan Hooi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16429">
<title>Diverse and Aligned Audio-to-Video Generation via Text-to-Video Model Adaptation. (arXiv:2309.16429v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.16429</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the task of generating diverse and realistic videos guided by
natural audio samples from a wide variety of semantic classes. For this task,
the videos are required to be aligned both globally and temporally with the
input audio: globally, the input audio is semantically associated with the
entire output video, and temporally, each segment of the input audio is
associated with a corresponding segment of that video. We utilize an existing
text-conditioned video generation model and a pre-trained audio encoder model.
The proposed method is based on a lightweight adaptor network, which learns to
map the audio-based representation to the input representation expected by the
text-to-video generation model. As such, it also enables video generation
conditioned on text, audio, and, for the first time as far as we can ascertain,
on both text and audio. We validate our method extensively on three datasets
demonstrating significant semantic diversity of audio-video samples and further
propose a novel evaluation metric (AV-Align) to assess the alignment of
generated videos with input audio samples. AV-Align is based on the detection
and comparison of energy peaks in both modalities. In comparison to recent
state-of-the-art approaches, our method generates videos that are better
aligned with the input sound, both with respect to content and temporal axis.
We also show that videos produced by our method present higher visual quality
and are more diverse.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yariv_G/0/1/0/all/0/1&quot;&gt;Guy Yariv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gat_I/0/1/0/all/0/1&quot;&gt;Itai Gat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benaim_S/0/1/0/all/0/1&quot;&gt;Sagie Benaim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1&quot;&gt;Lior Wolf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1&quot;&gt;Idan Schwartz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1&quot;&gt;Yossi Adi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16436">
<title>Neuro Symbolic Reasoning for Planning: Counterexample Guided Inductive Synthesis using Large Language Models and Satisfiability Solving. (arXiv:2309.16436v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2309.16436</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative large language models (LLMs) with instruct training such as GPT-4
can follow human-provided instruction prompts and generate human-like responses
to these prompts. Apart from natural language responses, they have also been
found to be effective at generating formal artifacts such as code, plans, and
logical specifications from natural language prompts. Despite their remarkably
improved accuracy, these models are still known to produce factually incorrect
or contextually inappropriate results despite their syntactic coherence - a
phenomenon often referred to as hallucination. This limitation makes it
difficult to use these models to synthesize formal artifacts that are used in
safety-critical applications. Unlike tasks such as text summarization and
question-answering, bugs in code, plan, and other formal artifacts produced by
LLMs can be catastrophic. We posit that we can use the satisfiability modulo
theory (SMT) solvers as deductive reasoning engines to analyze the generated
solutions from the LLMs, produce counterexamples when the solutions are
incorrect, and provide that feedback to the LLMs exploiting the dialog
capability of instruct-trained LLMs. This interaction between inductive LLMs
and deductive SMT solvers can iteratively steer the LLM to generate the correct
response. In our experiments, we use planning over the domain of blocks as our
synthesis task for evaluating our approach. We use GPT-4, GPT3.5 Turbo,
Davinci, Curie, Babbage, and Ada as the LLMs and Z3 as the SMT solver. Our
method allows the user to communicate the planning problem in natural language;
even the formulation of queries to SMT solvers is automatically generated from
natural language. Thus, the proposed technique can enable non-expert users to
describe their problems in natural language, and the combination of LLMs and
SMT solvers can produce provably correct solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1&quot;&gt;Sumit Kumar Jha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1&quot;&gt;Susmit Jha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lincoln_P/0/1/0/all/0/1&quot;&gt;Patrick Lincoln&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bastian_N/0/1/0/all/0/1&quot;&gt;Nathaniel D. Bastian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Velasquez_A/0/1/0/all/0/1&quot;&gt;Alvaro Velasquez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ewetz_R/0/1/0/all/0/1&quot;&gt;Rickard Ewetz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neema_S/0/1/0/all/0/1&quot;&gt;Sandeep Neema&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16459">
<title>Augmenting LLMs with Knowledge: A survey on hallucination prevention. (arXiv:2309.16459v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.16459</link>
<description rdf:parseType="Literal">&lt;p&gt;Large pre-trained language models have demonstrated their proficiency in
storing factual knowledge within their parameters and achieving remarkable
results when fine-tuned for downstream natural language processing tasks.
Nonetheless, their capacity to access and manipulate knowledge with precision
remains constrained, resulting in performance disparities on
knowledge-intensive tasks when compared to task-specific architectures.
Additionally, the challenges of providing provenance for model decisions and
maintaining up-to-date world knowledge persist as open research frontiers. To
address these limitations, the integration of pre-trained models with
differentiable access mechanisms to explicit non-parametric memory emerges as a
promising solution. This survey delves into the realm of language models (LMs)
augmented with the ability to tap into external knowledge sources, including
external knowledge bases and search engines. While adhering to the standard
objective of predicting missing tokens, these augmented LMs leverage diverse,
possibly non-parametric external modules to augment their contextual processing
capabilities, departing from the conventional language modeling paradigm.
Through an exploration of current advancements in augmenting large language
models with knowledge, this work concludes that this emerging research
direction holds the potential to address prevalent issues in traditional LMs,
such as hallucinations, un-grounded responses, and scalability challenges.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andriopoulos_K/0/1/0/all/0/1&quot;&gt;Konstantinos Andriopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pouwelse_J/0/1/0/all/0/1&quot;&gt;Johan Pouwelse&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16492">
<title>Asset Bundling for Wind Power Forecasting. (arXiv:2309.16492v1 [stat.ME])</title>
<link>http://arxiv.org/abs/2309.16492</link>
<description rdf:parseType="Literal">&lt;p&gt;The growing penetration of intermittent, renewable generation in US power
grids, especially wind and solar generation, results in increased operational
uncertainty. In that context, accurate forecasts are critical, especially for
wind generation, which exhibits large variability and is historically harder to
predict. To overcome this challenge, this work proposes a novel
Bundle-Predict-Reconcile (BPR) framework that integrates asset bundling,
machine learning, and forecast reconciliation techniques. The BPR framework
first learns an intermediate hierarchy level (the bundles), then predicts wind
power at the asset, bundle, and fleet level, and finally reconciles all
forecasts to ensure consistency. This approach effectively introduces an
auxiliary learning task (predicting the bundle-level time series) to help the
main learning tasks. The paper also introduces new asset-bundling criteria that
capture the spatio-temporal dynamics of wind power time series. Extensive
numerical experiments are conducted on an industry-size dataset of 283 wind
farms in the MISO footprint. The experiments consider short-term and day-ahead
forecasts, and evaluates a large variety of forecasting models that include
weather predictions as covariates. The results demonstrate the benefits of BPR,
which consistently and significantly improves forecast accuracy over baselines,
especially at the fleet level.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hanyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tanneau_M/0/1/0/all/0/1&quot;&gt;Mathieu Tanneau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chaofan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Joseph_V/0/1/0/all/0/1&quot;&gt;V. Roshan Joseph&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shangkun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hentenryck_P/0/1/0/all/0/1&quot;&gt;Pascal Van Hentenryck&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16511">
<title>Toloka Visual Question Answering Benchmark. (arXiv:2309.16511v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16511</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present Toloka Visual Question Answering, a new
crowdsourced dataset allowing comparing performance of machine learning systems
against human level of expertise in the grounding visual question answering
task. In this task, given an image and a textual question, one has to draw the
bounding box around the object correctly responding to that question. Every
image-question pair contains the response, with only one correct response per
image. Our dataset contains 45,199 pairs of images and questions in English,
provided with ground truth bounding boxes, split into train and two test
subsets. Besides describing the dataset and releasing it under a CC BY license,
we conducted a series of experiments on open source zero-shot baseline models
and organized a multi-phase competition at WSDM Cup that attracted 48
participants worldwide. However, by the time of paper submission, no machine
learning model outperformed the non-expert crowdsourcing baseline according to
the intersection over union evaluation score.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ustalov_D/0/1/0/all/0/1&quot;&gt;Dmitry Ustalov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavlichenko_N/0/1/0/all/0/1&quot;&gt;Nikita Pavlichenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koshelev_S/0/1/0/all/0/1&quot;&gt;Sergey Koshelev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Likhobaba_D/0/1/0/all/0/1&quot;&gt;Daniil Likhobaba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smirnova_A/0/1/0/all/0/1&quot;&gt;Alisa Smirnova&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16512">
<title>From Complexity to Clarity: Analytical Expressions of Deep Neural Network Weights via Clifford&apos;s Geometric Algebra and Convexity. (arXiv:2309.16512v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.16512</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce a novel analysis of neural networks based on
geometric (Clifford) algebra and convex optimization. We show that optimal
weights of deep ReLU neural networks are given by the wedge product of training
samples when trained with standard regularized loss. Furthermore, the training
problem reduces to convex optimization over wedge product features, which
encode the geometric structure of the training dataset. This structure is given
in terms of signed volumes of triangles and parallelotopes generated by data
vectors. The convex problem finds a small subset of samples via $\ell_1$
regularization to discover only relevant wedge product features. Our analysis
provides a novel perspective on the inner workings of deep neural networks and
sheds light on the role of the hidden layers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pilanci_M/0/1/0/all/0/1&quot;&gt;Mert Pilanci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16534">
<title>MotionLM: Multi-Agent Motion Forecasting as Language Modeling. (arXiv:2309.16534v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16534</link>
<description rdf:parseType="Literal">&lt;p&gt;Reliable forecasting of the future behavior of road agents is a critical
component to safe planning in autonomous vehicles. Here, we represent
continuous trajectories as sequences of discrete motion tokens and cast
multi-agent motion prediction as a language modeling task over this domain. Our
model, MotionLM, provides several advantages: First, it does not require
anchors or explicit latent variable optimization to learn multimodal
distributions. Instead, we leverage a single standard language modeling
objective, maximizing the average log probability over sequence tokens. Second,
our approach bypasses post-hoc interaction heuristics where individual agent
trajectory generation is conducted prior to interactive scoring. Instead,
MotionLM produces joint distributions over interactive agent futures in a
single autoregressive decoding process. In addition, the model&apos;s sequential
factorization enables temporally causal conditional rollouts. The proposed
approach establishes new state-of-the-art performance for multi-agent motion
prediction on the Waymo Open Motion Dataset, ranking 1st on the interactive
challenge leaderboard.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seff_A/0/1/0/all/0/1&quot;&gt;Ari Seff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cera_B/0/1/0/all/0/1&quot;&gt;Brian Cera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Dian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_M/0/1/0/all/0/1&quot;&gt;Mason Ng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1&quot;&gt;Aurick Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nayakanti_N/0/1/0/all/0/1&quot;&gt;Nigamaa Nayakanti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Refaat_K/0/1/0/all/0/1&quot;&gt;Khaled S. Refaat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Rfou_R/0/1/0/all/0/1&quot;&gt;Rami Al-Rfou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sapp_B/0/1/0/all/0/1&quot;&gt;Benjamin Sapp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16535">
<title>KLoB: a Benchmark for Assessing Knowledge Locating Methods in Language Models. (arXiv:2309.16535v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.16535</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, Locate-Then-Edit paradigm has emerged as one of the main approaches
in changing factual knowledge stored in the Language models. However, there is
a lack of research on whether present locating methods can pinpoint the exact
parameters embedding the desired knowledge. Moreover, although many researchers
have questioned the validity of locality hypothesis of factual knowledge, no
method is provided to test the a hypothesis for more in-depth discussion and
research. Therefore, we introduce KLoB, a benchmark examining three essential
properties that a reliable knowledge locating method should satisfy. KLoB can
serve as a benchmark for evaluating existing locating methods in language
models, and can contributes a method to reassessing the validity of locality
hypothesis of factual knowledge. Our is publicly available at
\url{https://github.com/juyiming/KLoB}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ju_Y/0/1/0/all/0/1&quot;&gt;Yiming Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16561">
<title>Voting Network for Contour Levee Farmland Segmentation and Classification. (arXiv:2309.16561v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16561</link>
<description rdf:parseType="Literal">&lt;p&gt;High-resolution aerial imagery allows fine details in the segmentation of
farmlands. However, small objects and features introduce distortions to the
delineation of object boundaries, and larger contextual views are needed to
mitigate class confusion. In this work, we present an end-to-end trainable
network for segmenting farmlands with contour levees from high-resolution
aerial imagery. A fusion block is devised that includes multiple voting blocks
to achieve image segmentation and classification. We integrate the fusion block
with a backbone and produce both semantic predictions and segmentation slices.
The segmentation slices are used to perform majority voting on the predictions.
The network is trained to assign the most likely class label of a segment to
its pixels, learning the concept of farmlands rather than analyzing
constitutive pixels separately. We evaluate our method using images from the
National Agriculture Imagery Program. Our method achieved an average accuracy
of 94.34\%. Compared to the state-of-the-art methods, the proposed method
obtains an improvement of 6.96% and 2.63% in the F1 score on average.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meyarian_A/0/1/0/all/0/1&quot;&gt;Abolfazl Meyarian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xiaohui Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16564">
<title>Augment to Interpret: Unsupervised and Inherently Interpretable Graph Embeddings. (arXiv:2309.16564v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.16564</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised learning allows us to leverage unlabelled data, which has become
abundantly available, and to create embeddings that are usable on a variety of
downstream tasks. However, the typical lack of interpretability of unsupervised
representation learning has become a limiting factor with regard to recent
transparent-AI regulations. In this paper, we study graph representation
learning and we show that data augmentation that preserves semantics can be
learned and used to produce interpretations. Our framework, which we named
INGENIOUS, creates inherently interpretable embeddings and eliminates the need
for costly additional post-hoc analysis. We also introduce additional metrics
addressing the lack of formalism and metrics in the understudied area of
unsupervised-representation learning interpretability. Our results are
supported by an experimental study applied to both graph-level and node-level
tasks and show that interpretable embeddings provide state-of-the-art
performance on subsequent downstream tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scafarto_G/0/1/0/all/0/1&quot;&gt;Gregory Scafarto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ciortan_M/0/1/0/all/0/1&quot;&gt;Madalina Ciortan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tihon_S/0/1/0/all/0/1&quot;&gt;Simon Tihon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferre_Q/0/1/0/all/0/1&quot;&gt;Quentin Ferre&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16573">
<title>The ARRT of Language-Models-as-a-Service: Overview of a New Paradigm and its Challenges. (arXiv:2309.16573v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2309.16573</link>
<description rdf:parseType="Literal">&lt;p&gt;Some of the most powerful language models currently are proprietary systems,
accessible only via (typically restrictive) web or software programming
interfaces. This is the Language-Models-as-a-Service (LMaaS) paradigm.
Contrasting with scenarios where full model access is available, as in the case
of open-source models, such closed-off language models create specific
challenges for evaluating, benchmarking, and testing them. This paper has two
goals: on the one hand, we delineate how the aforementioned challenges act as
impediments to the accessibility, replicability, reliability, and
trustworthiness (ARRT) of LMaaS. We systematically examine the issues that
arise from a lack of information about language models for each of these four
aspects. We shed light on current solutions, provide some recommendations, and
highlight the directions for future advancements. On the other hand, it serves
as a one-stop-shop for the extant knowledge about current, major LMaaS,
offering a synthesized overview of the licences and capabilities their
interfaces offer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malfa_E/0/1/0/all/0/1&quot;&gt;Emanuele La Malfa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petrov_A/0/1/0/all/0/1&quot;&gt;Aleksandar Petrov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frieder_S/0/1/0/all/0/1&quot;&gt;Simon Frieder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinhuber_C/0/1/0/all/0/1&quot;&gt;Christoph Weinhuber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burnell_R/0/1/0/all/0/1&quot;&gt;Ryan Burnell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohn_A/0/1/0/all/0/1&quot;&gt;Anthony G. Cohn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shadbolt_N/0/1/0/all/0/1&quot;&gt;Nigel Shadbolt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wooldridge_M/0/1/0/all/0/1&quot;&gt;Michael Wooldridge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16593">
<title>Navigating Healthcare Insights: A Birds Eye View of Explainability with Knowledge Graphs. (arXiv:2309.16593v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2309.16593</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge graphs (KGs) are gaining prominence in Healthcare AI, especially in
drug discovery and pharmaceutical research as they provide a structured way to
integrate diverse information sources, enhancing AI system interpretability.
This interpretability is crucial in healthcare, where trust and transparency
matter, and eXplainable AI (XAI) supports decision making for healthcare
professionals. This overview summarizes recent literature on the impact of KGs
in healthcare and their role in developing explainable AI models. We cover KG
workflow, including construction, relationship extraction, reasoning, and their
applications in areas like Drug-Drug Interactions (DDI), Drug Target
Interactions (DTI), Drug Development (DD), Adverse Drug Reactions (ADR), and
bioinformatics. We emphasize the importance of making KGs more interpretable
through knowledge-infused learning in healthcare. Finally, we highlight
research challenges and provide insights for future directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1&quot;&gt;Satvik Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parikh_S/0/1/0/all/0/1&quot;&gt;Shivam Parikh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1&quot;&gt;Somya Garg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16595">
<title>Can LLMs Effectively Leverage Structural Information for Graph Learning: When and Why. (arXiv:2309.16595v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.16595</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies Large Language Models (LLMs) for structured
data--particularly graphs--a crucial data modality that remains underexplored
in the LLM literature. We aim to understand when and why the incorporation of
structural information inherent in graph data can improve the prediction
performance of LLMs on node classification tasks. To address the ``when&apos;&apos;
question, we examine a variety of prompting methods for encoding structural
information, in settings where textual node features are either rich or scarce.
For the ``why&apos;&apos; questions, we probe into two potential contributing factors to
the LLM performance: data leakage and homophily. Our exploration of these
questions reveals that (i) LLMs can benefit from structural information,
especially when textual node features are scarce; (ii) there is no substantial
evidence indicating that the performance of LLMs is significantly attributed to
data leakage; and (iii) the performance of LLMs on a target node is strongly
positively related to the local homophily ratio of the node.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xingjian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_Q/0/1/0/all/0/1&quot;&gt;Qiaozhu Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jiaqi Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16597">
<title>Transfer Learning for Bayesian Optimization on Heterogeneous Search Spaces. (arXiv:2309.16597v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.16597</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian optimization (BO) is a popular black-box function optimization
method, which makes sequential decisions based on a Bayesian model, typically a
Gaussian process (GP), of the function. To ensure the quality of the model,
transfer learning approaches have been developed to automatically design GP
priors by learning from observations on &quot;training&quot; functions. These training
functions are typically required to have the same domain as the &quot;test&quot; function
(black-box function to be optimized). In this paper, we introduce MPHD, a model
pre-training method on heterogeneous domains, which uses a neural net mapping
from domain-specific contexts to specifications of hierarchical GPs. MPHD can
be seamlessly integrated with BO to transfer knowledge across heterogeneous
search spaces. Our theoretical and empirical results demonstrate the validity
of MPHD and its superior performance on challenging black-box function
optimization tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1&quot;&gt;Zhou Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xinran Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16606">
<title>&quot;AI enhances our performance, I have no doubt this one will do the same&quot;: The Placebo effect is robust to negative descriptions of AI. (arXiv:2309.16606v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2309.16606</link>
<description rdf:parseType="Literal">&lt;p&gt;Heightened AI expectations facilitate performance in human-AI interactions
through placebo effects. While lowering expectations to control for placebo
effects is advisable, overly negative expectations could induce nocebo effects.
In a letter discrimination task, we informed participants that an AI would
either increase or decrease their performance by adapting the interface, but in
reality, no AI was present in any condition. A Bayesian analysis showed that
participants had high expectations and performed descriptively better
irrespective of the AI description when a sham-AI was present. Using cognitive
modeling, we could trace this advantage back to participants gathering more
information. A replication study verified that negative AI descriptions do not
alter expectations, suggesting that performance expectations with AI are biased
and robust to negative verbal descriptions. We discuss the impact of user
expectations on AI interactions and evaluation and provide a behavioral placebo
marker for human-AI interaction
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kloft_A/0/1/0/all/0/1&quot;&gt;Agnes M. Kloft&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Welsch_R/0/1/0/all/0/1&quot;&gt;Robin Welsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kosch_T/0/1/0/all/0/1&quot;&gt;Thomas Kosch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Villa_S/0/1/0/all/0/1&quot;&gt;Steeven Villa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16618">
<title>Revisiting Neural Program Smoothing for Fuzzing. (arXiv:2309.16618v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2309.16618</link>
<description rdf:parseType="Literal">&lt;p&gt;Testing with randomly generated inputs (fuzzing) has gained significant
traction due to its capacity to expose program vulnerabilities automatically.
Fuzz testing campaigns generate large amounts of data, making them ideal for
the application of machine learning (ML). Neural program smoothing (NPS), a
specific family of ML-guided fuzzers, aims to use a neural network as a smooth
approximation of the program target for new test case generation.
&lt;/p&gt;
&lt;p&gt;In this paper, we conduct the most extensive evaluation of NPS fuzzers
against standard gray-box fuzzers (&amp;gt;11 CPU years and &amp;gt;5.5 GPU years), and make
the following contributions: (1) We find that the original performance claims
for NPS fuzzers do not hold; a gap we relate to fundamental, implementation,
and experimental limitations of prior works. (2) We contribute the first
in-depth analysis of the contribution of machine learning and gradient-based
mutations in NPS. (3) We implement Neuzz++, which shows that addressing the
practical limitations of NPS fuzzers improves performance, but that standard
gray-box fuzzers almost always surpass NPS-based fuzzers. (4) As a consequence,
we propose new guidelines targeted at benchmarking fuzzing based on machine
learning, and present MLFuzz, a platform with GPU access for easy and
reproducible evaluation of ML-based fuzzers. Neuzz++, MLFuzz, and all our data
are public.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nicolae_M/0/1/0/all/0/1&quot;&gt;Maria-Irina Nicolae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eisele_M/0/1/0/all/0/1&quot;&gt;Max Eisele&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeller_A/0/1/0/all/0/1&quot;&gt;Andreas Zeller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16620">
<title>Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit. (arXiv:2309.16620v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2309.16620</link>
<description rdf:parseType="Literal">&lt;p&gt;The cost of hyperparameter tuning in deep learning has been rising with model
sizes, prompting practitioners to find new tuning methods using a proxy of
smaller networks. One such proposal uses $\mu$P parameterized networks, where
the optimal hyperparameters for small width networks transfer to networks with
arbitrarily large width. However, in this scheme, hyperparameters do not
transfer across depths. As a remedy, we study residual networks with a residual
branch scale of $1/\sqrt{\text{depth}}$ in combination with the $\mu$P
parameterization. We provide experiments demonstrating that residual
architectures including convolutional ResNets and Vision Transformers trained
with this parameterization exhibit transfer of optimal hyperparameters across
width and depth on CIFAR-10 and ImageNet. Furthermore, our empirical findings
are supported and motivated by theory. Using recent developments in the
dynamical mean field theory (DMFT) description of neural network learning
dynamics, we show that this parameterization of ResNets admits a well-defined
feature learning joint infinite-width and infinite-depth limit and show
convergence of finite-size network dynamics towards this limit.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bordelon_B/0/1/0/all/0/1&quot;&gt;Blake Bordelon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Noci_L/0/1/0/all/0/1&quot;&gt;Lorenzo Noci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mufan Bill Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hanin_B/0/1/0/all/0/1&quot;&gt;Boris Hanin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pehlevan_C/0/1/0/all/0/1&quot;&gt;Cengiz Pehlevan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16621">
<title>Stress Testing Chain-of-Thought Prompting for Large Language Models. (arXiv:2309.16621v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.16621</link>
<description rdf:parseType="Literal">&lt;p&gt;This report examines the effectiveness of Chain-of-Thought (CoT) prompting in
improving the multi-step reasoning abilities of large language models (LLMs).
Inspired by previous studies \cite{Min2022RethinkingWork}, we analyze the
impact of three types of CoT prompt perturbations, namely CoT order, CoT
values, and CoT operators on the performance of GPT-3 on various tasks. Our
findings show that incorrect CoT prompting leads to poor performance on
accuracy metrics. Correct values in the CoT is crucial for predicting correct
answers. Moreover, incorrect demonstrations, where the CoT operators or the CoT
order are wrong, do not affect the performance as drastically when compared to
the value based perturbations. This research deepens our understanding of CoT
prompting and opens some new questions regarding the capability of LLMs to
learn reasoning in context.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1&quot;&gt;Aayush Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thakkar_K/0/1/0/all/0/1&quot;&gt;Karan Thakkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16633">
<title>Mixup Your Own Pairs. (arXiv:2309.16633v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.16633</link>
<description rdf:parseType="Literal">&lt;p&gt;In representation learning, regression has traditionally received less
attention than classification. Directly applying representation learning
techniques designed for classification to regression often results in
fragmented representations in the latent space, yielding sub-optimal
performance. In this paper, we argue that the potential of contrastive learning
for regression has been overshadowed due to the neglect of two crucial aspects:
ordinality-awareness and hardness. To address these challenges, we advocate
&quot;mixup your own contrastive pairs for supervised contrastive regression&quot;,
instead of relying solely on real/augmented samples. Specifically, we propose
Supervised Contrastive Learning for Regression with Mixup (SupReMix). It takes
anchor-inclusive mixtures (mixup of the anchor and a distinct negative sample)
as hard negative pairs and anchor-exclusive mixtures (mixup of two distinct
negative samples) as hard positive pairs at the embedding level. This strategy
formulates harder contrastive pairs by integrating richer ordinal information.
Through extensive experiments on six regression datasets including 2D images,
volumetric images, text, tabular data, and time-series signals, coupled with
theoretical analysis, we demonstrate that SupReMix pre-training fosters
continuous ordered representations of regression data, resulting in significant
improvement in regression performance. Furthermore, SupReMix is superior to
other approaches in a range of regression challenges including transfer
learning, imbalanced training data, and scenarios with fewer training samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yilei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1&quot;&gt;Zijian Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chongyao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wangchunshu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Juan Helen Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16639">
<title>MindShift: Leveraging Large Language Models for Mental-States-Based Problematic Smartphone Use Intervention. (arXiv:2309.16639v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.16639</link>
<description rdf:parseType="Literal">&lt;p&gt;Problematic smartphone use negatively affects physical and mental health.
Despite the wide range of prior research, existing persuasive techniques are
not flexible enough to provide dynamic persuasion content based on users&apos;
physical contexts and mental states. We first conduct a Wizard-of-Oz study
(N=12) and an interview study (N=10) to summarize the mental states behind
problematic smartphone use: boredom, stress, and inertia. This informs our
design of four persuasion strategies: understanding, comforting, evoking, and
scaffolding habits. We leverage large language models (LLMs) to enable the
automatic and dynamic generation of effective persuasion content. We develop
MindShift, a novel LLM-powered problematic smartphone use intervention
technique. MindShift takes users&apos; in-the-moment physical contexts, mental
states, app usage behaviors, users&apos; goals &amp;amp; habits as input, and generates
high-quality and flexible persuasive content with appropriate persuasion
strategies. We conduct a 5-week field experiment (N=25) to compare MindShift
with baseline techniques. The results show that MindShift significantly
improves intervention acceptance rates by 17.8-22.5% and reduces smartphone use
frequency by 12.1-14.4%. Moreover, users have a significant drop in smartphone
addiction scale scores and a rise in self-efficacy. Our study sheds light on
the potential of leveraging LLMs for context-aware persuasion in other behavior
change domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1&quot;&gt;Ruolan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Chun Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1&quot;&gt;Xiaole Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yujia Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Ningning Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yue Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuhan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1&quot;&gt;Zhi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Li Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1&quot;&gt;Qiaolei Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xuhai Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yuanchun Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16661">
<title>SA2-Net: Scale-aware Attention Network for Microscopic Image Segmentation. (arXiv:2309.16661v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16661</link>
<description rdf:parseType="Literal">&lt;p&gt;Microscopic image segmentation is a challenging task, wherein the objective
is to assign semantic labels to each pixel in a given microscopic image. While
convolutional neural networks (CNNs) form the foundation of many existing
frameworks, they often struggle to explicitly capture long-range dependencies.
Although transformers were initially devised to address this issue using
self-attention, it has been proven that both local and global features are
crucial for addressing diverse challenges in microscopic images, including
variations in shape, size, appearance, and target region density. In this
paper, we introduce SA2-Net, an attention-guided method that leverages
multi-scale feature learning to effectively handle diverse structures within
microscopic images. Specifically, we propose scale-aware attention (SA2) module
designed to capture inherent variations in scales and shapes of microscopic
regions, such as cells, for accurate segmentation. This module incorporates
local attention at each level of multi-stage features, as well as global
attention across multiple resolutions. Furthermore, we address the issue of
blurred region boundaries (e.g., cell boundaries) by introducing a novel
upsampling strategy called the Adaptive Up-Attention (AuA) module. This module
enhances the discriminative ability for improved localization of microscopic
regions using an explicit attention mechanism. Extensive experiments on five
challenging datasets demonstrate the benefits of our SA2-Net model. Our source
code is publicly available at \url{https://github.com/mustansarfiaz/SA2-Net}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fiaz_M/0/1/0/all/0/1&quot;&gt;Mustansar Fiaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heidari_M/0/1/0/all/0/1&quot;&gt;Moein Heidari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anwer_R/0/1/0/all/0/1&quot;&gt;Rao Muhammad Anwer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cholakkal_H/0/1/0/all/0/1&quot;&gt;Hisham Cholakkal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16668">
<title>RealFill: Reference-Driven Generation for Authentic Image Completion. (arXiv:2309.16668v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.16668</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in generative imagery have brought forth outpainting and
inpainting models that can produce high-quality, plausible image content in
unknown regions, but the content these models hallucinate is necessarily
inauthentic, since the models lack sufficient context about the true scene. In
this work, we propose RealFill, a novel generative approach for image
completion that fills in missing regions of an image with the content that
should have been there. RealFill is a generative inpainting model that is
personalized using only a few reference images of a scene. These reference
images do not have to be aligned with the target image, and can be taken with
drastically varying viewpoints, lighting conditions, camera apertures, or image
styles. Once personalized, RealFill is able to complete a target image with
visually compelling contents that are faithful to the original scene. We
evaluate RealFill on a new image completion benchmark that covers a set of
diverse and challenging scenarios, and find that it outperforms existing
approaches by a large margin. See more results on our project page:
https://realfill.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1&quot;&gt;Luming Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruiz_N/0/1/0/all/0/1&quot;&gt;Nataniel Ruiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_Q/0/1/0/all/0/1&quot;&gt;Qinghao Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanzhen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holynski_A/0/1/0/all/0/1&quot;&gt;Aleksander Holynski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacobs_D/0/1/0/all/0/1&quot;&gt;David E. Jacobs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hariharan_B/0/1/0/all/0/1&quot;&gt;Bharath Hariharan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pritch_Y/0/1/0/all/0/1&quot;&gt;Yael Pritch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wadhwa_N/0/1/0/all/0/1&quot;&gt;Neal Wadhwa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aberman_K/0/1/0/all/0/1&quot;&gt;Kfir Aberman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rubinstein_M/0/1/0/all/0/1&quot;&gt;Michael Rubinstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.05625">
<title>Quantum Self-Attention Neural Networks for Text Classification. (arXiv:2205.05625v2 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2205.05625</link>
<description rdf:parseType="Literal">&lt;p&gt;An emerging direction of quantum computing is to establish meaningful quantum
applications in various fields of artificial intelligence, including natural
language processing (NLP). Although some efforts based on syntactic analysis
have opened the door to research in Quantum NLP (QNLP), limitations such as
heavy syntactic preprocessing and syntax-dependent network architecture make
them impracticable on larger and real-world data sets. In this paper, we
propose a new simple network architecture, called the quantum self-attention
neural network (QSANN), which can compensate for these limitations.
Specifically, we introduce the self-attention mechanism into quantum neural
networks and then utilize a Gaussian projected quantum self-attention serving
as a sensible quantum version of self-attention. As a result, QSANN is
effective and scalable on larger data sets and has the desirable property of
being implementable on near-term quantum devices. In particular, our QSANN
outperforms the best existing QNLP model based on syntactic analysis as well as
a simple classical self-attention neural network in numerical experiments of
text classification tasks on public data sets. We further show that our method
exhibits robustness to low-level quantum noises and showcases resilience to
quantum neural network architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guangxi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xuanqiang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.04963">
<title>Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering. (arXiv:2209.04963v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2209.04963</link>
<description rdf:parseType="Literal">&lt;p&gt;Responsible AI is widely considered as one of the greatest scientific
challenges of our time and is key to increase the adoption of AI. Recently, a
number of AI ethics principles frameworks have been published. However, without
further guidance on best practices, practitioners are left with nothing much
beyond truisms. Also, significant efforts have been placed at algorithm-level
rather than system-level, mainly focusing on a subset of mathematics-amenable
ethical principles, such as fairness. Nevertheless, ethical issues can arise at
any step of the development lifecycle, cutting across many AI and non-AI
components of systems beyond AI algorithms and models. To operationalize
responsible AI from a system perspective, in this paper, we present a
Responsible AI Pattern Catalogue based on the results of a Multivocal
Literature Review (MLR). Rather than staying at the principle or algorithm
level, we focus on patterns that AI system stakeholders can undertake in
practice to ensure that the developed AI systems are responsible throughout the
entire governance and engineering lifecycle. The Responsible AI Pattern
Catalogue classifies the patterns into three groups: multi-level governance
patterns, trustworthy process patterns, and responsible-AI-by-design product
patterns. These patterns provide systematic and actionable guidance for
stakeholders to implement responsible AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1&quot;&gt;Qinghua Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Liming Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiwei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whittle_J/0/1/0/all/0/1&quot;&gt;Jon Whittle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zowghi_D/0/1/0/all/0/1&quot;&gt;Didar Zowghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacquet_A/0/1/0/all/0/1&quot;&gt;Aurelie Jacquet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.12814">
<title>Vertical Federated Learning: Concepts, Advances and Challenges. (arXiv:2211.12814v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.12814</link>
<description rdf:parseType="Literal">&lt;p&gt;Vertical Federated Learning (VFL) is a federated learning setting where
multiple parties with different features about the same set of users jointly
train machine learning models without exposing their raw data or model
parameters. Motivated by the rapid growth in VFL research and real-world
applications, we provide a comprehensive review of the concept and algorithms
of VFL, as well as current advances and challenges in various aspects,
including effectiveness, efficiency, and privacy. We provide an exhaustive
categorization for VFL settings and privacy-preserving protocols and
comprehensively analyze the privacy attacks and defense strategies for each
protocol. In the end, we propose a unified framework, termed VFLow, which
considers the VFL problem under communication, computation, privacy, as well as
effectiveness and fairness constraints. Finally, we review the most recent
advances in industrial applications, highlighting open challenges and future
directions for VFL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1&quot;&gt;Yan Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_T/0/1/0/all/0/1&quot;&gt;Tianyuan Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pu_Y/0/1/0/all/0/1&quot;&gt;Yanhong Pu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yuanqin He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1&quot;&gt;Xiaozhou Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_Y/0/1/0/all/0/1&quot;&gt;Ye Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Ya-Qin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qiang Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.11562">
<title>Is My Prediction Arbitrary? Confounding Effects of Variance in Fair Classification. (arXiv:2301.11562v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.11562</link>
<description rdf:parseType="Literal">&lt;p&gt;Variance in predictions across different trained models is a significant,
under-explored source of error in fair classification. In practice, the
variance on some data examples is so large that decisions can be effectively
arbitrary. To investigate this problem, we take an experimental approach and
make four overarching contributions: We 1) Define a metric called
self-consistency, derived from variance, which we use as a proxy for measuring
and reducing arbitrariness; 2) Develop an ensembling algorithm that abstains
from classification when a prediction would be arbitrary; 3) Conduct the
largest to-date empirical study of the role of variance (vis-a-vis
self-consistency and arbitrariness) in fair classification; and, 4) Release a
toolkit that makes the US Home Mortgage Disclosure Act (HMDA) datasets easily
usable for future research. Altogether, our experiments reveal shocking
insights about the reliability of conclusions on benchmark datasets. Most
fairness classification benchmarks are close-to-fair when taking into account
the amount of arbitrariness present in predictions -- before we even try to
apply common fairness interventions. This finding calls into question the
practical utility of common algorithmic fairness methods, and in turn suggests
that we should fundamentally reconsider how we choose to measure fairness in
machine learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cooper_A/0/1/0/all/0/1&quot;&gt;A. Feder Cooper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Katherine Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choksi_M/0/1/0/all/0/1&quot;&gt;Madiha Choksi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barocas_S/0/1/0/all/0/1&quot;&gt;Solon Barocas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1&quot;&gt;Christopher De Sa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grimmelmann_J/0/1/0/all/0/1&quot;&gt;James Grimmelmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleinberg_J/0/1/0/all/0/1&quot;&gt;Jon Kleinberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sen_S/0/1/0/all/0/1&quot;&gt;Siddhartha Sen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Baobao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.00407">
<title>On the Role of Morphological Information for Contextual Lemmatization. (arXiv:2302.00407v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2302.00407</link>
<description rdf:parseType="Literal">&lt;p&gt;Lemmatization is a natural language processing (NLP) task which consists of
producing, from a given inflected word, its canonical form or lemma.
Lemmatization is one of the basic tasks that facilitate downstream NLP
applications, and is of particular importance for high-inflected languages.
Given that the process to obtain a lemma from an inflected word can be
explained by looking at its morphosyntactic category, including fine-grained
morphosyntactic information to train contextual lemmatizers has become common
practice, without considering whether that is the optimum in terms of
downstream performance. In order to address this issue, in this paper we
empirically investigate the role of morphological information to develop
contextual lemmatizers in six languages within a varied spectrum of
morphological complexity: Basque, Turkish, Russian, Czech, Spanish and English.
Furthermore, and unlike the vast majority of previous work, we also evaluate
lemmatizers in out-of-domain settings, which constitutes, after all, their most
common application use. The results of our study are rather surprising. It
turns out that providing lemmatizers with fine-grained morphological features
during training is not that beneficial, not even for agglutinative languages.
In fact, modern contextual word representations seem to implicitly encode
enough morphological information to obtain competitive contextual lemmatizers
without seeing any explicit morphological signal. Moreover, our experiments
suggest that the best lemmatizers out-of-domain are those using simple UPOS
tags or those trained without morphology and, finally, that current evaluation
practices for lemmatization are not adequate to clearly discriminate between
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toporkov_O/0/1/0/all/0/1&quot;&gt;Olia Toporkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agerri_R/0/1/0/all/0/1&quot;&gt;Rodrigo Agerri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.03037">
<title>EvCenterNet: Uncertainty Estimation for Object Detection using Evidential Learning. (arXiv:2303.03037v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.03037</link>
<description rdf:parseType="Literal">&lt;p&gt;Uncertainty estimation is crucial in safety-critical settings such as
automated driving as it provides valuable information for several downstream
tasks including high-level decision making and path planning. In this work, we
propose EvCenterNet, a novel uncertainty-aware 2D object detection framework
using evidential learning to directly estimate both classification and
regression uncertainties. To employ evidential learning for object detection,
we devise a combination of evidential and focal loss functions for the sparse
heatmap inputs. We introduce class-balanced weighting for regression and
heatmap prediction to tackle the class imbalance encountered by evidential
learning. Moreover, we propose a learning scheme to actively utilize the
predicted heatmap uncertainties to improve the detection performance by
focusing on the most uncertain points. We train our model on the KITTI dataset
and evaluate it on challenging out-of-distribution datasets including BDD100K
and nuImages. Our experiments demonstrate that our approach improves the
precision and minimizes the execution time loss in relation to the base model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nallapareddy_M/0/1/0/all/0/1&quot;&gt;Monish R. Nallapareddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sirohi_K/0/1/0/all/0/1&quot;&gt;Kshitij Sirohi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+J%2E_P/0/1/0/all/0/1&quot;&gt;Paulo L. J. Drews-Jr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1&quot;&gt;Wolfram Burgard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1&quot;&gt;Chih-Hong Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1&quot;&gt;Abhinav Valada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.11003">
<title>Tubelet-Contrastive Self-Supervision for Video-Efficient Generalization. (arXiv:2303.11003v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.11003</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a self-supervised method for learning motion-focused video
representations. Existing approaches minimize distances between temporally
augmented videos, which maintain high spatial similarity. We instead propose to
learn similarities between videos with identical local motion dynamics but an
otherwise different appearance. We do so by adding synthetic motion
trajectories to videos which we refer to as tubelets. By simulating different
tubelet motions and applying transformations, such as scaling and rotation, we
introduce motion patterns beyond what is present in the pretraining data. This
allows us to learn a video representation that is remarkably data efficient:
our approach maintains performance when using only 25\% of the pretraining
videos. Experiments on 10 diverse downstream settings demonstrate our
competitive performance and generalizability to new domains and fine-grained
actions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thoker_F/0/1/0/all/0/1&quot;&gt;Fida Mohammad Thoker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doughty_H/0/1/0/all/0/1&quot;&gt;Hazel Doughty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1&quot;&gt;Cees Snoek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.16296">
<title>Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels. (arXiv:2303.16296v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.16296</link>
<description rdf:parseType="Literal">&lt;p&gt;The soft Dice loss (SDL) has taken a pivotal role in numerous automated
segmentation pipelines in the medical imaging community. Over the last years,
some reasons behind its superior functioning have been uncovered and further
optimizations have been explored. However, there is currently no implementation
that supports its direct utilization in scenarios involving soft labels. Hence,
a synergy between the use of SDL and research leveraging the use of soft
labels, also in the context of model calibration, is still missing. In this
work, we introduce Dice semimetric losses (DMLs), which (i) are by design
identical to SDL in a standard setting with hard labels, but (ii) can be
employed in settings with soft labels. Our experiments on the public QUBIQ,
LiTS and KiTS benchmarks confirm the potential synergy of DMLs with soft labels
(e.g.\ averaging, label smoothing, and knowledge distillation) over hard labels
(e.g.\ majority voting and random selection). As a result, we obtain superior
Dice scores and model calibration, which supports the wider adoption of DMLs in
practice. The code is available at
\href{https://github.com/zifuwanggg/JDTLosses}{https://github.com/zifuwanggg/JDTLosses}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zifu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Popordanoska_T/0/1/0/all/0/1&quot;&gt;Teodora Popordanoska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertels_J/0/1/0/all/0/1&quot;&gt;Jeroen Bertels&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lemmens_R/0/1/0/all/0/1&quot;&gt;Robin Lemmens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blaschko_M/0/1/0/all/0/1&quot;&gt;Matthew B. Blaschko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.06366">
<title>IBIA: An Incremental Build-Infer-Approximate Framework for Approximate Inference of Partition Function. (arXiv:2304.06366v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2304.06366</link>
<description rdf:parseType="Literal">&lt;p&gt;Exact computation of the partition function is known to be intractable,
necessitating approximate inference techniques. Existing methods for
approximate inference are slow to converge for many benchmarks. The control of
accuracy-complexity trade-off is also non-trivial in many of these methods. We
propose a novel incremental build-infer-approximate (IBIA) framework for
approximate inference that addresses these issues. In this framework, the
probabilistic graphical model is converted into a sequence of clique tree
forests (SCTF) with bounded clique sizes. We show that the SCTF can be used to
efficiently compute the partition function. We propose two new algorithms which
are used to construct the SCTF and prove the correctness of both. The first is
an algorithm for incremental construction of CTFs that is guaranteed to give a
valid CTF with bounded clique sizes and the second is an approximation
algorithm that takes a calibrated CTF as input and yields a valid and
calibrated CTF with reduced clique sizes as the output. We have evaluated our
method using several benchmark sets from recent UAI competitions and our
results show good accuracies with competitive runtimes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bathla_S/0/1/0/all/0/1&quot;&gt;Shivani Bathla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasudevan_V/0/1/0/all/0/1&quot;&gt;Vinita Vasudevan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.08551">
<title>Generative Disco: Text-to-Video Generation for Music Visualization. (arXiv:2304.08551v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2304.08551</link>
<description rdf:parseType="Literal">&lt;p&gt;Visuals can enhance our experience of music, owing to the way they can
amplify the emotions and messages conveyed within it. However, creating music
visualization is a complex, time-consuming, and resource-intensive process. We
introduce Generative Disco, a generative AI system that helps generate music
visualizations with large language models and text-to-video generation. The
system helps users visualize music in intervals by finding prompts to describe
the images that intervals start and end on and interpolating between them to
the beat of the music. We introduce design patterns for improving these
generated videos: transitions, which express shifts in color, time, subject, or
style, and holds, which help focus the video on subjects. A study with
professionals showed that transitions and holds were a highly expressive
framework that enabled them to build coherent visual narratives. We conclude on
the generalizability of these patterns and the potential of generated video for
creative professionals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_V/0/1/0/all/0/1&quot;&gt;Vivian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_T/0/1/0/all/0/1&quot;&gt;Tao Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raw_N/0/1/0/all/0/1&quot;&gt;Nathan Raw&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chilton_L/0/1/0/all/0/1&quot;&gt;Lydia Chilton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.10749">
<title>Emergence of Brain-inspired Small-world Spiking Neural Network through Neuroevolution. (arXiv:2304.10749v4 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2304.10749</link>
<description rdf:parseType="Literal">&lt;p&gt;Human brain is the product of evolution during hundreds over millions of
years and can engage in multiple advanced cognitive functions with low energy
consumption. Brain-inspired artificial intelligence serves as a computational
continuation of this natural evolutionary process, is imperative to take
inspiration from the evolutionary mechanisms of brain structure and function.
Studies suggest that the human brain&apos;s high efficiency and low energy
consumption may be closely related to its small-world topology and critical
dynamics. However, existing efforts on the performance-oriented structural
evolution of spiking neural networks (SNNs) are time-consuming and ignore the
core structural properties of the brain. In this paper, we propose a
multi-objective Evolutionary Liquid State Machine (ELSM) with the combination
of small-world coefficient and criticality as evolution goals and
simultaneously integrate the topological properties of spiking neural networks
from static and dynamic perspectives to guide the emergence of brain-inspired
efficient structures. Extensive experiments show a consistent and comparable
performance of the proposed model compared to LSM-based and hierarchical SNNs
algorithms: it achieves 97.23\% on NMNIST, and reaches the state-of-art
performance compared to all LSM models on MNIST and Fashion-MNIST (98.05\% and
88.81\%, respectively). A thorough analysis reveals the spontaneous emergence
of hub nodes, short paths, long-tailed degree distributions, and numerous
community structures in evolutionary models. This work evolves recurrent
spiking neural networks into brain-inspired efficient structures and dynamics,
providing the potential to achieve adaptive general aritficial intelligence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1&quot;&gt;Wenxuan Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1&quot;&gt;Feifei Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1&quot;&gt;Bing Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yiting Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1&quot;&gt;Yi Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.12014">
<title>Optimal Layout Synthesis for Quantum Circuits as Classical Planning (full version). (arXiv:2304.12014v4 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2304.12014</link>
<description rdf:parseType="Literal">&lt;p&gt;In Layout Synthesis, the logical qubits of a quantum circuit are mapped to
the physical qubits of a given quantum hardware platform, taking into account
the connectivity of physical qubits. This involves inserting SWAP gates before
an operation is applied on distant qubits. Optimal Layout Synthesis is crucial
for practical Quantum Computing on current error-prone hardware: Minimizing the
number of SWAP gates directly mitigates the error rates when running quantum
circuits. In recent years, several approaches have been proposed for minimizing
the required SWAP insertions. The proposed exact approaches can only scale to a
small number of qubits. Proving that a number of swap insertions is optimal is
much harder than producing near optimal mappings. In this paper, we provide two
encodings for Optimal Layout Synthesis as a classical planning problem. We use
optimal classical planners to synthesize the optimal layout for a standard set
of benchmarks. Our results show the scalability of our approach compared to
previous leading approaches. We can optimally map circuits with 9 qubits onto a
14 qubit platform, which could not be handled before by exact methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Shaik_I/0/1/0/all/0/1&quot;&gt;Irfansha Shaik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Pol_J/0/1/0/all/0/1&quot;&gt;Jaco van de Pol&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.03701">
<title>LMEye: An Interactive Perception Network for Large Language Models. (arXiv:2305.03701v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.03701</link>
<description rdf:parseType="Literal">&lt;p&gt;Training a Multimodal Large Language Model (MLLM) from scratch, like GPT-4,
is resource-intensive. Regarding Large Language Models (LLMs) as the core
processor for multimodal information, our paper introduces LMEye, a human-like
eye with a play-and-plug interactive perception network, designed to enable
dynamic interaction between LLMs and external vision information. Previous
methods incorporate visual information into LLMs with a simple visual mapping
network or Q-former from BLIP-2. Such networks project the image feature once
yet do not consider the interaction between the image and the human input
query. Hence, the obtained visual information without being connected to human
intention may be inadequate for LLMs to generate intention-following responses,
which we refer to as static visual information. LMEye addresses this issue by
allowing the LLM to request the desired visual information aligned with various
human instructions, which we term as the dynamic visual information
interaction. Specifically, LMEye consists of a simple visual mapping network to
provide the basic perception of an image for LLMs. It also contains additional
modules responsible for acquiring requests from LLMs, performing request-based
visual information interaction, and transmitting the resulting interacted
visual information to LLMs, respectively. In this way, LLMs act to understand
the human query, deliver the corresponding request to the request-based visual
information interaction module, and generate the response based on the
interleaved multimodal information. We evaluate LMEye through extensive
experiments on some multimodal benchmarks, demonstrating that it significantly
improves the zero-shot performance on various multimodal tasks compared to
previous methods, with less parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunxin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1&quot;&gt;Baotian Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Min Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.03942">
<title>HACMan: Learning Hybrid Actor-Critic Maps for 6D Non-Prehensile Manipulation. (arXiv:2305.03942v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2305.03942</link>
<description rdf:parseType="Literal">&lt;p&gt;Manipulating objects without grasping them is an essential component of human
dexterity, referred to as non-prehensile manipulation. Non-prehensile
manipulation may enable more complex interactions with the objects, but also
presents challenges in reasoning about gripper-object interactions. In this
work, we introduce Hybrid Actor-Critic Maps for Manipulation (HACMan), a
reinforcement learning approach for 6D non-prehensile manipulation of objects
using point cloud observations. HACMan proposes a temporally-abstracted and
spatially-grounded object-centric action representation that consists of
selecting a contact location from the object point cloud and a set of motion
parameters describing how the robot will move after making contact. We modify
an existing off-policy RL algorithm to learn in this hybrid discrete-continuous
action representation. We evaluate HACMan on a 6D object pose alignment task in
both simulation and in the real world. On the hardest version of our task, with
randomized initial poses, randomized 6D goals, and diverse object categories,
our policy demonstrates strong generalization to unseen object categories
without a performance drop, achieving an 89% success rate on unseen objects in
simulation and 50% success rate with zero-shot transfer in the real world.
Compared to alternative action representations, HACMan achieves a success rate
more than three times higher than the best baseline. With zero-shot sim2real
transfer, our policy can successfully manipulate unseen objects in the real
world for challenging non-planar goals, using dynamic and contact-rich
non-prehensile skills. Videos can be found on the project website:
https://hacman-2023.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wenxuan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1&quot;&gt;Bowen Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Fan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1&quot;&gt;Chris Paxton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1&quot;&gt;David Held&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.04811">
<title>Deep learning models for price forecasting of financial time series: A review of recent advancements: 2020-2022. (arXiv:2305.04811v2 [q-fin.ST] UPDATED)</title>
<link>http://arxiv.org/abs/2305.04811</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurately predicting the prices of financial time series is essential and
challenging for the financial sector. Owing to recent advancements in deep
learning techniques, deep learning models are gradually replacing traditional
statistical and machine learning models as the first choice for price
forecasting tasks. This shift in model selection has led to a notable rise in
research related to applying deep learning models to price forecasting,
resulting in a rapid accumulation of new knowledge. Therefore, we conducted a
literature review of relevant studies over the past three years with a view to
aiding researchers and practitioners in the field. This review delves deeply
into deep learning-based forecasting models, presenting information on model
architectures, practical applications, and their respective advantages and
disadvantages. In particular, detailed information is provided on advanced
models for price forecasting, such as Transformers, generative adversarial
networks (GANs), graph neural networks (GNNs), and deep quantum neural networks
(DQNNs). The present contribution also includes potential directions for future
research, such as examining the effectiveness of deep learning models with
complex structures for price forecasting, extending from point prediction to
interval prediction using deep learning models, scrutinising the reliability
and validity of decomposition ensembles, and exploring the influence of data
volume on model performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Cheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Sjarif_N/0/1/0/all/0/1&quot;&gt;Nilam Nur Amir Sjarif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Ibrahim_R/0/1/0/all/0/1&quot;&gt;Roslina Ibrahim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.04866">
<title>Causal Policy Gradient for Whole-Body Mobile Manipulation. (arXiv:2305.04866v4 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2305.04866</link>
<description rdf:parseType="Literal">&lt;p&gt;Developing the next generation of household robot helpers requires combining
locomotion and interaction capabilities, which is generally referred to as
mobile manipulation (MoMa). MoMa tasks are difficult due to the large action
space of the robot and the common multi-objective nature of the task, e.g.,
efficiently reaching a goal while avoiding obstacles. Current approaches often
segregate tasks into navigation without manipulation and stationary
manipulation without locomotion by manually matching parts of the action space
to MoMa sub-objectives (e.g. learning base actions for locomotion objectives
and learning arm actions for manipulation). This solution prevents simultaneous
combinations of locomotion and interaction degrees of freedom and requires
human domain knowledge for both partitioning the action space and matching the
action parts to the sub-objectives. In this paper, we introduce Causal MoMa, a
new reinforcement learning framework to train policies for typical MoMa tasks
that makes use of the most favorable subspace of the robot&apos;s action space to
address each sub-objective. Causal MoMa automatically discovers the causal
dependencies between actions and terms of the reward function and exploits
these dependencies through causal policy gradient that reduces gradient
variance compared to previous state-of-the-art reinforcement learning
algorithms, improving convergence and results. We evaluate the performance of
Causal MoMa on three types of simulated robots across different MoMa tasks and
demonstrate success in transferring the policies trained in simulation directly
to a real robot, where our agent is able to follow moving goals and react to
dynamic obstacles while simultaneously and synergistically controlling the
whole-body: base, arm, and head. More information at
https://sites.google.com/view/causal-moma.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jiaheng Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1&quot;&gt;Peter Stone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1&quot;&gt;Roberto Mart&amp;#xed;n-Mart&amp;#xed;n&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.11364">
<title>Visualizing Linguistic Diversity of Text Datasets Synthesized by Large Language Models. (arXiv:2305.11364v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.11364</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) can be used to generate smaller, more refined
datasets via few-shot prompting for benchmarking, fine-tuning or other use
cases. However, understanding and evaluating these datasets is difficult, and
the failure modes of LLM-generated data are still not well understood.
Specifically, the data can be repetitive in surprising ways, not only
semantically but also syntactically and lexically. We present LinguisticLens, a
novel inter-active visualization tool for making sense of and analyzing
syntactic diversity of LLM-generated datasets. LinguisticLens clusters text
along syntactic, lexical, and semantic axes. It supports hierarchical
visualization of a text dataset, allowing users to quickly scan for an overview
and inspect individual examples. The live demo is available at
shorturl.at/zHOUV.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reif_E/0/1/0/all/0/1&quot;&gt;Emily Reif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kahng_M/0/1/0/all/0/1&quot;&gt;Minsuk Kahng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petridis_S/0/1/0/all/0/1&quot;&gt;Savvas Petridis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14091">
<title>Revisiting Acceptability Judgements. (arXiv:2305.14091v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14091</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we revisit linguistic acceptability in the context of large
language models. We introduce CoLAC - Corpus of Linguistic Acceptability in
Chinese, the first large-scale acceptability dataset for a non-Indo-European
language. It is verified by native speakers and is the first acceptability
dataset that comes with two sets of labels: a linguist label and a crowd label.
Our experiments show that even the largest InstructGPT model performs only at
chance level on CoLAC, while ChatGPT&apos;s performance (48.30 MCC) is also much
below supervised models (59.03 MCC) and human (65.11 MCC). Through
cross-lingual transfer experiments and fine-grained linguistic analysis, we
provide detailed analysis of the model predictions and demonstrate for the
first time that knowledge of linguistic acceptability can be transferred across
typologically distinct languages, as well as be traced back to pre-training.
Our dataset is publicly available at
\url{https://github.com/huhailinguist/CoLAC}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Hai Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Ziyin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Weifang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1&quot;&gt;Jackie Yan-Ki Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1&quot;&gt;Aini Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patterson_Y/0/1/0/all/0/1&quot;&gt;Yina Patterson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jiahui Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Peng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chien-Jer Charles Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01026">
<title>Temporal Graph Benchmark for Machine Learning on Temporal Graphs. (arXiv:2307.01026v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01026</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the Temporal Graph Benchmark (TGB), a collection of challenging
and diverse benchmark datasets for realistic, reproducible, and robust
evaluation of machine learning models on temporal graphs. TGB datasets are of
large scale, spanning years in duration, incorporate both node and edge-level
prediction tasks and cover a diverse set of domains including social, trade,
transaction, and transportation networks. For both tasks, we design evaluation
protocols based on realistic use-cases. We extensively benchmark each dataset
and find that the performance of common models can vary drastically across
datasets. In addition, on dynamic node property prediction tasks, we show that
simple methods often achieve superior performance compared to existing temporal
graph models. We believe that these findings open up opportunities for future
research on temporal graphs. Finally, TGB provides an automated machine
learning pipeline for reproducible and accessible temporal graph research,
including data loading, experiment setup and performance evaluation. TGB will
be maintained and updated on a regular basis and welcomes community feedback.
TGB datasets, data loaders, example codes, evaluation setup, and leaderboards
are publicly available at https://tgb.complexdatalab.com/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shenyang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poursafaei_F/0/1/0/all/0/1&quot;&gt;Farimah Poursafaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Danovitch_J/0/1/0/all/0/1&quot;&gt;Jacob Danovitch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fey_M/0/1/0/all/0/1&quot;&gt;Matthias Fey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1&quot;&gt;Weihua Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rossi_E/0/1/0/all/0/1&quot;&gt;Emanuele Rossi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1&quot;&gt;Jure Leskovec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bronstein_M/0/1/0/all/0/1&quot;&gt;Michael Bronstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rabusseau_G/0/1/0/all/0/1&quot;&gt;Guillaume Rabusseau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rabbany_R/0/1/0/all/0/1&quot;&gt;Reihaneh Rabbany&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06135">
<title>SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning. (arXiv:2307.06135v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06135</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have demonstrated impressive results in
developing generalist planning agents for diverse tasks. However, grounding
these plans in expansive, multi-floor, and multi-room environments presents a
significant challenge for robotics. We introduce SayPlan, a scalable approach
to LLM-based, large-scale task planning for robotics using 3D scene graph
(3DSG) representations. To ensure the scalability of our approach, we: (1)
exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a &apos;semantic
search&apos; for task-relevant subgraphs from a smaller, collapsed representation of
the full graph; (2) reduce the planning horizon for the LLM by integrating a
classical path planner and (3) introduce an &apos;iterative replanning&apos; pipeline
that refines the initial plan using feedback from a scene graph simulator,
correcting infeasible actions and avoiding planning failures. We evaluate our
approach on two large-scale environments spanning up to 3 floors and 36 rooms
with 140 assets and objects and show that our approach is capable of grounding
large-scale, long-horizon task plans from abstract, and natural language
instruction for a mobile manipulator robot to execute. We provide real robot
video demonstrations on our project page https://sayplan.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rana_K/0/1/0/all/0/1&quot;&gt;Krishan Rana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haviland_J/0/1/0/all/0/1&quot;&gt;Jesse Haviland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1&quot;&gt;Sourav Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abou_Chakra_J/0/1/0/all/0/1&quot;&gt;Jad Abou-Chakra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1&quot;&gt;Ian Reid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suenderhauf_N/0/1/0/all/0/1&quot;&gt;Niko Suenderhauf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06822">
<title>TinyMetaFed: Efficient Federated Meta-Learning for TinyML. (arXiv:2307.06822v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06822</link>
<description rdf:parseType="Literal">&lt;p&gt;The field of Tiny Machine Learning (TinyML) has made substantial advancements
in democratizing machine learning on low-footprint devices, such as
microcontrollers. The prevalence of these miniature devices raises the question
of whether aggregating their knowledge can benefit TinyML applications.
Federated meta-learning is a promising answer to this question, as it addresses
the scarcity of labeled data and heterogeneous data distribution across devices
in the real world. However, deploying TinyML hardware faces unique resource
constraints, making existing methods impractical due to energy, privacy, and
communication limitations. We introduce TinyMetaFed, a model-agnostic
meta-learning framework suitable for TinyML. TinyMetaFed facilitates
collaborative training of a neural network initialization that can be quickly
fine-tuned on new devices. It offers communication savings and privacy
protection through partial local reconstruction and Top-P% selective
communication, computational efficiency via online learning, and robustness to
client heterogeneity through few-shot learning. The evaluations on three TinyML
use cases demonstrate that TinyMetaFed can significantly reduce energy
consumption and communication overhead, accelerate convergence, and stabilize
the training process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1&quot;&gt;Haoyu Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xue Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anicic_D/0/1/0/all/0/1&quot;&gt;Darko Anicic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Runkler_T/0/1/0/all/0/1&quot;&gt;Thomas A. Runkler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04412">
<title>Probabilistic Invariant Learning with Randomized Linear Classifiers. (arXiv:2308.04412v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.04412</link>
<description rdf:parseType="Literal">&lt;p&gt;Designing models that are both expressive and preserve known invariances of
tasks is an increasingly hard problem. Existing solutions tradeoff invariance
for computational or memory resources. In this work, we show how to leverage
randomness and design models that are both expressive and invariant but use
less resources. Inspired by randomized algorithms, our key insight is that
accepting probabilistic notions of universal approximation and invariance can
reduce our resource requirements. More specifically, we propose a class of
binary classification models called Randomized Linear Classifiers (RLCs). We
give parameter and sample size conditions in which RLCs can, with high
probability, approximate any (smooth) function while preserving invariance to
compact group transformations. Leveraging this result, we design three RLCs
that are provably probabilistic invariant for classification tasks over sets,
graphs, and spherical data. We show how these models can achieve probabilistic
invariance and universality using less resources than (deterministic) neural
networks and their invariant counterparts. Finally, we empirically demonstrate
the benefits of this new class of models on invariant tasks where deterministic
invariant neural networks are known to struggle.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cotta_L/0/1/0/all/0/1&quot;&gt;Leonardo Cotta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yehuda_G/0/1/0/all/0/1&quot;&gt;Gal Yehuda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuster_A/0/1/0/all/0/1&quot;&gt;Assaf Schuster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maddison_C/0/1/0/all/0/1&quot;&gt;Chris J. Maddison&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.06595">
<title>VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use. (arXiv:2308.06595v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.06595</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark for
evaluation of instruction-following vision-language models for real-world use.
Our starting point is curating 70 &apos;instruction families&apos; that we envision
instruction tuned vision-language models should be able to address. Extending
beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to
game playing and creative generation. Following curation, our dataset comprises
592 test queries, each with a human-authored instruction-conditioned caption.
These descriptions surface instruction-specific factors, e.g., for an
instruction asking about the accessibility of a storefront for wheelchair
users, the instruction-conditioned caption describes ramps/potential obstacles.
These descriptions enable 1) collecting human-verified reference outputs for
each instance; and 2) automatic evaluation of candidate multimodal generations
using a text-only LLM, aligning with human judgment. We quantify quality gaps
between models and references using both human and automatic evaluations; e.g.,
the top-performing instruction-following model wins against the GPT-4 reference
in just 27% of the comparison. VisIT-Bench is dynamic to participate,
practitioners simply submit their model&apos;s response on the project website;
Data, code and leaderboard is available at visit-bench.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1&quot;&gt;Yonatan Bitton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_H/0/1/0/all/0/1&quot;&gt;Hritik Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1&quot;&gt;Jack Hessel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1&quot;&gt;Rulin Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wanrong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awadalla_A/0/1/0/all/0/1&quot;&gt;Anas Awadalla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gardner_J/0/1/0/all/0/1&quot;&gt;Josh Gardner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taori_R/0/1/0/all/0/1&quot;&gt;Rohan Taori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schimdt_L/0/1/0/all/0/1&quot;&gt;Ludwig Schimdt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.06922">
<title>Probabilistic contingent planning based on HTN for high-quality plans. (arXiv:2308.06922v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.06922</link>
<description rdf:parseType="Literal">&lt;p&gt;Deterministic planning assumes that the planning evolves along a fully
predictable path, and therefore it loses the practical value in most real
projections. A more realistic view is that planning ought to take into
consideration partial observability beforehand and aim for a more flexible and
robust solution. What is more significant, it is inevitable that the quality of
plan varies dramatically in the partially observable environment. In this paper
we propose a probabilistic contingent Hierarchical Task Network (HTN) planner,
named High-Quality Contingent Planner (HQCP), to generate high-quality plans in
the partially observable environment. The formalisms in HTN planning are
extended into partial observability and are evaluated regarding the cost. Next,
we explore a novel heuristic for high-quality plans and develop the integrated
planning algorithm. Finally, an empirical study verifies the effectiveness and
efficiency of the planner both in probabilistic contingent planning and for
obtaining high-quality plans.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1&quot;&gt;Peng Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.09267">
<title>Enhancing Reasoning Capabilities of Large Language Models: A Graph-Based Verification Approach. (arXiv:2308.09267v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.09267</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have showcased impressive reasoning
capabilities, particularly when guided by specifically designed prompts in
complex reasoning tasks such as math word problems. These models typically
solve tasks using a chain-of-thought approach, which not only bolsters their
reasoning abilities but also provides valuable insights into their
problem-solving process. However, there is still significant room for enhancing
the reasoning abilities of LLMs. Some studies suggest that the integration of
an LLM output verifier can boost reasoning accuracy without necessitating
additional model training. In this paper, we follow these studies and introduce
a novel graph-based method to further augment the reasoning capabilities of
LLMs. We posit that multiple solutions to a reasoning task, generated by an
LLM, can be represented as a reasoning graph due to the logical connections
between intermediate steps from different reasoning paths. Therefore, we
propose the Reasoning Graph Verifier (RGV) to analyze and verify the solutions
generated by LLMs. By evaluating these graphs, models can yield more accurate
and reliable results.Our experimental results show that our graph-based
verification method not only significantly enhances the reasoning abilities of
LLMs but also outperforms existing verifier methods in terms of improving these
models&apos; reasoning performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1&quot;&gt;Lang Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.09830">
<title>Synergistic Integration of Large Language Models and Cognitive Architectures for Robust AI: An Exploratory Analysis. (arXiv:2308.09830v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.09830</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the integration of two AI subdisciplines employed in the
development of artificial agents that exhibit intelligent behavior: Large
Language Models (LLMs) and Cognitive Architectures (CAs). We present three
integration approaches, each grounded in theoretical models and supported by
preliminary empirical evidence. The modular approach, which introduces four
models with varying degrees of integration, makes use of chain-of-thought
prompting, and draws inspiration from augmented LLMs, the Common Model of
Cognition, and the simulation theory of cognition. The agency approach,
motivated by the Society of Mind theory and the LIDA cognitive architecture,
proposes the formation of agent collections that interact at micro and macro
cognitive levels, driven by either LLMs or symbolic components. The
neuro-symbolic approach, which takes inspiration from the CLARION cognitive
architecture, proposes a model where bottom-up learning extracts symbolic
representations from an LLM layer and top-down guidance utilizes symbolic
representations to direct prompt engineering in the LLM layer. These approaches
aim to harness the strengths of both LLMs and CAs, while mitigating their
weaknesses, thereby advancing the development of more robust AI systems. We
discuss the tradeoffs and challenges associated with each approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romero_O/0/1/0/all/0/1&quot;&gt;Oscar J. Romero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zimmerman_J/0/1/0/all/0/1&quot;&gt;John Zimmerman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steinfeld_A/0/1/0/all/0/1&quot;&gt;Aaron Steinfeld&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomasic_A/0/1/0/all/0/1&quot;&gt;Anthony Tomasic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.10379">
<title>Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models. (arXiv:2308.10379v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.10379</link>
<description rdf:parseType="Literal">&lt;p&gt;Current literature, aiming to surpass the &quot;Chain-of-Thought&quot; approach, often
resorts to an external modus operandi involving halting, modifying, and then
resuming the generation process to boost Large Language Models&apos; (LLMs)
reasoning capacities. This mode escalates the number of query requests, leading
to increased costs, memory, and computational overheads. Addressing this, we
propose the Algorithm of Thoughts -- a novel strategy that propels LLMs through
algorithmic reasoning pathways, pioneering a new mode of in-context learning.
By employing algorithmic examples, we exploit the innate recurrence dynamics of
LLMs, expanding their idea exploration with merely one or a few queries. Our
technique outperforms earlier single-query methods and stands on par with a
recent multi-query strategy that employs an extensive tree search algorithm.
Intriguingly, our results suggest that instructing an LLM using an algorithm
can lead to performance surpassing that of the algorithm itself, hinting at
LLM&apos;s inherent ability to weave its intuition into optimized searches. We probe
into the underpinnings of our method&apos;s efficacy and its nuances in application.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sel_B/0/1/0/all/0/1&quot;&gt;Bilgehan Sel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Tawaha_A/0/1/0/all/0/1&quot;&gt;Ahmad Al-Tawaha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khattar_V/0/1/0/all/0/1&quot;&gt;Vanshaj Khattar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1&quot;&gt;Ruoxi Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1&quot;&gt;Ming Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.13978">
<title>A Graph Neural Network-Based QUBO-Formulated Hamiltonian-Inspired Loss Function for Combinatorial Optimization using Reinforcement Learning. (arXiv:2308.13978v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.13978</link>
<description rdf:parseType="Literal">&lt;p&gt;Quadratic Unconstrained Binary Optimization (QUBO) is a generic technique to
model various NP-hard combinatorial optimization problems in the form of binary
variables. The Hamiltonian function is often used to formulate QUBO problems
where it is used as the objective function in the context of optimization.
Recently, PI-GNN, a generic scalable framework, has been proposed to address
the Combinatorial Optimization (CO) problems over graphs based on a simple
Graph Neural Network (GNN) architecture. Their novel contribution was a generic
QUBO-formulated Hamiltonian-inspired loss function that was optimized using
GNN. In this study, we address a crucial issue related to the aforementioned
setup especially observed in denser graphs. The reinforcement learning-based
paradigm has also been widely used to address numerous CO problems. Here we
also formulate and empirically evaluate the compatibility of the
QUBO-formulated Hamiltonian as the generic reward function in the Reinforcement
Learning paradigm to directly integrate the actual node projection status
during training as the form of rewards. In our experiments, we observed up to
44% improvement in the RL-based setup compared to the PI-GNN algorithm. Our
implementation can be found in
https://github.com/rizveeredwan/learning-graph-structure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rizvee_R/0/1/0/all/0/1&quot;&gt;Redwan Ahmed Rizvee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Md. Mosaddek Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.14359">
<title>Effect of Attention and Self-Supervised Speech Embeddings on Non-Semantic Speech Tasks. (arXiv:2308.14359v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.14359</link>
<description rdf:parseType="Literal">&lt;p&gt;Human emotion understanding is pivotal in making conversational technology
mainstream. We view speech emotion understanding as a perception task which is
a more realistic setting. With varying contexts (languages, demographics, etc.)
different share of people perceive the same speech segment as a non-unanimous
emotion. As part of the ACM Multimedia 2023 Computational Paralinguistics
ChallengE (ComParE) in the EMotion Share track, we leverage their rich dataset
of multilingual speakers and multi-label regression target of &apos;emotion share&apos;
or perception of that emotion. We demonstrate that the training scheme of
different foundation models dictates their effectiveness for tasks beyond
speech recognition, especially for non-semantic speech tasks like emotion
understanding. This is a very complex task due to multilingual speakers,
variability in the target labels, and inherent imbalance in the regression
dataset. Our results show that HuBERT-Large with a self-attention-based
light-weight sequence model provides 4.6% improvement over the reported
baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohapatra_P/0/1/0/all/0/1&quot;&gt;Payal Mohapatra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandey_A/0/1/0/all/0/1&quot;&gt;Akash Pandey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sui_Y/0/1/0/all/0/1&quot;&gt;Yueyuan Sui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1&quot;&gt;Qi Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.05516">
<title>Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs. (arXiv:2309.05516v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.05516</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have proven their exceptional capabilities in
performing language-related tasks. However, their deployment poses significant
challenges due to their considerable memory and storage requirements. In
response to this issue, weight-only quantization, particularly 3 and 4-bit
weight-only quantization, has emerged as one of the most viable solutions. As
the number of bits decreases, the quantization grid broadens, thus emphasizing
the importance of up and down rounding. While previous studies have
demonstrated that fine-tuning up and down rounding with the addition of
perturbations can enhance accuracy in some scenarios, our study is driven by
the precise and limited boundary of these perturbations, where only the
threshold for altering the rounding value is of significance. Consequently, we
propose a concise and highly effective approach for optimizing the weight
rounding task. Our method, named SignRound, involves lightweight block-wise
tuning using signed gradient descent, enabling us to achieve outstanding
results within 400 steps. SignRound competes impressively against recent
methods without introducing additional inference overhead. The source code will
be publicly available at \url{https://github.com/intel/neural-compressor} soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1&quot;&gt;Wenhua Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weiwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Haihao Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1&quot;&gt;Yiyang Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xin He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_K/0/1/0/all/0/1&quot;&gt;Kaokao Lv&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.06129">
<title>LEyes: A Lightweight Framework for Deep Learning-Based Eye Tracking using Synthetic Eye Images. (arXiv:2309.06129v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.06129</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has bolstered gaze estimation techniques, but real-world
deployment has been impeded by inadequate training datasets. This problem is
exacerbated by both hardware-induced variations in eye images and inherent
biological differences across the recorded participants, leading to both
feature and pixel-level variance that hinders the generalizability of models
trained on specific datasets. While synthetic datasets can be a solution, their
creation is both time and resource-intensive. To address this problem, we
present a framework called Light Eyes or &quot;LEyes&quot; which, unlike conventional
photorealistic methods, only models key image features required for video-based
eye tracking using simple light distributions. LEyes facilitates easy
configuration for training neural networks across diverse gaze-estimation
tasks. We demonstrate that models trained using LEyes are consistently on-par
or outperform other state-of-the-art algorithms in terms of pupil and CR
localization across well-known datasets. In addition, a LEyes trained model
outperforms the industry standard eye tracker using significantly more
cost-effective hardware. Going forward, we are confident that LEyes will
revolutionize synthetic data generation for gaze estimation models, and lead to
significant improvements of the next generation video-based eye trackers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Byrne_S/0/1/0/all/0/1&quot;&gt;Sean Anthony Byrne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maquiling_V/0/1/0/all/0/1&quot;&gt;Virmarie Maquiling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nystrom_M/0/1/0/all/0/1&quot;&gt;Marcus Nystr&amp;#xf6;m&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasneci_E/0/1/0/all/0/1&quot;&gt;Enkelejda Kasneci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niehorster_D/0/1/0/all/0/1&quot;&gt;Diederick C. Niehorster&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07461">
<title>Detecting Unknown Attacks in IoT Environments: An Open Set Classifier for Enhanced Network Intrusion Detection. (arXiv:2309.07461v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2309.07461</link>
<description rdf:parseType="Literal">&lt;p&gt;The widespread integration of Internet of Things (IoT) devices across all
facets of life has ushered in an era of interconnectedness, creating new
avenues for cybersecurity challenges and underscoring the need for robust
intrusion detection systems. However, traditional security systems are designed
with a closed-world perspective and often face challenges in dealing with the
ever-evolving threat landscape, where new and unfamiliar attacks are constantly
emerging. In this paper, we introduce a framework aimed at mitigating the open
set recognition (OSR) problem in the realm of Network Intrusion Detection
Systems (NIDS) tailored for IoT environments. Our framework capitalizes on
image-based representations of packet-level data, extracting spatial and
temporal patterns from network traffic. Additionally, we integrate stacking and
sub-clustering techniques, enabling the identification of unknown attacks by
effectively modeling the complex and diverse nature of benign behavior. The
empirical results prominently underscore the framework&apos;s efficacy, boasting an
impressive 88\% detection rate for previously unseen attacks when compared
against existing approaches and recent advancements. Future work will perform
extensive experimentation across various openness levels and attack scenarios,
further strengthening the adaptability and performance of our proposed solution
in safeguarding IoT environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farrukh_Y/0/1/0/all/0/1&quot;&gt;Yasir Ali Farrukh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wali_S/0/1/0/all/0/1&quot;&gt;Syed Wali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_I/0/1/0/all/0/1&quot;&gt;Irfan Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bastian_N/0/1/0/all/0/1&quot;&gt;Nathaniel D. Bastian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.09175">
<title>Imbalanced Data Stream Classification using Dynamic Ensemble Selection. (arXiv:2309.09175v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.09175</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern streaming data categorization faces significant challenges from
concept drift and class imbalanced data. This negatively impacts the output of
the classifier, leading to improper classification. Furthermore, other factors
such as the overlapping of multiple classes limit the extent of the correctness
of the output. This work proposes a novel framework for integrating data
pre-processing and dynamic ensemble selection, by formulating the
classification framework for the nonstationary drifting imbalanced data stream,
which employs the data pre-processing and dynamic ensemble selection
techniques. The proposed framework was evaluated using six artificially
generated data streams with differing imbalance ratios in combination with two
different types of concept drifts. Each stream is composed of 200 chunks of 500
objects described by eight features and contains five concept drifts. Seven
pre-processing techniques and two dynamic ensemble selection methods were
considered. According to experimental results, data pre-processing combined
with Dynamic Ensemble Selection techniques significantly delivers more accuracy
when dealing with imbalanced data streams.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+S_P/0/1/0/all/0/1&quot;&gt;Priya.S&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sivakumar_H/0/1/0/all/0/1&quot;&gt;Haribharathi Sivakumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+R_V/0/1/0/all/0/1&quot;&gt;Vijay Arvind.R&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.09979">
<title>General In-Hand Object Rotation with Vision and Touch. (arXiv:2309.09979v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2309.09979</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce RotateIt, a system that enables fingertip-based object rotation
along multiple axes by leveraging multimodal sensory inputs. Our system is
trained in simulation, where it has access to ground-truth object shapes and
physical properties. Then we distill it to operate on realistic yet noisy
simulated visuotactile and proprioceptive sensory inputs. These multimodal
inputs are fused via a visuotactile transformer, enabling online inference of
object shapes and physical properties during deployment. We show significant
performance improvements over prior methods and the importance of visual and
tactile sensing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1&quot;&gt;Haozhi Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_B/0/1/0/all/0/1&quot;&gt;Brent Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suresh_S/0/1/0/all/0/1&quot;&gt;Sudharshan Suresh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lambeta_M/0/1/0/all/0/1&quot;&gt;Mike Lambeta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yi Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calandra_R/0/1/0/all/0/1&quot;&gt;Roberto Calandra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1&quot;&gt;Jitendra Malik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13393">
<title>AgriSORT: A Simple Online Real-time Tracking-by-Detection framework for robotics in precision agriculture. (arXiv:2309.13393v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.13393</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of multi-object tracking (MOT) consists in detecting and tracking
all the objects in a video sequence while keeping a unique identifier for each
object. It is a challenging and fundamental problem for robotics. In precision
agriculture the challenge of achieving a satisfactory solution is amplified by
extreme camera motion, sudden illumination changes, and strong occlusions. Most
modern trackers rely on the appearance of objects rather than motion for
association, which can be ineffective when most targets are static objects with
the same appearance, as in the agricultural case. To this end, on the trail of
SORT [5], we propose AgriSORT, a simple, online, real-time
tracking-by-detection pipeline for precision agriculture based only on motion
information that allows for accurate and fast propagation of tracks between
frames. The main focuses of AgriSORT are efficiency, flexibility, minimal
dependencies, and ease of deployment on robotic platforms. We test the proposed
pipeline on a novel MOT benchmark specifically tailored for the agricultural
context, based on video sequences taken in a table grape vineyard, particularly
challenging due to strong self-similarity and density of the instances. Both
the code and the dataset are available for future comparisons.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saraceni_L/0/1/0/all/0/1&quot;&gt;Leonardo Saraceni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Motoi_I/0/1/0/all/0/1&quot;&gt;Ionut M. Motoi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nardi_D/0/1/0/all/0/1&quot;&gt;Daniele Nardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ciarfuglia_T/0/1/0/all/0/1&quot;&gt;Thomas A. Ciarfuglia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14181">
<title>Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision. (arXiv:2309.14181v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14181</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid evolution of Multi-modality Large Language Models (MLLMs) has
catalyzed a shift in computer vision from specialized models to general-purpose
foundation models. Nevertheless, there is still an inadequacy in assessing the
abilities of MLLMs on low-level visual perception and understanding. To address
this gap, we present Q-Bench, a holistic benchmark crafted to systematically
evaluate potential abilities of MLLMs on three realms: low-level visual
perception, low-level visual description, and overall visual quality
assessment. a) To evaluate the low-level perception ability, we construct the
LLVisionQA dataset, consisting of 2,990 diverse-sourced images, each equipped
with a human-asked question focusing on its low-level attributes. We then
measure the correctness of MLLMs on answering these questions. b) To examine
the description ability of MLLMs on low-level information, we propose the
LLDescribe dataset consisting of long expert-labelled golden low-level text
descriptions on 499 images, and a GPT-involved comparison pipeline between
outputs of MLLMs and the golden descriptions. c) Besides these two tasks, we
further measure their visual quality assessment ability to align with human
opinion scores. Specifically, we design a softmax-based strategy that enables
MLLMs to predict quantifiable quality scores, and evaluate them on various
existing image quality assessment (IQA) datasets. Our evaluation across the
three abilities confirms that MLLMs possess preliminary low-level visual
skills. However, these skills are still unstable and relatively imprecise,
indicating the need for specific enhancements on MLLMs towards these abilities.
We hope that our benchmark can encourage the research community to delve deeper
to discover and enhance these untapped potentials of MLLMs. Project Page:
https://vqassessment.github.io/Q-Bench.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Haoning Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zicheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1&quot;&gt;Erli Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chaofeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1&quot;&gt;Liang Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1&quot;&gt;Annan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chunyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Wenxiu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1&quot;&gt;Qiong Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1&quot;&gt;Guangtao Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Weisi Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14329">
<title>Innovative Digital Storytelling with AIGC: Exploration and Discussion of Recent Advances. (arXiv:2309.14329v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14329</link>
<description rdf:parseType="Literal">&lt;p&gt;Digital storytelling, as an art form, has struggled with cost-quality
balance. The emergence of AI-generated Content (AIGC) is considered as a
potential solution for efficient digital storytelling production. However, the
specific form, effects, and impacts of this fusion remain unclear, leaving the
boundaries of AIGC combined with storytelling undefined. This work explores the
current integration state of AIGC and digital storytelling, investigates the
artistic value of their fusion in a sample project, and addresses common issues
through interviews. Through our study, we conclude that AIGC, while proficient
in image creation, voiceover production, and music composition, falls short of
replacing humans due to the irreplaceable elements of human creativity and
aesthetic sensibilities at present, especially in complex character animations,
facial expressions, and sound effects. The research objective is to increase
public awareness of the current state, limitations, and challenges arising from
combining AIGC and digital storytelling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_R/0/1/0/all/0/1&quot;&gt;Rongzhang Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1&quot;&gt;Changyue Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wayne Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14564">
<title>Generative Escher Meshes. (arXiv:2309.14564v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14564</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a fully-automatic, text-guided generative method for
producing periodic, repeating, tile-able 2D art, such as the one seen on
floors, mosaics, ceramics, and the work of M.C. Escher. In contrast to the
standard concept of a seamless texture, i.e., square images that are seamless
when tiled, our method generates non-square tilings which comprise solely of
repeating copies of the same object. It achieves this by optimizing both
geometry and color of a 2D mesh, in order to generate a non-square tile in the
shape and appearance of the desired object, with close to no additional
background details. We enable geometric optimization of tilings by our key
technical contribution: an unconstrained, differentiable parameterization of
the space of all possible tileable shapes for a given symmetry group. Namely,
we prove that modifying the laplacian used in a 2D mesh-mapping technique -
Orbifold Tutte Embedding - can achieve all possible tiling configurations for a
chosen planar symmetry group. We thus consider both the mesh&apos;s tile-shape and
its texture as optimizable parameters, rendering the textured mesh via a
differentiable renderer. We leverage a trained image diffusion model to define
a loss on the resulting image, thereby updating the mesh&apos;s parameters based on
its appearance matching the text prompt. We show our method is able to produce
plausible, appealing results, with non-trivial tiles, for a variety of
different periodic tiling patterns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aigerman_N/0/1/0/all/0/1&quot;&gt;Noam Aigerman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Groueix_T/0/1/0/all/0/1&quot;&gt;Thibault Groueix&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14793">
<title>Semantic Map Learning of Traffic Light to Lane Assignment based on Motion Data. (arXiv:2309.14793v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14793</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding which traffic light controls which lane is crucial to navigate
intersections safely. Autonomous vehicles commonly rely on High Definition (HD)
maps that contain information about the assignment of traffic lights to lanes.
The manual provisioning of this information is tedious, expensive, and not
scalable. To remedy these issues, our novel approach derives the assignments
from traffic light states and the corresponding motion patterns of vehicle
traffic. This works in an automated way and independently of the geometric
arrangement. We show the effectiveness of basic statistical approaches for this
task by implementing and evaluating a pattern-based contribution method. In
addition, our novel rejection method includes accompanying safety
considerations by leveraging statistical hypothesis testing. Finally, we
propose a dataset transformation to re-purpose available motion prediction
datasets for semantic map learning. Our publicly available API for the Lyft
Level 5 dataset enables researchers to develop and evaluate their own
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monninger_T/0/1/0/all/0/1&quot;&gt;Thomas Monninger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weber_A/0/1/0/all/0/1&quot;&gt;Andreas Weber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Staab_S/0/1/0/all/0/1&quot;&gt;Steffen Staab&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15317">
<title>Joint Prediction and Denoising for Large-scale Multilingual Self-supervised Learning. (arXiv:2309.15317v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.15317</link>
<description rdf:parseType="Literal">&lt;p&gt;Multilingual self-supervised learning (SSL) has often lagged behind
state-of-the-art (SOTA) methods due to the expenses and complexity required to
handle many languages. This further harms the reproducibility of SSL, which is
already limited to few research groups due to its resource usage. We show that
more powerful techniques can actually lead to more efficient pre-training,
opening SSL to more research groups. We propose WavLabLM, which extends WavLM&apos;s
joint prediction and denoising to 40k hours of data across 136 languages. To
build WavLabLM, we devise a novel multi-stage pre-training method, designed to
address the language imbalance of multilingual data. WavLabLM achieves
comparable performance to XLS-R on ML-SUPERB with less than 10% of the training
data, making SSL realizable with academic compute. We show that further
efficiency can be achieved with a vanilla HuBERT Base model, which can maintain
94% of XLS-R&apos;s performance with only 3% of the data, 4 GPUs, and limited
trials. We open-source all code and models in ESPnet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;William Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jiatong Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1&quot;&gt;Brian Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berrebbi_D/0/1/0/all/0/1&quot;&gt;Dan Berrebbi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wangyou Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yifan Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1&quot;&gt;Xuankai Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maiti_S/0/1/0/all/0/1&quot;&gt;Soumi Maiti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1&quot;&gt;Shinji Watanabe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15757">
<title>Latent Graph Powered Semi-Supervised Learning on Biomedical Tabular Data. (arXiv:2309.15757v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.15757</link>
<description rdf:parseType="Literal">&lt;p&gt;In the domain of semi-supervised learning, the current approaches
insufficiently exploit the potential of considering inter-instance
relationships among (un)labeled data. In this work, we address this limitation
by providing an approach for inferring latent graphs that capture the intrinsic
data relationships. By leveraging graph-based representations, our approach
facilitates the seamless propagation of information throughout the graph,
enabling the effective incorporation of global and local knowledge. Through
evaluations on biomedical tabular datasets, we compare the capabilities of our
approach to other contemporary methods. Our work demonstrates the significance
of inter-instance relationship discovery as practical means for constructing
robust latent graphs to enhance semi-supervised learning techniques. Our method
achieves state-of-the-art results on three biomedical datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koloski_B/0/1/0/all/0/1&quot;&gt;Boshko Koloski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skrlj_B/0/1/0/all/0/1&quot;&gt;Bla&amp;#x17e; &amp;#x160;krlj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pollak_S/0/1/0/all/0/1&quot;&gt;Senja Pollak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lavrac_N/0/1/0/all/0/1&quot;&gt;Nada Lavra&amp;#x10d;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12481">
<title>HANS, are you clever? Clever Hans Effect Analysis of Neural Systems. (arXiv:2309.12481v1 [cs.CL] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2309.12481</link>
<description rdf:parseType="Literal">&lt;p&gt;Instruction-tuned Large Language Models (It-LLMs) have been exhibiting
outstanding abilities to reason around cognitive states, intentions, and
reactions of all people involved, letting humans guide and comprehend
day-to-day social interactions effectively. In fact, several multiple-choice
questions (MCQ) benchmarks have been proposed to construct solid assessments of
the models&apos; abilities. However, earlier works are demonstrating the presence of
inherent &quot;order bias&quot; in It-LLMs, posing challenges to the appropriate
evaluation. In this paper, we investigate It-LLMs&apos; resilience abilities towards
a series of probing tests using four MCQ benchmarks. Introducing adversarial
examples, we show a significant performance gap, mainly when varying the order
of the choices, which reveals a selection bias and brings into discussion
reasoning abilities. Following a correlation between first positions and model
choices due to positional bias, we hypothesized the presence of structural
heuristics in the decision-making process of the It-LLMs, strengthened by
including significant examples in few-shot scenarios. Finally, by using the
Chain-of-Thought (CoT) technique, we elicit the model to reason and mitigate
the bias by obtaining more robust models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ranaldi_L/0/1/0/all/0/1&quot;&gt;Leonardo Ranaldi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zanzotto_F/0/1/0/all/0/1&quot;&gt;Fabio Massimo Zanzotto&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>