<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-12-26T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14935" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14937" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14939" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14952" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14965" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14968" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14976" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14985" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14988" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14990" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14991" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14998" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14999" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15004" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15010" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15011" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15037" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15043" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15059" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15064" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15084" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15086" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15101" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15102" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15103" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15116" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15130" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15133" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15136" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15139" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.13393" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.02659" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.14539" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.03087" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.04026" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.13929" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.10368" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.11798" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.13335" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.06961" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.00952" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.08977" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.12332" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.00670" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.14660" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.03535" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.04186" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.06141" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.07223" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.08685" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10400" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13773" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17214" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.01461" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.03831" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06093" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06157" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06208" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06406" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10531" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11920" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.13531" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15419" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01097" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01200" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05473" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07516" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07635" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07763" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03321" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03448" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05695" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.06595" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.07948" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.09891" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.10531" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12009" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12590" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12960" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.13150" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16182" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.04780" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07640" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14745" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16924" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01393" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01545" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08475" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.09739" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.11346" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.14958" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16979" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18652" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01212" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01813" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02240" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05197" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05276" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07125" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07475" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11261" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16552" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16918" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17280" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17366" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17842" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00343" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02751" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03325" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06199" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07331" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07353" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07823" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08019" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08372" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08874" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08889" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09245" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09598" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09821" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10032" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10104" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10407" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10628" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11562" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12223" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12340" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13332" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13555" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14091" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14135" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14492" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14919" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14924" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2312.14935">
<title>AS-XAI: Self-supervised Automatic Semantic Interpretation for CNN. (arXiv:2312.14935v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14935</link>
<description rdf:parseType="Literal">&lt;p&gt;Explainable artificial intelligence (XAI) aims to develop transparent
explanatory approaches for &quot;black-box&quot; deep learning models. However,it remains
difficult for existing methods to achieve the trade-off of the three key
criteria in interpretability, namely, reliability, causality, and usability,
which hinder their practical applications. In this paper, we propose a
self-supervised automatic semantic interpretable explainable artificial
intelligence (AS-XAI) framework, which utilizes transparent orthogonal
embedding semantic extraction spaces and row-centered principal component
analysis (PCA) for global semantic interpretation of model decisions in the
absence of human interference, without additional computational costs. In
addition, the invariance of filter feature high-rank decomposition is used to
evaluate model sensitivity to different semantic concepts. Extensive
experiments demonstrate that robust and orthogonal semantic spaces can be
automatically extracted by AS-XAI, providing more effective global
interpretability for convolutional neural networks (CNNs) and generating
human-comprehensible explanations. The proposed approach offers broad
fine-grained extensible practical applications, including shared semantic
interpretation under out-of-distribution (OOD) categories, auxiliary
explanations for species that are challenging to distinguish, and
classification explanations from various perspectives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1&quot;&gt;Changqi Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuntian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dongxiao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14937">
<title>SC-GS: Sparse-Controlled Gaussian Splatting for Editable Dynamic Scenes. (arXiv:2312.14937v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14937</link>
<description rdf:parseType="Literal">&lt;p&gt;Novel view synthesis for dynamic scenes is still a challenging problem in
computer vision and graphics. Recently, Gaussian splatting has emerged as a
robust technique to represent static scenes and enable high-quality and
real-time novel view synthesis. Building upon this technique, we propose a new
representation that explicitly decomposes the motion and appearance of dynamic
scenes into sparse control points and dense Gaussians, respectively. Our key
idea is to use sparse control points, significantly fewer in number than the
Gaussians, to learn compact 6 DoF transformation bases, which can be locally
interpolated through learned interpolation weights to yield the motion field of
3D Gaussians. We employ a deformation MLP to predict time-varying 6 DoF
transformations for each control point, which reduces learning complexities,
enhances learning abilities, and facilitates obtaining temporal and spatial
coherent motion patterns. Then, we jointly learn the 3D Gaussians, the
canonical space locations of control points, and the deformation MLP to
reconstruct the appearance, geometry, and dynamics of 3D scenes. During
learning, the location and number of control points are adaptively adjusted to
accommodate varying motion complexities in different regions, and an ARAP loss
following the principle of as rigid as possible is developed to enforce spatial
continuity and local rigidity of learned motions. Finally, thanks to the
explicit sparse motion representation and its decomposition from appearance,
our method can enable user-controlled motion editing while retaining
high-fidelity appearances. Extensive experiments demonstrate that our approach
outperforms existing approaches on novel view synthesis with a high rendering
speed and enables novel appearance-preserved motion editing applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yi-Hua Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yang-Tian Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Ziyi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1&quot;&gt;Xiaoyang Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yan-Pei Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1&quot;&gt;Xiaojuan Qi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14939">
<title>Large-scale Graph Representation Learning of Dynamic Brain Connectome with Transformers. (arXiv:2312.14939v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/2312.14939</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Transformers have recently been successful in various graph
representation learning tasks, providing a number of advantages over
message-passing Graph Neural Networks. Utilizing Graph Transformers for
learning the representation of the brain functional connectivity network is
also gaining interest. However, studies to date have underlooked the temporal
dynamics of functional connectivity, which fluctuates over time. Here, we
propose a method for learning the representation of dynamic functional
connectivity with Graph Transformers. Specifically, we define the connectome
embedding, which holds the position, structure, and time information of the
functional connectivity graph, and use Transformers to learn its representation
across time. We perform experiments with over 50,000 resting-state fMRI samples
obtained from three datasets, which is the largest number of fMRI data used in
studies by far. The experimental results show that our proposed method
outperforms other competitive baselines in gender classification and age
regression tasks based on the functional connectivity extracted from the fMRI
data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kim_B/0/1/0/all/0/1&quot;&gt;Byung-Hoon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jungwon Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Yun_E/0/1/0/all/0/1&quot;&gt;EungGu Yun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kyungsang Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Juho Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14952">
<title>A Cascaded Neural Network System For Rating Student Performance In Surgical Knot Tying Simulation. (arXiv:2312.14952v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14952</link>
<description rdf:parseType="Literal">&lt;p&gt;As part of their training all medical students and residents have to pass
basic surgical tasks such as knot tying, needle-passing, and suturing. Their
assessment is typically performed in the operating room by surgical faculty
where mistakes and failure by the student increases the operation time and
cost. This evaluation is quantitative and has a low margin of error. Simulation
has emerged as a cost effective option but it lacks assessment or requires
additional expensive hardware for evaluation. Apps that provide training videos
on surgical knot trying are available to students but none have evaluation. We
propose a cascaded neural network architecture that evaluates a student&apos;s
performance just from a video of themselves simulating a surgical knot tying
task. Our model converts video frame images into feature vectors with a
pre-trained deep convolutional network and then models the sequence of frames
with a temporal network. We obtained videos of medical students and residents
from the Robert Wood Johnson Hospital performing knot tying on a standardized
simulation kit. We manually annotated each video and proceeded to do a
five-fold cross-validation study on them. Our model achieves a median
precision, recall, and F1-score of 0.71, 0.66, and 0.65 respectively in
determining the level of knot related tasks of tying and pushing the knot. Our
mean precision score averaged across different probability thresholds is 0.8.
Both our F1-score and mean precision score are 8% and 30% higher than that of a
recently published study for the same problem. We expect the accuracy of our
model to further increase as we add more training videos to the model thus
making it a practical solution that students can use to evaluate themselves.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1&quot;&gt;Yunzhe Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eletta_O/0/1/0/all/0/1&quot;&gt;Olanrewaju Eletta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ady_J/0/1/0/all/0/1&quot;&gt;Justin W. Ady&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_N/0/1/0/all/0/1&quot;&gt;Nell M. Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bongu_A/0/1/0/all/0/1&quot;&gt;Advaith Bongu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roshan_U/0/1/0/all/0/1&quot;&gt;Usman Roshan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14965">
<title>Unraveling the Temporal Dynamics of the Unet in Diffusion Models. (arXiv:2312.14965v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14965</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have garnered significant attention since they can
effectively learn complex multivariate Gaussian distributions, resulting in
diverse, high-quality outcomes. They introduce Gaussian noise into training
data and reconstruct the original data iteratively. Central to this iterative
process is a single Unet, adapting across time steps to facilitate generation.
Recent work revealed the presence of composition and denoising phases in this
generation process, raising questions about the Unets&apos; varying roles. Our study
dives into the dynamic behavior of Unets within denoising diffusion
probabilistic models (DDPM), focusing on (de)convolutional blocks and skip
connections across time steps. We propose an analytical method to
systematically assess the impact of time steps and core Unet components on the
final output. This method eliminates components to study causal relations and
investigate their influence on output changes. The main purpose is to
understand the temporal dynamics and identify potential shortcuts during
inference. Our findings provide valuable insights into the various generation
phases during inference and shed light on the Unets&apos; usage patterns across
these phases. Leveraging these insights, we identify redundancies in GLIDE (an
improved DDPM) and improve inference time by ~27% with minimal degradation in
output quality. Our ultimate goal is to guide more informed optimization
strategies for inference and influence new model designs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prasad_V/0/1/0/all/0/1&quot;&gt;Vidya Prasad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Tian_C/0/1/0/all/0/1&quot;&gt;Chen Zhu-Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vilanova_A/0/1/0/all/0/1&quot;&gt;Anna Vilanova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1&quot;&gt;Hanspeter Pfister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pezzotti_N/0/1/0/all/0/1&quot;&gt;Nicola Pezzotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strobelt_H/0/1/0/all/0/1&quot;&gt;Hendrik Strobelt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14968">
<title>Enhancing Edge Intelligence with Highly Discriminant LNT Features. (arXiv:2312.14968v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.14968</link>
<description rdf:parseType="Literal">&lt;p&gt;AI algorithms at the edge demand smaller model sizes and lower computational
complexity. To achieve these objectives, we adopt a green learning (GL)
paradigm rather than the deep learning paradigm. GL has three modules: 1)
unsupervised representation learning, 2) supervised feature learning, and 3)
supervised decision learning. We focus on the second module in this work. In
particular, we derive new discriminant features from proper linear combinations
of input features, denoted by x, obtained in the first module. They are called
complementary and raw features, respectively. Along this line, we present a
novel supervised learning method to generate highly discriminant complementary
features based on the least-squares normal transform (LNT). LNT consists of two
steps. First, we convert a C-class classification problem to a binary
classification problem. The two classes are assigned with 0 and 1,
respectively. Next, we formulate a least-squares regression problem from the
N-dimensional (N-D) feature space to the 1-D output space, and solve the
least-squares normal equation to obtain one N-D normal vector, denoted by a1.
Since one normal vector is yielded by one binary split, we can obtain M normal
vectors with M splits. Then, Ax is called an LNT of x, where transform matrix A
in R^{M by N} by stacking aj^T, j=1, ..., M, and the LNT, Ax, can generate M
new features. The newly generated complementary features are shown to be more
discriminant than the raw features. Experiments show that the classification
performance can be improved by these new features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mishra_V/0/1/0/all/0/1&quot;&gt;Vinod K. Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kuo_C/0/1/0/all/0/1&quot;&gt;C.-C. Jay Kuo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14976">
<title>Gaussian Harmony: Attaining Fairness in Diffusion-based Face Generation Models. (arXiv:2312.14976v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14976</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have achieved great progress in face generation. However,
these models amplify the bias in the generation process, leading to an
imbalance in distribution of sensitive attributes such as age, gender and race.
This paper proposes a novel solution to this problem by balancing the facial
attributes of the generated images. We mitigate the bias by localizing the
means of the facial attributes in the latent space of the diffusion model using
Gaussian mixture models (GMM). Our motivation for choosing GMMs over other
clustering frameworks comes from the flexible latent structure of diffusion
model. Since each sampling step in diffusion models follows a Gaussian
distribution, we show that fitting a GMM model helps us to localize the
subspace responsible for generating a specific attribute. Furthermore, our
method does not require retraining, we instead localize the subspace on-the-fly
and mitigate the bias for generating a fair dataset. We evaluate our approach
on multiple face attribute datasets to demonstrate the effectiveness of our
approach. Our results demonstrate that our approach leads to a more fair data
generation in terms of representational fairness while preserving the quality
of generated samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pal_B/0/1/0/all/0/1&quot;&gt;Basudha Pal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kannan_A/0/1/0/all/0/1&quot;&gt;Arunkumar Kannan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kathirvel_R/0/1/0/all/0/1&quot;&gt;Ram Prabhakar Kathirvel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OToole_A/0/1/0/all/0/1&quot;&gt;Alice J. O&amp;#x27;Toole&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1&quot;&gt;Rama Chellappa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14985">
<title>UniHuman: A Unified Model for Editing Human Images in the Wild. (arXiv:2312.14985v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14985</link>
<description rdf:parseType="Literal">&lt;p&gt;Human image editing includes tasks like changing a person&apos;s pose, their
clothing, or editing the image according to a text prompt. However, prior work
often tackles these tasks separately, overlooking the benefit of mutual
reinforcement from learning them jointly. In this paper, we propose UniHuman, a
unified model that addresses multiple facets of human image editing in
real-world settings. To enhance the model&apos;s generation quality and
generalization capacity, we leverage guidance from human visual encoders and
introduce a lightweight pose-warping module that can exploit different pose
representations, accommodating unseen textures and patterns. Furthermore, to
bridge the disparity between existing human editing benchmarks with real-world
data, we curated 400K high-quality human image-text pairs for training and
collected 2K human images for out-of-domain testing, both encompassing diverse
clothing styles, backgrounds, and age groups. Experiments on both in-domain and
out-of-domain test sets demonstrate that UniHuman outperforms task-specific
models by a significant margin. In user studies, UniHuman is preferred by the
users in an average of 77% of cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1&quot;&gt;Nannan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1&quot;&gt;Krishna Kumar Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yilin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plummer_B/0/1/0/all/0/1&quot;&gt;Bryan A. Plummer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhe Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14988">
<title>Emage: Non-Autoregressive Text-to-Image Generation. (arXiv:2312.14988v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14988</link>
<description rdf:parseType="Literal">&lt;p&gt;Autoregressive and diffusion models drive the recent breakthroughs on
text-to-image generation. Despite their huge success of generating
high-realistic images, a common shortcoming of these models is their high
inference latency - autoregressive models run more than a thousand times
successively to produce image tokens and diffusion models convert Gaussian
noise into images with many hundreds of denoising steps. In this work, we
explore non-autoregressive text-to-image models that efficiently generate
hundreds of image tokens in parallel. We develop many model variations with
different learning and inference strategies, initialized text encoders, etc.
Compared with autoregressive baselines that needs to run one thousand times,
our model only runs 16 times to generate images of competitive quality with an
order of magnitude lower inference latency. Our non-autoregressive model with
346M parameters generates an image of 256$\times$256 with about one second on
one V100 GPU.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1&quot;&gt;Zhangyin Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1&quot;&gt;Runyi Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Liangxin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1&quot;&gt;Duyu Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1&quot;&gt;Yong Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1&quot;&gt;Xiaocheng Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1&quot;&gt;Bing Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1&quot;&gt;Shuming Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14990">
<title>Learning to Prompt Knowledge Transfer for Open-World Continual Learning. (arXiv:2312.14990v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.14990</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies the problem of continual learning in an open-world
scenario, referred to as Open-world Continual Learning (OwCL). OwCL is
increasingly rising while it is highly challenging in two-fold: i) learning a
sequence of tasks without forgetting knowns in the past, and ii) identifying
unknowns (novel objects/classes) in the future. Existing OwCL methods suffer
from the adaptability of task-aware boundaries between knowns and unknowns, and
do not consider the mechanism of knowledge transfer. In this work, we propose
Pro-KT, a novel prompt-enhanced knowledge transfer model for OwCL. Pro-KT
includes two key components: (1) a prompt bank to encode and transfer both
task-generic and task-specific knowledge, and (2) a task-aware open-set
boundary to identify unknowns in the new tasks. Experimental results using two
real-world datasets demonstrate that the proposed Pro-KT outperforms the
state-of-the-art counterparts in both the detection of unknowns and the
classification of knowns markedly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yujie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiangkun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tianrui Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14991">
<title>FoodLMM: A Versatile Food Assistant using Large Multi-modal Model. (arXiv:2312.14991v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14991</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Multi-modal Models (LMMs) have made impressive progress in many
vision-language tasks. Nevertheless, the performance of general LMMs in
specific domains is still far from satisfactory. This paper proposes FoodLMM, a
versatile food assistant based on LMMs with various capabilities, including
food recognition, ingredient recognition, recipe generation, nutrition
estimation, food segmentation and multi-round conversation. To facilitate
FoodLMM to deal with tasks beyond pure text output, we introduce a series of
novel task-specific tokens and heads, enabling the model to predict food
nutritional values and multiple segmentation masks. We adopt a two-stage
training strategy. In the first stage, we utilize multiple public food
benchmarks for multi-task learning by leveraging instruct-following paradigm.
In the second stage, we construct a multi-round conversation and a reasoning
segmentation datasets to fine-tune the model, enabling it to conduct
professional dialogues and generate segmentation masks based on complex
reasoning in food domain. Our fine-tuned FoodLMM achieves state-of-the-art
results across several food benchmarks. We will make our code, models and
datasets publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1&quot;&gt;Yuehao Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1&quot;&gt;Huiyan Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1&quot;&gt;Bin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jingjing Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yu-Gang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ngo_C/0/1/0/all/0/1&quot;&gt;Chong-Wah Ngo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14998">
<title>Synthetic images aid the recognition of human-made art forgeries. (arXiv:2312.14998v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14998</link>
<description rdf:parseType="Literal">&lt;p&gt;Previous research has shown that Artificial Intelligence is capable of
distinguishing between authentic paintings by a given artist and human-made
forgeries with remarkable accuracy, provided sufficient training. However, with
the limited amount of existing known forgeries, augmentation methods for
forgery detection are highly desirable. In this work, we examine the potential
of incorporating synthetic artworks into training datasets to enhance the
performance of forgery detection. Our investigation focuses on paintings by
Vincent van Gogh, for which we release the first dataset specialized for
forgery detection. To reinforce our results, we conduct the same analyses on
the artists Amedeo Modigliani and Raphael. We train a classifier to distinguish
original artworks from forgeries. For this, we use human-made forgeries and
imitations in the style of well-known artists and augment our training sets
with images in a similar style generated by Stable Diffusion and StyleGAN. We
find that the additional synthetic forgeries consistently improve the detection
of human-made forgeries. In addition, we find that, in line with previous
research, the inclusion of synthetic forgeries in the training also enables the
detection of AI-generated forgeries, especially if created using a similar
generator.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ostmeyer_J/0/1/0/all/0/1&quot;&gt;Johann Ostmeyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaerf_L/0/1/0/all/0/1&quot;&gt;Ludovica Schaerf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buividovich_P/0/1/0/all/0/1&quot;&gt;Pavel Buividovich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Charles_T/0/1/0/all/0/1&quot;&gt;Tessa Charles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Postma_E/0/1/0/all/0/1&quot;&gt;Eric Postma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Popovici_C/0/1/0/all/0/1&quot;&gt;Carina Popovici&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14999">
<title>Leveraging Habitat Information for Fine-grained Bird Identification. (arXiv:2312.14999v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.14999</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional bird classifiers mostly rely on the visual characteristics of
birds. Some prior works even train classifiers to be invariant to the
background, completely discarding the living environment of birds. Instead, we
are the first to explore integrating habitat information, one of the four major
cues for identifying birds by ornithologists, into modern bird classifiers. We
focus on two leading model types: (1) CNNs and ViTs trained on the downstream
bird datasets; and (2) original, multi-modal CLIP. Training CNNs and ViTs with
habitat-augmented data results in an improvement of up to +0.83 and +0.23
points on NABirds and CUB-200, respectively. Similarly, adding habitat
descriptors to the prompts for CLIP yields a substantial accuracy boost of up
to +0.99 and +1.1 points on NABirds and CUB-200, respectively. We find
consistent accuracy improvement after integrating habitat features into the
image augmentation process and into the textual descriptors of vision-language
CLIP classifiers. Code is available at:
https://anonymous.4open.science/r/reasoning-8B7E/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tin Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1&quot;&gt;Anh Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15004">
<title>FineMoGen: Fine-Grained Spatio-Temporal Motion Generation and Editing. (arXiv:2312.15004v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.15004</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-driven motion generation has achieved substantial progress with the
emergence of diffusion models. However, existing methods still struggle to
generate complex motion sequences that correspond to fine-grained descriptions,
depicting detailed and accurate spatio-temporal actions. This lack of fine
controllability limits the usage of motion generation to a larger audience. To
tackle these challenges, we present FineMoGen, a diffusion-based motion
generation and editing framework that can synthesize fine-grained motions, with
spatial-temporal composition to the user instructions. Specifically, FineMoGen
builds upon diffusion model with a novel transformer architecture dubbed
Spatio-Temporal Mixture Attention (SAMI). SAMI optimizes the generation of the
global attention template from two perspectives: 1) explicitly modeling the
constraints of spatio-temporal composition; and 2) utilizing sparsely-activated
mixture-of-experts to adaptively extract fine-grained features. To facilitate a
large-scale study on this new fine-grained motion generation task, we
contribute the HuMMan-MoGen dataset, which consists of 2,968 videos and 102,336
fine-grained spatio-temporal descriptions. Extensive experiments validate that
FineMoGen exhibits superior motion generation quality over state-of-the-art
methods. Notably, FineMoGen further enables zero-shot motion editing
capabilities with the aid of modern large language models (LLM), which
faithfully manipulates motion sequences with fine-grained instructions. Project
Page: https://mingyuan-zhang.github.io/projects/FineMoGen.html
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mingyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huirong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1&quot;&gt;Zhongang Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1&quot;&gt;Jiawei Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziwei Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15010">
<title>SI-MIL: Taming Deep MIL for Self-Interpretability in Gigapixel Histopathology. (arXiv:2312.15010v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.15010</link>
<description rdf:parseType="Literal">&lt;p&gt;Introducing interpretability and reasoning into Multiple Instance Learning
(MIL) methods for Whole Slide Image (WSI) analysis is challenging, given the
complexity of gigapixel slides. Traditionally, MIL interpretability is limited
to identifying salient regions deemed pertinent for downstream tasks, offering
little insight to the end-user (pathologist) regarding the rationale behind
these selections. To address this, we propose Self-Interpretable MIL (SI-MIL),
a method intrinsically designed for interpretability from the very outset.
SI-MIL employs a deep MIL framework to guide an interpretable branch grounded
on handcrafted pathological features, facilitating linear predictions. Beyond
identifying salient regions, SI-MIL uniquely provides feature-level
interpretations rooted in pathological insights for WSIs. Notably, SI-MIL, with
its linear prediction constraints, challenges the prevalent myth of an
inevitable trade-off between model interpretability and performance,
demonstrating competitive results compared to state-of-the-art methods on
WSI-level prediction tasks across three cancer types. In addition, we
thoroughly benchmark the local- and global-interpretability of SI-MIL in terms
of statistical analysis, a domain expert study, and desiderata of
interpretability, namely, user-friendliness and faithfulness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kapse_S/0/1/0/all/0/1&quot;&gt;Saarthak Kapse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pati_P/0/1/0/all/0/1&quot;&gt;Pushpak Pati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1&quot;&gt;Srijan Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jingwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vakalopoulou_M/0/1/0/all/0/1&quot;&gt;Maria Vakalopoulou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saltz_J/0/1/0/all/0/1&quot;&gt;Joel Saltz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samaras_D/0/1/0/all/0/1&quot;&gt;Dimitris Samaras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1&quot;&gt;Rajarsi R. Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prasanna_P/0/1/0/all/0/1&quot;&gt;Prateek Prasanna&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15011">
<title>Gemini vs GPT-4V: A Preliminary Comparison and Combination of Vision-Language Models Through Qualitative Cases. (arXiv:2312.15011v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.15011</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapidly evolving sector of Multi-modal Large Language Models (MLLMs) is
at the forefront of integrating linguistic and visual processing in artificial
intelligence. This paper presents an in-depth comparative study of two
pioneering models: Google&apos;s Gemini and OpenAI&apos;s GPT-4V(ision). Our study
involves a multi-faceted evaluation of both models across key dimensions such
as Vision-Language Capability, Interaction with Humans, Temporal Understanding,
and assessments in both Intelligence and Emotional Quotients. The core of our
analysis delves into the distinct visual comprehension abilities of each model.
We conducted a series of structured experiments to evaluate their performance
in various industrial application scenarios, offering a comprehensive
perspective on their practical utility. We not only involve direct performance
comparisons but also include adjustments in prompts and scenarios to ensure a
balanced and fair analysis. Our findings illuminate the unique strengths and
niches of both models. GPT-4V distinguishes itself with its precision and
succinctness in responses, while Gemini excels in providing detailed, expansive
answers accompanied by relevant imagery and links. These understandings not
only shed light on the comparative merits of Gemini and GPT-4V but also
underscore the evolving landscape of multimodal foundation models, paving the
way for future advancements in this area. After the comparison, we attempted to
achieve better results by combining the two models. Finally, We would like to
express our profound gratitude to the teams behind GPT-4V and Gemini for their
pioneering contributions to the field. Our acknowledgments are also extended to
the comprehensive qualitative analysis presented in &apos;Dawn&apos; by Yang et al. This
work, with its extensive collection of image samples, prompts, and
GPT-4V-related results, provided a foundational basis for our analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1&quot;&gt;Zhangyang Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Ye Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mengchen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zeyi Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dahua Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiaqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hengshuang Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15037">
<title>Latents2Semantics: Leveraging the Latent Space of Generative Models for Localized Style Manipulation of Face Images. (arXiv:2312.15037v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.15037</link>
<description rdf:parseType="Literal">&lt;p&gt;With the metaverse slowly becoming a reality and given the rapid pace of
developments toward the creation of digital humans, the need for a principled
style editing pipeline for human faces is bound to increase manifold. We cater
to this need by introducing the Latents2Semantics Autoencoder (L2SAE), a
Generative Autoencoder model that facilitates highly localized editing of style
attributes of several Regions of Interest (ROIs) in face images. The L2SAE
learns separate latent representations for encoded images&apos; structure and style
information. Thus, allowing for structure-preserving style editing of the
chosen ROIs. The encoded structure representation is a multichannel 2D tensor
with reduced spatial dimensions, which captures both local and global structure
properties. The style representation is a 1D tensor that captures global style
attributes. In our framework, we slice the structure representation to build
strong and disentangled correspondences with different ROIs. Consequentially,
style editing of the chosen ROIs amounts to a simple combination of (a) the
ROI-mask generated from the sliced structure representation and (b) the decoded
image with global style changes, generated from the manipulated (using Gaussian
noise) global style and unchanged structure tensor. Style editing sans
additional human supervision is a significant win over SOTA style editing
pipelines because most existing works require additional human effort
(supervision) post-training for attributing semantic meaning to style edits. We
also do away with iterative-optimization-based inversion or determining
controllable latent directions post-training, which requires additional
computationally expensive operations. We provide qualitative and quantitative
results for the same over multiple applications, such as selective style
editing and swapping using test images sampled from several datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomar_S/0/1/0/all/0/1&quot;&gt;Snehal Singh Tomar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajagopalan_A/0/1/0/all/0/1&quot;&gt;A.N. Rajagopalan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15043">
<title>GroundVLP: Harnessing Zero-shot Visual Grounding from Vision-Language Pre-training and Open-Vocabulary Object Detection. (arXiv:2312.15043v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.15043</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual grounding, a crucial vision-language task involving the understanding
of the visual context based on the query expression, necessitates the model to
capture the interactions between objects, as well as various spatial and
attribute information. However, the annotation data of visual grounding task is
limited due to its time-consuming and labor-intensive annotation process,
resulting in the trained models being constrained from generalizing its
capability to a broader domain. To address this challenge, we propose
GroundVLP, a simple yet effective zero-shot method that harnesses visual
grounding ability from the existing models trained from image-text pairs and
pure object detection data, both of which are more conveniently obtainable and
offer a broader domain compared to visual grounding annotation data. GroundVLP
proposes a fusion mechanism that combines the heatmap from GradCAM and the
object proposals of open-vocabulary detectors. We demonstrate that the proposed
method significantly outperforms other zero-shot methods on RefCOCO/+/g
datasets, surpassing prior zero-shot state-of-the-art by approximately 28\% on
the test split of RefCOCO and RefCOCO+. Furthermore, GroundVLP performs
comparably to or even better than some non-VLP-based supervised models on the
Flickr30k entities dataset. Our code is available at
https://github.com/om-ai-lab/GroundVLP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Haozhan Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tiancheng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1&quot;&gt;Mingwei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1&quot;&gt;Jianwei Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15059">
<title>Deformable 3D Gaussian Splatting for Animatable Human Avatars. (arXiv:2312.15059v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.15059</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in neural radiance fields enable novel view synthesis of
photo-realistic images in dynamic settings, which can be applied to scenarios
with human animation. Commonly used implicit backbones to establish accurate
models, however, require many input views and additional annotations such as
human masks, UV maps and depth maps. In this work, we propose ParDy-Human
(Parameterized Dynamic Human Avatar), a fully explicit approach to construct a
digital avatar from as little as a single monocular sequence. ParDy-Human
introduces parameter-driven dynamics into 3D Gaussian Splatting where 3D
Gaussians are deformed by a human pose model to animate the avatar. Our method
is composed of two parts: A first module that deforms canonical 3D Gaussians
according to SMPL vertices and a consecutive module that further takes their
designed joint encodings and predicts per Gaussian deformations to deal with
dynamics beyond SMPL vertex deformations. Images are then synthesized by a
rasterizer. ParDy-Human constitutes an explicit model for realistic dynamic
human avatars which requires significantly fewer training views and images. Our
avatars learning is free of additional annotations such as masks and can be
trained with variable backgrounds while inferring full-resolution images
efficiently even on consumer hardware. We provide experimental evidence to show
that ParDy-Human outperforms state-of-the-art methods on ZJU-MoCap and
THUman4.0 datasets both quantitatively and visually.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1&quot;&gt;HyunJun Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brasch_N/0/1/0/all/0/1&quot;&gt;Nikolas Brasch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jifei Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_Pellitero_E/0/1/0/all/0/1&quot;&gt;Eduardo Perez-Pellitero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yiren Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhihao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1&quot;&gt;Nassir Navab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1&quot;&gt;Benjamin Busam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15064">
<title>Joint Self-Supervised and Supervised Contrastive Learning for Multimodal MRI Data: Towards Predicting Abnormal Neurodevelopment. (arXiv:2312.15064v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.15064</link>
<description rdf:parseType="Literal">&lt;p&gt;The integration of different imaging modalities, such as structural,
diffusion tensor, and functional magnetic resonance imaging, with deep learning
models has yielded promising outcomes in discerning phenotypic characteristics
and enhancing disease diagnosis. The development of such a technique hinges on
the efficient fusion of heterogeneous multimodal features, which initially
reside within distinct representation spaces. Naively fusing the multimodal
features does not adequately capture the complementary information and could
even produce redundancy. In this work, we present a novel joint self-supervised
and supervised contrastive learning method to learn the robust latent feature
representation from multimodal MRI data, allowing the projection of
heterogeneous features into a shared common space, and thereby amalgamating
both complementary and analogous information across various modalities and
among similar subjects. We performed a comparative analysis between our
proposed method and alternative deep multimodal learning approaches. Through
extensive experiments on two independent datasets, the results demonstrated
that our method is significantly superior to several other deep multimodal
learning methods in predicting abnormal neurodevelopment. Our method has the
capability to facilitate computer-aided diagnosis within clinical practice,
harnessing the power of multimodal data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hailong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ralescu_A/0/1/0/all/0/1&quot;&gt;Anca L. Ralescu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dillman_J/0/1/0/all/0/1&quot;&gt;Jonathan R. Dillman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Altaye_M/0/1/0/all/0/1&quot;&gt;Mekibib Altaye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cecil_K/0/1/0/all/0/1&quot;&gt;Kim M. Cecil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Parikh_N/0/1/0/all/0/1&quot;&gt;Nehal A. Parikh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+He_L/0/1/0/all/0/1&quot;&gt;Lili He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15084">
<title>Automated forest inventory: analysis of high-density airborne LiDAR point clouds with 3D deep learning. (arXiv:2312.15084v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.15084</link>
<description rdf:parseType="Literal">&lt;p&gt;Detailed forest inventories are critical for sustainable and flexible
management of forest resources, to conserve various ecosystem services. Modern
airborne laser scanners deliver high-density point clouds with great potential
for fine-scale forest inventory and analysis, but automatically partitioning
those point clouds into meaningful entities like individual trees or tree
components remains a challenge. The present study aims to fill this gap and
introduces a deep learning framework that is able to perform such a
segmentation across diverse forest types and geographic regions. From the
segmented data, we then derive relevant biophysical parameters of individual
trees as well as stands. The system has been tested on FOR-Instance, a dataset
of point clouds that have been acquired in five different countries using
surveying drones. The segmentation back-end achieves over 85% F-score for
individual trees, respectively over 73% mean IoU across five semantic
categories: ground, low vegetation, stems, live branches and dead branches.
Building on the segmentation results our pipeline then densely calculates
biophysical features of each individual tree (height, crown diameter, crown
volume, DBH, and location) and properties per stand (digital terrain model and
stand density). Especially crown-related features are in most cases retrieved
with high accuracy, whereas the estimates for DBH and location are less
reliable, due to the airborne scanning setup.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1&quot;&gt;Binbin Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wielgosz_M/0/1/0/all/0/1&quot;&gt;Maciej Wielgosz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kontogianni_T/0/1/0/all/0/1&quot;&gt;Theodora Kontogianni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peters_T/0/1/0/all/0/1&quot;&gt;Torben Peters&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puliti_S/0/1/0/all/0/1&quot;&gt;Stefano Puliti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Astrup_R/0/1/0/all/0/1&quot;&gt;Rasmus Astrup&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1&quot;&gt;Konrad Schindler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15086">
<title>HyperMix: Out-of-Distribution Detection and Classification in Few-Shot Settings. (arXiv:2312.15086v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.15086</link>
<description rdf:parseType="Literal">&lt;p&gt;Out-of-distribution (OOD) detection is an important topic for real-world
machine learning systems, but settings with limited in-distribution samples
have been underexplored. Such few-shot OOD settings are challenging, as models
have scarce opportunities to learn the data distribution before being tasked
with identifying OOD samples. Indeed, we demonstrate that recent
state-of-the-art OOD methods fail to outperform simple baselines in the
few-shot setting. We thus propose a hypernetwork framework called HyperMix,
using Mixup on the generated classifier parameters, as well as a natural
out-of-episode outlier exposure technique that does not require an additional
outlier dataset. We conduct experiments on CIFAR-FS and MiniImageNet,
significantly outperforming other OOD methods in the few-shot regime.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehta_N/0/1/0/all/0/1&quot;&gt;Nikhil Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1&quot;&gt;Kevin J Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jing Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_F/0/1/0/all/0/1&quot;&gt;Fu-Jen Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1&quot;&gt;Li Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassner_T/0/1/0/all/0/1&quot;&gt;Tal Hassner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15101">
<title>Fix-Con: Automatic Fault Localization and Repair of Deep Learning Model Conversions. (arXiv:2312.15101v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2312.15101</link>
<description rdf:parseType="Literal">&lt;p&gt;Converting deep learning models between frameworks is a common step to
maximize model compatibility across devices and leverage optimization features
that may be exclusively provided in one deep learning framework. However, this
conversion process may be riddled with bugs, making the converted models either
undeployable or problematic, considerably degrading their prediction
correctness.
&lt;/p&gt;
&lt;p&gt;We propose an automated approach for fault localization and repair, Fix-Con,
during model conversion between deep learning frameworks. Fix-Con is capable of
detecting and fixing faults introduced in model input, parameters,
hyperparameters, and the model graph during conversion.
&lt;/p&gt;
&lt;p&gt;Fix-Con uses a set of fault types mined from surveying conversion issues
raised to localize potential conversion faults in the converted target model,
and then repairs them appropriately, e.g. replacing the parameters of the
target model with those from the source model. This is done iteratively for
every image in the dataset with output label differences between the source
model and the converted target model until all differences are resolved. We
evaluate the effectiveness of Fix-Con in fixing model conversion bugs of three
widely used image recognition models converted across four different deep
learning frameworks. Overall, Fix-Con was able to either completely repair, or
significantly improve the performance of 14 out of the 15 erroneous conversion
cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Louloudakis_N/0/1/0/all/0/1&quot;&gt;Nikolaos Louloudakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gibson_P/0/1/0/all/0/1&quot;&gt;Perry Gibson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cano_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Cano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajan_A/0/1/0/all/0/1&quot;&gt;Ajitha Rajan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15102">
<title>Robust Sclera Segmentation for Skin-tone Agnostic Face Image Quality Assessment. (arXiv:2312.15102v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.15102</link>
<description rdf:parseType="Literal">&lt;p&gt;Face image quality assessment (FIQA) is crucial for obtaining good face
recognition performance. FIQA algorithms should be robust and insensitive to
demographic factors. The eye sclera has a consistent whitish color in all
humans regardless of their age, ethnicity and skin-tone. This work proposes a
robust sclera segmentation method that is suitable for face images in the
enrolment and the border control face recognition scenarios. It shows how the
statistical analysis of the sclera pixels produces features that are invariant
to skin-tone, age and ethnicity and thus can be incorporated into FIQA
algorithms to make them agnostic to demographic factors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kabbani_W/0/1/0/all/0/1&quot;&gt;Wassim Kabbani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1&quot;&gt;Christoph Busch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raja_K/0/1/0/all/0/1&quot;&gt;Kiran Raja&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15103">
<title>Energy-based learning algorithms for analog computing: a comparative study. (arXiv:2312.15103v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.15103</link>
<description rdf:parseType="Literal">&lt;p&gt;Energy-based learning algorithms have recently gained a surge of interest due
to their compatibility with analog (post-digital) hardware. Existing algorithms
include contrastive learning (CL), equilibrium propagation (EP) and coupled
learning (CpL), all consisting in contrasting two states, and differing in the
type of perturbation used to obtain the second state from the first one.
However, these algorithms have never been explicitly compared on equal footing
with same models and datasets, making it difficult to assess their scalability
and decide which one to select in practice. In this work, we carry out a
comparison of seven learning algorithms, namely CL and different variants of EP
and CpL depending on the signs of the perturbations. Specifically, using these
learning algorithms, we train deep convolutional Hopfield networks (DCHNs) on
five vision tasks (MNIST, F-MNIST, SVHN, CIFAR-10 and CIFAR-100). We find that,
while all algorithms yield comparable performance on MNIST, important
differences in performance arise as the difficulty of the task increases. Our
key findings reveal that negative perturbations are better than positive ones,
and highlight the centered variant of EP (which uses two perturbations of
opposite sign) as the best-performing algorithm. We also endorse these findings
with theoretical arguments. Additionally, we establish new SOTA results with
DCHNs on all five datasets, both in performance and speed. In particular, our
DCHN simulations are 13.5 times faster with respect to Laborieux et al. (2021),
which we achieve thanks to the use of a novel energy minimisation algorithm
based on asynchronous updates, combined with reduced precision (16 bits).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scellier_B/0/1/0/all/0/1&quot;&gt;Benjamin Scellier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ernoult_M/0/1/0/all/0/1&quot;&gt;Maxence Ernoult&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kendall_J/0/1/0/all/0/1&quot;&gt;Jack Kendall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Suhas Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15116">
<title>EGAIN: Extended GAn INversion. (arXiv:2312.15116v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.15116</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Networks (GANs) have witnessed significant advances in
recent years, generating increasingly higher quality images, which are
non-distinguishable from real ones. Recent GANs have proven to encode features
in a disentangled latent space, enabling precise control over various semantic
attributes of the generated facial images such as pose, illumination, or
gender. GAN inversion, which is projecting images into the latent space of a
GAN, opens the door for the manipulation of facial semantics of real face
images. This is useful for numerous applications such as evaluating the
performance of face recognition systems. In this work, EGAIN, an architecture
for constructing GAN inversion models, is presented. This architecture
explicitly addresses some of the shortcomings in previous GAN inversion models.
A specific model with the same name, egain, based on this architecture is also
proposed, demonstrating superior reconstruction quality over state-of-the-art
models, and illustrating the validity of the EGAIN architecture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kabbani_W/0/1/0/all/0/1&quot;&gt;Wassim Kabbani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grimmer_M/0/1/0/all/0/1&quot;&gt;Marcel Grimmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1&quot;&gt;Christoph Busch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15130">
<title>PACE: Pose Annotations in Cluttered Environments. (arXiv:2312.15130v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.15130</link>
<description rdf:parseType="Literal">&lt;p&gt;Pose estimation is a crucial task in computer vision, enabling tracking and
manipulating objects in images or videos. While several datasets exist for pose
estimation, there is a lack of large-scale datasets specifically focusing on
cluttered scenes with occlusions. This limitation is a bottleneck in the
development and evaluation of pose estimation methods, particularly toward the
goal of real-world application in environments where occlusions are common.
Addressing this, we introduce PACE (Pose Annotations in Cluttered
Environments), a large-scale benchmark designed to advance the development and
evaluation of pose estimation methods in cluttered scenarios. PACE encompasses
54,945 frames with 257,673 annotations across 300 videos, covering 576 objects
from 44 categories and featuring a mix of rigid and articulated items in
cluttered scenes. To annotate the real-world data efficiently, we developed an
innovative annotation system utilizing a calibrated 3-camera setup. We test
state-of-the-art algorithms in PACE along two tracks: pose estimation, and
object pose tracking, revealing the benchmark&apos;s challenges and research
opportunities. We plan to release PACE as a public evaluation benchmark, along
the annotations tools we developed, to stimulate further advancements in the
field. Our code and data is available on https://github.com/qq456cvb/PACE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1&quot;&gt;Yang You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_K/0/1/0/all/0/1&quot;&gt;Kai Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhening Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhengxiang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Junwei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_R/0/1/0/all/0/1&quot;&gt;Ruoxi Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1&quot;&gt;Zhou Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harley_A/0/1/0/all/0/1&quot;&gt;Adam W. Harley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Cewu Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15133">
<title>Learning Continuous Implicit Field with Local Distance Indicator for Arbitrary-Scale Point Cloud Upsampling. (arXiv:2312.15133v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.15133</link>
<description rdf:parseType="Literal">&lt;p&gt;Point cloud upsampling aims to generate dense and uniformly distributed point
sets from a sparse point cloud, which plays a critical role in 3D computer
vision. Previous methods typically split a sparse point cloud into several
local patches, upsample patch points, and merge all upsampled patches. However,
these methods often produce holes, outliers or nonuniformity due to the
splitting and merging process which does not maintain consistency among local
patches. To address these issues, we propose a novel approach that learns an
unsigned distance field guided by local priors for point cloud upsampling.
Specifically, we train a local distance indicator (LDI) that predicts the
unsigned distance from a query point to a local implicit surface. Utilizing the
learned LDI, we learn an unsigned distance field to represent the sparse point
cloud with patch consistency. At inference time, we randomly sample queries
around the sparse point cloud, and project these query points onto the
zero-level set of the learned implicit field to generate a dense point cloud.
We justify that the implicit field is naturally continuous, which inherently
enables the application of arbitrary-scale upsampling without necessarily
retraining for various scales. We conduct comprehensive experiments on both
synthetic data and real scans, and report state-of-the-art results under widely
used benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shujuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Junsheng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_B/0/1/0/all/0/1&quot;&gt;Baorui Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yu-Shen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1&quot;&gt;Zhizhong Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15136">
<title>Towards End-to-End Structure Solutions from Information-Compromised Diffraction Data via Generative Deep Learning. (arXiv:2312.15136v1 [physics.comp-ph])</title>
<link>http://arxiv.org/abs/2312.15136</link>
<description rdf:parseType="Literal">&lt;p&gt;The revolution in materials in the past century was built on a knowledge of
the atomic arrangements and the structure-property relationship. The sine qua
non for obtaining quantitative structural information is single crystal
crystallography. However, increasingly we need to solve structures in cases
where the information content in our input signal is significantly degraded,
for example, due to orientational averaging of grains, finite size effects due
to nanostructure, and mixed signals due to sample heterogeneity. Understanding
the structure property relationships in such situations is, if anything, more
important and insightful, yet we do not have robust approaches for
accomplishing it. In principle, machine learning (ML) and deep learning (DL)
are promising approaches since they augment information in the degraded input
signal with prior knowledge learned from large databases of already known
structures. Here we present a novel ML approach, a variational query-based
multi-branch deep neural network that has the promise to be a robust but
general tool to address this problem end-to-end. We demonstrate the approach on
computed powder x-ray diffraction (PXRD), along with partial chemical
composition information, as input. We choose as a structural representation a
modified electron density we call the Cartesian mapped electron density (CMED),
that straightforwardly allows our ML model to learn material structures across
different chemistries, symmetries and crystal systems. When evaluated on
theoretically simulated data for the cubic and trigonal crystal systems, the
system achieves up to $93.4\%$ average similarity with the ground truth on
unseen materials, both with known and partially-known chemical composition
information, showing great promise for successful structure solution even from
degraded and incomplete input data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Guo_G/0/1/0/all/0/1&quot;&gt;Gabe Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Goldfeder_J/0/1/0/all/0/1&quot;&gt;Judah Goldfeder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Lan_L/0/1/0/all/0/1&quot;&gt;Ling Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ray_A/0/1/0/all/0/1&quot;&gt;Aniv Ray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Yang_A/0/1/0/all/0/1&quot;&gt;Albert Hanming Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Boyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Billinge_S/0/1/0/all/0/1&quot;&gt;Simon JL Billinge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Lipson_H/0/1/0/all/0/1&quot;&gt;Hod Lipson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15139">
<title>Automatic Tooth Arrangement with Joint Features of Point and Mesh Representations via Diffusion Probabilistic Models. (arXiv:2312.15139v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.15139</link>
<description rdf:parseType="Literal">&lt;p&gt;Tooth arrangement is a crucial step in orthodontics treatment, in which
aligning teeth could improve overall well-being, enhance facial aesthetics, and
boost self-confidence. To improve the efficiency of tooth arrangement and
minimize errors associated with unreasonable designs by inexperienced
practitioners, some deep learning-based tooth arrangement methods have been
proposed. Currently, most existing approaches employ MLPs to model the
nonlinear relationship between tooth features and transformation matrices to
achieve tooth arrangement automatically. However, the limited datasets (which
to our knowledge, have not been made public) collected from clinical practice
constrain the applicability of existing methods, making them inadequate for
addressing diverse malocclusion issues. To address this challenge, we propose a
general tooth arrangement neural network based on the diffusion probabilistic
model. Conditioned on the features extracted from the dental model, the
diffusion probabilistic model can learn the distribution of teeth
transformation matrices from malocclusion to normal occlusion by gradually
denoising from a random variable, thus more adeptly managing real orthodontic
data. To take full advantage of effective features, we exploit both mesh and
point cloud representations by designing different encoding networks to extract
the tooth (local) and jaw (global) features, respectively. In addition to
traditional metrics ADD, PA-ADD, CSA, and ME_{rot}, we propose a new evaluation
metric based on dental arch curves to judge whether the generated teeth meet
the individual normal occlusion. Experimental results demonstrate that our
proposed method achieves state-of-the-art tooth alignment results and
satisfactory occlusal relationships between dental arches. We will publish the
code and dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_C/0/1/0/all/0/1&quot;&gt;Changsong Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1&quot;&gt;Mengfei Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shaofeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yaqian Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_R/0/1/0/all/0/1&quot;&gt;Ran Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yuhui Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yongjin Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.13393">
<title>TransKD: Transformer Knowledge Distillation for Efficient Semantic Segmentation. (arXiv:2202.13393v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2202.13393</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic segmentation benchmarks in the realm of autonomous driving are
dominated by large pre-trained transformers, yet their widespread adoption is
impeded by substantial computational costs and prolonged training durations. To
lift this constraint, we look at efficient semantic segmentation from a
perspective of comprehensive knowledge distillation and consider to bridge the
gap between multi-source knowledge extractions and transformer-specific patch
embeddings. We put forward the Transformer-based Knowledge Distillation
(TransKD) framework which learns compact student transformers by distilling
both feature maps and patch embeddings of large teacher transformers, bypassing
the long pre-training process and reducing the FLOPs by &amp;gt;85.0%. Specifically,
we propose two fundamental and two optimization modules: (1) Cross Selective
Fusion (CSF) enables knowledge transfer between cross-stage features via
channel attention and feature map distillation within hierarchical
transformers; (2) Patch Embedding Alignment (PEA) performs dimensional
transformation within the patchifying process to facilitate the patch embedding
distillation; (3) Global-Local Context Mixer (GL-Mixer) extracts both global
and local information of a representative embedding; (4) Embedding Assistant
(EA) acts as an embedding method to seamlessly bridge teacher and student
models with the teacher&apos;s number of channels. Experiments on Cityscapes, ACDC,
NYUv2, and Pascal VOC2012 datasets show that TransKD outperforms
state-of-the-art distillation frameworks and rivals the time-consuming
pre-training method. The source code is publicly available at
https://github.com/RuipingL/TransKD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Ruiping Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kailun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roitberg_A/0/1/0/all/0/1&quot;&gt;Alina Roitberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiaming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1&quot;&gt;Kunyu Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huayao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaonan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1&quot;&gt;Rainer Stiefelhagen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.02659">
<title>Robust Fine-Tuning of Deep Neural Networks with Hessian-based Generalization Guarantees. (arXiv:2206.02659v6 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.02659</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider fine-tuning a pretrained deep neural network on a target task. We
study the generalization properties of fine-tuning to understand the problem of
overfitting, which has often been observed (e.g., when the target dataset is
small or when the training labels are noisy). Existing generalization measures
for deep networks depend on notions such as distance from the initialization
(i.e., the pretrained network) of the fine-tuned model and noise stability
properties of deep networks. This paper identifies a Hessian-based distance
measure through PAC-Bayesian analysis, which is shown to correlate well with
observed generalization gaps of fine-tuned models. Theoretically, we prove
Hessian distance-based generalization bounds for fine-tuned models. We also
describe an extended study of fine-tuning against label noise, where
overfitting remains a critical problem. We present an algorithm and a
generalization error guarantee for this algorithm under a class conditional
independent noise model. Empirically, we observe that the Hessian-based
distance measure can match the scale of the observed generalization gap of
fine-tuned models in practice. We also test our algorithm on several image
classification tasks with noisy training labels, showing gains over prior
methods and decreases in the Hessian distance measure of the fine-tuned model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ju_H/0/1/0/all/0/1&quot;&gt;Haotian Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dongyue Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongyang R. Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.14539">
<title>Pre-training General Trajectory Embeddings with Maximum Multi-view Entropy Coding. (arXiv:2207.14539v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.14539</link>
<description rdf:parseType="Literal">&lt;p&gt;Spatio-temporal trajectories provide valuable information about movement and
travel behavior, enabling various downstream tasks that in turn power
real-world applications. Learning trajectory embeddings can improve task
performance but may incur high computational costs and face limited training
data availability. Pre-training learns generic embeddings by means of specially
constructed pretext tasks that enable learning from unlabeled data. Existing
pre-training methods face (i) difficulties in learning general embeddings due
to biases towards certain downstream tasks incurred by the pretext tasks, (ii)
limitations in capturing both travel semantics and spatio-temporal
correlations, and (iii) the complexity of long, irregularly sampled
trajectories.
&lt;/p&gt;
&lt;p&gt;To tackle these challenges, we propose Maximum Multi-view Trajectory Entropy
Coding (MMTEC) for learning general and comprehensive trajectory embeddings. We
introduce a pretext task that reduces biases in pre-trained trajectory
embeddings, yielding embeddings that are useful for a wide variety of
downstream tasks. We also propose an attention-based discrete encoder and a
NeuralCDE-based continuous encoder that extract and represent travel behavior
and continuous spatio-temporal correlations from trajectories in embeddings,
respectively. Extensive experiments on two real-world datasets and three
downstream tasks offer insight into the design properties of our proposal and
indicate that it is capable of outperforming existing trajectory embedding
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_H/0/1/0/all/0/1&quot;&gt;Huaiyu Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1&quot;&gt;Shengnan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jilin Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jensen_C/0/1/0/all/0/1&quot;&gt;Christian S. Jensen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Youfang Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.03087">
<title>Iterative Vision-and-Language Navigation. (arXiv:2210.03087v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.03087</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Iterative Vision-and-Language Navigation (IVLN), a paradigm for
evaluating language-guided agents navigating in a persistent environment over
time. Existing Vision-and-Language Navigation (VLN) benchmarks erase the
agent&apos;s memory at the beginning of every episode, testing the ability to
perform cold-start navigation with no prior information. However, deployed
robots occupy the same environment for long periods of time. The IVLN paradigm
addresses this disparity by training and evaluating VLN agents that maintain
memory across tours of scenes that consist of up to 100 ordered
instruction-following Room-to-Room (R2R) episodes, each defined by an
individual language instruction and a target path. We present discrete and
continuous Iterative Room-to-Room (IR2R) benchmarks comprising about 400 tours
each in 80 indoor scenes. We find that extending the implicit memory of
high-performing transformer VLN agents is not sufficient for IVLN, but agents
that build maps can benefit from environment persistence, motivating a renewed
focus on map-building agents in VLN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krantz_J/0/1/0/all/0/1&quot;&gt;Jacob Krantz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1&quot;&gt;Shurjo Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corso_J/0/1/0/all/0/1&quot;&gt;Jason Corso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anderson_P/0/1/0/all/0/1&quot;&gt;Peter Anderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Stefan Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1&quot;&gt;Jesse Thomason&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.04026">
<title>Enhancing Generalizable 6D Pose Tracking of an In-Hand Object with Tactile Sensing. (arXiv:2210.04026v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.04026</link>
<description rdf:parseType="Literal">&lt;p&gt;When manipulating an object to accomplish complex tasks, humans rely on both
vision and touch to keep track of the object&apos;s 6D pose. However, most existing
object pose tracking systems in robotics rely exclusively on visual signals,
which hinder a robot&apos;s ability to manipulate objects effectively. To address
this limitation, we introduce TEG-Track, a tactile-enhanced 6D pose tracking
system that can track previously unseen objects held in hand. From consecutive
tactile signals, TEG-Track optimizes object velocities from marker flows when
slippage does not occur, or regresses velocities using a slippage estimation
network when slippage is detected. The estimated object velocities are
integrated into a geometric-kinematic optimization scheme to enhance existing
visual pose trackers. To evaluate our method and to facilitate future research,
we construct a real-world dataset for visual-tactile in-hand object pose
tracking. Experimental results demonstrate that TEG-Track consistently enhances
state-of-the-art generalizable 6D pose trackers in synthetic and real-world
scenarios. Our code and dataset are available at
https://github.com/leolyliu/TEG-Track.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaomeng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weihang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1&quot;&gt;Haocheng Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;He Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Rui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1&quot;&gt;Li Yi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.13929">
<title>XKD: Cross-modal Knowledge Distillation with Domain Alignment for Video Representation Learning. (arXiv:2211.13929v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.13929</link>
<description rdf:parseType="Literal">&lt;p&gt;We present XKD, a novel self-supervised framework to learn meaningful
representations from unlabelled videos. XKD is trained with two pseudo
objectives. First, masked data reconstruction is performed to learn
modality-specific representations from audio and visual streams. Next,
self-supervised cross-modal knowledge distillation is performed between the two
modalities through a teacher-student setup to learn complementary information.
We introduce a novel domain alignment strategy to tackle domain discrepancy
between audio and visual modalities enabling effective cross-modal knowledge
distillation. Additionally, to develop a general-purpose network capable of
handling both audio and visual streams, modality-agnostic variants of XKD are
introduced, which use the same pretrained backbone for different audio and
visual tasks. Our proposed cross-modal knowledge distillation improves video
action classification by $8\%$ to $14\%$ on UCF101, HMDB51, and Kinetics400.
Additionally, XKD improves multimodal action classification by $5.5\%$ on
Kinetics-Sound. XKD shows state-of-the-art performance in sound classification
on ESC50, achieving top-1 accuracy of $96.5\%$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkar_P/0/1/0/all/0/1&quot;&gt;Pritam Sarkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1&quot;&gt;Ali Etemad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.10368">
<title>Masked Event Modeling: Self-Supervised Pretraining for Event Cameras. (arXiv:2212.10368v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.10368</link>
<description rdf:parseType="Literal">&lt;p&gt;Event cameras asynchronously capture brightness changes with low latency,
high temporal resolution, and high dynamic range. However, annotation of event
data is a costly and laborious process, which limits the use of deep learning
methods for classification and other semantic tasks with the event modality. To
reduce the dependency on labeled event data, we introduce Masked Event Modeling
(MEM), a self-supervised framework for events. Our method pretrains a neural
network on unlabeled events, which can originate from any event camera
recording. Subsequently, the pretrained model is finetuned on a downstream
task, leading to a consistent improvement of the task accuracy. For example,
our method reaches state-of-the-art classification accuracy across three
datasets, N-ImageNet, N-Cars, and N-Caltech101, increasing the top-1 accuracy
of previous work by significant margins. When tested on real-world event data,
MEM is even superior to supervised RGB-based pretraining. The models pretrained
with MEM are also label-efficient and generalize well to the dense task of
semantic image segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klenk_S/0/1/0/all/0/1&quot;&gt;Simon Klenk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonello_D/0/1/0/all/0/1&quot;&gt;David Bonello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koestler_L/0/1/0/all/0/1&quot;&gt;Lukas Koestler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Araslanov_N/0/1/0/all/0/1&quot;&gt;Nikita Araslanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1&quot;&gt;Daniel Cremers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.11798">
<title>MedSegDiff-V2: Diffusion based Medical Image Segmentation with Transformer. (arXiv:2301.11798v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.11798</link>
<description rdf:parseType="Literal">&lt;p&gt;The Diffusion Probabilistic Model (DPM) has recently gained popularity in the
field of computer vision, thanks to its image generation applications, such as
Imagen, Latent Diffusion Models, and Stable Diffusion, which have demonstrated
impressive capabilities and sparked much discussion within the community.
Recent investigations have further unveiled the utility of DPM in the domain of
medical image analysis, as underscored by the commendable performance exhibited
by the medical image segmentation model across various tasks. Although these
models were originally underpinned by a UNet architecture, there exists a
potential avenue for enhancing their performance through the integration of
vision transformer mechanisms. However, we discovered that simply combining
these two models resulted in subpar performance. To effectively integrate these
two cutting-edge techniques for the Medical image segmentation, we propose a
novel Transformer-based Diffusion framework, called MedSegDiff-V2. We verify
its effectiveness on 20 medical image segmentation tasks with different image
modalities. Through comprehensive evaluation, our approach demonstrates
superiority over prior state-of-the-art (SOTA) methodologies. Code is released
at https://github.com/KidsWithTokens/MedSegDiff
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Junde Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ji_W/0/1/0/all/0/1&quot;&gt;Wei Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Huazhu Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Min Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yueming Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yanwu Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.13335">
<title>Multi-modal Large Language Model Enhanced Pseudo 3D Perception Framework for Visual Commonsense Reasoning. (arXiv:2301.13335v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.13335</link>
<description rdf:parseType="Literal">&lt;p&gt;The visual commonsense reasoning (VCR) task is to choose an answer and
provide a justifying rationale based on the given image and textural question.
Representative works first recognize objects in images and then associate them
with key words in texts. However, existing approaches do not consider exact
positions of objects in a human-like three-dimensional (3D) manner, making them
incompetent to accurately distinguish objects and understand visual relation.
Recently, multi-modal large language models (MLLMs) have been used as powerful
tools for several multi-modal tasks but not for VCR yet, which requires
elaborate reasoning on specific visual objects referred by texts. In light of
the above, an MLLM enhanced pseudo 3D perception framework is designed for VCR.
Specifically, we first demonstrate that the relation between objects is
relevant to object depths in images, and hence introduce object depth into VCR
frameworks to infer 3D positions of objects in images. Then, a depth-aware
Transformer is proposed to encode depth differences between objects into the
attention mechanism of Transformer to discriminatively associate objects with
visual scenes guided by depth. To further associate the answer with the depth
of visual scene, each word in the answer is tagged with a pseudo depth to
realize depth-aware association between answer words and objects. On the other
hand, BLIP-2 as an MLLM is employed to process images and texts, and the
referring expressions in texts involving specific visual objects are modified
with linguistic object labels to serve as comprehensible MLLM inputs. Finally,
a parameter optimization technique is devised to fully consider the quality of
data batches based on multi-level reasoning confidence. Experiments on the VCR
dataset demonstrate the superiority of the proposed framework over
state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jian Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hanli Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1&quot;&gt;Miaojing Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.06961">
<title>DualStreamFoveaNet: A Dual Stream Fusion Architecture with Anatomical Awareness for Robust Fovea Localization. (arXiv:2302.06961v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.06961</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate fovea localization is essential for analyzing retinal diseases to
prevent irreversible vision loss. While current deep learning-based methods
outperform traditional ones, they still face challenges such as the lack of
local anatomical landmarks around the fovea, the inability to robustly handle
diseased retinal images, and the variations in image conditions. In this paper,
we propose a novel transformer-based architecture called DualStreamFoveaNet
(DSFN) for multi-cue fusion. This architecture explicitly incorporates
long-range connections and global features using retina and vessel
distributions for robust fovea localization. We introduce a spatial attention
mechanism in the dual-stream encoder to extract and fuse self-learned
anatomical information, focusing more on features distributed along blood
vessels and significantly reducing computational costs by decreasing token
numbers. Our extensive experiments show that the proposed architecture achieves
state-of-the-art performance on two public datasets and one large-scale private
dataset. Furthermore, we demonstrate that the DSFN is more robust on both
normal and diseased retina images and has better generalization capacity in
cross-dataset experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Sifan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinfeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zilong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1&quot;&gt;Jionglong Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1&quot;&gt;Xiaowei Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dang_K/0/1/0/all/0/1&quot;&gt;Kang Dang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.00952">
<title>MuscleMap: Towards Video-based Activated Muscle Group Estimation in the Wild. (arXiv:2303.00952v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.00952</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we tackle the new task of video-based Activated Muscle Group
Estimation (AMGE) aiming at identifying active muscle regions during physical
activity in the wild. To this intent, we provide the MuscleMap dataset
featuring &amp;gt;15K video clips with 135 different activities and 20 labeled muscle
groups. This dataset opens the vistas to multiple video-based applications in
sports and rehabilitation medicine under flexible environment constraints. The
proposed MuscleMap dataset is constructed with YouTube videos, specifically
targeting High-Intensity Interval Training (HIIT) physical exercise in the
wild. To make the AMGE model applicable in real-life situations, it is crucial
to ensure that the model can generalize well to numerous types of physical
activities not present during training and involving new combinations of
activated muscles. To achieve this, our benchmark also covers an evaluation
setting where the model is exposed to activity types excluded from the training
set. Our experiments reveal that the generalizability of existing architectures
adapted for the AMGE task remains a challenge. Therefore, we also propose a new
approach, TransM3E, which employs a multi-modality feature fusion mechanism
between both the video transformer model and the skeleton-based graph
convolution model with novel cross-modal knowledge distillation executed on
multi-classification tokens. The proposed method surpasses all popular video
classification models when dealing with both, previously seen and new types of
physical activities. The contributed dataset and code are made publicly
available at https://github.com/KPeng9510/MuscleMap.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1&quot;&gt;Kunyu Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_D/0/1/0/all/0/1&quot;&gt;David Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roitberg_A/0/1/0/all/0/1&quot;&gt;Alina Roitberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kailun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiaming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1&quot;&gt;Chen Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kaiyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarfraz_M/0/1/0/all/0/1&quot;&gt;M. Saquib Sarfraz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1&quot;&gt;Rainer Stiefelhagen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.08977">
<title>DeblurSR: Event-Based Motion Deblurring Under the Spiking Representation. (arXiv:2303.08977v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.08977</link>
<description rdf:parseType="Literal">&lt;p&gt;We present DeblurSR, a novel motion deblurring approach that converts a
blurry image into a sharp video. DeblurSR utilizes event data to compensate for
motion ambiguities and exploits the spiking representation to parameterize the
sharp output video as a mapping from time to intensity. Our key contribution,
the Spiking Representation (SR), is inspired by the neuromorphic principles
determining how biological neurons communicate with each other in living
organisms. We discuss why the spikes can represent sharp edges and how the
spiking parameters are interpreted from the neuromorphic perspective. DeblurSR
has higher output quality and requires fewer computing resources than
state-of-the-art event-based motion deblurring methods. We additionally show
that our approach easily extends to video super-resolution when combined with
recent advances in implicit neural representation. The implementation and
animated visualization of DeblurSR are available at
https://github.com/chensong1995/DeblurSR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1&quot;&gt;Chen Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bajaj_C/0/1/0/all/0/1&quot;&gt;Chandrajit Bajaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qixing Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.12332">
<title>Weakly-Supervised Temporal Action Localization by Inferring Salient Snippet-Feature. (arXiv:2303.12332v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.12332</link>
<description rdf:parseType="Literal">&lt;p&gt;Weakly-supervised temporal action localization aims to locate action regions
and identify action categories in untrimmed videos simultaneously by taking
only video-level labels as the supervision. Pseudo label generation is a
promising strategy to solve the challenging problem, but the current methods
ignore the natural temporal structure of the video that can provide rich
information to assist such a generation process. In this paper, we propose a
novel weakly-supervised temporal action localization method by inferring
salient snippet-feature. First, we design a saliency inference module that
exploits the variation relationship between temporal neighbor snippets to
discover salient snippet-features, which can reflect the significant dynamic
change in the video. Secondly, we introduce a boundary refinement module that
enhances salient snippet-features through the information interaction unit.
Then, a discrimination enhancement module is introduced to enhance the
discriminative nature of snippet-features. Finally, we adopt the refined
snippet-features to produce high-fidelity pseudo labels, which could be used to
supervise the training of the action localization network. Extensive
experiments on two publicly available datasets, i.e., THUMOS14 and ActivityNet
v1.3, demonstrate our proposed method achieves significant improvements
compared to the state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_W/0/1/0/all/0/1&quot;&gt;Wulian Yun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_M/0/1/0/all/0/1&quot;&gt;Mengshi Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chuanming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Huadong Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.00670">
<title>CRN: Camera Radar Net for Accurate, Robust, Efficient 3D Perception. (arXiv:2304.00670v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.00670</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous driving requires an accurate and fast 3D perception system that
includes 3D object detection, tracking, and segmentation. Although recent
low-cost camera-based approaches have shown promising results, they are
susceptible to poor illumination or bad weather conditions and have a large
localization error. Hence, fusing camera with low-cost radar, which provides
precise long-range measurement and operates reliably in all environments, is
promising but has not yet been thoroughly investigated. In this paper, we
propose Camera Radar Net (CRN), a novel camera-radar fusion framework that
generates a semantically rich and spatially accurate bird&apos;s-eye-view (BEV)
feature map for various tasks. To overcome the lack of spatial information in
an image, we transform perspective view image features to BEV with the help of
sparse but accurate radar points. We further aggregate image and radar feature
maps in BEV using multi-modal deformable attention designed to tackle the
spatial misalignment between inputs. CRN with real-time setting operates at 20
FPS while achieving comparable performance to LiDAR detectors on nuScenes, and
even outperforms at a far distance on 100m setting. Moreover, CRN with offline
setting yields 62.4% NDS, 57.5% mAP on nuScenes test set and ranks first among
all camera and camera-radar 3D object detectors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Youngseok Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1&quot;&gt;Juyeb Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sanmin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1&quot;&gt;In-Jae Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jun Won Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kum_D/0/1/0/all/0/1&quot;&gt;Dongsuk Kum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.14660">
<title>Segment Anything Model for Medical Images?. (arXiv:2304.14660v6 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.14660</link>
<description rdf:parseType="Literal">&lt;p&gt;The Segment Anything Model (SAM) is the first foundation model for general
image segmentation. It has achieved impressive results on various natural image
segmentation tasks. However, medical image segmentation (MIS) is more
challenging because of the complex modalities, fine anatomical structures,
uncertain and complex object boundaries, and wide-range object scales. To fully
validate SAM&apos;s performance on medical data, we collected and sorted 53
open-source datasets and built a large medical segmentation dataset with 18
modalities, 84 objects, 125 object-modality paired targets, 1050K 2D images,
and 6033K masks. We comprehensively analyzed different models and strategies on
the so-called COSMOS 1050K dataset. Our findings mainly include the following:
1) SAM showed remarkable performance in some specific objects but was unstable,
imperfect, or even totally failed in other situations. 2) SAM with the large
ViT-H showed better overall performance than that with the small ViT-B. 3) SAM
performed better with manual hints, especially box, than the Everything mode.
4) SAM could help human annotation with high labeling quality and less time. 5)
SAM was sensitive to the randomness in the center point and tight box prompts,
and may suffer from a serious performance drop. 6) SAM performed better than
interactive methods with one or a few points, but will be outpaced as the
number of points increases. 7) SAM&apos;s performance correlated to different
factors, including boundary complexity, intensity differences, etc. 8)
Finetuning the SAM on specific medical tasks could improve its average DICE
performance by 4.39% and 6.68% for ViT-B and ViT-H, respectively. We hope that
this comprehensive report can help researchers explore the potential of SAM
applications in MIS, and guide how to appropriately use and develop SAM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yuhao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Han Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chang_A/0/1/0/all/0/1&quot;&gt;Ao Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xinrui Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Rusi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Junxuan Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiongquan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chaoyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sijing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chi_H/0/1/0/all/0/1&quot;&gt;Haozhe Chi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xindi Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yue_K/0/1/0/all/0/1&quot;&gt;Kejuan Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Grau_V/0/1/0/all/0/1&quot;&gt;Vicente Grau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fan_D/0/1/0/all/0/1&quot;&gt;Deng-Ping Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dong_F/0/1/0/all/0/1&quot;&gt;Fajin Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ni_D/0/1/0/all/0/1&quot;&gt;Dong Ni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.03535">
<title>Next-generation Surgical Navigation: Marker-less Multi-view 6DoF Pose Estimation of Surgical Instruments. (arXiv:2305.03535v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.03535</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art research of traditional computer vision is increasingly
leveraged in the surgical domain. A particular focus in computer-assisted
surgery is to replace marker-based tracking systems for instrument localization
with pure image-based 6DoF pose estimation using deep-learning methods.
However, state-of-the-art single-view pose estimation methods do not yet meet
the accuracy required for surgical navigation. In this context, we investigate
the benefits of multi-view setups for highly accurate and occlusion-robust 6DoF
pose estimation of surgical instruments and derive recommendations for an ideal
camera system that addresses the challenges in the operating room.
&lt;/p&gt;
&lt;p&gt;The contributions of this work are threefold. First, we present a
multi-camera capture setup consisting of static and head-mounted cameras, which
allows us to study the performance of pose estimation methods under various
camera configurations. Second, we publish a multi-view RGB-D video dataset of
ex-vivo spine surgeries, captured in a surgical wet lab and a real operating
theatre and including rich annotations for surgeon, instrument, and patient
anatomy. Third, we evaluate three state-of-the-art single-view and multi-view
methods for the task of 6DoF pose estimation of surgical instruments and
analyze the influence of camera configurations, training data, and occlusions
on the pose accuracy and generalization ability. The best method utilizes five
cameras in a multi-view pose optimization and achieves an average position and
orientation error of 1.01 mm and 0.89\deg for a surgical drill as well as 2.79
mm and 3.33\deg for a screwdriver under optimal conditions. Our results
demonstrate that marker-less tracking of surgical instruments is becoming a
feasible alternative to existing marker-based systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hein_J/0/1/0/all/0/1&quot;&gt;Jonas Hein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cavalcanti_N/0/1/0/all/0/1&quot;&gt;Nicola Cavalcanti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suter_D/0/1/0/all/0/1&quot;&gt;Daniel Suter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zingg_L/0/1/0/all/0/1&quot;&gt;Lukas Zingg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carrillo_F/0/1/0/all/0/1&quot;&gt;Fabio Carrillo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calvet_L/0/1/0/all/0/1&quot;&gt;Lilian Calvet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farshad_M/0/1/0/all/0/1&quot;&gt;Mazda Farshad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1&quot;&gt;Marc Pollefeys&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1&quot;&gt;Nassir Navab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furnstahl_P/0/1/0/all/0/1&quot;&gt;Philipp F&amp;#xfc;rnstahl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.04186">
<title>Video-Specific Query-Key Attention Modeling for Weakly-Supervised Temporal Action Localization. (arXiv:2305.04186v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.04186</link>
<description rdf:parseType="Literal">&lt;p&gt;Weakly-supervised temporal action localization aims to identify and localize
the action instances in the untrimmed videos with only video-level action
labels. When humans watch videos, we can adapt our abstract-level knowledge
about actions in different video scenarios and detect whether some actions are
occurring. In this paper, we mimic how humans do and bring a new perspective
for locating and identifying multiple actions in a video. We propose a network
named VQK-Net with a video-specific query-key attention modeling that learns a
unique query for each action category of each input video. The learned queries
not only contain the actions&apos; knowledge features at the abstract level but also
have the ability to fit this knowledge into the target video scenario, and they
will be used to detect the presence of the corresponding action along the
temporal dimension. To better learn these action category queries, we exploit
not only the features of the current input video but also the correlation
between different videos through a novel video-specific action category query
learner worked with a query similarity loss. Finally, we conduct extensive
experiments on three commonly used datasets (THUMOS14, ActivityNet1.2, and
ActivityNet1.3) and achieve state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xijun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katsaggelos_A/0/1/0/all/0/1&quot;&gt;Aggelos K. Katsaggelos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.06141">
<title>Active Semantic Localization with Graph Neural Embedding. (arXiv:2305.06141v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.06141</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic localization, i.e., robot self-localization with semantic image
modality, is critical in recently emerging embodied AI applications (e.g.,
point-goal navigation, object-goal navigation, vision language navigation) and
topological mapping applications (e.g., graph neural SLAM, ego-centric
topological map). However, most existing works on semantic localization focus
on passive vision tasks without viewpoint planning, or rely on additional rich
modalities (e.g., depth measurements). Thus, the problem is largely unsolved.
In this work, we explore a lightweight, entirely CPU-based, domain-adaptive
semantic localization framework, called graph neural localizer. Our approach is
inspired by two recently emerging technologies: (1) Scene graph, which combines
the viewpoint- and appearance- invariance of local and global features; (2)
Graph neural network, which enables direct learning/recognition of graph data
(i.e., non-vector data). Specifically, a graph convolutional neural network is
first trained as a scene graph classifier for passive vision, and then its
knowledge is transferred to a reinforcement-learning planner for active vision.
Experiments on two scenarios, self-supervised learning and unsupervised domain
adaptation, using a photo-realistic Habitat simulator validate the
effectiveness of the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoshida_M/0/1/0/all/0/1&quot;&gt;Mitsuki Yoshida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanaka_K/0/1/0/all/0/1&quot;&gt;Kanji Tanaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamamoto_R/0/1/0/all/0/1&quot;&gt;Ryogo Yamamoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iwata_D/0/1/0/all/0/1&quot;&gt;Daiki Iwata&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.07223">
<title>Transavs: End-To-End Audio-Visual Segmentation With Transformer. (arXiv:2305.07223v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2305.07223</link>
<description rdf:parseType="Literal">&lt;p&gt;Audio-Visual Segmentation (AVS) is a challenging task, which aims to segment
sounding objects in video frames by exploring audio signals. Generally AVS
faces two key challenges: (1) Audio signals inherently exhibit a high degree of
information density, as sounds produced by multiple objects are entangled
within the same audio stream; (2) Objects of the same category tend to produce
similar audio signals, making it difficult to distinguish between them and thus
leading to unclear segmentation results. Toward this end, we propose TransAVS,
the first Transformer-based end-to-end framework for AVS task. Specifically,
TransAVS disentangles the audio stream as audio queries, which will interact
with images and decode into segmentation masks with full transformer
architectures. This scheme not only promotes comprehensive audio-image
communication but also explicitly excavates instance cues encapsulated in the
scene. Meanwhile, to encourage these audio queries to capture distinctive
sounding objects instead of degrading to be homogeneous, we devise two
self-supervised loss functions at both query and mask levels, allowing the
model to capture distinctive features within similar audio data and achieve
more precise segmentation. Our experiments demonstrate that TransAVS achieves
state-of-the-art results on the AVSBench dataset, highlighting its
effectiveness in bridging the gap between audio and visual modalities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_Y/0/1/0/all/0/1&quot;&gt;Yuhang Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuxi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1&quot;&gt;Zhenye Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiangning Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chi_M/0/1/0/all/0/1&quot;&gt;Mingmin Chi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yabiao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.08685">
<title>CLIP-VG: Self-paced Curriculum Adapting of CLIP for Visual Grounding. (arXiv:2305.08685v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.08685</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual Grounding (VG) is a crucial topic in the field of vision and language,
which involves locating a specific region described by expressions within an
image. To reduce the reliance on manually labeled data, unsupervised visual
grounding have been developed to locate regions using pseudo-labels. However,
the performance of existing unsupervised methods is highly dependent on the
quality of pseudo-labels and these methods always encounter issues with limited
diversity. In order to utilize vision and language pre-trained models to
address the grounding problem, and reasonably take advantage of pseudo-labels,
we propose CLIP-VG, a novel method that can conduct self-paced curriculum
adapting of CLIP with pseudo-language labels. We propose a simple yet efficient
end-to-end network architecture to realize the transfer of CLIP to the visual
grounding. Based on the CLIP-based architecture, we further propose
single-source and multi-source curriculum adapting algorithms, which can
progressively find more reliable pseudo-labels to learn an optimal model,
thereby achieving a balance between reliability and diversity for the
pseudo-language labels. Our method outperforms the current state-of-the-art
unsupervised method by a significant margin on RefCOCO/+/g datasets in both
single-source and multi-source scenarios, with improvements ranging from
6.78$\%$ to 10.67$\%$ and 11.39$\%$ to 14.87$\%$, respectively. The results
even outperform existing weakly supervised visual grounding methods.
Furthermore, our method is also competitive in fully supervised setting. The
code and models are available at https://github.com/linhuixiao/CLIP-VG.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1&quot;&gt;Linhui Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaoshan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_F/0/1/0/all/0/1&quot;&gt;Fang Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1&quot;&gt;Ming Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaowei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Changsheng Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10400">
<title>What You See is What You Read? Improving Text-Image Alignment Evaluation. (arXiv:2305.10400v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10400</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatically determining whether a text and a corresponding image are
semantically aligned is a significant challenge for vision-language models,
with applications in generative text-to-image and image-to-text tasks. In this
work, we study methods for automatic text-image alignment evaluation. We first
introduce SeeTRUE: a comprehensive evaluation set, spanning multiple datasets
from both text-to-image and image-to-text generation tasks, with human
judgements for whether a given text-image pair is semantically aligned. We then
describe two automatic methods to determine alignment: the first involving a
pipeline based on question generation and visual question answering models, and
the second employing an end-to-end classification approach by finetuning
multimodal pretrained models. Both methods surpass prior approaches in various
text-image alignment tasks, with significant improvements in challenging cases
that involve complex composition or unnatural images. Finally, we demonstrate
how our approaches can localize specific misalignments between an image and a
given text, and how they can be used to automatically re-rank candidates in
text-to-image generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yarom_M/0/1/0/all/0/1&quot;&gt;Michal Yarom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1&quot;&gt;Yonatan Bitton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Changpinyo_S/0/1/0/all/0/1&quot;&gt;Soravit Changpinyo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aharoni_R/0/1/0/all/0/1&quot;&gt;Roee Aharoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herzig_J/0/1/0/all/0/1&quot;&gt;Jonathan Herzig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lang_O/0/1/0/all/0/1&quot;&gt;Oran Lang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ofek_E/0/1/0/all/0/1&quot;&gt;Eran Ofek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szpektor_I/0/1/0/all/0/1&quot;&gt;Idan Szpektor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13773">
<title>Enhanced Fine-grained Motion Diffusion for Text-driven Human Motion Synthesis. (arXiv:2305.13773v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13773</link>
<description rdf:parseType="Literal">&lt;p&gt;The emergence of text-driven motion synthesis technique provides animators
with great potential to create efficiently. However, in most cases, textual
expressions only contain general and qualitative motion descriptions, while
lack fine depiction and sufficient intensity, leading to the synthesized
motions that either (a) semantically compliant but uncontrollable over specific
pose details, or (b) even deviates from the provided descriptions, bringing
animators with undesired cases. In this paper, we propose DiffKFC, a
conditional diffusion model for text-driven motion synthesis with KeyFrames
Collaborated, enabling realistic generation with collaborative and efficient
dual-level control: coarse guidance at semantic level, with only few keyframes
for direct and fine-grained depiction down to body posture level. Unlike
existing inference-editing diffusion models that incorporate conditions without
training, our conditional diffusion model is explicitly trained and can fully
exploit correlations among texts, keyframes and the diffused target frames. To
preserve the control capability of discrete and sparse keyframes, we customize
dilated mask attention modules where only partial valid tokens participate in
local-to-global attention, indicated by the dilated keyframe mask.
Additionally, we develop a simple yet effective smoothness prior, which steers
the generated frames towards seamless keyframe transitions at inference.
Extensive experiments show that our model not only achieves state-of-the-art
performance in terms of semantic fidelity, but more importantly, is able to
satisfy animator requirements through fine-grained guidance without tedious
labor.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1&quot;&gt;Dong Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xiaoning Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Huaijiang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shengxiang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Weiqing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jianfeng Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17214">
<title>Contrast, Attend and Diffuse to Decode High-Resolution Images from Brain Activities. (arXiv:2305.17214v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17214</link>
<description rdf:parseType="Literal">&lt;p&gt;Decoding visual stimuli from neural responses recorded by functional Magnetic
Resonance Imaging (fMRI) presents an intriguing intersection between cognitive
neuroscience and machine learning, promising advancements in understanding
human visual perception and building non-invasive brain-machine interfaces.
However, the task is challenging due to the noisy nature of fMRI signals and
the intricate pattern of brain visual representations. To mitigate these
challenges, we introduce a two-phase fMRI representation learning framework.
The first phase pre-trains an fMRI feature learner with a proposed
Double-contrastive Mask Auto-encoder to learn denoised representations. The
second phase tunes the feature learner to attend to neural activation patterns
most informative for visual reconstruction with guidance from an image
auto-encoder. The optimized fMRI feature learner then conditions a latent
diffusion model to reconstruct image stimuli from brain activities.
Experimental results demonstrate our model&apos;s superiority in generating
high-resolution and semantically accurate images, substantially exceeding
previous state-of-the-art methods by 39.34% in the 50-way-top-1 semantic
classification accuracy. Our research invites further exploration of the
decoding task&apos;s potential and contributes to the development of non-invasive
brain-machine interfaces.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jingyuan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mingxiao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zijiao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunhao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shaonan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1&quot;&gt;Marie-Francine Moens&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.01461">
<title>PolyDiffuse: Polygonal Shape Reconstruction via Guided Set Diffusion Models. (arXiv:2306.01461v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.01461</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents PolyDiffuse, a novel structured reconstruction algorithm
that transforms visual sensor data into polygonal shapes with Diffusion Models
(DM), an emerging machinery amid exploding generative AI, while formulating
reconstruction as a generation process conditioned on sensor data. The task of
structured reconstruction poses two fundamental challenges to DM: 1) A
structured geometry is a ``set&apos;&apos; (e.g., a set of polygons for a floorplan
geometry), where a sample of $N$ elements has $N!$ different but equivalent
representations, making the denoising highly ambiguous; and 2) A
``reconstruction&apos;&apos; task has a single solution, where an initial noise needs to
be chosen carefully, while any initial noise works for a generation task. Our
technical contribution is the introduction of a Guided Set Diffusion Model
where 1) the forward diffusion process learns guidance networks to control
noise injection so that one representation of a sample remains distinct from
its other permutation variants, thus resolving denoising ambiguity; and 2) the
reverse denoising process reconstructs polygonal shapes, initialized and
directed by the guidance networks, as a conditional generation process subject
to the sensor data. We have evaluated our approach for reconstructing two types
of polygonal shapes: floorplan as a set of polygons and HD map for autonomous
cars as a set of polylines. Through extensive experiments on standard
benchmarks, we demonstrate that PolyDiffuse significantly advances the current
state of the art and enables broader practical applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiacheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_R/0/1/0/all/0/1&quot;&gt;Ruizhi Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furukawa_Y/0/1/0/all/0/1&quot;&gt;Yasutaka Furukawa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.03831">
<title>GEO-Bench: Toward Foundation Models for Earth Monitoring. (arXiv:2306.03831v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.03831</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent progress in self-supervision has shown that pre-training large neural
networks on vast amounts of unsupervised data can lead to substantial increases
in generalization to downstream tasks. Such models, recently coined foundation
models, have been transformational to the field of natural language processing.
Variants have also been proposed for image data, but their applicability to
remote sensing tasks is limited. To stimulate the development of foundation
models for Earth monitoring, we propose a benchmark comprised of six
classification and six segmentation tasks, which were carefully curated and
adapted to be both relevant to the field and well-suited for model evaluation.
We accompany this benchmark with a robust methodology for evaluating models and
reporting aggregated results to enable a reliable assessment of progress.
Finally, we report results for 20 baselines to gain information about the
performance of existing models. We believe that this benchmark will be a driver
of progress across a variety of Earth monitoring tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lacoste_A/0/1/0/all/0/1&quot;&gt;Alexandre Lacoste&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lehmann_N/0/1/0/all/0/1&quot;&gt;Nils Lehmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_P/0/1/0/all/0/1&quot;&gt;Pau Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sherwin_E/0/1/0/all/0/1&quot;&gt;Evan David Sherwin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kerner_H/0/1/0/all/0/1&quot;&gt;Hannah Kerner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lutjens_B/0/1/0/all/0/1&quot;&gt;Bj&amp;#xf6;rn L&amp;#xfc;tjens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Irvin_J/0/1/0/all/0/1&quot;&gt;Jeremy Andrew Irvin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dao_D/0/1/0/all/0/1&quot;&gt;David Dao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alemohammad_H/0/1/0/all/0/1&quot;&gt;Hamed Alemohammad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Drouin_A/0/1/0/all/0/1&quot;&gt;Alexandre Drouin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunturkun_M/0/1/0/all/0/1&quot;&gt;Mehmet Gunturkun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1&quot;&gt;Gabriel Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vazquez_D/0/1/0/all/0/1&quot;&gt;David Vazquez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Newman_D/0/1/0/all/0/1&quot;&gt;Dava Newman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1&quot;&gt;Stefano Ermon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiao Xiang Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06093">
<title>HyP-NeRF: Learning Improved NeRF Priors using a HyperNetwork. (arXiv:2306.06093v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06093</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Fields (NeRF) have become an increasingly popular
representation to capture high-quality appearance and shape of scenes and
objects. However, learning generalizable NeRF priors over categories of scenes
or objects has been challenging due to the high dimensionality of network
weight space. To address the limitations of existing work on generalization,
multi-view consistency and to improve quality, we propose HyP-NeRF, a latent
conditioning method for learning generalizable category-level NeRF priors using
hypernetworks. Rather than using hypernetworks to estimate only the weights of
a NeRF, we estimate both the weights and the multi-resolution hash encodings
resulting in significant quality gains. To improve quality even further, we
incorporate a denoise and finetune strategy that denoises images rendered from
NeRFs estimated by the hypernetwork and finetunes it while retaining multiview
consistency. These improvements enable us to use HyP-NeRF as a generalizable
prior for multiple downstream tasks including NeRF reconstruction from
single-view or cluttered scenes and text-to-NeRF. We provide qualitative
comparisons and evaluate HyP-NeRF on three tasks: generalization, compression,
and retrieval, demonstrating our state-of-the-art results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sen_B/0/1/0/all/0/1&quot;&gt;Bipasha Sen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1&quot;&gt;Gaurav Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1&quot;&gt;Aditya Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agaram_R/0/1/0/all/0/1&quot;&gt;Rohith Agaram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1&quot;&gt;K Madhava Krishna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sridhar_S/0/1/0/all/0/1&quot;&gt;Srinath Sridhar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06157">
<title>Fault Localization for Buggy Deep Learning Framework Conversions in Image Recognition. (arXiv:2306.06157v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06157</link>
<description rdf:parseType="Literal">&lt;p&gt;When deploying Deep Neural Networks (DNNs), developers often convert models
from one deep learning framework to another (e.g., TensorFlow to PyTorch).
However, this process is error-prone and can impact target model accuracy. To
identify the extent of such impact, we perform and briefly present a
differential analysis against three DNNs widely used for image recognition
(MobileNetV2, ResNet101, and InceptionV3) converted across four well-known deep
learning frameworks (PyTorch, Keras, TensorFlow (TF), and TFLite), which
revealed numerous model crashes and output label discrepancies of up to 72%. To
mitigate such errors, we present a novel approach towards fault localization
and repair of buggy deep learning framework conversions, focusing on
pre-trained image recognition models. Our technique consists of four stages of
analysis: 1) conversion tools, 2) model parameters, 3) model hyperparameters,
and 4) graph representation. In addition, we propose various strategies towards
fault repair of the faults detected. We implement our technique on top of the
Apache TVM deep learning compiler, and we test it by conducting a preliminary
fault localization analysis for the conversion of InceptionV3 from TF to
TFLite. Our approach detected a fault in a common DNN converter tool, which
introduced precision errors in weights, reducing model accuracy. After our
fault localization, we repaired the issue, reducing our conversion error to
zero.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Louloudakis_N/0/1/0/all/0/1&quot;&gt;Nikolaos Louloudakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gibson_P/0/1/0/all/0/1&quot;&gt;Perry Gibson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cano_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Cano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajan_A/0/1/0/all/0/1&quot;&gt;Ajitha Rajan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06208">
<title>DeltaNN: Assessing the Impact of Computational Environment Parameters on the Performance of Image Recognition Models. (arXiv:2306.06208v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06208</link>
<description rdf:parseType="Literal">&lt;p&gt;Image recognition tasks typically use deep learning and require enormous
processing power, thus relying on hardware accelerators like GPUs and TPUs for
fast, timely processing. Failure in real-time image recognition tasks can occur
due to sub-optimal mapping on hardware accelerators during model deployment,
which may lead to timing uncertainty and erroneous behavior. Mapping on
hardware accelerators is done using multiple software components like deep
learning frameworks, compilers, and device libraries, that we refer to as the
computational environment. Owing to the increased use of image recognition
tasks in safety-critical applications like autonomous driving and medical
imaging, it is imperative to assess their robustness to changes in the
computational environment, as the impact of parameters like deep learning
frameworks, compiler optimizations, and hardware devices on model performance
and correctness is not yet well understood.
&lt;/p&gt;
&lt;p&gt;In this paper we present a differential testing framework, DeltaNN, that
allows us to assess the impact of different computational environment
parameters on the performance of image recognition models during deployment,
post training. DeltaNN generates different implementations of a given image
recognition model for variations in environment parameters, namely, deep
learning frameworks, compiler optimizations and hardware devices and analyzes
differences in model performance as a result. Using DeltaNN, we conduct an
empirical study of robustness analysis of three popular image recognition
models using the ImageNet dataset. We report the impact in terms of
misclassifications and inference time differences across different settings. In
total, we observed up to 72% output label differences across deep learning
frameworks, and up to 81% unexpected performance degradation in terms of
inference time, when applying compiler optimizations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Louloudakis_N/0/1/0/all/0/1&quot;&gt;Nikolaos Louloudakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gibson_P/0/1/0/all/0/1&quot;&gt;Perry Gibson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cano_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Cano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajan_A/0/1/0/all/0/1&quot;&gt;Ajitha Rajan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06406">
<title>D3L: Decomposition of 3D Rotation and Lift from 2D Joint to 3D for Human Mesh Recovery. (arXiv:2306.06406v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06406</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing methods for 3D human mesh recovery always directly estimate SMPL
parameters, which involve both joint rotations and shape parameters. However,
these methods present rotation semantic ambiguity, rotation error accumulation,
and shape estimation overfitting, which also leads to errors in the estimated
pose. Additionally, these methods have not efficiently leveraged the
advancements in another hot topic, human pose estimation. To address these
issues, we propose a novel approach, Decomposition of 3D Rotation and Lift from
2D Joint to 3D mesh (D3L). We disentangle 3D joint rotation into bone direction
and bone twist direction so that the human mesh recovery task is broken down
into estimation of pose, twist, and shape, which can be handled independently.
Then we design a 2D-to-3D lifting network for estimating twist direction and 3D
joint position from 2D joint position sequences and introduce a nonlinear
optimization method for fitting shape parameters and bone directions. Our
approach can leverage human pose estimation methods, and avoid pose errors
introduced by shape estimation overfitting. We conduct experiments on the
Human3.6M dataset and demonstrate improved performance compared to existing
methods by a large margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_X/0/1/0/all/0/1&quot;&gt;Xiaoyang Hao&lt;/a&gt; (1 and 2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Han Li&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jun Cheng&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lei Wang&lt;/a&gt; (2) ((1) Southern University of Science and Technology, (2) Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10531">
<title>GenPose: Generative Category-level Object Pose Estimation via Diffusion Models. (arXiv:2306.10531v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10531</link>
<description rdf:parseType="Literal">&lt;p&gt;Object pose estimation plays a vital role in embodied AI and computer vision,
enabling intelligent agents to comprehend and interact with their surroundings.
Despite the practicality of category-level pose estimation, current approaches
encounter challenges with partially observed point clouds, known as the
multihypothesis issue. In this study, we propose a novel solution by reframing
categorylevel object pose estimation as conditional generative modeling,
departing from traditional point-to-point regression. Leveraging score-based
diffusion models, we estimate object poses by sampling candidates from the
diffusion model and aggregating them through a two-step process: filtering out
outliers via likelihood estimation and subsequently mean-pooling the remaining
candidates. To avoid the costly integration process when estimating the
likelihood, we introduce an alternative method that trains an energy-based
model from the original score-based model, enabling end-to-end likelihood
estimation. Our approach achieves state-of-the-art performance on the REAL275
dataset, surpassing 50% and 60% on strict 5d2cm and 5d5cm metrics,
respectively. Furthermore, our method demonstrates strong generalizability to
novel categories sharing similar symmetric properties without fine-tuning and
can readily adapt to object pose tracking tasks, yielding comparable results to
the current state-of-the-art baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiyao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Mingdong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hao Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11920">
<title>NILUT: Conditional Neural Implicit 3D Lookup Tables for Image Enhancement. (arXiv:2306.11920v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11920</link>
<description rdf:parseType="Literal">&lt;p&gt;3D lookup tables (3D LUTs) are a key component for image enhancement. Modern
image signal processors (ISPs) have dedicated support for these as part of the
camera rendering pipeline. Cameras typically provide multiple options for
picture styles, where each style is usually obtained by applying a unique
handcrafted 3D LUT. Current approaches for learning and applying 3D LUTs are
notably fast, yet not so memory-efficient, as storing multiple 3D LUTs is
required. For this reason and other implementation limitations, their use on
mobile devices is less popular. In this work, we propose a Neural Implicit LUT
(NILUT), an implicitly defined continuous 3D color transformation parameterized
by a neural network. We show that NILUTs are capable of accurately emulating
real 3D LUTs. Moreover, a NILUT can be extended to incorporate multiple styles
into a single network with the ability to blend styles implicitly. Our novel
approach is memory-efficient, controllable and can complement previous methods,
including learned ISPs. Code, models and dataset available at:
https://github.com/mv-lab/nilut
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Conde_M/0/1/0/all/0/1&quot;&gt;Marcos V. Conde&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vazquez_Corral_J/0/1/0/all/0/1&quot;&gt;Javier Vazquez-Corral&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_M/0/1/0/all/0/1&quot;&gt;Michael S. Brown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1&quot;&gt;Radu Timofte&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.13531">
<title>WBCAtt: A White Blood Cell Dataset Annotated with Detailed Morphological Attributes. (arXiv:2306.13531v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.13531</link>
<description rdf:parseType="Literal">&lt;p&gt;The examination of blood samples at a microscopic level plays a fundamental
role in clinical diagnostics, influencing a wide range of medical conditions.
For instance, an in-depth study of White Blood Cells (WBCs), a crucial
component of our blood, is essential for diagnosing blood-related diseases such
as leukemia and anemia. While multiple datasets containing WBC images have been
proposed, they mostly focus on cell categorization, often lacking the necessary
morphological details to explain such categorizations, despite the importance
of explainable artificial intelligence (XAI) in medical domains. This paper
seeks to address this limitation by introducing comprehensive annotations for
WBC images. Through collaboration with pathologists, a thorough literature
review, and manual inspection of microscopic images, we have identified 11
morphological attributes associated with the cell and its components (nucleus,
cytoplasm, and granules). We then annotated ten thousand WBC images with these
attributes. Moreover, we conduct experiments to predict these attributes from
images, providing insights beyond basic WBC classification. As the first public
dataset to offer such extensive annotations, we also illustrate specific
applications that can benefit from our attribute annotations. Overall, our
dataset paves the way for interpreting WBC recognition models, further
advancing XAI in the fields of pathology and hematology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsutsui_S/0/1/0/all/0/1&quot;&gt;Satoshi Tsutsui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_W/0/1/0/all/0/1&quot;&gt;Winnie Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1&quot;&gt;Bihan Wen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15419">
<title>Freestyle 3D-Aware Portrait Synthesis Based on Compositional Generative Priors. (arXiv:2306.15419v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15419</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficiently generating a freestyle 3D portrait with high quality and
3D-consistency is a promising yet challenging task. The portrait styles
generated by most existing methods are usually restricted by their 3D
generators, which are learned in specific facial datasets, such as FFHQ. To get
the diverse 3D portraits, one can build a large-scale multi-style database to
retrain a 3D-aware generator, or use a off-the-shelf tool to do the style
translation. However, the former is time-consuming due to data collection and
training process, the latter may destroy the multi-view consistency. To tackle
this problem, we propose a novel text-driven 3D-aware portrait synthesis
framework that can generate out-of-distribution portrait styles. Specifically,
for a given portrait style prompt, we first composite two generative priors, a
3D-aware GAN generator and a text-guided image editor, to quickly construct a
few-shot stylized portrait set. Then we map the special style domain of this
set to our proposed 3D latent feature generator and obtain a 3D representation
containing the given style information. Finally we use a pre-trained 3D
renderer to generate view-consistent stylized portraits from the 3D
representation. Extensive experimental results show that our method is capable
of synthesizing high-quality 3D portraits with specified styles in a few
minutes, outperforming the state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1&quot;&gt;Tianxiang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1&quot;&gt;Kang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jianxin Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yingya Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1&quot;&gt;Jing Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01097">
<title>MVDiffusion: Enabling Holistic Multi-view Image Generation with Correspondence-Aware Diffusion. (arXiv:2307.01097v7 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01097</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces MVDiffusion, a simple yet effective method for
generating consistent multi-view images from text prompts given pixel-to-pixel
correspondences (e.g., perspective crops from a panorama or multi-view images
given depth maps and poses). Unlike prior methods that rely on iterative image
warping and inpainting, MVDiffusion simultaneously generates all images with a
global awareness, effectively addressing the prevalent error accumulation
issue. At its core, MVDiffusion processes perspective images in parallel with a
pre-trained text-to-image diffusion model, while integrating novel
correspondence-aware attention layers to facilitate cross-view interactions.
For panorama generation, while only trained with 10k panoramas, MVDiffusion is
able to generate high-resolution photorealistic images for arbitrary texts or
extrapolate one perspective image to a 360-degree view. For multi-view
depth-to-image generation, MVDiffusion demonstrates state-of-the-art
performance for texturing a scene mesh.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Shitao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fuyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiacheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furukawa_Y/0/1/0/all/0/1&quot;&gt;Yasutaka Furukawa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01200">
<title>ProxyCap: Real-time Monocular Full-body Capture in World Space via Human-Centric Proxy-to-Motion Learning. (arXiv:2307.01200v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01200</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning-based approaches to monocular motion capture have recently shown
promising results by learning to regress in a data-driven manner. However, due
to the challenges in data collection and network designs, it remains
challenging for existing solutions to achieve real-time full-body capture while
being accurate in world space. In this work, we introduce ProxyCap, a
human-centric proxy-to-motion learning scheme to learn world-space motions from
a proxy dataset of 2D skeleton sequences and 3D rotational motions. Such proxy
data enables us to build a learning-based network with accurate world-space
supervision while also mitigating the generalization issues. For more accurate
and physically plausible predictions in world space, our network is designed to
learn human motions from a human-centric perspective, which enables the
understanding of the same motion captured with different camera trajectories.
Moreover, a contact-aware neural motion descent module is proposed in our
network so that it can be aware of foot-ground contact and motion misalignment
with the proxy observations. With the proposed learning-based solution, we
demonstrate the first real-time monocular full-body capture system with
plausible foot-ground contact in world space even using hand-held moving
cameras. Our project page is https://zhangyux15.github.io/ProxyCapV2.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuxiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongwen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1&quot;&gt;Liangxiao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiajun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_H/0/1/0/all/0/1&quot;&gt;Hongwei Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shengping Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yebin Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05473">
<title>Differentiable Blocks World: Qualitative 3D Decomposition by Rendering Primitives. (arXiv:2307.05473v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05473</link>
<description rdf:parseType="Literal">&lt;p&gt;Given a set of calibrated images of a scene, we present an approach that
produces a simple, compact, and actionable 3D world representation by means of
3D primitives. While many approaches focus on recovering high-fidelity 3D
scenes, we focus on parsing a scene into mid-level 3D representations made of a
small set of textured primitives. Such representations are interpretable, easy
to manipulate and suited for physics-based simulations. Moreover, unlike
existing primitive decomposition methods that rely on 3D input data, our
approach operates directly on images through differentiable rendering.
Specifically, we model primitives as textured superquadric meshes and optimize
their parameters from scratch with an image rendering loss. We highlight the
importance of modeling transparency for each primitive, which is critical for
optimization and also enables handling varying numbers of primitives. We show
that the resulting textured primitives faithfully reconstruct the input images
and accurately model the visible 3D points, while providing amodal shape
completions of unseen object regions. We compare our approach to the state of
the art on diverse scenes from DTU, and demonstrate its robustness on real-life
captures from BlendedMVS and Nerfstudio. We also showcase how our results can
be used to effortlessly edit a scene or perform physical simulations. Code and
video results are available at https://www.tmonnier.com/DBW .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monnier_T/0/1/0/all/0/1&quot;&gt;Tom Monnier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Austin_J/0/1/0/all/0/1&quot;&gt;Jake Austin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanazawa_A/0/1/0/all/0/1&quot;&gt;Angjoo Kanazawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1&quot;&gt;Alexei A. Efros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aubry_M/0/1/0/all/0/1&quot;&gt;Mathieu Aubry&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07516">
<title>Voting-based Multimodal Automatic Deception Detection. (arXiv:2307.07516v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07516</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic Deception Detection has been a hot research topic for a long time,
using machine learning and deep learning to automatically detect deception,
brings new light to this old field. In this paper, we proposed a voting-based
method for automatic deception detection from videos using audio, visual and
lexical features. Experiments were done on two datasets, the Real-life trial
dataset by Michigan University and the Miami University deception detection
dataset. Video samples were split into frames of images, audio, and
manuscripts. Our Voting-based Multimodal proposed solution consists of three
models. The first model is CNN for detecting deception from images, the second
model is Support Vector Machine (SVM) on Mel spectrograms for detecting
deception from audio and the third model is Word2Vec on Support Vector Machine
(SVM) for detecting deception from manuscripts. Our proposed solution
outperforms state of the art. Best results achieved on images, audio and text
were 97%, 96%, 92% respectively on Real-Life Trial Dataset, and 97%, 82%, 73%
on video, audio and text respectively on Miami University Deception Detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Touma_L/0/1/0/all/0/1&quot;&gt;Lana Touma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horani_M/0/1/0/all/0/1&quot;&gt;Mohammad Al Horani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tailouni_M/0/1/0/all/0/1&quot;&gt;Manar Tailouni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dahabiah_A/0/1/0/all/0/1&quot;&gt;Anas Dahabiah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jallad_K/0/1/0/all/0/1&quot;&gt;Khloud Al Jallad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07635">
<title>CoTracker: It is Better to Track Together. (arXiv:2307.07635v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07635</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce CoTracker, a transformer-based model that tracks dense points in
a frame jointly across a video sequence. This differs from most existing
state-of-the-art approaches that track points independently, ignoring their
correlation. We show that joint tracking results in a significantly higher
tracking accuracy and robustness. We also provide several technical
innovations, including the concept of virtual tracks, which allows CoTracker to
track 70k points jointly and simultaneously. Furthermore, CoTracker operates
causally on short windows (hence, it is suitable for online tasks), but is
trained by unrolling the windows across longer video sequences, which enables
and significantly improves long-term tracking. We demonstrate qualitatively
impressive tracking results, where points can be tracked for a long time even
when they are occluded or leave the field of view. Quantitatively, CoTracker
outperforms all recent trackers on standard benchmarks, often by a substantial
margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karaev_N/0/1/0/all/0/1&quot;&gt;Nikita Karaev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rocco_I/0/1/0/all/0/1&quot;&gt;Ignacio Rocco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Graham_B/0/1/0/all/0/1&quot;&gt;Benjamin Graham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neverova_N/0/1/0/all/0/1&quot;&gt;Natalia Neverova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1&quot;&gt;Andrea Vedaldi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rupprecht_C/0/1/0/all/0/1&quot;&gt;Christian Rupprecht&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07763">
<title>Tightly-Coupled LiDAR-Visual SLAM Based on Geometric Features for Mobile Agents. (arXiv:2307.07763v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07763</link>
<description rdf:parseType="Literal">&lt;p&gt;The mobile robot relies on SLAM (Simultaneous Localization and Mapping) to
provide autonomous navigation and task execution in complex and unknown
environments. However, it is hard to develop a dedicated algorithm for mobile
robots due to dynamic and challenging situations, such as poor lighting
conditions and motion blur. To tackle this issue, we propose a tightly-coupled
LiDAR-visual SLAM based on geometric features, which includes two sub-systems
(LiDAR and monocular visual SLAM) and a fusion framework. The fusion framework
associates the depth and semantics of the multi-modal geometric features to
complement the visual line landmarks and to add direction optimization in
Bundle Adjustment (BA). This further constrains visual odometry. On the other
hand, the entire line segment detected by the visual subsystem overcomes the
limitation of the LiDAR subsystem, which can only perform the local calculation
for geometric features. It adjusts the direction of linear feature points and
filters out outliers, leading to a higher accurate odometry system. Finally, we
employ a module to detect the subsystem&apos;s operation, providing the LiDAR
subsystem&apos;s output as a complementary trajectory to our system while visual
subsystem tracking fails. The evaluation results on the public dataset M2DGR,
gathered from ground robots across various indoor and outdoor scenarios, show
that our system achieves more accurate and robust pose estimation compared to
current state-of-the-art multi-modal methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_K/0/1/0/all/0/1&quot;&gt;Ke Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Ruiping Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ze Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1&quot;&gt;Kunyu Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiaming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Junwei Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teng_Z/0/1/0/all/0/1&quot;&gt;Zhifeng Teng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kailun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1&quot;&gt;Rainer Stiefelhagen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03321">
<title>AFN: Adaptive Fusion Normalization via an Encoder-Decoder Framework. (arXiv:2308.03321v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.03321</link>
<description rdf:parseType="Literal">&lt;p&gt;The success of deep learning is inseparable from normalization layers.
Researchers have proposed various normalization functions, and each of them has
both advantages and disadvantages. In response, efforts have been made to
design a unified normalization function that combines all normalization
procedures and mitigates their weaknesses. We also proposed a new normalization
function called Adaptive Fusion Normalization. Through experiments, we
demonstrate AFN outperforms the previous normalization techniques in domain
generalization and image classification tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zikai Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huanran Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03448">
<title>Make Explicit Calibration Implicit: Calibrate Denoiser Instead of the Noise Model. (arXiv:2308.03448v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.03448</link>
<description rdf:parseType="Literal">&lt;p&gt;Explicit calibration-based methods have dominated RAW image denoising under
extremely low-light environments. However, these methods are impeded by several
critical limitations: a) the explicit calibration process is both labor- and
time-intensive, b) challenge exists in transferring denoisers across different
camera models, and c) the disparity between synthetic and real noise is
exacerbated by digital gain. To address these issues, we introduce a
groundbreaking pipeline named Lighting Every Darkness (LED), which is effective
regardless of the digital gain or the camera sensor. LED eliminates the need
for explicit noise model calibration, instead utilizing an implicit fine-tuning
process that allows quick deployment and requires minimal data. Structural
modifications are also included to reduce the discrepancy between synthetic and
real noise without extra computational demands. Our method surpasses existing
methods in various camera models, including new ones not in public datasets,
with just a few pairs per digital gain and only 0.5% of the typical iterations.
Furthermore, LED also allows researchers to focus more on deep learning
advancements while still utilizing sensor engineering benefits. Code and
related materials can be found in https://srameo.github.io/projects/led-iccv23/ .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1&quot;&gt;Xin Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1&quot;&gt;Jia-Wen Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1&quot;&gt;Ling-Hao Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1&quot;&gt;Chunle Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xialei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chongyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1&quot;&gt;Ming-Ming Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05695">
<title>Masked Diffusion as Self-supervised Representation Learner. (arXiv:2308.05695v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.05695</link>
<description rdf:parseType="Literal">&lt;p&gt;Denoising diffusion probabilistic models have recently demonstrated
state-of-the-art generative performance and have been used as strong
pixel-level representation learners. This paper decomposes the interrelation
between the generative capability and representation learning ability inherent
in diffusion models. We present the masked diffusion model (MDM), a scalable
self-supervised representation learner for semantic segmentation, substituting
the conventional additive Gaussian noise of traditional diffusion with a
masking mechanism. Our proposed approach convincingly surpasses prior
benchmarks, demonstrating remarkable advancements in both medical and natural
image semantic segmentation tasks, particularly in few-shot scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1&quot;&gt;Zixuan Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jianxu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yiyu Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.06595">
<title>VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use. (arXiv:2308.06595v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.06595</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark for
evaluation of instruction-following vision-language models for real-world use.
Our starting point is curating 70 &apos;instruction families&apos; that we envision
instruction tuned vision-language models should be able to address. Extending
beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to
game playing and creative generation. Following curation, our dataset comprises
592 test queries, each with a human-authored instruction-conditioned caption.
These descriptions surface instruction-specific factors, e.g., for an
instruction asking about the accessibility of a storefront for wheelchair
users, the instruction-conditioned caption describes ramps/potential obstacles.
These descriptions enable 1) collecting human-verified reference outputs for
each instance; and 2) automatic evaluation of candidate multimodal generations
using a text-only LLM, aligning with human judgment. We quantify quality gaps
between models and references using both human and automatic evaluations; e.g.,
the top-performing instruction-following model wins against the GPT-4 reference
in just 27% of the comparison. VisIT-Bench is dynamic to participate,
practitioners simply submit their model&apos;s response on the project website;
Data, code and leaderboard is available at visit-bench.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1&quot;&gt;Yonatan Bitton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_H/0/1/0/all/0/1&quot;&gt;Hritik Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1&quot;&gt;Jack Hessel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1&quot;&gt;Rulin Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wanrong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awadalla_A/0/1/0/all/0/1&quot;&gt;Anas Awadalla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gardner_J/0/1/0/all/0/1&quot;&gt;Josh Gardner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taori_R/0/1/0/all/0/1&quot;&gt;Rohan Taori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1&quot;&gt;Ludwig Schmidt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.07948">
<title>Leveraging Symmetries in Pick and Place. (arXiv:2308.07948v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2308.07948</link>
<description rdf:parseType="Literal">&lt;p&gt;Robotic pick and place tasks are symmetric under translations and rotations
of both the object to be picked and the desired place pose. For example, if the
pick object is rotated or translated, then the optimal pick action should also
rotate or translate. The same is true for the place pose; if the desired place
pose changes, then the place action should also transform accordingly. A
recently proposed pick and place framework known as Transporter Net captures
some of these symmetries, but not all. This paper analytically studies the
symmetries present in planar robotic pick and place and proposes a method of
incorporating equivariant neural models into Transporter Net in a way that
captures all symmetries. The new model, which we call Equivariant Transporter
Net, is equivariant to both pick and place symmetries and can immediately
generalize pick and place knowledge to different pick and place poses. We
evaluate the new model empirically and show that it is much more sample
efficient than the non-symmetric version, resulting in a system that can
imitate demonstrated pick and place behavior using very few human
demonstrations on a variety of imitation learning tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Haojie Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tangri_A/0/1/0/all/0/1&quot;&gt;Arsh Tangri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walters_R/0/1/0/all/0/1&quot;&gt;Robin Walters&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Platt_R/0/1/0/all/0/1&quot;&gt;Robert Platt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.09891">
<title>SwinLSTM:Improving Spatiotemporal Prediction Accuracy using Swin Transformer and LSTM. (arXiv:2308.09891v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.09891</link>
<description rdf:parseType="Literal">&lt;p&gt;Integrating CNNs and RNNs to capture spatiotemporal dependencies is a
prevalent strategy for spatiotemporal prediction tasks. However, the property
of CNNs to learn local spatial information decreases their efficiency in
capturing spatiotemporal dependencies, thereby limiting their prediction
accuracy. In this paper, we propose a new recurrent cell, SwinLSTM, which
integrates Swin Transformer blocks and the simplified LSTM, an extension that
replaces the convolutional structure in ConvLSTM with the self-attention
mechanism. Furthermore, we construct a network with SwinLSTM cell as the core
for spatiotemporal prediction. Without using unique tricks, SwinLSTM
outperforms state-of-the-art methods on Moving MNIST, Human3.6m, TaxiBJ, and
KTH datasets. In particular, it exhibits a significant improvement in
prediction accuracy compared to ConvLSTM. Our competitive experimental results
demonstrate that learning global spatial dependencies is more advantageous for
models to capture spatiotemporal dependencies. We hope that SwinLSTM can serve
as a solid baseline to promote the advancement of spatiotemporal prediction
accuracy. The codes are publicly available at
https://github.com/SongTang-x/SwinLSTM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Song Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chuang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Pu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1&quot;&gt;RongNian Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.10531">
<title>SRFormer: Text Detection Transformer with Incorporated Segmentation and Regression. (arXiv:2308.10531v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.10531</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing techniques for text detection can be broadly classified into two
primary groups: segmentation-based and regression-based methods. Segmentation
models offer enhanced robustness to font variations but require intricate
post-processing, leading to high computational overhead. Regression-based
methods undertake instance-aware prediction but face limitations in robustness
and data efficiency due to their reliance on high-level representations. In our
academic pursuit, we propose SRFormer, a unified DETR-based model with
amalgamated Segmentation and Regression, aiming at the synergistic harnessing
of the inherent robustness in segmentation representations, along with the
straightforward post-processing of instance-level regression. Our empirical
analysis indicates that favorable segmentation predictions can be obtained at
the initial decoder layers. In light of this, we constrain the incorporation of
segmentation branches to the first few decoder layers and employ progressive
regression refinement in subsequent layers, achieving performance gains while
minimizing computational load from the mask.Furthermore, we propose a
Mask-informed Query Enhancement module. We take the segmentation result as a
natural soft-ROI to pool and extract robust pixel representations, which are
then employed to enhance and diversify instance queries. Extensive
experimentation across multiple benchmarks has yielded compelling findings,
highlighting our method&apos;s exceptional robustness, superior training and data
efficiency, as well as its state-of-the-art performance. Our code is available
at https://github.com/retsuh-bqw/SRFormer-Text-Det.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bu_Q/0/1/0/all/0/1&quot;&gt;Qingwen Bu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Sungrae Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khang_M/0/1/0/all/0/1&quot;&gt;Minsoo Khang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yichuan Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12009">
<title>StofNet: Super-resolution Time of Flight Network. (arXiv:2308.12009v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12009</link>
<description rdf:parseType="Literal">&lt;p&gt;Time of Flight (ToF) is a prevalent depth sensing technology in the fields of
robotics, medical imaging, and non-destructive testing. Yet, ToF sensing faces
challenges from complex ambient conditions making an inverse modelling from the
sparse temporal information intractable. This paper highlights the potential of
modern super-resolution techniques to learn varying surroundings for a reliable
and accurate ToF detection. Unlike existing models, we tailor an architecture
for sub-sample precise semi-global signal localization by combining
super-resolution with an efficient residual contraction block to balance
between fine signal details and large scale contextual information. We
consolidate research on ToF by conducting a benchmark comparison against six
state-of-the-art methods for which we employ two publicly available datasets.
This includes the release of our SToF-Chirp dataset captured by an airborne
ultrasound transducer. Results showcase the superior performance of our
proposed StofNet in terms of precision, reliability and model complexity. Our
code is available at https://github.com/hahnec/stofnet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hahne_C/0/1/0/all/0/1&quot;&gt;Christopher Hahne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayoz_M/0/1/0/all/0/1&quot;&gt;Michel Hayoz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sznitman_R/0/1/0/all/0/1&quot;&gt;Raphael Sznitman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12590">
<title>Self-supervised Learning of Implicit Shape Representation with Dense Correspondence for Deformable Objects. (arXiv:2308.12590v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12590</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning 3D shape representation with dense correspondence for deformable
objects is a fundamental problem in computer vision. Existing approaches often
need additional annotations of specific semantic domain, e.g., skeleton poses
for human bodies or animals, which require extra annotation effort and suffer
from error accumulation, and they are limited to specific domain. In this
paper, we propose a novel self-supervised approach to learn neural implicit
shape representation for deformable objects, which can represent shapes with a
template shape and dense correspondence in 3D. Our method does not require the
priors of skeleton and skinning weight, and only requires a collection of
shapes represented in signed distance fields. To handle the large deformation,
we constrain the learned template shape in the same latent space with the
training shapes, design a new formulation of local rigid constraint that
enforces rigid transformation in local region and addresses local reflection
issue, and present a new hierarchical rigid constraint to reduce the ambiguity
due to the joint learning of template shape and correspondences. Extensive
experiments show that our model can represent shapes with large deformations.
We also show that our shape representation can support two typical
applications, such as texture transfer and shape editing, with competitive
performance. The code and models are available at
https://iscas3dv.github.io/deformshape
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Baowen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiahe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1&quot;&gt;Xiaoming Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yinda Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Cuixia Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12960">
<title>Towards Realistic Zero-Shot Classification via Self Structural Semantic Alignment. (arXiv:2308.12960v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12960</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale pre-trained Vision Language Models (VLMs) have proven effective
for zero-shot classification. Despite the success, most traditional VLMs-based
methods are restricted by the assumption of partial source supervision or ideal
vocabularies, which rarely satisfy the open-world scenario. In this paper, we
aim at a more challenging setting, Realistic Zero-Shot Classification, which
assumes no annotation but instead a broad vocabulary. To address this
challenge, we propose the Self Structural Semantic Alignment (S^3A) framework,
which extracts the structural semantic information from unlabeled data while
simultaneously self-learning. Our S^3A framework adopts a unique
Cluster-Vote-Prompt-Realign (CVPR) algorithm, which iteratively groups
unlabeled data to derive structural semantics for pseudo-supervision. Our CVPR
process includes iterative clustering on images, voting within each cluster to
identify initial class candidates from the vocabulary, generating
discriminative prompts with large language models to discern confusing
candidates, and realigning images and the vocabulary as structural semantic
alignment. Finally, we propose to self-learn the CLIP image encoder with both
individual and structural semantic alignment through a teacher-student learning
strategy. Our comprehensive experiments across various generic and fine-grained
benchmarks demonstrate that the S^3A method offers substantial improvements
over existing VLMs-based approaches, achieving a more than 15% accuracy
improvement over CLIP on average. Our codes, models, and prompts are publicly
released at https://github.com/sheng-eatamath/S3A.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Sheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1&quot;&gt;Muzammal Naseer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guangyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Salman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1&quot;&gt;Fahad Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.13150">
<title>Enhancing Breast Cancer Histopathology Image Classification Using Dual-Activated Lightweight Attention ResNet50 Model. (arXiv:2308.13150v5 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.13150</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the remarkable results of deep learning in breast cancer
histopathology image classification, challenges such as data imbalance and
interpretability still exist and require cross-domain knowledge and
collaboration among medical experts. This study proposes a breast cancer
classification method using a dual-activated lightweight attention ResNet50
model, effectively addressing data imbalance and interpretability challenges.
The model fuses a pre-trained deep ResNet50 and a lightweight attention
mechanism to accomplish classification by embedding an attention module in
layer 4 of ResNet50 and adding two fully connected layers. The fully connected
network design employs LeakyReLU and ReLU activation functions.
&lt;/p&gt;
&lt;p&gt;The model outperforms SEResNet50, DensNet121, VGG16, VGG16Inception, ViT,
Swin- Transformer, Dinov2_Vitb14, and ResNet50 models regarding precision,
accuracy, recall, F1 score, and GMean, especially in the application
performance on the BreakHis dataset. In particular, the model demonstrates
significant robustness and broad applicability when dealing with the unbalanced
breast cancer dataset. The model has been evaluated on histopathology images at
magnification factors of 40X, 100X, 200X, and 400X, achieving accuracies of
98.5%, 98.7%, 97.9%, and 94.3%, respectively. The study comprehensively
assessed the model&apos;s performance. In the later stages of training, the
validated losses and accuracies change minimally, showing that the model avoids
overfitting and exhibits good generalization ability. This model exhibited the
fastest convergence in all laboratory experiments, even though its parameters
are not the smallest. This highlights the model&apos;s efficacy as a lightweight
attention framework, showcasing its efficiency in achieving rapid convergence
without compromising performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Suxing Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16182">
<title>GREC: Generalized Referring Expression Comprehension. (arXiv:2308.16182v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.16182</link>
<description rdf:parseType="Literal">&lt;p&gt;The objective of Classic Referring Expression Comprehension (REC) is to
produce a bounding box corresponding to the object mentioned in a given textual
description. Commonly, existing datasets and techniques in classic REC are
tailored for expressions that pertain to a single target, meaning a sole
expression is linked to one specific object. Expressions that refer to multiple
targets or involve no specific target have not been taken into account. This
constraint hinders the practical applicability of REC. This study introduces a
new benchmark termed as Generalized Referring Expression Comprehension (GREC).
This benchmark extends the classic REC by permitting expressions to describe
any number of target objects. To achieve this goal, we have built the first
large-scale GREC dataset named gRefCOCO. This dataset encompasses a range of
expressions: those referring to multiple targets, expressions with no specific
target, and the single-target expressions. The design of GREC and gRefCOCO
ensures smooth compatibility with classic REC. The proposed gRefCOCO dataset, a
GREC method implementation code, and GREC evaluation code are available at
https://github.com/henghuiding/gRefCOCO.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1&quot;&gt;Shuting He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Henghui Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xudong Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.04780">
<title>Latent Degradation Representation Constraint for Single Image Deraining. (arXiv:2309.04780v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.04780</link>
<description rdf:parseType="Literal">&lt;p&gt;Since rain streaks show a variety of shapes and directions, learning the
degradation representation is extremely challenging for single image deraining.
Existing methods are mainly targeted at designing complicated modules to
implicitly learn latent degradation representation from coupled rainy images.
This way, it is hard to decouple the content-independent degradation
representation due to the lack of explicit constraint, resulting in over- or
under-enhancement problems. To tackle this issue, we propose a novel Latent
Degradation Representation Constraint Network (LDRCNet) that consists of
Direction-Aware Encoder (DAEncoder), UNet Deraining Network, and Multi-Scale
Interaction Block (MSIBlock). Specifically, the DAEncoder is proposed to
adaptively extract latent degradation representation by using the deformable
convolutions to exploit the direction consistency of rain streaks. Next, a
constraint loss is introduced to explicitly constraint the degradation
representation learning during training. Last, we propose an MSIBlock to fuse
with the learned degradation representation and decoder features of the
deraining network for adaptive information interaction, which enables the
deraining network to remove various complicated rainy patterns and reconstruct
image details. Experimental results on synthetic and real datasets demonstrate
that our method achieves new state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yuhong He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1&quot;&gt;Long Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jun Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07640">
<title>Indoor Scene Reconstruction with Fine-Grained Details Using Hybrid Representation and Normal Prior Enhancement. (arXiv:2309.07640v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.07640</link>
<description rdf:parseType="Literal">&lt;p&gt;The reconstruction of indoor scenes from multi-view RGB images is challenging
due to the coexistence of flat and texture-less regions alongside delicate and
fine-grained regions. Recent methods leverage neural radiance fields aided by
predicted surface normal priors to recover the scene geometry. These methods
excel in producing complete and smooth results for floor and wall areas.
However, they struggle to capture complex surfaces with high-frequency
structures due to the inadequate neural representation and the inaccurately
predicted normal priors. This work aims to reconstruct high-fidelity surfaces
with fine-grained details by addressing the above limitations. To improve the
capacity of the implicit representation, we propose a hybrid architecture to
represent low-frequency and high-frequency regions separately. To enhance the
normal priors, we introduce a simple yet effective image sharpening and
denoising technique, coupled with a network that estimates the pixel-wise
uncertainty of the predicted surface normal vectors. Identifying such
uncertainty can prevent our model from being misled by unreliable surface
normal supervisions that hinder the accurate reconstruction of intricate
geometries. Experiments on the benchmark datasets show that our method
outperforms existing methods in terms of reconstruction quality. Furthermore,
the proposed method also generalizes well to real-world indoor scenarios
captured by our hand-held mobile phones. Our code is publicly available at:
https://github.com/yec22/Fine-Grained-Indoor-Recon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1&quot;&gt;Sheng Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yubin Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1&quot;&gt;Matthieu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yu-Hui Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong-Jin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenping Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14745">
<title>SSPFusion: A Semantic Structure-Preserving Approach for Infrared and Visible Image Fusion. (arXiv:2309.14745v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14745</link>
<description rdf:parseType="Literal">&lt;p&gt;Most existing learning-based infrared and visible image fusion (IVIF) methods
exhibit massive redundant information in the fusion images, i.e., yielding
edge-blurring effect or unrecognizable for object detectors. To alleviate these
issues, we propose a semantic structure-preserving approach for IVIF, namely
SSPFusion. At first, we design a Structural Feature Extractor (SFE) to extract
the structural features of infrared and visible images. Then, we introduce a
multi-scale Structure-Preserving Fusion (SPF) module to fuse the structural
features of infrared and visible images, while maintaining the consistency of
semantic structures between the fusion and source images. Owing to these two
effective modules, our method is able to generate high-quality fusion images
from pairs of infrared and visible images, which can boost the performance of
downstream computer-vision tasks. Experimental results on three benchmarks
demonstrate that our method outperforms eight state-of-the-art image fusion
methods in terms of both qualitative and quantitative evaluations. The code for
our method, along with additional comparison results, will be made available
at: https://github.com/QiaoYang-CV/SSPFUSION.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qiao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zijing Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shunli Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinqiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junzhe Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16924">
<title>Incremental Rotation Averaging Revisited. (arXiv:2309.16924v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.16924</link>
<description rdf:parseType="Literal">&lt;p&gt;In order to further advance the accuracy and robustness of the incremental
parameter estimation-based rotation averaging methods, in this paper, a new
member of the Incremental Rotation Averaging (IRA) family is introduced, which
is termed as IRAv4. As its most significant feature, a task-specific connected
dominating set is extracted in IRAv4 to serve as a more reliable and accurate
reference for rotation local-to-global alignment. This alignment reference is
incrementally constructed, together with the absolute rotations of the vertices
belong to it simultaneously estimated. Comprehensive evaluations are performed
on the 1DSfM dataset, by which the effectiveness of both the reference
construction method and the entire rotation averaging pipeline proposed in this
paper is demonstrated.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xiang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1&quot;&gt;Hainan Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yangdong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1&quot;&gt;Shuhan Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01393">
<title>DST-Det: Simple Dynamic Self-Training for Open-Vocabulary Object Detection. (arXiv:2310.01393v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01393</link>
<description rdf:parseType="Literal">&lt;p&gt;Open-vocabulary object detection (OVOD) aims to detect the objects beyond the
set of classes observed during training. This work presents a simple yet
effective strategy that leverages the zero-shot classification ability of
pre-trained vision-language models (VLM), such as CLIP, to directly discover
proposals of possible novel classes. Unlike previous works that ignore novel
classes during training and rely solely on the region proposal network (RPN)
for novel object detection, our method selectively filters proposals based on
specific design criteria. The resulting sets of identified proposals serve as
pseudo-labels of potential novel classes during the training phase. This
self-training strategy improves the recall and accuracy of novel classes
without requiring additional annotations or datasets. We further propose a
simple offline pseudo-label generation strategy to refine the object detector.
Empirical evaluations on three datasets, including LVIS, V3Det, and COCO,
demonstrate significant improvements over the baseline performance without
incurring additional parameters or computational costs during inference. In
particular, compared with previous F-VLM, our method achieves a 1.7\%
improvement on the LVIS dataset. We also achieve over 6.5\% improvement on the
recent challenging V3Det dataset. When combined with the recent method
CLIPSelf, our method also achieves 46.7 novel class AP on COCO without
introducing extra data for pertaining.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shilin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiangtai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Size Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yining Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1&quot;&gt;Guangliang Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1&quot;&gt;Yunhai Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1&quot;&gt;Chen Change Loy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01545">
<title>RF-ULM: Deep Learning for Radio-Frequency Ultrasound Localization Microscopy. (arXiv:2310.01545v2 [cs.CG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01545</link>
<description rdf:parseType="Literal">&lt;p&gt;In Ultrasound Localization Microscopy (ULM),achieving high-resolution images
relies on the precise localization of contrast agent particles across
consecutive beam-formed frames. However, our study uncovers an enormous
potential: The process of delay-and-sum beamforming leads to an irreversible
reduction of Radio-Frequency (RF) data, while its implications for localization
remain largely unexplored. The rich contextual information embedded within RF
wavefronts, including their hyperbolic shape and phase, offers great promise
for guiding Deep Neural Networks (DNNs) in challenging localization scenarios.
To fully exploit this data, we propose to directly localize scatterers in RF
signals. Our approach involves a custom super-resolution DNN using learned
feature channel shuffling and a novel semi-global convolutional sampling block
tailored for reliable and accurate wavefront localization. Additionally, we
introduce a geometric point transformation that facilitates seamless mapping
between RF and B-mode coordinate space. To understand the impact of beamforming
on ULM, we validate the effectiveness of our method by conducting an extensive
comparison with State-Of-The-Art (SOTA) techniques. We present the inaugural in
vivo results from an RF-trained DNN, highlighting its real-world practicality.
Our findings show that RF-ULM bridges the domain gap between synthetic and real
datasets, offering a considerable advantage in terms of precision and
complexity. To enable the broader research community to benefit from our
findings, our code and the associated SOTA methods are made available at
https://github.com/hahnec/rf-ulm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hahne_C/0/1/0/all/0/1&quot;&gt;Christopher Hahne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chabouh_G/0/1/0/all/0/1&quot;&gt;Georges Chabouh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chavignon_A/0/1/0/all/0/1&quot;&gt;Arthur Chavignon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Couture_O/0/1/0/all/0/1&quot;&gt;Olivier Couture&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sznitman_R/0/1/0/all/0/1&quot;&gt;Raphael Sznitman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08475">
<title>Can We Edit Multimodal Large Language Models?. (arXiv:2310.08475v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08475</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we focus on editing Multimodal Large Language Models (MLLMs).
Compared to editing single-modal LLMs, multimodal model editing is more
challenging, which demands a higher level of scrutiny and careful consideration
in the editing process. To facilitate research in this area, we construct a new
benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite
of innovative metrics for evaluation. We conduct comprehensive experiments
involving various model editing baselines and analyze the impact of editing
different components for multimodal LLMs. Empirically, we notice that previous
baselines can implement editing multimodal LLMs to some extent, but the effect
is still barely satisfactory, indicating the potential difficulty of this task.
We hope that our work can provide the NLP community with insights. Code and
dataset are available in https://github.com/zjunlp/EasyEdit.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;Siyuan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1&quot;&gt;Bozhong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qingbin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yongheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huajun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Ningyu Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.09739">
<title>AugUndo: Scaling Up Augmentations for Unsupervised Depth Completion. (arXiv:2310.09739v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.09739</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised depth completion methods are trained by minimizing sparse depth
and image reconstruction error. Block artifacts from resampling, intensity
saturation, and occlusions are amongst the many undesirable by-products of
common data augmentation schemes that affect image reconstruction quality, and
thus the training signal. Hence, typical augmentations on images viewed as
essential to training pipelines in other vision tasks have seen limited use
beyond small image intensity changes and flipping. The sparse depth modality
have seen even less as intensity transformations alter the scale of the 3D
scene, and geometric transformations may decimate the sparse points during
resampling. We propose a method that unlocks a wide range of
previously-infeasible geometric augmentations for unsupervised depth
completion. This is achieved by reversing, or ``undo&quot;-ing, geometric
transformations to the coordinates of the output depth, warping the depth map
back to the original reference frame. This enables computing the reconstruction
losses using the original images and sparse depth maps, eliminating the
pitfalls of naive loss computation on the augmented inputs. This simple yet
effective strategy allows us to scale up augmentations to boost performance. We
demonstrate our method on indoor (VOID) and outdoor (KITTI) datasets where we
improve upon three existing methods by an average of 11.75% across both
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yangchao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tian Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1&quot;&gt;Hyoungseob Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1&quot;&gt;Stefano Soatto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lao_D/0/1/0/all/0/1&quot;&gt;Dong Lao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1&quot;&gt;Alex Wong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.11346">
<title>Towards Generalizable Multi-Camera 3D Object Detection via Perspective Debiasing. (arXiv:2310.11346v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.11346</link>
<description rdf:parseType="Literal">&lt;p&gt;Detecting objects in 3D space using multiple cameras, known as Multi-Camera
3D Object Detection (MC3D-Det), has gained prominence with the advent of
bird&apos;s-eye view (BEV) approaches. However, these methods often struggle when
faced with unfamiliar testing environments due to the lack of diverse training
data encompassing various viewpoints and environments. To address this, we
propose a novel method that aligns 3D detection with 2D camera plane results,
ensuring consistent and accurate detections. Our framework, anchored in
perspective debiasing, helps the learning of features resilient to domain
shifts. In our approach, we render diverse view maps from BEV features and
rectify the perspective bias of these maps, leveraging implicit foreground
volumes to bridge the camera and BEV planes. This two-step process promotes the
learning of perspective- and context-independent features, crucial for accurate
object detection across varying viewpoints, camera parameters, and
environmental conditions. Notably, our model-agnostic approach preserves the
original network structure without incurring additional inference costs,
facilitating seamless integration across various models and simplifying
deployment. Furthermore, we also show our approach achieves satisfactory
results in real data when trained only with virtual datasets, eliminating the
need for real scene annotations. Experimental results on both Domain
Generalization (DG) and Unsupervised Domain Adaptation (UDA) clearly
demonstrate its effectiveness. The codes are available at
https://github.com/EnVision-Research/Generalizable-BEV.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Hao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunpeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lian_Q/0/1/0/all/0/1&quot;&gt;Qing Lian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_D/0/1/0/all/0/1&quot;&gt;Dalong Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yingcong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.14958">
<title>Learning Real-World Image De-Weathering with Imperfect Supervision. (arXiv:2310.14958v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.14958</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-world image de-weathering aims at removing various undesirable
weather-related artifacts. Owing to the impossibility of capturing image pairs
concurrently, existing real-world de-weathering datasets often exhibit
inconsistent illumination, position, and textures between the ground-truth
images and the input degraded images, resulting in imperfect supervision. Such
non-ideal supervision negatively affects the training process of learning-based
de-weathering methods. In this work, we attempt to address the problem with a
unified solution for various inconsistencies. Specifically, inspired by
information bottleneck theory, we first develop a Consistent Label Constructor
(CLC) to generate a pseudo-label as consistent as possible with the input
degraded image while removing most weather-related degradations. In particular,
multiple adjacent frames of the current input are also fed into CLC to enhance
the pseudo-label. Then we combine the original imperfect labels and
pseudo-labels to jointly supervise the de-weathering model by the proposed
Information Allocation Strategy (IAS). During testing, only the de-weathering
model is used for inference. Experiments on two real-world de-weathering
datasets show that our method helps existing de-weathering models achieve
better performance. Codes are available at
https://github.&lt;a href=&quot;/abs/com/1180300&quot;&gt;com/1180300&lt;/a&gt;419/imperfect-deweathering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaohui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhilu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaohe Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1&quot;&gt;Chaoyu Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaotao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_L/0/1/0/all/0/1&quot;&gt;Lei Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1&quot;&gt;Wangmeng Zuo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16979">
<title>Unsupervised Domain Adaptation for Semantic Segmentation with Pseudo Label Self-Refinement. (arXiv:2310.16979v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.16979</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning-based solutions for semantic segmentation suffer from
significant performance degradation when tested on data with different
characteristics than what was used during the training. Adapting the models
using annotated data from the new domain is not always practical. Unsupervised
Domain Adaptation (UDA) approaches are crucial in deploying these models in the
actual operating conditions. Recent state-of-the-art (SOTA) UDA methods employ
a teacher-student self-training approach, where a teacher model is used to
generate pseudo-labels for the new data which in turn guide the training
process of the student model. Though this approach has seen a lot of success,
it suffers from the issue of noisy pseudo-labels being propagated in the
training process. To address this issue, we propose an auxiliary pseudo-label
refinement network (PRN) for online refining of the pseudo labels and also
localizing the pixels whose predicted labels are likely to be noisy. Being able
to improve the quality of pseudo labels and select highly reliable ones, PRN
helps self-training of segmentation models to be robust against pseudo label
noise propagation during different stages of adaptation. We evaluate our
approach on benchmark datasets with three different domain shifts, and our
approach consistently performs significantly better than the previous
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xingchen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mithun_N/0/1/0/all/0/1&quot;&gt;Niluthpol Chowdhury Mithun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajvanshi_A/0/1/0/all/0/1&quot;&gt;Abhinav Rajvanshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiu_H/0/1/0/all/0/1&quot;&gt;Han-Pang Chiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samarasekera_S/0/1/0/all/0/1&quot;&gt;Supun Samarasekera&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18652">
<title>EHRXQA: A Multi-Modal Question Answering Dataset for Electronic Health Records with Chest X-ray Images. (arXiv:2310.18652v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18652</link>
<description rdf:parseType="Literal">&lt;p&gt;Electronic Health Records (EHRs), which contain patients&apos; medical histories
in various multi-modal formats, often overlook the potential for joint
reasoning across imaging and table modalities underexplored in current EHR
Question Answering (QA) systems. In this paper, we introduce EHRXQA, a novel
multi-modal question answering dataset combining structured EHRs and chest
X-ray images. To develop our dataset, we first construct two uni-modal
resources: 1) The MIMIC-CXR-VQA dataset, our newly created medical visual
question answering (VQA) benchmark, specifically designed to augment the
imaging modality in EHR QA, and 2) EHRSQL (MIMIC-IV), a refashioned version of
a previously established table-based EHR QA dataset. By integrating these two
uni-modal resources, we successfully construct a multi-modal EHR QA dataset
that necessitates both uni-modal and cross-modal reasoning. To address the
unique challenges of multi-modal questions within EHRs, we propose a
NeuralSQL-based strategy equipped with an external VQA API. This pioneering
endeavor enhances engagement with multi-modal EHR sources and we believe that
our dataset can catalyze advances in real-world medical scenarios such as
clinical decision-making and research. EHRXQA is available at
https://github.com/baeseongsu/ehrxqa.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1&quot;&gt;Seongsu Bae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kyung_D/0/1/0/all/0/1&quot;&gt;Daeun Kyung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryu_J/0/1/0/all/0/1&quot;&gt;Jaehee Ryu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_E/0/1/0/all/0/1&quot;&gt;Eunbyeol Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1&quot;&gt;Gyubok Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kweon_S/0/1/0/all/0/1&quot;&gt;Sunjun Kweon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1&quot;&gt;Jungwoo Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1&quot;&gt;Lei Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1&quot;&gt;Eric I-Chao Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Tackeun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1&quot;&gt;Edward Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01212">
<title>Multi-level Relation Learning for Cross-domain Few-shot Hyperspectral Image Classification. (arXiv:2311.01212v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01212</link>
<description rdf:parseType="Literal">&lt;p&gt;Cross-domain few-shot hyperspectral image classification focuses on learning
prior knowledge from a large number of labeled samples from source domains and
then transferring the knowledge to the tasks which contain few labeled samples
in target domains. Following the metric-based manner, many current methods
first extract the features of the query and support samples, and then directly
predict the classes of query samples according to their distance to the support
samples or prototypes. The relations between samples have not been fully
explored and utilized. Different from current works, this paper proposes to
learn sample relations on different levels and take them into the model
learning process, to improve the cross-domain few-shot hyperspectral image
classification. Building on current method of &quot;Deep Cross-Domain Few-Shot
Learning for Hyperspectral Image Classification&quot; which adopts a domain
discriminator to deal with domain-level distribution difference, the proposed
method applies contrastive learning to learn the class-level sample relations
to obtain more discriminable sample features. In addition, it adopts a
transformer based cross-attention learning module to learn the set-level sample
relations and acquire the attention from query samples to support samples. Our
experimental results have demonstrated the contribution of the multi-level
relation learning mechanism for few-shot hyperspectral image classification
when compared with the state of the art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Longwei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1&quot;&gt;Zhigang Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jianzhong Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Junyong Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01813">
<title>FETV: A Benchmark for Fine-Grained Evaluation of Open-Domain Text-to-Video Generation. (arXiv:2311.01813v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01813</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, open-domain text-to-video (T2V) generation models have made
remarkable progress. However, the promising results are mainly shown by the
qualitative cases of generated videos, while the quantitative evaluation of T2V
models still faces two critical problems. Firstly, existing studies lack
fine-grained evaluation of T2V models on different categories of text prompts.
Although some benchmarks have categorized the prompts, their categorization
either only focuses on a single aspect or fails to consider the temporal
information in video generation. Secondly, it is unclear whether the automatic
evaluation metrics are consistent with human standards. To address these
problems, we propose FETV, a benchmark for Fine-grained Evaluation of
Text-to-Video generation. FETV is multi-aspect, categorizing the prompts based
on three orthogonal aspects: the major content, the attributes to control and
the prompt complexity. FETV is also temporal-aware, which introduces several
temporal categories tailored for video generation. Based on FETV, we conduct
comprehensive manual evaluations of four representative T2V models, revealing
their pros and cons on different categories of prompts from different aspects.
We also extend FETV as a testbed to evaluate the reliability of automatic T2V
metrics. The multi-aspect categorization of FETV enables fine-grained analysis
of the metrics&apos; reliability in different scenarios. We find that existing
automatic metrics (e.g., CLIPScore and FVD) correlate poorly with human
evaluation. To address this problem, we explore several solutions to improve
CLIPScore and FVD, and develop two automatic metrics that exhibit significant
higher correlation with humans than existing metrics. Benchmark page:
https://github.com/llyx97/FETV.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuanxin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1&quot;&gt;Shuhuai Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1&quot;&gt;Rundong Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shicheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Sishuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1&quot;&gt;Lu Hou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02240">
<title>Towards Machine Unlearning Benchmarks: Forgetting the Personal Identities in Facial Recognition Systems. (arXiv:2311.02240v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02240</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine unlearning is a crucial tool for enabling a classification model to
forget specific data that are used in the training time. Recently, various
studies have presented machine unlearning algorithms and evaluated their
methods on several datasets. However, most of the current machine unlearning
algorithms have been evaluated solely on traditional computer vision datasets
such as CIFAR-10, MNIST, and SVHN. Furthermore, previous studies generally
evaluate the unlearning methods in the class-unlearning setup. Most previous
work first trains the classification models and then evaluates the machine
unlearning performance of machine unlearning algorithms by forgetting selected
image classes (categories) in the experiments. Unfortunately, these
class-unlearning settings might not generalize to real-world scenarios. In this
work, we propose a machine unlearning setting that aims to unlearn specific
instance that contains personal privacy (identity) while maintaining the
original task of a given model. Specifically, we propose two machine unlearning
benchmark datasets, MUFAC and MUCAC, that are greatly useful to evaluate the
performance and robustness of a machine unlearning algorithm. In our benchmark
datasets, the original model performs facial feature recognition tasks: face
age estimation (multi-class classification) and facial attribute classification
(binary class classification), where a class does not depend on any single
target subject (personal identity), which can be a realistic setting. Moreover,
we also report the performance of the state-of-the-art machine unlearning
methods on our proposed benchmark datasets. All the datasets, source codes, and
trained models are publicly available at
https://github.com/ndb796/MachineUnlearning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1&quot;&gt;Dasol Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Na_D/0/1/0/all/0/1&quot;&gt;Dongbin Na&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05197">
<title>Deep Learning in Computed Tomography Pulmonary Angiography Imaging: A Dual-Pronged Approach for Pulmonary Embolism Detection. (arXiv:2311.05197v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.05197</link>
<description rdf:parseType="Literal">&lt;p&gt;The increasing reliance on Computed Tomography Pulmonary Angiography for
Pulmonary Embolism (PE) diagnosis presents challenges and a pressing need for
improved diagnostic solutions. The primary objective of this study is to
leverage deep learning techniques to enhance the Computer Assisted Diagnosis of
PE. With this aim, we propose a classifier-guided detection approach that
effectively leverages the classifier&apos;s probabilistic inference to direct the
detection predictions, marking a novel contribution in the domain of automated
PE diagnosis. Our end-to-end classification framework introduces an
Attention-Guided Convolutional Neural Network (AG-CNN) that leverages local
context by utilizing an attention mechanism. This approach emulates a human
expert&apos;s attention by looking at both global appearances and local lesion
regions before forming a conclusive decision. The classifier demonstrates
strong performance on the FUMPE dataset, achieving AUROC, sensitivity,
specificity, and F1-score of 0.927, 0.862, 0.879, and 0.805 respectively with
Inception-v3 backbone architecture. Moreover, AG-CNN outperforms the baseline
DenseNet-121 model, achieving an 8.1% AUROC gain. While prior studies have
primarily focused on PE detection in main arteries, our utilization of
cutting-edge object detection models and ensembling techniques greatly improves
the accuracy of finding small embolisms in the peripheral arteries. Finally,
our proposed classifier-guided detection approach further refines the detection
metrics contributing new state-of-the-art to the community: mAP$_{50}$,
sensitivity and F1-score of 0.846, 0.901 and 0.779 respectively outperforming
the former benchmark with a significant 3.7% improvement in mAP$_{50}$. Our
research aims to elevate PE patient care by integrating AI solutions into
clinical workflows, highlighting the potential of human-AI collaboration in
medical diagnostics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bushra_F/0/1/0/all/0/1&quot;&gt;Fabiha Bushra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1&quot;&gt;Muhammad E. H. Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarmun_R/0/1/0/all/0/1&quot;&gt;Rusab Sarmun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kabir_S/0/1/0/all/0/1&quot;&gt;Saidul Kabir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Said_M/0/1/0/all/0/1&quot;&gt;Menatalla Said&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zoghoul_S/0/1/0/all/0/1&quot;&gt;Sohaib Bassam Zoghoul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mushtak_A/0/1/0/all/0/1&quot;&gt;Adam Mushtak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Hashimi_I/0/1/0/all/0/1&quot;&gt;Israa Al-Hashimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alqahtani_A/0/1/0/all/0/1&quot;&gt;Abdulrahman Alqahtani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1&quot;&gt;Anwarul Hasan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05276">
<title>SAMVG: A Multi-stage Image Vectorization Model with the Segment-Anything Model. (arXiv:2311.05276v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.05276</link>
<description rdf:parseType="Literal">&lt;p&gt;Vector graphics are widely used in graphical designs and have received more
and more attention. However, unlike raster images which can be easily obtained,
acquiring high-quality vector graphics, typically through automatically
converting from raster images remains a significant challenge, especially for
more complex images such as photos or artworks. In this paper, we propose
SAMVG, a multi-stage model to vectorize raster images into SVG (Scalable Vector
Graphics). Firstly, SAMVG uses general image segmentation provided by the
Segment-Anything Model and uses a novel filtering method to identify the best
dense segmentation map for the entire image. Secondly, SAMVG then identifies
missing components and adds more detailed components to the SVG. Through a
series of extensive experiments, we demonstrate that SAMVG can produce high
quality SVGs in any domain while requiring less computation time and complexity
compared to previous state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Haokun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chong_J/0/1/0/all/0/1&quot;&gt;Juang Ian Chong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1&quot;&gt;Teng Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_R/0/1/0/all/0/1&quot;&gt;Ran Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1&quot;&gt;Yu-Kun Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosin_P/0/1/0/all/0/1&quot;&gt;Paul L. Rosin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07125">
<title>Attention-Challenging Multiple Instance Learning for Whole Slide Image Classification. (arXiv:2311.07125v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.07125</link>
<description rdf:parseType="Literal">&lt;p&gt;Overfitting is a significant challenge in the application of Multiple
Instance Learning (MIL) methods for Whole Slide Image (WSI) analysis.
Visualizing attention heatmaps reveals that current MIL methods focus on a
subset of discriminative instances, hindering effective model generalization.
To tackle this, we propose Attention-Challenging MIL (ACMIL), aimed at forcing
the attention mechanism to focus on more challenging instances. ACMIL
incorporates two techniques, Multiple Branch Attention (MBA) to capture more
discriminative instances and Stochastic Top-K Instance Masking (STKIM) to
suppress top-k salient instances. Evaluation on three WSI datasets with two
pre-trained backbones outperforms state-of-the-art methods. Additionally,
through heatmap visualization and UMAP visualization, this paper
comprehensively illustrates ACMIL&apos;s effectiveness in overcoming the overfitting
challenge. The source code is available at
\url{https://github.com/dazhangyu123/ACMIL}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunlong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Honglin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Sunyi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Chenglu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lin Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07475">
<title>Masked Face Dataset Generation and Masked Face Recognition. (arXiv:2311.07475v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.07475</link>
<description rdf:parseType="Literal">&lt;p&gt;In the post-pandemic era, wearing face masks has posed great challenge to the
ordinary face recognition. In the previous study, researchers has applied
pretrained VGG16, and ResNet50 to extract features on the elaborate curated
existing masked face recognition (MFR) datasets, RMFRD and SMFRD. To make the
model more adaptable to the real world situation where the sample size is
smaller and the camera environment has greater changes, we created a more
challenging masked face dataset ourselves, by selecting 50 identities with 1702
images from Labelled Faces in the Wild (LFW) Dataset, and simulated face masks
through key point detection. The another part of our study is to solve the
masked face recognition problem, and we chose models by referring to the former
state of the art results, instead of directly using pretrained models, we fine
tuned the model on our new dataset and use the last linear layer to do the
classification directly. Furthermore, we proposed using data augmentation
strategy to further increase the test accuracy, and fine tuned a new networks
beyond the former study, one of the most SOTA networks, Inception ResNet v1.
The best test accuracy on 50 identity MFR has achieved 95%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1&quot;&gt;Rui Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1&quot;&gt;Xuying Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belhumeur_P/0/1/0/all/0/1&quot;&gt;Peter N. Belhumeur&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11261">
<title>Adversarial Prompt Tuning for Vision-Language Models. (arXiv:2311.11261v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11261</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rapid advancement of multimodal learning, pre-trained
Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable
capacities in bridging the gap between visual and language modalities. However,
these models remain vulnerable to adversarial attacks, particularly in the
image modality, presenting considerable security risks. This paper introduces
Adversarial Prompt Tuning (AdvPT), a novel technique to enhance the adversarial
robustness of image encoders in VLMs. AdvPT innovatively leverages learnable
text prompts and aligns them with adversarial image embeddings, to address the
vulnerabilities inherent in VLMs without the need for extensive parameter
training or modification of the model architecture. We demonstrate that AdvPT
improves resistance against white-box and black-box adversarial attacks and
exhibits a synergistic effect when combined with existing
image-processing-based defense techniques, further boosting defensive
capabilities. Comprehensive experimental analyses provide insights into
adversarial prompt tuning, a novel paradigm devoted to improving resistance to
adversarial images through textual input modifications, paving the way for
future robust multimodal learning research. These findings open up new
possibilities for enhancing the security of VLMs. Our code is available at
https://github.com/jiamingzhang94/Adversarial-Prompt-Tuning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiaming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xingjun Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1&quot;&gt;Lingyu Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiaqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yu-Gang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sang_J/0/1/0/all/0/1&quot;&gt;Jitao Sang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16552">
<title>HandyPriors: Physically Consistent Perception of Hand-Object Interactions with Differentiable Priors. (arXiv:2311.16552v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16552</link>
<description rdf:parseType="Literal">&lt;p&gt;Various heuristic objectives for modeling hand-object interaction have been
proposed in past work. However, due to the lack of a cohesive framework, these
objectives often possess a narrow scope of applicability and are limited by
their efficiency or accuracy. In this paper, we propose HandyPriors, a unified
and general pipeline for pose estimation in human-object interaction scenes by
leveraging recent advances in differentiable physics and rendering. Our
approach employs rendering priors to align with input images and segmentation
masks along with physics priors to mitigate penetration and relative-sliding
across frames. Furthermore, we present two alternatives for hand and object
pose estimation. The optimization-based pose estimation achieves higher
accuracy, while the filtering-based tracking, which utilizes the differentiable
priors as dynamics and observation models, executes faster. We demonstrate that
HandyPriors attains comparable or superior results in the pose estimation task,
and that the differentiable physics module can predict contact information for
pose refinement. We also show that our approach generalizes to perception
tasks, including robotic hand manipulation and human-object pose estimation in
the wild.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shutong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yi-Ling Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1&quot;&gt;Guanglei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heiden_E/0/1/0/all/0/1&quot;&gt;Eric Heiden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turpin_D/0/1/0/all/0/1&quot;&gt;Dylan Turpin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jingzhou Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1&quot;&gt;Ming Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Macklin_M/0/1/0/all/0/1&quot;&gt;Miles Macklin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1&quot;&gt;Animesh Garg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16918">
<title>RichDreamer: A Generalizable Normal-Depth Diffusion Model for Detail Richness in Text-to-3D. (arXiv:2311.16918v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16918</link>
<description rdf:parseType="Literal">&lt;p&gt;Lifting 2D diffusion for 3D generation is a challenging problem due to the
lack of geometric prior and the complex entanglement of materials and lighting
in natural images. Existing methods have shown promise by first creating the
geometry through score-distillation sampling (SDS) applied to rendered surface
normals, followed by appearance modeling. However, relying on a 2D RGB
diffusion model to optimize surface normals is suboptimal due to the
distribution discrepancy between natural images and normals maps, leading to
instability in optimization. In this paper, recognizing that the normal and
depth information effectively describe scene geometry and be automatically
estimated from images, we propose to learn a generalizable Normal-Depth
diffusion model for 3D generation. We achieve this by training on the
large-scale LAION dataset together with the generalizable image-to-depth and
normal prior models. In an attempt to alleviate the mixed illumination effects
in the generated materials, we introduce an albedo diffusion model to impose
data-driven constraints on the albedo component. Our experiments show that when
integrated into existing text-to-3D pipelines, our models significantly enhance
the detail richness, achieving state-of-the-art results. Our project page is
https://aigc3d.github.io/richdreamer/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1&quot;&gt;Lingteng Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guanying Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1&quot;&gt;Xiaodong Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_Q/0/1/0/all/0/1&quot;&gt;Qi Zuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Mutian Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yushuang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1&quot;&gt;Weihao Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1&quot;&gt;Zilong Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1&quot;&gt;Liefeng Bo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xiaoguang Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17280">
<title>Does VLN Pretraining Work with Nonsensical or Irrelevant Instructions?. (arXiv:2311.17280v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17280</link>
<description rdf:parseType="Literal">&lt;p&gt;Data augmentation via back-translation is common when pretraining
Vision-and-Language Navigation (VLN) models, even though the generated
instructions are noisy. But: does that noise matter? We find that nonsensical
or irrelevant language instructions during pretraining can have little effect
on downstream performance for both HAMT and VLN-BERT on R2R, and is still
better than only using clean, human data. To underscore these results, we
concoct an efficient augmentation method, Unigram + Object, which generates
nonsensical instructions that nonetheless improve downstream performance. Our
findings suggest that what matters for VLN R2R pretraining is the quantity of
visual trajectories, not the quality of instructions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_I/0/1/0/all/0/1&quot;&gt;Ishika Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1&quot;&gt;Robin Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1&quot;&gt;Jesse Thomason&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17366">
<title>Generative Hierarchical Temporal Transformer for Hand Action Recognition and Motion Prediction. (arXiv:2311.17366v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17366</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel framework that concurrently tackles hand action
recognition and 3D future hand motion prediction. While previous works focus on
either recognition or prediction, we propose a generative Transformer VAE
architecture to jointly capture both aspects, facilitating realistic motion
prediction by leveraging the short-term hand motion and long-term action
consistency observed across timestamps. To ensure faithful representation of
the semantic dependency and different temporal granularity of hand pose and
action, our framework is decomposed into two cascaded VAE blocks. The lower
pose block models short-span poses, while the upper action block models
long-span action. These are connected by a mid-level feature that represents
sub-second series of hand poses. Our framework is trained across multiple
datasets, where pose and action blocks are trained separately to fully utilize
pose-action annotations of different qualities. Evaluations show that on
multiple datasets, the joint modeling of recognition and prediction improves
over separate solutions, and the semantic and temporal hierarchy enables
long-term pose and action modeling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yilin Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1&quot;&gt;Hao Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ohkawa_T/0/1/0/all/0/1&quot;&gt;Takehiko Ohkawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Jia Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1&quot;&gt;Yoichi Sato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Komura_T/0/1/0/all/0/1&quot;&gt;Taku Komura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenping Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17842">
<title>Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning. (arXiv:2311.17842v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17842</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we are interested in imbuing robots with the capability of
physically-grounded task planning. Recent advancements have shown that large
language models (LLMs) possess extensive knowledge useful in robotic tasks,
especially in reasoning and planning. However, LLMs are constrained by their
lack of world grounding and dependence on external affordance models to
perceive environmental information, which cannot jointly reason with LLMs. We
argue that a task planner should be an inherently grounded, unified multimodal
system. To this end, we introduce Robotic Vision-Language Planning (ViLa), a
novel approach for long-horizon robotic planning that leverages vision-language
models (VLMs) to generate a sequence of actionable steps. ViLa directly
integrates perceptual data into its reasoning and planning process, enabling a
profound understanding of commonsense knowledge in the visual world, including
spatial layouts and object attributes. It also supports flexible multimodal
goal specification and naturally incorporates visual feedback. Our extensive
evaluation, conducted in both real-robot and simulated environments,
demonstrates ViLa&apos;s superiority over existing LLM-based planners, highlighting
its effectiveness in a wide array of open-world manipulation tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yingdong Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1&quot;&gt;Fanqi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1&quot;&gt;Li Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yang Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00343">
<title>OpenStereo: A Comprehensive Benchmark for Stereo Matching and Strong Baseline. (arXiv:2312.00343v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.00343</link>
<description rdf:parseType="Literal">&lt;p&gt;Stereo matching, a pivotal technique in computer vision, plays a crucial role
in robotics, autonomous navigation, and augmented reality. Despite the
development of numerous impressive methods in recent years, replicating their
results and determining the most suitable architecture for practical
application remains challenging. Addressing this gap, our paper introduces a
comprehensive benchmark focusing on practical applicability rather than solely
on performance enhancement. Specifically, we develop a flexible and efficient
stereo matching codebase, called OpenStereo. OpenStereo includes training and
inference codes of more than 12 network models, making it, to our knowledge,
the most complete stereo matching toolbox available. Based on OpenStereo, we
conducted experiments on the SceneFlow dataset and have achieved or surpassed
the performance metrics reported in the original paper. Additionally, we
conduct an in-depth revisitation of recent developments in stereo matching
through ablative experiments. These investigations inspired the creation of
StereoBase, a simple yet strong baseline model. Our extensive comparative
analyses of StereoBase against numerous contemporary stereo matching methods on
the SceneFlow dataset demonstrate its remarkably strong performance. The source
code is available at https://github.com/XiandaGuo/OpenStereo.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xianda Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Juntao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chenming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yiqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1&quot;&gt;Yiqun Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Tian Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Long Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02751">
<title>C-NERF: Representing Scene Changes as Directional Consistency Difference-based NeRF. (arXiv:2312.02751v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02751</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we aim to detect the changes caused by object variations in a
scene represented by the neural radiance fields (NeRFs). Given an arbitrary
view and two sets of scene images captured at different timestamps, we can
predict the scene changes in that view, which has significant potential
applications in scene monitoring and measuring. We conducted preliminary
studies and found that such an exciting task cannot be easily achieved by
utilizing existing NeRFs and 2D change detection methods with many false or
missing detections. The main reason is that the 2D change detection is based on
the pixel appearance difference between spatial-aligned image pairs and
neglects the stereo information in the NeRF. To address the limitations, we
propose the C-NERF to represent scene changes as directional consistency
difference-based NeRF, which mainly contains three modules. We first perform
the spatial alignment of two NeRFs captured before and after changes. Then, we
identify the change points based on the direction-consistent constraint; that
is, real change points have similar change representations across view
directions, but fake change points do not. Finally, we design the change map
rendering process based on the built NeRFs and can generate the change map of
an arbitrarily specified view direction. To validate the effectiveness, we
build a new dataset containing ten scenes covering diverse scenarios with
different changing objects. Our approach surpasses state-of-the-art 2D change
detection and NeRF-based methods by a significant margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1&quot;&gt;Rui Huang&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1&quot;&gt;Binbin Jiang&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qingyi Zhao&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;William Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuxiang Zhang&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qing Guo&lt;/a&gt; (2 and 3) ((1) College of Computer Science and Technology, Civil Aviation University of China, China, (2) IHPC, Agency for Science, Technology and Research, Singapore, (3) CFAR, Agency for Science, Technology and Research, Singapore)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03325">
<title>FAGC:Feature Augmentation on Geodesic Curve in the Pre-Shape Space. (arXiv:2312.03325v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.03325</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has yielded remarkable outcomes in various domains. However,
the challenge of requiring large-scale labeled samples still persists in deep
learning. Thus, data augmentation has been introduced as a critical strategy to
train deep learning models. However, data augmentation suffers from information
loss and poor performance in small sample environments. To overcome these
drawbacks, we propose a feature augmentation method based on shape space
theory, i.e., feature augmentation on Geodesic curve, called FAGC in
brevity.First, we extract features from the image with the neural network
model. Then, the multiple image features are projected into a pre-shape space
as features. In the pre-shape space, a Geodesic curve is built to fit the
features. Finally, the many generated features on the Geodesic curve are used
to train the various machine learning models. The FAGC module can be seamlessly
integrated with most machine learning methods. And the proposed method is
simple, effective and insensitive for the small sample datasets.Several
examples demonstrate that the FAGC method can greatly improve the performance
of the data preprocessing model in a small sample environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yuexing Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_G/0/1/0/all/0/1&quot;&gt;Guanxin Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bing Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06199">
<title>Towards Transferable Adversarial Attacks with Centralized Perturbation. (arXiv:2312.06199v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06199</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial transferability enables black-box attacks on unknown victim deep
neural networks (DNNs), rendering attacks viable in real-world scenarios.
Current transferable attacks create adversarial perturbation over the entire
image, resulting in excessive noise that overfit the source model.
Concentrating perturbation to dominant image regions that are model-agnostic is
crucial to improving adversarial efficacy. However, limiting perturbation to
local regions in the spatial domain proves inadequate in augmenting
transferability. To this end, we propose a transferable adversarial attack with
fine-grained perturbation optimization in the frequency domain, creating
centralized perturbation. We devise a systematic pipeline to dynamically
constrain perturbation optimization to dominant frequency coefficients. The
constraint is optimized in parallel at each iteration, ensuring the directional
alignment of perturbation optimization with model prediction. Our approach
allows us to centralize perturbation towards sample-specific important
frequency features, which are shared by DNNs, effectively mitigating source
model overfitting. Experiments demonstrate that by dynamically centralizing
perturbation on dominating frequency coefficients, crafted adversarial examples
exhibit stronger transferability, and allowing them to bypass various defenses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Shangbo Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1&quot;&gt;Yu-an Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yajie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1&quot;&gt;Ruinan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1&quot;&gt;Wencong Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanzhang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07331">
<title>Coupled Confusion Correction: Learning from Crowds with Sparse Annotations. (arXiv:2312.07331v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07331</link>
<description rdf:parseType="Literal">&lt;p&gt;As the size of the datasets getting larger, accurately annotating such
datasets is becoming more impractical due to the expensiveness on both time and
economy. Therefore, crowd-sourcing has been widely adopted to alleviate the
cost of collecting labels, which also inevitably introduces label noise and
eventually degrades the performance of the model. To learn from crowd-sourcing
annotations, modeling the expertise of each annotator is a common but
challenging paradigm, because the annotations collected by crowd-sourcing are
usually highly-sparse. To alleviate this problem, we propose Coupled Confusion
Correction (CCC), where two models are simultaneously trained to correct the
confusion matrices learned by each other. Via bi-level optimization, the
confusion matrices learned by one model can be corrected by the distilled data
from the other. Moreover, we cluster the ``annotator groups&apos;&apos; who share similar
expertise so that their confusion matrices could be corrected together. In this
way, the expertise of the annotators, especially of those who provide seldom
labels, could be better captured. Remarkably, we point out that the annotation
sparsity not only means the average number of labels is low, but also there are
always some annotators who provide very few labels, which is neglected by
previous works when constructing synthetic crowd-sourcing annotations. Based on
that, we propose to use Beta distribution to control the generation of the
crowd-sourcing labels so that the synthetic annotations could be more
consistent with the real-world ones. Extensive experiments are conducted on two
types of synthetic datasets and three real-world datasets, the results of which
demonstrate that CCC significantly outperforms state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hansong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shikun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1&quot;&gt;Dan Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1&quot;&gt;Chenggang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1&quot;&gt;Shiming Ge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07353">
<title>CLIP in Medical Imaging: A Comprehensive Survey. (arXiv:2312.07353v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07353</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive Language-Image Pre-training (CLIP), a simple yet effective
pre-training paradigm, successfully introduces text supervision to vision
models. It has shown promising results across various tasks, attributable to
its generalizability and interpretability. The use of CLIP has recently gained
increasing interest in the medical imaging domain, serving both as a
pre-training paradigm for aligning medical vision and language, and as a
critical component in diverse clinical tasks. With the aim of facilitating a
deeper understanding of this promising direction, this survey offers an
in-depth exploration of the CLIP paradigm within the domain of medical imaging,
regarding both refined CLIP pre-training and CLIP-driven applications. In this
study, We (1) start with a brief introduction to the fundamentals of CLIP
methodology. (2) Then, we investigate the adaptation of CLIP pre-training in
the medical domain, focusing on how to optimize CLIP given characteristics of
medical images and reports. (3) Furthermore, we explore the practical
utilization of CLIP pre-trained models in various tasks, including
classification, dense prediction, and cross-modal tasks. (4) Finally, we
discuss existing limitations of CLIP in the context of medical imaging and
propose forward-looking directions to address the demands of medical imaging
domain. We expect that this comprehensive survey will provide researchers in
the field of medical image analysis with a holistic understanding of the CLIP
paradigm and its potential implications. The project page can be found on
https://github.com/zhaozh10/Awesome-CLIP-in-Medical-Imaging.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zihao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuxiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Han Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yonghao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teng_L/0/1/0/all/0/1&quot;&gt;Lin Teng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Disheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1&quot;&gt;Zhiming Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1&quot;&gt;Dinggang Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07823">
<title>Semantic Lens: Instance-Centric Semantic Alignment for Video Super-Resolution. (arXiv:2312.07823v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07823</link>
<description rdf:parseType="Literal">&lt;p&gt;As a critical clue of video super-resolution (VSR), inter-frame alignment
significantly impacts overall performance. However, accurate pixel-level
alignment is a challenging task due to the intricate motion interweaving in the
video. In response to this issue, we introduce a novel paradigm for VSR named
Semantic Lens, predicated on semantic priors drawn from degraded videos.
Specifically, video is modeled as instances, events, and scenes via a Semantic
Extractor. Those semantics assist the Pixel Enhancer in understanding the
recovered contents and generating more realistic visual results. The distilled
global semantics embody the scene information of each frame, while the
instance-specific semantics assemble the spatial-temporal contexts related to
each instance. Furthermore, we devise a Semantics-Powered Attention
Cross-Embedding (SPACE) block to bridge the pixel-level features with semantic
knowledge, composed of a Global Perspective Shifter (GPS) and an
Instance-Specific Semantic Embedding Encoder (ISEE). Concretely, the GPS module
generates pairs of affine transformation parameters for pixel-level feature
modulation conditioned on global semantics. After that, the ISEE module
harnesses the attention mechanism to align the adjacent frames in the
instance-centric semantic space. In addition, we incorporate a simple yet
effective pre-alignment module to alleviate the difficulty of model training.
Extensive experiments demonstrate the superiority of our model over existing
state-of-the-art VSR methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Q/0/1/0/all/0/1&quot;&gt;Qi Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Meiqin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1&quot;&gt;Jian Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_C/0/1/0/all/0/1&quot;&gt;Chao Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08019">
<title>AdapEdit: Spatio-Temporal Guided Adaptive Editing Algorithm for Text-Based Continuity-Sensitive Image Editing. (arXiv:2312.08019v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08019</link>
<description rdf:parseType="Literal">&lt;p&gt;With the great success of text-conditioned diffusion models in creative
text-to-image generation, various text-driven image editing approaches have
attracted the attentions of many researchers. However, previous works mainly
focus on discreteness-sensitive instructions such as adding, removing or
replacing specific objects, background elements or global styles (i.e., hard
editing), while generally ignoring subject-binding but semantically
fine-changing continuity-sensitive instructions such as actions, poses or
adjectives, and so on (i.e., soft editing), which hampers generative AI from
generating user-customized visual contents. To mitigate this predicament, we
propose a spatio-temporal guided adaptive editing algorithm AdapEdit, which
realizes adaptive image editing by introducing a soft-attention strategy to
dynamically vary the guiding degree from the editing conditions to visual
pixels from both temporal and spatial perspectives. Note our approach has a
significant advantage in preserving model priors and does not require model
training, fine-tuning, extra data, or optimization. We present our results over
a wide variety of raw images and editing instructions, demonstrating
competitive performance and showing it significantly outperforms the previous
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_G/0/1/0/all/0/1&quot;&gt;Guoli Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Bowen Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08372">
<title>SAM-guided Graph Cut for 3D Instance Segmentation. (arXiv:2312.08372v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08372</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper addresses the challenge of 3D instance segmentation by
simultaneously leveraging 3D geometric and multi-view image information. Many
previous works have applied deep learning techniques to 3D point clouds for
instance segmentation. However, these methods often failed to generalize to
various types of scenes due to the scarcity and low-diversity of labeled 3D
point cloud data. Some recent works have attempted to lift 2D instance
segmentations to 3D within a bottom-up framework. The inconsistency in 2D
instance segmentations among views can substantially degrade the performance of
3D segmentation. In this work, we introduce a novel 3D-to-2D query framework to
effectively exploit 2D segmentation models for 3D instance segmentation.
Specifically, we pre-segment the scene into several superpoints in 3D,
formulating the task into a graph cut problem. The superpoint graph is
constructed based on 2D segmentation models, where node features are obtained
from multi-view image features and edge weights are computed based on
multi-view segmentation results, enabling the better generalization ability. To
process the graph, we train a graph neural network using pseudo 3D labels from
2D segmentation models. Experimental results on the ScanNet, ScanNet++ and
KITTI-360 datasets demonstrate that our method achieves robust segmentation
performance and can generalize across different types of scenes. Our project
page is available at https://zju3dv.github.io/sam_graph.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1&quot;&gt;Haoyu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;He Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1&quot;&gt;Sida Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yujun Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1&quot;&gt;Ruizhen Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xiaowei Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08874">
<title>Agent Attention: On the Integration of Softmax and Linear Attention. (arXiv:2312.08874v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08874</link>
<description rdf:parseType="Literal">&lt;p&gt;The attention module is the key component in Transformers. While the global
attention mechanism offers high expressiveness, its excessive computational
cost restricts its applicability in various scenarios. In this paper, we
propose a novel attention paradigm, Agent Attention, to strike a favorable
balance between computational efficiency and representation power.
Specifically, the Agent Attention, denoted as a quadruple $(Q, A, K, V)$,
introduces an additional set of agent tokens $A$ into the conventional
attention module. The agent tokens first act as the agent for the query tokens
$Q$ to aggregate information from $K$ and $V$, and then broadcast the
information back to $Q$. Given the number of agent tokens can be designed to be
much smaller than the number of query tokens, the agent attention is
significantly more efficient than the widely adopted Softmax attention, while
preserving global context modelling capability. Interestingly, we show that the
proposed agent attention is equivalent to a generalized form of linear
attention. Therefore, agent attention seamlessly integrates the powerful
Softmax attention and the highly efficient linear attention. Extensive
experiments demonstrate the effectiveness of agent attention with various
vision Transformers and across diverse vision tasks, including image
classification, object detection, semantic segmentation and image generation.
Notably, agent attention has shown remarkable performance in high-resolution
scenarios, owning to its linear attention nature. For instance, when applied to
Stable Diffusion, our agent attention accelerates generation and substantially
enhances image generation quality without any additional training. Code is
available at https://github.com/LeapLabTHU/Agent-Attention.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1&quot;&gt;Dongchen Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1&quot;&gt;Tianzhu Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yizeng Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Z/0/1/0/all/0/1&quot;&gt;Zhuofan Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Shiji Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1&quot;&gt;Gao Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08889">
<title>SEEAvatar: Photorealistic Text-to-3D Avatar Generation with Constrained Geometry and Appearance. (arXiv:2312.08889v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08889</link>
<description rdf:parseType="Literal">&lt;p&gt;Powered by large-scale text-to-image generation models, text-to-3D avatar
generation has made promising progress. However, most methods fail to produce
photorealistic results, limited by imprecise geometry and low-quality
appearance. Towards more practical avatar generation, we present SEEAvatar, a
method for generating photorealistic 3D avatars from text with SElf-Evolving
constraints for decoupled geometry and appearance. For geometry, we propose to
constrain the optimized avatar in a decent global shape with a template avatar.
The template avatar is initialized with human prior and can be updated by the
optimized avatar periodically as an evolving template, which enables more
flexible shape generation. Besides, the geometry is also constrained by the
static human prior in local parts like face and hands to maintain the delicate
structures. For appearance generation, we use diffusion model enhanced by
prompt engineering to guide a physically based rendering pipeline to generate
realistic textures. The lightness constraint is applied on the albedo texture
to suppress incorrect lighting effect. Experiments show that our method
outperforms previous methods on both global and local geometry and appearance
quality by a large margin. Since our method can produce high-quality meshes and
textures, such assets can be directly applied in classic graphics pipeline for
realistic rendering under any lighting condition. Project page at:
https://yoxu515.github.io/SEEAvatar/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yuanyou Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zongxin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09245">
<title>DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving. (arXiv:2312.09245v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.09245</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have opened up new possibilities for intelligent
agents, endowing them with human-like thinking and cognitive abilities. In this
work, we delve into the potential of large language models (LLMs) in autonomous
driving (AD). We introduce DriveMLM, an LLM-based AD framework that can perform
close-loop autonomous driving in realistic simulators. To this end, (1) we
bridge the gap between the language decisions and the vehicle control commands
by standardizing the decision states according to the off-the-shelf motion
planning module. (2) We employ a multi-modal LLM (MLLM) to model the behavior
planning module of a module AD system, which uses driving rules, user commands,
and inputs from various sensors (e.g., camera, lidar) as input and makes
driving decisions and provide explanations; This model can plug-and-play in
existing AD systems such as Apollo for close-loop driving. (3) We design an
effective data engine to collect a dataset that includes decision state and
corresponding explanation annotation for model training and evaluation. We
conduct extensive experiments and show that our model achieves 76.1 driving
score on the CARLA Town05 Long, and surpasses the Apollo baseline by 4.7 points
under the same settings, demonstrating the effectiveness of our model. We hope
this work can serve as a baseline for autonomous driving with LLMs. Code and
models shall be released at https://github.com/OpenGVLab/DriveMLM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenhai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1&quot;&gt;Jiangwei Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1&quot;&gt;ChuanYang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_H/0/1/0/all/0/1&quot;&gt;Haoming Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1&quot;&gt;Jianan Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_W/0/1/0/all/0/1&quot;&gt;Wenwen Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yang Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Silei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1&quot;&gt;Hanming Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhiqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1&quot;&gt;Hao Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1&quot;&gt;Lewei Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xizhou Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaogang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1&quot;&gt;Jifeng Dai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09520">
<title>SlowTrack: Increasing the Latency of Camera-based Perception in Autonomous Driving Using Adversarial Examples. (arXiv:2312.09520v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.09520</link>
<description rdf:parseType="Literal">&lt;p&gt;In Autonomous Driving (AD), real-time perception is a critical component
responsible for detecting surrounding objects to ensure safe driving. While
researchers have extensively explored the integrity of AD perception due to its
safety and security implications, the aspect of availability (real-time
performance) or latency has received limited attention. Existing works on
latency-based attack have focused mainly on object detection, i.e., a component
in camera-based AD perception, overlooking the entire camera-based AD
perception, which hinders them to achieve effective system-level effects, such
as vehicle crashes. In this paper, we propose SlowTrack, a novel framework for
generating adversarial attacks to increase the execution time of camera-based
AD perception. We propose a novel two-stage attack strategy along with the
three new loss function designs. Our evaluation is conducted on four popular
camera-based AD perception pipelines, and the results demonstrate that
SlowTrack significantly outperforms existing latency-based attacks while
maintaining comparable imperceptibility levels. Furthermore, we perform the
evaluation on Baidu Apollo, an industry-grade full-stack AD system, and LGSVL,
a production-grade AD simulator, with two scenarios to compare the system-level
effects of SlowTrack and existing attacks. Our evaluation results show that the
system-level effects can be significantly improved, i.e., the vehicle crash
rate of SlowTrack is around 95% on average while existing works only have
around 30%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chen Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Ningfei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qi Alfred Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Chao Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09598">
<title>CLAF: Contrastive Learning with Augmented Features for Imbalanced Semi-Supervised Learning. (arXiv:2312.09598v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.09598</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the advantages of leveraging unlabeled data and learning meaningful
representations, semi-supervised learning and contrastive learning have been
progressively combined to achieve better performances in popular applications
with few labeled data and abundant unlabeled data. One common manner is
assigning pseudo-labels to unlabeled samples and selecting positive and
negative samples from pseudo-labeled samples to apply contrastive learning.
However, the real-world data may be imbalanced, causing pseudo-labels to be
biased toward the majority classes and further undermining the effectiveness of
contrastive learning. To address the challenge, we propose Contrastive Learning
with Augmented Features (CLAF). We design a class-dependent feature
augmentation module to alleviate the scarcity of minority class samples in
contrastive learning. For each pseudo-labeled sample, we select positive and
negative samples from labeled data instead of unlabeled data to compute
contrastive loss. Comprehensive experiments on imbalanced image classification
datasets demonstrate the effectiveness of CLAF in the context of imbalanced
semi-supervised learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_B/0/1/0/all/0/1&quot;&gt;Bowen Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xin-Chun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1&quot;&gt;De-Chuan Zhan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09821">
<title>Fragility, Robustness and Antifragility in Deep Learning. (arXiv:2312.09821v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.09821</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a systematic analysis of deep neural networks (DNNs) based on a
signal processing technique for network parameter removal, in the form of
synaptic filters that identifies the fragility, robustness and antifragility
characteristics of DNN parameters. Our proposed analysis investigates if the
DNN performance is impacted negatively, invariantly, or positively on both
clean and adversarially perturbed test datasets when the DNN undergoes synaptic
filtering. We define three \textit{filtering scores} for quantifying the
fragility, robustness and antifragility characteristics of DNN parameters based
on the performances for (i) clean dataset, (ii) adversarial dataset, and (iii)
the difference in performances of clean and adversarial datasets. We validate
the proposed systematic analysis on ResNet-18, ResNet-50, SqueezeNet-v1.1 and
ShuffleNet V2 x1.0 network architectures for MNIST, CIFAR10 and Tiny ImageNet
datasets. The filtering scores, for a given network architecture, identify
network parameters that are invariant in characteristics across different
datasets over learning epochs. Vice-versa, for a given dataset, the filtering
scores identify the parameters that are invariant in characteristics across
different network architectures. We show that our synaptic filtering method
improves the test accuracy of ResNet and ShuffleNet models on adversarial
datasets when only the robust and antifragile parameters are selectively
retrained at any given epoch, thus demonstrating applications of the proposed
strategy in improving model robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pravin_C/0/1/0/all/0/1&quot;&gt;Chandresh Pravin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martino_I/0/1/0/all/0/1&quot;&gt;Ivan Martino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nicosia_G/0/1/0/all/0/1&quot;&gt;Giuseppe Nicosia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ojha_V/0/1/0/all/0/1&quot;&gt;Varun Ojha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10032">
<title>Osprey: Pixel Understanding with Visual Instruction Tuning. (arXiv:2312.10032v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10032</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal large language models (MLLMs) have recently achieved impressive
general-purpose vision-language capabilities through visual instruction tuning.
However, current MLLMs primarily focus on image-level or box-level
understanding, falling short of achieving fine-grained vision-language
alignment at the pixel level. Besides, the lack of mask-based instruction data
limits their advancements. In this paper, we propose Osprey, a mask-text
instruction tuning approach, to extend MLLMs by incorporating fine-grained mask
regions into language instruction, aiming at achieving pixel-wise visual
understanding. To achieve this goal, we first meticulously curate a mask-based
region-text dataset with 724K samples, and then design a vision-language model
by injecting pixel-level representation into LLM. Especially, Osprey adopts a
convolutional CLIP backbone as the vision encoder and employs a mask-aware
visual extractor to extract precise visual mask features from high resolution
input. Experimental results demonstrate Osprey&apos;s superiority in various region
understanding tasks, showcasing its new capability for pixel-level instruction
tuning. In particular, Osprey can be integrated with Segment Anything Model
(SAM) seamlessly to obtain multi-granularity semantics. The source code,
dataset and demo can be found at https://github.com/CircleRadon/Osprey.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yuqian Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wentong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1&quot;&gt;Dongqi Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xinjie Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1&quot;&gt;Chi Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jianke Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10104">
<title>ICD-LM: Configuring Vision-Language In-Context Demonstrations by Language Modeling. (arXiv:2312.10104v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10104</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies how to configure powerful In-Context Demonstration (ICD)
sequences for a Large Vision-Language Model (LVLM) to solve Vision-Language
tasks through In-Context Learning (ICL). After observing that configuring an
ICD sequence is a mirror process of composing a sentence, i.e., just as a
sentence can be composed word by word via a Language Model, an ICD sequence can
also be configured one by one. Consequently, we introduce an ICD Language Model
(ICD-LM) specifically designed to generate effective ICD sequences. This
involves creating a dataset of hand-crafted ICD sequences for various query
samples and using it to train the ICD-LM. Our approach, diverging from
traditional methods in NLP that select and order ICDs separately, enables to
simultaneously learn how to select and order ICDs, enhancing the effect of the
sequences. Moreover, during data construction, we use the LVLM intended for ICL
implementation to validate the strength of each ICD sequence, resulting in a
model-specific dataset and the ICD-LM trained by this dataset is also
model-specific. We validate our methodology through experiments in Visual
Question Answering and Image Captioning, confirming the viability of using a
Language Model for ICD configuration. Our comprehensive ablation studies
further explore the impact of various dataset construction and ICD-LM
development settings on the outcomes. The code is given in
https://github.com/ForJadeForest/ICD-LM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yingzhe Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Haoxuan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shuo Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yucheng Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hanwang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10407">
<title>DeepArt: A Benchmark to Advance Fidelity Research in AI-Generated Content. (arXiv:2312.10407v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10407</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the image synthesis capabilities of GPT-4, a leading
multi-modal large language model. We establish a benchmark for evaluating the
fidelity of texture features in images generated by GPT-4, comprising manually
painted pictures and their AI-generated counterparts. The contributions of this
study are threefold: First, we provide an in-depth analysis of the fidelity of
image synthesis features based on GPT-4, marking the first such study on this
state-of-the-art model. Second, the quantitative and qualitative experiments
fully reveals the limitations of the GPT-4 model in image synthesis. Third, we
have compiled a unique benchmark of manual drawings and corresponding
GPT-4-generated images, introducing a new task to advance fidelity research in
AI-generated content (AIGC). The dataset is available at:
\url{https://github.com/rickwang28574/DeepArt}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wentao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xuanyao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1&quot;&gt;Swalpa Kumar Roy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10628">
<title>T2M-HiFiGPT: Generating High Quality Human Motion from Textual Descriptions with Residual Discrete Representations. (arXiv:2312.10628v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10628</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we introduce T2M-HiFiGPT, a novel conditional generative
framework for synthesizing human motion from textual descriptions. This
framework is underpinned by a Residual Vector Quantized Variational AutoEncoder
(RVQ-VAE) and a double-tier Generative Pretrained Transformer (GPT)
architecture. We demonstrate that our CNN-based RVQ-VAE is capable of producing
highly accurate 2D temporal-residual discrete motion representations. Our
proposed double-tier GPT structure comprises a temporal GPT and a residual GPT.
The temporal GPT efficiently condenses information from previous frames and
textual descriptions into a 1D context vector. This vector then serves as a
context prompt for the residual GPT, which generates the final residual
discrete indices. These indices are subsequently transformed back into motion
data by the RVQ-VAE decoder. To mitigate the exposure bias issue, we employ
straightforward code corruption techniques for RVQ and a conditional dropout
strategy, resulting in enhanced synthesis performance. Remarkably, T2M-HiFiGPT
not only simplifies the generative process but also surpasses existing methods
in both performance and parameter efficacy, including the latest
diffusion-based and GPT-based models. On the HumanML3D and KIT-ML datasets, our
framework achieves exceptional results across nearly all primary metrics. We
further validate the efficacy of our framework through comprehensive ablation
studies on the HumanML3D dataset, examining the contribution of each component.
Our findings reveal that RVQ-VAE is more adept at capturing precise 3D human
motion with comparable computational demand compared to its VQ-VAE
counterparts. As a result, T2M-HiFiGPT enables the generation of human motion
with significantly increased accuracy, outperforming recent state-of-the-art
approaches such as T2M-GPT and Att-T2M.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Congyi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11562">
<title>A Survey of Reasoning with Foundation Models. (arXiv:2312.11562v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11562</link>
<description rdf:parseType="Literal">&lt;p&gt;Reasoning, a crucial ability for complex problem-solving, plays a pivotal
role in various real-world settings such as negotiation, medical diagnosis, and
criminal investigation. It serves as a fundamental methodology in the field of
Artificial General Intelligence (AGI). With the ongoing development of
foundation models, there is a growing interest in exploring their abilities in
reasoning tasks. In this paper, we introduce seminal foundation models proposed
or adaptable for reasoning, highlighting the latest advancements in various
reasoning tasks, methods, and benchmarks. We then delve into the potential
future directions behind the emergence of reasoning abilities within foundation
models. We also discuss the relevance of multimodal learning, autonomous
agents, and super alignment in the context of reasoning. By discussing these
future research directions, we hope to inspire researchers in their exploration
of this field, stimulate further advancements in reasoning with foundation
models, and contribute to the development of AGI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiankai Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Chuanyang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1&quot;&gt;Enze Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhengying Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_R/0/1/0/all/0/1&quot;&gt;Ruihang Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1&quot;&gt;Jianing Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiaqi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1&quot;&gt;Mingyu Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_M/0/1/0/all/0/1&quot;&gt;Mengzhe Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yue Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenhai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junsong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Zhangyue Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xiaozhe Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Junxian He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1&quot;&gt;Wu Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xihui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yu Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Ming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1&quot;&gt;Pheng Ann Heng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1&quot;&gt;Jifeng Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingdong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Ji-Rong Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1&quot;&gt;Xipeng Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yike Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Hui Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenguo Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12223">
<title>Self-Supervised Detection of Perfect and Partial Input-Dependent Symmetries. (arXiv:2312.12223v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.12223</link>
<description rdf:parseType="Literal">&lt;p&gt;Group equivariance ensures consistent responses to group transformations of
the input, leading to more robust models and enhanced generalization
capabilities. However, this property can lead to overly constrained models if
the symmetries considered in the group differ from those observed in data.
While common methods address this by determining the appropriate level of
symmetry at the dataset level, they are limited to supervised settings and
ignore scenarios in which multiple levels of symmetry co-exist in the same
dataset. For instance, pictures of cars and planes exhibit different levels of
rotation, yet both are included in the CIFAR-10 dataset. In this paper, we
propose a method able to detect the level of symmetry of each input without the
need for labels. To this end, we derive a sufficient and necessary condition to
learn the distribution of symmetries in the data. Using the learned
distribution, we generate pseudo-labels that allow us to learn the levels of
symmetry of each input in a self-supervised manner. We validate the
effectiveness of our approach on synthetic datasets with different per-class
levels of symmetries e.g. MNISTMultiple, in which digits are uniformly rotated
within a class-dependent interval. We demonstrate that our method can be used
for practical applications such as the generation of standardized datasets in
which the symmetries are not present, as well as the detection of
out-of-distribution symmetries during inference. By doing so, both the
generalization and robustness of non-equivariant models can be improved. Our
code is publicly available at https://github.com/aurban0/ssl-sym.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urbano_A/0/1/0/all/0/1&quot;&gt;Alonso Urbano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romero_D/0/1/0/all/0/1&quot;&gt;David W. Romero&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12340">
<title>Scalable Geometric Fracture Assembly via Co-creation Space among Assemblers. (arXiv:2312.12340v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.12340</link>
<description rdf:parseType="Literal">&lt;p&gt;Geometric fracture assembly presents a challenging practical task in
archaeology and 3D computer vision. Previous methods have focused solely on
assembling fragments based on semantic information, which has limited the
quantity of objects that can be effectively assembled. Therefore, there is a
need to develop a scalable framework for geometric fracture assembly without
relying on semantic information. To improve the effectiveness of assembling
geometric fractures without semantic information, we propose a co-creation
space comprising several assemblers capable of gradually and unambiguously
assembling fractures. Additionally, we introduce a novel loss function, i.e.,
the geometric-based collision loss, to address collision issues during the
fracture assembly process and enhance the results. Our framework exhibits
better performance on both PartNet and Breaking Bad datasets compared to
existing state-of-the-art frameworks. Extensive experiments and quantitative
comparisons demonstrate the effectiveness of our proposed framework, which
features linear computational complexity, enhanced abstraction, and improved
generalization. Our code is publicly available at
https://github.com/Ruiyuan-Zhang/CCS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruiyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiaxiang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zexi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chao Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13332">
<title>Ternary-type Opacity and Hybrid Odometry for RGB-only NeRF-SLAM. (arXiv:2312.13332v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.13332</link>
<description rdf:parseType="Literal">&lt;p&gt;The opacity of rigid 3D scenes with opaque surfaces is considered to be of a
binary type. However, we observed that this property is not followed by the
existing RGB-only NeRF-SLAM. Therefore, we are motivated to introduce this
prior into the RGB-only NeRF-SLAM pipeline. Unfortunately, the optimization
through the volumetric rendering function does not facilitate easy integration
of the desired prior. Instead, we observed that the opacity of ternary-type
(TT) is well supported. In this work, we study why ternary-type opacity is
well-suited and desired for the task at hand. In particular, we provide
theoretical insights into the process of jointly optimizing radiance and
opacity through the volumetric rendering process. Through exhaustive
experiments on benchmark datasets, we validate our claim and provide insights
into the optimization process, which we believe will unleash the potential of
RGB-only NeRF-SLAM. To foster this line of research, we also propose a simple
yet novel visual odometry scheme that uses a hybrid combination of volumetric
and warping-based image renderings. More specifically, the proposed hybrid
odometry (HO) additionally uses image warping-based coarse odometry, leading up
to an order of magnitude final speed-up. Furthermore, we show that the proposed
TT and HO well complement each other, offering state-of-the-art results on
benchmark datasets in terms of both speed and accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Junru Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nachkov_A/0/1/0/all/0/1&quot;&gt;Asen Nachkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1&quot;&gt;Songyou Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1&quot;&gt;Luc Van Gool&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1&quot;&gt;Danda Pani Paudel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13555">
<title>CR-SAM: Curvature Regularized Sharpness-Aware Minimization. (arXiv:2312.13555v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.13555</link>
<description rdf:parseType="Literal">&lt;p&gt;The capacity to generalize to future unseen data stands as one of the utmost
crucial attributes of deep neural networks. Sharpness-Aware Minimization (SAM)
aims to enhance the generalizability by minimizing worst-case loss using
one-step gradient ascent as an approximation. However, as training progresses,
the non-linearity of the loss landscape increases, rendering one-step gradient
ascent less effective. On the other hand, multi-step gradient ascent will incur
higher training cost. In this paper, we introduce a normalized Hessian trace to
accurately measure the curvature of loss landscape on {\em both} training and
test sets. In particular, to counter excessive non-linearity of loss landscape,
we propose Curvature Regularized SAM (CR-SAM), integrating the normalized
Hessian trace as a SAM regularizer. Additionally, we present an efficient way
to compute the trace via finite differences with parallelism. Our theoretical
analysis based on PAC-Bayes bounds establishes the regularizer&apos;s efficacy in
reducing generalization error. Empirical evaluation on CIFAR and ImageNet
datasets shows that CR-SAM consistently enhances classification performance for
ResNet and Vision Transformer (ViT) models across various datasets. Our code is
available at https://github.com/TrustAIoT/CR-SAM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1&quot;&gt;Tie Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wunsch_D/0/1/0/all/0/1&quot;&gt;Donald C. Wunsch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14091">
<title>HD-Painter: High-Resolution and Prompt-Faithful Text-Guided Image Inpainting with Diffusion Models. (arXiv:2312.14091v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.14091</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent progress in text-guided image inpainting, based on the unprecedented
success of text-to-image diffusion models, has led to exceptionally realistic
and visually plausible results. However, there is still significant potential
for improvement in current text-to-image inpainting models, particularly in
better aligning the inpainted area with user prompts and performing
high-resolution inpainting. Therefore, in this paper we introduce HD-Painter, a
completely training-free approach that accurately follows to prompts and
coherently scales to high-resolution image inpainting. To this end, we design
the Prompt-Aware Introverted Attention (PAIntA) layer enhancing self-attention
scores by prompt information and resulting in better text alignment
generations. To further improve the prompt coherence we introduce the
Reweighting Attention Score Guidance (RASG) mechanism seamlessly integrating a
post-hoc sampling strategy into general form of DDIM to prevent
out-of-distribution latent shifts. Moreover, HD-Painter allows extension to
larger scales by introducing a specialized super-resolution technique
customized for inpainting, enabling the completion of missing regions in images
of up to 2K resolution. Our experiments demonstrate that HD-Painter surpasses
existing state-of-the-art approaches qualitatively and quantitatively,
achieving an impressive generation accuracy improvement of 61.4% vs 51.9%. We
will make the codes publicly available at:
https://github.com/Picsart-AI-Research/HD-Painter
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manukyan_H/0/1/0/all/0/1&quot;&gt;Hayk Manukyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sargsyan_A/0/1/0/all/0/1&quot;&gt;Andranik Sargsyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atanyan_B/0/1/0/all/0/1&quot;&gt;Barsegh Atanyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navasardyan_S/0/1/0/all/0/1&quot;&gt;Shant Navasardyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Humphrey Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14135">
<title>V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs. (arXiv:2312.14135v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.14135</link>
<description rdf:parseType="Literal">&lt;p&gt;When we look around and perform complex tasks, how we see and selectively
process what we see is crucial. However, the lack of this visual search
mechanism in current multimodal LLMs (MLLMs) hinders their ability to focus on
important visual details, especially when handling high-resolution and visually
crowded images. To address this, we introduce V*, an LLM-guided visual search
mechanism that employs the world knowledge in LLMs for efficient visual
querying. When combined with an MLLM, this mechanism enhances collaborative
reasoning, contextual understanding, and precise targeting of specific visual
elements. This integration results in a new MLLM meta-architecture, named Show,
sEArch, and TelL (SEAL). We further create V*Bench, a benchmark specifically
designed to evaluate MLLMs in their ability to process high-resolution images
and focus on visual details. Our study highlights the necessity of
incorporating visual search capabilities into multimodal systems. The code is
available https://github.com/penghao-wu/vstar.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1&quot;&gt;Penghao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1&quot;&gt;Saining Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14492">
<title>Context Enhanced Transformer for Single Image Object Detection. (arXiv:2312.14492v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.14492</link>
<description rdf:parseType="Literal">&lt;p&gt;With the increasing importance of video data in real-world applications,
there is a rising need for efficient object detection methods that utilize
temporal information. While existing video object detection (VOD) techniques
employ various strategies to address this challenge, they typically depend on
locally adjacent frames or randomly sampled images within a clip. Although
recent Transformer-based VOD methods have shown promising results, their
reliance on multiple inputs and additional network complexity to incorporate
temporal information limits their practical applicability. In this paper, we
propose a novel approach to single image object detection, called Context
Enhanced TRansformer (CETR), by incorporating temporal context into DETR using
a newly designed memory module. To efficiently store temporal information, we
construct a class-wise memory that collects contextual information across data.
Additionally, we present a classification-based sampling technique to
selectively utilize the relevant memory for the current image. In the testing,
We introduce a test-time memory adaptation method that updates individual
memory functions by considering the test distribution. Experiments with CityCam
and ImageNet VID datasets exhibit the efficiency of the framework on various
video systems. The project page and code will be made available at:
https://ku-cvlab.github.io/CETR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1&quot;&gt;Seungjun An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Seonghoon Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1&quot;&gt;Gyeongnyeon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baek_J/0/1/0/all/0/1&quot;&gt;Jeongyeol Baek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1&quot;&gt;Byeongwon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seungryong Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14919">
<title>Lift-Attend-Splat: Bird&apos;s-eye-view camera-lidar fusion using transformers. (arXiv:2312.14919v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.14919</link>
<description rdf:parseType="Literal">&lt;p&gt;Combining complementary sensor modalities is crucial to providing robust
perception for safety-critical robotics applications such as autonomous driving
(AD). Recent state-of-the-art camera-lidar fusion methods for AD rely on
monocular depth estimation which is a notoriously difficult task compared to
using depth information from the lidar directly. Here, we find that this
approach does not leverage depth as expected and show that naively improving
depth estimation does not lead to improvements in object detection performance
and that, strikingly, removing depth estimation altogether does not degrade
object detection performance. This suggests that relying on monocular depth
could be an unnecessary architectural bottleneck during camera-lidar fusion. In
this work, we introduce a novel fusion method that bypasses monocular depth
estimation altogether and instead selects and fuses camera and lidar features
in a bird&apos;s-eye-view grid using a simple attention mechanism. We show that our
model can modulate its use of camera features based on the availability of
lidar features and that it yields better 3D object detection on the nuScenes
dataset than baselines relying on monocular depth estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunn_J/0/1/0/all/0/1&quot;&gt;James Gunn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lenyk_Z/0/1/0/all/0/1&quot;&gt;Zygmunt Lenyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1&quot;&gt;Anuj Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Donati_A/0/1/0/all/0/1&quot;&gt;Andrea Donati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buburuzan_A/0/1/0/all/0/1&quot;&gt;Alexandru Buburuzan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Redford_J/0/1/0/all/0/1&quot;&gt;John Redford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mueller_R/0/1/0/all/0/1&quot;&gt;Romain Mueller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14924">
<title>Training Convolutional Neural Networks with the Forward-Forward algorithm. (arXiv:2312.14924v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.14924</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent successes in analyzing images with deep neural networks are almost
exclusively achieved with Convolutional Neural Networks (CNNs). The training of
these CNNs, and in fact of all deep neural network architectures, uses the
backpropagation algorithm where the output of the network is compared with the
desired result and the difference is then used to tune the weights of the
network towards the desired outcome. In a 2022 preprint, Geoffrey Hinton
suggested an alternative way of training which passes the desired results
together with the images at the input of the network. This so called Forward
Forward (FF) algorithm has up to now only been used in fully connected
networks. In this paper, we show how the FF paradigm can be extended to CNNs.
Our FF-trained CNN, featuring a novel spatially-extended labeling technique,
achieves a classification accuracy of 99.0% on the MNIST hand-written digits
dataset. We show how different hyperparameters affect the performance of the
proposed algorithm and compare the results with CNN trained with the standard
backpropagation approach. Furthermore, we use Class Activation Maps to
investigate which type of features are learnt by the FF algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scodellaro_R/0/1/0/all/0/1&quot;&gt;Riccardo Scodellaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1&quot;&gt;Ajinkya Kulkarni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alves_F/0/1/0/all/0/1&quot;&gt;Frauke Alves&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schroter_M/0/1/0/all/0/1&quot;&gt;Matthias Schr&amp;#xf6;ter&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>