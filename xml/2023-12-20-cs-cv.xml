<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-12-18T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10046" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10069" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10070" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10083" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10088" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10089" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10099" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10103" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10104" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10105" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10108" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10109" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10111" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10112" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10113" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10114" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10115" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10116" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10118" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10120" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10132" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10136" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10144" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10163" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10165" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10175" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10191" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10195" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10200" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10208" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10217" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2106.05152" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.05216" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.09445" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.03005" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.05980" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.06754" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.09141" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.02178" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.04253" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.06663" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.10682" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.11133" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.11174" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.15060" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.10621" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.02761" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.12662" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.00215" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.00355" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.03932" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.06315" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.08815" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.08977" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.11098" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.11611" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.11728" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.12787" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.13269" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.17561" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.17594" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.17743" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.00242" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.01168" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.08796" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.04926" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.06252" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.07490" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10135" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17423" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17770" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.20052" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00800" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04445" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05144" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06963" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08707" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.12153" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15521" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00371" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01003" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01146" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03856" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03998" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05845" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08506" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15421" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02669" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.07728" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.09437" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.09616" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.09936" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.10997" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11875" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12558" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.13177" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.14604" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.00872" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.01574" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.02270" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.05173" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.06023" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.06618" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.06810" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.06933" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.06961" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.09907" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11082" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14950" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16899" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02601" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03059" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05524" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10059" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10912" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.13356" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15646" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17796" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18651" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18890" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19629" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04498" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10522" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11125" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12813" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13444" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15145" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00347" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00377" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01597" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02222" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02246" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04875" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05797" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06198" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06372" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06833" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06988" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07823" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08168" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08343" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08606" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08746" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08916" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08985" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09038" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09783" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09909" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.01402" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.14673" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.07317" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.08870" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09910" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03154" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09196" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2312.10046">
<title>Deep Metric Learning for Computer Vision: A Brief Overview. (arXiv:2312.10046v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.10046</link>
<description rdf:parseType="Literal">&lt;p&gt;Objective functions that optimize deep neural networks play a vital role in
creating an enhanced feature representation of the input data. Although
cross-entropy-based loss formulations have been extensively used in a variety
of supervised deep-learning applications, these methods tend to be less
adequate when there is large intra-class variance and low inter-class variance
in input data distribution. Deep Metric Learning seeks to develop methods that
aim to measure the similarity between data samples by learning a representation
function that maps these data samples into a representative embedding space. It
leverages carefully designed sampling strategies and loss functions that aid in
optimizing the generation of a discriminative embedding space even for
distributions having low inter-class and high intra-class variances. In this
chapter, we will provide an overview of recent progress in this area and
discuss state-of-the-art Deep Metric Learning approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohan_D/0/1/0/all/0/1&quot;&gt;Deen Dayal Mohan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jawade_B/0/1/0/all/0/1&quot;&gt;Bhavin Jawade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Setlur_S/0/1/0/all/0/1&quot;&gt;Srirangaraj Setlur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Govindaraj_V/0/1/0/all/0/1&quot;&gt;Venu Govindaraj&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10069">
<title>Understanding Representations Pretrained with Auxiliary Losses for Embodied Agent Planning. (arXiv:2312.10069v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.10069</link>
<description rdf:parseType="Literal">&lt;p&gt;Pretrained representations from large-scale vision models have boosted the
performance of downstream embodied policy learning. We look to understand
whether additional self-supervised pretraining on exploration trajectories can
build on these general-purpose visual representations to better support
embodied planning in realistic environments. We evaluated four common auxiliary
losses in embodied AI, two hindsight-based losses, and a standard imitation
learning loss, by pretraining the agent&apos;s visual compression module and state
belief representations with each objective and using CLIP as a representative
visual backbone. The learned representations are then frozen for downstream
multi-step evaluation on two goal-directed tasks. Surprisingly, we find that
imitation learning on these exploration trajectories out-performs all other
auxiliary losses even despite the exploration trajectories being dissimilar
from the downstream tasks. This suggests that imitation of exploration may be
&apos;&apos;all you need&apos;&apos; for building powerful planning representations. Additionally,
we find that popular auxiliary losses can benefit from simple modifications to
improve their support for downstream planning ability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weihs_L/0/1/0/all/0/1&quot;&gt;Luca Weihs&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10070">
<title>Gaussian-SLAM: Photo-realistic Dense SLAM with Gaussian Splatting. (arXiv:2312.10070v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.10070</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a new dense simultaneous localization and mapping (SLAM) method
that uses Gaussian splats as a scene representation. The new representation
enables interactive-time reconstruction and photo-realistic rendering of
real-world and synthetic scenes. We propose novel strategies for seeding and
optimizing Gaussian splats to extend their use from multiview offline scenarios
to sequential monocular RGBD input data setups. In addition, we extend Gaussian
splats to encode geometry and experiment with tracking against this scene
representation. Our method achieves state-of-the-art rendering quality on both
real-world and synthetic datasets while being competitive in reconstruction
performance and runtime.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yugay_V/0/1/0/all/0/1&quot;&gt;Vladimir Yugay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yue Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gevers_T/0/1/0/all/0/1&quot;&gt;Theo Gevers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oswald_M/0/1/0/all/0/1&quot;&gt;Martin R. Oswald&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10083">
<title>The Limits of Fair Medical Imaging AI In The Wild. (arXiv:2312.10083v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2312.10083</link>
<description rdf:parseType="Literal">&lt;p&gt;As artificial intelligence (AI) rapidly approaches human-level performance in
medical imaging, it is crucial that it does not exacerbate or propagate
healthcare disparities. Prior research has established AI&apos;s capacity to infer
demographic data from chest X-rays, leading to a key concern: do models using
demographic shortcuts have unfair predictions across subpopulations? In this
study, we conduct a thorough investigation into the extent to which medical AI
utilizes demographic encodings, focusing on potential fairness discrepancies
within both in-distribution training sets and external test sets. Our analysis
covers three key medical imaging disciplines: radiology, dermatology, and
ophthalmology, and incorporates data from six global chest X-ray datasets. We
confirm that medical imaging AI leverages demographic shortcuts in disease
classification. While correcting shortcuts algorithmically effectively
addresses fairness gaps to create &quot;locally optimal&quot; models within the original
data distribution, this optimality is not true in new test settings.
Surprisingly, we find that models with less encoding of demographic attributes
are often most &quot;globally optimal&quot;, exhibiting better fairness during model
evaluation in new test environments. Our work establishes best practices for
medical imaging models which maintain their performance and fairness in
deployments beyond their initial training contexts, underscoring critical
considerations for AI clinical deployments across populations and sites.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuzhe Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haoran Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gichoya_J/0/1/0/all/0/1&quot;&gt;Judy W Gichoya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katabi_D/0/1/0/all/0/1&quot;&gt;Dina Katabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1&quot;&gt;Marzyeh Ghassemi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10088">
<title>On Robustness to Missing Video for Audiovisual Speech Recognition. (arXiv:2312.10088v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2312.10088</link>
<description rdf:parseType="Literal">&lt;p&gt;It has been shown that learning audiovisual features can lead to improved
speech recognition performance over audio-only features, especially for noisy
speech. However, in many common applications, the visual features are partially
or entirely missing, e.g.~the speaker might move off screen. Multi-modal models
need to be robust: missing video frames should not degrade the performance of
an audiovisual model to be worse than that of a single-modality audio-only
model. While there have been many attempts at building robust models, there is
little consensus on how robustness should be evaluated. To address this, we
introduce a framework that allows claims about robustness to be evaluated in a
precise and testable way. We also conduct a systematic empirical study of the
robustness of common audiovisual speech recognition architectures on a range of
acoustic noise conditions and test suites. Finally, we show that an
architecture-agnostic solution based on cascades can consistently achieve
robustness to missing video, even in settings where existing techniques for
robustness like dropout fall short.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chang_O/0/1/0/all/0/1&quot;&gt;Oscar Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Braga_O/0/1/0/all/0/1&quot;&gt;Otavio Braga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liao_H/0/1/0/all/0/1&quot;&gt;Hank Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Serdyuk_D/0/1/0/all/0/1&quot;&gt;Dmitriy Serdyuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Siohan_O/0/1/0/all/0/1&quot;&gt;Olivier Siohan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10089">
<title>Advancements in Content-Based Image Retrieval: A Comprehensive Survey of Relevance Feedback Techniques. (arXiv:2312.10089v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.10089</link>
<description rdf:parseType="Literal">&lt;p&gt;Content-based image retrieval (CBIR) systems have emerged as crucial tools in
the field of computer vision, allowing for image search based on visual content
rather than relying solely on metadata. This survey paper presents a
comprehensive overview of CBIR, emphasizing its role in object detection and
its potential to identify and retrieve visually similar images based on content
features. Challenges faced by CBIR systems, including the semantic gap and
scalability, are discussed, along with potential solutions. It elaborates on
the semantic gap, which arises from the disparity between low-level features
and high-level semantic concepts, and explores approaches to bridge this gap.
One notable solution is the integration of relevance feedback (RF), empowering
users to provide feedback on retrieved images and refine search results
iteratively. The survey encompasses long-term and short-term learning
approaches that leverage RF for enhanced CBIR accuracy and relevance. These
methods focus on weight optimization and the utilization of active learning
algorithms to select samples for training classifiers. Furthermore, the paper
investigates machine learning techniques and the utilization of deep learning
and convolutional neural networks to enhance CBIR performance. This survey
paper plays a significant role in advancing the understanding of CBIR and RF
techniques. It guides researchers and practitioners in comprehending existing
methodologies, challenges, and potential solutions while fostering knowledge
dissemination and identifying research gaps. By addressing future research
directions, it sets the stage for advancements in CBIR that will enhance
retrieval accuracy, usability, and effectiveness in various application
domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qazanfari_H/0/1/0/all/0/1&quot;&gt;Hamed Qazanfari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+AlyanNezhadi_M/0/1/0/all/0/1&quot;&gt;Mohammad M. AlyanNezhadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khoshdaregi_Z/0/1/0/all/0/1&quot;&gt;Zohreh Nozari Khoshdaregi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10099">
<title>ADA-YOLO: Dynamic Fusion of YOLOv8 and Adaptive Heads for Precise Image Detection and Diagnosis. (arXiv:2312.10099v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.10099</link>
<description rdf:parseType="Literal">&lt;p&gt;Object detection and localization are crucial tasks for biomedical image
analysis, particularly in the field of hematology where the detection and
recognition of blood cells are essential for diagnosis and treatment decisions.
While attention-based methods have shown significant progress in object
detection in various domains, their application in medical object detection has
been limited due to the unique challenges posed by medical imaging datasets. To
address this issue, we propose ADA-YOLO, a light-weight yet effective method
for medical object detection that integrates attention-based mechanisms with
the YOLOv8 architecture. Our proposed method leverages the dynamic feature
localisation and parallel regression for computer vision tasks through
\textit{adaptive head} module. Empirical experiments were conducted on the
Blood Cell Count and Detection (BCCD) dataset to evaluate the effectiveness of
ADA-YOLO. The results showed that ADA-YOLO outperforms the YOLOv8 model in mAP
(mean average precision) on the BCCD dataset by using more than 3 times less
space than YOLOv8. This indicates that our proposed method is effective.
Moreover, the light-weight nature of our proposed method makes it suitable for
deployment in resource-constrained environments such as mobile devices or edge
computing systems. which could ultimately lead to improved diagnosis and
treatment outcomes in the field of hematology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1&quot;&gt;Ruocheng Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teoh_T/0/1/0/all/0/1&quot;&gt;Teik Toe Teoh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10103">
<title>GSVA: Generalized Segmentation via Multimodal Large Language Models. (arXiv:2312.10103v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.10103</link>
<description rdf:parseType="Literal">&lt;p&gt;Generalized Referring Expression Segmentation (GRES) extends the scope of
classic RES to referring to multiple objects in one expression or identifying
the empty targets absent in the image. GRES poses challenges in modeling the
complex spatial relationships of the instances in the image and identifying
non-existing referents. Recently, Multimodal Large Language Models (MLLMs) have
shown tremendous progress in these complicated vision-language tasks.
Connecting Large Language Models (LLMs) and vision models, MLLMs are proficient
in understanding contexts with visual inputs. Among them, LISA, as a
representative, adopts a special [SEG] token to prompt a segmentation mask
decoder, e.g., SAM, to enable MLLMs in the RES task. However, existing
solutions to of GRES remain unsatisfactory since current segmentation MLLMs
cannot properly handle the cases where users might reference multiple subjects
in a singular prompt or provide descriptions incongruent with any image target.
In this paper, we propose Generalized Segmentation Vision Assistant (GSVA) to
address this gap. Specifically, GSVA reuses the [SEG] token to prompt the
segmentation model towards supporting multiple mask references simultaneously
and innovatively learns to generate a [REJ] token to reject the null targets
explicitly. Experiments validate GSVA&apos;s efficacy in resolving the GRES issue,
marking a notable enhancement and setting a new record on the GRES benchmark
gRefCOCO dataset. GSVA also proves effective across various classic referring
expression segmentation and comprehension tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Z/0/1/0/all/0/1&quot;&gt;Zhuofan Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1&quot;&gt;Dongchen Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yizeng Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1&quot;&gt;Xuran Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Shiji Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1&quot;&gt;Gao Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10104">
<title>ICD-LM: Configuring Vision-Language In-Context Demonstrations by Language Modeling. (arXiv:2312.10104v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.10104</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies how to configure powerful In-Context Demonstration (ICD)
sequences for a Large Vision-Language Model (LVLM) to solve Vision-Language
tasks through In-Context Learning (ICL). After observing that configuring an
ICD sequence is a mirror process of composing a sentence, i.e., just as a
sentence can be composed word by word via a Language Model, an ICD sequence can
also be configured one by one. Consequently, we introduce an ICD Language Model
(ICD-LM) specifically designed to generate effective ICD sequences. This
involves creating a dataset of hand-crafted ICD sequences for various query
samples and using it to train the ICD-LM. Our approach, diverging from
traditional methods in NLP that select and order ICDs separately, enables to
simultaneously learn how to select and order ICDs, enhancing the effect of the
sequences. Moreover, during data construction, we use the LVLM intended for ICL
implementation to validate the strength of each ICD sequence, resulting in a
model-specific dataset and the ICD-LM trained by this dataset is also
model-specific. We validate our methodology through experiments in Visual
Question Answering and Image Captioning, confirming the viability of using a
Language Model for ICD configuration. Our comprehensive ablation studies
further explore the impact of various dataset construction and ICD-LM
development settings on the outcomes. The code is given in
https://github.com/ForJadeForest/ICD-LM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yingzhe Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Haoxuan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shuo Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yucheng Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hanwang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10105">
<title>Forging Tokens for Improved Storage-efficient Training. (arXiv:2312.10105v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.10105</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in Deep Neural Network (DNN) models have significantly
improved performance across computer vision tasks. However, achieving highly
generalizable and high-performing vision models requires extensive datasets,
leading to large storage requirements. This storage challenge poses a critical
bottleneck for scaling up vision models. Motivated by the success of discrete
representations, SeiT proposes to use Vector-Quantized (VQ) feature vectors
(i.e., tokens) as network inputs for vision classification. However, applying
traditional data augmentations to tokens faces challenges due to input domain
shift. To address this issue, we introduce TokenAdapt and ColorAdapt, simple
yet effective token-based augmentation strategies. TokenAdapt realigns token
embedding space for compatibility with spatial augmentations, preserving the
model&apos;s efficiency without requiring fine-tuning. Additionally, ColorAdapt
addresses color-based augmentations for tokens inspired by Adaptive Instance
Normalization (AdaIN). We evaluate our approach across various scenarios,
including storage-efficient ImageNet-1k classification, fine-grained
classification, robustness benchmarks, and ADE-20k semantic segmentation.
Experimental results demonstrate consistent performance improvement in diverse
experiments. Code is available at https://github.com/naver-ai/tokenadapt.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1&quot;&gt;Minhyun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Song Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heo_B/0/1/0/all/0/1&quot;&gt;Byeongho Heo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1&quot;&gt;Dongyoon Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shim_H/0/1/0/all/0/1&quot;&gt;Hyunjung Shim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10108">
<title>Privacy-Aware Document Visual Question Answering. (arXiv:2312.10108v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.10108</link>
<description rdf:parseType="Literal">&lt;p&gt;Document Visual Question Answering (DocVQA) is a fast growing branch of
document understanding. Despite the fact that documents contain sensitive or
copyrighted information, none of the current DocVQA methods offers strong
privacy guarantees.
&lt;/p&gt;
&lt;p&gt;In this work, we explore privacy in the domain of DocVQA for the first time.
We highlight privacy issues in state of the art multi-modal LLM models used for
DocVQA, and explore possible solutions.
&lt;/p&gt;
&lt;p&gt;Specifically, we focus on the invoice processing use case as a realistic,
widely used scenario for document understanding, and propose a large scale
DocVQA dataset comprising invoice documents and associated questions and
answers. We employ a federated learning scheme, that reflects the real-life
distribution of documents in different businesses, and we explore the use case
where the ID of the invoice issuer is the sensitive information to be
protected.
&lt;/p&gt;
&lt;p&gt;We demonstrate that non-private models tend to memorise, behaviour that can
lead to exposing private information. We then evaluate baseline training
schemes employing federated learning and differential privacy in this
multi-modal scenario, where the sensitive information might be exposed through
any of the two input modalities: vision (document image) or language (OCR
tokens).
&lt;/p&gt;
&lt;p&gt;Finally, we design an attack exploiting the memorisation effect of the model,
and demonstrate its effectiveness in probing different DocVQA models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tito_R/0/1/0/all/0/1&quot;&gt;Rub&amp;#xe8;n Tito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1&quot;&gt;Khanh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tobaben_M/0/1/0/all/0/1&quot;&gt;Marlon Tobaben&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kerkouche_R/0/1/0/all/0/1&quot;&gt;Raouf Kerkouche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Souibgui_M/0/1/0/all/0/1&quot;&gt;Mohamed Ali Souibgui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1&quot;&gt;Kangsoo Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_L/0/1/0/all/0/1&quot;&gt;Lei Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valveny_E/0/1/0/all/0/1&quot;&gt;Ernest Valveny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Honkela_A/0/1/0/all/0/1&quot;&gt;Antti Honkela&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fritz_M/0/1/0/all/0/1&quot;&gt;Mario Fritz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karatzas_D/0/1/0/all/0/1&quot;&gt;Dimosthenis Karatzas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10109">
<title>Enlighten-Your-Voice: When Multimodal Meets Zero-shot Low-light Image Enhancement. (arXiv:2312.10109v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.10109</link>
<description rdf:parseType="Literal">&lt;p&gt;Low-light image enhancement is a crucial visual task, and many unsupervised
methods tend to overlook the degradation of visible information in low-light
scenes, which adversely affects the fusion of complementary information and
hinders the generation of satisfactory results. To address this, our study
introduces ``Enlighten-Your-Voice&apos;&apos;, a multimodal enhancement framework that
innovatively enriches user interaction through voice and textual commands. This
approach does not merely signify a technical leap but also represents a
paradigm shift in user engagement. Our model is equipped with a Dual
Collaborative Attention Module (DCAM) that meticulously caters to distinct
content and color discrepancies, thereby facilitating nuanced enhancements.
Complementarily, we introduce a Semantic Feature Fusion (SFM) plug-and-play
module that synergizes semantic context with low-light enhancement operations,
sharpening the algorithm&apos;s efficacy. Crucially, ``Enlighten-Your-Voice&apos;&apos;
showcases remarkable generalization in unsupervised zero-shot scenarios. The
source code can be accessed from
https://github.com/zhangbaijin/Enlighten-Your-Voice
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zishan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Hao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_C/0/1/0/all/0/1&quot;&gt;Chaochen Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Shanying Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_X/0/1/0/all/0/1&quot;&gt;Xinping Guan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10111">
<title>Plasticine3D: Non-rigid 3D editting with text guidance. (arXiv:2312.10111v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.10111</link>
<description rdf:parseType="Literal">&lt;p&gt;With the help of Score Distillation Sampling(SDS) and the rapid development
of various trainable 3D representations, Text-to-Image(T2I) diffusion models
have been applied to 3D generation tasks and achieved considerable results.
There are also some attempts toward the task of editing 3D objects leveraging
this Text-to-3D pipeline. However, most methods currently focus on adding
additional geometries, overwriting textures or both. But few of them can
perform non-rigid transformation of 3D objects. For those who can perform
non-rigid editing, on the other hand, suffer from low-resolution, lack of
fidelity and poor flexibility. In order to address these issues, we present:
Plasticine3D, a general, high-fidelity, photo-realistic and controllable
non-rigid editing pipeline. Firstly, our work divides the editing process into
a geometry editing stage and a texture editing stage to achieve more detailed
and photo-realistic results ; Secondly, in order to perform non-rigid
transformation with controllable results while maintain the fidelity towards
original 3D models in the same time, we propose a multi-view-embedding(MVE)
optimization strategy to ensure that the diffusion model learns the overall
features of the original object and an embedding-fusion(EF) to control the
degree of editing by adjusting the value of the fusing rate. We also design a
geometry processing step before optimizing on the base geometry to cope with
different needs of various editing tasks. Further more, to fully leverage the
geometric prior from the original 3D object, we provide an optional replacement
of score distillation sampling named score projection sampling(SPS) which
enables us to directly perform optimization from the origin 3D mesh in most
common median non-rigid editing scenarios. We demonstrate the effectiveness of
our method on both the non-rigid 3D editing task and general 3D editing task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yige Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1&quot;&gt;Ang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Siyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_R/0/1/0/all/0/1&quot;&gt;Ran Yi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10112">
<title>NM-FlowGAN: Modeling sRGB Noise with a Hybrid Approach based on Normalizing Flows and Generative Adversarial Networks. (arXiv:2312.10112v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.10112</link>
<description rdf:parseType="Literal">&lt;p&gt;Modeling and synthesizing real sRGB noise is crucial for various low-level
vision tasks. The distribution of real sRGB noise is highly complex and
affected by a multitude of factors, making its accurate modeling extremely
challenging. Therefore, recent studies have proposed methods that employ
data-driven generative models, such as generative adversarial networks (GAN)
and Normalizing Flows. These studies achieve more accurate modeling of sRGB
noise compared to traditional noise modeling methods. However, there are
performance limitations due to the inherent characteristics of each generative
model. To address this issue, we propose NM-FlowGAN, a hybrid approach that
exploits the strengths of both GAN and Normalizing Flows. We simultaneously
employ a pixel-wise noise modeling network based on Normalizing Flows, and
spatial correlation modeling networks based on GAN. In our experiments, our
NM-FlowGAN outperforms other baselines on the sRGB noise synthesis task.
Moreover, the denoising neural network, trained with synthesized image pairs
from our model, also shows superior performance compared to other baselines.
Our code is available at: https://github.com/YoungJooHan/NM-FlowGAN
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Young Joo Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Ha-Jin Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10113">
<title>Focus on Your Instruction: Fine-grained and Multi-instruction Image Editing by Attention Modulation. (arXiv:2312.10113v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.10113</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, diffusion-based methods, like InstructPix2Pix (IP2P), have achieved
effective instruction-based image editing, requiring only natural language
instructions from the user. However, these methods often inadvertently alter
unintended areas and struggle with multi-instruction editing, resulting in
compromised outcomes. To address these issues, we introduce the Focus on Your
Instruction (FoI), a method designed to ensure precise and harmonious editing
across multiple instructions without extra training or test-time optimization.
In the FoI, we primarily emphasize two aspects: (1) precisely extracting
regions of interest for each instruction and (2) guiding the denoising process
to concentrate within these regions of interest. For the first objective, we
identify the implicit grounding capability of IP2P from the cross-attention
between instruction and image, then develop an effective mask extraction
method. For the second objective, we introduce a cross attention modulation
module for rough isolation of target editing regions and unrelated regions.
Additionally, we introduce a mask-guided disentangle sampling strategy to
further ensure clear region isolation. Experimental results demonstrate that
FoI surpasses existing methods in both quantitative and qualitative
evaluations, especially excelling in multi-instruction editing task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qin Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1&quot;&gt;Tianwei Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10114">
<title>FoMo-Bench: a multi-modal, multi-scale and multi-task Forest Monitoring Benchmark for remote sensing foundation models. (arXiv:2312.10114v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.10114</link>
<description rdf:parseType="Literal">&lt;p&gt;Forests are an essential part of Earth&apos;s ecosystems and natural systems, as
well as providing services on which humanity depends, yet they are rapidly
changing as a result of land use decisions and climate change. Understanding
and mitigating negative effects requires parsing data on forests at global
scale from a broad array of sensory modalities, and recently many such problems
have been approached using machine learning algorithms for remote sensing. To
date, forest-monitoring problems have largely been approached in isolation.
Inspired by the rise of foundation models for computer vision and remote
sensing, we here present the first unified Forest Monitoring Benchmark
(FoMo-Bench). FoMo-Bench consists of 15 diverse datasets encompassing
satellite, aerial, and inventory data, covering a variety of geographical
regions, and including multispectral, red-green-blue, synthetic aperture radar
(SAR) and LiDAR data with various temporal, spatial and spectral resolutions.
FoMo-Bench includes multiple types of forest-monitoring tasks, spanning
classification, segmentation, and object detection. To further enhance the
diversity of tasks and geographies represented in FoMo-Bench, we introduce a
novel global dataset, TalloS, combining satellite imagery with ground-based
annotations for tree species classification, spanning 1,000+ hierarchical
taxonomic levels (species, genus, family). Finally, we propose FoMo-Net, a
foundation model baseline designed for forest monitoring with the flexibility
to process any combination of commonly used sensors in remote sensing. This
work aims to inspire research collaborations between machine learning and
forest biology researchers in exploring scalable multi-modal and multi-task
models for forest monitoring. All code and data will be made publicly
available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bountos_N/0/1/0/all/0/1&quot;&gt;Nikolaos Ioannis Bountos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouaknine_A/0/1/0/all/0/1&quot;&gt;Arthur Ouaknine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rolnick_D/0/1/0/all/0/1&quot;&gt;David Rolnick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10115">
<title>SkySense: A Multi-Modal Remote Sensing Foundation Model Towards Universal Interpretation for Earth Observation Imagery. (arXiv:2312.10115v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.10115</link>
<description rdf:parseType="Literal">&lt;p&gt;Prior studies on Remote Sensing Foundation Model (RSFM) reveal immense
potential towards a generic model for Earth Observation. Nevertheless, these
works primarily focus on a single modality without temporal and geo-context
modeling, hampering their capabilities for diverse tasks. In this study, we
present SkySense, a generic billion-scale model, pre-trained on a curated
multi-modal Remote Sensing Imagery (RSI) dataset with 21.5 million temporal
sequences. SkySense incorporates a factorized multi-modal spatiotemporal
encoder taking temporal sequences of optical and Synthetic Aperture Radar (SAR)
data as input. This encoder is pre-trained by our proposed Multi-Granularity
Contrastive Learning to learn representations across different modal and
spatial granularities. To further enhance the RSI representations by the
geo-context clue, we introduce Geo-Context Prototype Learning to learn
region-aware prototypes upon RSI&apos;s multi-modal spatiotemporal features. To our
best knowledge, SkySense is the largest Multi-Modal RSFM to date, whose modules
can be flexibly combined or used individually to accommodate various tasks. It
demonstrates remarkable generalization capabilities on a thorough evaluation
encompassing 16 datasets over 7 tasks, from single- to multi-modal, static to
temporal, and classification to localization. SkySense surpasses 18 recent
RSFMs in all test scenarios. Specifically, it outperforms the latest models
such as GFM, SatLas and Scale-MAE by a large margin, i.e., 2.76%, 3.67% and
3.61% on average respectively. We will release the pre-trained weights to
facilitate future research and Earth Observation applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xin Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lao_J/0/1/0/all/0/1&quot;&gt;Jiangwei Lao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dang_B/0/1/0/all/0/1&quot;&gt;Bo Dang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yingying Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Lei Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ru_L/0/1/0/all/0/1&quot;&gt;Lixiang Ru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1&quot;&gt;Liheng Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Ziyuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1&quot;&gt;Kang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1&quot;&gt;Dingxiang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;Huimei He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jingdong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Ming Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongjun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yansheng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10116">
<title>Bayesian Estimate of Mean Proper Scores for Diversity-Enhanced Active Learning. (arXiv:2312.10116v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.10116</link>
<description rdf:parseType="Literal">&lt;p&gt;The effectiveness of active learning largely depends on the sampling
efficiency of the acquisition function. Expected Loss Reduction (ELR) focuses
on a Bayesian estimate of the reduction in classification error, and more
general costs fit in the same framework. We propose Bayesian Estimate of Mean
Proper Scores (BEMPS) to estimate the increase in strictly proper scores such
as log probability or negative mean square error within this framework. We also
prove convergence results for this general class of costs. To facilitate better
experimentation with the new acquisition functions, we develop a complementary
batch AL algorithm that encourages diversity in the vector of expected changes
in scores for unlabeled data. To allow high-performance classifiers, we combine
deep ensembles, and dynamic validation set construction on pretrained models,
and further speed up the ensemble process with the idea of Monte Carlo Dropout.
Extensive experiments on both texts and images show that the use of mean square
error and log probability with BEMPS yields robust acquisition functions and
well-calibrated classifiers, and consistently outperforms the others tested.
The advantages of BEMPS over the others are further supported by a set of
qualitative analyses, where we visualise their sampling behaviour using data
maps and t-SNE plots.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1&quot;&gt;Wei Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1&quot;&gt;Lan Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buntine_W/0/1/0/all/0/1&quot;&gt;Wray Buntine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10118">
<title>From-Ground-To-Objects: Coarse-to-Fine Self-supervised Monocular Depth Estimation of Dynamic Objects with Ground Contact Prior. (arXiv:2312.10118v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.10118</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised monocular depth estimation (DE) is an approach to learning
depth without costly depth ground truths. However, it often struggles with
moving objects that violate the static scene assumption during training. To
address this issue, we introduce a coarse-to-fine training strategy leveraging
the ground contacting prior based on the observation that most moving objects
in outdoor scenes contact the ground. In the coarse training stage, we exclude
the objects in dynamic classes from the reprojection loss calculation to avoid
inaccurate depth learning. To provide precise supervision on the depth of the
objects, we present a novel Ground-contacting-prior Disparity Smoothness Loss
(GDS-Loss) that encourages a DE network to align the depth of the objects with
their ground-contacting points. Subsequently, in the fine training stage, we
refine the DE network to learn the detailed depth of the objects from the
reprojection loss, while ensuring accurate DE on the moving object regions by
employing our regularization loss with a cost-volume-based weighting factor.
Our overall coarse-to-fine training strategy can easily be integrated with
existing DE methods without any modifications, significantly enhancing DE
performance on challenging Cityscapes and KITTI datasets, especially in the
moving object regions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1&quot;&gt;Jaeho Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bello_J/0/1/0/all/0/1&quot;&gt;Juan Luis Gonzalez Bello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_B/0/1/0/all/0/1&quot;&gt;Byeongjun Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Munchurl Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10120">
<title>MVHuman: Tailoring 2D Diffusion with Multi-view Sampling For Realistic 3D Human Generation. (arXiv:2312.10120v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.10120</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent months have witnessed rapid progress in 3D generation based on
diffusion models. Most advances require fine-tuning existing 2D Stable
Diffsuions into multi-view settings or tedious distilling operations and hence
fall short of 3D human generation due to the lack of diverse 3D human datasets.
We present an alternative scheme named MVHuman to generate human radiance
fields from text guidance, with consistent multi-view images directly sampled
from pre-trained Stable Diffsuions without any fine-tuning or distilling. Our
core is a multi-view sampling strategy to tailor the denoising processes of the
pre-trained network for generating consistent multi-view images. It encompasses
view-consistent conditioning, replacing the original noises with
``consistency-guided noises&apos;&apos;, optimizing latent codes, as well as utilizing
cross-view attention layers. With the multi-view images through the sampling
process, we adopt geometry refinement and 3D radiance field generation followed
by a subsequent neural blending scheme for free-view rendering. Extensive
experiments demonstrate the efficacy of our method, as well as its superiority
to state-of-the-art 3D human generation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1&quot;&gt;Suyi Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1&quot;&gt;Haimin Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Haoran Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jingyi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Lan Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10132">
<title>Closing the Gap: Achieving Better Accuracy-Robustness Tradeoffs Against Query-Based Attacks. (arXiv:2312.10132v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.10132</link>
<description rdf:parseType="Literal">&lt;p&gt;Although promising, existing defenses against query-based attacks share a
common limitation: they offer increased robustness against attacks at the price
of a considerable accuracy drop on clean samples. In this work, we show how to
efficiently establish, at test-time, a solid tradeoff between robustness and
accuracy when mitigating query-based attacks. Given that these attacks
necessarily explore low-confidence regions, our insight is that activating
dedicated defenses, such as RND (Qin et al., NeuRIPS 2021) and Random Image
Transformations (Xie et al., ICLR 2018), only for low-confidence inputs is
sufficient to prevent them. Our approach is independent of training and
supported by theory. We verify the effectiveness of our approach for various
existing defenses by conducting extensive experiments on CIFAR-10, CIFAR-100,
and ImageNet. Our results confirm that our proposal can indeed enhance these
defenses by providing better tradeoffs between robustness and accuracy when
compared to state-of-the-art approaches while being completely training-free.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zimmer_P/0/1/0/all/0/1&quot;&gt;Pascal Zimmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andreina_S/0/1/0/all/0/1&quot;&gt;S&amp;#xe9;bastien Andreina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marson_G/0/1/0/all/0/1&quot;&gt;Giorgia Azzurra Marson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karame_G/0/1/0/all/0/1&quot;&gt;Ghassan Karame&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10136">
<title>Gradient-based Parameter Selection for Efficient Fine-Tuning. (arXiv:2312.10136v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.10136</link>
<description rdf:parseType="Literal">&lt;p&gt;With the growing size of pre-trained models, full fine-tuning and storing all
the parameters for various downstream tasks is costly and infeasible. In this
paper, we propose a new parameter-efficient fine-tuning method, Gradient-based
Parameter Selection (GPS), demonstrating that only tuning a few selected
parameters from the pre-trained model while keeping the remainder of the model
frozen can generate similar or better performance compared with the full model
fine-tuning method. Different from the existing popular and state-of-the-art
parameter-efficient fine-tuning approaches, our method does not introduce any
additional parameters and computational costs during both the training and
inference stages. Another advantage is the model-agnostic and non-destructive
property, which eliminates the need for any other design specific to a
particular model. Compared with the full fine-tuning, GPS achieves 3.33%
(91.78% vs. 88.45%, FGVC) and 9.61% (73.1% vs. 65.57%, VTAB) improvement of the
accuracy with tuning only 0.36% parameters of the pre-trained model on average
over 24 image classification tasks; it also demonstrates a significant
improvement of 17% and 16.8% in mDice and mIoU, respectively, on medical image
segmentation task. Moreover, GPS achieves state-of-the-art performance compared
with existing PEFT methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qizhe Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1&quot;&gt;Zijun Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Renrui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shutova_E/0/1/0/all/0/1&quot;&gt;Ekaterina Shutova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shiji Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shanghang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10144">
<title>Data-Efficient Multimodal Fusion on a Single GPU. (arXiv:2312.10144v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.10144</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of multimodal alignment is to learn a single latent space that is
shared between multimodal inputs. The most powerful models in this space have
been trained using massive datasets of paired inputs and large-scale
computational resources, making them prohibitively expensive to train in many
practical scenarios. We surmise that existing unimodal encoders pre-trained on
large amounts of unimodal data should provide an effective bootstrap to create
multimodal models from unimodal ones at much lower costs. We therefore propose
FuseMix, a multimodal augmentation scheme that operates on the latent spaces of
arbitrary pre-trained unimodal encoders. Using FuseMix for multimodal
alignment, we achieve competitive performance -- and in certain cases
outperform state-of-the art methods -- in both image-text and audio-text
retrieval, with orders of magnitude less compute and data: for example, we
outperform CLIP on the Flickr30K text-to-image retrieval task with $\sim \!
600\times$ fewer GPU days and $\sim \! 80\times$ fewer image-text pairs.
Additionally, we show how our method can be applied to convert pre-trained
text-to-image generative models into audio-to-image ones. Code is available at:
https://github.com/layer6ai-labs/fusemix.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vouitsis_N/0/1/0/all/0/1&quot;&gt;No&amp;#xeb;l Vouitsis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhaoyan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gorti_S/0/1/0/all/0/1&quot;&gt;Satya Krishna Gorti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Villecroze_V/0/1/0/all/0/1&quot;&gt;Valentin Villecroze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cresswell_J/0/1/0/all/0/1&quot;&gt;Jesse C. Cresswell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1&quot;&gt;Guangwei Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loaiza_Ganem_G/0/1/0/all/0/1&quot;&gt;Gabriel Loaiza-Ganem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Volkovs_M/0/1/0/all/0/1&quot;&gt;Maksims Volkovs&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10163">
<title>Towards the Unification of Generative and Discriminative Visual Foundation Model: A Survey. (arXiv:2312.10163v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.10163</link>
<description rdf:parseType="Literal">&lt;p&gt;The advent of foundation models, which are pre-trained on vast datasets, has
ushered in a new era of computer vision, characterized by their robustness and
remarkable zero-shot generalization capabilities. Mirroring the transformative
impact of foundation models like large language models (LLMs) in natural
language processing, visual foundation models (VFMs) have become a catalyst for
groundbreaking developments in computer vision. This review paper delineates
the pivotal trajectories of VFMs, emphasizing their scalability and proficiency
in generative tasks such as text-to-image synthesis, as well as their adeptness
in discriminative tasks including image segmentation. While generative and
discriminative models have historically charted distinct paths, we undertake a
comprehensive examination of the recent strides made by VFMs in both domains,
elucidating their origins, seminal breakthroughs, and pivotal methodologies.
Additionally, we collate and discuss the extensive resources that facilitate
the development of VFMs and address the challenges that pave the way for future
research endeavors. A crucial direction for forthcoming innovation is the
amalgamation of generative and discriminative paradigms. The nascent
application of generative models within discriminative contexts signifies the
early stages of this confluence. This survey aspires to be a contemporary
compendium for scholars and practitioners alike, charting the course of VFMs
and illuminating their multifaceted landscape.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1&quot;&gt;Tong Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuanxin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuping Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1&quot;&gt;Qinjingwen Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1&quot;&gt;Weizhi Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yonghuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Junjun He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yiqing Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10165">
<title>Test-Time Domain Adaptation by Learning Domain-Aware Batch Normalization. (arXiv:2312.10165v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.10165</link>
<description rdf:parseType="Literal">&lt;p&gt;Test-time domain adaptation aims to adapt the model trained on source domains
to unseen target domains using a few unlabeled images. Emerging research has
shown that the label and domain information is separately embedded in the
weight matrix and batch normalization (BN) layer. Previous works normally
update the whole network naively without explicitly decoupling the knowledge
between label and domain. As a result, it leads to knowledge interference and
defective distribution adaptation. In this work, we propose to reduce such
learning interference and elevate the domain knowledge learning by only
manipulating the BN layer. However, the normalization step in BN is
intrinsically unstable when the statistics are re-estimated from a few samples.
We find that ambiguities can be greatly reduced when only updating the two
affine parameters in BN while keeping the source domain statistics. To further
enhance the domain knowledge extraction from unlabeled data, we construct an
auxiliary branch with label-independent self-supervised learning (SSL) to
provide supervision. Moreover, we propose a bi-level optimization based on
meta-learning to enforce the alignment of two learning objectives of auxiliary
and main branches. The goal is to use the auxiliary branch to adapt the domain
and benefit main task for subsequent inference. Our method keeps the same
computational cost at inference as the auxiliary branch can be thoroughly
discarded after adaptation. Extensive experiments show that our method
outperforms the prior works on five WILDS real-world domain shift datasets. Our
method can also be integrated with methods with label-dependent optimization to
further push the performance boundary. Our code is available at
https://github.com/ynanwu/MABN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yanan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chi_Z/0/1/0/all/0/1&quot;&gt;Zhixiang Chi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plataniotis_K/0/1/0/all/0/1&quot;&gt;Konstantinos N. Plataniotis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1&quot;&gt;Songhe Feng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10175">
<title>UniAR: Unifying Human Attention and Response Prediction on Visual Content. (arXiv:2312.10175v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.10175</link>
<description rdf:parseType="Literal">&lt;p&gt;Progress in human behavior modeling involves understanding both implicit,
early-stage perceptual behavior such as human attention and explicit,
later-stage behavior such as subjective ratings/preferences. Yet, most prior
research has focused on modeling implicit and explicit human behavior in
isolation. Can we build a unified model of human attention and preference
behavior that reliably works across diverse types of visual content? Such a
model would enable predicting subjective feedback such as overall satisfaction
or aesthetic quality ratings, along with the underlying human attention or
interaction heatmaps and viewing order, enabling designers and content-creation
models to optimize their creation for human-centric improvements. In this
paper, we propose UniAR -- a unified model that predicts both implicit and
explicit human behavior across different types of visual content. UniAR
leverages a multimodal transformer, featuring distinct prediction heads for
each facet, and predicts attention heatmap, scanpath or viewing order, and
subjective rating/preference. We train UniAR on diverse public datasets
spanning natural images, web pages and graphic designs, and achieve leading
performance on multiple benchmarks across different image domains and various
behavior modeling tasks. Potential applications include providing instant
feedback on the effectiveness of UIs/digital designs/images, and serving as a
reward model to further optimize design/image creation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Peizhao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Junfeng He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Gang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhargava_R/0/1/0/all/0/1&quot;&gt;Rachit Bhargava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1&quot;&gt;Shaolei Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valliappan_N/0/1/0/all/0/1&quot;&gt;Nachiappan Valliappan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Youwei Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_H/0/1/0/all/0/1&quot;&gt;Hongxiang Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramachandran_V/0/1/0/all/0/1&quot;&gt;Venky Ramachandran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farhadi_G/0/1/0/all/0/1&quot;&gt;Golnaz Farhadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kohlhoff_K/0/1/0/all/0/1&quot;&gt;Kai J Kohlhoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navalpakkam_V/0/1/0/all/0/1&quot;&gt;Vidhya Navalpakkam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10191">
<title>Tell Me What You See: Text-Guided Real-World Image Denoising. (arXiv:2312.10191v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.10191</link>
<description rdf:parseType="Literal">&lt;p&gt;Image reconstruction in low-light conditions is a challenging problem. Many
solutions have been proposed for it, where the main approach is trying to learn
a good prior of natural images along with modeling the true statistics of the
noise in the scene. In the presence of very low lighting conditions, such
approaches are usually not enough, and additional information is required,
e.g., in the form of using multiple captures. In this work, we suggest as an
alternative to add a description of the scene as prior, which can be easily
done by the photographer who is capturing the scene. Using a text-conditioned
diffusion model, we show that adding image caption information improves
significantly the image reconstruction in low-light conditions on both
synthetic and real-world images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yosef_E/0/1/0/all/0/1&quot;&gt;Erez Yosef&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1&quot;&gt;Raja Giryes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10195">
<title>SoloPose: One-Shot Kinematic 3D Human Pose Estimation with Video Data Augmentation. (arXiv:2312.10195v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.10195</link>
<description rdf:parseType="Literal">&lt;p&gt;While recent two-stage many-to-one deep learning models have demonstrated
great success in 3D human pose estimation, such models are inefficient ways to
detect 3D key points in a sequential video relative to one-shot and
many-to-many models. Another key drawback of two-stage and many-to-one models
is that errors in the first stage will be passed onto the second stage. In this
paper, we introduce SoloPose, a novel one-shot, many-to-many spatio-temporal
transformer model for kinematic 3D human pose estimation of video. SoloPose is
further fortified by HeatPose, a 3D heatmap based on Gaussian Mixture Model
distributions that factors target key points as well as kinematically adjacent
key points. Finally, we address data diversity constraints with the 3D
AugMotion Toolkit, a methodology to augment existing 3D human pose datasets,
specifically by projecting four top public 3D human pose datasets (Humans3.6M,
MADS, AIST Dance++, MPI INF 3DHP) into a novel dataset (Humans7.1M) with a
universal coordinate system. Extensive experiments are conducted on Human3.6M
as well as the augmented Humans7.1M dataset, and SoloPose demonstrates superior
results relative to the state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_D/0/1/0/all/0/1&quot;&gt;David C. Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hongji Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salazar_S/0/1/0/all/0/1&quot;&gt;Saunder Salazar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jessie Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kitts_C/0/1/0/all/0/1&quot;&gt;Christopher A. Kitts&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10200">
<title>Deep Active Perception for Object Detection using Navigation Proposals. (arXiv:2312.10200v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.10200</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Learning (DL) has brought significant advances to robotics vision tasks.
However, most existing DL methods have a major shortcoming, they rely on a
static inference paradigm inherent in traditional computer vision pipelines. On
the other hand, recent studies have found that active perception improves the
perception abilities of various models by going beyond these static paradigms.
Despite the significant potential of active perception, it poses several
challenges, primarily involving significant changes in training pipelines for
deep learning models. To overcome these limitations, in this work, we propose a
generic supervised active perception pipeline for object detection that can be
trained using existing off-the-shelf object detectors, while also leveraging
advances in simulation environments. To this end, the proposed method employs
an additional neural network architecture that estimates better viewpoints in
cases where the object detector confidence is insufficient. The proposed method
was evaluated on synthetic datasets, constructed within the Webots robotics
simulator, showcasing its effectiveness in two object detection cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ginargiros_S/0/1/0/all/0/1&quot;&gt;Stefanos Ginargiros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Passalis_N/0/1/0/all/0/1&quot;&gt;Nikolaos Passalis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tefas_A/0/1/0/all/0/1&quot;&gt;Anastasios Tefas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10208">
<title>Video-based Surgical Skill Assessment using Tree-based Gaussian Process Classifier. (arXiv:2312.10208v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.10208</link>
<description rdf:parseType="Literal">&lt;p&gt;assessment using video data and to showcase the effectiveness of the proposed
approach in evaluating surgeon proficiency, its potential for targeted training
interventions, and quality assurance in surgical departments. The pipeline
incorporates a representation flow convolutional neural network and a novel
tree-based Gaussian process classifier, which is robust to noise, while being
computationally efficient. Additionally, new kernels are introduced to enhance
accuracy. The performance of the pipeline is evaluated using the JIGSAWS
dataset. Comparative analysis with existing literature reveals significant
improvement in accuracy and betterment in computation cost. The proposed
pipeline contributes to computational efficiency and accuracy improvement in
surgical skill assessment using video data. Results of our study based on
comments of our colleague surgeons show that the proposed method has the
potential to facilitate skill improvement among surgery fellows and enhance
patient safety through targeted training interventions and quality assurance in
surgical departments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rezaei_A/0/1/0/all/0/1&quot;&gt;Arefeh Rezaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmadi_M/0/1/0/all/0/1&quot;&gt;Mohammad Javad Ahmadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molaei_A/0/1/0/all/0/1&quot;&gt;Amir Molaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taghirad_H/0/1/0/all/0/1&quot;&gt;Hamid. D. Taghirad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10217">
<title>T-MAE: Temporal Masked Autoencoders for Point Cloud Representation Learning. (arXiv:2312.10217v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.10217</link>
<description rdf:parseType="Literal">&lt;p&gt;The scarcity of annotated data in outdoor point cloud segmentation poses a
significant obstacle in harnessing the modeling capabilities of advanced
networks like transformers. Consequently, scholars have been actively
investigating efficacious self-supervised pre-training strategies, e.g.
contrasting learning and reconstruction-based pretext tasks. Nevertheless,
temporal information, which is inherent in the LiDAR point cloud sequence, is
consistently disregarded. To better utilize this property, we propose an
effective pre-training strategy, namely Temporal Masked AutoEncoders (T-MAE),
which takes as input temporally adjacent frames and learns temporal dependency.
A SiamWCA backbone, containing a Siamese encoder and a window-based
cross-attention (WCA) module, is established for the two-frame input. Taking
into account that the motion of an ego-vehicle alters the illumination angles
of the same instance, temporal modeling also serves as a robust and natural
data augmentation, enhancing the comprehension of target objects. Moreover,
instead of utilizing consecutive frames, it is more cost-effective and powerful
by using distant historical frames. SiamWCA is a powerful architecture but
heavily relies on annotated data. With our T-MAE pre-training strategy, we
achieve the best performance on the Waymo dataset among self-supervised
learning methods. Comprehensive experiments are conducted to validate all
components of our proposal. Upon acceptance, the source code will be made
accessible.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1&quot;&gt;Weijie Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nejadasl_F/0/1/0/all/0/1&quot;&gt;Fatemeh Karimi Nejadasl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gevers_T/0/1/0/all/0/1&quot;&gt;Theo Gevers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oswald_M/0/1/0/all/0/1&quot;&gt;Martin R. Oswald&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2106.05152">
<title>Rethinking Transfer Learning for Medical Image Classification. (arXiv:2106.05152v7 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2106.05152</link>
<description rdf:parseType="Literal">&lt;p&gt;Transfer learning (TL) from pretrained deep models is a standard practice in
modern medical image classification (MIC). However, what levels of features to
be reused are problem-dependent, and uniformly finetuning all layers of
pretrained models may be suboptimal. This insight has partly motivated the
recent differential TL strategies, such as TransFusion (TF) and layer-wise
finetuning (LWFT), which treat the layers in the pretrained models
differentially. In this paper, we add one more strategy into this family,
called TruncatedTL, which reuses and finetunes appropriate bottom layers and
directly discards the remaining layers. This yields not only superior MIC
performance but also compact models for efficient inference, compared to other
differential TL methods. Our code is available at:
https://github.com/sun-umn/TTL
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Peng_L/0/1/0/all/0/1&quot;&gt;Le Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liang_H/0/1/0/all/0/1&quot;&gt;Hengyue Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Luo_G/0/1/0/all/0/1&quot;&gt;Gaoxiang Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Taihui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Ju Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.05216">
<title>High-order Tensor Pooling with Attention for Action Recognition. (arXiv:2110.05216v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2110.05216</link>
<description rdf:parseType="Literal">&lt;p&gt;We aim at capturing high-order statistics of feature vectors formed by a
neural network, and propose end-to-end second- and higher-order pooling to form
a tensor descriptor. Tensor descriptors require a robust similarity measure due
to low numbers of aggregated vectors and the burstiness phenomenon, when a
given feature appears more/less frequently than statistically expected. The
Heat Diffusion Process (HDP) on a graph Laplacian is closely related to the
Eigenvalue Power Normalization (EPN) of the covariance/autocorrelation matrix,
whose inverse forms a loopy graph Laplacian. We show that the HDP and the EPN
play the same role, i.e., to boost or dampen the magnitude of the eigenspectrum
thus preventing the burstiness. We equip higher-order tensors with EPN which
acts as a spectral detector of higher-order occurrences to prevent burstiness.
We also prove that for a tensor of order r built from d dimensional feature
descriptors, such a detector gives the likelihood if at least one higher-order
occurrence is &apos;projected&apos; into one of binom(d,r) subspaces represented by the
tensor; thus forming a tensor power normalization metric endowed with
binom(d,r) such &apos;detectors&apos;. For experimental contributions, we apply several
second- and higher-order pooling variants to action recognition, provide
previously not presented comparisons of such pooling variants, and show
state-of-the-art results on HMDB-51, YUP++ and MPII Cooking Activities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1&quot;&gt;Ke Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1&quot;&gt;Piotr Koniusz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.09445">
<title>Data Efficient Language-supervised Zero-shot Recognition with Optimal Transport Distillation. (arXiv:2112.09445v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2112.09445</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional computer vision models are trained to predict a fixed set of
predefined categories. Recently, natural language has been shown to be a
broader and richer source of supervision that provides finer descriptions to
visual concepts than supervised &quot;gold&quot; labels. Previous works, such as CLIP,
use InfoNCE loss to train a model to predict the pairing between images and
text captions. CLIP, however, is data hungry and requires more than 400M
image-text pairs for training. The inefficiency can be partially attributed to
the fact that the image-text pairs are noisy. To address this, we propose OTTER
(Optimal TransporT distillation for Efficient zero-shot Recognition), which
uses online entropic optimal transport to find a soft image-text match as
labels for contrastive learning. Based on pretrained image and text encoders,
models trained with OTTER achieve strong performance with only 3M image text
pairs. Compared with InfoNCE loss, label smoothing, and knowledge distillation,
OTTER consistently outperforms these baselines in zero shot evaluation on
Google Open Images (19,958 classes) and multi-labeled ImageNet 10K (10032
classes) from Tencent ML-Images. Over 42 evaluations on 7 different
dataset/architecture settings x 6 metrics, OTTER outperforms (32) or ties (2)
all baselines in 34 of them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Bichen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1&quot;&gt;Ruizhe Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Peizhao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1&quot;&gt;Tianren Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vajda_P/0/1/0/all/0/1&quot;&gt;Peter Vajda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1&quot;&gt;Joseph E. Gonzalez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.03005">
<title>Self-Supervised Face Image Restoration with a One-Shot Reference. (arXiv:2203.03005v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.03005</link>
<description rdf:parseType="Literal">&lt;p&gt;For image restoration, methods leveraging priors from generative models have
been proposed and demonstrated a promising capacity to robustly restore
photorealistic and high-quality results. However, these methods are susceptible
to semantic ambiguity, particularly with images that have obviously correct
semantics such as facial images. In this paper, we propose a semantic-aware
latent space exploration method for image restoration (SAIR). By explicitly
modeling semantics information from a given reference image, SAIR is able to
reliably restore severely degraded images not only to high-resolution and
highly realistic looks but also to correct semantics. Quantitative and
qualitative experiments collectively demonstrate the superior performance of
the proposed SAIR. Our code is available at https://github.com/Liamkuo/SAIR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yanhui Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1&quot;&gt;Fangzhou Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shaoyuan Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.05980">
<title>Optical flow GNSS for navigation in the Indian subcontinent (NavIC). (arXiv:2204.05980v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2204.05980</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper reveals about global navigation satellite system GNSS in the
indian subcontinent known as the navigation in the indian subcontinent(NavIC)
We have tried to model a new technique in GNSS known as the optical flow
tracking global navigation system (OF GNSS). This method using differential
equations is very accurate for very small distances on the surface of the earth
in the 1500km range of the Indian subcontinent satellite coverage. When we talk
of accuracy of the GPS system it should be very accurate on the surface of the
earth when used to show changes in coordinate of the moving body with respect
to the ground by the satellite which is situated on the earths orbit. Optical
flow is a method which uses movements with respect to x and y axis for
infinitesimal changes in its coordinates and then uses this algorithm to use it
in global positioning system to find accurate position of the body with respect
to the satellite coordinates with respect to ground positioning. The modern
method of differential frames is also very accurate as it involves
infinitesimal frames which are modelled together from the satellite to find
changes in the coordinates on the earths surface, so we have designed a new
algorithm in this paper on the Optical flow GNSS system which is an alternative
and can improve the study done in the design of these algorithms in this field
of applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fulari_S/0/1/0/all/0/1&quot;&gt;Sunit Shantanu Digamber Fulari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.06754">
<title>RecurSeed and EdgePredictMix: Pseudo-Label Refinement Learning for Weakly Supervised Semantic Segmentation across Single- and Multi-Stage Frameworks. (arXiv:2204.06754v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2204.06754</link>
<description rdf:parseType="Literal">&lt;p&gt;Although weakly supervised semantic segmentation using only image-level
labels (WSSS-IL) is potentially useful, its low performance and implementation
complexity still limit its application. The main causes are (a) non-detection
and (b) false-detection phenomena: (a) The class activation maps refined from
existing WSSS-IL methods still only represent partial regions for large-scale
objects, and (b) for small-scale objects, over-activation causes them to
deviate from the object edges. We propose RecurSeed, which alternately reduces
non- and false detections through recursive iterations, thereby implicitly
finding an optimal junction that minimizes both errors. We also propose a novel
data augmentation (DA) approach called EdgePredictMix, which further expresses
an object&apos;s edge by utilizing the probability difference information between
adjacent pixels in combining the segmentation results, thereby compensating for
the shortcomings when applying the existing DA methods to WSSS. We achieved new
state-of-the-art performances on both the PASCAL VOC 2012 and MS COCO 2014
benchmarks (VOC val: 74.4%, COCO val: 46.4%). The code is available at
https://github.com/shjo-april/RecurSeed_and_EdgePredictMix.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jo_S/0/1/0/all/0/1&quot;&gt;Sanghyun Jo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_I/0/1/0/all/0/1&quot;&gt;In-Jae Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kyungsu Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.09141">
<title>G2P-DDM: Generating Sign Pose Sequence from Gloss Sequence with Discrete Diffusion Model. (arXiv:2208.09141v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2208.09141</link>
<description rdf:parseType="Literal">&lt;p&gt;The Sign Language Production (SLP) project aims to automatically translate
spoken languages into sign sequences. Our approach focuses on the
transformation of sign gloss sequences into their corresponding sign pose
sequences (G2P). In this paper, we present a novel solution for this task by
converting the continuous pose space generation problem into a discrete
sequence generation problem. We introduce the Pose-VQVAE framework, which
combines Variational Autoencoders (VAEs) with vector quantization to produce a
discrete latent representation for continuous pose sequences. Additionally, we
propose the G2P-DDM model, a discrete denoising diffusion architecture for
length-varied discrete sequence data, to model the latent prior. To further
enhance the quality of pose sequence generation in the discrete space, we
present the CodeUnet model to leverage spatial-temporal information. Lastly, we
develop a heuristic sequential clustering method to predict variable lengths of
pose sequences for corresponding gloss sequences. Our results show that our
model outperforms state-of-the-art G2P models on the public SLP evaluation
benchmark. For more generated results, please visit our project page:
\textcolor{blue}{\url{https://slpdiffusier.github.io/g2p-ddm}}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1&quot;&gt;Pan Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qipeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_T/0/1/0/all/0/1&quot;&gt;Taiyi Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Hao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yao Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zexian Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.02178">
<title>Transformer-CNN Cohort: Semi-supervised Semantic Segmentation by the Best of Both Students. (arXiv:2209.02178v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.02178</link>
<description rdf:parseType="Literal">&lt;p&gt;The popular methods for semi-supervised semantic segmentation mostly adopt a
unitary network model using convolutional neural networks (CNNs) and enforce
consistency of the model&apos;s predictions over perturbations applied to the inputs
or model. However, such a learning paradigm suffers from two critical
limitations: a) learning the discriminative features for the unlabeled data; b)
learning both global and local information from the whole image. In this paper,
we propose a novel Semi-supervised Learning (SSL) approach, called
Transformer-CNN Cohort (TCC), that consists of two students with one based on
the vision transformer (ViT) and the other based on the CNN. Our method subtly
incorporates the multi-level consistency regularization on the predictions and
the heterogeneous feature spaces via pseudo-labeling for the unlabeled data.
First, as the inputs of the ViT student are image patches, the feature maps
extracted encode crucial class-wise statistics. To this end, we propose
class-aware feature consistency distillation (CFCD) that first leverages the
outputs of each student as the pseudo labels and generates class-aware feature
(CF) maps for knowledge transfer between the two students. Second, as the ViT
student has more uniform representations for all layers, we propose
consistency-aware cross distillation (CCD) to transfer knowledge between the
pixel-wise predictions from the cohort. We validate the TCC framework on
Cityscapes and Pascal VOC 2012 datasets, which outperforms existing SSL methods
by a large margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1&quot;&gt;Xu Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yunhao Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1&quot;&gt;Chong Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kangcheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.04253">
<title>Eat-Radar: Continuous Fine-Grained Intake Gesture Detection Using FMCW Radar and 3D Temporal Convolutional Network with Attention. (arXiv:2211.04253v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.04253</link>
<description rdf:parseType="Literal">&lt;p&gt;Unhealthy dietary habits are considered as the primary cause of various
chronic diseases, including obesity and diabetes. The automatic food intake
monitoring system has the potential to improve the quality of life (QoL) of
people with diet-related diseases through dietary assessment. In this work, we
propose a novel contactless radar-based approach for food intake monitoring.
Specifically, a Frequency Modulated Continuous Wave (FMCW) radar sensor is
employed to recognize fine-grained eating and drinking gestures. The
fine-grained eating/drinking gesture contains a series of movements from
raising the hand to the mouth until putting away the hand from the mouth. A 3D
temporal convolutional network with self-attention (3D-TCN-Att) is developed to
detect and segment eating and drinking gestures in meal sessions by processing
the Range-Doppler Cube (RD Cube). Unlike previous radar-based research, this
work collects data in continuous meal sessions (more realistic scenarios). We
create a public dataset comprising 70 meal sessions (4,132 eating gestures and
893 drinking gestures) from 70 participants with a total duration of 1,155
minutes. Four eating styles (fork &amp;amp; knife, chopsticks, spoon, hand) are
included in this dataset. To validate the performance of the proposed approach,
seven-fold cross-validation method is applied. The 3D-TCN-Att model achieves a
segmental F1-score of 0.896 and 0.868 for eating and drinking gestures,
respectively. The results of the proposed approach indicate the feasibility of
using radar for fine-grained eating and drinking gesture detection and
segmentation in meal sessions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chunzhuo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_T/0/1/0/all/0/1&quot;&gt;T. Sunil Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raedt_W/0/1/0/all/0/1&quot;&gt;Walter De Raedt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Camps_G/0/1/0/all/0/1&quot;&gt;Guido Camps&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hallez_H/0/1/0/all/0/1&quot;&gt;Hans Hallez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vanrumste_B/0/1/0/all/0/1&quot;&gt;Bart Vanrumste&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.06663">
<title>NeighborTrack: Improving Single Object Tracking by Bipartite Matching with Neighbor Tracklets. (arXiv:2211.06663v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.06663</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a post-processor, called NeighborTrack, that leverages neighbor
information of the tracking target to validate and improve single-object
tracking (SOT) results. It requires no additional data or retraining. Instead,
it uses the confidence score predicted by the backbone SOT network to
automatically derive neighbor information and then uses this information to
improve the tracking results. When tracking an occluded target, its appearance
features are untrustworthy. However, a general siamese network often cannot
tell whether the tracked object is occluded by reading the confidence score
alone, because it could be misled by neighbors with high confidence scores. Our
proposed NeighborTrack takes advantage of unoccluded neighbors&apos; information to
reconfirm the tracking target and reduces false tracking when the target is
occluded. It not only reduces the impact caused by occlusion, but also fixes
tracking problems caused by object appearance changes. NeighborTrack is
agnostic to SOT networks and post-processing methods. For the VOT challenge
dataset commonly used in short-term object tracking, we improve three famous
SOT networks, Ocean, TransT, and OSTrack, by an average of ${1.92\%}$ EAO and
${2.11\%}$ robustness. For the mid- and long-term tracking experiments based on
OSTrack, we achieve state-of-the-art ${72.25\%}$ AUC on LaSOT and ${75.7\%}$ AO
on GOT-10K. Code duplication can be found in
https://github.com/franktpmvu/NeighborTrack.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yu-Hsi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chien-Yao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Cheng-Yun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1&quot;&gt;Hung-Shuo Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Youn-Long Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1&quot;&gt;Yung-Yu Chuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1&quot;&gt;Hong-Yuan Mark Liao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.10682">
<title>DiffStyler: Controllable Dual Diffusion for Text-Driven Image Stylization. (arXiv:2211.10682v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.10682</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the impressive results of arbitrary image-guided style transfer
methods, text-driven image stylization has recently been proposed for
transferring a natural image into a stylized one according to textual
descriptions of the target style provided by the user. Unlike the previous
image-to-image transfer approaches, text-guided stylization progress provides
users with a more precise and intuitive way to express the desired style.
However, the huge discrepancy between cross-modal inputs/outputs makes it
challenging to conduct text-driven image stylization in a typical feed-forward
CNN pipeline. In this paper, we present DiffStyler, a dual diffusion processing
architecture to control the balance between the content and style of the
diffused results. The cross-modal style information can be easily integrated as
guidance during the diffusion process step-by-step. Furthermore, we propose a
content image-based learnable noise on which the reverse denoising process is
based, enabling the stylization results to better preserve the structure
information of the content image. We validate the proposed DiffStyler beyond
the baseline methods through extensive qualitative and quantitative
experiments. Code is available at
\url{https://github.com/haha-lisa/Diffstyler}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_N/0/1/0/all/0/1&quot;&gt;Nisha Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuxin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1&quot;&gt;Fan Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chongyang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Haibin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1&quot;&gt;Weiming Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Changsheng Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.11133">
<title>Enhancing Accuracy and Robustness of Steering Angle Prediction with Attention Mechanism. (arXiv:2211.11133v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.11133</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we investigate the two most popular families of deep neural
architectures (i.e., ResNets and InceptionNets) for the autonomous driving task
of steering angle prediction. To ensure a comprehensive comparison, we
conducted experiments on the Kaggle SAP dataset and custom dataset and
carefully examined a range of different model sizes within both the ResNet and
InceptionNet families. Our derived models can achieve state-of-the-art results
in terms of steering angle MSE. In addition to this analysis, we introduced the
attention mechanism to enhance steering angle prediction. This attention
mechanism facilitated an in-depth exploration of the model&apos;s selective focus on
essential elements within the input data. Furthermore, recognizing the
importance of security and robustness in autonomous driving assessed the
resilience of our models to adversarial attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nadella_S/0/1/0/all/0/1&quot;&gt;Swetha Nadella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barua_P/0/1/0/all/0/1&quot;&gt;Pramiti Barua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hagler_J/0/1/0/all/0/1&quot;&gt;Jeremy C. Hagler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lamb_D/0/1/0/all/0/1&quot;&gt;David J. Lamb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1&quot;&gt;Qing Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.11174">
<title>The Role of Robust Generalization in Continual Learning: Better Transfer and Less Forgetting. (arXiv:2211.11174v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.11174</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper considers learning a sequence of tasks continually with the
objectives of generalizing over unseen data regardless of its distribution,
accumulating knowledge and transferring knowledge across tasks. To the best of
our knowledge, no existing technique can accomplish all of these objectives
simultaneously. This paper proposes such a technique by investigating the role
of robust generalization in Continual Learning (CL). Recent findings show that
models trained to exhibit robust generalization not only generalize better, but
also demonstrate improved transferability and tend to find flatter local
minima. This motivates us to achieve robust generalization in each task in CL,
facilitating learning a new task and reducing the risk of forgetting previously
learned tasks. To achieve this, we propose a new online shape-texture
self-distillation (STSD) method that learns both shape and texture
representations for each task, improving robust generalization. Extensive
experiments demonstrate that our approach can be easily combined with existing
CL methods to improve generalization, encourage knowledge transfer, and reduce
forgetting. We also show that our approach finds flatter local minima, further
highlighting the importance of improving robust generalization in CL. Our
proposed technique is a significant step forward in achieving the
aforementioned CL objectives simultaneously.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zenglin Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Ying Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1&quot;&gt;Joo Hwee Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mengmi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.15060">
<title>Interactive Visual Feature Search. (arXiv:2211.15060v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.15060</link>
<description rdf:parseType="Literal">&lt;p&gt;Many visualization techniques have been created to explain the behavior of
computer vision models, but they largely consist of static diagrams that convey
limited information. Interactive visualizations allow users to more easily
interpret a model&apos;s behavior, but most are not easily reusable for new models.
We introduce Visual Feature Search, a novel interactive visualization that is
adaptable to any CNN and can easily be incorporated into a researcher&apos;s
workflow. Our tool allows a user to highlight an image region and search for
images from a given dataset with the most similar model features. We
demonstrate how our tool elucidates different aspects of model behavior by
performing experiments on a range of applications, such as in medical imaging
and wildlife classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ulrich_D/0/1/0/all/0/1&quot;&gt;Devon Ulrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fong_R/0/1/0/all/0/1&quot;&gt;Ruth Fong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.10621">
<title>Full-Body Articulated Human-Object Interaction. (arXiv:2212.10621v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.10621</link>
<description rdf:parseType="Literal">&lt;p&gt;Fine-grained capturing of 3D HOI boosts human activity understanding and
facilitates downstream visual tasks, including action recognition, holistic
scene reconstruction, and human motion synthesis. Despite its significance,
existing works mostly assume that humans interact with rigid objects using only
a few body parts, limiting their scope. In this paper, we address the
challenging problem of f-AHOI, wherein the whole human bodies interact with
articulated objects, whose parts are connected by movable joints. We present
CHAIRS, a large-scale motion-captured f-AHOI dataset, consisting of 16.2 hours
of versatile interactions between 46 participants and 81 articulated and rigid
sittable objects. CHAIRS provides 3D meshes of both humans and articulated
objects during the entire interactive process, as well as realistic and
physically plausible full-body interactions. We show the value of CHAIRS with
object pose estimation. By learning the geometrical relationships in HOI, we
devise the very first model that leverage human pose estimation to tackle the
estimation of articulated object poses and shapes during whole-body
interactions. Given an image and an estimated human pose, our model first
reconstructs the pose and shape of the object, then optimizes the
reconstruction according to a learned interaction prior. Under both evaluation
settings (e.g., with or without the knowledge of objects&apos;
geometries/structures), our model significantly outperforms baselines. We hope
CHAIRS will promote the community towards finer-grained interaction
understanding. We will make the data/code publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1&quot;&gt;Nan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tengyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1&quot;&gt;Zhexuan Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1&quot;&gt;Jieming Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+zhang_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yixin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;He Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yixin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Siyuan Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.02761">
<title>Active Learning Guided by Efficient Surrogate Learners. (arXiv:2301.02761v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.02761</link>
<description rdf:parseType="Literal">&lt;p&gt;Re-training a deep learning model each time a single data point receives a
new label is impractical due to the inherent complexity of the training
process. Consequently, existing active learning (AL) algorithms tend to adopt a
batch-based approach where, during each AL iteration, a set of data points is
collectively chosen for annotation. However, this strategy frequently leads to
redundant sampling, ultimately eroding the efficacy of the labeling procedure.
In this paper, we introduce a new AL algorithm that harnesses the power of a
Gaussian process surrogate in conjunction with the neural network principal
learner. Our proposed model adeptly updates the surrogate learner for every new
data instance, enabling it to emulate and capitalize on the continuous learning
dynamics of the neural network without necessitating a complete retraining of
the principal model for each individual label. Experiments on four benchmark
datasets demonstrate that this approach yields significant enhancements, either
rivaling or aligning with the performance of state-of-the-art techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_Y/0/1/0/all/0/1&quot;&gt;Yunpyo An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Suyeong Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kwang In Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.12662">
<title>FedDBL: Communication and Data Efficient Federated Deep-Broad Learning for Histopathological Tissue Classification. (arXiv:2302.12662v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.12662</link>
<description rdf:parseType="Literal">&lt;p&gt;Histopathological tissue classification is a fundamental task in
computational pathology. Deep learning-based models have achieved superior
performance but centralized training with data centralization suffers from the
privacy leakage problem. Federated learning (FL) can safeguard privacy by
keeping training samples locally, but existing FL-based frameworks require a
large number of well-annotated training samples and numerous rounds of
communication which hinder their practicability in the real-world clinical
scenario. In this paper, we propose a universal and lightweight federated
learning framework, named Federated Deep-Broad Learning (FedDBL), to achieve
superior classification performance with limited training samples and only
one-round communication. By simply associating a pre-trained deep learning
feature extractor, a fast and lightweight broad learning inference system and a
classical federated aggregation approach, FedDBL can dramatically reduce data
dependency and improve communication efficiency. Five-fold cross-validation
demonstrates that FedDBL greatly outperforms the competitors with only
one-round communication and limited training samples, while it even achieves
comparable performance with the ones under multiple-round communications.
Furthermore, due to the lightweight design and one-round communication, FedDBL
reduces the communication burden from 4.6GB to only 276.5KB per client using
the ResNet-50 backbone at 50-round training. Since no data or deep model
sharing across different clients, the privacy issue is well-solved and the
model security is guaranteed with no model inversion attack risk. Code is
available at https://github.com/tianpeng-deng/FedDBL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Deng_T/0/1/0/all/0/1&quot;&gt;Tianpeng Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yanqi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Han_G/0/1/0/all/0/1&quot;&gt;Guoqiang Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhenwei Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jiatai Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dou_Q/0/1/0/all/0/1&quot;&gt;Qi Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zaiyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xiao-jing Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;C. L. Philip Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Han_C/0/1/0/all/0/1&quot;&gt;Chu Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.00215">
<title>Single Image Backdoor Inversion via Robust Smoothed Classifiers. (arXiv:2303.00215v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.00215</link>
<description rdf:parseType="Literal">&lt;p&gt;Backdoor inversion, a central step in many backdoor defenses, is a
reverse-engineering process to recover the hidden backdoor trigger inserted
into a machine learning model. Existing approaches tackle this problem by
searching for a backdoor pattern that is able to flip a set of clean images
into the target class, while the exact size needed of this support set is
rarely investigated. In this work, we present a new approach for backdoor
inversion, which is able to recover the hidden backdoor with as few as a single
image. Insipired by recent advances in adversarial robustness, our method
SmoothInv starts from a single clean image, and then performs projected
gradient descent towards the target class on a robust smoothed version of the
original backdoored classifier. We find that backdoor patterns emerge naturally
from such optimization process. Compared to existing backdoor inversion
methods, SmoothInv introduces minimum optimization variables and does not
require complex regularization schemes. We perform a comprehensive quantitative
and qualitative study on backdoored classifiers obtained from existing backdoor
attacks. We demonstrate that SmoothInv consistently recovers successful
backdoors from single images: for backdoored ImageNet classifiers, our
reconstructed backdoors have close to 100% attack success rates. We also show
that they maintain high fidelity to the underlying true backdoors. Last, we
propose and analyze two countermeasures to our approach and show that SmoothInv
remains robust in the face of an adaptive attacker. Our code is available at
https://github.com/locuslab/smoothinv.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;Mingjie Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1&quot;&gt;J. Zico Kolter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.00355">
<title>Progressive Scale-aware Network for Remote sensing Image Change Captioning. (arXiv:2303.00355v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.00355</link>
<description rdf:parseType="Literal">&lt;p&gt;Remote sensing (RS) images contain numerous objects of different scales,
which poses significant challenges for the RS image change captioning (RSICC)
task to identify visual changes of interest in complex scenes and describe them
via language. However, current methods still have some weaknesses in
sufficiently extracting and utilizing multi-scale information. In this paper,
we propose a progressive scale-aware network (PSNet) to address the problem.
PSNet is a pure Transformer-based model. To sufficiently extract multi-scale
visual features, multiple progressive difference perception (PDP) layers are
stacked to progressively exploit the differencing features of bitemporal
features. To sufficiently utilize the extracted multi-scale features for
captioning, we propose a scale-aware reinforcement (SR) module and combine it
with the Transformer decoding layer to progressively utilize the features from
different PDP layers. Experiments show that the PDP layer and SR module are
effective and our PSNet outperforms previous methods. Our code is public at
https://github.com/Chen-Yang-Liu/PSNet
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chenyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jiajun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1&quot;&gt;Zipeng Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1&quot;&gt;Zhengxia Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhenwei Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.03932">
<title>FFT-based Dynamic Token Mixer for Vision. (arXiv:2303.03932v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.03932</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-head-self-attention (MHSA)-equipped models have achieved notable
performance in computer vision. Their computational complexity is proportional
to quadratic numbers of pixels in input feature maps, resulting in slow
processing, especially when dealing with high-resolution images. New types of
token-mixer are proposed as an alternative to MHSA to circumvent this problem:
an FFT-based token-mixer involves global operations similar to MHSA but with
lower computational complexity. However, despite its attractive properties, the
FFT-based token-mixer has not been carefully examined in terms of its
compatibility with the rapidly evolving MetaFormer architecture. Here, we
propose a novel token-mixer called Dynamic Filter and novel image recognition
models, DFFormer and CDFFormer, to close the gaps above. The results of image
classification and downstream tasks, analysis, and visualization show that our
models are helpful. Notably, their throughput and memory efficiency when
dealing with high-resolution image recognition is remarkable. Our results
indicate that Dynamic Filter is one of the token-mixer options that should be
seriously considered. The code is available at
https://github.com/okojoalg/dfformer
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tatsunami_Y/0/1/0/all/0/1&quot;&gt;Yuki Tatsunami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taki_M/0/1/0/all/0/1&quot;&gt;Masato Taki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.06315">
<title>DETA: Denoised Task Adaptation for Few-Shot Learning. (arXiv:2303.06315v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.06315</link>
<description rdf:parseType="Literal">&lt;p&gt;Test-time task adaptation in few-shot learning aims to adapt a pre-trained
task-agnostic model for capturing taskspecific knowledge of the test task, rely
only on few-labeled support samples. Previous approaches generally focus on
developing advanced algorithms to achieve the goal, while neglecting the
inherent problems of the given support samples. In fact, with only a handful of
samples available, the adverse effect of either the image noise (a.k.a.
X-noise) or the label noise (a.k.a. Y-noise) from support samples can be
severely amplified. To address this challenge, in this work we propose DEnoised
Task Adaptation (DETA), a first, unified image- and label-denoising framework
orthogonal to existing task adaptation approaches. Without extra supervision,
DETA filters out task-irrelevant, noisy representations by taking advantage of
both global visual information and local region details of support samples. On
the challenging Meta-Dataset, DETA consistently improves the performance of a
broad spectrum of baseline methods applied on various pre-trained models.
Notably, by tackling the overlooked image noise in Meta-Dataset, DETA
establishes new state-of-the-art results. Code is released at
https://github.com/JimZAI/DETA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Ji Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1&quot;&gt;Lianli Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xu Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Hengtao Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jingkuan Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.08815">
<title>Lane Graph as Path: Continuity-preserving Path-wise Modeling for Online Lane Graph Construction. (arXiv:2303.08815v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.08815</link>
<description rdf:parseType="Literal">&lt;p&gt;Online lane graph construction is a promising but challenging task in
autonomous driving. Previous methods usually model the lane graph at the pixel
or piece level, and recover the lane graph by pixel-wise or piece-wise
connection, which breaks down the continuity of the lane. Human drivers focus
on and drive along the continuous and complete paths instead of considering
lane pieces. Autonomous vehicles also require path-specific guidance from lane
graph for trajectory planning. We argue that the path, which indicates the
traffic flow, is the primitive of the lane graph. Motivated by this, we propose
to model the lane graph in a novel path-wise manner, which well preserves the
continuity of the lane and encodes traffic information for planning. We present
a path-based online lane graph construction method, termed LaneGAP, which
end-to-end learns the path and recovers the lane graph via a Path2Graph
algorithm. We qualitatively and quantitatively demonstrate the superiority of
LaneGAP over conventional pixel-based and piece-based methods on challenging
nuScenes and Argoverse2 datasets. Abundant visualizations show LaneGAP can cope
with diverse traffic conditions. Code and models will be released at
\url{https://github.com/hustvl/LaneGAP} for facilitating future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_B/0/1/0/all/0/1&quot;&gt;Bencheng Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shaoyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1&quot;&gt;Bo Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_T/0/1/0/all/0/1&quot;&gt;Tianheng Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wenyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinggang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.08977">
<title>DeblurSR: Event-Based Motion Deblurring Under the Spiking Representation. (arXiv:2303.08977v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.08977</link>
<description rdf:parseType="Literal">&lt;p&gt;We present DeblurSR, a novel motion deblurring approach that converts a
blurry image into a sharp video. DeblurSR utilizes event data to compensate for
motion ambiguities and exploits the spiking representation to parameterize the
sharp output video as a mapping from time to intensity. Our key contribution,
the Spiking Representation (SR), is inspired by the neuromorphic principles
determining how biological neurons communicate with each other in living
organisms. We discuss why the spikes can represent sharp edges and how the
spiking parameters are interpreted from the neuromorphic perspective. DeblurSR
has higher output quality and requires fewer computing resources than
state-of-the-art event-based motion deblurring methods. We additionally show
that our approach easily extends to video super-resolution when combined with
recent advances in implicit neural representation. The implementation and
animated visualization of DeblurSR are available at
https://github.com/chensong1995/DeblurSR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1&quot;&gt;Chen Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bajaj_C/0/1/0/all/0/1&quot;&gt;Chandrajit Bajaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qixing Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.11098">
<title>Understanding the Role of the Projector in Knowledge Distillation. (arXiv:2303.11098v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.11098</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we revisit the efficacy of knowledge distillation as a function
matching and metric learning problem. In doing so we verify three important
design decisions, namely the normalisation, soft maximum function, and
projection layers as key ingredients. We theoretically show that the projector
implicitly encodes information on past examples, enabling relational gradients
for the student. We then show that the normalisation of representations is
tightly coupled with the training dynamics of this projector, which can have a
large impact on the students performance. Finally, we show that a simple soft
maximum function can be used to address any significant capacity gap problems.
Experimental results on various benchmark datasets demonstrate that using these
insights can lead to superior or comparable performance to state-of-the-art
knowledge distillation techniques, despite being much more computationally
efficient. In particular, we obtain these results across image classification
(CIFAR100 and ImageNet), object detection (COCO2017), and on more difficult
distillation objectives, such as training data efficient transformers, whereby
we attain a 77.2% top-1 accuracy with DeiT-Ti on ImageNet. Code and models are
publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miles_R/0/1/0/all/0/1&quot;&gt;Roy Miles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mikolajczyk_K/0/1/0/all/0/1&quot;&gt;Krystian Mikolajczyk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.11611">
<title>Out of Thin Air: Exploring Data-Free Adversarial Robustness Distillation. (arXiv:2303.11611v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.11611</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial Robustness Distillation (ARD) is a promising task to solve the
issue of limited adversarial robustness of small capacity models while
optimizing the expensive computational costs of Adversarial Training (AT).
Despite the good robust performance, the existing ARD methods are still
impractical to deploy in natural high-security scenes due to these methods rely
entirely on original or publicly available data with a similar distribution. In
fact, these data are almost always private, specific, and distinctive for
scenes that require high robustness. To tackle these issues, we propose a
challenging but significant task called Data-Free Adversarial Robustness
Distillation (DFARD), which aims to train small, easily deployable, robust
models without relying on data. We demonstrate that the challenge lies in the
lower upper bound of knowledge transfer information, making it crucial to
mining and transferring knowledge more efficiently. Inspired by human
education, we design a plug-and-play Interactive Temperature Adjustment (ITA)
strategy to improve the efficiency of knowledge transfer and propose an
Adaptive Generator Balance (AGB) module to retain more data information. Our
method uses adaptive hyperparameters to avoid a large number of parameter
tuning, which significantly outperforms the combination of existing techniques.
Meanwhile, our method achieves stable and reliable performance on multiple
benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuzheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhaoyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Dingkang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1&quot;&gt;Pinxue Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1&quot;&gt;Kaixun Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenqiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1&quot;&gt;Lizhe Qi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.11728">
<title>Few-shot Neural Radiance Fields Under Unconstrained Illumination. (arXiv:2303.11728v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.11728</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce a new challenge for synthesizing novel view
images in practical environments with limited input multi-view images and
varying lighting conditions. Neural radiance fields (NeRF), one of the
pioneering works for this task, demand an extensive set of multi-view images
taken under constrained illumination, which is often unattainable in real-world
settings. While some previous works have managed to synthesize novel views
given images with different illumination, their performance still relies on a
substantial number of input multi-view images. To address this problem, we
suggest ExtremeNeRF, which utilizes multi-view albedo consistency, supported by
geometric alignment. Specifically, we extract intrinsic image components that
should be illumination-invariant across different views, enabling direct
appearance comparison between the input and novel view under unconstrained
illumination. We offer thorough experimental results for task evaluation,
employing the newly created NeRF Extreme benchmark-the first in-the-wild
benchmark for novel view synthesis under multiple viewing directions and
varying illuminations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;SeokYeong Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;JunYong Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seungryong Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_I/0/1/0/all/0/1&quot;&gt;Ig-Jae Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1&quot;&gt;Junghyun Cho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.12787">
<title>EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation. (arXiv:2303.12787v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.12787</link>
<description rdf:parseType="Literal">&lt;p&gt;Locating 3D objects from a single RGB image via Perspective-n-Point (PnP) is
a long-standing problem in computer vision. Driven by end-to-end deep learning,
recent studies suggest interpreting PnP as a differentiable layer, allowing for
partial learning of 2D-3D point correspondences by backpropagating the
gradients of pose loss. Yet, learning the entire correspondences from scratch
is highly challenging, particularly for ambiguous pose solutions, where the
globally optimal pose is theoretically non-differentiable w.r.t. the points. In
this paper, we propose the EPro-PnP, a probabilistic PnP layer for general
end-to-end pose estimation, which outputs a distribution of pose with
differentiable probability density on the SE(3) manifold. The 2D-3D coordinates
and corresponding weights are treated as intermediate variables learned by
minimizing the KL divergence between the predicted and target pose
distribution. The underlying principle generalizes previous approaches, and
resembles the attention mechanism. EPro-PnP can enhance existing correspondence
networks, closing the gap between PnP-based method and the task-specific
leaders on the LineMOD 6DoF pose estimation benchmark. Furthermore, EPro-PnP
helps to explore new possibilities of network design, as we demonstrate a novel
deformable correspondence network with the state-of-the-art pose accuracy on
the nuScenes 3D object detection benchmark. Our code is available at
https://github.com/tjiiv-cprg/EPro-PnP-v2.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hansheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_W/0/1/0/all/0/1&quot;&gt;Wei Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Pichao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1&quot;&gt;Lu Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hao Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.13269">
<title>Disguise without Disruption: Utility-Preserving Face De-Identification. (arXiv:2303.13269v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.13269</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rise of cameras and smart sensors, humanity generates an exponential
amount of data. This valuable information, including underrepresented cases
like AI in medical settings, can fuel new deep-learning tools. However, data
scientists must prioritize ensuring privacy for individuals in these untapped
datasets, especially for images or videos with faces, which are prime targets
for identification methods. Proposed solutions to de-identify such images often
compromise non-identifying facial attributes relevant to downstream tasks. In
this paper, we introduce Disguise, a novel algorithm that seamlessly
de-identifies facial images while ensuring the usability of the modified data.
Unlike previous approaches, our solution is firmly grounded in the domains of
differential privacy and ensemble-learning research. Our method involves
extracting and substituting depicted identities with synthetic ones, generated
using variational mechanisms to maximize obfuscation and non-invertibility.
Additionally, we leverage supervision from a mixture-of-experts to disentangle
and preserve other utility attributes. We extensively evaluate our method using
multiple datasets, demonstrating a higher de-identification rate and superior
consistency compared to prior approaches in various downstream tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1&quot;&gt;Zikui Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1&quot;&gt;Zhongpai Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Planche_B/0/1/0/all/0/1&quot;&gt;Benjamin Planche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1&quot;&gt;Meng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Terrence Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asif_M/0/1/0/all/0/1&quot;&gt;M. Salman Asif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Ziyan Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.17561">
<title>SoftCLIP: Softer Cross-modal Alignment Makes CLIP Stronger. (arXiv:2303.17561v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.17561</link>
<description rdf:parseType="Literal">&lt;p&gt;During the preceding biennium, vision-language pre-training has achieved
noteworthy success on several downstream tasks. Nevertheless, acquiring
high-quality image-text pairs, where the pairs are entirely exclusive of each
other, remains a challenging task, and noise exists in the commonly used
datasets. To address this issue, we propose SoftCLIP, a novel approach that
relaxes the strict one-to-one constraint and achieves a soft cross-modal
alignment by introducing a softened target, which is generated from the
fine-grained intra-modal self-similarity. The intra-modal guidance is
indicative to enable two pairs have some local similarities and model
many-to-many relationships between the two modalities. Besides, since the
positive still dominates in the softened target distribution, we disentangle
the negatives in the distribution to further boost the relation alignment with
the negatives in the cross-modal learning. Extensive experiments demonstrate
the effectiveness of SoftCLIP. In particular, on ImageNet zero-shot
classification task, using CC3M/CC12M as pre-training dataset, SoftCLIP brings
a top-1 accuracy improvement of 6.8%/7.2% over the CLIP baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yuting Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jinfeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zihan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tong Wu Enwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jie Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Ke Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xing Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.17594">
<title>MobileInst: Video Instance Segmentation on the Mobile. (arXiv:2303.17594v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.17594</link>
<description rdf:parseType="Literal">&lt;p&gt;Video instance segmentation on mobile devices is an important yet very
challenging edge AI problem. It mainly suffers from (1) heavy computation and
memory costs for frame-by-frame pixel-level instance perception and (2)
complicated heuristics for tracking objects. To address those issues, we
present MobileInst, a lightweight and mobile-friendly framework for video
instance segmentation on mobile devices. Firstly, MobileInst adopts a mobile
vision transformer to extract multi-level semantic features and presents an
efficient query-based dual-transformer instance decoder for mask kernels and a
semantic-enhanced mask decoder to generate instance segmentation per frame.
Secondly, MobileInst exploits simple yet effective kernel reuse and kernel
association to track objects for video instance segmentation. Further, we
propose temporal query passing to enhance the tracking ability for kernels. We
conduct experiments on COCO and YouTube-VIS datasets to demonstrate the
superiority of MobileInst and evaluate the inference latency on one single CPU
core of Snapdragon 778G Mobile Platform, without other methods of acceleration.
On the COCO dataset, MobileInst achieves 31.2 mask AP and 433 ms on the mobile
CPU, which reduces the latency by 50% compared to the previous SOTA. For video
instance segmentation, MobileInst achieves 35.0 AP on YouTube-VIS 2019 and 30.1
AP on YouTube-VIS 2021. Code will be available to facilitate real-world
applications and future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Renhong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_T/0/1/0/all/0/1&quot;&gt;Tianheng Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shusheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Haoyi Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shuai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1&quot;&gt;Jiancheng Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ying_X/0/1/0/all/0/1&quot;&gt;Xiaowen Ying&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1&quot;&gt;Dashan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wenyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinggang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.17743">
<title>FairGen: Towards Fair Graph Generation. (arXiv:2303.17743v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.17743</link>
<description rdf:parseType="Literal">&lt;p&gt;There have been tremendous efforts over the past decades dedicated to the
generation of realistic graphs in a variety of domains, ranging from social
networks to computer networks, from gene regulatory networks to online
transaction networks. Despite the remarkable success, the vast majority of
these works are unsupervised in nature and are typically trained to minimize
the expected graph reconstruction loss, which would result in the
representation disparity issue in the generated graphs, i.e., the protected
groups (often minorities) contribute less to the objective and thus suffer from
systematically higher errors. In this paper, we aim to tailor graph generation
to downstream mining tasks by leveraging label information and user-preferred
parity constraints. In particular, we start from the investigation of
representation disparity in the context of graph generative models. To mitigate
the disparity, we propose a fairness-aware graph generative model named
FairGen. Our model jointly trains a label-informed graph generation module and
a fair representation learning module by progressively learning the behaviors
of the protected and unprotected groups, from the `easy&apos; concepts to the `hard&apos;
ones. In addition, we propose a generic context sampling strategy for graph
generative models, which is proven to be capable of fairly capturing the
contextual information of each group with a high probability. Experimental
results on seven real-world data sets, including web-based graphs, demonstrate
that FairGen (1) obtains performance on par with state-of-the-art graph
generative models across nine network properties, (2) mitigates the
representation disparity issues in the generated graphs, and (3) substantially
boosts the model performance by up to 17% in downstream tasks via data
augmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1&quot;&gt;Lecheng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1&quot;&gt;Dawei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1&quot;&gt;Hanghang Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiejun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yada Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jingrui He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.00242">
<title>GLT-T++: Global-Local Transformer for 3D Siamese Tracking with Ranking Loss. (arXiv:2304.00242v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.00242</link>
<description rdf:parseType="Literal">&lt;p&gt;Siamese trackers based on 3D region proposal network (RPN) have shown
remarkable success with deep Hough voting. However, using a single seed point
feature as the cue for voting fails to produce high-quality 3D proposals.
Additionally, the equal treatment of seed points in the voting process,
regardless of their significance, exacerbates this limitation. To address these
challenges, we propose a novel transformer-based voting scheme to generate
better proposals. Specifically, a global-local transformer (GLT) module is
devised to integrate object- and patch-aware geometric priors into seed point
features, resulting in robust and accurate cues for offset learning of seed
points. To train the GLT module, we introduce an importance prediction branch
that learns the potential importance weights of seed points as a training
constraint. Incorporating this transformer-based voting scheme into 3D RPN, a
novel Siamese method dubbed GLT-T is developed for 3D single object tracking on
point clouds. Moreover, we identify that the highest-scored proposal in the
Siamese paradigm may not be the most accurate proposal, which limits tracking
performance. Towards this concern, we approach the binary score prediction task
as a ranking problem, and design a target-aware ranking loss and a
localization-aware ranking loss to produce accurate ranking of proposals. With
the ranking losses, we further present GLT-T++, an enhanced version of GLT-T.
Extensive experiments on multiple benchmarks demonstrate that our GLT-T and
GLT-T++ outperform state-of-the-art methods in terms of tracking accuracy while
maintaining a real-time inference speed. The source code will be made available
at https://github.com/haooozi/GLT-T.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1&quot;&gt;Jiahao Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zhiwei He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuxiang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1&quot;&gt;Xudong Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1&quot;&gt;Mingyu Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.01168">
<title>DeepAccident: A Motion and Accident Prediction Benchmark for V2X Autonomous Driving. (arXiv:2304.01168v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.01168</link>
<description rdf:parseType="Literal">&lt;p&gt;Safety is the primary priority of autonomous driving. Nevertheless, no
published dataset currently supports the direct and explainable safety
evaluation for autonomous driving. In this work, we propose DeepAccident, a
large-scale dataset generated via a realistic simulator containing diverse
accident scenarios that frequently occur in real-world driving. The proposed
DeepAccident dataset includes 57K annotated frames and 285K annotated samples,
approximately 7 times more than the large-scale nuScenes dataset with 40k
annotated samples. In addition, we propose a new task, end-to-end motion and
accident prediction, which can be used to directly evaluate the accident
prediction ability for different autonomous driving algorithms. Furthermore,
for each scenario, we set four vehicles along with one infrastructure to record
data, thus providing diverse viewpoints for accident scenarios and enabling V2X
(vehicle-to-everything) research on perception and prediction tasks. Finally,
we present a baseline V2X model named V2XFormer that demonstrates superior
performance for motion and accident prediction and 3D object detection compared
to the single-vehicle model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sukmin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1&quot;&gt;Wenxuan Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1&quot;&gt;Enze Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_C/0/1/0/all/0/1&quot;&gt;Chongjian Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junsong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenguo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.08796">
<title>Deep Unrestricted Document Image Rectification. (arXiv:2304.08796v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.08796</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, tremendous efforts have been made on document image
rectification, but existing advanced algorithms are limited to processing
restricted document images, i.e., the input images must incorporate a complete
document. Once the captured image merely involves a local text region, its
rectification quality is degraded and unsatisfactory. Our previously proposed
DocTr, a transformer-assisted network for document image rectification, also
suffers from this limitation. In this work, we present DocTr++, a novel unified
framework for document image rectification, without any restrictions on the
input distorted images. Our major technical improvements can be concluded in
three aspects. Firstly, we upgrade the original architecture by adopting a
hierarchical encoder-decoder structure for multi-scale representation
extraction and parsing. Secondly, we reformulate the pixel-wise mapping
relationship between the unrestricted distorted document images and the
distortion-free counterparts. The obtained data is used to train our DocTr++
for unrestricted document image rectification. Thirdly, we contribute a
real-world test set and metrics applicable for evaluating the rectification
quality. To our best knowledge, this is the first learning-based method for the
rectification of unrestricted document images. Extensive experiments are
conducted, and the results demonstrate the effectiveness and superiority of our
method. We hope our DocTr++ will serve as a strong baseline for generic
document image rectification, prompting the further advancement and application
of learning-based algorithms. The source code and the proposed dataset are
publicly available at https://github.com/fh2019ustc/DocTr-Plus.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1&quot;&gt;Hao Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shaokai Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1&quot;&gt;Jiajun Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wengang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Houqiang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.04926">
<title>RelPose++: Recovering 6D Poses from Sparse-view Observations. (arXiv:2305.04926v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.04926</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the task of estimating 6D camera poses from sparse-view image sets
(2-8 images). This task is a vital pre-processing stage for nearly all
contemporary (neural) reconstruction algorithms but remains challenging given
sparse views, especially for objects with visual symmetries and texture-less
surfaces. We build on the recent RelPose framework which learns a network that
infers distributions over relative rotations over image pairs. We extend this
approach in two key ways; first, we use attentional transformer layers to
process multiple images jointly, since additional views of an object may
resolve ambiguous symmetries in any given image pair (such as the handle of a
mug that becomes visible in a third view). Second, we augment this network to
also report camera translations by defining an appropriate coordinate system
that decouples the ambiguity in rotation estimation from translation
prediction. Our final system results in large improvements in 6D pose
prediction over prior art on both seen and unseen object categories and also
enables pose estimation and 3D reconstruction for in-the-wild objects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_A/0/1/0/all/0/1&quot;&gt;Amy Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jason Y. Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1&quot;&gt;Deva Ramanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tulsiani_S/0/1/0/all/0/1&quot;&gt;Shubham Tulsiani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.06252">
<title>Embedded Feature Similarity Optimization with Specific Parameter Initialization for 2D/3D Medical Image Registration. (arXiv:2305.06252v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.06252</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel deep learning-based framework: Embedded Feature Similarity
Optimization with Specific Parameter Initialization (SOPI) for 2D/3D medical
image registration which is a most challenging problem due to the difficulty
such as dimensional mismatch, heavy computation load and lack of golden
evaluation standard. The framework we design includes a parameter specification
module to efficiently choose initialization pose parameter and a
fine-registration module to align images. The proposed framework takes
extracting multi-scale features into consideration using a novel composite
connection encoder with special training techniques. We compare the method with
both learning-based methods and optimization-based methods on a in-house
CT/X-ray dataset as well as simulated data to further evaluate performance. Our
experiments demonstrate that the method in this paper has improved the
registration performance, and thereby outperforms the existing methods in terms
of accuracy and running time. We also show the potential of the proposed method
as an initial pose estimator. The code is available at
https://github.com/m1nhengChen/SOPI
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Minheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhirun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1&quot;&gt;Shuheng Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1&quot;&gt;Youyong Kong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.07490">
<title>ArtGPT-4: Towards Artistic-understanding Large Vision-Language Models with Enhanced Adapter. (arXiv:2305.07490v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.07490</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, advancements in large language models have been remarkable,
with models such as ChatGPT demonstrating exceptional proficiency in diverse
linguistic tasks. The pre-training of large models with billions of parameters,
poses a formidable challenge, primarily due to the scarcity of datasets of a
commensurate scale for effective training. Nevertheless, innovative strategies
have emerged, including methods to fine-tune these pre-trained models using
fewer parameters set, as evidenced by models like MiniGPT-4 and LLaVA. Despite
their potential in various domains, these models remain limited in their
understanding of artistic imagery. They have yet to fully grasp the intricate
nuances of art images or to provide an objective articulation of the emotions
they evoke, in a manner akin to human perception. This work introduces
ArtGPT-4, a pioneering large vision-language model tailored to address the
deficiencies of contemporary models in artistic comprehension. ArtGPT-4
underwent training on image-text pairs utilizing a Tesla A100 device in a mere
2 hours, with a dataset comprising approximately 0.52M entries. Impressively,
the model can render images with an artistic-understanding and convey the
emotions they inspire, mirroring human interpretation. Additionally, this work
presents a unique dataset designed to evaluate the efficacy of vision-language
models. In subsequent evaluations, ArtGPT-4 not only achieved state-of-the-art
performance on the ArtEmis and ArtEmis-v2.0 datasets but also exceeded the
established benchmarks introduced in This study, lagging behind professional
artists&apos; descriptions by a negligible 0.15 points on a 6-point scale. The code
and the pre-trained model are accessible in
https://huggingface.co/Tyrannosaurus/ArtGPT-4.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1&quot;&gt;Zhengqing Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Lichao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1&quot;&gt;Yanyang Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10135">
<title>Controllable Mind Visual Diffusion Model. (arXiv:2305.10135v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10135</link>
<description rdf:parseType="Literal">&lt;p&gt;Brain signal visualization has emerged as an active research area, serving as
a critical interface between the human visual system and computer vision
models. Although diffusion models have shown promise in analyzing functional
magnetic resonance imaging (fMRI) data, including reconstructing high-quality
images consistent with original visual stimuli, their accuracy in extracting
semantic and silhouette information from brain signals remains limited. In this
regard, we propose a novel approach, referred to as Controllable Mind Visual
Diffusion Model (CMVDM). CMVDM extracts semantic and silhouette information
from fMRI data using attribute alignment and assistant networks. Additionally,
a residual block is incorporated to capture information beyond semantic and
silhouette features. We then leverage a control model to fully exploit the
extracted information for image synthesis, resulting in generated images that
closely resemble the visual stimuli in terms of semantics and silhouette.
Through extensive experimentation, we demonstrate that CMVDM outperforms
existing state-of-the-art methods both qualitatively and quantitatively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_B/0/1/0/all/0/1&quot;&gt;Bohan Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shanglin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xuhui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Sicheng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xiaolong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1&quot;&gt;Xu Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jianzhuang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Baochang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17423">
<title>Accelerating Text-to-image Editing via Cache-enabled Sparse Diffusion Inference. (arXiv:2305.17423v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17423</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the recent success of diffusion models, text-to-image generation is
becoming increasingly popular and achieves a wide range of applications. Among
them, text-to-image editing, or continuous text-to-image generation, attracts
lots of attention and can potentially improve the quality of generated images.
It&apos;s common to see that users may want to slightly edit the generated image by
making minor modifications to their input textual descriptions for several
rounds of diffusion inference. However, such an image editing process suffers
from the low inference efficiency of many existing diffusion models even using
GPU accelerators. To solve this problem, we introduce Fast Image Semantically
Edit (FISEdit), a cached-enabled sparse diffusion model inference engine for
efficient text-to-image editing. The key intuition behind our approach is to
utilize the semantic mapping between the minor modifications on the input text
and the affected regions on the output image. For each text editing step,
FISEdit can automatically identify the affected image regions and utilize the
cached unchanged regions&apos; feature map to accelerate the inference process.
Extensive empirical results show that FISEdit can be $3.4\times$ and
$4.4\times$ faster than existing methods on NVIDIA TITAN RTX and A100 GPUs
respectively, and even generates more satisfactory images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zihao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haoyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_F/0/1/0/all/0/1&quot;&gt;Fangcheng Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_X/0/1/0/all/0/1&quot;&gt;Xupeng Miao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1&quot;&gt;Bin Cui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17770">
<title>Point Cloud Completion Guided by Prior Knowledge via Causal Inference. (arXiv:2305.17770v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17770</link>
<description rdf:parseType="Literal">&lt;p&gt;Point cloud completion aims to recover raw point clouds captured by scanners
from partial observations caused by occlusion and limited view angles. This
makes it hard to recover details because the global feature is unlikely to
capture the full details of all missing parts. In this paper, we propose a
novel approach to point cloud completion task called Point-PC, which uses a
memory network to retrieve shape priors and designs a causal inference model to
filter missing shape information as supplemental geometric information to aid
point cloud completion. Specifically, we propose a memory operating mechanism
where the complete shape features and the corresponding shapes are stored in
the form of ``key-value&apos;&apos; pairs. To retrieve similar shapes from the partial
input, we also apply a contrastive learning-based pre-training scheme to
transfer the features of incomplete shapes into the domain of complete shape
features. Experimental results on the ShapeNet-55, PCN, and KITTI datasets
demonstrate that Point-PC outperforms the state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Songxue Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_C/0/1/0/all/0/1&quot;&gt;Chuanqi Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Ruidong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weijie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_W/0/1/0/all/0/1&quot;&gt;Weizhi Nie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.20052">
<title>Integrated Decision Gradients: Compute Your Attributions Where the Model Makes Its Decision. (arXiv:2305.20052v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.20052</link>
<description rdf:parseType="Literal">&lt;p&gt;Attribution algorithms are frequently employed to explain the decisions of
neural network models. Integrated Gradients (IG) is an influential attribution
method due to its strong axiomatic foundation. The algorithm is based on
integrating the gradients along a path from a reference image to the input
image. Unfortunately, it can be observed that gradients computed from regions
where the output logit changes minimally along the path provide poor
explanations for the model decision, which is called the saturation effect
problem. In this paper, we propose an attribution algorithm called integrated
decision gradients (IDG). The algorithm focuses on integrating gradients from
the region of the path where the model makes its decision, i.e., the portion of
the path where the output logit rapidly transitions from zero to its final
value. This is practically realized by scaling each gradient by the derivative
of the output logit with respect to the path. The algorithm thereby provides a
principled solution to the saturation problem. Additionally, we minimize the
errors within the Riemann sum approximation of the path integral by utilizing
non-uniform subdivisions determined by adaptive sampling. In the evaluation on
ImageNet, it is demonstrated that IDG outperforms IG, Left-IG, Guided IG, and
adversarial gradient integration both qualitatively and quantitatively using
standard insertion and deletion metrics across three common models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walker_C/0/1/0/all/0/1&quot;&gt;Chase Walker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1&quot;&gt;Sumit Jha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kenny Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ewetz_R/0/1/0/all/0/1&quot;&gt;Rickard Ewetz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00800">
<title>FigGen: Text to Scientific Figure Generation. (arXiv:2306.00800v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00800</link>
<description rdf:parseType="Literal">&lt;p&gt;The generative modeling landscape has experienced tremendous growth in recent
years, particularly in generating natural images and art. Recent techniques
have shown impressive potential in creating complex visual compositions while
delivering impressive realism and quality. However, state-of-the-art methods
have been focusing on the narrow domain of natural images, while other
distributions remain unexplored. In this paper, we introduce the problem of
text-to-figure generation, that is creating scientific figures of papers from
text descriptions. We present FigGen, a diffusion-based approach for
text-to-figure as well as the main challenges of the proposed task. Code and
models are available at https://github.com/joanrod/figure-diffusion
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_J/0/1/0/all/0/1&quot;&gt;Juan A Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vazquez_D/0/1/0/all/0/1&quot;&gt;David Vazquez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laradji_I/0/1/0/all/0/1&quot;&gt;Issam Laradji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedersoli_M/0/1/0/all/0/1&quot;&gt;Marco Pedersoli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_P/0/1/0/all/0/1&quot;&gt;Pau Rodriguez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04445">
<title>Multi-modal Latent Diffusion. (arXiv:2306.04445v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04445</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-modal data-sets are ubiquitous in modern applications, and multi-modal
Variational Autoencoders are a popular family of models that aim to learn a
joint representation of the different modalities. However, existing approaches
suffer from a coherence-quality tradeoff, where models with good generation
quality lack generative coherence across modalities, and vice versa. We discuss
the limitations underlying the unsatisfactory performance of existing methods,
to motivate the need for a different approach. We propose a novel method that
uses a set of independently trained, uni-modal, deterministic autoencoders.
Individual latent variables are concatenated into a common latent space, which
is fed to a masked diffusion model to enable generative modeling. We also
introduce a new multi-time training method to learn the conditional score
network for multi-modal diffusion. Our methodology substantially outperforms
competitors in both generation quality and coherence, as shown through an
extensive experimental campaign.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bounoua_M/0/1/0/all/0/1&quot;&gt;Mustapha Bounoua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Franzese_G/0/1/0/all/0/1&quot;&gt;Giulio Franzese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michiardi_P/0/1/0/all/0/1&quot;&gt;Pietro Michiardi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05144">
<title>Mesogeos: A multi-purpose dataset for data-driven wildfire modeling in the Mediterranean. (arXiv:2306.05144v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05144</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Mesogeos, a large-scale multi-purpose dataset for wildfire
modeling in the Mediterranean. Mesogeos integrates variables representing
wildfire drivers (meteorology, vegetation, human activity) and historical
records of wildfire ignitions and burned areas for 17 years (2006-2022). It is
designed as a cloud-friendly spatio-temporal dataset, namely a datacube,
harmonizing all variables in a grid of 1km x 1km x 1-day resolution. The
datacube structure offers opportunities to assess machine learning (ML) usage
in various wildfire modeling tasks. We extract two ML-ready datasets that
establish distinct tracks to demonstrate this potential: (1) short-term
wildfire danger forecasting and (2) final burned area estimation given the
point of ignition. We define appropriate metrics and baselines to evaluate the
performance of models in each track. By publishing the datacube, along with the
code to create the ML datasets and models, we encourage the community to foster
the implementation of additional tracks for mitigating the increasing threat of
wildfires in the Mediterranean.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kondylatos_S/0/1/0/all/0/1&quot;&gt;Spyros Kondylatos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prapas_I/0/1/0/all/0/1&quot;&gt;Ioannis Prapas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Camps_Valls_G/0/1/0/all/0/1&quot;&gt;Gustau Camps-Valls&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papoutsis_I/0/1/0/all/0/1&quot;&gt;Ioannis Papoutsis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06963">
<title>Feature Fusion from Head to Tail for Long-Tailed Visual Recognition. (arXiv:2306.06963v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06963</link>
<description rdf:parseType="Literal">&lt;p&gt;The imbalanced distribution of long-tailed data presents a considerable
challenge for deep learning models, as it causes them to prioritize the
accurate classification of head classes but largely disregard tail classes. The
biased decision boundary caused by inadequate semantic information in tail
classes is one of the key factors contributing to their low recognition
accuracy. To rectify this issue, we propose to augment tail classes by grafting
the diverse semantic information from head classes, referred to as head-to-tail
fusion (H2T). We replace a portion of feature maps from tail classes with those
belonging to head classes. These fused features substantially enhance the
diversity of tail classes. Both theoretical analysis and practical
experimentation demonstrate that H2T can contribute to a more optimized
solution for the decision boundary. We seamlessly integrate H2T in the
classifier adjustment stage, making it a plug-and-play module. Its simplicity
and ease of implementation allow for smooth integration with existing
long-tailed recognition methods, facilitating a further performance boost.
Extensive experiments on various long-tailed benchmarks demonstrate the
effectiveness of the proposed H2T. The source code is available at
https://github.com/Keke921/H2T.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mengke Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zhikai Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yang Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_W/0/1/0/all/0/1&quot;&gt;Weichao Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheung_Y/0/1/0/all/0/1&quot;&gt;Yiu-ming Cheung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Hui Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08707">
<title>VidEdit: Zero-Shot and Spatially Aware Text-Driven Video Editing. (arXiv:2306.08707v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08707</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, diffusion-based generative models have achieved remarkable success
for image generation and edition. However, their use for video editing still
faces important limitations. This paper introduces VidEdit, a novel method for
zero-shot text-based video editing ensuring strong temporal and spatial
consistency. Firstly, we propose to combine atlas-based and pre-trained
text-to-image diffusion models to provide a training-free and efficient editing
method, which by design fulfills temporal smoothness. Secondly, we leverage
off-the-shelf panoptic segmenters along with edge detectors and adapt their use
for conditioned diffusion-based atlas editing. This ensures a fine spatial
control on targeted regions while strictly preserving the structure of the
original video. Quantitative and qualitative experiments show that VidEdit
outperforms state-of-the-art methods on DAVIS dataset, regarding semantic
faithfulness, image preservation, and temporal consistency metrics. With this
framework, processing a single video only takes approximately one minute, and
it can generate multiple compatible edits based on a unique text prompt.
Project web-page at https://videdit.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Couairon_P/0/1/0/all/0/1&quot;&gt;Paul Couairon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rambour_C/0/1/0/all/0/1&quot;&gt;Cl&amp;#xe9;ment Rambour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haugeard_J/0/1/0/all/0/1&quot;&gt;Jean-Emmanuel Haugeard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thome_N/0/1/0/all/0/1&quot;&gt;Nicolas Thome&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.12153">
<title>DIAS: A Dataset and Benchmark for Intracranial Artery Segmentation in DSA sequences. (arXiv:2306.12153v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.12153</link>
<description rdf:parseType="Literal">&lt;p&gt;Digital subtraction angiography (DSA) is universally acknowledged as the gold
standard for examining lesion angioarchitecture, elucidating arterial blood
supply dynamics, and guiding endovascular interventions. The automatic
segmentation of intracranial arteries (IA) in DSA, which is pivotal for
quantifying vascular morphology, plays an essential role in computer-assisted
stroke research and clinical practices. Nevertheless, research in this specific
domain remains constrained, primarily owing to the unavailability of publicly
datasets for IA segmentation within the research community. Currently, the
predominant focus of methodologies lies in the segmentation of single-frame DSA
using in-house datasets. These methods, limited by the partial inclusion of
contrast in single-frame DSA, encounters challenges in rendering a precise
representation of vascular structures. In this paper, we introduces DIAS, a
dataset specifically developed for IA segmentation in DSA sequences. A
comprehensive benchmark has been established for evaluating DIAS, covering
fully, weakly, and semi-supervised segmentation methods. Specifically, we
propose a vessel sequence segmentation network that captures the spatiotemporal
representation of intravascular contrast for segmenting vessels in DSA
sequences. For weakly-supervised learning, we propose a novel scribble
learning-based image segmentation framework, incorporating both scribble
supervision and consistency regularization. Furthermore, we introduce a random
patch-based self-training framework that harnesses unlabeled DSA sequences to
improve segmentation performance. Our extensive experiments on the DIAS dataset
demonstrate the effectiveness of these methods as potential baselines for
future research and clinical applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wentao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tian_T/0/1/0/all/0/1&quot;&gt;Tong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lemeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Weijin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haoyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wenyi Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tian_S/0/1/0/all/0/1&quot;&gt;Siyu Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pan_X/0/1/0/all/0/1&quot;&gt;Xipeng Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Huihua Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gao_F/0/1/0/all/0/1&quot;&gt;Feng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yiming Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Su_R/0/1/0/all/0/1&quot;&gt;Ruisheng Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15521">
<title>What a MESS: Multi-Domain Evaluation of Zero-Shot Semantic Segmentation. (arXiv:2306.15521v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15521</link>
<description rdf:parseType="Literal">&lt;p&gt;While semantic segmentation has seen tremendous improvements in the past,
there are still significant labeling efforts necessary and the problem of
limited generalization to classes that have not been present during training.
To address this problem, zero-shot semantic segmentation makes use of large
self-supervised vision-language models, allowing zero-shot transfer to unseen
classes. In this work, we build a benchmark for Multi-domain Evaluation of
Semantic Segmentation (MESS), which allows a holistic analysis of performance
across a wide range of domain-specific datasets such as medicine, engineering,
earth monitoring, biology, and agriculture. To do this, we reviewed 120
datasets, developed a taxonomy, and classified the datasets according to the
developed taxonomy. We select a representative subset consisting of 22 datasets
and propose it as the MESS benchmark. We evaluate eight recently published
models on the proposed MESS benchmark and analyze characteristics for the
performance of zero-shot transfer models. The toolkit is available at
https://github.com/blumenstiel/MESS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blumenstiel_B/0/1/0/all/0/1&quot;&gt;Benedikt Blumenstiel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jakubik_J/0/1/0/all/0/1&quot;&gt;Johannes Jakubik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuhne_H/0/1/0/all/0/1&quot;&gt;Hilde K&amp;#xfc;hne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vossing_M/0/1/0/all/0/1&quot;&gt;Michael V&amp;#xf6;ssing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00371">
<title>Learning Content-enhanced Mask Transformer for Domain Generalized Urban-Scene Segmentation. (arXiv:2307.00371v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00371</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain-generalized urban-scene semantic segmentation (USSS) aims to learn
generalized semantic predictions across diverse urban-scene styles. Unlike
domain gap challenges, USSS is unique in that the semantic categories are often
similar in different urban scenes, while the styles can vary significantly due
to changes in urban landscapes, weather conditions, lighting, and other
factors. Existing approaches typically rely on convolutional neural networks
(CNNs) to learn the content of urban scenes.
&lt;/p&gt;
&lt;p&gt;In this paper, we propose a Content-enhanced Mask TransFormer (CMFormer) for
domain-generalized USSS. The main idea is to enhance the focus of the
fundamental component, the mask attention mechanism, in Transformer
segmentation models on content information. To achieve this, we introduce a
novel content-enhanced mask attention mechanism. It learns mask queries from
both the image feature and its down-sampled counterpart, as lower-resolution
image features usually contain more robust content information and are less
sensitive to style variations. These features are fused into a Transformer
decoder and integrated into a multi-resolution content-enhanced mask attention
learning scheme.
&lt;/p&gt;
&lt;p&gt;Extensive experiments conducted on various domain-generalized urban-scene
segmentation datasets demonstrate that the proposed CMFormer significantly
outperforms existing CNN-based methods for domain-generalized semantic
segmentation, achieving improvements of up to 14.00\% in terms of mIoU (mean
intersection over union). The source code is publicly available at
\url{https://github.com/BiQiWHU/CMFormer}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_Q/0/1/0/all/0/1&quot;&gt;Qi Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1&quot;&gt;Shaodi You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gevers_T/0/1/0/all/0/1&quot;&gt;Theo Gevers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01003">
<title>Visual Instruction Tuning with Polite Flamingo. (arXiv:2307.01003v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01003</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent research has demonstrated that the multi-task fine-tuning of
multi-modal Large Language Models (LLMs) using an assortment of annotated
downstream vision-language datasets significantly enhances their performance.
Yet, during this process, a side effect, which we termed as the &quot;multi-modal
alignment tax&quot;, surfaces. This side effect negatively impacts the model&apos;s
ability to format responses appropriately -- for instance, its &quot;politeness&quot; --
due to the overly succinct and unformatted nature of raw annotations, resulting
in reduced human preference. In this paper, we introduce Polite Flamingo, a
multi-modal response rewriter that transforms raw annotations into a more
appealing, &quot;polite&quot; format. Polite Flamingo is trained to reconstruct
high-quality responses from their automatically distorted counterparts and is
subsequently applied to a vast array of vision-language datasets for response
rewriting. After rigorous filtering, we generate the PF-1M dataset and further
validate its value by fine-tuning a multi-modal LLM with it. Combined with
novel methodologies including U-shaped multi-stage tuning and multi-turn
augmentation, the resulting model, Clever Flamingo, demonstrates its advantages
in both multi-modal understanding and response politeness according to
automated and human evaluations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Delong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jianfeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1&quot;&gt;Wenliang Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Baoyuan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01146">
<title>AVSegFormer: Audio-Visual Segmentation with Transformer. (arXiv:2307.01146v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01146</link>
<description rdf:parseType="Literal">&lt;p&gt;The combination of audio and vision has long been a topic of interest in the
multi-modal community. Recently, a new audio-visual segmentation (AVS) task has
been introduced, aiming to locate and segment the sounding objects in a given
video. This task demands audio-driven pixel-level scene understanding for the
first time, posing significant challenges. In this paper, we propose
AVSegFormer, a novel framework for AVS tasks that leverages the transformer
architecture. Specifically, we introduce audio queries and learnable queries
into the transformer decoder, enabling the network to selectively attend to
interested visual features. Besides, we present an audio-visual mixer, which
can dynamically adjust visual features by amplifying relevant and suppressing
irrelevant spatial channels. Additionally, we devise an intermediate mask loss
to enhance the supervision of the decoder, encouraging the network to produce
more accurate intermediate predictions. Extensive experiments demonstrate that
AVSegFormer achieves state-of-the-art results on the AVS benchmark. The code is
available at https://github.com/vvvb-github/AVSegFormer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Shengyi Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhe Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenhai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1&quot;&gt;Tong Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03856">
<title>Novel Categories Discovery Via Constraints on Empirical Prediction Statistics. (arXiv:2307.03856v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03856</link>
<description rdf:parseType="Literal">&lt;p&gt;Novel Categories Discovery (NCD) aims to cluster novel data based on the
class semantics of known classes using the open-world partial class space
annotated dataset. As an alternative to the traditional pseudo-labeling-based
approaches, we leverage the connection between the data sampling and the
provided multinoulli (categorical) distribution of novel classes. We introduce
constraints on individual and collective statistics of predicted novel class
probabilities to implicitly achieve semantic-based clustering. More
specifically, we align the class neuron activation distributions under
Monte-Carlo sampling of novel classes in large batches by matching their
empirical first-order (mean) and second-order (covariance) statistics with the
multinoulli distribution of the labels while applying instance information
constraints and prediction consistency under label-preserving augmentations. We
then explore a directional statistics-based probability formation that learns
the mixture of Von Mises-Fisher distribution of class labels in a unit
hypersphere. We demonstrate the discriminative ability of our approach to
realize semantic clustering of novel samples in image, video, and time-series
modalities. We perform extensive ablation studies regarding data, networks, and
framework components to provide better insights. Our approach maintains 94%,
93%, 85%, and 93% (approx.) classification accuracy in labeled data while
achieving 90%, 84%, 72%, and 75% (approx.) clustering accuracy for novel
categories in Cifar10, UCF101, MPSC-ARL, and SHAR datasets that match
state-of-the-art approaches without any external clustering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_Z/0/1/0/all/0/1&quot;&gt;Zahid Hasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faridee_A/0/1/0/all/0/1&quot;&gt;Abu Zaher Md Faridee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_M/0/1/0/all/0/1&quot;&gt;Masud Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Purushotham_S/0/1/0/all/0/1&quot;&gt;Sanjay Purushotham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1&quot;&gt;Heesung Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hyungtae Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_N/0/1/0/all/0/1&quot;&gt;Nirmalya Roy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03998">
<title>Lightweight Improved Residual Network for Efficient Inverse Tone Mapping. (arXiv:2307.03998v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03998</link>
<description rdf:parseType="Literal">&lt;p&gt;The display devices like HDR10 televisions are increasingly prevalent in our
daily life for visualizing high dynamic range (HDR) images. But the majority of
media images on the internet remain in 8-bit standard dynamic range (SDR)
format. Therefore, converting SDR images to HDR ones by inverse tone mapping
(ITM) is crucial to unlock the full potential of abundant media images.
However, existing ITM methods are usually developed with complex network
architectures requiring huge computational costs. In this paper, we propose a
lightweight Improved Residual Network (IRNet) by enhancing the power of popular
residual block for efficient ITM. Specifically, we propose a new Improved
Residual Block (IRB) to extract and fuse multi-layer features for fine-grained
HDR image reconstruction. Experiments on three benchmark datasets demonstrate
that our IRNet achieves state-of-the-art performance on both the ITM and joint
SR-ITM tasks. The code, models and data will be publicly available at
https://github.com/ThisisVikki/ITM-baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1&quot;&gt;Liqi Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Tianyi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yongbao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1&quot;&gt;Xiantong Zhen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jun Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05845">
<title>PIGEON: Predicting Image Geolocations. (arXiv:2307.05845v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05845</link>
<description rdf:parseType="Literal">&lt;p&gt;Planet-scale image geolocalization remains a challenging problem due to the
diversity of images originating from anywhere in the world. Although approaches
based on vision transformers have made significant progress in geolocalization
accuracy, success in prior literature is constrained to narrow distributions of
images of landmarks, and performance has not generalized to unseen places. We
present a new geolocalization system that combines semantic geocell creation,
multi-task contrastive pretraining, and a novel loss function. Additionally,
our work is the first to perform retrieval over location clusters for guess
refinements. We train two models for evaluations on street-level data and
general-purpose image geolocalization; the first model, PIGEON, is trained on
data from the game of Geoguessr and is capable of placing over 40% of its
guesses within 25 kilometers of the target location globally. We also develop a
bot and deploy PIGEON in a blind experiment against humans, ranking in the top
0.01% of players. We further challenge one of the world&apos;s foremost professional
Geoguessr players to a series of six matches with millions of viewers, winning
all six games. Our second model, PIGEOTTO, differs in that it is trained on a
dataset of images from Flickr and Wikipedia, achieving state-of-the-art results
on a wide range of image geolocalization benchmarks, outperforming the previous
SOTA by up to 7.7 percentage points on the city accuracy level and up to 38.8
percentage points on the country level. Our findings suggest that PIGEOTTO is
the first image geolocalization model that effectively generalizes to unseen
places and that our approach can pave the way for highly accurate, planet-scale
image geolocalization systems. Our code is available on GitHub.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haas_L/0/1/0/all/0/1&quot;&gt;Lukas Haas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skreta_M/0/1/0/all/0/1&quot;&gt;Michal Skreta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alberti_S/0/1/0/all/0/1&quot;&gt;Silas Alberti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1&quot;&gt;Chelsea Finn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08506">
<title>Does Visual Pretraining Help End-to-End Reasoning?. (arXiv:2307.08506v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08506</link>
<description rdf:parseType="Literal">&lt;p&gt;We aim to investigate whether end-to-end learning of visual reasoning can be
achieved with general-purpose neural networks, with the help of visual
pretraining. A positive result would refute the common belief that explicit
visual abstraction (e.g. object detection) is essential for compositional
generalization on visual reasoning, and confirm the feasibility of a neural
network &quot;generalist&quot; to solve visual recognition and reasoning tasks. We
propose a simple and general self-supervised framework which &quot;compresses&quot; each
video frame into a small set of tokens with a transformer network, and
reconstructs the remaining frames based on the compressed temporal context. To
minimize the reconstruction loss, the network must learn a compact
representation for each image, as well as capture temporal dynamics and object
permanence from temporal context. We perform evaluation on two visual reasoning
benchmarks, CATER and ACRE. We observe that pretraining is essential to achieve
compositional generalization for end-to-end visual reasoning. Our proposed
framework outperforms traditional supervised pretraining, including image
classification and explicit object detection, by large margins.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1&quot;&gt;Chen Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1&quot;&gt;Calvin Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xingyi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arnab_A/0/1/0/all/0/1&quot;&gt;Anurag Arnab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1&quot;&gt;Cordelia Schmid&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15421">
<title>MLIC++: Linear Complexity Attention-based Multi-Reference Entropy Modeling for Learned Image Compression. (arXiv:2307.15421v4 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.15421</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, learned image compression has achieved impressive performance. The
entropy model, which estimates the distribution of the latent representation,
plays a crucial role in enhancing rate-distortion performance. However,
existing global context modules rely on computationally intensive quadratic
complexity computations to capture global correlations. This quadratic
complexity imposes limitations on the potential of high-resolution image
coding. Moreover, effectively capturing local, global, and channel-wise
contexts with acceptable even linear complexity within a single entropy model
remains a challenge. To address these limitations, we propose the Linear
Complexity Attention-based Multi-Reference Entropy Model (MEM++). MEM++
effectively captures the diverse range of correlations inherent in the latent
representation. Specifically, the latent representation is first divided into
multiple slices. When compressing a particular slice, the previously compressed
slices serve as its channel-wise contexts. To capture local contexts without
sacrificing performance, we introduce a novel checkerboard attention module.
Additionally, to capture global contexts, we propose the linear complexity
attention-based global correlations capturing by leveraging the decomposition
of the softmax operation. The attention map of the previously decoded slice is
implicitly computed and employed to predict global correlations in the current
slice. Based on MEM++, we propose image compression model MLIC++. Extensive
experimental evaluations demonstrate that our MLIC++ achieves state-of-the-art
performance, reducing BD-rate by 13.39% on the Kodak dataset compared to
VTM-17.0 in PSNR. Furthermore, MLIC++ exhibits linear GPU memory consumption
with resolution, making it highly suitable for high-resolution image coding.
Code and pre-trained models are available at
https://github.com/JiangWeibeta/MLIC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jiang_W/0/1/0/all/0/1&quot;&gt;Wei Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jiayu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhai_Y/0/1/0/all/0/1&quot;&gt;Yongqi Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gao_F/0/1/0/all/0/1&quot;&gt;Feng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ronggang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02669">
<title>ConceptLab: Creative Concept Generation using VLM-Guided Diffusion Prior Constraints. (arXiv:2308.02669v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.02669</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent text-to-image generative models have enabled us to transform our words
into vibrant, captivating imagery. The surge of personalization techniques that
has followed has also allowed us to imagine unique concepts in new scenes.
However, an intriguing question remains: How can we generate a new, imaginary
concept that has never been seen before? In this paper, we present the task of
creative text-to-image generation, where we seek to generate new members of a
broad category (e.g., generating a pet that differs from all existing pets). We
leverage the under-studied Diffusion Prior models and show that the creative
generation problem can be formulated as an optimization process over the output
space of the diffusion prior, resulting in a set of &quot;prior constraints&quot;. To
keep our generated concept from converging into existing members, we
incorporate a question-answering Vision-Language Model (VLM) that adaptively
adds new constraints to the optimization problem, encouraging the model to
discover increasingly more unique creations. Finally, we show that our prior
constraints can also serve as a strong mixing mechanism allowing us to create
hybrids between generated concepts, introducing even more flexibility into the
creative process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richardson_E/0/1/0/all/0/1&quot;&gt;Elad Richardson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1&quot;&gt;Kfir Goldberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alaluf_Y/0/1/0/all/0/1&quot;&gt;Yuval Alaluf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1&quot;&gt;Daniel Cohen-Or&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.07728">
<title>Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability. (arXiv:2308.07728v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.07728</link>
<description rdf:parseType="Literal">&lt;p&gt;Fine-tuning pre-trained neural network models has become a widely adopted
approach across various domains. However, it can lead to the distortion of
pre-trained feature extractors that already possess strong generalization
capabilities. Mitigating feature distortion during adaptation to new target
domains is crucial. Recent studies have shown promising results in handling
feature distortion by aligning the head layer on in-distribution datasets
before performing fine-tuning. Nonetheless, a significant limitation arises
from the treatment of batch normalization layers during fine-tuning, leading to
suboptimal performance. In this paper, we propose Domain-Aware Fine-Tuning
(DAFT), a novel approach that incorporates batch normalization conversion and
the integration of linear probing and fine-tuning. Our batch normalization
conversion method effectively mitigates feature distortion by reducing
modifications to the neural network during fine-tuning. Additionally, we
introduce the integration of linear probing and fine-tuning to optimize the
head layer with gradual adaptation of the feature extractor. By leveraging
batch normalization layers and integrating linear probing and fine-tuning, our
DAFT significantly mitigates feature distortion and achieves improved model
performance on both in-distribution and out-of-distribution datasets. Extensive
experiments demonstrate that our method outperforms other baseline methods,
demonstrating its effectiveness in not only improving performance but also
mitigating feature distortion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ha_S/0/1/0/all/0/1&quot;&gt;Seokhyeon Ha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1&quot;&gt;Sunbeom Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jungwoo Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.09437">
<title>From Hope to Safety: Unlearning Biases of Deep Models via Gradient Penalization in Latent Space. (arXiv:2308.09437v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.09437</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Neural Networks are prone to learning spurious correlations embedded in
the training data, leading to potentially biased predictions. This poses risks
when deploying these models for high-stake decision-making, such as in medical
applications. Current methods for post-hoc model correction either require
input-level annotations which are only possible for spatially localized biases,
or augment the latent feature space, thereby hoping to enforce the right
reasons. We present a novel method for model correction on the concept level
that explicitly reduces model sensitivity towards biases via gradient
penalization. When modeling biases via Concept Activation Vectors, we highlight
the importance of choosing robust directions, as traditional regression-based
approaches such as Support Vector Machines tend to result in diverging
directions. We effectively mitigate biases in controlled and real-world
settings on the ISIC, Bone Age, ImageNet and CelebA datasets using VGG, ResNet
and EfficientNet architectures. Code is available on
https://github.com/frederikpahde/rrclarc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dreyer_M/0/1/0/all/0/1&quot;&gt;Maximilian Dreyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pahde_F/0/1/0/all/0/1&quot;&gt;Frederik Pahde&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anders_C/0/1/0/all/0/1&quot;&gt;Christopher J. Anders&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1&quot;&gt;Wojciech Samek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lapuschkin_S/0/1/0/all/0/1&quot;&gt;Sebastian Lapuschkin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.09616">
<title>Far3D: Expanding the Horizon for Surround-view 3D Object Detection. (arXiv:2308.09616v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.09616</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently 3D object detection from surround-view images has made notable
advancements with its low deployment cost. However, most works have primarily
focused on close perception range while leaving long-range detection less
explored. Expanding existing methods directly to cover long distances poses
challenges such as heavy computation costs and unstable convergence. To address
these limitations, this paper proposes a novel sparse query-based framework,
dubbed Far3D. By utilizing high-quality 2D object priors, we generate 3D
adaptive queries that complement the 3D global queries. To efficiently capture
discriminative features across different views and scales for long-range
objects, we introduce a perspective-aware aggregation module. Additionally, we
propose a range-modulated 3D denoising approach to address query error
propagation and mitigate convergence issues in long-range tasks. Significantly,
Far3D demonstrates SoTA performance on the challenging Argoverse 2 dataset,
covering a wide range of 150 meters, surpassing several LiDAR-based approaches.
Meanwhile, Far3D exhibits superior performance compared to previous methods on
the nuScenes dataset. The code is available at
https://github.com/megvii-research/Far3D.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xiaohui Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuailin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yingfei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shihao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_F/0/1/0/all/0/1&quot;&gt;Fan Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tiancai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1&quot;&gt;Lijin Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiangyu Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.09936">
<title>BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions. (arXiv:2308.09936v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.09936</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Language Models (VLMs), which extend Large Language Models (LLM) by
incorporating visual understanding capability, have demonstrated significant
advancements in addressing open-ended visual question-answering (VQA) tasks.
However, these models cannot accurately interpret images infused with text, a
common occurrence in real-world scenarios. Standard procedures for extracting
information from images often involve learning a fixed set of query embeddings.
These embeddings are designed to encapsulate image contexts and are later used
as soft prompt inputs in LLMs. Yet, this process is limited to the token count,
potentially curtailing the recognition of scenes with text-rich context. To
improve upon them, the present study introduces BLIVA: an augmented version of
InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings
from InstructBLIP and also directly projects encoded patch embeddings into the
LLM, a technique inspired by LLaVA. This approach assists the model to capture
intricate details potentially missed during the query decoding process.
Empirical evidence demonstrates that our model, BLIVA, significantly enhances
performance in processing text-rich VQA benchmarks (up to 17.76% in OCR-VQA
benchmark) and in undertaking general (not particularly text-rich) VQA
benchmarks (up to 7.9% in Visual Spatial Reasoning benchmark), and achieved
17.72% overall improvement in a comprehensive multimodal LLM benchmark (MME),
comparing to our baseline InstructBLIP. BLIVA demonstrates significant
capability in decoding real-world images, irrespective of text presence. To
demonstrate the broad industry applications enabled by BLIVA, we evaluate the
model using a new dataset comprising YouTube thumbnails paired with
question-answer sets across 11 diverse categories. Our code and models are
freely accessible at https://github.com/mlpc-ucsd/BLIVA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1&quot;&gt;Wenbo Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yifan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Weiyue Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zeyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1&quot;&gt;Zhuowen Tu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.10997">
<title>MarkovGen: Structured Prediction for Efficient Text-to-Image Generation. (arXiv:2308.10997v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.10997</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern text-to-image generation models produce high-quality images that are
both photorealistic and faithful to the text prompts. However, this quality
comes at significant computational cost: nearly all of these models are
iterative and require running sampling multiple times with large models. This
iterative process is needed to ensure that different regions of the image are
not only aligned with the text prompt, but also compatible with each other. In
this work, we propose a light-weight approach to achieving this compatibility
between different regions of an image, using a Markov Random Field (MRF) model.
We demonstrate the effectiveness of this method on top of the latent
token-based Muse text-to-image model. The MRF richly encodes the compatibility
among image tokens at different spatial locations to improve quality and
significantly reduce the required number of Muse sampling steps. Inference with
the MRF is significantly cheaper, and its parameters can be quickly learned
through back-propagation by modeling MRF inference as a differentiable
neural-network layer. Our full model, MarkovGen, uses this proposed MRF model
to both speed up Muse by 1.5X and produce higher quality images by decreasing
undesirable image artifacts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayasumana_S/0/1/0/all/0/1&quot;&gt;Sadeep Jayasumana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glasner_D/0/1/0/all/0/1&quot;&gt;Daniel Glasner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramalingam_S/0/1/0/all/0/1&quot;&gt;Srikumar Ramalingam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veit_A/0/1/0/all/0/1&quot;&gt;Andreas Veit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakrabarti_A/0/1/0/all/0/1&quot;&gt;Ayan Chakrabarti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Sanjiv Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11875">
<title>Motion-to-Matching: A Mixed Paradigm for 3D Single Object Tracking. (arXiv:2308.11875v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11875</link>
<description rdf:parseType="Literal">&lt;p&gt;3D single object tracking with LiDAR points is an important task in the
computer vision field. Previous methods usually adopt the matching-based or
motion-centric paradigms to estimate the current target status. However, the
former is sensitive to the similar distractors and the sparseness of point
cloud due to relying on appearance matching, while the latter usually focuses
on short-term motion clues (eg. two frames) and ignores the long-term motion
pattern of target. To address these issues, we propose a mixed paradigm with
two stages, named MTM-Tracker, which combines motion modeling with feature
matching into a single network. Specifically, in the first stage, we exploit
the continuous historical boxes as motion prior and propose an encoder-decoder
structure to locate target coarsely. Then, in the second stage, we introduce a
feature interaction module to extract motion-aware features from consecutive
point clouds and match them to refine target movement as well as regress other
target states. Extensive experiments validate that our paradigm achieves
competitive performance on large-scale datasets (70.9% in KITTI and 51.70% in
NuScenes). The code will be open soon at
https://github.com/LeoZhiheng/MTM-Tracker.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhiheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1&quot;&gt;Yubo Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1&quot;&gt;Zheng Fang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12558">
<title>Hyperbolic Audio-visual Zero-shot Learning. (arXiv:2308.12558v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12558</link>
<description rdf:parseType="Literal">&lt;p&gt;Audio-visual zero-shot learning aims to classify samples consisting of a pair
of corresponding audio and video sequences from classes that are not present
during training. An analysis of the audio-visual data reveals a large degree of
hyperbolicity, indicating the potential benefit of using a hyperbolic
transformation to achieve curvature-aware geometric learning, with the aim of
exploring more complex hierarchical data structures for this task. The proposed
approach employs a novel loss function that incorporates cross-modality
alignment between video and audio features in the hyperbolic space.
Additionally, we explore the use of multiple adaptive curvatures for hyperbolic
projections. The experimental results on this very challenging task demonstrate
that our proposed hyperbolic approach for zero-shot learning outperforms the
SOTA method on three datasets: VGGSound-GZSL, UCF-GZSL, and ActivityNet-GZSL
achieving a harmonic mean (HM) improvement of around 3.0%, 7.0%, and 5.3%,
respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1&quot;&gt;Jie Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayder_Z/0/1/0/all/0/1&quot;&gt;Zeeshan Hayder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Junlin Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_P/0/1/0/all/0/1&quot;&gt;Pengfei Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harandi_M/0/1/0/all/0/1&quot;&gt;Mehrtash Harandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1&quot;&gt;Lars Petersson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.13177">
<title>How to Evaluate the Generalization of Detection? A Benchmark for Comprehensive Open-Vocabulary Detection. (arXiv:2308.13177v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.13177</link>
<description rdf:parseType="Literal">&lt;p&gt;Object detection (OD) in computer vision has made significant progress in
recent years, transitioning from closed-set labels to open-vocabulary detection
(OVD) based on large-scale vision-language pre-training (VLP). However, current
evaluation methods and datasets are limited to testing generalization over
object types and referral expressions, which do not provide a systematic,
fine-grained, and accurate benchmark of OVD models&apos; abilities. In this paper,
we propose a new benchmark named OVDEval, which includes 9 sub-tasks and
introduces evaluations on commonsense knowledge, attribute understanding,
position understanding, object relation comprehension, and more. The dataset is
meticulously created to provide hard negatives that challenge models&apos; true
understanding of visual and linguistic input. Additionally, we identify a
problem with the popular Average Precision (AP) metric when benchmarking models
on these fine-grained label datasets and propose a new metric called
Non-Maximum Suppression Average Precision (NMS-AP) to address this issue.
Extensive experimental results show that existing top OVD models all fail on
the new tasks except for simple object types, demonstrating the value of the
proposed dataset in pinpointing the weakness of current OVD models and guiding
future research. Furthermore, the proposed NMS-AP metric is verified by
experiments to provide a much more truthful evaluation of OVD models, whereas
traditional AP metrics yield deceptive results. Data is available at
\url{https://github.com/om-ai-lab/OVDEval}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yiyang Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1&quot;&gt;Peng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tiancheng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qianqian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1&quot;&gt;Jiajia Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1&quot;&gt;Chunxin Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kyusong Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qing Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.14604">
<title>SAM-PARSER: Fine-tuning SAM Efficiently by Parameter Space Reconstruction. (arXiv:2308.14604v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.14604</link>
<description rdf:parseType="Literal">&lt;p&gt;Segment Anything Model (SAM) has received remarkable attention as it offers a
powerful and versatile solution for object segmentation in images. However,
fine-tuning SAM for downstream segmentation tasks under different scenarios
remains a challenge, as the varied characteristics of different scenarios
naturally requires diverse model parameter spaces. Most existing fine-tuning
methods attempt to bridge the gaps among different scenarios by introducing a
set of new parameters to modify SAM&apos;s original parameter space. Unlike these
works, in this paper, we propose fine-tuning SAM efficiently by parameter space
reconstruction (SAM-PARSER), which introduce nearly zero trainable parameters
during fine-tuning. In SAM-PARSER, we assume that SAM&apos;s original parameter
space is relatively complete, so that its bases are able to reconstruct the
parameter space of a new scenario. We obtain the bases by matrix decomposition,
and fine-tuning the coefficients to reconstruct the parameter space tailored to
the new scenario by an optimal linear combination of the bases. Experimental
results show that SAM-PARSER exhibits superior segmentation performance across
various scenarios, while reducing the number of trainable parameters by
$\approx 290$ times compared with current parameter-efficient fine-tuning
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1&quot;&gt;Zelin Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhengqin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1&quot;&gt;Zhilin Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaokang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1&quot;&gt;Wei Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.00872">
<title>Fearless Luminance Adaptation: A Macro-Micro-Hierarchical Transformer for Exposure Correction. (arXiv:2309.00872v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.00872</link>
<description rdf:parseType="Literal">&lt;p&gt;Photographs taken with less-than-ideal exposure settings often display poor
visual quality. Since the correction procedures vary significantly, it is
difficult for a single neural network to handle all exposure problems.
Moreover, the inherent limitations of convolutions, hinder the models ability
to restore faithful color or details on extremely over-/under- exposed regions.
To overcome these limitations, we propose a Macro-Micro-Hierarchical
transformer, which consists of a macro attention to capture long-range
dependencies, a micro attention to extract local features, and a hierarchical
structure for coarse-to-fine correction. In specific, the complementary
macro-micro attention designs enhance locality while allowing global
interactions. The hierarchical structure enables the network to correct
exposure errors of different scales layer by layer. Furthermore, we propose a
contrast constraint and couple it seamlessly in the loss function, where the
corrected image is pulled towards the positive sample and pushed away from the
dynamically generated negative samples. Thus the remaining color distortion and
loss of detail can be removed. We also extend our method as an image enhancer
for low-light face recognition and low-light semantic segmentation. Experiments
demonstrate that our approach obtains more attractive results than
state-of-the-art methods quantitatively and qualitatively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Gehui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jinyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Long Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhiying Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1&quot;&gt;Xin Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Risheng Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.01574">
<title>Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data. (arXiv:2309.01574v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.01574</link>
<description rdf:parseType="Literal">&lt;p&gt;Rising maintenance costs of ageing infrastructure necessitate innovative
monitoring techniques. This paper presents a new approach for detecting axles,
enabling real-time application of Bridge Weigh-In-Motion (BWIM) systems without
dedicated axle detectors. The proposed Virtual Axle Detector with Enhanced
Receptive Field (VADER) is independent of bridge type and sensor placement
while only using raw acceleration data as input. By using raw data instead of
spectograms as input, the receptive field can be enhanced without increasing
the number of parameters. We also introduce a novel receptive field (RF) rule
for an object-size driven design of Convolutional Neural Network (CNN)
architectures. We were able to show, that the RF rule has the potential to
bridge the gap between physical boundary conditions and deep learning model
development. Based on the RF rule, our results suggest that models using raw
data could achieve better performance than those using spectrograms, offering a
compelling reason to consider raw data as input. The proposed VADER achieves to
detect 99.9 % of axles with a spatial error of 4.13 cm using only acceleration
measurements, while cutting computational and memory costs by 99 % compared to
the state-of-the-art using spectograms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riedel_H/0/1/0/all/0/1&quot;&gt;Henik Riedel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lorenzen_R/0/1/0/all/0/1&quot;&gt;Robert Steven Lorenzen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hubler_C/0/1/0/all/0/1&quot;&gt;Clemens H&amp;#xfc;bler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.02270">
<title>SAM-Deblur: Let Segment Anything Boost Image Deblurring. (arXiv:2309.02270v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.02270</link>
<description rdf:parseType="Literal">&lt;p&gt;Image deblurring is a critical task in the field of image restoration, aiming
to eliminate blurring artifacts. However, the challenge of addressing
non-uniform blurring leads to an ill-posed problem, which limits the
generalization performance of existing deblurring models. To solve the problem,
we propose a framework SAM-Deblur, integrating prior knowledge from the Segment
Anything Model (SAM) into the deblurring task for the first time. In
particular, SAM-Deblur is divided into three stages. First, we preprocess the
blurred images, obtain segment masks via SAM, and propose a mask dropout method
for training to enhance model robustness. Then, to fully leverage the
structural priors generated by SAM, we propose a Mask Average Pooling (MAP)
unit specifically designed to average SAM-generated segmented areas, serving as
a plug-and-play component which can be seamlessly integrated into existing
deblurring networks. Finally, we feed the fused features generated by the MAP
Unit into the deblurring model to obtain a sharp image. Experimental results on
the RealBlurJ, ReloBlur, and REDS datasets reveal that incorporating our
methods improves GoPro-trained NAFNet&apos;s PSNR by 0.05, 0.96, and 7.03,
respectively. Project page is available at GitHub
\href{https://hplqaq.github.io/projects/sam-deblur}{HPLQAQ/SAM-Deblur}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Siwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mingxuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yating Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haoxiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1&quot;&gt;Zifei Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.05173">
<title>DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning. (arXiv:2309.05173v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.05173</link>
<description rdf:parseType="Literal">&lt;p&gt;Prompt tuning (PT), where a small amount of trainable soft (continuous)
prompt vectors is affixed to the input of language models (LM), has shown
promising results across various tasks and models for parameter-efficient
fine-tuning (PEFT). PT stands out from other PEFT approaches because it
maintains competitive performance with fewer trainable parameters and does not
drastically scale up its parameters as the model size expands. However, PT
introduces additional soft prompt tokens, leading to longer input sequences,
which significantly impacts training and inference time and memory usage due to
the Transformer&apos;s quadratic complexity. Particularly concerning for Large
Language Models (LLMs) that face heavy daily querying. To address this issue,
we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt
into a shorter soft prompt and a pair of low-rank matrices that are then
optimised with two different learning rates. This allows DePT to achieve better
performance while saving over 20% memory and time costs compared to vanilla PT
and its variants, without changing trainable parameter sizes. Through extensive
experiments on 23 natural language processing (NLP) and vision-language (VL)
tasks, we demonstrate that DePT outperforms state-of-the-art PEFT approaches,
including the full fine-tuning baseline in some scenarios. Additionally, we
empirically show that DEPT grows more efficient as the model size increases.
Our further study reveals that DePT integrates seamlessly with
parameter-efficient transfer learning in the few-shot learning setting and
highlights its adaptability to various model architectures and sizes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhengxiang Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipani_A/0/1/0/all/0/1&quot;&gt;Aldo Lipani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.06023">
<title>Learning from History: Task-agnostic Model Contrastive Learning for Image Restoration. (arXiv:2309.06023v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.06023</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive learning has emerged as a prevailing paradigm for high-level
vision tasks, which, by introducing properly negative samples, has also been
exploited for low-level vision tasks to achieve a compact optimization space to
account for their ill-posed nature. However, existing methods rely on manually
predefined and task-oriented negatives, which often exhibit pronounced
task-specific biases. To address this challenge, our paper introduces an
innovative method termed &apos;learning from history&apos;, which dynamically generates
negative samples from the target model itself. Our approach, named Model
Contrastive paradigm for Image Restoration (MCIR), rejuvenates latency models
as negative models, making it compatible with diverse image restoration tasks.
We propose the Self-Prior guided Negative loss (SPN) to enable it. This
approach significantly enhances existing models when retrained with the
proposed model contrastive paradigm. The results show significant improvements
in image restoration across various tasks and architectures. For example,
models retrained with SPN outperform the original FFANet and DehazeFormer by
3.41 dB and 0.57 dB on the RESIDE indoor dataset for image dehazing. Similarly,
they achieve notable improvements of 0.47 dB on SPA-Data over IDT for image
deraining and 0.12 dB on Manga109 for a 4x scale super-resolution over
lightweight SwinIR, respectively. Code and retrained models are available at
https://github.com/Aitical/MCIR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1&quot;&gt;Gang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Junjun Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1&quot;&gt;Kui Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xianming Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.06618">
<title>Multi-dimensional Fusion and Consistency for Semi-supervised Medical Image Segmentation. (arXiv:2309.06618v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.06618</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce a novel semi-supervised learning framework
tailored for medical image segmentation. Central to our approach is the
innovative Multi-scale Text-aware ViT-CNN Fusion scheme. This scheme adeptly
combines the strengths of both ViTs and CNNs, capitalizing on the unique
advantages of both architectures as well as the complementary information in
vision-language modalities. Further enriching our framework, we propose the
Multi-Axis Consistency framework for generating robust pseudo labels, thereby
enhancing the semisupervised learning process. Our extensive experiments on
several widelyused datasets unequivocally demonstrate the efficacy of our
approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yixing Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1&quot;&gt;Zhaoxin Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Min Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.06810">
<title>Leveraging SE(3) Equivariance for Learning 3D Geometric Shape Assembly. (arXiv:2309.06810v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.06810</link>
<description rdf:parseType="Literal">&lt;p&gt;Shape assembly aims to reassemble parts (or fragments) into a complete
object, which is a common task in our daily life. Different from the semantic
part assembly (e.g., assembling a chair&apos;s semantic parts like legs into a whole
chair), geometric part assembly (e.g., assembling bowl fragments into a
complete bowl) is an emerging task in computer vision and robotics. Instead of
semantic information, this task focuses on geometric information of parts. As
the both geometric and pose space of fractured parts are exceptionally large,
shape pose disentanglement of part representations is beneficial to geometric
shape assembly. In our paper, we propose to leverage SE(3) equivariance for
such shape pose disentanglement. Moreover, while previous works in vision and
robotics only consider SE(3) equivariance for the representations of single
objects, we move a step forward and propose leveraging SE(3) equivariance for
representations considering multi-part correlations, which further boosts the
performance of the multi-part assembly. Experiments demonstrate the
significance of SE(3) equivariance and our proposed method for geometric shape
assembly. Project page: https://crtie.github.io/SE-3-part-assembly/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1&quot;&gt;Ruihai Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tie_C/0/1/0/all/0/1&quot;&gt;Chenrui Tie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yushi Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hao Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.06933">
<title>DreamStyler: Paint by Style Inversion with Text-to-Image Diffusion Models. (arXiv:2309.06933v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.06933</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent progresses in large-scale text-to-image models have yielded remarkable
accomplishments, finding various applications in art domain. However,
expressing unique characteristics of an artwork (e.g. brushwork, colortone, or
composition) with text prompts alone may encounter limitations due to the
inherent constraints of verbal description. To this end, we introduce
DreamStyler, a novel framework designed for artistic image synthesis,
proficient in both text-to-image synthesis and style transfer. DreamStyler
optimizes a multi-stage textual embedding with a context-aware text prompt,
resulting in prominent image quality. In addition, with content and style
guidance, DreamStyler exhibits flexibility to accommodate a range of style
references. Experimental results demonstrate its superior performance across
multiple scenarios, suggesting its promising potential in artistic product
creation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahn_N/0/1/0/all/0/1&quot;&gt;Namhyuk Ahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Junsoo Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1&quot;&gt;Chunggi Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kunhee Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Daesik Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nam_S/0/1/0/all/0/1&quot;&gt;Seung-Hun Nam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_K/0/1/0/all/0/1&quot;&gt;Kibeom Hong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.06961">
<title>Towards Reliable Dermatology Evaluation Benchmarks. (arXiv:2309.06961v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.06961</link>
<description rdf:parseType="Literal">&lt;p&gt;Benchmark datasets for digital dermatology unwittingly contain inaccuracies
that reduce trust in model performance estimates. We propose a
resource-efficient data-cleaning protocol to identify issues that escaped
previous curation. The protocol leverages an existing algorithmic cleaning
strategy and is followed by a confirmation process terminated by an intuitive
stopping criterion. Based on confirmation by multiple dermatologists, we remove
irrelevant samples and near duplicates and estimate the percentage of label
errors in six dermatology image datasets for model evaluation promoted by the
International Skin Imaging Collaboration. Along with this paper, we publish
revised file lists for each dataset which should be used for model evaluation.
Our work paves the way for more trustworthy performance assessment in digital
dermatology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Groger_F/0/1/0/all/0/1&quot;&gt;Fabian Gr&amp;#xf6;ger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lionetti_S/0/1/0/all/0/1&quot;&gt;Simone Lionetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gottfrois_P/0/1/0/all/0/1&quot;&gt;Philippe Gottfrois&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_Jimenez_A/0/1/0/all/0/1&quot;&gt;Alvaro Gonzalez-Jimenez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Groh_M/0/1/0/all/0/1&quot;&gt;Matthew Groh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daneshjou_R/0/1/0/all/0/1&quot;&gt;Roxana Daneshjou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Consortium_L/0/1/0/all/0/1&quot;&gt;Labelling Consortium&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navarini_A/0/1/0/all/0/1&quot;&gt;Alexander A. Navarini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pouly_M/0/1/0/all/0/1&quot;&gt;Marc Pouly&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.09907">
<title>Quantum Vision Clustering. (arXiv:2309.09907v2 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2309.09907</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised visual clustering has garnered significant attention in recent
times, aiming to characterize distributions of unlabeled visual images through
clustering based on a parameterized appearance approach. Alternatively,
clustering algorithms can be viewed as assignment problems, often characterized
as NP-hard, yet precisely solvable for small instances on contemporary
hardware. Adiabatic quantum computing (AQC) emerges as a promising solution,
poised to deliver substantial speedups for a range of NP-hard optimization
problems. However, existing clustering formulations face challenges in quantum
computing adoption due to scalability issues. In this study, we present the
first clustering formulation tailored for resolution using Adiabatic quantum
computing. An Ising model is introduced to represent the quantum mechanical
system implemented on AQC. The proposed approach demonstrates high
competitiveness compared to state-of-the-art optimization-based methods, even
when utilizing off-the-shelf integer programming solvers. Lastly, this work
showcases the solvability of the proposed clustering problem on
current-generation real quantum computers for small examples and analyzes the
properties of the obtained solutions
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Nguyen_X/0/1/0/all/0/1&quot;&gt;Xuan Bac Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Churchill_H/0/1/0/all/0/1&quot;&gt;Hugh Churchill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Luu_K/0/1/0/all/0/1&quot;&gt;Khoa Luu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Samee U. Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11082">
<title>Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial Margin Contrastive Learning. (arXiv:2309.11082v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.11082</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the explosion of web videos makes text-video retrieval
increasingly essential and popular for video filtering, recommendation, and
search. Text-video retrieval aims to rank relevant text/video higher than
irrelevant ones. The core of this task is to precisely measure the cross-modal
similarity between texts and videos. Recently, contrastive learning methods
have shown promising results for text-video retrieval, most of which focus on
the construction of positive and negative pairs to learn text and video
representations. Nevertheless, they do not pay enough attention to hard
negative pairs and lack the ability to model different levels of semantic
similarity. To address these two issues, this paper improves contrastive
learning using two novel techniques. First, to exploit hard examples for robust
discriminative power, we propose a novel Dual-Modal Attention-Enhanced Module
(DMAE) to mine hard negative pairs from textual and visual clues. By further
introducing a Negative-aware InfoNCE (NegNCE) loss, we are able to adaptively
identify all these hard negatives and explicitly highlight their impacts in the
training loss. Second, our work argues that triplet samples can better model
fine-grained semantic similarity compared to pairwise samples. We thereby
present a new Triplet Partial Margin Contrastive Learning (TPM-CL) module to
construct partial order triplet samples by automatically generating
fine-grained hard negatives for matched text-video pairs. The proposed TPM-CL
designs an adaptive token masking strategy with cross-modal interaction to
model subtle semantic differences. Extensive experiments demonstrate that the
proposed approach outperforms existing methods on four widely-used text-video
retrieval datasets, including MSR-VTT, MSVD, DiDeMo and ActivityNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1&quot;&gt;Chen Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xuzheng Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yuan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jia Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhongyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qingpei Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1&quot;&gt;Wei Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Ming Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1&quot;&gt;Yuan Qi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14950">
<title>Multi-Source Domain Adaptation for Object Detection with Prototype-based Mean-teacher. (arXiv:2309.14950v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14950</link>
<description rdf:parseType="Literal">&lt;p&gt;Adapting visual object detectors to operational target domains is a
challenging task, commonly achieved using unsupervised domain adaptation (UDA)
methods. Recent studies have shown that when the labeled dataset comes from
multiple source domains, treating them as separate domains and performing a
multi-source domain adaptation (MSDA) improves the accuracy and robustness over
blending these source domains and performing a UDA. For adaptation, existing
MSDA methods learn domain-invariant and domain-specific parameters (for each
source domain). However, unlike single-source UDA methods, learning
domain-specific parameters makes them grow significantly in proportion to the
number of source domains. This paper proposes a novel MSDA method called
Prototype-based Mean Teacher (PMT), which uses class prototypes instead of
domain-specific subnets to encode domain-specific information. These prototypes
are learned using a contrastive loss, aligning the same categories across
domains and separating different categories far apart. Given the use of
prototypes, the number of parameters required for our PMT method does not
increase significantly with the number of source domains, thus reducing memory
issues and possible overfitting. Empirical studies indicate that PMT
outperforms state-of-the-art MSDA methods on several challenging object
detection datasets. Our code is available at
https://github.com/imatif17/Prototype-Mean-Teacher.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belal_A/0/1/0/all/0/1&quot;&gt;Atif Belal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meethal_A/0/1/0/all/0/1&quot;&gt;Akhil Meethal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romero_F/0/1/0/all/0/1&quot;&gt;Francisco Perdigon Romero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedersoli_M/0/1/0/all/0/1&quot;&gt;Marco Pedersoli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1&quot;&gt;Eric Granger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16899">
<title>On the Contractivity of Plug-and-Play Operators. (arXiv:2309.16899v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.16899</link>
<description rdf:parseType="Literal">&lt;p&gt;In plug-and-play (PnP) regularization, the proximal operator in algorithms
such as ISTA and ADMM is replaced by a powerful denoiser. This formal
substitution works surprisingly well in practice. In fact, PnP has been shown
to give state-of-the-art results for various imaging applications. The
empirical success of PnP has motivated researchers to understand its
theoretical underpinnings and, in particular, its convergence. It was shown in
prior work that for kernel denoisers such as the nonlocal means, PnP-ISTA
provably converges under some strong assumptions on the forward model. The
present work is motivated by the following questions: Can we relax the
assumptions on the forward model? Can the convergence analysis be extended to
PnP-ADMM? Can we estimate the convergence rate? In this letter, we resolve
these questions using the contraction mapping theorem: (i) for symmetric
denoisers, we show that (under mild conditions) PnP-ISTA and PnP-ADMM exhibit
linear convergence; and (ii) for kernel denoisers, we show that PnP-ISTA and
PnP-ADMM converge linearly for image inpainting. We validate our theoretical
findings using reconstruction experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Athalye_C/0/1/0/all/0/1&quot;&gt;Chirayu D. Athalye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhury_K/0/1/0/all/0/1&quot;&gt;Kunal N. Chaudhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_B/0/1/0/all/0/1&quot;&gt;Bhartendu Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02601">
<title>MagicDrive: Street View Generation with Diverse 3D Geometry Control. (arXiv:2310.02601v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02601</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in diffusion models have significantly enhanced the data
synthesis with 2D control. Yet, precise 3D control in street view generation,
crucial for 3D perception tasks, remains elusive. Specifically, utilizing
Bird&apos;s-Eye View (BEV) as the primary condition often leads to challenges in
geometry control (e.g., height), affecting the representation of object shapes,
occlusion patterns, and road surface elevations, all of which are essential to
perception data synthesis, especially for 3D object detection tasks. In this
paper, we introduce MagicDrive, a novel street view generation framework
offering diverse 3D geometry controls, including camera poses, road maps, and
3D bounding boxes, together with textual descriptions, achieved through
tailored encoding strategies. Besides, our design incorporates a cross-view
attention module, ensuring consistency across multiple camera views. With
MagicDrive, we achieve high-fidelity street-view synthesis that captures
nuanced 3D geometry and various scene descriptions, enhancing tasks like BEV
segmentation and 3D object detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1&quot;&gt;Ruiyuan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1&quot;&gt;Enze Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1&quot;&gt;Lanqing Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenguo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeung_D/0/1/0/all/0/1&quot;&gt;Dit-Yan Yeung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qiang Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03059">
<title>Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models. (arXiv:2310.03059v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03059</link>
<description rdf:parseType="Literal">&lt;p&gt;The popularity of pre-trained large models has revolutionized downstream
tasks across diverse fields, such as language, vision, and multi-modality. To
minimize the adaption cost for downstream tasks, many Parameter-Efficient
Fine-Tuning (PEFT) techniques are proposed for language and 2D image
pre-trained models. However, the specialized PEFT method for 3D pre-trained
models is still under-explored. To this end, we introduce Point-PEFT, a novel
framework for adapting point cloud pre-trained models with minimal learnable
parameters. Specifically, for a pre-trained 3D model, we freeze most of its
parameters, and only tune the newly added PEFT modules on downstream tasks,
which consist of a Point-prior Prompt and a Geometry-aware Adapter. The
Point-prior Prompt adopts a set of learnable prompt tokens, for which we
propose to construct a memory bank with domain-specific knowledge, and utilize
a parameter-free attention to enhance the prompt tokens. The Geometry-aware
Adapter aims to aggregate point cloud features within spatial neighborhoods to
capture fine-grained geometric information through local interactions.
Extensive experiments indicate that our Point-PEFT can achieve better
performance than the full fine-tuning on various downstream tasks, while using
only 5% of the trainable parameters, demonstrating the efficiency and
effectiveness of our approach. Code is released at
https://github.com/Ivan-Tang-3D/PEFT-3D.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yiwen Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ray Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zoey Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xianzheng Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhigang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1&quot;&gt;Bin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuelong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05524">
<title>Parameterization-driven Neural Implicit Surfaces Editing. (arXiv:2310.05524v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05524</link>
<description rdf:parseType="Literal">&lt;p&gt;The growing capabilities of neural rendering have increased the demand for
new techniques that enable intuitive editing of 3D objects, particularly when
they are represented as neural implicit surfaces. In this paper, we present a
novel neural algorithm to parameterize neural implicit surfaces to simple
parametric domains, such as spheres, cubes, or polycubes, thereby facilitating
visualization and various editing tasks. Technically, our method computes a
bi-directional deformation between 3D objects and their chosen parametric
domains, eliminating the need for any prior information. We adopt a forward
mapping of points on the zero level set of the 3D object to a parametric
domain, followed by a backward mapping through inverse deformation. To ensure
the map is bijective, we employ a cycle loss while optimizing the smoothness of
both deformations. Additionally, we leverage a Laplacian regularizer to
effectively control angle distortion and offer the flexibility to choose from a
range of parametric domains for managing area distortion. Designed for
compatibility, our framework integrates seamlessly with existing neural
rendering pipelines, taking multi-view images as input to reconstruct 3D
geometry and compute the corresponding texture map. We also introduce a simple
yet effective technique for intrinsic radiance decomposition, facilitating both
view-independent material editing and view-dependent shading editing. Our
method allows for the immediate rendering of edited textures through volume
rendering, without the need for network re-training. Moreover, our approach
supports the co-parameterization of multiple objects and enables texture
transfer between them. We demonstrate the effectiveness of our method on images
of human heads and man-made objects. We will make the source code publicly
available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1&quot;&gt;Baixin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jiangbei Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_F/0/1/0/all/0/1&quot;&gt;Fei Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1&quot;&gt;Kwan-Yee Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wayne Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1&quot;&gt;Chen Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Ying He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10059">
<title>Flow Dynamics Correction for Action Recognition. (arXiv:2310.10059v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10059</link>
<description rdf:parseType="Literal">&lt;p&gt;Various research studies indicate that action recognition performance highly
depends on the types of motions being extracted and how accurate the human
actions are represented. In this paper, we investigate different optical flow,
and features extracted from these optical flow that capturing both short-term
and long-term motion dynamics. We perform power normalization on the magnitude
component of optical flow for flow dynamics correction to boost subtle or
dampen sudden motions. We show that existing action recognition models which
rely on optical flow are able to get performance boosted with our corrected
optical flow. To further improve performance, we integrate our corrected flow
dynamics into popular models through a simple hallucination step by selecting
only the best performing optical flow features, and we show that by
&apos;translating&apos; the CNN feature maps into these optical flow features with
different scales of motions leads to the new state-of-the-art performance on
several benchmarks including HMDB-51, YUP++, fine-grained action recognition on
MPII Cooking Activities, and large-scale Charades.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1&quot;&gt;Piotr Koniusz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10912">
<title>Towards Training-free Open-world Segmentation via Image Prompt Foundation Models. (arXiv:2310.10912v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10912</link>
<description rdf:parseType="Literal">&lt;p&gt;The realm of computer vision has witnessed a paradigm shift with the advent
of foundational models, mirroring the transformative influence of large
language models in the domain of natural language processing. This paper delves
into the exploration of open-world segmentation, presenting a novel approach
called Image Prompt Segmentation (IPSeg) that harnesses the power of vision
foundational models. IPSeg lies the principle of a training-free paradigm,
which capitalizes on image prompt techniques. Specifically, IPSeg utilizes a
single image containing a subjective visual concept as a flexible prompt to
query vision foundation models like DINOv2 and Stable Diffusion. Our approach
extracts robust features for the prompt image and input image, then matches the
input representations to the prompt representations via a novel feature
interaction module to generate point prompts highlighting target objects in the
input image. The generated point prompts are further utilized to guide the
Segment Anything Model to segment the target object in the input image. The
proposed method stands out by eliminating the need for exhaustive training
sessions, thereby offering a more efficient and scalable solution. Experiments
on COCO, PASCAL VOC, and other datasets demonstrate IPSeg&apos;s efficacy for
flexible open-world segmentation using intuitive image prompts. This work
pioneers tapping foundation models for open-world understanding through visual
concepts conveyed in images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1&quot;&gt;Lv Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1&quot;&gt;Peng-Tao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1&quot;&gt;Hao-Ke Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.13356">
<title>Sync-NeRF: Generalizing Dynamic NeRFs to Unsynchronized Videos. (arXiv:2310.13356v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.13356</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in 4D scene reconstruction using neural radiance fields
(NeRF) have demonstrated the ability to represent dynamic scenes from
multi-view videos. However, they fail to reconstruct the dynamic scenes and
struggle to fit even the training views in unsynchronized settings. It happens
because they employ a single latent embedding for a frame while the multi-view
images at the same frame were actually captured at different moments. To
address this limitation, we introduce time offsets for individual
unsynchronized videos and jointly optimize the offsets with NeRF. By design,
our method is applicable for various baselines and improves them with large
margins. Furthermore, finding the offsets naturally works as synchronizing the
videos without manual effort. Experiments are conducted on the common Plenoptic
Video Dataset and a newly built Unsynchronized Dynamic Blender Dataset to
verify the performance of our method. Project page:
https://seoha-kim.github.io/sync-nerf
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seoha Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bae_J/0/1/0/all/0/1&quot;&gt;Jeongmin Bae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_Y/0/1/0/all/0/1&quot;&gt;Youngsik Yun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hahyun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bang_G/0/1/0/all/0/1&quot;&gt;Gun Bang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uh_Y/0/1/0/all/0/1&quot;&gt;Youngjung Uh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15646">
<title>Mean Teacher DETR with Masked Feature Alignment: A Robust Domain Adaptive Detection Transformer Framework. (arXiv:2310.15646v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15646</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised domain adaptation object detection (UDAOD) research on Detection
Transformer(DETR) mainly focuses on feature alignment and existing methods can
be divided into two kinds, each of which has its unresolved issues. One-stage
feature alignment methods can easily lead to performance fluctuation and
training stagnation. Two-stage feature alignment method based on mean teacher
comprises a pretraining stage followed by a self-training stage, each facing
problems in obtaining reliable pretrained model and achieving consistent
performance gains. Methods mentioned above have not yet explore how to utilize
the third related domain such as target-like domain to assist adaptation. To
address these issues, we propose a two-stage framework named MTM, i.e. Mean
Teacher-DETR with Masked Feature Alignment. In the pretraining stage, we
utilize labeled target-like images produced by image style transfer to avoid
performance fluctuation. In the self-training stage, we leverage unlabeled
target images by pseudo labels based on mean teacher and propose a module
called Object Queries Knowledge Transfer (OQKT) to ensure consistent
performance gains of the student model. Most importantly, we propose masked
feature alignment methods including Masked Domain Query-based Feature Alignment
(MDQFA) and Masked Token-wise Feature Alignment (MTWFA) to alleviate domain
shift in a more robust way, which not only prevent training stagnation and lead
to a robust pretrained model in the pretraining stage, but also enhance the
model&apos;s target performance in the self-training stage. Experiments on three
challenging scenarios and a theoretical analysis verify the effectiveness of
MTM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weng_W/0/1/0/all/0/1&quot;&gt;Weixi Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1&quot;&gt;Chun Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17796">
<title>ControlLLM: Augment Language Models with Tools by Searching on Graphs. (arXiv:2310.17796v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.17796</link>
<description rdf:parseType="Literal">&lt;p&gt;We present ControlLLM, a novel framework that enables large language models
(LLMs) to utilize multi-modal tools for solving complex real-world tasks.
Despite the remarkable performance of LLMs, they still struggle with tool
invocation due to ambiguous user prompts, inaccurate tool selection and
parameterization, and inefficient tool scheduling. To overcome these
challenges, our framework comprises three key components: (1) a \textit{task
decomposer} that breaks down a complex task into clear subtasks with
well-defined inputs and outputs; (2) a \textit{Thoughts-on-Graph (ToG)
paradigm} that searches the optimal solution path on a pre-built tool graph,
which specifies the parameter and dependency relations among different tools;
and (3) an \textit{execution engine with a rich toolbox} that interprets the
solution path and runs the tools efficiently on different computational
devices. We evaluate our framework on diverse tasks involving image, audio, and
video processing, demonstrating its superior accuracy, efficiency, and
versatility compared to existing methods. The code is at
https://github.com/OpenGVLab/ControlLLM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhaoyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_Z/0/1/0/all/0/1&quot;&gt;Zeqiang Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1&quot;&gt;Zhangwei Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_E/0/1/0/all/0/1&quot;&gt;Erfei Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Ziheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xizhou Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1&quot;&gt;Lewei Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qifeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1&quot;&gt;Jifeng Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenhai Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18651">
<title>PW-Self: Patch-Wise Self-Supervised Visual Representation Learning. (arXiv:2310.18651v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18651</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised visual representation learning traditionally focuses on
image-level instance discrimination. Our study introduces an innovative
dimension by integrating patch-level discrimination into these methodologies.
This integration allows for the simultaneous analysis of both local and global
visual features, thereby enriching the quality of the representations learned.
Initially, the original images undergo spatial augmentation. Subsequently, we
employ a distinctive photometric patch-level augmentation, where each patch is
individually augmented, independent from other patches within the same view.
This approach generates a diverse training dataset with distinct color
variations in each segment. The augmented images are then processed through a
self-distillation learning framework, utilizing the Vision Transformer (ViT) as
its backbone. The proposed method minimizes the representation distances across
both image and patch levels to capture details from macro to micro
perspectives. To this end, we present a simple yet effective patch-matching
algorithm that can find the corresponding patches across the augmented views.
Thanks to the efficient structure of the patch-matching algorithm, our method
reduces computational complexity compared to similar approaches. Consequently,
we achieve an advanced understanding of the model without adding significant
computational requirements. We have extensively pre-trained our method on
datasets of varied scales, such as Cifar10, ImageNet-100, and ImageNet-1K. It
demonstrates superior performance over state-of-the-art self-supervised
representation learning methods in image classification and downstream tasks,
such as copy detection and image retrieval. The implementation of our method is
accessible on GitHub.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Javidani_A/0/1/0/all/0/1&quot;&gt;Ali Javidani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadeghi_M/0/1/0/all/0/1&quot;&gt;Mohammad Amin Sadeghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Araabi_B/0/1/0/all/0/1&quot;&gt;Babak Nadjar Araabi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18890">
<title>Towards Generalized Multi-stage Clustering: Multi-view Self-distillation. (arXiv:2310.18890v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18890</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing multi-stage clustering methods independently learn the salient
features from multiple views and then perform the clustering task.
Particularly, multi-view clustering (MVC) has attracted a lot of attention in
multi-view or multi-modal scenarios. MVC aims at exploring common semantics and
pseudo-labels from multiple views and clustering in a self-supervised manner.
However, limited by noisy data and inadequate feature learning, such a
clustering paradigm generates overconfident pseudo-labels that mis-guide the
model to produce inaccurate predictions. Therefore, it is desirable to have a
method that can correct this pseudo-label mistraction in multi-stage clustering
to avoid the bias accumulation. To alleviate the effect of overconfident
pseudo-labels and improve the generalization ability of the model, this paper
proposes a novel multi-stage deep MVC framework where multi-view
self-distillation (DistilMVC) is introduced to distill dark knowledge of label
distribution. Specifically, in the feature subspace at different hierarchies,
we explore the common semantics of multiple views through contrastive learning
and obtain pseudo-labels by maximizing the mutual information between views.
Additionally, a teacher network is responsible for distilling pseudo-labels
into dark knowledge, supervising the student network and improving its
predictive capabilities to enhance the robustness. Extensive experiments on
real-world multi-view datasets show that our method has better clustering
performance than state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiatai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tao Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19629">
<title>RayDF: Neural Ray-surface Distance Fields with Multi-view Consistency. (arXiv:2310.19629v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19629</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the problem of continuous 3D shape representations.
The majority of existing successful methods are coordinate-based implicit
neural representations. However, they are inefficient to render novel views or
recover explicit surface points. A few works start to formulate 3D shapes as
ray-based neural functions, but the learned structures are inferior due to the
lack of multi-view geometry consistency. To tackle these challenges, we propose
a new framework called RayDF. It consists of three major components: 1) the
simple ray-surface distance field, 2) the novel dual-ray visibility classifier,
and 3) a multi-view consistency optimization module to drive the learned
ray-surface distances to be multi-view geometry consistent. We extensively
evaluate our method on three public datasets, demonstrating remarkable
performance in 3D surface point reconstruction on both synthetic and
challenging real-world 3D scenes, clearly surpassing existing coordinate-based
and ray-based baselines. Most notably, our method achieves a 1000x faster speed
than coordinate-based methods to render an 800x800 depth image, showing the
superiority of our method for 3D shape representation. Our code and data are
available at https://github.com/vLAR-group/RayDF
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhuoman Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luximon_Y/0/1/0/all/0/1&quot;&gt;Yan Luximon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Ajay Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jinxi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04498">
<title>NExT-Chat: An LMM for Chat, Detection and Segmentation. (arXiv:2311.04498v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.04498</link>
<description rdf:parseType="Literal">&lt;p&gt;The development of large language models (LLMs) has greatly advanced the
field of multimodal understanding, leading to the emergence of large multimodal
models (LMMs). In order to enhance the level of visual comprehension, recent
studies have equipped LMMs with region-level understanding capabilities by
representing object bounding box coordinates as a series of text sequences
(pix2seq). In this paper, we introduce a novel paradigm for object location
modeling called pix2emb method, where we ask the LMM to output the location
embeddings and then decode them with different decoders. This paradigm allows
us to use different location formats (such as bounding boxes and masks) in
multimodal conversations. Leveraging the proposed pix2emb method, we train an
LMM named NExT-Chat and demonstrate its capability of handling multiple tasks
like visual grounding, region captioning, and grounded reasoning. Comprehensive
experiments show the effectiveness of our NExT-Chat on various tasks, e.g.,
NExT-Chat (87.7) vs. Shikra (86.9) on POPE-Random, NExT-Chat (68.9) vs. LISA
(67.9) on referring expression segmentation task, and NExT-Chat (79.6) vs.
Kosmos-2 (62.3) on region caption task. The code and model are released at
https://github.com/NExT-ChatV/NExT-Chat.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Ao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yuan Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1&quot;&gt;Wei Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1&quot;&gt;Tat-Seng Chua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10522">
<title>Enhancing Object Coherence in Layout-to-Image Synthesis. (arXiv:2311.10522v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10522</link>
<description rdf:parseType="Literal">&lt;p&gt;Layout-to-image synthesis is an emerging technique in conditional image
generation. It aims to generate complex scenes, where users require fine
control over the layout of the objects in a scene. However, it remains
challenging to control the object coherence, including semantic coherence
(e.g., the cat looks at the flowers or not) and physical coherence (e.g., the
hand and the racket should not be misaligned). In this paper, we propose a
novel diffusion model with effective global semantic fusion (GSF) and
self-similarity feature enhancement modules to guide the object coherence for
this task. For semantic coherence, we argue that the image caption contains
rich information for defining the semantic relationship within the objects in
the images. Instead of simply employing cross-attention between captions and
generated images, which addresses the highly relevant layout restriction and
semantic coherence separately and thus leads to unsatisfying results shown in
our experiments, we develop GSF to fuse the supervision from the layout
restriction and semantic coherence requirement and exploit it to guide the
image synthesis process. Moreover, to improve the physical coherence, we
develop a Self-similarity Coherence Attention (SCA) module to explicitly
integrate local contextual physical coherence into each pixel&apos;s generation
process. Specifically, we adopt a self-similarity map to encode the coherence
restrictions and employ it to extract coherent features from text embedding.
Through visualization of our self-similarity map, we explore the essence of
SCA, revealing that its effectiveness is not only in capturing reliable
physical coherence patterns but also in enhancing complex texture generation.
Extensive experiments demonstrate the superiority of our proposed method in
both image generation quality and controllability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yibin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weizhong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Jianwei Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1&quot;&gt;Cheng Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11125">
<title>SecondPose: SE(3)-Consistent Dual-Stream Feature Fusion for Category-Level Pose Estimation. (arXiv:2311.11125v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11125</link>
<description rdf:parseType="Literal">&lt;p&gt;Category-level object pose estimation, aiming to predict the 6D pose and 3D
size of objects from known categories, typically struggles with large
intra-class shape variation. Existing works utilizing mean shapes often fall
short of capturing this variation. To address this issue, we present
SecondPose, a novel approach integrating object-specific geometric features
with semantic category priors from DINOv2. Leveraging the advantage of DINOv2
in providing SE(3)-consistent semantic features, we hierarchically extract two
types of SE(3)-invariant geometric features to further encapsulate
local-to-global object-specific information. These geometric features are then
point-aligned with DINOv2 features to establish a consistent object
representation under SE(3) transformations, facilitating the mapping from
camera space to the pre-defined canonical space, thus further enhancing pose
estimation. Extensive experiments on NOCS-REAL275 demonstrate that SecondPose
achieves a 12.4% leap forward over the state-of-the-art. Moreover, on a more
complex dataset HouseCat6D which provides photometrically challenging objects,
SecondPose still surpasses other competitors by a large margin. The code will
be released soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yamei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Di_Y/0/1/0/all/0/1&quot;&gt;Yan Di&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1&quot;&gt;Guangyao Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manhardt_F/0/1/0/all/0/1&quot;&gt;Fabian Manhardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chenyangguang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruida Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1&quot;&gt;Federico Tombari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1&quot;&gt;Nassir Navab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1&quot;&gt;Benjamin Busam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12813">
<title>Targeted Activation Penalties Help CNNs Ignore Spurious Signals. (arXiv:2311.12813v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12813</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks (NNs) can learn to rely on spurious signals in the training
data, leading to poor generalisation. Recent methods tackle this problem by
training NNs with additional ground-truth annotations of such signals. These
methods may, however, let spurious signals re-emerge in deep convolutional NNs
(CNNs). We propose Targeted Activation Penalty (TAP), a new method tackling the
same problem by penalising activations to control the re-emergence of spurious
signals in deep CNNs, while also lowering training times and memory usage. In
addition, ground-truth annotations can be expensive to obtain. We show that TAP
still works well with annotations generated by pre-trained models as effective
substitutes of ground-truth annotations. We demonstrate the power of TAP
against two state-of-the-art baselines on the MNIST benchmark and on two
clinical image datasets, using four different CNN architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dekai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Williams_M/0/1/0/all/0/1&quot;&gt;Matthew Williams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toni_F/0/1/0/all/0/1&quot;&gt;Francesca Toni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13444">
<title>SkeletonGait: Gait Recognition Using Skeleton Maps. (arXiv:2311.13444v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13444</link>
<description rdf:parseType="Literal">&lt;p&gt;The choice of the representations is essential for deep gait recognition
methods. The binary silhouettes and skeletal coordinates are two dominant
representations in recent literature, achieving remarkable advances in many
scenarios. However, inherent challenges remain, in which silhouettes are not
always guaranteed in unconstrained scenes, and structural cues have not been
fully utilized from skeletons. In this paper, we introduce a novel skeletal
gait representation named skeleton map, together with SkeletonGait, a
skeleton-based method to exploit structural information from human skeleton
maps. Specifically, the skeleton map represents the coordinates of human joints
as a heatmap with Gaussian approximation, exhibiting a silhouette-like image
devoid of exact body structure. Beyond achieving state-of-the-art performances
over five popular gait datasets, more importantly, SkeletonGait uncovers novel
insights about how important structural features are in describing gait and
when they play a role. Furthermore, we propose a multi-branch architecture,
named SkeletonGait++, to make use of complementary features from both skeletons
and silhouettes. Experiments indicate that SkeletonGait++ outperforms existing
state-of-the-art methods by a significant margin in various scenarios. For
instance, it achieves an impressive rank-1 accuracy of over 85% on the
challenging GREW dataset. All the source code is available at
https://github.com/ShiqiYu/OpenGait.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1&quot;&gt;Chao Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jingzhe Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1&quot;&gt;Dongyang Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Chuanfu Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Shiqi Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15145">
<title>Choosing Wisely and Learning Deeply: Selective Cross-Modality Distillation via CLIP for Domain Generalization. (arXiv:2311.15145v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.15145</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain Generalization (DG), a crucial research area, seeks to train models
across multiple domains and test them on unseen ones. In this paper, we
introduce a novel approach, namely, Selective Cross-Modality Distillation for
Domain Generalization (SCMD). SCMD leverages the capabilities of large
vision-language models, specifically the CLIP model, to train a more efficient
model, ensuring it acquires robust generalization capabilities across unseen
domains. Our primary contribution is a unique selection framework strategically
designed to identify hard-to-learn samples for distillation. In parallel, we
introduce a novel cross-modality module. This module seamlessly combines the
projected features of the student model with the text embeddings from CLIP,
ensuring the alignment of similarity distributions. We assess SCMD&apos;s
performance on various benchmarks, where it empowers a ResNet50 to deliver
state-of-the-art performance, surpassing existing domain generalization
methods. Furthermore, we provide a theoretical analysis of our selection
strategy, offering deeper insight into its effectiveness and potential in the
field of DG.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leng_J/0/1/0/all/0/1&quot;&gt;Jixuan Leng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yijiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haohan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00347">
<title>RTQ: Rethinking Video-language Understanding Based on Image-text Model. (arXiv:2312.00347v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.00347</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in video-language understanding have been established on
the foundation of image-text models, resulting in promising outcomes due to the
shared knowledge between images and videos. However, video-language
understanding presents unique challenges due to the inclusion of highly complex
semantic details, which result in information redundancy, temporal dependency,
and scene complexity. Current techniques have only partially tackled these
issues, and our quantitative analysis indicates that some of these methods are
complementary. In light of this, we propose a novel framework called RTQ
(Refine, Temporal model, and Query), which addresses these challenges
simultaneously. The approach involves refining redundant information within
frames, modeling temporal relations among frames, and querying task-specific
information from the videos. Remarkably, our model demonstrates outstanding
performance even in the absence of video-language pre-training, and the results
are comparable with or superior to those achieved by state-of-the-art
pre-training methods. Code is available at
https://github.com/SCZwangxiao/RTQ-MM2023.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yaoyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_T/0/1/0/all/0/1&quot;&gt;Tian Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1&quot;&gt;Jingjing Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1&quot;&gt;Liqiang Nie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00377">
<title>SynFundus: A synthetic fundus images dataset with millions of samples and multi-disease annotations. (arXiv:2312.00377v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.00377</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of medical imaging, there are seldom large-scale public datasets
with high-quality annotations due to data privacy and annotation cost. To
address this issue, we release SynFundus-1M, a high-quality synthetic dataset
containing over \textbf{1 million} fundus images w.r.t. 11 disease types.
Moreover, we intentionally diversify the readability of the images and
accordingly provide 4 types of the quality score for each image. To the best of
our knowledge, SynFundus-1M is currently the largest fundus dataset with the
most sophisticated annotations. All the images are generated by a Denoising
Diffusion Probabilistic Model, named SynFundus-Generator. Trained with over 1.3
million private fundus images, our SynFundus-Generator achieves significant
superior performance in generating fundus images compared to some recent
related works. Furthermore, we blend some synthetic images from SynFundus-1M
with real fundus images, and ophthalmologists can hardly distinguish the
synthetic images from real ones. Through extensive experiments, we demonstrate
that both convolutional neural networs (CNN) and Vision Transformer (ViT) can
benefit from SynFundus-1M by pretraining or training directly. Compared to
datasets like ImageNet or EyePACS, models trained on SynFundus-1M not only
achieve better performance but also faster convergence on various downstream
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1&quot;&gt;Fangxin Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yehui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Haifeng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Junwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lei Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01597">
<title>SCLIP: Rethinking Self-Attention for Dense Vision-Language Inference. (arXiv:2312.01597v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.01597</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in contrastive language-image pretraining (CLIP) have
demonstrated strong capabilities in zero-shot classification by aligning visual
representations with target text embeddings in an image level. However, in
dense prediction tasks, CLIP often struggles to localize visual features within
an image and fails to give accurate pixel-level predictions, which prevents it
from functioning as a generalized visual foundation model. In this work, we aim
to enhance CLIP&apos;s potential for semantic segmentation with minimal
modifications to its pretrained models. By rethinking self-attention, we
surprisingly find that CLIP can adapt to dense prediction tasks by simply
introducing a novel Correlative Self-Attention (CSA) mechanism. Specifically,
we replace the traditional self-attention block of CLIP vision encoder&apos;s last
layer by our CSA module and reuse its pretrained projection matrices of query,
key, and value, leading to a training-free adaptation approach for CLIP&apos;s
zero-shot semantic segmentation. Extensive experiments show the advantage of
CSA: we obtain a 38.2% average zero-shot mIoU across eight semantic
segmentation benchmarks highlighted in this paper, significantly outperforming
the existing SoTA&apos;s 33.9% and the vanilla CLIP&apos;s 14.1%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Feng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1&quot;&gt;Jieru Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1&quot;&gt;Alan Yuille&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02222">
<title>InvertAvatar: Incremental GAN Inversion for Generalized Head Avatars. (arXiv:2312.02222v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02222</link>
<description rdf:parseType="Literal">&lt;p&gt;While high fidelity and efficiency are central to the creation of digital
head avatars, recent methods relying on 2D or 3D generative models often
experience limitations such as shape distortion, expression inaccuracy, and
identity flickering. Additionally, existing one-shot inversion techniques fail
to fully leverage multiple input images for detailed feature extraction. We
propose a novel framework, \textbf{Incremental 3D GAN Inversion}, that enhances
avatar reconstruction performance using an algorithm designed to increase the
fidelity from multiple frames, resulting in improved reconstruction quality
proportional to frame count. Our method introduces a unique animatable 3D GAN
prior with two crucial modifications for enhanced expression controllability
alongside an innovative neural texture encoder that categorizes texture feature
spaces based on UV parameterization. Differentiating from traditional
techniques, our architecture emphasizes pixel-aligned image-to-image
translation, mitigating the need to learn correspondences between observation
and canonical spaces. Furthermore, we incorporate ConvGRU-based recurrent
networks for temporal data aggregation from multiple frames, boosting geometry
and texture detail reconstruction. The proposed paradigm demonstrates
state-of-the-art performance on one-shot and few-shot avatar animation tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xiaochen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jingxiang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lizhen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yebin Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02246">
<title>Conditional Variational Diffusion Models. (arXiv:2312.02246v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02246</link>
<description rdf:parseType="Literal">&lt;p&gt;Inverse problems aim to determine parameters from observations, a crucial
task in engineering and science. Lately, generative models, especially
diffusion models, have gained popularity in this area for their ability to
produce realistic solutions and their good mathematical properties. Despite
their success, an important drawback of diffusion models is their sensitivity
to the choice of variance schedule, which controls the dynamics of the
diffusion process. Fine-tuning this schedule for specific applications is
crucial but time-costly and does not guarantee an optimal result. We propose a
novel approach for learning the schedule as part of the training process. Our
method supports probabilistic conditioning on data, provides high-quality
solutions, and is flexible, proving able to adapt to different applications
with minimum overhead. This approach is tested in two unrelated inverse
problems: super-resolution microscopy and quantitative phase imaging, yielding
comparable or superior results to previous methods and fine-tuned diffusion
models. We conclude that fine-tuning the schedule by experimentation should be
avoided because it can be learned during training in a stable way that yields
better results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maggiora_G/0/1/0/all/0/1&quot;&gt;Gabriel della Maggiora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Croquevielle_L/0/1/0/all/0/1&quot;&gt;Luis Alberto Croquevielle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Desphande_N/0/1/0/all/0/1&quot;&gt;Nikita Desphande&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horsley_H/0/1/0/all/0/1&quot;&gt;Harry Horsley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heinis_T/0/1/0/all/0/1&quot;&gt;Thomas Heinis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yakimovich_A/0/1/0/all/0/1&quot;&gt;Artur Yakimovich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04875">
<title>MVDD: Multi-View Depth Diffusion Models. (arXiv:2312.04875v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04875</link>
<description rdf:parseType="Literal">&lt;p&gt;Denoising diffusion models have demonstrated outstanding results in 2D image
generation, yet it remains a challenge to replicate its success in 3D shape
generation. In this paper, we propose leveraging multi-view depth, which
represents complex 3D shapes in a 2D data format that is easy to denoise. We
pair this representation with a diffusion model, MVDD, that is capable of
generating high-quality dense point clouds with 20K+ points with fine-grained
details. To enforce 3D consistency in multi-view depth, we introduce an
epipolar line segment attention that conditions the denoising step for a view
on its neighboring views. Additionally, a depth fusion module is incorporated
into diffusion steps to further ensure the alignment of depth maps. When
augmented with surface reconstruction, MVDD can also produce high-quality 3D
meshes. Furthermore, MVDD stands out in other tasks such as depth completion,
and can serve as a 3D prior, significantly boosting many downstream tasks, such
as GAN inversion. State-of-the-art results from extensive experiments
demonstrate MVDD&apos;s excellent ability in 3D shape generation, depth completion,
and its potential as a 3D prior for downstream tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qiangeng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_F/0/1/0/all/0/1&quot;&gt;Feitong Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chai_M/0/1/0/all/0/1&quot;&gt;Menglei Chai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shichen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandey_R/0/1/0/all/0/1&quot;&gt;Rohit Pandey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fanello_S/0/1/0/all/0/1&quot;&gt;Sean Fanello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kadambi_A/0/1/0/all/0/1&quot;&gt;Achuta Kadambi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yinda Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05797">
<title>Multimodality in Online Education: A Comparative Study. (arXiv:2312.05797v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.05797</link>
<description rdf:parseType="Literal">&lt;p&gt;The commencement of the decade brought along with it a grave pandemic and in
response the movement of education forums predominantly into the online world.
With a surge in the usage of online video conferencing platforms and tools to
better gauge student understanding, there needs to be a mechanism to assess
whether instructors can grasp the extent to which students understand the
subject and their response to the educational stimuli. The current systems
consider only a single cue with a lack of focus in the educational domain.
Thus, there is a necessity for the measurement of an all-encompassing holistic
overview of the students&apos; reaction to the subject matter. This paper highlights
the need for a multimodal approach to affect recognition and its deployment in
the online classroom while considering four cues, posture and gesture, facial,
eye tracking and verbal recognition. It compares the various machine learning
models available for each cue and provides the most suitable approach given the
available dataset and parameters of classroom footage. A multimodal approach
derived from weighted majority voting is proposed by combining the most fitting
models from this analysis of individual cues based on accuracy, ease of
procuring data corpus, sensitivity and any major drawbacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Immadisetty_P/0/1/0/all/0/1&quot;&gt;Praneeta Immadisetty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajesh_P/0/1/0/all/0/1&quot;&gt;Pooja Rajesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Akshita Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+R_A/0/1/0/all/0/1&quot;&gt;Anala M R&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+A_S/0/1/0/all/0/1&quot;&gt;Soumya A&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramanya_K/0/1/0/all/0/1&quot;&gt;K. N. Subramanya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06198">
<title>Optimized View and Geometry Distillation from Multi-view Diffuser. (arXiv:2312.06198v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06198</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating multi-view images from a single input view using image-conditioned
diffusion models is a recent advancement and has shown considerable potential.
However, issues such as the lack of consistency in synthesized views and
over-smoothing in extracted geometry persist. Previous methods integrate
multi-view consistency modules or impose additional supervisory to enhance view
consistency while compromising on the flexibility of camera positioning and
limiting the versatility of view synthesis. In this study, we consider the
radiance field optimized during geometry extraction as a more rigid consistency
prior, compared to volume and ray aggregation used in previous works. We
further identify and rectify a critical bias in the traditional radiance field
optimization process through score distillation from a multi-view diffuser. We
introduce an Unbiased Score Distillation (USD) that utilizes unconditioned
noises from a 2D diffusion model, greatly refining the radiance field fidelity.
we leverage the rendered views from the optimized radiance field as the basis
and develop a two-step specialization process of a 2D diffusion model, which is
adept at conducting object-specific denoising and generating high-quality
multi-view images. Finally, we recover faithful geometry and texture directly
from the refined multi-view images. Empirical evaluations demonstrate that our
optimized geometry and view distillation technique generates comparable results
to the state-of-the-art models trained on extensive datasets, all while
maintaining freedom in camera positioning. Please see our project page at
https://youjiazhang.github.io/USD/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Youjia Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Junqing Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zikai Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wei Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06372">
<title>Ternary Spike: Learning Ternary Spikes for Spiking Neural Networks. (arXiv:2312.06372v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06372</link>
<description rdf:parseType="Literal">&lt;p&gt;The Spiking Neural Network (SNN), as one of the biologically inspired neural
network infrastructures, has drawn increasing attention recently. It adopts
binary spike activations to transmit information, thus the multiplications of
activations and weights can be substituted by additions, which brings high
energy efficiency. However, in the paper, we theoretically and experimentally
prove that the binary spike activation map cannot carry enough information,
thus causing information loss and resulting in accuracy decreasing. To handle
the problem, we propose a ternary spike neuron to transmit information. The
ternary spike neuron can also enjoy the event-driven and multiplication-free
operation advantages of the binary spike neuron but will boost the information
capacity. Furthermore, we also embed a trainable factor in the ternary spike
neuron to learn the suitable spike amplitude, thus our SNN will adopt different
spike amplitudes along layers, which can better suit the phenomenon that the
membrane potential distributions are different along layers. To retain the
efficiency of the vanilla ternary spike, the trainable ternary spike SNN will
be converted to a standard one again via a re-parameterization technique in the
inference. Extensive experiments with several popular network structures over
static and dynamic datasets show that the ternary spike can consistently
outperform state-of-the-art methods. Our code is open-sourced at
https://github.com/yfguo91/Ternary-Spike.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yufei Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuanpei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaode Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1&quot;&gt;Weihang Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuhan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xuhui Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zhe Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06833">
<title>The unreasonable effectiveness of AI CADe polyp detectors to generalize to new countries. (arXiv:2312.06833v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06833</link>
<description rdf:parseType="Literal">&lt;p&gt;$\textbf{Background and aims}$: Artificial Intelligence (AI) Computer-Aided
Detection (CADe) is commonly used for polyp detection, but data seen in
clinical settings can differ from model training. Few studies evaluate how well
CADe detectors perform on colonoscopies from countries not seen during
training, and none are able to evaluate performance without collecting
expensive and time-intensive labels.
&lt;/p&gt;
&lt;p&gt;$\textbf{Methods}$: We trained a CADe polyp detector on Israeli colonoscopy
videos (5004 videos, 1106 hours) and evaluated on Japanese videos (354 videos,
128 hours) by measuring the True Positive Rate (TPR) versus false alarms per
minute (FAPM). We introduce a colonoscopy dissimilarity measure called &quot;MAsked
mediCal Embedding Distance&quot; (MACE) to quantify differences between
colonoscopies, without labels. We evaluated CADe on all Japan videos and on
those with the highest MACE.
&lt;/p&gt;
&lt;p&gt;$\textbf{Results}$: MACE correctly quantifies that narrow-band imaging (NBI)
and chromoendoscopy (CE) frames are less similar to Israel data than Japan
whitelight (bootstrapped z-test, |z| &amp;gt; 690, p &amp;lt; $10^{-8}$ for both). Despite
differences in the data, CADe performance on Japan colonoscopies was
non-inferior to Israel ones without additional training (TPR at 0.5 FAPM: 0.957
and 0.972 for Israel and Japan; TPR at 1.0 FAPM: 0.972 and 0.989 for Israel and
Japan; superiority test t &amp;gt; 45.2, p &amp;lt; $10^{-8}$). Despite not being trained on
NBI or CE, TPR on those subsets were non-inferior to Japan overall
(non-inferiority test t &amp;gt; 47.3, p &amp;lt; $10^{-8}$, $\delta$ = 1.5% for both).
&lt;/p&gt;
&lt;p&gt;$\textbf{Conclusion}$: Differences that prevent CADe detectors from
performing well in non-medical settings do not degrade the performance of our
AI CADe polyp detector when applied to data from a new country. MACE can help
medical AI models internationalize by identifying the most &quot;dissimilar&quot; data on
which to evaluate models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shor_J/0/1/0/all/0/1&quot;&gt;Joel Shor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamano_H/0/1/0/all/0/1&quot;&gt;Hiro-o Yamano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsurumaru_D/0/1/0/all/0/1&quot;&gt;Daisuke Tsurumaru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Intrator_Y/0/1/0/all/0/1&quot;&gt;Yotami Intrator&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kayama_H/0/1/0/all/0/1&quot;&gt;Hiroki Kayama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ledsam_J/0/1/0/all/0/1&quot;&gt;Joe Ledsam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamabe_A/0/1/0/all/0/1&quot;&gt;Atsushi Hamabe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ando_K/0/1/0/all/0/1&quot;&gt;Koji Ando&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ota_M/0/1/0/all/0/1&quot;&gt;Mitsuhiko Ota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ogino_H/0/1/0/all/0/1&quot;&gt;Haruei Ogino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakase_H/0/1/0/all/0/1&quot;&gt;Hiroshi Nakase&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kobayashi_K/0/1/0/all/0/1&quot;&gt;Kaho Kobayashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oki_E/0/1/0/all/0/1&quot;&gt;Eiji Oki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldenberg_R/0/1/0/all/0/1&quot;&gt;Roman Goldenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rivlin_E/0/1/0/all/0/1&quot;&gt;Ehud Rivlin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takemasa_I/0/1/0/all/0/1&quot;&gt;Ichiro Takemasa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06988">
<title>MWSIS: Multimodal Weakly Supervised Instance Segmentation with 2D Box Annotations for Autonomous Driving. (arXiv:2312.06988v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06988</link>
<description rdf:parseType="Literal">&lt;p&gt;Instance segmentation is a fundamental research in computer vision,
especially in autonomous driving. However, manual mask annotation for instance
segmentation is quite time-consuming and costly. To address this problem, some
prior works attempt to apply weakly supervised manner by exploring 2D or 3D
boxes. However, no one has ever successfully segmented 2D and 3D instances
simultaneously by only using 2D box annotations, which could further reduce the
annotation cost by an order of magnitude. Thus, we propose a novel framework
called Multimodal Weakly Supervised Instance Segmentation (MWSIS), which
incorporates various fine-grained label generation and correction modules for
both 2D and 3D modalities to improve the quality of pseudo labels, along with a
new multimodal cross-supervision approach, named Consistency Sparse Cross-modal
Supervision (CSCS), to reduce the inconsistency of multimodal predictions by
response distillation. Particularly, transferring the 3D backbone to downstream
tasks not only improves the performance of the 3D detectors, but also
outperforms fully supervised instance segmentation with only 5% fully
supervised annotations. On the Waymo dataset, the proposed framework
demonstrates significant improvements over the baseline, especially achieving
2.59% mAP and 12.75% mAP increases for 2D and 3D instance segmentation tasks,
respectively. The code is available at
https://github.com/jiangxb98/mwsis-plugin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_G/0/1/0/all/0/1&quot;&gt;Guangfeng Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yuzhi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1&quot;&gt;Wenlong Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1&quot;&gt;Tao He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_P/0/1/0/all/0/1&quot;&gt;Pai Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07823">
<title>Semantic Lens: Instance-Centric Semantic Alignment for Video Super-Resolution. (arXiv:2312.07823v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07823</link>
<description rdf:parseType="Literal">&lt;p&gt;As a critical clue of video super-resolution (VSR), inter-frame alignment
significantly impacts overall performance. However, accurate pixel-level
alignment is a challenging task due to the intricate motion interweaving in the
video. In response to this issue, we introduce a novel paradigm for VSR named
Semantic Lens, predicated on semantic priors drawn from degraded videos.
Specifically, video is modeled as instances, events, and scenes via a Semantic
Extractor. Those semantics assist the Pixel Enhancer in understanding the
recovered contents and generating more realistic visual results. The distilled
global semantics embody the scene information of each frame, while the
instance-specific semantics assemble the spatial-temporal contexts related to
each instance. Furthermore, we devise a Semantics-Powered Attention
Cross-Embedding (SPACE) block to bridge the pixel-level features with semantic
knowledge, composed of a Global Perspective Shifter (GPS) and an
Instance-Specific Semantic Embedding Encoder (ISEE). Concretely, the GPS module
generates pairs of affine transformation parameters for pixel-level feature
modulation conditioned on global semantics. After that, the ISEE module
harnesses the attention mechanism to align the adjacent frames in the
instance-centric semantic space. In addition, we incorporate a simple yet
effective pre-alignment module to alleviate the difficulty of model training.
Extensive experiments demonstrate the superiority of our model over existing
state-of-the-art VSR methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Q/0/1/0/all/0/1&quot;&gt;Qi Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Meiqin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1&quot;&gt;Jian Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_C/0/1/0/all/0/1&quot;&gt;Chao Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08168">
<title>Chat-3D v2: Bridging 3D Scene and Large Language Models with Object Identifiers. (arXiv:2312.08168v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08168</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent research has evidenced the significant potentials of Large Language
Models (LLMs) in handling challenging tasks within 3D scenes. However, current
models are constrained to addressing object-centric tasks, where each
question-answer pair focuses solely on an individual object. In real-world
applications, users may pose queries involving multiple objects or expect for
answers that precisely reference various objects. We introduce the use of
object identifiers to freely reference objects during a conversation. While
this solution appears straightforward, it presents two main challenges: 1) How
to establish a reliable one-to-one correspondence between each object and its
identifier? 2) How to incorporate complex spatial relationships among dozens of
objects into the embedding space of the LLM? To address these challenges, we
propose a two-stage alignment method, which involves learning an
attribute-aware token and a relation-aware token for each object. These tokens
capture the object&apos;s attributes and spatial relationships with surrounding
objects in the 3D scene. Once the alignment is established, we can fine-tune
our model on various downstream tasks using instruction tuning. Experiments
conducted on traditional datasets like ScanQA, ScanRefer, and Nr3D/Sr3D
showcase the effectiveness of our proposed method. Additionally, we create a 3D
scene captioning dataset annotated with rich object identifiers, with the
assistant of GPT-4. This dataset aims to further explore the capability of
object identifiers in effective object referencing and precise scene
understanding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Haifeng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zehan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1&quot;&gt;Rongjie Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Luping Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xize Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_T/0/1/0/all/0/1&quot;&gt;Tao Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhou Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08343">
<title>Enhancing CT Image synthesis from multi-modal MRI data based on a multi-task neural network framework. (arXiv:2312.08343v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08343</link>
<description rdf:parseType="Literal">&lt;p&gt;Image segmentation, real-value prediction, and cross-modal translation are
critical challenges in medical imaging. In this study, we propose a versatile
multi-task neural network framework, based on an enhanced Transformer U-Net
architecture, capable of simultaneously, selectively, and adaptively addressing
these medical image tasks. Validation is performed on a public repository of
human brain MR and CT images. We decompose the traditional problem of
synthesizing CT images into distinct subtasks, which include skull
segmentation, Hounsfield unit (HU) value prediction, and image sequential
reconstruction. To enhance the framework&apos;s versatility in handling multi-modal
data, we expand the model with multiple image channels. Comparisons between
synthesized CT images derived from T1-weighted and T2-Flair images were
conducted, evaluating the model&apos;s capability to integrate multi-modal
information from both morphological and pixel value perspectives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xin_Z/0/1/0/all/0/1&quot;&gt;Zhuoyao Xin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Christopher Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gu_C/0/1/0/all/0/1&quot;&gt;Chunming Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jia Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hua_J/0/1/0/all/0/1&quot;&gt;Jun Hua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08606">
<title>VQCNIR: Clearer Night Image Restoration with Vector-Quantized Codebook. (arXiv:2312.08606v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08606</link>
<description rdf:parseType="Literal">&lt;p&gt;Night photography often struggles with challenges like low light and
blurring, stemming from dark environments and prolonged exposures. Current
methods either disregard priors and directly fitting end-to-end networks,
leading to inconsistent illumination, or rely on unreliable handcrafted priors
to constrain the network, thereby bringing the greater error to the final
result. We believe in the strength of data-driven high-quality priors and
strive to offer a reliable and consistent prior, circumventing the restrictions
of manual priors. In this paper, we propose Clearer Night Image Restoration
with Vector-Quantized Codebook (VQCNIR) to achieve remarkable and consistent
restoration outcomes on real-world and synthetic benchmarks. To ensure the
faithful restoration of details and illumination, we propose the incorporation
of two essential modules: the Adaptive Illumination Enhancement Module (AIEM)
and the Deformable Bi-directional Cross-Attention (DBCA) module. The AIEM
leverages the inter-channel correlation of features to dynamically maintain
illumination consistency between degraded features and high-quality codebook
features. Meanwhile, the DBCA module effectively integrates texture and
structural information through bi-directional cross-attention and deformable
convolution, resulting in enhanced fine-grained detail and structural fidelity
across parallel decoders. Extensive experiments validate the remarkable
benefits of VQCNIR in enhancing image quality under low-light conditions,
showcasing its state-of-the-art performance on both synthetic and real-world
datasets. The code is available at https://github.com/AlexZou14/VQCNIR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_W/0/1/0/all/0/1&quot;&gt;Wenbin Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1&quot;&gt;Hongxia Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1&quot;&gt;Tian Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Liang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Weipeng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shasha Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hongsheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Sixiang Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08746">
<title>DreamDrone. (arXiv:2312.08746v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08746</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce DreamDrone, an innovative method for generating unbounded
flythrough scenes from textual prompts. Central to our method is a novel
feature-correspondence-guidance diffusion process, which utilizes the strong
correspondence of intermediate features in the diffusion model. Leveraging this
guidance strategy, we further propose an advanced technique for editing the
intermediate latent code, enabling the generation of subsequent novel views
with geometric consistency. Extensive experiments reveal that DreamDrone
significantly surpasses existing methods, delivering highly authentic scene
generation with exceptional visual quality. This approach marks a significant
step in zero-shot perpetual view generation from textual prompts, enabling the
creation of diverse scenes, including natural landscapes like oases and caves,
as well as complex urban settings such as Lego-style street views. Our code is
publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_H/0/1/0/all/0/1&quot;&gt;Hanyang Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1&quot;&gt;Dongze Lian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mi_M/0/1/0/all/0/1&quot;&gt;Michael Bi Mi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinchao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08916">
<title>Progressive Feature Self-reinforcement for Weakly Supervised Semantic Segmentation. (arXiv:2312.08916v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08916</link>
<description rdf:parseType="Literal">&lt;p&gt;Compared to conventional semantic segmentation with pixel-level supervision,
Weakly Supervised Semantic Segmentation (WSSS) with image-level labels poses
the challenge that it always focuses on the most discriminative regions,
resulting in a disparity between fully supervised conditions. A typical
manifestation is the diminished precision on the object boundaries, leading to
a deteriorated accuracy of WSSS. To alleviate this issue, we propose to
adaptively partition the image content into deterministic regions (e.g.,
confident foreground and background) and uncertain regions (e.g., object
boundaries and misclassified categories) for separate processing. For uncertain
cues, we employ an activation-based masking strategy and seek to recover the
local information with self-distilled knowledge. We further assume that the
unmasked confident regions should be robust enough to preserve the global
semantics. Building upon this, we introduce a complementary self-enhancement
method that constrains the semantic consistency between these confident regions
and an augmented image with the same class labels. Extensive experiments
conducted on PASCAL VOC 2012 and MS COCO 2014 demonstrate that our proposed
single-stage approach for WSSS not only outperforms state-of-the-art benchmarks
remarkably but also surpasses multi-stage methodologies that trade complexity
for accuracy. The code can be found at
\url{https://github.com/Jessie459/feature-self-reinforcement}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jingxuan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1&quot;&gt;Lechao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1&quot;&gt;Chaowei Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1&quot;&gt;Zunlei Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1&quot;&gt;Tingting Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1&quot;&gt;Mingli Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08985">
<title>OMG: Towards Open-vocabulary Motion Generation via Mixture of Controllers. (arXiv:2312.08985v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08985</link>
<description rdf:parseType="Literal">&lt;p&gt;We have recently seen tremendous progress in realistic text-to-motion
generation. Yet, the existing methods often fail or produce implausible motions
with unseen text inputs, which limits the applications. In this paper, we
present OMG, a novel framework, which enables compelling motion generation from
zero-shot open-vocabulary text prompts. Our key idea is to carefully tailor the
pretrain-then-finetune paradigm into the text-to-motion generation. At the
pre-training stage, our model improves the generation ability by learning the
rich out-of-domain inherent motion traits. To this end, we scale up a large
unconditional diffusion model up to 1B parameters, so as to utilize the massive
unlabeled motion data up to over 20M motion instances. At the subsequent
fine-tuning stage, we introduce motion ControlNet, which incorporates text
prompts as conditioning information, through a trainable copy of the
pre-trained model and the proposed novel Mixture-of-Controllers (MoC) block.
MoC block adaptively recognizes various ranges of the sub-motions with a
cross-attention mechanism and processes them separately with the
text-token-specific experts. Such a design effectively aligns the CLIP token
embeddings of text prompts to various ranges of compact and expressive motion
features. Extensive experiments demonstrate that our OMG achieves significant
improvements over the state-of-the-art methods on zero-shot text-to-motion
generation. Project page: https://tr3e.github.io/omg-page.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1&quot;&gt;Han Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1&quot;&gt;Jiacheng Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruichi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1&quot;&gt;Sihan Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yuecheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Sibei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jingyi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Lan Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09038">
<title>Object Recognition from Scientific Document based on Compartment Refinement Framework. (arXiv:2312.09038v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.09038</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rapid development of the internet in the past decade, it has become
increasingly important to extract valuable information from vast resources
efficiently, which is crucial for establishing a comprehensive digital
ecosystem, particularly in the context of research surveys and comprehension.
The foundation of these tasks focuses on accurate extraction and deep mining of
data from scientific documents, which are essential for building a robust data
infrastructure. However, parsing raw data or extracting data from complex
scientific documents have been ongoing challenges. Current data extraction
methods for scientific documents typically use rule-based (RB) or machine
learning (ML) approaches. However, using rule-based methods can incur high
coding costs for articles with intricate typesetting. Conversely, relying
solely on machine learning methods necessitates annotation work for complex
content types within the scientific document, which can be costly.
Additionally, few studies have thoroughly defined and explored the hierarchical
layout within scientific documents. The lack of a comprehensive definition of
the internal structure and elements of the documents indirectly impacts the
accuracy of text classification and object recognition tasks. From the
perspective of analyzing the standard layout and typesetting used in the
specified publication, we propose a new document layout analysis framework
called CTBR(Compartment &amp;amp; Text Blocks Refinement). Firstly, we define
scientific documents into hierarchical divisions: base domain, compartment, and
text blocks. Next, we conduct an in-depth exploration and classification of the
meanings of text blocks. Finally, we utilize the results of text block
classification to implement object recognition within scientific documents
based on rule-based compartment segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jinghong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_W/0/1/0/all/0/1&quot;&gt;Wen Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ota_K/0/1/0/all/0/1&quot;&gt;Koichi Ota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasegawa_S/0/1/0/all/0/1&quot;&gt;Shinobu Hasegawa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09783">
<title>Keep the Faith: Faithful Explanations in Convolutional Neural Networks for Case-Based Reasoning. (arXiv:2312.09783v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.09783</link>
<description rdf:parseType="Literal">&lt;p&gt;Explaining predictions of black-box neural networks is crucial when applied
to decision-critical tasks. Thus, attribution maps are commonly used to
identify important image regions, despite prior work showing that humans prefer
explanations based on similar examples. To this end, ProtoPNet learns a set of
class-representative feature vectors (prototypes) for case-based reasoning.
During inference, similarities of latent features to prototypes are linearly
classified to form predictions and attribution maps are provided to explain the
similarity. In this work, we evaluate whether architectures for case-based
reasoning fulfill established axioms required for faithful explanations using
the example of ProtoPNet. We show that such architectures allow the extraction
of faithful explanations. However, we prove that the attribution maps used to
explain the similarities violate the axioms. We propose a new procedure to
extract explanations for trained ProtoPNets, named ProtoPFaith. Conceptually,
these explanations are Shapley values, calculated on the similarity scores of
each prototype. They allow to faithfully answer which prototypes are present in
an unseen image and quantify each pixel&apos;s contribution to that presence,
thereby complying with all axioms. The theoretical violations of ProtoPNet
manifest in our experiments on three datasets (CUB-200-2011, Stanford Dogs,
RSNA) and five architectures (ConvNet, ResNet, ResNet50, WideResNet50,
ResNeXt50). Our experiments show a qualitative difference between the
explanations given by ProtoPNet and ProtoPFaith. Additionally, we quantify the
explanations with the Area Over the Perturbation Curve, on which ProtoPFaith
outperforms ProtoPNet on all experiments by a factor $&amp;gt;10^3$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolf_T/0/1/0/all/0/1&quot;&gt;Tom Nuno Wolf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bongratz_F/0/1/0/all/0/1&quot;&gt;Fabian Bongratz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rickmann_A/0/1/0/all/0/1&quot;&gt;Anne-Marie Rickmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Polsterl_S/0/1/0/all/0/1&quot;&gt;Sebastian P&amp;#xf6;lsterl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wachinger_C/0/1/0/all/0/1&quot;&gt;Christian Wachinger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09909">
<title>TMP: Temporal Motion Propagation for Online Video Super-Resolution. (arXiv:2312.09909v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.09909</link>
<description rdf:parseType="Literal">&lt;p&gt;Online video super-resolution (online-VSR) highly relies on an effective
alignment module to aggregate temporal information, while the strict latency
requirement makes accurate and efficient alignment very challenging. Though
much progress has been achieved, most of the existing online-VSR methods
estimate the motion fields of each frame separately to perform alignment, which
is computationally redundant and ignores the fact that the motion fields of
adjacent frames are correlated. In this work, we propose an efficient Temporal
Motion Propagation (TMP) method, which leverages the continuity of motion field
to achieve fast pixel-level alignment among consecutive frames. Specifically,
we first propagate the offsets from previous frames to the current frame, and
then refine them in the neighborhood, which significantly reduces the matching
space and speeds up the offset estimation process. Furthermore, to enhance the
robustness of alignment, we perform spatial-wise weighting on the warped
features, where the positions with more precise offsets are assigned higher
importance. Experiments on benchmark datasets demonstrate that the proposed TMP
method achieves leading online-VSR accuracy as well as inference speed. The
source code of TMP can be found at https://github.com/xtudbxk/TMP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhengqiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ruihuang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1&quot;&gt;Shi Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yang Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.01402">
<title>GALAXY: Graph-based Active Learning at the Extreme. (arXiv:2202.01402v2 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2202.01402</link>
<description rdf:parseType="Literal">&lt;p&gt;Active learning is a label-efficient approach to train highly effective
models while interactively selecting only small subsets of unlabelled data for
labelling and training. In &quot;open world&quot; settings, the classes of interest can
make up a small fraction of the overall dataset -- most of the data may be
viewed as an out-of-distribution or irrelevant class. This leads to extreme
class-imbalance, and our theory and methods focus on this core issue. We
propose a new strategy for active learning called GALAXY (Graph-based Active
Learning At the eXtrEme), which blends ideas from graph-based active learning
and deep learning. GALAXY automatically and adaptively selects more
class-balanced examples for labeling than most other methods for active
learning. Our theory shows that GALAXY performs a refined form of uncertainty
sampling that gathers a much more class-balanced dataset than vanilla
uncertainty sampling. Experimentally, we demonstrate GALAXY&apos;s superiority over
existing state-of-art deep active learning algorithms in unbalanced vision
classification settings generated from popular datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jifan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katz_Samuels_J/0/1/0/all/0/1&quot;&gt;Julian Katz-Samuels&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nowak_R/0/1/0/all/0/1&quot;&gt;Robert Nowak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.14673">
<title>Using Machine Learning to generate an open-access cropland map from satellite images time series in the Indian Himalayan Region. (arXiv:2203.14673v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2203.14673</link>
<description rdf:parseType="Literal">&lt;p&gt;Crop maps are crucial for agricultural monitoring and food management and can
additionally support domain-specific applications, such as setting cold supply
chain infrastructure in developing countries. Machine learning (ML) models,
combined with freely-available satellite imagery, can be used to produce
cost-effective and high spatial-resolution crop maps. However, accessing ground
truth data for supervised learning is especially challenging in developing
countries due to factors such as smallholding and fragmented geography, which
often results in a lack of crop type maps or even reliable cropland maps. Our
area of interest for this study lies in Himachal Pradesh, India, where we aim
at producing an open-access binary cropland map at 10-meter resolution for the
Kullu, Shimla, and Mandi districts. To this end, we developed an ML pipeline
that relies on Sentinel-2 satellite images time series. We investigated two
pixel-based supervised classifiers, support vector machines (SVM) and random
forest (RF), which are used to classify per-pixel time series for binary
cropland mapping. The ground truth data used for training, validation and
testing was manually annotated from a combination of field survey reference
points and visual interpretation of very high resolution (VHR) imagery. We
trained and validated the models via spatial cross-validation to account for
local spatial autocorrelation and selected the RF model due to overall
robustness and lower computational cost. We tested the generalization
capability of the chosen model at the pixel level by computing the accuracy,
recall, precision, and F1-score on hold-out test sets of each district,
achieving an average accuracy for the RF (our best model) of 87%. We used this
model to generate a cropland map for three districts of Himachal Pradesh,
spanning 14,600 km2, which improves the resolution and quality of existing
public maps.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Danya Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gajardo_J/0/1/0/all/0/1&quot;&gt;Joaquin Gajardo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Volpi_M/0/1/0/all/0/1&quot;&gt;Michele Volpi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Defraeye_T/0/1/0/all/0/1&quot;&gt;Thijs Defraeye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.07317">
<title>Algorithm Selection for Deep Active Learning with Imbalanced Datasets. (arXiv:2302.07317v3 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2302.07317</link>
<description rdf:parseType="Literal">&lt;p&gt;Label efficiency has become an increasingly important objective in deep
learning applications. Active learning aims to reduce the number of labeled
examples needed to train deep networks, but the empirical performance of active
learning algorithms can vary dramatically across datasets and applications. It
is difficult to know in advance which active learning strategy will perform
well or best in a given application. To address this, we propose the first
adaptive algorithm selection strategy for deep active learning. For any
unlabeled dataset, our (meta) algorithm TAILOR (Thompson ActIve Learning
algORithm selection) iteratively and adaptively chooses among a set of
candidate active learning algorithms. TAILOR uses novel reward functions aimed
at gathering class-balanced examples. Extensive experiments in multi-class and
multi-label applications demonstrate TAILOR&apos;s effectiveness in achieving
accuracy comparable or better than that of the best of the candidate
algorithms. Our implementation of TAILOR is open-sourced at
https://github.com/jifanz/TAILOR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jifan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_S/0/1/0/all/0/1&quot;&gt;Shuai Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1&quot;&gt;Saurabh Verma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nowak_R/0/1/0/all/0/1&quot;&gt;Robert Nowak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.08870">
<title>UPGPT: Universal Diffusion Model for Person Image Generation, Editing and Pose Transfer. (arXiv:2304.08870v2 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2304.08870</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-image models (T2I) such as StableDiffusion have been used to generate
high quality images of people. However, due to the random nature of the
generation process, the person has a different appearance e.g. pose, face, and
clothing, despite using the same text prompt. The appearance inconsistency
makes T2I unsuitable for pose transfer. We address this by proposing a
multimodal diffusion model that accepts text, pose, and visual prompting. Our
model is the first unified method to perform all person image tasks -
generation, pose transfer, and mask-less edit. We also pioneer using small
dimensional 3D body model parameters directly to demonstrate new capability -
simultaneous pose and camera view interpolation while maintaining the person&apos;s
appearance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheong_S/0/1/0/all/0/1&quot;&gt;Soon Yau Cheong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mustafa_A/0/1/0/all/0/1&quot;&gt;Armin Mustafa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilbert_A/0/1/0/all/0/1&quot;&gt;Andrew Gilbert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09910">
<title>LabelBench: A Comprehensive Framework for Benchmarking Adaptive Label-Efficient Learning. (arXiv:2306.09910v2 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2306.09910</link>
<description rdf:parseType="Literal">&lt;p&gt;Labeled data are critical to modern machine learning applications, but
obtaining labels can be expensive. To mitigate this cost, machine learning
methods, such as transfer learning, semi-supervised learning and active
learning, aim to be label-efficient: achieving high predictive performance from
relatively few labeled examples. While obtaining the best label-efficiency in
practice often requires combinations of these techniques, existing benchmark
and evaluation frameworks do not capture a concerted combination of all such
techniques. This paper addresses this deficiency by introducing LabelBench, a
new computationally-efficient framework for joint evaluation of multiple
label-efficient learning techniques. As an application of LabelBench, we
introduce a novel benchmark of state-of-the-art active learning methods in
combination with semi-supervised learning for fine-tuning pretrained vision
transformers. Our benchmark demonstrates better label-efficiencies than
previously reported in active learning. LabelBench&apos;s modular codebase is
open-sourced for the broader community to contribute label-efficient learning
methods and benchmarks. The repository can be found at:
https://github.com/EfficientTraining/LabelBench.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jifan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yifang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Canal_G/0/1/0/all/0/1&quot;&gt;Gregory Canal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mussmann_S/0/1/0/all/0/1&quot;&gt;Stephen Mussmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1&quot;&gt;Arnav M. Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatt_G/0/1/0/all/0/1&quot;&gt;Gantavya Bhatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yinglun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1&quot;&gt;Simon Shaolei Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jamieson_K/0/1/0/all/0/1&quot;&gt;Kevin Jamieson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nowak_R/0/1/0/all/0/1&quot;&gt;Robert D Nowak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03154">
<title>ViscoNet: Bridging and Harmonizing Visual and Textual Conditioning for ControlNet. (arXiv:2312.03154v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2312.03154</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces ViscoNet, a novel method that enhances text-to-image
human generation models with visual prompting. Unlike existing methods that
rely on lengthy text descriptions to control the image structure, ViscoNet
allows users to specify the visual appearance of the target object with a
reference image. ViscoNet disentangles the object&apos;s appearance from the image
background and injects it into a pre-trained latent diffusion model (LDM) model
via a ControlNet branch. This way, ViscoNet mitigates the style mode collapse
problem and enables precise and flexible visual control. We demonstrate the
effectiveness of ViscoNet on human image generation, where it can manipulate
visual attributes and artistic styles with text and image prompts. We also show
that ViscoNet can learn visual conditioning from small and specific object
domains while preserving the generative power of the LDM backbone.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheong_S/0/1/0/all/0/1&quot;&gt;Soon Yau Cheong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mustafa_A/0/1/0/all/0/1&quot;&gt;Armin Mustafa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilbert_A/0/1/0/all/0/1&quot;&gt;Andrew Gilbert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09196">
<title>DIRECT: Deep Active Learning under Imbalance and Label Noise. (arXiv:2312.09196v1 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2312.09196</link>
<description rdf:parseType="Literal">&lt;p&gt;Class imbalance is a prevalent issue in real world machine learning
applications, often leading to poor performance in rare and minority classes.
With an abundance of wild unlabeled data, active learning is perhaps the most
effective technique in solving the problem at its root -- collecting a more
balanced and informative set of labeled examples during annotation. In this
work, we propose a novel algorithm that first identifies the class separation
threshold and then annotate the most uncertain examples from the minority
classes, close to the separation threshold. Through a novel reduction to
one-dimensional active learning, our algorithm DIRECT is able to leverage the
classic active learning literature to address issues such as batch labeling and
tolerance towards label noise. Compared to existing algorithms, our algorithm
saves more than 15\% of the annotation budget compared to state-of-art active
learning algorithm and more than 90\% of annotation budget compared to random
sampling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nuggehalli_S/0/1/0/all/0/1&quot;&gt;Shyam Nuggehalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jifan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_L/0/1/0/all/0/1&quot;&gt;Lalit Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nowak_R/0/1/0/all/0/1&quot;&gt;Robert Nowak&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>