<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CL updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-12-06T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computation and Language</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03003" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03016" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03022" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03025" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03042" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03052" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03077" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03088" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03093" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03095" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03122" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03140" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03171" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03173" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03194" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03195" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03217" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03290" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03303" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03312" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03330" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03342" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03360" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03361" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03367" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03379" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03414" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03458" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03463" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03480" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03483" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03523" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03549" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03567" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03577" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03633" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03656" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03664" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03668" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03689" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03699" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03700" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.08225" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.10706" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.02160" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.12407" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.09710" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.07371" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.07320" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.09128" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05403" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.11746" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12524" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13388" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13406" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13675" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13683" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14246" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14387" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14815" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15065" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17390" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.20046" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09597" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02697" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.10954" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00752" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05736" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08577" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00694" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01907" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04064" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09428" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16173" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17030" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.18260" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01454" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02125" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02439" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2312.03003">
<title>Explore, Select, Derive, and Recall: Augmenting LLM with Human-like Memory for Mobile Task Automation. (arXiv:2312.03003v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2312.03003</link>
<description rdf:parseType="Literal">&lt;p&gt;The advent of large language models (LLMs) has opened up new opportunities in
the field of mobile task automation. Their superior language understanding and
reasoning capabilities allow users to automate complex and repetitive tasks.
However, due to the inherent unreliability and high operational cost of LLMs,
their practical applicability is quite limited. To address these issues, this
paper introduces MemoDroid, an innovative LLM-based mobile task automator
enhanced with a unique app memory. MemoDroid emulates the cognitive process of
humans interacting with a mobile app -- explore, select, derive, and recall.
This approach allows for a more precise and efficient learning of a task&apos;s
procedure by breaking it down into smaller, modular components that can be
re-used, re-arranged, and adapted for various objectives. We implement
MemoDroid using online LLMs services (GPT-3.5 and GPT-4) and evaluate its
performance on 50 unique mobile tasks across 5 widely used mobile apps. The
results indicate that MemoDroid can adapt learned tasks to varying contexts
with 100% accuracy and reduces their latency and cost by 69.22% and 77.36%
compared to a GPT-4 powered baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sunjae Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Junyoung Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jungjae Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1&quot;&gt;Hojun Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ko_S/0/1/0/all/0/1&quot;&gt;Steven Y. Ko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1&quot;&gt;Sangeun Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_I/0/1/0/all/0/1&quot;&gt;Insik Shin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03016">
<title>Protein Language Model-Powered 3D Ligand Binding Site Prediction from Protein Sequence. (arXiv:2312.03016v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/2312.03016</link>
<description rdf:parseType="Literal">&lt;p&gt;Prediction of ligand binding sites of proteins is a fundamental and important
task for understanding the function of proteins and screening potential drugs.
Most existing methods require experimentally determined protein holo-structures
as input. However, such structures can be unavailable on novel or less-studied
proteins. To tackle this limitation, we propose LaMPSite, which only takes
protein sequences and ligand molecular graphs as input for ligand binding site
predictions. The protein sequences are used to retrieve residue-level
embeddings and contact maps from the pre-trained ESM-2 protein language model.
The ligand molecular graphs are fed into a graph neural network to compute
atom-level embeddings. Then we compute and update the protein-ligand
interaction embedding based on the protein residue-level embeddings and ligand
atom-level embeddings, and the geometric constraints in the inferred protein
contact map and ligand distance map. A final pooling on protein-ligand
interaction embedding would indicate which residues belong to the binding
sites. Without any 3D coordinate information of proteins, our proposed model
achieves competitive performance compared to baseline methods that require 3D
protein structures when predicting binding sites. Given that less than 50% of
proteins have reliable structure information in the current stage, LaMPSite
will provide new opportunities for drug discovery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shuo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Xie_L/0/1/0/all/0/1&quot;&gt;Lei Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03022">
<title>Beyond Isolation: Multi-Agent Synergy for Improving Knowledge Graph Construction. (arXiv:2312.03022v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.03022</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge graph construction (KGC) is a multifaceted undertaking involving
the extraction of entities, relations, and events. Traditionally, large
language models (LLMs) have been viewed as solitary task-solving agents in this
complex landscape. However, this paper challenges this paradigm by introducing
a novel framework, CooperKGC. Departing from the conventional approach,
CooperKGC establishes a collaborative processing network, assembling a KGC
collaboration team capable of concurrently addressing entity, relation, and
event extraction tasks. Our experiments unequivocally demonstrate that
fostering collaboration and information interaction among diverse agents within
CooperKGC yields superior results compared to individual cognitive processes
operating in isolation. Importantly, our findings reveal that the collaboration
facilitated by CooperKGC enhances knowledge selection, correction, and
aggregation capabilities across multiple rounds of interactions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1&quot;&gt;Hongbin Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gui_H/0/1/0/all/0/1&quot;&gt;Honghao Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Aijia Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1&quot;&gt;Wei Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1&quot;&gt;Weiqiang Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03025">
<title>Training on Synthetic Data Beats Real Data in Multimodal Relation Extraction. (arXiv:2312.03025v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.03025</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of multimodal relation extraction has attracted significant research
attention, but progress is constrained by the scarcity of available training
data. One natural thought is to extend existing datasets with cross-modal
generative models. In this paper, we consider a novel problem setting, where
only unimodal data, either text or image, are available during training. We aim
to train a multimodal classifier from synthetic data that perform well on real
multimodal test data. However, training with synthetic data suffers from two
obstacles: lack of data diversity and label information loss. To alleviate the
issues, we propose Mutual Information-aware Multimodal Iterated Relational dAta
GEneration (MI2RAGE), which applies Chained Cross-modal Generation (CCG) to
promote diversity in the generated data and exploits a teacher network to
select valuable training samples with high mutual information with the
ground-truth labels. Comparing our method to direct training on synthetic data,
we observed a significant improvement of 24.06% F1 with synthetic text and
26.42% F1 with synthetic images. Notably, our best model trained on completely
synthetic images outperforms prior state-of-the-art models trained on real
multimodal data by a margin of 3.76% in F1. Our codebase will be made available
upon acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1&quot;&gt;Zilin Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haoxin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Boyang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03042">
<title>Inherent limitations of LLMs regarding spatial information. (arXiv:2312.03042v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.03042</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the significant advancements in natural language processing
capabilities demonstrated by large language models such as ChatGPT, their
proficiency in comprehending and processing spatial information, especially
within the domains of 2D and 3D route planning, remains notably underdeveloped.
This paper investigates the inherent limitations of ChatGPT and similar models
in spatial reasoning and navigation-related tasks, an area critical for
applications ranging from autonomous vehicle guidance to assistive technologies
for the visually impaired. In this paper, we introduce a novel evaluation
framework complemented by a baseline dataset, meticulously crafted for this
study. This dataset is structured around three key tasks: plotting spatial
points, planning routes in two-dimensional (2D) spaces, and devising pathways
in three-dimensional (3D) environments. We specifically developed this dataset
to assess the spatial reasoning abilities of ChatGPT. Our evaluation reveals
key insights into the model&apos;s capabilities and limitations in spatial
understanding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1&quot;&gt;He Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xinyao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1&quot;&gt;Xiangpeng Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chengyu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_K/0/1/0/all/0/1&quot;&gt;Kai Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shiqi Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03052">
<title>Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models. (arXiv:2312.03052v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.03052</link>
<description rdf:parseType="Literal">&lt;p&gt;Solving complex visual tasks such as &quot;Who invented the musical instrument on
the right?&quot; involves a composition of skills: understanding space, recognizing
instruments, and also retrieving prior knowledge. Recent work shows promise by
decomposing such tasks using a large language model (LLM) into an executable
program that invokes specialized vision models. However, generated programs are
error-prone: they omit necessary steps, include spurious ones, and are unable
to recover when the specialized models give incorrect outputs. Moreover, they
require loading multiple models, incurring high latency and computation costs.
We propose Visual Program Distillation (VPD), an instruction tuning framework
that produces a vision-language model (VLM) capable of solving complex visual
tasks with a single forward pass. VPD distills the reasoning ability of LLMs by
using them to sample multiple candidate programs, which are then executed and
verified to identify a correct one. It translates each correct program into a
language description of the reasoning steps, which are then distilled into a
VLM. Extensive experiments show that VPD improves the VLM&apos;s ability to count,
understand spatial relations, and reason compositionally. Our VPD-trained
PaLI-X outperforms all prior VLMs, achieving state-of-the-art performance
across complex vision tasks, including MMBench, OK-VQA, A-OKVQA, TallyQA, POPE,
and Hateful Memes. An evaluation with human annotators also confirms that VPD
improves model response factuality and consistency. Finally, experiments on
content moderation demonstrate that VPD is also helpful for adaptation to
real-world applications with limited data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yushi Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stretcu_O/0/1/0/all/0/1&quot;&gt;Otilia Stretcu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Chun-Ta Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Viswanathan_K/0/1/0/all/0/1&quot;&gt;Krishnamurthy Viswanathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hata_K/0/1/0/all/0/1&quot;&gt;Kenji Hata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_E/0/1/0/all/0/1&quot;&gt;Enming Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1&quot;&gt;Ranjay Krishna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fuxman_A/0/1/0/all/0/1&quot;&gt;Ariel Fuxman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03077">
<title>Clinical Notes Reveal Physician Fatigue. (arXiv:2312.03077v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.03077</link>
<description rdf:parseType="Literal">&lt;p&gt;Physicians write notes about patients. In doing so, they reveal much about
themselves. Using data from 129,228 emergency room visits, we train a model to
identify notes written by fatigued physicians -- those who worked 5 or more of
the prior 7 days. In a hold-out set, the model accurately identifies notes
written by these high-workload physicians, and also flags notes written in
other high-fatigue settings: on overnight shifts, and after high patient
volumes. Model predictions also correlate with worse decision-making on at
least one important metric: yield of testing for heart attack is 18% lower with
each standard deviation increase in model-predicted fatigue. Finally, the model
indicates that notes written about Black and Hispanic patients have 12% and 21%
higher predicted fatigue than Whites -- larger than overnight vs. daytime
differences. These results have an important implication for large language
models (LLMs). Our model indicates that fatigued doctors write more predictable
notes. Perhaps unsurprisingly, because word prediction is the core of how LLMs
work, we find that LLM-written notes have 17% higher predicted fatigue than
real physicians&apos; notes. This indicates that LLMs may introduce distortions in
generated text that are not yet fully understood.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1&quot;&gt;Chao-Chun Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Obermeyer_Z/0/1/0/all/0/1&quot;&gt;Ziad Obermeyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Chenhao Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03088">
<title>LLMs for Multi-Modal Knowledge Extraction and Analysis in Intelligence/Safety-Critical Applications. (arXiv:2312.03088v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.03088</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models have seen rapid progress in capability in recent years;
this progress has been accelerating and their capabilities, measured by various
benchmarks, are beginning to approach those of humans. There is a strong demand
to use such models in a wide variety of applications but, due to unresolved
vulnerabilities and limitations, great care needs to be used before applying
them to intelligence and safety-critical applications. This paper reviews
recent literature related to LLM assessment and vulnerabilities to synthesize
the current research landscape and to help understand what advances are most
critical to enable use of of these technologies in intelligence and
safety-critical applications. The vulnerabilities are broken down into ten
high-level categories and overlaid onto a high-level life cycle of an LLM. Some
general categories of mitigations are reviewed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Israelsen_B/0/1/0/all/0/1&quot;&gt;Brett Israelsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1&quot;&gt;Soumalya Sarkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03093">
<title>RESIN-EDITOR: A Schema-guided Hierarchical Event Graph Visualizer and Editor. (arXiv:2312.03093v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2312.03093</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present RESIN-EDITOR, an interactive event graph visualizer
and editor designed for analyzing complex events. Our RESIN-EDITOR system
allows users to render and freely edit hierarchical event graphs extracted from
multimedia and multi-document news clusters with guidance from human-curated
event schemas. RESIN-EDITOR&apos;s unique features include hierarchical graph
visualization, comprehensive source tracing, and interactive user editing,
which is more powerful and versatile than existing Information Extraction (IE)
visualization tools. In our evaluation of RESIN-EDITOR, we demonstrate ways in
which our tool is effective in understanding complex events and enhancing
system performance. The source code, a video demonstration, and a live website
for RESIN-EDITOR have been made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1&quot;&gt;Khanh Duy Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zixuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suchocki_R/0/1/0/all/0/1&quot;&gt;Reece Suchocki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sha Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palmer_M/0/1/0/all/0/1&quot;&gt;Martha Palmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_S/0/1/0/all/0/1&quot;&gt;Susan Brown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jiawei Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1&quot;&gt;Heng Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03095">
<title>Understanding Environmental Posts: Sentiment and Emotion Analysis of Social Media Data. (arXiv:2312.03095v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.03095</link>
<description rdf:parseType="Literal">&lt;p&gt;Social media is now the predominant source of information due to the
availability of immediate public response. As a result, social media data has
become a valuable resource for comprehending public sentiments. Studies have
shown that it can amplify ideas and influence public sentiments. This study
analyzes the public perception of climate change and the environment over a
decade from 2014 to 2023. Using the Pointwise Mutual Information (PMI)
algorithm, we identify sentiment and explore prevailing emotions expressed
within environmental tweets across various social media platforms, namely
Twitter, Reddit, and YouTube. Accuracy on a human-annotated dataset was 0.65,
higher than Vader score but lower than that of an expert rater (0.90). Our
findings suggest that negative environmental tweets are far more common than
positive or neutral ones. Climate change, air quality, emissions, plastic, and
recycling are the most discussed topics on all social media platforms,
highlighting its huge global concern. The most common emotions in environmental
tweets are fear, trust, and anticipation, demonstrating public reactions wide
and complex nature. By identifying patterns and trends in opinions related to
the environment, we hope to provide insights that can help raise awareness
regarding environmental issues, inform the development of interventions, and
adapt further actions to meet environmental challenges.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amangeldi_D/0/1/0/all/0/1&quot;&gt;Daniyar Amangeldi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Usmanova_A/0/1/0/all/0/1&quot;&gt;Aida Usmanova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shamoi_P/0/1/0/all/0/1&quot;&gt;Pakizar Shamoi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03122">
<title>Assertion Enhanced Few-Shot Learning: Instructive Technique for Large Language Models to Generate Educational Explanations. (arXiv:2312.03122v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.03122</link>
<description rdf:parseType="Literal">&lt;p&gt;Human educators possess an intrinsic ability to anticipate and seek
educational explanations from students, which drives them to pose
thought-provoking questions when students cannot articulate these explanations
independently. We aim to imbue Intelligent Tutoring Systems with this ability
using few-shot learning capability of Large Language Models. Our work proposes
a novel prompting technique, Assertion Enhanced Few-Shot Learning, to
facilitate the generation of accurate, detailed oriented educational
explanations. Our central hypothesis is that, in educational domain, few-shot
demonstrations are necessary but not a sufficient condition for quality
explanation generation. We conducted a study involving 12 in-service teachers,
comparing our approach to Traditional Few-Shot Learning. The results show that
Assertion Enhanced Few-Shot Learning improves explanation accuracy by 15% and
yields higher-quality explanations, as evaluated by teachers. We also conduct a
qualitative ablation study to factor the impact of assertions to provide
educator-friendly prompting guidelines for generating explanations in their
domain of interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahriar_T/0/1/0/all/0/1&quot;&gt;Tasmia Shahriar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matsuda_N/0/1/0/all/0/1&quot;&gt;Noboru Matsuda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramos_K/0/1/0/all/0/1&quot;&gt;Kelly Ramos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03140">
<title>FlexModel: A Framework for Interpretability of Distributed Large Language Models. (arXiv:2312.03140v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.03140</link>
<description rdf:parseType="Literal">&lt;p&gt;With the growth of large language models, now incorporating billions of
parameters, the hardware prerequisites for their training and deployment have
seen a corresponding increase. Although existing tools facilitate model
parallelization and distributed training, deeper model interactions, crucial
for interpretability and responsible AI techniques, still demand thorough
knowledge of distributed computing. This often hinders contributions from
researchers with machine learning expertise but limited distributed computing
background. Addressing this challenge, we present FlexModel, a software package
providing a streamlined interface for engaging with models distributed across
multi-GPU and multi-node configurations. The library is compatible with
existing model distribution libraries and encapsulates PyTorch models. It
exposes user-registerable HookFunctions to facilitate straightforward
interaction with distributed model internals, bridging the gap between
distributed and single-device model paradigms. Primarily, FlexModel enhances
accessibility by democratizing model interactions and promotes more inclusive
research in the domain of large-scale neural networks. The package is found at
https://github.com/VectorInstitute/flex_model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1&quot;&gt;Matthew Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asif_M/0/1/0/all/0/1&quot;&gt;Muhammad Adil Asif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Willes_J/0/1/0/all/0/1&quot;&gt;John Willes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emerson_D/0/1/0/all/0/1&quot;&gt;David Emerson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03171">
<title>Combining Counting Processes and Classification Improves a Stopping Rule for Technology Assisted Review. (arXiv:2312.03171v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2312.03171</link>
<description rdf:parseType="Literal">&lt;p&gt;Technology Assisted Review (TAR) stopping rules aim to reduce the cost of
manually assessing documents for relevance by minimising the number of
documents that need to be examined to ensure a desired level of recall. This
paper extends an effective stopping rule using information derived from a text
classifier that can be trained without the need for any additional annotation.
Experiments on multiple data sets (CLEF e-Health, TREC Total Recall, TREC Legal
and RCV1) showed that the proposed approach consistently improves performance
and outperforms several alternative methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bin_Hezam_R/0/1/0/all/0/1&quot;&gt;Reem Bin-Hezam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stevenson_M/0/1/0/all/0/1&quot;&gt;Mark Stevenson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03173">
<title>A Comparative Study of AI-Generated (GPT-4) and Human-crafted MCQs in Programming Education. (arXiv:2312.03173v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2312.03173</link>
<description rdf:parseType="Literal">&lt;p&gt;There is a constant need for educators to develop and maintain effective
up-to-date assessments. While there is a growing body of research in computing
education on utilizing large language models (LLMs) in generation and
engagement with coding exercises, the use of LLMs for generating programming
MCQs has not been extensively explored. We analyzed the capability of GPT-4 to
produce multiple-choice questions (MCQs) aligned with specific learning
objectives (LOs) from Python programming classes in higher education.
Specifically, we developed an LLM-powered (GPT-4) system for generation of MCQs
from high-level course context and module-level LOs. We evaluated 651
LLM-generated and 449 human-crafted MCQs aligned to 246 LOs from 6 Python
courses. We found that GPT-4 was capable of producing MCQs with clear language,
a single correct choice, and high-quality distractors. We also observed that
the generated MCQs appeared to be well-aligned with the LOs. Our findings can
be leveraged by educators wishing to take advantage of the state-of-the-art
generative models to support MCQ authoring efforts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doughty_J/0/1/0/all/0/1&quot;&gt;Jacob Doughty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1&quot;&gt;Zipiao Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bompelli_A/0/1/0/all/0/1&quot;&gt;Anishka Bompelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qayum_J/0/1/0/all/0/1&quot;&gt;Jubahed Qayum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Taozhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Juran Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yujia Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doyle_A/0/1/0/all/0/1&quot;&gt;Aidan Doyle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sridhar_P/0/1/0/all/0/1&quot;&gt;Pragnya Sridhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1&quot;&gt;Arav Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bogart_C/0/1/0/all/0/1&quot;&gt;Christopher Bogart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keylor_E/0/1/0/all/0/1&quot;&gt;Eric Keylor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kultur_C/0/1/0/all/0/1&quot;&gt;Can Kultur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savelka_J/0/1/0/all/0/1&quot;&gt;Jaromir Savelka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sakr_M/0/1/0/all/0/1&quot;&gt;Majd Sakr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03194">
<title>Corporate Bankruptcy Prediction with Domain-Adapted BERT. (arXiv:2312.03194v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.03194</link>
<description rdf:parseType="Literal">&lt;p&gt;This study performs BERT-based analysis, which is a representative
contextualized language model, on corporate disclosure data to predict
impending bankruptcies. Prior literature on bankruptcy prediction mainly
focuses on developing more sophisticated prediction methodologies with
financial variables. However, in our study, we focus on improving the quality
of input dataset. Specifically, we employ BERT model to perform sentiment
analysis on MD&amp;amp;A disclosures. We show that BERT outperforms dictionary-based
predictions and Word2Vec-based predictions in terms of adjusted R-square in
logistic regression, k-nearest neighbor (kNN-5), and linear kernel support
vector machine (SVM). Further, instead of pre-training the BERT model from
scratch, we apply self-learning with confidence-based filtering to corporate
disclosure data (10-K). We achieve the accuracy rate of 91.56% and demonstrate
that the domain adaptation procedure brings a significant improvement in
prediction accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1&quot;&gt;Alex Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1&quot;&gt;Sangwon Yoon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03195">
<title>Detecting Rumor Veracity with Only Textual Information by Double-Channel Structure. (arXiv:2312.03195v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.03195</link>
<description rdf:parseType="Literal">&lt;p&gt;Kyle (1985) proposes two types of rumors: informed rumors which are based on
some private information and uninformed rumors which are not based on any
information (i.e. bluffing). Also, prior studies find that when people have
credible source of information, they are likely to use a more confident textual
tone in their spreading of rumors. Motivated by these theoretical findings, we
propose a double-channel structure to determine the ex-ante veracity of rumors
on social media. Our ultimate goal is to classify each rumor into true, false,
or unverifiable category. We first assign each text into either certain
(informed rumor) or uncertain (uninformed rumor) category. Then, we apply lie
detection algorithm to informed rumors and thread-reply agreement detection
algorithm to uninformed rumors. Using the dataset of SemEval 2019 Task 7, which
requires ex-ante threefold classification (true, false, or unverifiable) of
social media rumors, our model yields a macro-F1 score of 0.4027, outperforming
all the baseline models and the second-place winner (Gorrell et al., 2019).
Furthermore, we empirically validate that the double-channel structure
outperforms single-channel structures which use either lie detection or
agreement detection algorithm to all posts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1&quot;&gt;Alex Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1&quot;&gt;Sangwon Yoon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03217">
<title>Rethinking E-Commerce Search. (arXiv:2312.03217v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2312.03217</link>
<description rdf:parseType="Literal">&lt;p&gt;E-commerce search and recommendation usually operate on structured data such
as product catalogs and taxonomies. However, creating better search and
recommendation systems often requires a large variety of unstructured data
including customer reviews and articles on the web. Traditionally, the solution
has always been converting unstructured data into structured data through
information extraction, and conducting search over the structured data.
However, this is a costly approach that often has low quality. In this paper,
we envision a solution that does entirely the opposite. Instead of converting
unstructured data (web pages, customer reviews, etc) to structured data, we
instead convert structured data (product inventory, catalogs, taxonomies, etc)
into textual data, which can be easily integrated into the text corpus that
trains LLMs. Then, search and recommendation can be performed through a Q/A
mechanism through an LLM instead of using traditional information retrieval
methods over structured data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haixun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Na_T/0/1/0/all/0/1&quot;&gt;Taesik Na&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03290">
<title>Can language agents be alternatives to PPO? A Preliminary Empirical Study On OpenAI Gym. (arXiv:2312.03290v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.03290</link>
<description rdf:parseType="Literal">&lt;p&gt;The formidable capacity for zero- or few-shot decision-making in language
agents encourages us to pose a compelling question: Can language agents be
alternatives to PPO agents in traditional sequential decision-making tasks? To
investigate this, we first take environments collected in OpenAI Gym as our
testbeds and ground them to textual environments that construct the TextGym
simulator. This allows for straightforward and efficient comparisons between
PPO agents and language agents, given the widespread adoption of OpenAI Gym. To
ensure a fair and effective benchmarking, we introduce $5$ levels of scenario
for accurate domain-knowledge controlling and a unified RL-inspired framework
for language agents. Additionally, we propose an innovative
explore-exploit-guided language (EXE) agent to solve tasks within TextGym.
Through numerical experiments and ablation studies, we extract valuable
insights into the decision-making capabilities of language agents and make a
preliminary evaluation of their potential to be alternatives to PPO in
classical sequential decision-making problems. This paper sheds light on the
performance of language agents and paves the way for future research in this
exciting domain. Our code is publicly available
at~\url{https://github.com/mail-ecnu/Text-Gym-Agents}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_J/0/1/0/all/0/1&quot;&gt;Junjie Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zixiao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Chuyun Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenhao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1&quot;&gt;Yun Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1&quot;&gt;Bo Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1&quot;&gt;Hongyuan Zha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiangfeng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03303">
<title>Dyport: Dynamic Importance-based Hypothesis Generation Benchmarking Technique. (arXiv:2312.03303v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.03303</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel benchmarking framework Dyport for evaluating
biomedical hypothesis generation systems. Utilizing curated datasets, our
approach tests these systems under realistic conditions, enhancing the
relevance of our evaluations. We integrate knowledge from the curated databases
into a dynamic graph, accompanied by a method to quantify discovery importance.
This not only assesses hypothesis accuracy but also their potential impact in
biomedical research which significantly extends traditional link prediction
benchmarks. Applicability of our benchmarking process is demonstrated on
several link prediction systems applied on biomedical semantic knowledge
graphs. Being flexible, our benchmarking system is designed for broad
application in hypothesis generation quality verification, aiming to expand the
scope of scientific discovery within the biomedical research community.
Availability and implementation: Dyport framework is fully open-source. All
code and datasets are available at: https://github.com/IlyaTyagin/Dyport
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tyagin_I/0/1/0/all/0/1&quot;&gt;Ilya Tyagin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Safro_I/0/1/0/all/0/1&quot;&gt;Ilya Safro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03312">
<title>Optimizing Two-Pass Cross-Lingual Transfer Learning: Phoneme Recognition and Phoneme to Grapheme Translation. (arXiv:2312.03312v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.03312</link>
<description rdf:parseType="Literal">&lt;p&gt;This research optimizes two-pass cross-lingual transfer learning in
low-resource languages by enhancing phoneme recognition and phoneme-to-grapheme
translation models. Our approach optimizes these two stages to improve speech
recognition across languages. We optimize phoneme vocabulary coverage by
merging phonemes based on shared articulatory characteristics, thus improving
recognition accuracy. Additionally, we introduce a global phoneme noise
generator for realistic ASR noise during phoneme-to-grapheme training to reduce
error propagation. Experiments on the CommonVoice 12.0 dataset show significant
reductions in Word Error Rate (WER) for low-resource languages, highlighting
the effectiveness of our approach. This research contributes to the
advancements of two-pass ASR systems in low-resource languages, offering the
potential for improved cross-lingual transfer learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1&quot;&gt;Wonjun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1&quot;&gt;Gary Geunbae Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yunsu Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03330">
<title>Measuring Misogyny in Natural Language Generation: Preliminary Results from a Case Study on two Reddit Communities. (arXiv:2312.03330v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.03330</link>
<description rdf:parseType="Literal">&lt;p&gt;Generic `toxicity&apos; classifiers continue to be used for evaluating the
potential for harm in natural language generation, despite mounting evidence of
their shortcomings. We consider the challenge of measuring misogyny in natural
language generation, and argue that generic `toxicity&apos; classifiers are
inadequate for this task. We use data from two well-characterised `Incel&apos;
communities on Reddit that differ primarily in their degrees of misogyny to
construct a pair of training corpora which we use to fine-tune two language
models. We show that an open source `toxicity&apos; classifier is unable to
distinguish meaningfully between generations from these models. We contrast
this with a misogyny-specific lexicon recently proposed by feminist
subject-matter experts, demonstrating that, despite the limitations of simple
lexicon-based approaches, this shows promise as a benchmark to evaluate
language models for misogyny, and that it is sensitive enough to reveal the
known differences in these Reddit communities. Our preliminary findings
highlight the limitations of a generic approach to evaluating harms, and
further emphasise the need for careful benchmark design and selection in
natural language evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Snoswell_A/0/1/0/all/0/1&quot;&gt;Aaron J. Snoswell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nelson_L/0/1/0/all/0/1&quot;&gt;Lucinda Nelson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1&quot;&gt;Hao Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salim_F/0/1/0/all/0/1&quot;&gt;Flora D. Salim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suzor_N/0/1/0/all/0/1&quot;&gt;Nicolas Suzor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burgess_J/0/1/0/all/0/1&quot;&gt;Jean Burgess&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03342">
<title>Topic and genre in dialogue. (arXiv:2312.03342v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.03342</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we argue that topic plays a fundamental role in conversations,
and that the concept is needed in addition to that of genre to define
interactions. In particular, the concepts of genre and topic need to be
separated and orthogonally defined. This would enable modular, reliable and
controllable flexible-domain dialogue systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Decker_A/0/1/0/all/0/1&quot;&gt;Amandine Decker&lt;/a&gt; (LORIA, GU, UL), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Breitholtz_E/0/1/0/all/0/1&quot;&gt;Ellen Breitholtz&lt;/a&gt; (GU), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Howes_C/0/1/0/all/0/1&quot;&gt;Christine Howes&lt;/a&gt; (GU), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Larsson_S/0/1/0/all/0/1&quot;&gt;Staffan Larsson&lt;/a&gt; (GU)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03360">
<title>Teaching Specific Scientific Knowledge into Large Language Models through Additional Training. (arXiv:2312.03360v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.03360</link>
<description rdf:parseType="Literal">&lt;p&gt;Through additional training, we explore embedding specialized scientific
knowledge into the Llama 2 Large Language Model (LLM). Key findings reveal that
effective knowledge integration requires reading texts from multiple
perspectives, especially in instructional formats. We utilize text augmentation
to tackle the scarcity of specialized texts, including style conversions and
translations. Hyperparameter optimization proves crucial, with different size
models (7b, 13b, and 70b) reasonably undergoing additional training. Validating
our methods, we construct a dataset of 65,000 scientific papers. Although we
have succeeded in partially embedding knowledge, the study highlights the
complexities and limitations of incorporating specialized information into
LLMs, suggesting areas for further improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hatakeyama_Sato_K/0/1/0/all/0/1&quot;&gt;Kan Hatakeyama-Sato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Igarashi_Y/0/1/0/all/0/1&quot;&gt;Yasuhiko Igarashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katakami_S/0/1/0/all/0/1&quot;&gt;Shun Katakami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nabae_Y/0/1/0/all/0/1&quot;&gt;Yuta Nabae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayakawa_T/0/1/0/all/0/1&quot;&gt;Teruaki Hayakawa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03361">
<title>KhabarChin: Automatic Detection of Important News in the Persian Language. (arXiv:2312.03361v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.03361</link>
<description rdf:parseType="Literal">&lt;p&gt;Being aware of important news is crucial for staying informed and making
well-informed decisions efficiently. Natural Language Processing (NLP)
approaches can significantly automate this process. This paper introduces the
detection of important news, in a previously unexplored area, and presents a
new benchmarking dataset (Khabarchin) for detecting important news in the
Persian language. We define important news articles as those deemed significant
for a considerable portion of society, capable of influencing their mindset or
decision-making. The news articles are obtained from seven different prominent
Persian news agencies, resulting in the annotation of 7,869 samples and the
creation of the dataset. Two challenges of high disagreement and imbalance
between classes were faced, and solutions were provided for them. We also
propose several learning-based models, ranging from conventional machine
learning to state-of-the-art transformer models, to tackle this task.
Furthermore, we introduce the second task of important sentence detection in
news articles, as they often come with a significant contextual length that
makes it challenging for readers to identify important information. We identify
these sentences in a weakly supervised manner.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hemati_H/0/1/0/all/0/1&quot;&gt;Hamed Hematian Hemati&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lagzian_A/0/1/0/all/0/1&quot;&gt;Arash Lagzian&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sartakhti_M/0/1/0/all/0/1&quot;&gt;Moein Salimi Sartakhti&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beigy_H/0/1/0/all/0/1&quot;&gt;Hamid Beigy&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asgari_E/0/1/0/all/0/1&quot;&gt;Ehsaneddin Asgari&lt;/a&gt; (2) ((1) AI Group, Computer Engineering Department, Sharif University of Technology, (2) AI Innovation, Data:Lab Munich, Volkswagen AG)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03367">
<title>Lazy-k: Decoding for Constrained Token Classification. (arXiv:2312.03367v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.03367</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore the possibility of improving probabilistic models in structured
prediction. Specifically, we combine the models with constrained decoding
approaches in the context of token classification for information extraction.
The decoding methods search for constraint-satisfying label-assignments while
maximizing the total probability. To do this, we evaluate several existing
approaches, as well as propose a novel decoding method called Lazy-$k$. Our
findings demonstrate that constrained decoding approaches can significantly
improve the models&apos; performances, especially when using smaller models. The
Lazy-$k$ approach allows for more flexibility between decoding time and
accuracy. The code for using Lazy-$k$ decoding can be found here:
https://github.com/ArthurDevNL/lazyk.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hemmer_A/0/1/0/all/0/1&quot;&gt;Arthur Hemmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coustaty_M/0/1/0/all/0/1&quot;&gt;Micka&amp;#xeb;l Coustaty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bartolo_N/0/1/0/all/0/1&quot;&gt;Nicola Bartolo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brachat_J/0/1/0/all/0/1&quot;&gt;J&amp;#xe9;r&amp;#xf4;me Brachat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ogier_J/0/1/0/all/0/1&quot;&gt;Jean-Marc Ogier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03379">
<title>A Text-to-Text Model for Multilingual Offensive Language Identification. (arXiv:2312.03379v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.03379</link>
<description rdf:parseType="Literal">&lt;p&gt;The ubiquity of offensive content on social media is a growing cause for
concern among companies and government organizations. Recently,
transformer-based models such as BERT, XLNET, and XLM-R have achieved
state-of-the-art performance in detecting various forms of offensive content
(e.g. hate speech, cyberbullying, and cyberaggression). However, the majority
of these models are limited in their capabilities due to their encoder-only
architecture, which restricts the number and types of labels in downstream
tasks. Addressing these limitations, this study presents the first pre-trained
model with encoder-decoder architecture for offensive language identification
with text-to-text transformers (T5) trained on two large offensive language
identification datasets; SOLID and CCTK. We investigate the effectiveness of
combining two datasets and selecting an optimal threshold in semi-supervised
instances in SOLID in the T5 retraining step. Our pre-trained T5 model
outperforms other transformer-based models fine-tuned for offensive language
detection, such as fBERT and HateBERT, in multiple English benchmarks.
Following a similar approach, we also train the first multilingual pre-trained
model for offensive language identification using mT5 and evaluate its
performance on a set of six different languages (German, Hindi, Korean,
Marathi, Sinhala, and Spanish). The results demonstrate that this multilingual
model achieves a new state-of-the-art on all the above datasets, showing its
usefulness in multilingual scenarios. Our proposed T5-based models will be made
freely available to the community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1&quot;&gt;Tharindu Ranasinghe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1&quot;&gt;Marcos Zampieri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03414">
<title>Compressed Context Memory For Online Language Model Interaction. (arXiv:2312.03414v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.03414</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel context compression method for Transformer
language models in online scenarios such as ChatGPT, where the context
continually expands. As the context lengthens, the attention process requires
more memory and computational resources, which in turn reduces the throughput
of the language model. To this end, we propose a compressed context memory
system that continually compresses the growing context into a compact memory
space. The compression process simply involves integrating a lightweight
conditional LoRA into the language model&apos;s forward pass during inference. Based
on the compressed context memory, the language model can perform inference with
reduced memory and attention operations. Through evaluations on conversation,
personalization, and multi-task learning, we demonstrate that our approach
achieves the performance level of a full context model with $5\times$ smaller
context memory space. Codes are available at
https://github.com/snu-mllab/context-memory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jang-Hyun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeom_J/0/1/0/all/0/1&quot;&gt;Junyoung Yeom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1&quot;&gt;Sangdoo Yun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1&quot;&gt;Hyun Oh Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03458">
<title>Think from Words(TFW): Initiating Human-Like Cognition in Large Language Models Through Think from Words for Japanese Text-level Classification. (arXiv:2312.03458v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.03458</link>
<description rdf:parseType="Literal">&lt;p&gt;The proliferation of Large Language Models (LLMs) has spurred extensive
research into LLM-related Prompt investigations, such as Instruction Learning
(IL), In-context Learning (ICL), and Chain-of-Thought (CoT). These approaches
aim to improve LLMs&apos; responses by enabling them to provide concise statements
or examples for deeper contemplation when addressing questions. However,
independent thinking by LLMs can introduce variability in their thought
processes, leading to potential inaccuracies. In response, our study seeks to
bridge the gap between LLM and human-like thinking processes, recognizing that
text comprehension begins with understanding individual words. To tackle this
challenge, we have expanded the CoT method to cater to a specific domain. Our
approach, known as &quot;Think from Words&quot; (TFW), initiates the comprehension
process at the word level and then extends it to encompass the entire text. We
also propose &quot;TFW with Extra word-level information&quot; (TFW Extra), augmenting
comprehension with additional word-level data. To assess our methods, we employ
text classification on six Japanese datasets comprising text-level and
word-level elements. Our findings not only validate the effectiveness of TFW
but also shed light on the impact of various word-level information types on
LLMs&apos; text comprehension, offering insights into their potential to cause
misinterpretations and errors in the overall comprehension of the final text.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1&quot;&gt;Chengguang Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qinghao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mori_T/0/1/0/all/0/1&quot;&gt;Tatsunori Mori&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03463">
<title>DBCopilot: Scaling Natural Language Querying to Massive Databases. (arXiv:2312.03463v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.03463</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-SQL simplifies database interactions by enabling non-experts to
convert their natural language (NL) questions into Structured Query Language
(SQL) queries. While recent advances in large language models (LLMs) have
improved the zero-shot text-to-SQL paradigm, existing methods face scalability
challenges when dealing with massive, dynamically changing databases. This
paper introduces DBCopilot, a framework that addresses these challenges by
employing a compact and flexible copilot model for routing across massive
databases. Specifically, DBCopilot decouples the text-to-SQL process into
schema routing and SQL generation, leveraging a lightweight
sequence-to-sequence neural network-based router to formulate database
connections and navigate natural language questions through databases and
tables. The routed schemas and questions are then fed into LLMs for efficient
SQL generation. Furthermore, DBCopilot also introduced a reverse
schema-to-question generation paradigm, which can learn and adapt the router
over massive databases automatically without requiring manual intervention.
Experimental results demonstrate that DBCopilot is a scalable and effective
solution for real-world text-to-SQL tasks, providing a significant advancement
in handling large-scale schemas.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianshu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1&quot;&gt;Hongyu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xianpei Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Le Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaoyang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03480">
<title>AMR Parsing is Far from Solved: GrAPES, the Granular AMR Parsing Evaluation Suite. (arXiv:2312.03480v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.03480</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the Granular AMR Parsing Evaluation Suite (GrAPES), a challenge
set for Abstract Meaning Representation (AMR) parsing with accompanying
evaluation metrics. AMR parsers now obtain high scores on the standard AMR
evaluation metric Smatch, close to or even above reported inter-annotator
agreement. But that does not mean that AMR parsing is solved; in fact, human
evaluation in previous work indicates that current parsers still quite
frequently make errors on node labels or graph structure that substantially
distort sentence meaning. Here, we provide an evaluation suite that tests AMR
parsers on a range of phenomena of practical, technical, and linguistic
interest. Our 36 categories range from seen and unseen labels, to structural
generalization, to coreference. GrAPES reveals in depth the abilities and
shortcomings of current AMR parsers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Groschwitz_J/0/1/0/all/0/1&quot;&gt;Jonas Groschwitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1&quot;&gt;Shay B. Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Donatelli_L/0/1/0/all/0/1&quot;&gt;Lucia Donatelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fowlie_M/0/1/0/all/0/1&quot;&gt;Meaghan Fowlie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03483">
<title>Exploring Answer Information Methods for Question Generation with Transformers. (arXiv:2312.03483v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.03483</link>
<description rdf:parseType="Literal">&lt;p&gt;There has been a lot of work in question generation where different methods
to provide target answers as input, have been employed. This experimentation
has been mostly carried out for RNN based models. We use three different
methods and their combinations for incorporating answer information and explore
their effect on several automatic evaluation metrics. The methods that are used
are answer prompting, using a custom product method using answer embeddings and
encoder outputs, choosing sentences from the input paragraph that have answer
related information, and using a separate cross-attention attention block in
the decoder which attends to the answer. We observe that answer prompting
without any additional modes obtains the best scores across rouge, meteor
scores. Additionally, we use a custom metric to calculate how many of the
generated questions have the same answer, as the answer which is used to
generate them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chafekar_T/0/1/0/all/0/1&quot;&gt;Talha Chafekar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hussain_A/0/1/0/all/0/1&quot;&gt;Aafiya Hussain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_G/0/1/0/all/0/1&quot;&gt;Grishma Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1&quot;&gt;Deepak Sharma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03523">
<title>Sig-Networks Toolkit: Signature Networks for Longitudinal Language Modelling. (arXiv:2312.03523v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.03523</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an open-source, pip installable toolkit, Sig-Networks, the first
of its kind for longitudinal language modelling. A central focus is the
incorporation of Signature-based Neural Network models, which have recently
shown success in temporal tasks. We apply and extend published research
providing a full suite of signature-based models. Their components can be used
as PyTorch building blocks in future architectures. Sig-Networks enables
task-agnostic dataset plug-in, seamless pre-processing for sequential data,
parameter flexibility, automated tuning across a range of models. We examine
signature networks under three different NLP tasks of varying temporal
granularity: counselling conversations, rumour stance switch and mood changes
in social media threads, showing SOTA performance in all three, and provide
guidance for future tasks. We release the Toolkit as a PyTorch package with an
introductory video, Git repositories for preprocessing and modelling including
sample notebooks on the modeled NLP tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tseriotou_T/0/1/0/all/0/1&quot;&gt;Talia Tseriotou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_R/0/1/0/all/0/1&quot;&gt;Ryan Sze-Yin Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsakalidis_A/0/1/0/all/0/1&quot;&gt;Adam Tsakalidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bilal_I/0/1/0/all/0/1&quot;&gt;Iman Munire Bilal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kochkina_E/0/1/0/all/0/1&quot;&gt;Elena Kochkina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyons_T/0/1/0/all/0/1&quot;&gt;Terry Lyons&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liakata_M/0/1/0/all/0/1&quot;&gt;Maria Liakata&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03549">
<title>Holmes: Towards Distributed Training Across Clusters with Heterogeneous NIC Environment. (arXiv:2312.03549v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.03549</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) such as GPT-3, OPT, and LLaMA have demonstrated
remarkable accuracy in a wide range of tasks. However, training these models
can incur significant expenses, often requiring tens of thousands of GPUs for
months of continuous operation. Typically, this training is carried out in
specialized GPU clusters equipped with homogeneous high-speed Remote Direct
Memory Access (RDMA) network interface cards (NICs). The acquisition and
maintenance of such dedicated clusters is challenging. Current LLM training
frameworks, like Megatron-LM and Megatron-DeepSpeed, focus primarily on
optimizing training within homogeneous cluster settings. In this paper, we
introduce Holmes, a training framework for LLMs that employs thoughtfully
crafted data and model parallelism strategies over the heterogeneous NIC
environment. Our primary technical contribution lies in a novel scheduling
method that intelligently allocates distinct computational tasklets in LLM
training to specific groups of GPU devices based on the characteristics of
their connected NICs. Furthermore, our proposed framework, utilizing pipeline
parallel techniques, demonstrates scalability to multiple GPU clusters, even in
scenarios without high-speed interconnects between nodes in distinct clusters.
We conducted comprehensive experiments that involved various scenarios in the
heterogeneous NIC environment. In most cases, our framework achieves
performance levels close to those achievable with homogeneous RDMA-capable
networks (InfiniBand or RoCE), significantly exceeding training efficiency
within the pure Ethernet environment. Additionally, we verified that our
framework outperforms other mainstream LLM frameworks under heterogeneous NIC
environment in terms of training efficiency and can be seamlessly integrated
with them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Fei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1&quot;&gt;Shuang Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_N/0/1/0/all/0/1&quot;&gt;Ning Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fangyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1&quot;&gt;Ke Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Fu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1&quot;&gt;Jiezhong Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_A/0/1/0/all/0/1&quot;&gt;Aimin Pan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03567">
<title>XAIQA: Explainer-Based Data Augmentation for Extractive Question Answering. (arXiv:2312.03567v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.03567</link>
<description rdf:parseType="Literal">&lt;p&gt;Extractive question answering (QA) systems can enable physicians and
researchers to query medical records, a foundational capability for designing
clinical studies and understanding patient medical history. However, building
these systems typically requires expert-annotated QA pairs. Large language
models (LLMs), which can perform extractive QA, depend on high quality data in
their prompts, specialized for the application domain. We introduce a novel
approach, XAIQA, for generating synthetic QA pairs at scale from data naturally
available in electronic health records. Our method uses the idea of a
classification model explainer to generate questions and answers about medical
concepts corresponding to medical codes. In an expert evaluation with two
physicians, our method identifies $2.2\times$ more semantic matches and
$3.8\times$ more clinical abbreviations than two popular approaches that use
sentence transformers to create QA pairs. In an ML evaluation, adding our QA
pairs improves performance of GPT-4 as an extractive QA model, including on
difficult questions. In both the expert and ML evaluations, we examine
trade-offs between our method and sentence transformers for QA pair generation
depending on question difficulty.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stremmel_J/0/1/0/all/0/1&quot;&gt;Joel Stremmel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saeedi_A/0/1/0/all/0/1&quot;&gt;Ardavan Saeedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassanzadeh_H/0/1/0/all/0/1&quot;&gt;Hamid Hassanzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Batra_S/0/1/0/all/0/1&quot;&gt;Sanjit Batra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hertzberg_J/0/1/0/all/0/1&quot;&gt;Jeffrey Hertzberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murillo_J/0/1/0/all/0/1&quot;&gt;Jaime Murillo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Halperin_E/0/1/0/all/0/1&quot;&gt;Eran Halperin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03577">
<title>Improving Bias Mitigation through Bias Experts in Natural Language Understanding. (arXiv:2312.03577v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.03577</link>
<description rdf:parseType="Literal">&lt;p&gt;Biases in the dataset often enable the model to achieve high performance on
in-distribution data, while poorly performing on out-of-distribution data. To
mitigate the detrimental effect of the bias on the networks, previous works
have proposed debiasing methods that down-weight the biased examples identified
by an auxiliary model, which is trained with explicit bias labels. However,
finding a type of bias in datasets is a costly process. Therefore, recent
studies have attempted to make the auxiliary model biased without the guidance
(or annotation) of bias labels, by constraining the model&apos;s training
environment or the capability of the model itself. Despite the promising
debiasing results of recent works, the multi-class learning objective, which
has been naively used to train the auxiliary model, may harm the bias
mitigation effect due to its regularization effect and competitive nature
across classes. As an alternative, we propose a new debiasing framework that
introduces binary classifiers between the auxiliary model and the main model,
coined bias experts. Specifically, each bias expert is trained on a binary
classification task derived from the multi-class classification task via the
One-vs-Rest approach. Experimental results demonstrate that our proposed
strategy improves the bias identification ability of the auxiliary model.
Consequently, our debiased model consistently outperforms the state-of-the-art
on various challenge datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeon_E/0/1/0/all/0/1&quot;&gt;Eojin Jeon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1&quot;&gt;Mingyu Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Juhyeong Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yeachan Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mok_W/0/1/0/all/0/1&quot;&gt;Wing-Lam Mok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;SangKeun Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03633">
<title>Not All Large Language Models (LLMs) Succumb to the &quot;Reversal Curse&quot;: A Comparative Study of Deductive Logical Reasoning in BERT and GPT Models. (arXiv:2312.03633v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.03633</link>
<description rdf:parseType="Literal">&lt;p&gt;The &quot;Reversal Curse&quot; refers to the scenario where auto-regressive decoder
large language models (LLMs), such as ChatGPT, trained on &quot;A is B&quot; fail to
learn &quot;B is A&quot;, demonstrating a basic failure of logical deduction. This raises
a red flag in the use of GPT models for certain general tasks such as
constructing knowledge graphs, considering their adherence to this symmetric
principle. In our study, we examined a bidirectional LLM, BERT, and found that
it is immune to the reversal curse. Driven by ongoing efforts to construct
biomedical knowledge graphs with LLMs, we also embarked on evaluating more
complex but essential deductive reasoning capabilities. This process included
first training encoder and decoder language models to master the intersection
($\cap$) and union ($\cup$) operations on two sets and then moving on to assess
their capability to infer different combinations of union ($\cup$) and
intersection ($\cap$) operations on three newly created sets. The findings
showed that while both encoder and decoder language models, trained for tasks
involving two sets (union/intersection), were proficient in such scenarios,
they encountered difficulties when dealing with operations that included three
sets (various combinations of union and intersection). Our research highlights
the distinct characteristics of encoder and decoder models in simple and
complex logical reasoning. In practice, the choice between BERT and GPT should
be guided by the specific requirements and nature of the task at hand,
leveraging their respective strengths in bidirectional context comprehension
and sequence prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jingye Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Da Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kai Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03656">
<title>Interpretability Illusions in the Generalization of Simplified Models. (arXiv:2312.03656v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.03656</link>
<description rdf:parseType="Literal">&lt;p&gt;A common method to study deep learning systems is to use simplified model
representations -- for example, using singular value decomposition to visualize
the model&apos;s hidden states in a lower dimensional space. This approach assumes
that the results of these simplified are faithful to the original model. Here,
we illustrate an important caveat to this assumption: even if the simplified
representations can accurately approximate the full model on the training set,
they may fail to accurately capture the model&apos;s behavior out of distribution --
the understanding developed from simplified representations may be an illusion.
We illustrate this by training Transformer models on controlled datasets with
systematic generalization splits. First, we train models on the Dyck
balanced-parenthesis languages. We simplify these models using tools like
dimensionality reduction and clustering, and then explicitly test how these
simplified proxies match the behavior of the original model on various
out-of-distribution test sets. We find that the simplified proxies are
generally less faithful out of distribution. In cases where the original model
generalizes to novel structures or deeper depths, the simplified versions may
fail, or generalize better. This finding holds even if the simplified
representations do not directly depend on the training distribution. Next, we
study a more naturalistic task: predicting the next character in a dataset of
computer code. We find similar generalization gaps between the original model
and simplified proxies, and conduct further analysis to investigate which
aspects of the code completion task are associated with the largest gaps.
Together, our results raise questions about the extent to which mechanistic
interpretations derived using tools like SVD can reliably predict what a model
will do in novel situations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Friedman_D/0/1/0/all/0/1&quot;&gt;Dan Friedman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lampinen_A/0/1/0/all/0/1&quot;&gt;Andrew Lampinen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dixon_L/0/1/0/all/0/1&quot;&gt;Lucas Dixon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Danqi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghandeharioun_A/0/1/0/all/0/1&quot;&gt;Asma Ghandeharioun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03664">
<title>Generative agent-based modeling with actions grounded in physical, social, or digital space using Concordia. (arXiv:2312.03664v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.03664</link>
<description rdf:parseType="Literal">&lt;p&gt;Agent-based modeling has been around for decades, and applied widely across
the social and natural sciences. The scope of this research method is now
poised to grow dramatically as it absorbs the new affordances provided by Large
Language Models (LLM)s. Generative Agent-Based Models (GABM) are not just
classic Agent-Based Models (ABM)s where the agents talk to one another. Rather,
GABMs are constructed using an LLM to apply common sense to situations, act
&quot;reasonably&quot;, recall common semantic knowledge, produce API calls to control
digital technologies like apps, and communicate both within the simulation and
to researchers viewing it from the outside. Here we present Concordia, a
library to facilitate constructing and working with GABMs. Concordia makes it
easy to construct language-mediated simulations of physically- or
digitally-grounded environments. Concordia agents produce their behavior using
a flexible component system which mediates between two fundamental operations:
LLM calls and associative memory retrieval. A special agent called the Game
Master (GM), which was inspired by tabletop role-playing games, is responsible
for simulating the environment where the agents interact. Agents take actions
by describing what they want to do in natural language. The GM then translates
their actions into appropriate implementations. In a simulated physical world,
the GM checks the physical plausibility of agent actions and describes their
effects. In digital environments simulating technologies such as apps and
services, the GM may handle API calls to integrate with external tools such as
general AI assistants (e.g., Bard, ChatGPT), and digital apps (e.g., Calendar,
Email, Search, etc.). Concordia was designed to support a wide array of
applications both in scientific research and for evaluating performance of real
digital services by simulating users and/or generating synthetic data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vezhnevets_A/0/1/0/all/0/1&quot;&gt;Alexander Sasha Vezhnevets&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agapiou_J/0/1/0/all/0/1&quot;&gt;John P. Agapiou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aharon_A/0/1/0/all/0/1&quot;&gt;Avia Aharon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziv_R/0/1/0/all/0/1&quot;&gt;Ron Ziv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matyas_J/0/1/0/all/0/1&quot;&gt;Jayd Matyas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duenez_Guzman_E/0/1/0/all/0/1&quot;&gt;Edgar A. Du&amp;#xe9;&amp;#xf1;ez-Guzm&amp;#xe1;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cunningham_W/0/1/0/all/0/1&quot;&gt;William A. Cunningham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osindero_S/0/1/0/all/0/1&quot;&gt;Simon Osindero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karmon_D/0/1/0/all/0/1&quot;&gt;Danny Karmon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leibo_J/0/1/0/all/0/1&quot;&gt;Joel Z. Leibo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03668">
<title>An Integration of Pre-Trained Speech and Language Models for End-to-End Speech Recognition. (arXiv:2312.03668v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2312.03668</link>
<description rdf:parseType="Literal">&lt;p&gt;Advances in machine learning have made it possible to perform various text
and speech processing tasks, including automatic speech recognition (ASR), in
an end-to-end (E2E) manner. Since typical E2E approaches require large amounts
of training data and resources, leveraging pre-trained foundation models
instead of training from scratch is gaining attention. Although there have been
attempts to use pre-trained speech and language models in ASR, most of them are
limited to using either. This paper explores the potential of integrating a
pre-trained speech representation model with a large language model (LLM) for
E2E ASR. The proposed model enables E2E ASR by generating text tokens in an
autoregressive manner via speech representations as speech prompts, taking
advantage of the vast knowledge provided by the LLM. Furthermore, the proposed
model can incorporate remarkable developments for LLM utilization, such as
inference optimization and parameter-efficient domain adaptation. Experimental
results show that the proposed model achieves performance comparable to modern
E2E ASR models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hono_Y/0/1/0/all/0/1&quot;&gt;Yukiya Hono&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mitsuda_K/0/1/0/all/0/1&quot;&gt;Koh Mitsuda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tianyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mitsui_K/0/1/0/all/0/1&quot;&gt;Kentaro Mitsui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wakatsuki_T/0/1/0/all/0/1&quot;&gt;Toshiaki Wakatsuki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sawada_K/0/1/0/all/0/1&quot;&gt;Kei Sawada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03689">
<title>Evaluating and Mitigating Discrimination in Language Model Decisions. (arXiv:2312.03689v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.03689</link>
<description rdf:parseType="Literal">&lt;p&gt;As language models (LMs) advance, interest is growing in applying them to
high-stakes societal decisions, such as determining financing or housing
eligibility. However, their potential for discrimination in such contexts
raises ethical concerns, motivating the need for better methods to evaluate
these risks. We present a method for proactively evaluating the potential
discriminatory impact of LMs in a wide range of use cases, including
hypothetical use cases where they have not yet been deployed. Specifically, we
use an LM to generate a wide array of potential prompts that decision-makers
may input into an LM, spanning 70 diverse decision scenarios across society,
and systematically vary the demographic information in each prompt. Applying
this methodology reveals patterns of both positive and negative discrimination
in the Claude 2.0 model in select settings when no interventions are applied.
While we do not endorse or permit the use of language models to make automated
decisions for the high-risk use cases we study, we demonstrate techniques to
significantly decrease both positive and negative discrimination through
careful prompt engineering, providing pathways toward safer deployment in use
cases where they may be appropriate. Our work enables developers and
policymakers to anticipate, measure, and address discrimination as language
model capabilities and applications continue to expand. We release our dataset
and prompts at https://huggingface.co/datasets/Anthropic/discrim-eval
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tamkin_A/0/1/0/all/0/1&quot;&gt;Alex Tamkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Askell_A/0/1/0/all/0/1&quot;&gt;Amanda Askell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lovitt_L/0/1/0/all/0/1&quot;&gt;Liane Lovitt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durmus_E/0/1/0/all/0/1&quot;&gt;Esin Durmus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joseph_N/0/1/0/all/0/1&quot;&gt;Nicholas Joseph&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kravec_S/0/1/0/all/0/1&quot;&gt;Shauna Kravec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1&quot;&gt;Karina Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaplan_J/0/1/0/all/0/1&quot;&gt;Jared Kaplan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganguli_D/0/1/0/all/0/1&quot;&gt;Deep Ganguli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03699">
<title>PROMISE: A Framework for Model-Driven Stateful Prompt Orchestration. (arXiv:2312.03699v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.03699</link>
<description rdf:parseType="Literal">&lt;p&gt;The advent of increasingly powerful language models has raised expectations
for language-based interactions. However, controlling these models is a
challenge, emphasizing the need to be able to investigate the feasibility and
value of their application. We present PROMISE, a framework that facilitates
the development of complex language-based interactions with information
systems. Its use of state machine modeling concepts enables model-driven,
dynamic prompt orchestration across hierarchically nested states and
transitions. This improves the control of the behavior of language models and
thus enables their effective and efficient use. We show the benefits of PROMISE
in the context of application scenarios within health information systems and
demonstrate its ability to handle complex interactions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wenyuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heierli_J/0/1/0/all/0/1&quot;&gt;Jasmin Heierli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meisterhans_M/0/1/0/all/0/1&quot;&gt;Max Meisterhans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moser_A/0/1/0/all/0/1&quot;&gt;Adrian Moser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farber_A/0/1/0/all/0/1&quot;&gt;Andri F&amp;#xe4;rber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dolata_M/0/1/0/all/0/1&quot;&gt;Mateusz Dolata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gavagnin_E/0/1/0/all/0/1&quot;&gt;Elena Gavagnin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spindler_A/0/1/0/all/0/1&quot;&gt;Alexandre de Spindler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwabe_G/0/1/0/all/0/1&quot;&gt;Gerhard Schwabe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03700">
<title>OneLLM: One Framework to Align All Modalities with Language. (arXiv:2312.03700v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.03700</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal large language models (MLLMs) have gained significant attention
due to their strong multimodal understanding capability. However, existing
works rely heavily on modality-specific encoders, which usually differ in
architecture and are limited to common modalities. In this paper, we present
OneLLM, an MLLM that aligns eight modalities to language using a unified
framework. We achieve this through a unified multimodal encoder and a
progressive multimodal alignment pipeline. In detail, we first train an image
projection module to connect a vision encoder with LLM. Then, we build a
universal projection module (UPM) by mixing multiple image projection modules
and dynamic routing. Finally, we progressively align more modalities to LLM
with the UPM. To fully leverage the potential of OneLLM in following
instructions, we also curated a comprehensive multimodal instruction dataset,
including 2M items from image, audio, video, point cloud, depth/normal map, IMU
and fMRI brain activity. OneLLM is evaluated on 25 diverse benchmarks,
encompassing tasks such as multimodal captioning, question answering and
reasoning, where it delivers excellent performance. Code, data, model and
online demo are available at https://github.com/csuhan/OneLLM
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jiaming Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_K/0/1/0/all/0/1&quot;&gt;Kaixiong Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yiyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiaqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kaipeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dahua Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1&quot;&gt;Peng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1&quot;&gt;Xiangyu Yue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.08225">
<title>All the World&apos;s a (Hyper)Graph: A Data Drama. (arXiv:2206.08225v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.08225</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Hyperbard, a dataset of diverse relational data representations
derived from Shakespeare&apos;s plays. Our representations range from simple graphs
capturing character co-occurrence in single scenes to hypergraphs encoding
complex communication settings and character contributions as hyperedges with
edge-specific node weights. By making multiple intuitive representations
readily available for experimentation, we facilitate rigorous representation
robustness checks in graph learning, graph mining, and network analysis,
highlighting the advantages and drawbacks of specific representations.
Leveraging the data released in Hyperbard, we demonstrate that many solutions
to popular graph mining problems are highly dependent on the representation
choice, thus calling current graph curation practices into question. As an
homage to our data source, and asserting that science can also be art, we
present all our points in the form of a play.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coupette_C/0/1/0/all/0/1&quot;&gt;Corinna Coupette&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vreeken_J/0/1/0/all/0/1&quot;&gt;Jilles Vreeken&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rieck_B/0/1/0/all/0/1&quot;&gt;Bastian Rieck&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.10706">
<title>TraSE: Towards Tackling Authorial Style from a Cognitive Science Perspective. (arXiv:2206.10706v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2206.10706</link>
<description rdf:parseType="Literal">&lt;p&gt;Stylistic analysis of text is a key task in research areas ranging from
authorship attribution to forensic analysis and personality profiling. The
existing approaches for stylistic analysis are plagued by issues like topic
influence, lack of discriminability for large number of authors and the
requirement for large amounts of diverse data. In this paper, the source of
these issues are identified along with the necessity for a cognitive
perspective on authorial style in addressing them. A novel feature
representation, called Trajectory-based Style Estimation (TraSE), is introduced
to support this purpose. Authorship attribution experiments with over 27,000
authors and 1.4 million samples in a cross-domain scenario resulted in 90%
attribution accuracy suggesting that the feature representation is immune to
such negative influences and an excellent candidate for stylistic analysis.
Finally, a qualitative analysis is performed on TraSE using physical human
characteristics, like age, to validate its claim on capturing cognitive traits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_R/0/1/0/all/0/1&quot;&gt;Ronald Wilson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhandarkar_A/0/1/0/all/0/1&quot;&gt;Avanti Bhandarkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woodard_D/0/1/0/all/0/1&quot;&gt;Damon Woodard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.02160">
<title>A Comprehensive Review of Visual-Textual Sentiment Analysis from Social Media Networks. (arXiv:2207.02160v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2207.02160</link>
<description rdf:parseType="Literal">&lt;p&gt;Social media networks have become a significant aspect of people&apos;s lives,
serving as a platform for their ideas, opinions and emotions. Consequently,
automated sentiment analysis (SA) is critical for recognising people&apos;s feelings
in ways that other information sources cannot. The analysis of these feelings
revealed various applications, including brand evaluations, YouTube film
reviews and healthcare applications. As social media continues to develop,
people post a massive amount of information in different forms, including text,
photos, audio and video. Thus, traditional SA algorithms have become limited,
as they do not consider the expressiveness of other modalities. By including
such characteristics from various material sources, these multimodal data
streams provide new opportunities for optimising the expected results beyond
text-based SA. Our study focuses on the forefront field of multimodal SA, which
examines visual and textual data posted on social media networks. Many people
are more likely to utilise this information to express themselves on these
platforms. To serve as a resource for academics in this rapidly growing field,
we introduce a comprehensive overview of textual and visual SA, including data
pre-processing, feature extraction techniques, sentiment benchmark datasets,
and the efficacy of multiple classification methodologies suited to each field.
We also provide a brief introduction of the most frequently utilised data
fusion strategies and a summary of existing research on visual-textual SA.
Finally, we highlight the most significant challenges and investigate several
important sentiment applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Tameemi_I/0/1/0/all/0/1&quot;&gt;Israa Khalaf Salman Al-Tameemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feizi_Derakhshi_M/0/1/0/all/0/1&quot;&gt;Mohammad-Reza Feizi-Derakhshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pashazadeh_S/0/1/0/all/0/1&quot;&gt;Saeed Pashazadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asadpour_M/0/1/0/all/0/1&quot;&gt;Mohammad Asadpour&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.12407">
<title>Entailment Semantics Can Be Extracted from an Ideal Language Model. (arXiv:2209.12407v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2209.12407</link>
<description rdf:parseType="Literal">&lt;p&gt;Language models are often trained on text alone, without additional
grounding. There is debate as to how much of natural language semantics can be
inferred from such a procedure. We prove that entailment judgments between
sentences can be extracted from an ideal language model that has perfectly
learned its target distribution, assuming the training sentences are generated
by Gricean agents, i.e., agents who follow fundamental principles of
communication from the linguistic theory of pragmatics. We also show entailment
judgments can be decoded from the predictions of a language model trained on
such Gricean data. Our results reveal a pathway for understanding the semantic
information encoded in unlabeled linguistic data and a potential framework for
extracting semantics from language models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1&quot;&gt;William Merrill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Warstadt_A/0/1/0/all/0/1&quot;&gt;Alex Warstadt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1&quot;&gt;Tal Linzen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.09710">
<title>Continual Learning for Instruction Following from Realtime Feedback. (arXiv:2212.09710v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2212.09710</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose and deploy an approach to continually train an
instruction-following agent from feedback provided by users during
collaborative interactions. During interaction, human users instruct an agent
using natural language, and provide realtime binary feedback as they observe
the agent following their instructions. We design a contextual bandit learning
approach, converting user feedback to immediate reward. We evaluate through
thousands of human-agent interactions, demonstrating 15.4% absolute improvement
in instruction execution accuracy over time. We also show our approach is
robust to several design variations, and that the feedback signal is roughly
equivalent to the learning signal of supervised demonstration data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suhr_A/0/1/0/all/0/1&quot;&gt;Alane Suhr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1&quot;&gt;Yoav Artzi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.07371">
<title>BiasTestGPT: Using ChatGPT for Social Bias Testing of Language Models. (arXiv:2302.07371v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2302.07371</link>
<description rdf:parseType="Literal">&lt;p&gt;Pretrained Language Models (PLMs) harbor inherent social biases that can
result in harmful real-world implications. Such social biases are measured
through the probability values that PLMs output for different social groups and
attributes appearing in a set of test sentences. However, bias testing is
currently cumbersome since the test sentences are generated either from a
limited set of manual templates or need expensive crowd-sourcing. We instead
propose using ChatGPT for the controllable generation of test sentences, given
any arbitrary user-specified combination of social groups and attributes
appearing in the test sentences. When compared to template-based methods, our
approach using ChatGPT for test sentence generation is superior in detecting
social bias, especially in challenging settings such as intersectional biases.
We present an open-source comprehensive bias testing framework (BiasTestGPT),
hosted on HuggingFace, that can be plugged into any open-source PLM for bias
testing. User testing with domain experts from various fields has shown their
interest in being able to test modern AI for social biases. Our tool has
significantly improved their awareness of such biases in PLMs, proving to be
learnable and user-friendly. We thus enable seamless open-ended social bias
testing of PLMs by domain experts through an automatic large-scale generation
of diverse test sentences for any combination of social categories and
attributes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kocielnik_R/0/1/0/all/0/1&quot;&gt;Rafal Kocielnik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabhumoye_S/0/1/0/all/0/1&quot;&gt;Shrimai Prabhumoye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_V/0/1/0/all/0/1&quot;&gt;Vivian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1&quot;&gt;Roy Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alvarez_R/0/1/0/all/0/1&quot;&gt;R. Michael Alvarez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1&quot;&gt;Anima Anandkumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.07320">
<title>Model-tuning Via Prompts Makes NLP Models Adversarially Robust. (arXiv:2303.07320v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2303.07320</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, NLP practitioners have converged on the following practice:
(i) import an off-the-shelf pretrained (masked) language model; (ii) append a
multilayer perceptron atop the CLS token&apos;s hidden representation (with randomly
initialized weights); and (iii) fine-tune the entire model on a downstream task
(MLP-FT). This procedure has produced massive gains on standard NLP benchmarks,
but these models remain brittle, even to mild adversarial perturbations. In
this work, we demonstrate surprising gains in adversarial robustness enjoyed by
Model-tuning Via Prompts (MVP), an alternative method of adapting to downstream
tasks. Rather than appending an MLP head to make output prediction, MVP appends
a prompt template to the input, and makes prediction via text
infilling/completion. Across 5 NLP datasets, 4 adversarial attacks, and 3
different models, MVP improves performance against adversarial substitutions by
an average of 8% over standard methods and even outperforms adversarial
training-based state-of-art defenses by 3.5%. By combining MVP with adversarial
training, we achieve further improvements in adversarial robustness while
maintaining performance on unperturbed examples. Finally, we conduct ablations
to investigate the mechanism underlying these gains. Notably, we find that the
main causes of vulnerability of MLP-FT can be attributed to the misalignment
between pre-training and fine-tuning tasks, and the randomly initialized MLP
parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raman_M/0/1/0/all/0/1&quot;&gt;Mrigank Raman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maini_P/0/1/0/all/0/1&quot;&gt;Pratyush Maini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1&quot;&gt;J. Zico Kolter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1&quot;&gt;Zachary C. Lipton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pruthi_D/0/1/0/all/0/1&quot;&gt;Danish Pruthi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.09128">
<title>Exploring Distributional Shifts in Large Language Models for Code Analysis. (arXiv:2303.09128v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2303.09128</link>
<description rdf:parseType="Literal">&lt;p&gt;We systematically study how three large language models with code
capabilities - CodeT5, Codex, and ChatGPT - generalize to out-of-domain data.
We consider two fundamental applications - code summarization, and code
generation. We split data into domains following its natural boundaries - by an
organization, by a project, and by a module within the software project. We
establish that samples from each new domain present all the models with a
significant challenge of distribution shift. We study how established methods
adapt models to better generalize to new domains. Our experiments show that
while multitask learning alone is a reasonable baseline, combining it with
few-shot finetuning on examples retrieved from training data can achieve very
strong performance. Moreover, this solution can outperform direct finetuning
for very low-data scenarios. Finally, we consider variations of this approach
to create a more broadly applicable method to adapt to multiple domains at
once. We find that for code generation, a model adapted to multiple domains
simultaneously performs on par with those adapted to a single domain
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arakelyan_S/0/1/0/all/0/1&quot;&gt;Shushan Arakelyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1&quot;&gt;Rocktim Jyoti Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1&quot;&gt;Yi Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xiang Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05403">
<title>Completeness, Recall, and Negation in Open-World Knowledge Bases: A Survey. (arXiv:2305.05403v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05403</link>
<description rdf:parseType="Literal">&lt;p&gt;General-purpose knowledge bases (KBs) are a cornerstone of knowledge-centric
AI. Many of them are constructed pragmatically from Web sources, and are thus
far from complete. This poses challenges for the consumption as well as the
curation of their content. While several surveys target the problem of
completing incomplete KBs, the first problem is arguably to know whether and
where the KB is incomplete in the first place, and to which degree.
&lt;/p&gt;
&lt;p&gt;In this survey we discuss how knowledge about completeness, recall, and
negation in KBs can be expressed, extracted, and inferred. We cover (i) the
logical foundations of knowledge representation and querying under partial
closed-world semantics; (ii) the estimation of this information via statistical
patterns; (iii) the extraction of information about recall from KBs and text;
(iv) the identification of interesting negative statements; and (v) relaxed
notions of relative recall.
&lt;/p&gt;
&lt;p&gt;This survey is targeted at two types of audiences: (1) practitioners who are
interested in tracking KB quality, focusing extraction efforts, and building
quality-aware downstream applications; and (2) data management, knowledge base
and semantic web researchers who wish to understand the state of the art of
knowledge bases beyond the open-world assumption. Consequently, our survey
presents both fundamental methodologies and their working, and gives
practice-oriented recommendations on how to choose between different approaches
for a problem at hand.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Razniewski_S/0/1/0/all/0/1&quot;&gt;Simon Razniewski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arnaout_H/0/1/0/all/0/1&quot;&gt;Hiba Arnaout&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Shrestha Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suchanek_F/0/1/0/all/0/1&quot;&gt;Fabian Suchanek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.11746">
<title>HalOmi: A Manually Annotated Benchmark for Multilingual Hallucination and Omission Detection in Machine Translation. (arXiv:2305.11746v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.11746</link>
<description rdf:parseType="Literal">&lt;p&gt;Hallucinations in machine translation are translations that contain
information completely unrelated to the input. Omissions are translations that
do not include some of the input information. While both cases tend to be
catastrophic errors undermining user trust, annotated data with these types of
pathologies is extremely scarce and is limited to a few high-resource
languages. In this work, we release an annotated dataset for the hallucination
and omission phenomena covering 18 translation directions with varying resource
levels and scripts. Our annotation covers different levels of partial and full
hallucinations as well as omissions both at the sentence and at the word level.
Additionally, we revisit previous methods for hallucination and omission
detection, show that conclusions made based on a single language pair largely
do not hold for a large-scale evaluation, and establish new solid baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dale_D/0/1/0/all/0/1&quot;&gt;David Dale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Voita_E/0/1/0/all/0/1&quot;&gt;Elena Voita&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_J/0/1/0/all/0/1&quot;&gt;Janice Lam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hansanti_P/0/1/0/all/0/1&quot;&gt;Prangthip Hansanti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ropers_C/0/1/0/all/0/1&quot;&gt;Christophe Ropers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalbassi_E/0/1/0/all/0/1&quot;&gt;Elahe Kalbassi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Cynthia Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barrault_L/0/1/0/all/0/1&quot;&gt;Lo&amp;#xef;c Barrault&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1&quot;&gt;Marta R. Costa-juss&amp;#xe0;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12524">
<title>TheoremQA: A Theorem-driven Question Answering dataset. (arXiv:2305.12524v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12524</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent LLMs like GPT-4 and PaLM-2 have made tremendous progress in
solving fundamental math problems like GSM8K by achieving over 90% accuracy.
However, their capabilities to solve more challenging math problems which
require domain-specific knowledge (i.e. theorem) have yet to be investigated.
In this paper, we introduce TheoremQA, the first theorem-driven
question-answering dataset designed to evaluate AI models&apos; capabilities to
apply theorems to solve challenging science problems. TheoremQA is curated by
domain experts containing 800 high-quality questions covering 350 theorems
(e.g. Taylor&apos;s theorem, Lagrange&apos;s theorem, Huffman coding, Quantum Theorem,
Elasticity Theorem, etc) from Math, Physics, EE&amp;amp;CS, and Finance. We evaluate a
wide spectrum of 16 large language and code models with different prompting
strategies like Chain-of-Thoughts and Program-of-Thoughts. We found that
GPT-4&apos;s capabilities to solve these problems are unparalleled, achieving an
accuracy of 51% with Program-of-Thoughts Prompting. All the existing
open-sourced models are below 15%, barely surpassing the random-guess baseline.
Given the diversity and broad coverage of TheoremQA, we believe it can be used
as a better benchmark to evaluate LLMs&apos; capabilities to solve challenging
science problems. The data and code are released in
https://github.com/wenhuchen/TheoremQA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenhu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1&quot;&gt;Ming Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ku_M/0/1/0/all/0/1&quot;&gt;Max Ku&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1&quot;&gt;Pan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1&quot;&gt;Yixin Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xueguang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jianyu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1&quot;&gt;Tony Xia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13388">
<title>The neural dynamics of auditory word recognition and integration. (arXiv:2305.13388v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13388</link>
<description rdf:parseType="Literal">&lt;p&gt;Listeners recognize and integrate words in rapid and noisy everyday speech by
combining expectations about upcoming content with incremental sensory
evidence. We present a computational model of word recognition which formalizes
this perceptual process in Bayesian decision theory. We fit this model to
explain scalp EEG signals recorded as subjects passively listened to a
fictional story, revealing both the dynamics of the online auditory word
recognition process and the neural correlates of the recognition and
integration of words.
&lt;/p&gt;
&lt;p&gt;The model reveals distinct neural processing of words depending on whether or
not they can be quickly recognized. While all words trigger a neural response
characteristic of probabilistic integration -- voltage modulations predicted by
a word&apos;s surprisal in context -- these modulations are amplified for words
which require more than roughly 150 ms of input to be recognized. We observe no
difference in the latency of these neural responses according to words&apos;
recognition times. Our results are consistent with a two-part model of speech
comprehension, combining an eager and rapid process of word recognition with a
temporally independent process of word integration. However, we also developed
alternative models of the scalp EEG signal not incorporating word recognition
dynamics which showed similar performance improvements. We discuss potential
future modeling steps which may help to separate these hypotheses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gauthier_J/0/1/0/all/0/1&quot;&gt;Jon Gauthier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1&quot;&gt;Roger Levy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13406">
<title>DADA: Dialect Adaptation via Dynamic Aggregation of Linguistic Rules. (arXiv:2305.13406v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13406</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing large language models (LLMs) that mainly focus on Standard American
English (SAE) often lead to significantly worse performance when being applied
to other English dialects. While existing mitigations tackle discrepancies for
individual target dialects, they assume access to high-accuracy dialect
identification systems. The boundaries between dialects are inherently
flexible, making it difficult to categorize language into discrete predefined
categories. In this paper, we propose DADA (Dialect Adaptation via Dynamic
Aggregation), a modular approach to imbue SAE-trained models with
multi-dialectal robustness by composing adapters which handle specific
linguistic features. The compositional architecture of DADA allows for both
targeted adaptation to specific dialect variants and simultaneous adaptation to
various dialects. We show that DADA is effective for both single task and
instruction finetuned language models, offering an extensible and interpretable
framework for adapting existing LLMs to different English dialects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yanchen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Held_W/0/1/0/all/0/1&quot;&gt;William Held&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Diyi Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13675">
<title>Polyglot or Not? Measuring Multilingual Encyclopedic Knowledge in Foundation Models. (arXiv:2305.13675v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13675</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we assess the ability of foundation models to recall
encyclopedic knowledge across a wide range of linguistic contexts. To support
this, we: 1) produce a 20-language dataset that contains 303k factual
associations paired with counterfactuals, 2) evaluate 5 models in a
multilingual test, and 3) benchmark a diverse set of 24 models in an
English-only test. Meta&apos;s LLaMA achieves the highest scores in both
multilingual and English-only evaluations. Yet, an analysis of LLaMA&apos;s errors
reveals significant limitations in its ability to recall facts in languages
other than English, plus difficulties related to the location and gender of
fact subjects. Overall, our findings suggest that today&apos;s foundation models are
far from polyglots.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schott_T/0/1/0/all/0/1&quot;&gt;Tim Schott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furman_D/0/1/0/all/0/1&quot;&gt;Daniel Furman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhat_S/0/1/0/all/0/1&quot;&gt;Shreshta Bhat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13683">
<title>Error Detection for Text-to-SQL Semantic Parsing. (arXiv:2305.13683v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13683</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite remarkable progress in text-to-SQL semantic parsing in recent years,
the performance of existing parsers is still far from perfect. Specifically,
modern text-to-SQL parsers based on deep learning are often over-confident,
thus casting doubt on their trustworthiness when deployed for real use. In this
paper, we propose a parser-independent error detection model for text-to-SQL
semantic parsing. Using a language model of code as its bedrock, we enhance our
error detection model with graph neural networks that learn structural features
of both natural language questions and SQL queries. We train our model on
realistic parsing errors collected from a cross-domain setting, which leads to
stronger generalization ability. Experiments with three strong text-to-SQL
parsers featuring different decoding mechanisms show that our approach
outperforms parser-dependent uncertainty metrics. Our model could also
effectively improve the performance and usability of text-to-SQL semantic
parsers regardless of their architectures. (Our implementation is available at
https://github.com/OSU-NLP-Group/Text2SQL-Error-Detection)
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shijie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Ziru Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Huan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yu Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14246">
<title>Modeling Empathic Similarity in Personal Narratives. (arXiv:2305.14246v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14246</link>
<description rdf:parseType="Literal">&lt;p&gt;The most meaningful connections between people are often fostered through
expression of shared vulnerability and emotional experiences in personal
narratives. We introduce a new task of identifying similarity in personal
stories based on empathic resonance, i.e., the extent to which two people
empathize with each others&apos; experiences, as opposed to raw semantic or lexical
similarity, as has predominantly been studied in NLP. Using insights from
social psychology, we craft a framework that operationalizes empathic
similarity in terms of three key features of stories: main events, emotional
trajectories, and overall morals or takeaways. We create EmpathicStories, a
dataset of 1,500 personal stories annotated with our empathic similarity
features, and 2,000 pairs of stories annotated with empathic similarity scores.
Using our dataset, we fine-tune a model to compute empathic similarity of story
pairs, and show that this outperforms semantic similarity models on automated
correlation and retrieval metrics. Through a user study with 150 participants,
we also assess the effect our model has on retrieving stories that users
empathize with, compared to naive semantic similarity-based retrieval, and find
that participants empathized significantly more with stories retrieved by our
model. Our work has strong implications for the use of empathy-aware models to
foster human connection and empathy between people.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1&quot;&gt;Jocelyn Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1&quot;&gt;Maarten Sap&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colon_Hernandez_P/0/1/0/all/0/1&quot;&gt;Pedro Colon-Hernandez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1&quot;&gt;Hae Won Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Breazeal_C/0/1/0/all/0/1&quot;&gt;Cynthia Breazeal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14387">
<title>AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback. (arXiv:2305.14387v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14387</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) such as ChatGPT have seen widespread adoption
due to their ability to follow user instructions well. Developing these LLMs
involves a complex yet poorly understood workflow requiring training with human
feedback. Replicating and understanding this instruction-following process
faces three major challenges: the high cost of data collection, the lack of
trustworthy evaluation, and the absence of reference method implementations. We
address these challenges with AlpacaFarm, a simulator that enables research and
development for learning from feedback at a low cost. First, we design LLM
prompts to simulate human feedback that are 45x cheaper than crowdworkers and
display high agreement with humans. Second, we propose an automatic evaluation
and validate it against human instructions obtained on real-world interactions.
Third, we contribute reference implementations for several methods (PPO, DPO,
best-of-n, expert iteration, and more) that learn from pairwise feedback.
Finally, as an end-to-end validation of AlpacaFarm, we train and evaluate
eleven models on 10k pairs of real human feedback and show that rankings of
models trained in AlpacaFarm match rankings of models trained on human data. As
a demonstration of the research possible in AlpacaFarm, we find that methods
that use a reward model can substantially improve over supervised fine-tuning
and that our reference PPO implementation leads to a +10% improvement in
win-rate against Davinci003. We release all components of AlpacaFarm at
https://github.com/tatsu-lab/alpaca_farm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubois_Y/0/1/0/all/0/1&quot;&gt;Yann Dubois&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuechen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taori_R/0/1/0/all/0/1&quot;&gt;Rohan Taori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gulrajani_I/0/1/0/all/0/1&quot;&gt;Ishaan Gulrajani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ba_J/0/1/0/all/0/1&quot;&gt;Jimmy Ba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guestrin_C/0/1/0/all/0/1&quot;&gt;Carlos Guestrin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Percy Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1&quot;&gt;Tatsunori B. Hashimoto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14815">
<title>Machine Reading Comprehension using Case-based Reasoning. (arXiv:2305.14815v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14815</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an accurate and interpretable method for answer extraction in
machine reading comprehension that is reminiscent of case-based reasoning (CBR)
from classical AI. Our method (CBR-MRC) builds upon the hypothesis that
contextualized answers to similar questions share semantic similarities with
each other. Given a test question, CBR-MRC first retrieves a set of similar
cases from a nonparametric memory and then predicts an answer by selecting the
span in the test context that is most similar to the contextualized
representations of answers in the retrieved cases. The semi-parametric nature
of our approach allows it to attribute a prediction to the specific set of
evidence cases, making it a desirable choice for building reliable and
debuggable QA systems. We show that CBR-MRC provides high accuracy comparable
with large reader models and outperforms baselines by 11.5 and 8.4 EM on
NaturalQuestions and NewsQA, respectively. Further, we demonstrate the ability
of CBR-MRC in identifying not just the correct answer tokens but also the span
with the most relevant supporting evidence. Lastly, we observe that contexts
for certain question types show higher lexical diversity than others and find
that CBR-MRC is robust to these variations while performance using
fully-parametric methods drops.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thai_D/0/1/0/all/0/1&quot;&gt;Dung Thai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_D/0/1/0/all/0/1&quot;&gt;Dhruv Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhary_M/0/1/0/all/0/1&quot;&gt;Mudit Chaudhary&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wenlong Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1&quot;&gt;Rajarshi Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1&quot;&gt;Manzil Zaheer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jay-Yoon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1&quot;&gt;Hannaneh Hajishirzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1&quot;&gt;Andrew McCallum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15065">
<title>Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning. (arXiv:2305.15065v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15065</link>
<description rdf:parseType="Literal">&lt;p&gt;While extreme-scale language models have demonstrated exceptional performance
on a variety of language tasks, the degree of control over these language
models through pure prompting can often be limited. Directly fine-tuning such
language models can be effective for tailoring them, but it can be either
extremely costly (e.g., GPT-3) or not even feasible for the broader community
(e.g., GPT-4).
&lt;/p&gt;
&lt;p&gt;We propose Inference-time Policy Adapters (IPA), which efficiently tailors a
language model such as GPT-3 without fine-tuning it. IPA guides a large base
model during decoding time through a lightweight policy adapter trained to
optimize an arbitrary user objective with reinforcement learning.
&lt;/p&gt;
&lt;p&gt;On five challenging text generation tasks, such as toxicity reduction and
lexically constrained generation, IPA consistently brings significant
improvements over off-the-shelf language models. It outperforms competitive
baseline methods, sometimes even including expensive fine-tuning. In
particular, tailoring GPT-2 with IPA can outperform GPT-3, while tailoring
GPT-3 with IPA brings a major performance boost over GPT-3 (and sometimes even
over GPT-4). Our promising results highlight the potential of IPA as a
lightweight alternative to tailoring extreme-scale language models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Ximing Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1&quot;&gt;Faeze Brahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1&quot;&gt;Peter West&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1&quot;&gt;Jaehun Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandu_K/0/1/0/all/0/1&quot;&gt;Khyathi Chandu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravichander_A/0/1/0/all/0/1&quot;&gt;Abhilasha Ravichander&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1&quot;&gt;Lianhui Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ammanabrolu_P/0/1/0/all/0/1&quot;&gt;Prithviraj Ammanabrolu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1&quot;&gt;Liwei Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramnath_S/0/1/0/all/0/1&quot;&gt;Sahana Ramnath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dziri_N/0/1/0/all/0/1&quot;&gt;Nouha Dziri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fisher_J/0/1/0/all/0/1&quot;&gt;Jillian Fisher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Bill Yuchen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hallinan_S/0/1/0/all/0/1&quot;&gt;Skyler Hallinan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xiang Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Welleck_S/0/1/0/all/0/1&quot;&gt;Sean Welleck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1&quot;&gt;Yejin Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17390">
<title>SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks. (arXiv:2305.17390v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17390</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce SwiftSage, a novel agent framework inspired by the dual-process
theory of human cognition, designed to excel in action planning for complex
interactive reasoning tasks. SwiftSage integrates the strengths of behavior
cloning and prompting large language models (LLMs) to enhance task completion
performance. The framework comprises two primary modules: the Swift module,
representing fast and intuitive thinking, and the Sage module, emulating
deliberate thought processes. The Swift module is a small encoder-decoder LM
fine-tuned on the oracle agent&apos;s action trajectories, while the Sage module
employs LLMs such as GPT-4 for subgoal planning and grounding. We develop a
heuristic method to harmoniously integrate the two modules, resulting in a more
efficient and robust problem-solving process. In 30 tasks from the ScienceWorld
benchmark, SwiftSage significantly outperforms other methods such as SayCan,
ReAct, and Reflexion, demonstrating its effectiveness in solving complex
interactive tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Bill Yuchen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yicheng Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Karina Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1&quot;&gt;Faeze Brahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shiyu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1&quot;&gt;Chandra Bhagavatula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ammanabrolu_P/0/1/0/all/0/1&quot;&gt;Prithviraj Ammanabrolu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1&quot;&gt;Yejin Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xiang Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.20046">
<title>Assessing Language Disorders using Artificial Intelligence: a Paradigm Shift. (arXiv:2305.20046v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.20046</link>
<description rdf:parseType="Literal">&lt;p&gt;Speech, language, and communication deficits are present in most
neurodegenerative syndromes. They enable the early detection, diagnosis,
treatment planning, and monitoring of neurocognitive disease progression as
part of traditional neurological assessment. Nevertheless, standard speech and
language evaluation is time-consuming and resource-intensive for clinicians. We
argue that using machine learning methodologies, natural language processing,
and modern artificial intelligence (AI) for Language Assessment is an
improvement over conventional manual assessment. Using these methodologies,
Computational Language Assessment (CLA) accomplishes three goals: (i) provides
a neuro-cognitive evaluation of speech, language, and communication in elderly
and high-risk individuals for dementia; (ii) facilitates the diagnosis,
prognosis, and therapy efficacy in at-risk and language-impaired populations;
and (iii) allows easier extensibility to assess patients from a wide range of
languages. By employing AI models, CLA may inform neurocognitive theory on the
relationship between language symptoms and their neural bases. Finally, it
signals a paradigm shift by significantly advancing our ability to optimize the
prevention and treatment of elderly individuals with communication disorders,
allowing them to age gracefully with social engagement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Themistocleous_C/0/1/0/all/0/1&quot;&gt;Charalambos Themistocleous&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsapkini_K/0/1/0/all/0/1&quot;&gt;Kyrana Tsapkini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kokkinakis_D/0/1/0/all/0/1&quot;&gt;Dimitrios Kokkinakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09597">
<title>Clickbait Detection via Large Language Models. (arXiv:2306.09597v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09597</link>
<description rdf:parseType="Literal">&lt;p&gt;Clickbait, which aims to induce users with some surprising and even thrilling
headlines for increasing click-through rates, permeates almost all online
content publishers, such as news portals and social media. Recently, Large
Language Models (LLMs) have emerged as a powerful instrument and achieved
tremendous success in a series of NLP downstream tasks. However, it is not yet
known whether LLMs can be served as a high-quality clickbait detection system.
In this paper, we analyze the performance of LLMs in the few-shot and zero-shot
scenarios on several English and Chinese benchmark datasets. Experimental
results show that LLMs cannot achieve the best results compared to the
state-of-the-art deep and fine-tuning PLMs methods. Different from human
intuition, the experiments demonstrated that LLMs cannot make satisfied
clickbait detection just by the headlines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Han Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Ye Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yunhao Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiang_J/0/1/0/all/0/1&quot;&gt;Jipeng Qiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02697">
<title>Strahler Number of Natural Language Sentences in Comparison with Random Trees. (arXiv:2307.02697v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02697</link>
<description rdf:parseType="Literal">&lt;p&gt;The Strahler number was originally proposed to characterize the complexity of
river bifurcation and has found various applications. This article proposes
computation of the Strahler number&apos;s upper and lower limits for natural
language sentence tree structures. Through empirical measurements across
grammatically annotated data, the Strahler number of natural language sentences
is shown to be almost 3 or 4, similarly to the case of river bifurcation as
reported by Strahler (1957). From the theory behind the number, we show that it
is one kind of lower limit on the amount of memory required to process
sentences. We consider the Strahler number to provide reasoning that explains
reports showing that the number of required memory areas to process sentences
is 3 to 4 for parsing (Schuler et al., 2010), and reports indicating a
psychological &quot;magical number&quot; of 3 to 5 (Cowan, 2001). An analytical and
empirical analysis shows that the Strahler number is not constant but grows
logarithmically; therefore, the Strahler number of sentences derives from the
range of sentence lengths. Furthermore, the Strahler number is not different
for random trees, which could suggest that its origin is not specific to
natural language.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanaka_Ishii_K/0/1/0/all/0/1&quot;&gt;Kumiko Tanaka-Ishii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanaka_A/0/1/0/all/0/1&quot;&gt;Akira Tanaka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.10954">
<title>In-Context Learning for Text Classification with Many Labels. (arXiv:2309.10954v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.10954</link>
<description rdf:parseType="Literal">&lt;p&gt;In-context learning (ICL) using large language models for tasks with many
labels is challenging due to the limited context window, which makes it
difficult to fit a sufficient number of examples in the prompt. In this paper,
we use a pre-trained dense retrieval model to bypass this limitation, giving
the model only a partial view of the full label space for each inference call.
Testing with recent open-source LLMs (OPT, LLaMA), we set new state of the art
performance in few-shot settings for three common intent classification
datasets, with no finetuning. We also surpass fine-tuned performance on
fine-grained sentiment classification in certain cases. We analyze the
performance across number of in-context examples and different model scales,
showing that larger models are necessary to effectively and consistently make
use of larger context lengths for ICL. By running several ablations, we analyze
the model&apos;s use of: a) the similarity of the in-context examples to the current
input, b) the semantic content of the class names, and c) the correct
correspondence between examples and labels. We demonstrate that all three are
needed to varying degrees depending on the domain, contrary to certain recent
works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milios_A/0/1/0/all/0/1&quot;&gt;Aristides Milios&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1&quot;&gt;Siva Reddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bahdanau_D/0/1/0/all/0/1&quot;&gt;Dzmitry Bahdanau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00752">
<title>TIGERScore: Towards Building Explainable Metric for All Text Generation Tasks. (arXiv:2310.00752v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00752</link>
<description rdf:parseType="Literal">&lt;p&gt;We present TIGERScore, a \textbf{T}rained metric that follows
\textbf{I}nstruction \textbf{G}uidance to perform \textbf{E}xplainable, and
\textbf{R}eference-free evaluation over a wide spectrum of text generation
tasks. Different from other automatic evaluation methods that only provide
arcane scores, TIGERScore is guided by natural language instruction to provide
error analysis to pinpoint the mistakes in the generated text. Our metric is
based on LLaMA-2, trained on our meticulously curated instruction-tuning
dataset MetricInstruct which covers 6 text generation tasks and 23 text
generation datasets. The dataset consists of 42K quadruple in the form of
(instruction, input, system output $\rightarrow$ error analysis). We collected
the `system outputs&apos; through from a large variety of models to cover different
types of errors. To quantitatively assess our metric, we evaluate its
correlation with human ratings on 5 held-in datasets, 2 held-out datasets and
show that TIGERScore can achieve the open-source SoTA correlation with human
ratings across these datasets and almost approaches GPT-4 evaluator. As a
reference-free metric, its correlation can even surpass the best existing
reference-based metrics. To further qualitatively assess the rationale
generated by our metric, we conduct human evaluation on the generated
explanations and found that the explanations are 70.8\% accurate. Through these
experimental results, we believe TIGERScore demonstrates the possibility of
building universal explainable metrics to evaluate any text generation task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1&quot;&gt;Dongfu Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yishan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Ge Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wenhao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Bill Yuchen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenhu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05736">
<title>LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models. (arXiv:2310.05736v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05736</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have been applied in various applications due to
their astonishing capabilities. With advancements in technologies such as
chain-of-thought (CoT) prompting and in-context learning (ICL), the prompts fed
to LLMs are becoming increasingly lengthy, even exceeding tens of thousands of
tokens. To accelerate model inference and reduce cost, this paper presents
LLMLingua, a coarse-to-fine prompt compression method that involves a budget
controller to maintain semantic integrity under high compression ratios, a
token-level iterative compression algorithm to better model the interdependence
between compressed contents, and an instruction tuning based method for
distribution alignment between language models. We conduct experiments and
analysis over four datasets from different scenarios, i.e., GSM8K, BBH,
ShareGPT, and Arxiv-March23; showing that the proposed approach yields
state-of-the-art performance and allows for up to 20x compression with little
performance loss. Our code is available at https://aka.ms/LLMLingua.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Huiqiang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qianhui Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chin-Yew Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuqing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1&quot;&gt;Lili Qiu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08577">
<title>Visual Data-Type Understanding does not emerge from Scaling Vision-Language Models. (arXiv:2310.08577v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08577</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of Visual Data-Type Identification, a basic perceptual skill
with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual data-types, spanning four broad categories. An extensive zero-shot
evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a nuanced
performance landscape. While VLMs are reasonably good at identifying certain
stylistic \textit{data-types}, such as cartoons and sketches, they struggle
with simpler data-types arising from basic manipulations like image rotations
or additive noise. Our findings reveal that (i) model scaling alone yields
marginal gains for contrastively-trained models like CLIP, and (ii) there is a
pronounced drop in performance for the largest auto-regressively trained VLMs
like OpenFlamingo. This finding points to a blind spot in current frontier
VLMs: they excel in recognizing semantic content but fail to acquire an
understanding of visual data-types through scaling. By analyzing the
pre-training distributions of these models and incorporating data-type
information into the captions during fine-tuning, we achieve a significant
enhancement in performance. By exploring this previously uncharted task, we aim
to set the stage for further advancing VLMs to equip them with visual data-type
understanding. Code and datasets are released at
https://github.com/bethgelab/DataTypeIdentification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Udandarao_V/0/1/0/all/0/1&quot;&gt;Vishaal Udandarao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burg_M/0/1/0/all/0/1&quot;&gt;Max F. Burg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albanie_S/0/1/0/all/0/1&quot;&gt;Samuel Albanie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1&quot;&gt;Matthias Bethge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00694">
<title>Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving. (arXiv:2311.00694v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00694</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have achieved tremendous progress, yet they
still often struggle with challenging reasoning problems. Current approaches
address this challenge by sampling or searching detailed and low-level
reasoning chains. However, these methods are still limited in their exploration
capabilities, making it challenging for correct solutions to stand out in the
huge solution space. In this work, we unleash LLMs&apos; creative potential for
exploring multiple diverse problem solving strategies by framing an LLM as a
hierarchical policy via in-context learning. This policy comprises of a
visionary leader that proposes multiple diverse high-level problem-solving
tactics as hints, accompanied by a follower that executes detailed
problem-solving processes following each of the high-level instruction. The
follower uses each of the leader&apos;s directives as a guide and samples multiple
reasoning chains to tackle the problem, generating a solution group for each
leader proposal. Additionally, we propose an effective and efficient
tournament-based approach to select among these explored solution groups to
reach the final answer. Our approach produces meaningful and inspiring hints,
enhances problem-solving strategy exploration, and improves the final answer
accuracy on challenging problems in the MATH dataset. Code will be released at
https://github.com/lz1oceani/LLM-As-Hierarchical-Policy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1&quot;&gt;Zhan Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yunhao Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuanlin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1&quot;&gt;Tongzhou Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1&quot;&gt;Mingu Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pourreza_R/0/1/0/all/0/1&quot;&gt;Reza Pourreza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Memisevic_R/0/1/0/all/0/1&quot;&gt;Roland Memisevic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hao Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01907">
<title>BoschAI @ PLABA 2023: Leveraging Edit Operations in End-to-End Neural Sentence Simplification. (arXiv:2311.01907v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01907</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic simplification can help laypeople to comprehend complex scientific
text. Language models are frequently applied to this task by translating from
complex to simple language. In this paper, we describe our system based on
Llama 2, which ranked first in the PLABA shared task addressing the
simplification of biomedical text. We find that the large portion of shared
tokens between input and output leads to weak training signals and
conservatively editing models. To mitigate these issues, we propose
sentence-level and token-level loss weights. They give higher weight to
modified tokens, indicated by edit distance and edit operations, respectively.
We conduct an empirical evaluation on the PLABA dataset and find that both
approaches lead to simplifications closer to those created by human annotators
(+1.8% / +3.5% SARI), simpler language (-1 / -1.1 FKGL) and more edits (1.6x /
1.8x edit distance) compared to the same model fine-tuned with standard cross
entropy. We furthermore show that the hyperparameter $\lambda$ in token-level
loss weights can be used to control the edit distance and the simplicity level
(FKGL).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knappich_V/0/1/0/all/0/1&quot;&gt;Valentin Knappich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Razniewski_S/0/1/0/all/0/1&quot;&gt;Simon Razniewski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Friedrich_A/0/1/0/all/0/1&quot;&gt;Annemarie Friedrich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04064">
<title>KPI Extraction from Maintenance Work Orders -- A Comparison of Expert Labeling, Text Classification and AI-Assisted Tagging for Computing Failure Rates of Wind Turbines. (arXiv:2311.04064v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.04064</link>
<description rdf:parseType="Literal">&lt;p&gt;Maintenance work orders are commonly used to document information about wind
turbine operation and maintenance. This includes details about proactive and
reactive wind turbine downtimes, such as preventative and corrective
maintenance. However, the information contained in maintenance work orders is
often unstructured and difficult to analyze, presenting challenges for
decision-makers wishing to use it for optimizing operation and maintenance. To
address this issue, this work compares three different approaches to calculate
reliability by performance indicators from maintenance work orders. The first
approach involves manual labeling of the maintenance work orders by domain
experts, using the schema defined in an industrial guideline to assign the
label accordingly. The second approach involves the development of a model that
automatically labels the maintenance work orders using text classification
methods. Through this method, we are able to achieve macro average and weighted
average F1-Scores of 0.75 and 0.85 respectively. The third technique uses an
AI-assisted tagging tool to tag and structure the raw maintenance information,
together with a novel rule-based approach for extracting relevant maintenance
work orders for failure rate calculation. In our experiments the AI-assisted
tool leads to a 88% drop in tagging time in comparison to the other two
approaches, while expert labeling and text classification are more accurate in
KPI extraction. Overall, our findings make extracting maintenance information
from maintenance work orders more efficient, enable the assessment of
reliability key performance indicators and therefore support the optimization
of wind turbine operation and maintenance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lutz_M/0/1/0/all/0/1&quot;&gt;Marc-Alexander Lutz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schafermeier_B/0/1/0/all/0/1&quot;&gt;Bastian Sch&amp;#xe4;fermeier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sexton_R/0/1/0/all/0/1&quot;&gt;Rachael Sexton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharp_M/0/1/0/all/0/1&quot;&gt;Michael Sharp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dima_A/0/1/0/all/0/1&quot;&gt;Alden Dima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faulstich_S/0/1/0/all/0/1&quot;&gt;Stefan Faulstich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aluri_J/0/1/0/all/0/1&quot;&gt;Jagan Mohini Aluri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09428">
<title>Beyond Detection: Unveiling Fairness Vulnerabilities in Abusive Language Models. (arXiv:2311.09428v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09428</link>
<description rdf:parseType="Literal">&lt;p&gt;This work investigates the potential of undermining both fairness and
detection performance in abusive language detection. In a dynamic and complex
digital world, it is crucial to investigate the vulnerabilities of these
detection models to adversarial fairness attacks to improve their fairness
robustness. We propose a simple yet effective framework FABLE that leverages
backdoor attacks as they allow targeted control over the fairness and detection
performance. FABLE explores three types of trigger designs (i.e., rare,
artificial, and natural triggers) and novel sampling strategies. Specifically,
the adversary can inject triggers into samples in the minority group with the
favored outcome (i.e., &quot;non-abusive&quot;) and flip their labels to the unfavored
outcome, i.e., &quot;abusive&quot;. Experiments on benchmark datasets demonstrate the
effectiveness of FABLE attacking fairness and utility in abusive language
detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yueqing Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1&quot;&gt;Lu Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Payani_A/0/1/0/all/0/1&quot;&gt;Ali Payani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_K/0/1/0/all/0/1&quot;&gt;Kai Shu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16173">
<title>Conditions for Length Generalization in Learning Reasoning Skills. (arXiv:2311.16173v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16173</link>
<description rdf:parseType="Literal">&lt;p&gt;Reasoning is a fundamental capability of AI agents. Recently, large language
models (LLMs) have shown remarkable abilities to perform reasoning tasks.
However, numerous evaluations of the reasoning capabilities of LLMs have also
showed some limitations. An outstanding limitation is length generalization,
meaning that when trained on reasoning problems of smaller lengths or sizes,
the resulting models struggle with problems of larger sizes or lengths. This
potentially indicates some theoretical limitations of generalization in
learning reasoning skills. These evaluations and their observations motivated
us to perform a theoretical study of the length generalization problem. This
work focuses on reasoning tasks that can be formulated as Markov dynamic
processes (MDPs) and/or directed acyclic graphs (DAGs). It identifies and
proves conditions that decide whether the length generalization problem can be
solved or not for a reasoning task in a particular representation. Experiments
are also conducted to verify the theoretical results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Changnan Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bing Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17030">
<title>Is This the Subspace You Are Looking for? An Interpretability Illusion for Subspace Activation Patching. (arXiv:2311.17030v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17030</link>
<description rdf:parseType="Literal">&lt;p&gt;Mechanistic interpretability aims to understand model behaviors in terms of
specific, interpretable features, often hypothesized to manifest as
low-dimensional subspaces of activations. Specifically, recent studies have
explored subspace interventions (such as activation patching) as a way to
simultaneously manipulate model behavior and attribute the features behind it
to given subspaces.
&lt;/p&gt;
&lt;p&gt;In this work, we demonstrate that these two aims diverge, potentially leading
to an illusory sense of interpretability. Counterintuitively, even if a
subspace intervention makes the model&apos;s output behave as if the value of a
feature was changed, this effect may be achieved by activating a dormant
parallel pathway leveraging another subspace that is causally disconnected from
model outputs. We demonstrate this phenomenon in a distilled mathematical
example, in two real-world domains (the indirect object identification task and
factual recall), and present evidence for its prevalence in practice. In the
context of factual recall, we further show a link to rank-1 fact editing,
providing a mechanistic explanation for previous work observing an
inconsistency between fact editing performance and fact localization.
&lt;/p&gt;
&lt;p&gt;However, this does not imply that activation patching of subspaces is
intrinsically unfit for interpretability. To contextualize our findings, we
also show what a success case looks like in a task (indirect object
identification) where prior manual circuit analysis informs an understanding of
the location of a feature. We explore the additional evidence needed to argue
that a patched subspace is faithful.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Makelov_A/0/1/0/all/0/1&quot;&gt;Aleksandar Makelov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lange_G/0/1/0/all/0/1&quot;&gt;Georg Lange&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nanda_N/0/1/0/all/0/1&quot;&gt;Neel Nanda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.18260">
<title>Consensus, dissensus and synergy between clinicians and specialist foundation models in radiology report generation. (arXiv:2311.18260v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.18260</link>
<description rdf:parseType="Literal">&lt;p&gt;Radiology reports are an instrumental part of modern medicine, informing key
clinical decisions such as diagnosis and treatment. The worldwide shortage of
radiologists, however, restricts access to expert care and imposes heavy
workloads, contributing to avoidable errors and delays in report delivery.
While recent progress in automated report generation with vision-language
models offer clear potential in ameliorating the situation, the path to
real-world adoption has been stymied by the challenge of evaluating the
clinical quality of AI-generated reports. In this study, we build a
state-of-the-art report generation system for chest radiographs,
\textit{Flamingo-CXR}, by fine-tuning a well-known vision-language foundation
model on radiology data. To evaluate the quality of the AI-generated reports, a
group of 16 certified radiologists provide detailed evaluations of AI-generated
and human written reports for chest X-rays from an intensive care setting in
the United States and an inpatient setting in India. At least one radiologist
(out of two per case) preferred the AI report to the ground truth report in
over 60$\%$ of cases for both datasets. Amongst the subset of AI-generated
reports that contain errors, the most frequently cited reasons were related to
the location and finding, whereas for human written reports, most mistakes were
related to severity and finding. This disparity suggested potential
complementarity between our AI system and human experts, prompting us to
develop an assistive scenario in which \textit{Flamingo-CXR} generates a
first-draft report, which is subsequently revised by a clinician. This is the
first demonstration of clinician-AI collaboration for report writing, and the
resultant reports are assessed to be equivalent or preferred by at least one
radiologist to reports written by experts alone in 80$\%$ of in-patient cases
and 60$\%$ of intensive care cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tanno_R/0/1/0/all/0/1&quot;&gt;Ryutaro Tanno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Barrett_D/0/1/0/all/0/1&quot;&gt;David G.T. Barrett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sellergren_A/0/1/0/all/0/1&quot;&gt;Andrew Sellergren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ghaisas_S/0/1/0/all/0/1&quot;&gt;Sumedh Ghaisas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dathathri_S/0/1/0/all/0/1&quot;&gt;Sumanth Dathathri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+See_A/0/1/0/all/0/1&quot;&gt;Abigail See&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Welbl_J/0/1/0/all/0/1&quot;&gt;Johannes Welbl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Singhal_K/0/1/0/all/0/1&quot;&gt;Karan Singhal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Azizi_S/0/1/0/all/0/1&quot;&gt;Shekoofeh Azizi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tu_T/0/1/0/all/0/1&quot;&gt;Tao Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schaekermann_M/0/1/0/all/0/1&quot;&gt;Mike Schaekermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+May_R/0/1/0/all/0/1&quot;&gt;Rhys May&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_R/0/1/0/all/0/1&quot;&gt;Roy Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Man_S/0/1/0/all/0/1&quot;&gt;SiWai Man&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ahmed_Z/0/1/0/all/0/1&quot;&gt;Zahra Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mahdavi_S/0/1/0/all/0/1&quot;&gt;Sara Mahdavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Belgrave_D/0/1/0/all/0/1&quot;&gt;Danielle Belgrave&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Natarajan_V/0/1/0/all/0/1&quot;&gt;Vivek Natarajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shetty_S/0/1/0/all/0/1&quot;&gt;Shravya Shetty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kohli_P/0/1/0/all/0/1&quot;&gt;Pushmeet Kohli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_P/0/1/0/all/0/1&quot;&gt;Po-Sen Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Karthikesalingam_A/0/1/0/all/0/1&quot;&gt;Alan Karthikesalingam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ktena_I/0/1/0/all/0/1&quot;&gt;Ira Ktena&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01454">
<title>D-Bot: Database Diagnosis System using Large Language Models. (arXiv:2312.01454v2 [cs.DB] UPDATED)</title>
<link>http://arxiv.org/abs/2312.01454</link>
<description rdf:parseType="Literal">&lt;p&gt;Database administrators (DBAs) play an important role in managing,
maintaining and optimizing database systems. However, it is hard and tedious
for DBAs to manage a large number of databases and give timely response
(waiting for hours is intolerable in many online cases). In addition, existing
empirical methods only support limited diagnosis scenarios, which are also
labor-intensive to update the diagnosis rules for database version updates.
Recently large language models (LLMs) have shown great potential in various
fields. Thus, we propose D-Bot, an LLM-based database diagnosis system that can
automatically acquire knowledge from diagnosis documents, and generate
reasonable and well-founded diagnosis report (i.e., identifying the root causes
and solutions) within acceptable time (e.g., under 10 minutes compared to hours
by a DBA). The techniques in D-Bot include (i) offline knowledge extraction
from documents, (ii) automatic prompt generation (e.g., knowledge matching,
tool retrieval), (iii) root cause analysis using tree search algorithm, and
(iv) collaborative mechanism for complex anomalies with multiple root causes.
We verify D-Bot on real benchmarks (including 539 anomalies of six typical
applications), and the results show that D-Bot can effectively analyze the root
causes of unseen anomalies and significantly outperforms traditional methods
and vanilla models like GPT-4.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xuanhe Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guoliang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhaoyan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weize Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jianming Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiesi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1&quot;&gt;Ruohang Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1&quot;&gt;Guoyang Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02125">
<title>TPPoet: Transformer-Based Persian Poem Generation using Minimal Data and Advanced Decoding Techniques. (arXiv:2312.02125v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02125</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in language models (LMs), have demonstrated significant
efficacy in tasks related to the arts and humanities. While LMs have exhibited
exceptional performance across a wide range of natural language processing
tasks, there are notable challenges associated with their utilization on small
datasets and their ability to replicate more creative human capacities. In this
study, we aim to address these challenges by training a Persian classical
poetry generation model using a transformer architecture on a specialized
dataset with no pretraining. Additionally, we propose a novel decoding method
to enhance coherence and meaningfulness in the generated poetry, effectively
managing the tradeoff between diversity and quality. Furthermore, the results
of our training approach and the proposed decoding method are evaluated through
comprehensive set of automatic and human evaluations and showed its superior
capability to generate coherent and meaningful poetry in compare to other
decoding methods and an existing Persian large language model (LLM).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panahandeh_A/0/1/0/all/0/1&quot;&gt;Amir Panahandeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asemi_H/0/1/0/all/0/1&quot;&gt;Hanie Asemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nourani_E/0/1/0/all/0/1&quot;&gt;Esmaeil Nourani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02439">
<title>Let&apos;s Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation. (arXiv:2312.02439v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02439</link>
<description rdf:parseType="Literal">&lt;p&gt;Chain-of-Thought (CoT) guides large language models (LLMs) to reason
step-by-step, and can motivate their logical reasoning ability. While effective
for logical tasks, CoT is not conducive to creative problem-solving which often
requires out-of-box thoughts and is crucial for innovation advancements. In
this paper, we explore the Leap-of-Thought (LoT) abilities within LLMs -- a
non-sequential, creative paradigm involving strong associations and knowledge
leaps. To this end, we study LLMs on the popular Oogiri game which needs
participants to have good creativity and strong associative thinking for
responding unexpectedly and humorously to the given image, text, or both, and
thus is suitable for LoT study. Then to investigate LLMs&apos; LoT ability in the
Oogiri game, we first build a multimodal and multilingual Oogiri-GO dataset
which contains over 130,000 samples from the Oogiri game, and observe the
insufficient LoT ability or failures of most existing LLMs on the Oogiri game.
Accordingly, we introduce a creative Leap-of-Thought (CLoT) paradigm to improve
LLM&apos;s LoT ability. CLoT first formulates the Oogiri-GO dataset into
LoT-oriented instruction tuning data to train pretrained LLM for achieving
certain LoT humor generation and discrimination abilities. Then CLoT designs an
explorative self-refinement that encourages the LLM to generate more creative
LoT data via exploring parallels between seemingly unrelated concepts and
selects high-quality data to train itself for self-refinement. CLoT not only
excels in humor generation in the Oogiri game but also boosts creative
abilities in various tasks like cloud guessing game and divergent association
task. These findings advance our understanding and offer a pathway to improve
LLMs&apos; creative capacities for innovative applications across domains. The
dataset, code, and models will be released online.
https://zhongshsh.github.io/CLoT/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1&quot;&gt;Shanshan Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhongzhan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Shanghua Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1&quot;&gt;Wushao Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Liang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zitnik_M/0/1/0/all/0/1&quot;&gt;Marinka Zitnik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1&quot;&gt;Pan Zhou&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>