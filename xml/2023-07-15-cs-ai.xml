<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-13T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06341" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06382" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06391" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06396" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06399" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06439" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06440" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06463" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06483" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06496" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06501" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06507" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06513" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06521" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06541" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06564" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06566" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06577" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06608" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06616" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06630" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06647" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06682" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06687" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06698" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06701" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06709" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06736" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06742" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06758" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06775" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06794" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06822" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06824" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06825" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06844" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06845" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06857" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06858" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06865" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06869" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06870" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06877" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06908" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06913" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06917" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06924" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06933" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06941" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06945" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06947" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06949" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1912.13122" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2106.06613" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2109.05714" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2109.13445" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.07372" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.02819" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.14838" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.04827" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.11290" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.07734" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.10081" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.14905" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.00241" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.06407" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.15944" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.00679" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.03044" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.01802" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05222" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.13339" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00470" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01316" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03109" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03198" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03875" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05766" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05921" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.06341">
<title>Assessment of the suitability of degradation models for the planning of CCTV inspections of sewer pipes. (arXiv:2307.06341v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06341</link>
<description rdf:parseType="Literal">&lt;p&gt;The degradation of sewer pipes poses significant economical, environmental
and health concerns. The maintenance of such assets requires structured plans
to perform inspections, which are more efficient when structural and
environmental features are considered along with the results of previous
inspection reports. The development of such plans requires degradation models
that can be based on statistical and machine learning methods. This work
proposes a methodology to assess their suitability to plan inspections
considering three dimensions: accuracy metrics, ability to produce long-term
degradation curves and explainability. Results suggest that although ensemble
models yield the highest accuracy, they are unable to infer the long-term
degradation of the pipes, whereas the Logistic Regression offers a slightly
less accurate model that is able to produce consistent degradation curves with
a high explainability. A use case is presented to demonstrate this methodology
and the efficiency of model-based planning compared to the current inspection
plan.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morer_F/0/1/0/all/0/1&quot;&gt;Fidae El Morer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wittek_S/0/1/0/all/0/1&quot;&gt;Stefan Wittek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rausch_A/0/1/0/all/0/1&quot;&gt;Andreas Rausch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06382">
<title>Rethinking Answer Set Programming Templates. (arXiv:2307.06382v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.06382</link>
<description rdf:parseType="Literal">&lt;p&gt;In imperative programming, the Domain-Driven Design methodology helps in
coping with the complexity of software development by materializing in code the
invariants of a domain of interest. Code is cleaner and more secure because any
implicit assumption is removed in favor of invariants, thus enabling a fail
fast mindset and the immediate reporting of unexpected conditions. This article
introduces a notion of template for Answer Set Programming that, in addition to
the don&apos;t repeat yourself principle, enforces locality of some predicates by
means of a simple naming convention. Local predicates are mapped to the usual
global namespace adopted by mainstream engines, using universally unique
identifiers to avoid name clashes. This way, local predicates can be used to
enforce invariants on the expected outcome of a template in a possibly empty
context of application, independently by other rules that can be added to such
a context. Template applications transpiled this way can be processed by
mainstream engines and safely shared with other knowledge designers, even when
they have zero knowledge of templates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alviano_M/0/1/0/all/0/1&quot;&gt;Mario Alviano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ianni_G/0/1/0/all/0/1&quot;&gt;Giovambattista Ianni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pacenza_F/0/1/0/all/0/1&quot;&gt;Francesco Pacenza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zangari_J/0/1/0/all/0/1&quot;&gt;Jessica Zangari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06391">
<title>Maximizing Penetration Testing Success with Effective Reconnaissance Techniques using ChatGPT. (arXiv:2307.06391v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2307.06391</link>
<description rdf:parseType="Literal">&lt;p&gt;ChatGPT is a generative pretrained transformer language model created using
artificial intelligence implemented as chatbot which can provide very detailed
responses to a wide variety of questions. As a very contemporary phenomenon,
this tool has a wide variety of potential use cases that have yet to be
explored. With the significant extent of information on a broad assortment of
potential topics, ChatGPT could add value to many information security uses
cases both from an efficiency perspective as well as to offer another source of
security information that could be used to assist with securing Internet
accessible assets of organizations. One information security practice that
could benefit from ChatGPT is the reconnaissance phase of penetration testing.
This research uses a case study methodology to explore and investigate the uses
of ChatGPT in obtaining valuable reconnaissance data. ChatGPT is able to
provide many types of intel regarding targeted properties which includes
Internet Protocol (IP) address ranges, domain names, network topology, vendor
technologies, SSL/TLS ciphers, ports &amp;amp; services, and operating systems used by
the target. The reconnaissance information can then be used during the planning
phase of a penetration test to determine the tactics, tools, and techniques to
guide the later phases of the penetration test in order to discover potential
risks such as unpatched software components and security misconfiguration
related issues. The study provides insights into how artificial intelligence
language models can be used in cybersecurity and contributes to the advancement
of penetration testing techniques.
&lt;/p&gt;
&lt;p&gt;Keywords: ChatGPT, Penetration Testing, Reconnaissance
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Temara_S/0/1/0/all/0/1&quot;&gt;Sheetal Temara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06396">
<title>Introduction to Facial Micro Expressions Analysis Using Color and Depth Images: A Matlab Coding Approach (Second Edition, 2023). (arXiv:2307.06396v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06396</link>
<description rdf:parseType="Literal">&lt;p&gt;The book attempts to introduce a gentle introduction to the field of Facial
Micro Expressions Recognition (FMER) using Color and Depth images, with the aid
of MATLAB programming environment. FMER is a subset of image processing and it
is a multidisciplinary topic to analysis. So, it requires familiarity with
other topics of Artifactual Intelligence (AI) such as machine learning, digital
image processing, psychology and more. So, it is a great opportunity to write a
book which covers all of these topics for beginner to professional readers in
the field of AI and even without having background of AI. Our goal is to
provide a standalone introduction in the field of MFER analysis in the form of
theorical descriptions for readers with no background in image processing with
reproducible Matlab practical examples. Also, we describe any basic definitions
for FMER analysis and MATLAB library which is used in the text, that helps
final reader to apply the experiments in the real-world applications. We
believe that this book is suitable for students, researchers, and professionals
alike, who need to develop practical skills, along with a basic understanding
of the field. We expect that, after reading this book, the reader feels
comfortable with different key stages such as color and depth image processing,
color and depth image representation, classification, machine learning, facial
micro-expressions recognition, feature extraction and dimensionality reduction.
The book attempts to introduce a gentle introduction to the field of Facial
Micro Expressions Recognition (FMER) using Color and Depth images, with the aid
of MATLAB programming environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mousavi_S/0/1/0/all/0/1&quot;&gt;Seyed Muhammad Hossein Mousavi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06399">
<title>Designing Behavior Trees from Goal-Oriented LTLf Formulas. (arXiv:2307.06399v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.06399</link>
<description rdf:parseType="Literal">&lt;p&gt;Temporal logic can be used to formally specify autonomous agent goals, but
synthesizing planners that guarantee goal satisfaction can be computationally
prohibitive. This paper shows how to turn goals specified using a subset of
finite trace Linear Temporal Logic (LTL) into a behavior tree (BT) that
guarantees that successful traces satisfy the LTL goal. Useful LTL formulas for
achievement goals can be derived using achievement-oriented task mission
grammars, leading to missions made up of tasks combined using LTL operators.
Constructing BTs from LTL formulas leads to a relaxed behavior synthesis
problem in which a wide range of planners can implement the action nodes in the
BT. Importantly, any successful trace induced by the planners satisfies the
corresponding LTL formula. The usefulness of the approach is demonstrated in
two ways: a) exploring the alignment between two planners and LTL goals, and b)
solving a sequential key-door problem for a Fetch robot.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neupane_A/0/1/0/all/0/1&quot;&gt;Aadesh Neupane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodrich_M/0/1/0/all/0/1&quot;&gt;Michael A. Goodrich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06439">
<title>Distilling Large Language Models for Biomedical Knowledge Extraction: A Case Study on Adverse Drug Events. (arXiv:2307.06439v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.06439</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs), such as GPT-4, have demonstrated remarkable
capabilities across a wide range of tasks, including health applications. In
this paper, we study how LLMs can be used to scale biomedical knowledge
curation. We find that while LLMs already possess decent competency in
structuring biomedical text, by distillation into a task-specific student model
through self-supervised learning, substantial gains can be attained over
out-of-box LLMs, with additional advantages such as cost, efficiency, and
white-box model access.
&lt;/p&gt;
&lt;p&gt;We conduct a case study on adverse drug event (ADE) extraction, which is an
important area for improving care. On standard ADE extraction evaluation, a
GPT-3.5 distilled PubMedBERT model attained comparable accuracy as supervised
state-of-the-art models without using any labeled data. Despite being over
1,000 times smaller, the distilled model outperformed its teacher GPT-3.5 by
over 6 absolute points in F1 and GPT-4 by over 5 absolute points.
&lt;/p&gt;
&lt;p&gt;Ablation studies on distillation model choice (e.g., PubMedBERT vs BioGPT)
and ADE extraction architecture shed light on best practice for biomedical
knowledge extraction. Similar gains were attained by distillation for other
standard biomedical knowledge extraction tasks such as gene-disease
associations and protected health information, further illustrating the promise
of this approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yu Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Sheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1&quot;&gt;Naoto Usuyama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woldesenbet_Y/0/1/0/all/0/1&quot;&gt;Yonas Woldesenbet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1&quot;&gt;Cliff Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanapathi_P/0/1/0/all/0/1&quot;&gt;Praneeth Sanapathi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1&quot;&gt;Mu Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valluri_N/0/1/0/all/0/1&quot;&gt;Naveen Valluri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strandberg_E/0/1/0/all/0/1&quot;&gt;Erika Strandberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1&quot;&gt;Tristan Naumann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1&quot;&gt;Hoifung Poon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06440">
<title>No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models. (arXiv:2307.06440v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06440</link>
<description rdf:parseType="Literal">&lt;p&gt;The computation necessary for training Transformer-based language models has
skyrocketed in recent years. This trend has motivated research on efficient
training algorithms designed to improve training, validation, and downstream
performance faster than standard training. In this work, we revisit three
categories of such algorithms: dynamic architectures (layer stacking, layer
dropping), batch selection (selective backprop, RHO loss), and efficient
optimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed
computation budget using such methods, we find that their training, validation,
and downstream gains vanish compared to a baseline with a fully-decayed
learning rate. We define an evaluation protocol that enables computation to be
done on arbitrary machines by mapping all computation time to a reference
machine which we call reference system time. We discuss the limitations of our
proposed protocol and release our code to encourage rigorous research in
efficient training procedures: https://github.com/JeanKaddour/NoTrainNoGain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaddour_J/0/1/0/all/0/1&quot;&gt;Jean Kaddour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Key_O/0/1/0/all/0/1&quot;&gt;Oscar Key&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nawrot_P/0/1/0/all/0/1&quot;&gt;Piotr Nawrot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1&quot;&gt;Pasquale Minervini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kusner_M/0/1/0/all/0/1&quot;&gt;Matt J. Kusner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06463">
<title>Efficiently-Verifiable Strong Uniquely Solvable Puzzles and Matrix Multiplication. (arXiv:2307.06463v1 [cs.CC])</title>
<link>http://arxiv.org/abs/2307.06463</link>
<description rdf:parseType="Literal">&lt;p&gt;We advance the Cohn-Umans framework for developing fast matrix multiplication
algorithms. We introduce, analyze, and search for a new subclass of strong
uniquely solvable puzzles (SUSP), which we call simplifiable SUSPs. We show
that these puzzles are efficiently verifiable, which remains an open question
for general SUSPs. We also show that individual simplifiable SUSPs can achieve
the same strength of bounds on the matrix multiplication exponent $\omega$ that
infinite families of SUSPs can. We report on the construction, by computer
search, of larger SUSPs than previously known for small width. This, combined
with our tighter analysis, strengthens the upper bound on the matrix
multiplication exponent from $2.66$ to $2.505$ obtainable via this
computational approach, and nears the results of the handcrafted constructions
of Cohn et al.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anderson_M/0/1/0/all/0/1&quot;&gt;Matthew Anderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_V/0/1/0/all/0/1&quot;&gt;Vu Le&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06483">
<title>Misclassification in Automated Content Analysis Causes Bias in Regression. Can We Fix It? Yes We Can!. (arXiv:2307.06483v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06483</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated classifiers (ACs), often built via supervised machine learning
(SML), can categorize large, statistically powerful samples of data ranging
from text to images and video, and have become widely popular measurement
devices in communication science and related fields. Despite this popularity,
even highly accurate classifiers make errors that cause misclassification bias
and misleading results in downstream analyses-unless such analyses account for
these errors. As we show in a systematic literature review of SML applications,
communication scholars largely ignore misclassification bias. In principle,
existing statistical methods can use &quot;gold standard&quot; validation data, such as
that created by human annotators, to correct misclassification bias and produce
consistent estimates. We introduce and test such methods, including a new
method we design and implement in the R package misclassificationmodels, via
Monte Carlo simulations designed to reveal each method&apos;s limitations, which we
also release. Based on our results, we recommend our new error correction
method as it is versatile and efficient. In sum, automated classifiers, even
those below common accuracy standards or making systematic misclassifications,
can be useful for measurement with careful study design and appropriate error
correction methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+TeBlunthuis_N/0/1/0/all/0/1&quot;&gt;Nathan TeBlunthuis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hase_V/0/1/0/all/0/1&quot;&gt;Valerie Hase&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1&quot;&gt;Chung-Hong Chan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06496">
<title>Microbial Genetic Algorithm-based Black-box Attack against Interpretable Deep Learning Systems. (arXiv:2307.06496v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06496</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models are susceptible to adversarial samples in white and
black-box environments. Although previous studies have shown high attack
success rates, coupling DNN models with interpretation models could offer a
sense of security when a human expert is involved, who can identify whether a
given sample is benign or malicious. However, in white-box environments,
interpretable deep learning systems (IDLSes) have been shown to be vulnerable
to malicious manipulations. In black-box settings, as access to the components
of IDLSes is limited, it becomes more challenging for the adversary to fool the
system. In this work, we propose a Query-efficient Score-based black-box attack
against IDLSes, QuScore, which requires no knowledge of the target model and
its coupled interpretation model. QuScore is based on transfer-based and
score-based methods by employing an effective microbial genetic algorithm. Our
method is designed to reduce the number of queries necessary to carry out
successful attacks, resulting in a more efficient process. By continuously
refining the adversarial samples created based on feedback scores from the
IDLS, our approach effectively navigates the search space to identify
perturbations that can fool the system. We evaluate the attack&apos;s effectiveness
on four CNN models (Inception, ResNet, VGG, DenseNet) and two interpretation
models (CAM, Grad), using both ImageNet and CIFAR datasets. Our results show
that the proposed approach is query-efficient with a high attack success rate
that can reach between 95% and 100% and transferability with an average success
rate of 69% in the ImageNet and CIFAR datasets. Our attack method generates
adversarial examples with attribution maps that resemble benign samples. We
have also demonstrated that our attack is resilient against various
preprocessing defense techniques and can easily be transferred to different DNN
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdukhamidov_E/0/1/0/all/0/1&quot;&gt;Eldor Abdukhamidov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abuhamad_M/0/1/0/all/0/1&quot;&gt;Mohammed Abuhamad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1&quot;&gt;Simon S. Woo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_Tin_E/0/1/0/all/0/1&quot;&gt;Eric Chan-Tin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abuhmed_T/0/1/0/all/0/1&quot;&gt;Tamer Abuhmed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06501">
<title>Hybrid Control Policy for Artificial Pancreas via Ensemble Deep Reinforcement Learning. (arXiv:2307.06501v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.06501</link>
<description rdf:parseType="Literal">&lt;p&gt;Objective: The artificial pancreas (AP) has shown promising potential in
achieving closed-loop glucose control for individuals with type 1 diabetes
mellitus (T1DM). However, designing an effective control policy for the AP
remains challenging due to the complex physiological processes, delayed insulin
response, and inaccurate glucose measurements. While model predictive control
(MPC) offers safety and stability through the dynamic model and safety
constraints, it lacks individualization and is adversely affected by
unannounced meals. Conversely, deep reinforcement learning (DRL) provides
personalized and adaptive strategies but faces challenges with distribution
shifts and substantial data requirements. Methods: We propose a hybrid control
policy for the artificial pancreas (HyCPAP) to address the above challenges.
HyCPAP combines an MPC policy with an ensemble DRL policy, leveraging the
strengths of both policies while compensating for their respective limitations.
To facilitate faster deployment of AP systems in real-world settings, we
further incorporate meta-learning techniques into HyCPAP, leveraging previous
experience and patient-shared knowledge to enable fast adaptation to new
patients with limited available data. Results: We conduct extensive experiments
using the FDA-accepted UVA/Padova T1DM simulator across three scenarios. Our
approaches achieve the highest percentage of time spent in the desired
euglycemic range and the lowest occurrences of hypoglycemia. Conclusion: The
results clearly demonstrate the superiority of our methods for closed-loop
glucose management in individuals with T1DM. Significance: The study presents
novel control policies for AP systems, affirming the great potential of
proposed methods for efficient closed-loop glucose control.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_W/0/1/0/all/0/1&quot;&gt;Wenzhou Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tianyu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1&quot;&gt;Luolin Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Liang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jian Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1&quot;&gt;Feng Qi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06507">
<title>Improving Nonalcoholic Fatty Liver Disease Classification Performance With Latent Diffusion Models. (arXiv:2307.06507v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06507</link>
<description rdf:parseType="Literal">&lt;p&gt;Integrating deep learning with clinical expertise holds great potential for
addressing healthcare challenges and empowering medical professionals with
improved diagnostic tools. However, the need for annotated medical images is
often an obstacle to leveraging the full power of machine learning models. Our
research demonstrates that by combining synthetic images, generated using
diffusion models, with real images, we can enhance nonalcoholic fatty liver
disease (NAFLD) classification performance. We evaluate the quality of the
synthetic images by comparing two metrics: Inception Score (IS) and Fr\&apos;{e}chet
Inception Distance (FID), computed on diffusion-generated images and generative
adversarial networks (GANs)-generated images. Our results show superior
performance for the diffusion-generated images, with a maximum IS score of
$1.90$ compared to $1.67$ for GANs, and a minimum FID score of $69.45$ compared
to $99.53$ for GANs. Utilizing a partially frozen CNN backbone (EfficientNet
v1), our synthetic augmentation method achieves a maximum image-level ROC AUC
of $0.904$ on a NAFLD prediction task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hardy_R/0/1/0/all/0/1&quot;&gt;Romain Hardy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilin_C/0/1/0/all/0/1&quot;&gt;Cornelia Ilin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klepich_J/0/1/0/all/0/1&quot;&gt;Joe Klepich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitchell_R/0/1/0/all/0/1&quot;&gt;Ryan Mitchell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hall_S/0/1/0/all/0/1&quot;&gt;Steve Hall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Villareal_J/0/1/0/all/0/1&quot;&gt;Jericho Villareal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06513">
<title>Leveraging Contextual Counterfactuals Toward Belief Calibration. (arXiv:2307.06513v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.06513</link>
<description rdf:parseType="Literal">&lt;p&gt;Beliefs and values are increasingly being incorporated into our AI systems
through alignment processes, such as carefully curating data collection
principles or regularizing the loss function used for training. However, the
meta-alignment problem is that these human beliefs are diverse and not aligned
across populations; furthermore, the implicit strength of each belief may not
be well calibrated even among humans, especially when trying to generalize
across contexts. Specifically, in high regret situations, we observe that
contextual counterfactuals and recourse costs are particularly important in
updating a decision maker&apos;s beliefs and the strengths to which such beliefs are
held. Therefore, we argue that including counterfactuals is key to an accurate
calibration of beliefs during alignment. To do this, we first segment belief
diversity into two categories: subjectivity (across individuals within a
population) and epistemic uncertainty (within an individual across different
contexts). By leveraging our notion of epistemic uncertainty, we introduce `the
belief calibration cycle&apos; framework to more holistically calibrate this
diversity of beliefs with context-driven counterfactual reasoning by using a
multi-objective optimization. We empirically apply our framework for finding a
Pareto frontier of clustered optimal belief strengths that generalize across
different contexts, demonstrating its efficacy on a toy dataset for credit
decisions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiuyi/0/1/0/all/0/1&quot;&gt;Qiuyi&lt;/a&gt; (Richard) &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang/0/1/0/all/0/1&quot;&gt;Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1&quot;&gt;Michael S. Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Sherol Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06521">
<title>Artificial Intelligence for Drug Discovery: Are We There Yet?. (arXiv:2307.06521v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.06521</link>
<description rdf:parseType="Literal">&lt;p&gt;Drug discovery is adapting to novel technologies such as data science,
informatics, and artificial intelligence (AI) to accelerate effective treatment
development while reducing costs and animal experiments. AI is transforming
drug discovery, as indicated by increasing interest from investors, industrial
and academic scientists, and legislators. Successful drug discovery requires
optimizing properties related to pharmacodynamics, pharmacokinetics, and
clinical outcomes. This review discusses the use of AI in the three pillars of
drug discovery: diseases, targets, and therapeutic modalities, with a focus on
small molecule drugs. AI technologies, such as generative chemistry, machine
learning, and multi-property optimization, have enabled several compounds to
enter clinical trials. The scientific community must carefully vet known
information to address the reproducibility crisis. The full potential of AI in
drug discovery can only be realized with sufficient ground truth and
appropriate human intervention at later pipeline stages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasselgren_C/0/1/0/all/0/1&quot;&gt;Catrin Hasselgren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oprea_T/0/1/0/all/0/1&quot;&gt;Tudor I. Oprea&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06541">
<title>On the Effective Horizon of Inverse Reinforcement Learning. (arXiv:2307.06541v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06541</link>
<description rdf:parseType="Literal">&lt;p&gt;Inverse reinforcement learning (IRL) algorithms often rely on (forward)
reinforcement learning or planning over a given time horizon to compute an
approximately optimal policy for a hypothesized reward function and then match
this policy with expert demonstrations. The time horizon plays a critical role
in determining both the accuracy of reward estimate and the computational
efficiency of IRL algorithms. Interestingly, an effective time horizon shorter
than the ground-truth value often produces better results faster. This work
formally analyzes this phenomenon and provides an explanation: the time horizon
controls the complexity of an induced policy class and mitigates overfitting
with limited data. This analysis leads to a principled choice of the effective
horizon for IRL. It also prompts us to reexamine the classic IRL formulation:
it is more natural to learn jointly the reward and the effective horizon
together rather than the reward alone with a given horizon. Our experimental
results confirm the theoretical analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yiqing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doshi_Velez_F/0/1/0/all/0/1&quot;&gt;Finale Doshi-Velez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_D/0/1/0/all/0/1&quot;&gt;David Hsu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06564">
<title>Prescriptive Process Monitoring Under Resource Constraints: A Reinforcement Learning Approach. (arXiv:2307.06564v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.06564</link>
<description rdf:parseType="Literal">&lt;p&gt;Prescriptive process monitoring methods seek to optimize the performance of
business processes by triggering interventions at runtime, thereby increasing
the probability of positive case outcomes. These interventions are triggered
according to an intervention policy. Reinforcement learning has been put
forward as an approach to learning intervention policies through trial and
error. Existing approaches in this space assume that the number of resources
available to perform interventions in a process is unlimited, an unrealistic
assumption in practice. This paper argues that, in the presence of resource
constraints, a key dilemma in the field of prescriptive process monitoring is
to trigger interventions based not only on predictions of their necessity,
timeliness, or effect but also on the uncertainty of these predictions and the
level of resource utilization. Indeed, committing scarce resources to an
intervention when the necessity or effects of this intervention are highly
uncertain may intuitively lead to suboptimal intervention effects. Accordingly,
the paper proposes a reinforcement learning approach for prescriptive process
monitoring that leverages conformal prediction techniques to consider the
uncertainty of the predictions upon which an intervention decision is based. An
evaluation using real-life datasets demonstrates that explicitly modeling
uncertainty using conformal predictions helps reinforcement learning agents
converge towards policies with higher net intervention gain
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shoush_M/0/1/0/all/0/1&quot;&gt;Mahmoud Shoush&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dumas_M/0/1/0/all/0/1&quot;&gt;Marlon Dumas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06566">
<title>Regression-Oriented Knowledge Distillation for Lightweight Ship Orientation Angle Prediction with Optical Remote Sensing Images. (arXiv:2307.06566v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06566</link>
<description rdf:parseType="Literal">&lt;p&gt;Ship orientation angle prediction (SOAP) with optical remote sensing images
is an important image processing task, which often relies on deep convolutional
neural networks (CNNs) to make accurate predictions. This paper proposes a
novel framework to reduce the model sizes and computational costs of SOAP
models without harming prediction accuracy. First, a new SOAP model called
Mobile-SOAP is designed based on MobileNetV2, achieving state-of-the-art
prediction accuracy. Four tiny SOAP models are also created by replacing the
convolutional blocks in Mobile-SOAP with four small-scale networks,
respectively. Then, to transfer knowledge from Mobile-SOAP to four lightweight
models, we propose a novel knowledge distillation (KD) framework termed SOAP-KD
consisting of a novel feature-based guidance loss and an optimized synthetic
samples-based knowledge transfer mechanism. Lastly, extensive experiments on
the FGSC-23 dataset confirm the superiority of Mobile-SOAP over existing models
and also demonstrate the effectiveness of SOAP-KD in improving the prediction
performance of four specially designed tiny models. Notably, by using SOAP-KD,
the test mean absolute error of the ShuffleNetV2x1.0-based model is only 8%
higher than that of Mobile-SOAP, but its number of parameters and
multiply-accumulate operations (MACs) are respectively 61.6% and 60.8% less.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhan Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1&quot;&gt;Xin Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_P/0/1/0/all/0/1&quot;&gt;Peng Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1&quot;&gt;Ru Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1&quot;&gt;Xiaoxuan Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06577">
<title>RVD: A Handheld Device-Based Fundus Video Dataset for Retinal Vessel Segmentation. (arXiv:2307.06577v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06577</link>
<description rdf:parseType="Literal">&lt;p&gt;Retinal vessel segmentation is generally grounded in image-based datasets
collected with bench-top devices. The static images naturally lose the dynamic
characteristics of retina fluctuation, resulting in diminished dataset
richness, and the usage of bench-top devices further restricts dataset
scalability due to its limited accessibility. Considering these limitations, we
introduce the first video-based retinal dataset by employing handheld devices
for data acquisition. The dataset comprises 635 smartphone-based fundus videos
collected from four different clinics, involving 415 patients from 50 to 75
years old. It delivers comprehensive and precise annotations of retinal
structures in both spatial and temporal dimensions, aiming to advance the
landscape of vasculature segmentation. Specifically, the dataset provides three
levels of spatial annotations: binary vessel masks for overall retinal
structure delineation, general vein-artery masks for distinguishing the vein
and artery, and fine-grained vein-artery masks for further characterizing the
granularities of each artery and vein. In addition, the dataset offers temporal
annotations that capture the vessel pulsation characteristics, assisting in
detecting ocular diseases that require fine-grained recognition of hemodynamic
fluctuation. In application, our dataset exhibits a significant domain shift
with respect to data captured by bench-top devices, thus posing great
challenges to existing methods. In the experiments, we provide evaluation
metrics and benchmark results on our dataset, reflecting both the potential and
challenges it offers for vessel segmentation tasks. We hope this challenging
dataset would significantly contribute to the development of eye disease
diagnosis and early prevention.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;MD Wahiduzzaman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_H/0/1/0/all/0/1&quot;&gt;Hongwei Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1&quot;&gt;Heming Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coroneo_M/0/1/0/all/0/1&quot;&gt;Minas Theodore Coroneo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hajati_F/0/1/0/all/0/1&quot;&gt;Farshid Hajati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shariflou_S/0/1/0/all/0/1&quot;&gt;Sahar Shariflou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalloniatis_M/0/1/0/all/0/1&quot;&gt;Michael Kalloniatis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phu_J/0/1/0/all/0/1&quot;&gt;Jack Phu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agar_A/0/1/0/all/0/1&quot;&gt;Ashish Agar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golzan_M/0/1/0/all/0/1&quot;&gt;Mojtaba Golzan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xin Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06608">
<title>Introducing Foundation Models as Surrogate Models: Advancing Towards More Practical Adversarial Attacks. (arXiv:2307.06608v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06608</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the no-box adversarial attack, in which the attacker lacks access
to the model&apos;s architecture, weights, and training data, become the most
practical and challenging attack setup. However, there is an unawareness of the
potential and flexibility inherent in the surrogate model selection process on
no-box setting. Inspired by the burgeoning interest in utilizing foundational
models to address downstream tasks, this paper adopts an innovative idea that
1) recasting adversarial attack as a downstream task. Specifically, image noise
generation to meet the emerging trend and 2) introducing foundational models as
surrogate models. Harnessing the concept of non-robust features, we elaborate
on two guiding principles for surrogate model selection to explain why the
foundational model is an optimal choice for this role. However, paradoxically,
we observe that these foundational models underperform. Analyzing this
unexpected behavior within the feature space, we attribute the lackluster
performance of foundational models (e.g., CLIP) to their significant
representational capacity and, conversely, their lack of discriminative
prowess. To mitigate this issue, we propose the use of a margin-based loss
strategy for the fine-tuning of foundational models on target images. The
experimental results verify that our approach, which employs the basic Fast
Gradient Sign Method (FGSM) attack algorithm, outstrips the performance of
other, more convoluted algorithms. We conclude by advocating for the research
community to consider surrogate models as crucial determinants in the
effectiveness of adversarial attacks in no-box settings. The implications of
our work bear relevance for improving the efficacy of such adversarial attacks
and the overall robustness of AI systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiaming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sang_J/0/1/0/all/0/1&quot;&gt;Jitao Sang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_Q/0/1/0/all/0/1&quot;&gt;Qi Yi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06616">
<title>SecureFalcon: The Next Cyber Reasoning System for Cyber Security. (arXiv:2307.06616v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2307.06616</link>
<description rdf:parseType="Literal">&lt;p&gt;Software vulnerabilities leading to various detriments such as crashes, data
loss, and security breaches, significantly hinder the quality, affecting the
market adoption of software applications and systems. Although traditional
methods such as automated software testing, fault localization, and repair have
been intensively studied, static analysis tools are most commonly used and have
an inherent false positives rate, posing a solid challenge to developer
productivity. Large Language Models (LLMs) offer a promising solution to these
persistent issues. Among these, FalconLLM has shown substantial potential in
identifying intricate patterns and complex vulnerabilities, hence crucial in
software vulnerability detection. In this paper, for the first time, FalconLLM
is being fine-tuned for cybersecurity applications, thus introducing
SecureFalcon, an innovative model architecture built upon FalconLLM.
SecureFalcon is trained to differentiate between vulnerable and non-vulnerable
C code samples. We build a new training dataset, FormAI, constructed thanks to
Generative Artificial Intelligence (AI) and formal verification to evaluate its
performance. SecureFalcon achieved an impressive 94% accuracy rate in detecting
software vulnerabilities, emphasizing its significant potential to redefine
software vulnerability detection methods in cybersecurity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferrag_M/0/1/0/all/0/1&quot;&gt;Mohamed Amine Ferrag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Battah_A/0/1/0/all/0/1&quot;&gt;Ammar Battah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tihanyi_N/0/1/0/all/0/1&quot;&gt;Norbert Tihanyi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Debbah_M/0/1/0/all/0/1&quot;&gt;Merouane Debbah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lestable_T/0/1/0/all/0/1&quot;&gt;Thierry Lestable&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cordeiro_L/0/1/0/all/0/1&quot;&gt;Lucas C. Cordeiro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06630">
<title>Image Transformation Sequence Retrieval with General Reinforcement Learning. (arXiv:2307.06630v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06630</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, the novel Image Transformation Sequence Retrieval (ITSR) task
is presented, in which a model must retrieve the sequence of transformations
between two given images that act as source and target, respectively. Given
certain characteristics of the challenge such as the multiplicity of a correct
sequence or the correlation between consecutive steps of the process, we
propose a solution to ITSR using a general model-based Reinforcement Learning
such as Monte Carlo Tree Search (MCTS), which is combined with a deep neural
network. Our experiments provide a benchmark in both synthetic and real
domains, where the proposed approach is compared with supervised training. The
results report that a model trained with MCTS is able to outperform its
supervised counterpart in both the simplest and the most complex cases. Our
work draws interesting conclusions about the nature of ITSR and its associated
challenges.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mas_Candela_E/0/1/0/all/0/1&quot;&gt;Enrique Mas-Candela&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rios_Vila_A/0/1/0/all/0/1&quot;&gt;Antonio R&amp;#xed;os-Vila&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calvo_Zaragoza_J/0/1/0/all/0/1&quot;&gt;Jorge Calvo-Zaragoza&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06647">
<title>DeepIPCv2: LiDAR-powered Robust Environmental Perception and Navigational Control for Autonomous Vehicle. (arXiv:2307.06647v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.06647</link>
<description rdf:parseType="Literal">&lt;p&gt;We present DeepIPCv2, an autonomous driving model that perceives the
environment using a LiDAR sensor for more robust drivability, especially when
driving under poor illumination conditions. DeepIPCv2 takes a set of LiDAR
point clouds for its main perception input. As point clouds are not affected by
illumination changes, they can provide a clear observation of the surroundings
no matter what the condition is. This results in a better scene understanding
and stable features provided by the perception module to support the controller
module in estimating navigational control properly. To evaluate its
performance, we conduct several tests by deploying the model to predict a set
of driving records and perform real automated driving under three different
conditions. We also conduct ablation and comparative studies with some recent
models to justify its performance. Based on the experimental results, DeepIPCv2
shows a robust performance by achieving the best drivability in all conditions.
Codes are available at https://github.com/oskarnatan/DeepIPCv2
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Natan_O/0/1/0/all/0/1&quot;&gt;Oskar Natan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miura_J/0/1/0/all/0/1&quot;&gt;Jun Miura&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06682">
<title>Explainable Artificial Intelligence driven mask design for self-supervised seismic denoising. (arXiv:2307.06682v1 [physics.geo-ph])</title>
<link>http://arxiv.org/abs/2307.06682</link>
<description rdf:parseType="Literal">&lt;p&gt;The presence of coherent noise in seismic data leads to errors and
uncertainties, and as such it is paramount to suppress noise as early and
efficiently as possible. Self-supervised denoising circumvents the common
requirement of deep learning procedures of having noisy-clean training pairs.
However, self-supervised coherent noise suppression methods require extensive
knowledge of the noise statistics. We propose the use of explainable artificial
intelligence approaches to see inside the black box that is the denoising
network and use the gained knowledge to replace the need for any prior
knowledge of the noise itself. This is achieved in practice by leveraging
bias-free networks and the direct linear link between input and output provided
by the associated Jacobian matrix; we show that a simple averaging of the
Jacobian contributions over a number of randomly selected input pixels,
provides an indication of the most effective mask to suppress noise present in
the data. The proposed method therefore becomes a fully automated denoising
procedure requiring no clean training labels or prior knowledge. Realistic
synthetic examples with noise signals of varying complexities, ranging from
simple time-correlated noise to complex pseudo rig noise propagating at the
velocity of the ocean, are used to validate the proposed approach. Its
automated nature is highlighted further by an application to two field
datasets. Without any substantial pre-processing or any knowledge of the
acquisition environment, the automatically identified blind-masks are shown to
perform well in suppressing both trace-wise noise in common shot gathers from
the Volve marine dataset and colored noise in post stack seismic images from a
land seismic survey.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Birnie_C/0/1/0/all/0/1&quot;&gt;Claire Birnie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ravasi_M/0/1/0/all/0/1&quot;&gt;Matteo Ravasi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06687">
<title>Towards Ubiquitous Semantic Metaverse: Challenges, Approaches, and Opportunities. (arXiv:2307.06687v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2307.06687</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, ubiquitous semantic Metaverse has been studied to
revolutionize immersive cyber-virtual experiences for augmented reality (AR)
and virtual reality (VR) users, which leverages advanced semantic understanding
and representation to enable seamless, context-aware interactions within
mixed-reality environments. This survey focuses on the intelligence and
spatio-temporal characteristics of four fundamental system components in
ubiquitous semantic Metaverse, i.e., artificial intelligence (AI),
spatio-temporal data representation (STDR), semantic Internet of Things (SIoT),
and semantic-enhanced digital twin (SDT). We thoroughly survey the
representative techniques of the four fundamental system components that enable
intelligent, personalized, and context-aware interactions with typical use
cases of the ubiquitous semantic Metaverse, such as remote education, work and
collaboration, entertainment and socialization, healthcare, and e-commerce
marketing. Furthermore, we outline the opportunities for constructing the
future ubiquitous semantic Metaverse, including scalability and
interoperability, privacy and security, performance measurement and
standardization, as well as ethical considerations and responsible AI.
Addressing those challenges is important for creating a robust, secure, and
ethically sound system environment that offers engaging immersive experiences
for the users and AR/VR applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lau_B/0/1/0/all/0/1&quot;&gt;Billy Lau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xin Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_W/0/1/0/all/0/1&quot;&gt;Wei Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guizani_M/0/1/0/all/0/1&quot;&gt;Mohsen Guizani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuen_C/0/1/0/all/0/1&quot;&gt;Chau Yuen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06698">
<title>IntelliGraphs: Datasets for Benchmarking Knowledge Graph Generation. (arXiv:2307.06698v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.06698</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge Graph Embedding (KGE) models are used to learn continuous
representations of entities and relations. A key task in the literature is
predicting missing links between entities. However, Knowledge Graphs are not
just sets of links but also have semantics underlying their structure.
Semantics is crucial in several downstream tasks, such as query answering or
reasoning. We introduce the subgraph inference task, where a model has to
generate likely and semantically valid subgraphs. We propose IntelliGraphs, a
set of five new Knowledge Graph datasets. The IntelliGraphs datasets contain
subgraphs with semantics expressed in logical rules for evaluating subgraph
inference. We also present the dataset generator that produced the synthetic
datasets. We designed four novel baseline models, which include three models
based on traditional KGEs. We evaluate their expressiveness and show that these
models cannot capture the semantics. We believe this benchmark will encourage
the development of machine learning models that emphasize semantic
understanding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thanapalasingam_T/0/1/0/all/0/1&quot;&gt;Thiviyan Thanapalasingam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krieken_E/0/1/0/all/0/1&quot;&gt;Emile van Krieken&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bloem_P/0/1/0/all/0/1&quot;&gt;Peter Bloem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Groth_P/0/1/0/all/0/1&quot;&gt;Paul Groth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06701">
<title>S-HR-VQVAE: Sequential Hierarchical Residual Learning Vector Quantized Variational Autoencoder for Video Prediction. (arXiv:2307.06701v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06701</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the video prediction task by putting forth a novel model that
combines (i) our recently proposed hierarchical residual vector quantized
variational autoencoder (HR-VQVAE), and (ii) a novel spatiotemporal PixelCNN
(ST-PixelCNN). We refer to this approach as a sequential hierarchical residual
learning vector quantized variational autoencoder (S-HR-VQVAE). By leveraging
the intrinsic capabilities of HR-VQVAE at modeling still images with a
parsimonious representation, combined with the ST-PixelCNN&apos;s ability at
handling spatiotemporal information, S-HR-VQVAE can better deal with chief
challenges in video prediction. These include learning spatiotemporal
information, handling high dimensional data, combating blurry prediction, and
implicit modeling of physical characteristics. Extensive experimental results
on the KTH Human Action and Moving-MNIST tasks demonstrate that our model
compares favorably against top video prediction techniques both in quantitative
and qualitative evaluations despite a much smaller model size. Finally, we
boost S-HR-VQVAE by proposing a novel training method to jointly estimate the
HR-VQVAE and ST-PixelCNN parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adiban_M/0/1/0/all/0/1&quot;&gt;Mohammad Adiban&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stefanov_K/0/1/0/all/0/1&quot;&gt;Kalin Stefanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siniscalchi_S/0/1/0/all/0/1&quot;&gt;Sabato Marco Siniscalchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salvi_G/0/1/0/all/0/1&quot;&gt;Giampiero Salvi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06709">
<title>GRAN is superior to GraphRNN: node orderings, kernel- and graph embeddings-based metrics for graph generators. (arXiv:2307.06709v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06709</link>
<description rdf:parseType="Literal">&lt;p&gt;A wide variety of generative models for graphs have been proposed. They are
used in drug discovery, road networks, neural architecture search, and program
synthesis. Generating graphs has theoretical challenges, such as isomorphic
representations -- evaluating how well a generative model performs is
difficult. Which model to choose depending on the application domain?
&lt;/p&gt;
&lt;p&gt;We extensively study kernel-based metrics on distributions of graph
invariants and manifold-based and kernel-based metrics in graph embedding
space. Manifold-based metrics outperform kernel-based metrics in embedding
space. We use these metrics to compare GraphRNN and GRAN, two well-known
generative models for graphs, and unveil the influence of node orderings. It
shows the superiority of GRAN over GraphRNN - further, our proposed adaptation
of GraphRNN with a depth-first search ordering is effective for small-sized
graphs.
&lt;/p&gt;
&lt;p&gt;A guideline on good practices regarding dataset selection and node feature
initialization is provided. Our work is accompanied by open-source code and
reproducible experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Touat_O/0/1/0/all/0/1&quot;&gt;Ousmane Touat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stier_J/0/1/0/all/0/1&quot;&gt;Julian Stier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Portier_P/0/1/0/all/0/1&quot;&gt;Pierre-Edouard Portier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Granitzer_M/0/1/0/all/0/1&quot;&gt;Michael Granitzer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06736">
<title>MPR-Net:Multi-Scale Pattern Reproduction Guided Universality Time Series Interpretable Forecasting. (arXiv:2307.06736v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06736</link>
<description rdf:parseType="Literal">&lt;p&gt;Time series forecasting has received wide interest from existing research due
to its broad applications and inherent challenging. The research challenge lies
in identifying effective patterns in historical series and applying them to
future forecasting. Advanced models based on point-wise connected MLP and
Transformer architectures have strong fitting power, but their secondary
computational complexity limits practicality. Additionally, those structures
inherently disrupt the temporal order, reducing the information utilization and
making the forecasting process uninterpretable. To solve these problems, this
paper proposes a forecasting model, MPR-Net. It first adaptively decomposes
multi-scale historical series patterns using convolution operation, then
constructs a pattern extension forecasting method based on the prior knowledge
of pattern reproduction, and finally reconstructs future patterns into future
series using deconvolution operation. By leveraging the temporal dependencies
present in the time series, MPR-Net not only achieves linear time complexity,
but also makes the forecasting process interpretable. By carrying out
sufficient experiments on more than ten real data sets of both short and long
term forecasting tasks, MPR-Net achieves the state of the art forecasting
performance, as well as good generalization and robustness performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tianlong Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuemei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Caiming Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06742">
<title>Vehicle Dispatching and Routing of On-Demand Intercity Ride-Pooling Services: A Multi-Agent Hierarchical Reinforcement Learning Approach. (arXiv:2307.06742v1 [eess.SY])</title>
<link>http://arxiv.org/abs/2307.06742</link>
<description rdf:parseType="Literal">&lt;p&gt;The integrated development of city clusters has given rise to an increasing
demand for intercity travel. Intercity ride-pooling service exhibits
considerable potential in upgrading traditional intercity bus services by
implementing demand-responsive enhancements. Nevertheless, its online
operations suffer the inherent complexities due to the coupling of vehicle
resource allocation among cities and pooled-ride vehicle routing. To tackle
these challenges, this study proposes a two-level framework designed to
facilitate online fleet management. Specifically, a novel multi-agent feudal
reinforcement learning model is proposed at the upper level of the framework to
cooperatively assign idle vehicles to different intercity lines, while the
lower level updates the routes of vehicles using an adaptive large neighborhood
search heuristic. Numerical studies based on the realistic dataset of Xiamen
and its surrounding cities in China show that the proposed framework
effectively mitigates the supply and demand imbalances, and achieves
significant improvement in both the average daily system profit and order
fulfillment ratio.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Si_J/0/1/0/all/0/1&quot;&gt;Jinhua Si&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+He_F/0/1/0/all/0/1&quot;&gt;Fang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tang_X/0/1/0/all/0/1&quot;&gt;Xindi Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06758">
<title>Layered controller synthesis for dynamic multi-agent systems. (arXiv:2307.06758v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.06758</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we present a layered approach for multi-agent control problem,
decomposed into three stages, each building upon the results of the previous
one. First, a high-level plan for a coarse abstraction of the system is
computed, relying on parametric timed automata augmented with stopwatches as
they allow to efficiently model simplified dynamics of such systems. In the
second stage, the high-level plan, based on SMT-formulation, mainly handles the
combinatorial aspects of the problem, provides a more dynamically accurate
solution. These stages are collectively referred to as the SWA-SMT solver. They
are correct by construction but lack a crucial feature: they cannot be executed
in real time. To overcome this, we use SWA-SMT solutions as the initial
training dataset for our last stage, which aims at obtaining a neural network
control policy. We use reinforcement learning to train the policy, and show
that the initial dataset is crucial for the overall success of the method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clement_E/0/1/0/all/0/1&quot;&gt;Emily Clement&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perrin_Gilbert_N/0/1/0/all/0/1&quot;&gt;Nicolas Perrin-Gilbert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlehuber_Caissier_P/0/1/0/all/0/1&quot;&gt;Philipp Schlehuber-Caissier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06775">
<title>A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media. (arXiv:2307.06775v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06775</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the last decade, there has been a vast increase in eating disorder
diagnoses and eating disorder-attributed deaths, reaching their zenith during
the Covid-19 pandemic. This immense growth derived in part from the stressors
of the pandemic but also from increased exposure to social media, which is rife
with content that promotes eating disorders. Such content can induce eating
disorders in viewers. This study aimed to create a multimodal deep learning
model capable of determining whether a given social media post promotes eating
disorders based on a combination of visual and textual data. A labeled dataset
of Tweets was collected from Twitter, upon which twelve deep learning models
were trained and tested. Based on model performance, the most effective deep
learning model was the multimodal fusion of the RoBERTa natural language
processing model and the MaxViT image classification model, attaining accuracy
and F1 scores of 95.9% and 0.959 respectively. The RoBERTa and MaxViT fusion
model, deployed to classify an unlabeled dataset of posts from the social media
sites Tumblr and Reddit, generated similar classifications as previous research
studies that did not employ artificial intelligence, showing that artificial
intelligence can develop insights congruent to those of researchers.
Additionally, the model was used to conduct a time-series analysis of yet
unseen Tweets from eight Twitter hashtags, uncovering that the relative
abundance of pro-eating disorder content has decreased drastically. However,
since approximately 2018, pro-eating disorder content has either stopped its
decline or risen once more in ampleness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feldman_J/0/1/0/all/0/1&quot;&gt;Jonathan Feldman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06794">
<title>Negated Complementary Commonsense using Large Language Models. (arXiv:2307.06794v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.06794</link>
<description rdf:parseType="Literal">&lt;p&gt;Larger language models, such as GPT-3, have shown to be excellent in many
tasks. However, we demonstrate that out-of-ordinary questions can throw the
model off guard. This work focuses on finding answers to negated complementary
questions in commonsense scenarios. We illustrate how such questions adversely
affect the model responses. We propose a model-agnostic methodology to improve
the performance in negated complementary scenarios. Our method outperforms
few-shot generation from GPT-3 (by more than 11 points) and, more importantly,
highlights the significance of studying the response of large language models
in negated complementary questions. The code, data, and experiments are
available under: https://github.com/navidre/negated_complementary_commonsense.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rezaei_N/0/1/0/all/0/1&quot;&gt;Navid Rezaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reformat_M/0/1/0/all/0/1&quot;&gt;Marek Z. Reformat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06822">
<title>TinyMetaFed: Efficient Federated Meta-Learning for TinyML. (arXiv:2307.06822v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06822</link>
<description rdf:parseType="Literal">&lt;p&gt;The field of Tiny Machine Learning (TinyML) has made substantial advancements
in democratizing machine learning on low-footprint devices, such as
microcontrollers. The prevalence of these miniature devices raises the question
of whether aggregating their knowledge can benefit TinyML applications.
Federated meta-learning is a promising answer to this question, as it addresses
the scarcity of labeled data and heterogeneous data distribution across devices
in the real world. However, deploying TinyML hardware faces unique resource
constraints, making existing methods impractical due to energy, privacy, and
communication limitations. We introduce TinyMetaFed, a model-agnostic
meta-learning framework suitable for TinyML. TinyMetaFed facilitates
collaborative training of a neural network initialization that can be quickly
fine-tuned on new devices. It offers communication savings and privacy
protection through partial local reconstruction and Top-P% selective
communication, computational efficiency via online learning, and robustness to
client heterogeneity through few-shot learning. The evaluations on three TinyML
use cases demonstrate that TinyMetaFed can significantly reduce energy
consumption and communication overhead, accelerate convergence, and stabilize
the training process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1&quot;&gt;Haoyu Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xue Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anicic_D/0/1/0/all/0/1&quot;&gt;Darko Anicic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Runkler_T/0/1/0/all/0/1&quot;&gt;Thomas A. Runkler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06824">
<title>CLAIMED -- the open source framework for building coarse-grained operators for accelerated discovery in science. (arXiv:2307.06824v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.06824</link>
<description rdf:parseType="Literal">&lt;p&gt;In modern data-driven science, reproducibility and reusability are key
challenges. Scientists are well skilled in the process from data to
publication. Although some publication channels require source code and data to
be made accessible, rerunning and verifying experiments is usually hard due to
a lack of standards. Therefore, reusing existing scientific data processing
code from state-of-the-art research is hard as well. This is why we introduce
CLAIMED, which has a proven track record in scientific research for addressing
the repeatability and reusability issues in modern data-driven science. CLAIMED
is a framework to build reusable operators and scalable scientific workflows by
supporting the scientist to draw from previous work by re-composing workflows
from existing libraries of coarse-grained scientific operators. Although
various implementations exist, CLAIMED is programming language, scientific
library, and execution environment agnostic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kienzler_R/0/1/0/all/0/1&quot;&gt;Romeo Kienzler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_R/0/1/0/all/0/1&quot;&gt;Rafflesia Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nilmeier_J/0/1/0/all/0/1&quot;&gt;Jerome Nilmeier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nesic_I/0/1/0/all/0/1&quot;&gt;Ivan Nesic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haddad_I/0/1/0/all/0/1&quot;&gt;Ibrahim Haddad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06825">
<title>A Causal Framework to Unify Common Domain Generalization Approaches. (arXiv:2307.06825v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06825</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain generalization (DG) is about learning models that generalize well to
new domains that are related to, but different from, the training domain(s). It
is a fundamental problem in machine learning and has attracted much attention
in recent years. A large number of approaches have been proposed. Different
approaches are motivated from different perspectives, making it difficult to
gain an overall understanding of the area. In this paper, we propose a causal
framework for domain generalization and present an understanding of common DG
approaches in the framework. Our work sheds new lights on the following
questions: (1) What are the key ideas behind each DG method? (2) Why is it
expected to improve generalization to new domains theoretically? (3) How are
different DG methods related to each other and what are relative advantages and
limitations? By providing a unified perspective on DG, we hope to help
researchers better understand the underlying principles and develop more
effective approaches for this critical problem in machine learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Nevin L. Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kaican Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1&quot;&gt;Han Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1&quot;&gt;Weiyan Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenguo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Luning Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yongxiang Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06844">
<title>Garbage in, garbage out: Zero-shot detection of crime using Large Language Models. (arXiv:2307.06844v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.06844</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes exploiting the common sense knowledge learned by large
language models to perform zero-shot reasoning about crimes given textual
descriptions of surveillance videos. We show that when video is (manually)
converted to high quality textual descriptions, large language models are
capable of detecting and classifying crimes with state-of-the-art performance
using only zero-shot reasoning. However, existing automated video-to-text
approaches are unable to generate video descriptions of sufficient quality to
support reasoning (garbage video descriptions into the large language model,
garbage out).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simmons_A/0/1/0/all/0/1&quot;&gt;Anj Simmons&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasa_R/0/1/0/all/0/1&quot;&gt;Rajesh Vasa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06845">
<title>Self-Supervised Learning for Interactive Perception of Surgical Thread for Autonomous Suture Tail-Shortening. (arXiv:2307.06845v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.06845</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate 3D sensing of suturing thread is a challenging problem in automated
surgical suturing because of the high state-space complexity, thinness and
deformability of the thread, and possibility of occlusion by the grippers and
tissue. In this work we present a method for tracking surgical thread in 3D
which is robust to occlusions and complex thread configurations, and apply it
to autonomously perform the surgical suture &quot;tail-shortening&quot; task: pulling
thread through tissue until a desired &quot;tail&quot; length remains exposed. The method
utilizes a learned 2D surgical thread detection network to segment suturing
thread in RGB images. It then identifies the thread path in 2D and reconstructs
the thread in 3D as a NURBS spline by triangulating the detections from two
stereo cameras. Once a 3D thread model is initialized, the method tracks the
thread across subsequent frames. Experiments suggest the method achieves a 1.33
pixel average reprojection error on challenging single-frame 3D thread
reconstructions, and an 0.84 pixel average reprojection error on two tracking
sequences. On the tail-shortening task, it accomplishes a 90% success rate
across 20 trials. Supplemental materials are available at
https://sites.google.com/berkeley.edu/autolab-surgical-thread/ .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schorp_V/0/1/0/all/0/1&quot;&gt;Vincent Schorp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panitch_W/0/1/0/all/0/1&quot;&gt;Will Panitch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shivakumar_K/0/1/0/all/0/1&quot;&gt;Kaushik Shivakumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Viswanath_V/0/1/0/all/0/1&quot;&gt;Vainavi Viswanath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kerr_J/0/1/0/all/0/1&quot;&gt;Justin Kerr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Avigal_Y/0/1/0/all/0/1&quot;&gt;Yahav Avigal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fer_D/0/1/0/all/0/1&quot;&gt;Danyal M Fer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ott_L/0/1/0/all/0/1&quot;&gt;Lionel Ott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1&quot;&gt;Ken Goldberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06857">
<title>Self-consistency for open-ended generations. (arXiv:2307.06857v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.06857</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a novel approach for improving the quality and
consistency of generated outputs from large-scale pre-trained language models
(LLMs). Self-consistency has emerged as an effective approach for prompts with
fixed answers, selecting the answer with the highest number of votes. In this
paper, we introduce a generalized framework for self-consistency that extends
its applicability beyond problems that have fixed-answer answers. Through
extensive simulations, we demonstrate that our approach consistently recovers
the optimal or near-optimal generation from a set of candidates. We also
propose lightweight parameter-free similarity functions that show significant
and consistent improvements across code generation, autoformalization, and
summarization tasks, even without access to token log probabilities. Our method
incurs minimal computational overhead, requiring no auxiliary reranker models
or modifications to the existing model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1&quot;&gt;Siddhartha Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiaofei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deoras_A/0/1/0/all/0/1&quot;&gt;Anoop Deoras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1&quot;&gt;Bing Xiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06858">
<title>Unconventional Cognitive Intelligent Robotic Control: Quantum Soft Computing Approach in Human Being Emotion Estimation -- QCOptKB Toolkit Application. (arXiv:2307.06858v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2307.06858</link>
<description rdf:parseType="Literal">&lt;p&gt;Strategy of intelligent cognitive control systems based on quantum and soft
computing presented. Quantum self-organization knowledge base synergetic effect
extracted from intelligent fuzzy controllers imperfect knowledge bases
described. That technology improved of robustness of intelligent cognitive
control systems in hazard control situations described with the cognitive
neuro-interface and different types of robot cooperation. Examples demonstrated
the introduction of quantum fuzzy inference gate design as prepared
programmable algorithmic solution for board embedded control systems. The
possibility of neuro-interface application based on cognitive helmet with
quantum fuzzy controller for driving of the vehicle is shown.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Ulyanov_S/0/1/0/all/0/1&quot;&gt;Sergey V. Ulyanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Kurawaki_I/0/1/0/all/0/1&quot;&gt;Ichiro Kurawaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Ulyanov_V/0/1/0/all/0/1&quot;&gt;Viktor S. Ulyanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Hagiwara_T/0/1/0/all/0/1&quot;&gt;Takakhide Hagiwara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06865">
<title>Prompts Should not be Seen as Secrets: Systematically Measuring Prompt Extraction Attack Success. (arXiv:2307.06865v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.06865</link>
<description rdf:parseType="Literal">&lt;p&gt;The generations of large language models are commonly controlled through
prompting techniques, where a user&apos;s query to the model is prefixed with a
prompt that aims to guide the model&apos;s behaviour on the query. The prompts used
by companies to guide their models are often treated as secrets, to be hidden
from the user making the query. They have even been treated as commodities to
be bought and sold. However, there has been anecdotal evidence showing that the
prompts can be extracted by a user even when they are kept secret. In this
paper, we present a framework for systematically measuring the success of
prompt extraction attacks. In experiments with multiple sources of prompts and
multiple underlying language models, we find that simple text-based attacks can
in fact reveal prompts with high probability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yiming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1&quot;&gt;Daphne Ippolito&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06869">
<title>DecompEval: Evaluating Generated Texts as Unsupervised Decomposed Question Answering. (arXiv:2307.06869v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.06869</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing evaluation metrics for natural language generation (NLG) tasks face
the challenges on generalization ability and interpretability. Specifically,
most of the well-performed metrics are required to train on evaluation datasets
of specific NLG tasks and evaluation dimensions, which may cause over-fitting
to task-specific datasets. Furthermore, existing metrics only provide an
evaluation score for each dimension without revealing the evidence to interpret
how this score is obtained. To deal with these challenges, we propose a simple
yet effective metric called DecompEval. This metric formulates NLG evaluation
as an instruction-style question answering task and utilizes instruction-tuned
pre-trained language models (PLMs) without training on evaluation datasets,
aiming to enhance the generalization ability. To make the evaluation process
more interpretable, we decompose our devised instruction-style question about
the quality of generated texts into the subquestions that measure the quality
of each sentence. The subquestions with their answers generated by PLMs are
then recomposed as evidence to obtain the evaluation result. Experimental
results show that DecompEval achieves state-of-the-art performance in untrained
metrics for evaluating text summarization and dialogue generation, which also
exhibits strong dimension-level / task-level generalization ability and
interpretability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ke_P/0/1/0/all/0/1&quot;&gt;Pei Ke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Fei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1&quot;&gt;Fei Mi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yasheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiaoyan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1&quot;&gt;Minlie Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06870">
<title>Embodied Lifelong Learning for Task and Motion Planning. (arXiv:2307.06870v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.06870</link>
<description rdf:parseType="Literal">&lt;p&gt;A robot deployed in a home over long stretches of time faces a true lifelong
learning problem. As it seeks to provide assistance to its users, the robot
should leverage any accumulated experience to improve its own knowledge to
become a more proficient assistant. We formalize this setting with a novel
lifelong learning problem formulation in the context of learning for task and
motion planning (TAMP). Exploiting the modularity of TAMP systems, we develop a
generative mixture model that produces candidate continuous parameters for a
planner. Whereas most existing lifelong learning approaches determine a priori
how data is shared across task models, our approach learns shared and
non-shared models and determines which to use online during planning based on
auxiliary tasks that serve as a proxy for each model&apos;s understanding of a
state. Our method exhibits substantial improvements in planning success on
simulated 2D domains and on several problems from the BEHAVIOR benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mendez_J/0/1/0/all/0/1&quot;&gt;Jorge A. Mendez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1&quot;&gt;Leslie Pack Kaelbling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lozano_Perez_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;s Lozano-P&amp;#xe9;rez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06877">
<title>The complexity of non-stationary reinforcement learning. (arXiv:2307.06877v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06877</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of continual learning in the domain of reinforcement learning,
often called non-stationary reinforcement learning, has been identified as an
important challenge to the application of reinforcement learning. We prove a
worst-case complexity result, which we believe captures this challenge:
Modifying the probabilities or the reward of a single state-action pair in a
reinforcement learning problem requires an amount of time almost as large as
the number of states in order to keep the value function up to date, unless the
strong exponential time hypothesis (SETH) is false; SETH is a widely accepted
strengthening of the P $\neq$ NP conjecture. Recall that the number of states
in current applications of reinforcement learning is typically astronomical. In
contrast, we show that just $\textit{adding}$ a new state-action pair is
considerably easier to implement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papadimitriou_C/0/1/0/all/0/1&quot;&gt;Christos Papadimitriou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1&quot;&gt;Binghui Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06908">
<title>Generating Benchmarks for Factuality Evaluation of Language Models. (arXiv:2307.06908v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.06908</link>
<description rdf:parseType="Literal">&lt;p&gt;Before deploying a language model (LM) within a given domain, it is important
to measure its tendency to generate factually incorrect information in that
domain. Existing factual generation evaluation methods focus on facts sampled
from the LM itself, and thus do not control the set of evaluated facts and
might under-represent rare and unlikely facts. We propose FACTOR: Factual
Assessment via Corpus TransfORmation, a scalable approach for evaluating LM
factuality. FACTOR automatically transforms a factual corpus of interest into a
benchmark evaluating an LM&apos;s propensity to generate true facts from the corpus
vs. similar but incorrect statements. We use our framework to create two
benchmarks: Wiki-FACTOR and News-FACTOR. We show that: (i) our benchmark scores
increase with model size and improve when the LM is augmented with retrieval;
(ii) benchmark score correlates with perplexity, but the two metrics do not
always agree on model ranking; and (iii) when perplexity and benchmark score
disagree, the latter better reflects factuality in open-ended generation, as
measured by human annotators. We make our data and code publicly available in
https://github.com/AI21Labs/factor.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muhlgay_D/0/1/0/all/0/1&quot;&gt;Dor Muhlgay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ram_O/0/1/0/all/0/1&quot;&gt;Ori Ram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magar_I/0/1/0/all/0/1&quot;&gt;Inbal Magar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_Y/0/1/0/all/0/1&quot;&gt;Yoav Levine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ratner_N/0/1/0/all/0/1&quot;&gt;Nir Ratner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1&quot;&gt;Yonatan Belinkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1&quot;&gt;Omri Abend&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leyton_Brown_K/0/1/0/all/0/1&quot;&gt;Kevin Leyton-Brown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shashua_A/0/1/0/all/0/1&quot;&gt;Amnon Shashua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shoham_Y/0/1/0/all/0/1&quot;&gt;Yoav Shoham&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06913">
<title>Uncovering Unique Concept Vectors through Latent Space Decomposition. (arXiv:2307.06913v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06913</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpreting the inner workings of deep learning models is crucial for
establishing trust and ensuring model safety. Concept-based explanations have
emerged as a superior approach that is more interpretable than feature
attribution estimates such as pixel saliency. However, defining the concepts
for the interpretability analysis biases the explanations by the user&apos;s
expectations on the concepts. To address this, we propose a novel post-hoc
unsupervised method that automatically uncovers the concepts learned by deep
models during training. By decomposing the latent space of a layer in singular
vectors and refining them by unsupervised clustering, we uncover concept
vectors aligned with directions of high variance that are relevant to the model
prediction, and that point to semantically distinct concepts. Our extensive
experiments reveal that the majority of our concepts are readily understandable
to humans, exhibit coherency, and bear relevance to the task at hand. Moreover,
we showcase the practical utility of our method in dataset exploration, where
our concept vectors successfully identify outlier training samples affected by
various confounding factors. This novel exploration technique has remarkable
versatility to data types and model architectures and it will facilitate the
identification of biases and the discovery of sources of error within training
data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Graziani_M/0/1/0/all/0/1&quot;&gt;Mara Graziani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahony_L/0/1/0/all/0/1&quot;&gt;Laura O&amp;#x27; Mahony&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1&quot;&gt;An-Phi Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_H/0/1/0/all/0/1&quot;&gt;Henning M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andrearczyk_V/0/1/0/all/0/1&quot;&gt;Vincent Andrearczyk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06917">
<title>LLM-assisted Knowledge Graph Engineering: Experiments with ChatGPT. (arXiv:2307.06917v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.06917</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge Graphs (KG) provide us with a structured, flexible, transparent,
cross-system, and collaborative way of organizing our knowledge and data across
various domains in society and industrial as well as scientific disciplines.
KGs surpass any other form of representation in terms of effectiveness.
However, Knowledge Graph Engineering (KGE) requires in-depth experiences of
graph structures, web technologies, existing models and vocabularies, rule
sets, logic, as well as best practices. It also demands a significant amount of
work. Considering the advancements in large language models (LLMs) and their
interfaces and applications in recent years, we have conducted comprehensive
experiments with ChatGPT to explore its potential in supporting KGE. In this
paper, we present a selection of these experiments and their results to
demonstrate how ChatGPT can assist us in the development and management of KGs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meyer_L/0/1/0/all/0/1&quot;&gt;Lars-Peter Meyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stadler_C/0/1/0/all/0/1&quot;&gt;Claus Stadler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frey_J/0/1/0/all/0/1&quot;&gt;Johannes Frey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radtke_N/0/1/0/all/0/1&quot;&gt;Norman Radtke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Junghanns_K/0/1/0/all/0/1&quot;&gt;Kurt Junghanns&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meissner_R/0/1/0/all/0/1&quot;&gt;Roy Meissner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dziwis_G/0/1/0/all/0/1&quot;&gt;Gordian Dziwis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bulert_K/0/1/0/all/0/1&quot;&gt;Kirill Bulert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_M/0/1/0/all/0/1&quot;&gt;Michael Martin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06924">
<title>DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual Language Grounding. (arXiv:2307.06924v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.06924</link>
<description rdf:parseType="Literal">&lt;p&gt;Persons with visual impairments (PwVI) have difficulties understanding and
navigating spaces around them. Current wayfinding technologies either focus
solely on navigation or provide limited communication about the environment.
Motivated by recent advances in visual-language grounding and semantic
navigation, we propose DRAGON, a guiding robot powered by a dialogue system and
the ability to associate the environment with natural language. By
understanding the commands from the user, DRAGON is able to guide the user to
the desired landmarks on the map, describe the environment, and answer
questions from visual observations. Through effective utilization of dialogue,
the robot can ground the user&apos;s free-form descriptions to landmarks in the
environment, and give the user semantic information through spoken language. We
conduct a user study with blindfolded participants in an everyday indoor
environment. Our results demonstrate that DRAGON is able to communicate with
the user smoothly, provide a good guiding experience, and connect users with
their surrounding environment in an intuitive manner.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shuijing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1&quot;&gt;Aamir Hasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_K/0/1/0/all/0/1&quot;&gt;Kaiwen Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Runxuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_P/0/1/0/all/0/1&quot;&gt;Peixin Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mizrachi_Z/0/1/0/all/0/1&quot;&gt;Zachary Mizrachi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Justin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McPherson_D/0/1/0/all/0/1&quot;&gt;D. Livingston McPherson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rogers_W/0/1/0/all/0/1&quot;&gt;Wendy A. Rogers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Driggs_Campbell_K/0/1/0/all/0/1&quot;&gt;Katherine Driggs-Campbell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06933">
<title>FDAPT: Federated Domain-adaptive Pre-training for Language Models. (arXiv:2307.06933v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06933</link>
<description rdf:parseType="Literal">&lt;p&gt;Combining Domain-adaptive Pre-training (DAPT) with Federated Learning (FL)
can enhance model adaptation by leveraging more sensitive and distributed data
while preserving data privacy. However, few studies have focused on this
method. Therefore, we conduct the first comprehensive empirical study to
evaluate the performance of Federated Domain-adaptive Pre-training (FDAPT). We
demonstrate that FDAPT can maintain competitive downstream task performance to
the centralized baseline in both IID and non-IID situations. Furthermore, we
propose a novel algorithm, Frozen Federated Domain-adaptive Pre-training
(FFDAPT). FFDAPT improves the computational efficiency by 12.1% on average and
exhibits similar downstream task performance to standard FDAPT, with general
performance fluctuations remaining less than 1%. Finally, through a critical
evaluation of our work, we identify promising future research directions for
this new research area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1&quot;&gt;Lekang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Svoboda_F/0/1/0/all/0/1&quot;&gt;Filip Svoboda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lane_N/0/1/0/all/0/1&quot;&gt;Nicholas D. Lane&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06941">
<title>On the Connection between Game-Theoretic Feature Attributions and Counterfactual Explanations. (arXiv:2307.06941v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.06941</link>
<description rdf:parseType="Literal">&lt;p&gt;Explainable Artificial Intelligence (XAI) has received widespread interest in
recent years, and two of the most popular types of explanations are feature
attributions, and counterfactual explanations. These classes of approaches have
been largely studied independently and the few attempts at reconciling them
have been primarily empirical. This work establishes a clear theoretical
connection between game-theoretic feature attributions, focusing on but not
limited to SHAP, and counterfactuals explanations. After motivating operative
changes to Shapley values based feature attributions and counterfactual
explanations, we prove that, under conditions, they are in fact equivalent. We
then extend the equivalency result to game-theoretic solution concepts beyond
Shapley values. Moreover, through the analysis of the conditions of such
equivalence, we shed light on the limitations of naively using counterfactual
explanations to provide feature importances. Experiments on three datasets
quantitatively show the difference in explanations at every stage of the
connection between the two approaches and corroborate the theoretical findings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albini_E/0/1/0/all/0/1&quot;&gt;Emanuele Albini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1&quot;&gt;Shubham Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1&quot;&gt;Saumitra Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dervovic_D/0/1/0/all/0/1&quot;&gt;Danial Dervovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magazzeni_D/0/1/0/all/0/1&quot;&gt;Daniele Magazzeni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06945">
<title>In-context Autoencoder for Context Compression in a Large Language Model. (arXiv:2307.06945v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.06945</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose the In-context Autoencoder (ICAE) for context compression in a
large language model (LLM). The ICAE has two modules: a learnable encoder
adapted with LoRA from an LLM for compressing a long context into a limited
number of memory slots, and a fixed decoder which is the target LLM that can
condition on the memory slots for various purposes. We first pretrain the ICAE
using both autoencoding and language modeling objectives on massive text data,
enabling it to generate memory slots that accurately and comprehensively
represent the original context. Then, we fine-tune the pretrained ICAE on a
small amount of instruct data to enhance its interaction with various prompts
for producing desirable responses. Our experimental results demonstrate that
the ICAE learned with our proposed pretraining and fine-tuning paradigm can
effectively produce memory slots with $4\times$ context compression, which can
be well conditioned on by the target LLM to respond to various prompts. The
promising results demonstrate significant implications of the ICAE for its
novel approach to the long context problem and its potential to reduce
computation and memory overheads for LLM inference in practice, suggesting
further research effort in context management for an LLM. Our code and data
will be released shortly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1&quot;&gt;Tao Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jing Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Si-Qing Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1&quot;&gt;Furu Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06947">
<title>Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action Recognition. (arXiv:2307.06947v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06947</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent video recognition models utilize Transformer models for long-range
spatio-temporal context modeling. Video transformer designs are based on
self-attention that can model global context at a high computational cost. In
comparison, convolutional designs for videos offer an efficient alternative but
lack long-range dependency modeling. Towards achieving the best of both
designs, this work proposes Video-FocalNet, an effective and efficient
architecture for video recognition that models both local and global contexts.
Video-FocalNet is based on a spatio-temporal focal modulation architecture that
reverses the interaction and aggregation steps of self-attention for better
efficiency. Further, the aggregation step and the interaction step are both
implemented using efficient convolution and element-wise multiplication
operations that are computationally less expensive than their self-attention
counterparts on video representations. We extensively explore the design space
of focal modulation-based spatio-temporal context modeling and demonstrate our
parallel spatial and temporal encoding design to be the optimal choice.
Video-FocalNets perform favorably well against the state-of-the-art
transformer-based models for video recognition on three large-scale datasets
(Kinetics-400, Kinetics-600, and SS-v2) at a lower computational cost. Our
code/models are released at https://github.com/TalalWasim/Video-FocalNets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wasim_S/0/1/0/all/0/1&quot;&gt;Syed Talal Wasim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khattak_M/0/1/0/all/0/1&quot;&gt;Muhammad Uzair Khattak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1&quot;&gt;Muzammal Naseer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Salman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1&quot;&gt;Mubarak Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1&quot;&gt;Fahad Shahbaz Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06949">
<title>HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models. (arXiv:2307.06949v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06949</link>
<description rdf:parseType="Literal">&lt;p&gt;Personalization has emerged as a prominent aspect within the field of
generative AI, enabling the synthesis of individuals in diverse contexts and
styles, while retaining high-fidelity to their identities. However, the process
of personalization presents inherent challenges in terms of time and memory
requirements. Fine-tuning each personalized model needs considerable GPU time
investment, and storing a personalized model per subject can be demanding in
terms of storage capacity. To overcome these challenges, we propose
HyperDreamBooth-a hypernetwork capable of efficiently generating a small set of
personalized weights from a single image of a person. By composing these
weights into the diffusion model, coupled with fast finetuning, HyperDreamBooth
can generate a person&apos;s face in various contexts and styles, with high subject
details while also preserving the model&apos;s crucial knowledge of diverse styles
and semantic modifications. Our method achieves personalization on faces in
roughly 20 seconds, 25x faster than DreamBooth and 125x faster than Textual
Inversion, using as few as one reference image, with the same quality and style
diversity as DreamBooth. Also our method yields a model that is 10000x smaller
than a normal DreamBooth model. Project page: https://hyperdreambooth.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruiz_N/0/1/0/all/0/1&quot;&gt;Nataniel Ruiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanzhen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1&quot;&gt;Varun Jampani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1&quot;&gt;Wei Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_T/0/1/0/all/0/1&quot;&gt;Tingbo Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pritch_Y/0/1/0/all/0/1&quot;&gt;Yael Pritch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wadhwa_N/0/1/0/all/0/1&quot;&gt;Neal Wadhwa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rubinstein_M/0/1/0/all/0/1&quot;&gt;Michael Rubinstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aberman_K/0/1/0/all/0/1&quot;&gt;Kfir Aberman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1912.13122">
<title>Declarative Mechanism Design. (arXiv:1912.13122v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1912.13122</link>
<description rdf:parseType="Literal">&lt;p&gt;Regulation of Multi-Agent Systems (MAS) and Declarative Electronic
Institutions (DEIs) was a multidisciplinary research topic of the past decade
involving (Physical and Software) Agents and Law since the beginning, but
recently evolved towards News-claimed Robot Lawyer since 2016. One of these
first proposals of restricting the behaviour of Software Agentswas Electronic
Institutions.However, with the recent reformulation of Artificial Neural
Networks (ANNs) as Deep Learning (DL), Security, Privacy,Ethical and Legal
issues regarding the use of DL has raised concerns in the Artificial
Intelligence (AI) Community. Now that the Regulation of MAS is almost correctly
addressed, we propose the Regulation of Artificial Neural Networks as
Agent-based Training of a special type of regulated Artificial Neural Network
that we call Institutional Neural Network (INN).The main purpose of this paper
is to bring attention to Artificial Teaching (AT) and to give a tentative
answer showing a proof-of-concept implementation of Regulated Deep Learning
(RDL). This paper introduces the former concept and provide sI, a language
previously used to model declaratively and extend Electronic Institutions, as a
means to regulate the execution of Artificial Neural Networks and their
interactions with Artificial Teachers (ATs)
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_Camino_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9;s Garc&amp;#xed;a-Camino&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2106.06613">
<title>A New Formalism, Method and Open Issues for Zero-Shot Coordination. (arXiv:2106.06613v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2106.06613</link>
<description rdf:parseType="Literal">&lt;p&gt;In many coordination problems, independently reasoning humans are able to
discover mutually compatible policies. In contrast, independently trained
self-play policies are often mutually incompatible. Zero-shot coordination
(ZSC) has recently been proposed as a new frontier in multi-agent reinforcement
learning to address this fundamental issue. Prior work approaches the ZSC
problem by assuming players can agree on a shared learning algorithm but not on
labels for actions and observations, and proposes other-play as an optimal
solution. However, until now, this &quot;label-free&quot; problem has only been
informally defined. We formalize this setting as the label-free coordination
(LFC) problem by defining the label-free coordination game. We show that
other-play is not an optimal solution to the LFC problem as it fails to
consistently break ties between incompatible maximizers of the other-play
objective. We introduce an extension of the algorithm, other-play with
tie-breaking, and prove that it is optimal in the LFC problem and an
equilibrium in the LFC game. Since arbitrary tie-breaking is precisely what the
ZSC setting aims to prevent, we conclude that the LFC problem does not reflect
the aims of ZSC. To address this, we introduce an alternative informal
operationalization of ZSC as a starting point for future work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Treutlein_J/0/1/0/all/0/1&quot;&gt;Johannes Treutlein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dennis_M/0/1/0/all/0/1&quot;&gt;Michael Dennis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oesterheld_C/0/1/0/all/0/1&quot;&gt;Caspar Oesterheld&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1&quot;&gt;Jakob Foerster&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2109.05714">
<title>Autonomous Navigation of Underactuated Bipedal Robots in Height-Constrained Environments. (arXiv:2109.05714v4 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2109.05714</link>
<description rdf:parseType="Literal">&lt;p&gt;Navigating a large-scaled robot in unknown and cluttered height-constrained
environments is challenging. Not only is a fast and reliable planning algorithm
required to go around obstacles, the robot should also be able to change its
intrinsic dimension by crouching in order to travel underneath
height-constrained regions. There are few mobile robots that are capable of
handling such a challenge, and bipedal robots provide a solution. However, as
bipedal robots have nonlinear and hybrid dynamics, trajectory planning while
ensuring dynamic feasibility and safety on these robots is challenging. This
paper presents an end-to-end autonomous navigation framework which leverages
three layers of planners and a variable walking height controller to enable
bipedal robots to safely explore height-constrained environments. A
vertically-actuated Spring-Loaded Inverted Pendulum (vSLIP) model is introduced
to capture the robot&apos;s coupled dynamics of planar walking and vertical walking
height. This reduced-order model is utilized to optimize for long-term and
short-term safe trajectory plans. A variable walking height controller is
leveraged to enable the bipedal robot to maintain stable periodic walking gaits
while following the planned trajectory. The entire framework is tested and
experimentally validated using a bipedal robot Cassie. This demonstrates
reliable autonomy to drive the robot to safely avoid obstacles while walking to
the goal location in various kinds of height-constrained cluttered
environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhongyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1&quot;&gt;Jun Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shuxiao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sreenath_K/0/1/0/all/0/1&quot;&gt;Koushil Sreenath&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2109.13445">
<title>Emergent Neural Network Mechanisms for Generalization to Objects in Novel Orientations. (arXiv:2109.13445v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2109.13445</link>
<description rdf:parseType="Literal">&lt;p&gt;The capability of Deep Neural Networks (DNNs) to recognize objects in
orientations outside the distribution of the training data is not well
understood. We present evidence that DNNs are capable of generalizing to
objects in novel orientations by disseminating orientation-invariance obtained
from familiar objects seen from many viewpoints. This capability strengthens
when training the DNN with an increasing number of familiar objects, but only
in orientations that involve 2D rotations of familiar orientations. We show
that this dissemination is achieved via neurons tuned to common features
between familiar and unfamiliar objects. These results implicate brain-like
neural mechanisms for generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cooper_A/0/1/0/all/0/1&quot;&gt;Avi Cooper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1&quot;&gt;Xavier Boix&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harari_D/0/1/0/all/0/1&quot;&gt;Daniel Harari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madan_S/0/1/0/all/0/1&quot;&gt;Spandan Madan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1&quot;&gt;Hanspeter Pfister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1&quot;&gt;Tomotake Sasaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sinha_P/0/1/0/all/0/1&quot;&gt;Pawan Sinha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.07372">
<title>Prospective Learning: Principled Extrapolation to the Future. (arXiv:2201.07372v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2201.07372</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning is a process which can update decision rules, based on past
experience, such that future performance improves. Traditionally, machine
learning is often evaluated under the assumption that the future will be
identical to the past in distribution or change adversarially. But these
assumptions can be either too optimistic or pessimistic for many problems in
the real world. Real world scenarios evolve over multiple spatiotemporal scales
with partially predictable dynamics. Here we reformulate the learning problem
to one that centers around this idea of dynamic futures that are partially
learnable. We conjecture that certain sequences of tasks are not
retrospectively learnable (in which the data distribution is fixed), but are
prospectively learnable (in which distributions may be dynamic), suggesting
that prospective learning is more difficult in kind than retrospective
learning. We argue that prospective learning more accurately characterizes many
real world problems that (1) currently stymie existing artificial intelligence
solutions and/or (2) lack adequate explanations for how natural intelligences
solve them. Thus, studying prospective learning will lead to deeper insights
and solutions to currently vexing challenges in both natural and artificial
intelligences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_A/0/1/0/all/0/1&quot;&gt;Ashwin De Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramesh_R/0/1/0/all/0/1&quot;&gt;Rahul Ramesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ungar_L/0/1/0/all/0/1&quot;&gt;Lyle Ungar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shuler_M/0/1/0/all/0/1&quot;&gt;Marshall Hussain Shuler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cowan_N/0/1/0/all/0/1&quot;&gt;Noah J. Cowan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Platt_M/0/1/0/all/0/1&quot;&gt;Michael Platt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isik_L/0/1/0/all/0/1&quot;&gt;Leyla Isik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roh_S/0/1/0/all/0/1&quot;&gt;Seung-Eon Roh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Charles_A/0/1/0/all/0/1&quot;&gt;Adam Charles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkataraman_A/0/1/0/all/0/1&quot;&gt;Archana Venkataraman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caffo_B/0/1/0/all/0/1&quot;&gt;Brian Caffo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+How_J/0/1/0/all/0/1&quot;&gt;Javier J. How&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kebschull_J/0/1/0/all/0/1&quot;&gt;Justus M Kebschull&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krakauer_J/0/1/0/all/0/1&quot;&gt;John W. Krakauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bichuch_M/0/1/0/all/0/1&quot;&gt;Maxim Bichuch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kinfu_K/0/1/0/all/0/1&quot;&gt;Kaleab Alemayehu Kinfu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yezerets_E/0/1/0/all/0/1&quot;&gt;Eva Yezerets&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayaraman_D/0/1/0/all/0/1&quot;&gt;Dinesh Jayaraman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1&quot;&gt;Jong M. Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Villar_S/0/1/0/all/0/1&quot;&gt;Soledad Villar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phillips_I/0/1/0/all/0/1&quot;&gt;Ian Phillips&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Priebe_C/0/1/0/all/0/1&quot;&gt;Carey E. Priebe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hartung_T/0/1/0/all/0/1&quot;&gt;Thomas Hartung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miller_M/0/1/0/all/0/1&quot;&gt;Michael I. Miller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dey_J/0/1/0/all/0/1&quot;&gt;Jayanta Dey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ningyuan/0/1/0/all/0/1&quot;&gt;Ningyuan&lt;/a&gt; (Teresa) &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang/0/1/0/all/0/1&quot;&gt;Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eaton_E/0/1/0/all/0/1&quot;&gt;Eric Eaton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Etienne_Cummings_R/0/1/0/all/0/1&quot;&gt;Ralph Etienne-Cummings&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ogburn_E/0/1/0/all/0/1&quot;&gt;Elizabeth L. Ogburn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burns_R/0/1/0/all/0/1&quot;&gt;Randal Burns&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osuagwu_O/0/1/0/all/0/1&quot;&gt;Onyema Osuagwu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mensh_B/0/1/0/all/0/1&quot;&gt;Brett Mensh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muotri_A/0/1/0/all/0/1&quot;&gt;Alysson R. Muotri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_J/0/1/0/all/0/1&quot;&gt;Julia Brown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+White_C/0/1/0/all/0/1&quot;&gt;Chris White&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Weiwei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rusu_A/0/1/0/all/0/1&quot;&gt;Andrei A. Rusu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verstynen_T/0/1/0/all/0/1&quot;&gt;Timothy Verstynen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kording_K/0/1/0/all/0/1&quot;&gt;Konrad P. Kording&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhari_P/0/1/0/all/0/1&quot;&gt;Pratik Chaudhari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vogelstein_J/0/1/0/all/0/1&quot;&gt;Joshua T. Vogelstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.02819">
<title>Block shuffling learning for Deepfake Detection. (arXiv:2202.02819v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2202.02819</link>
<description rdf:parseType="Literal">&lt;p&gt;Deepfake detection methods based on convolutional neural networks (CNN) have
demonstrated high accuracy. \textcolor{black}{However, these methods often
suffer from decreased performance when faced with unknown forgery methods and
common transformations such as resizing and blurring, resulting in deviations
between training and testing domains.} This phenomenon, known as overfitting,
poses a significant challenge. To address this issue, we propose a novel block
shuffling regularization method. Firstly, our approach involves dividing the
images into blocks and applying both intra-block and inter-block shuffling
techniques. This process indirectly achieves weight-sharing across different
dimensions. Secondly, we introduce an adversarial loss algorithm to mitigate
the overfitting problem induced by the shuffling noise. Finally, we restore the
spatial layout of the blocks to capture the semantic associations among them.
Extensive experiments validate the effectiveness of our proposed method, which
surpasses existing approaches in forgery face detection. Notably, our method
exhibits excellent generalization capabilities, demonstrating robustness
against cross-dataset evaluations and common image transformations. Especially
our method can be easily integrated with various CNN models. Source code is
available at
\href{https://github.com/NoWindButRain/BlockShuffleLearning}{Github}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sitong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1&quot;&gt;Zhichao Lian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1&quot;&gt;Siqi Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1&quot;&gt;Liang Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.14838">
<title>Most Equitable Voting Rules. (arXiv:2205.14838v3 [cs.GT] UPDATED)</title>
<link>http://arxiv.org/abs/2205.14838</link>
<description rdf:parseType="Literal">&lt;p&gt;In social choice theory, anonymity (all agents being treated equally) and
neutrality (all alternatives being treated equally) are widely regarded as
``minimal demands&apos;&apos; and ``uncontroversial&apos;&apos; axioms of equity and fairness.
However, the ANR impossibility -- there is no voting rule that satisfies
anonymity, neutrality, and resolvability (always choosing one winner) -- holds
even in the simple setting of two alternatives and two agents. How to design
voting rules that optimally satisfy anonymity, neutrality, and resolvability
remains an open question.
&lt;/p&gt;
&lt;p&gt;We address the optimal design question for a wide range of preferences and
decisions that include ranked lists and committees. Our conceptual contribution
is a novel and strong notion of most equitable refinements that optimally
preserves anonymity and neutrality for any irresolute rule that satisfies the
two axioms. Our technical contributions are twofold. First, we characterize the
conditions for the ANR impossibility to hold under general settings, especially
when the number of agents is large. Second, we propose the
most-favorable-permutation (MFP) tie-breaking to compute a most equitable
refinement and design a polynomial-time algorithm to compute MFP when agents&apos;
preferences are full rankings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_L/0/1/0/all/0/1&quot;&gt;Lirong Xia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.04827">
<title>Classification and Generation of real-world data with an Associative Memory Model. (arXiv:2207.04827v4 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2207.04827</link>
<description rdf:parseType="Literal">&lt;p&gt;Drawing from memory the face of a friend you have not seen in years is a
difficult task. However, if you happen to cross paths, you would easily
recognize each other. The biological memory is equipped with an impressive
compression algorithm that can store the essential, and then infer the details
to match perception. The Willshaw Memory is a simple abstract model for
cortical computations which implements mechanisms of biological memories. Using
our recently proposed sparse coding prescription for visual patterns, this
model can store and retrieve an impressive amount of real-world data in a
fault-tolerant manner. In this paper, we extend the capabilities of the basic
Associative Memory Model by using a Multiple-Modality framework. In this
setting, the memory stores several modalities (e.g., visual, or textual) of
each pattern simultaneously. After training, the memory can be used to infer
missing modalities when just a subset is perceived. Using a simple
encoder-memory-decoder architecture, and a newly proposed iterative retrieval
algorithm for the Willshaw Model, we perform experiments on the MNIST dataset.
By storing both the images and labels as modalities, a single Memory can be
used not only to retrieve and complete patterns but also to classify and
generate new ones. We further discuss how this model could be used for other
learning tasks, thus serving as a biologically-inspired framework for learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simas_R/0/1/0/all/0/1&quot;&gt;Rodrigo Simas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sa_Couto_L/0/1/0/all/0/1&quot;&gt;Luis Sa-Couto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wichert_A/0/1/0/all/0/1&quot;&gt;Andreas Wichert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.11290">
<title>TRUST-LAPSE: An Explainable and Actionable Mistrust Scoring Framework for Model Monitoring. (arXiv:2207.11290v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2207.11290</link>
<description rdf:parseType="Literal">&lt;p&gt;Continuous monitoring of trained ML models to determine when their
predictions should and should not be trusted is essential for their safe
deployment. Such a framework ought to be high-performing, explainable, post-hoc
and actionable. We propose TRUST-LAPSE, a &quot;mistrust&quot; scoring framework for
continuous model monitoring. We assess the trustworthiness of each input
sample&apos;s model prediction using a sequence of latent-space embeddings.
Specifically, (a) our latent-space mistrust score estimates mistrust using
distance metrics (Mahalanobis distance) and similarity metrics (cosine
similarity) in the latent-space and (b) our sequential mistrust score
determines deviations in correlations over the sequence of past input
representations in a non-parametric, sliding-window based algorithm for
actionable continuous monitoring. We evaluate TRUST-LAPSE via two downstream
tasks: (1) distributionally shifted input detection, and (2) data drift
detection. We evaluate across diverse domains - audio and vision using public
datasets and further benchmark our approach on challenging, real-world
electroencephalograms (EEG) datasets for seizure detection. Our latent-space
mistrust scores achieve state-of-the-art results with AUROCs of 84.1 (vision),
73.9 (audio), and 77.1 (clinical EEGs), outperforming baselines by over 10
points. We expose critical failures in popular baselines that remain
insensitive to input semantic content, rendering them unfit for real-world
model monitoring. We show that our sequential mistrust scores achieve high
drift detection rates; over 90% of the streams show &amp;lt; 20% error for all
domains. Through extensive qualitative and quantitative evaluations, we show
that our mistrust scores are more robust and provide explainability for easy
adoption into practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhaskhar_N/0/1/0/all/0/1&quot;&gt;Nandita Bhaskhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rubin_D/0/1/0/all/0/1&quot;&gt;Daniel L. Rubin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Messer_C/0/1/0/all/0/1&quot;&gt;Christopher Lee-Messer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.07734">
<title>Data Augmentation is a Hyperparameter: Cherry-picked Self-Supervision for Unsupervised Anomaly Detection is Creating the Illusion of Success. (arXiv:2208.07734v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2208.07734</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised learning (SSL) has emerged as a promising alternative to
create supervisory signals to real-world problems, avoiding the extensive cost
of manual labeling. SSL is particularly attractive for unsupervised tasks such
as anomaly detection (AD), where labeled anomalies are rare or often
nonexistent. A large catalog of augmentation functions has been used for
SSL-based AD (SSAD) on image data, and recent works have reported that the type
of augmentation has a significant impact on accuracy. Motivated by those, this
work sets out to put image-based SSAD under a larger lens and investigate the
role of data augmentation in SSAD. Through extensive experiments on 3 different
detector models and across 420 AD tasks, we provide comprehensive numerical and
visual evidences that the alignment between data augmentation and
anomaly-generating mechanism is the key to the success of SSAD, and in the lack
thereof, SSL may even impair accuracy. To the best of our knowledge, this is
the first meta-analysis on the role of data augmentation in SSAD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1&quot;&gt;Jaemin Yoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tiancheng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akoglu_L/0/1/0/all/0/1&quot;&gt;Leman Akoglu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.10081">
<title>Revisiting Discrete Soft Actor-Critic. (arXiv:2209.10081v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.10081</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the adaption of soft actor-critic (SAC) from continuous action space
to discrete action space. We revisit vanilla SAC and provide an in-depth
understanding of its Q value underestimation and performance instability issues
when applied to discrete settings. We thereby propose entropy-penalty and
double average Q-learning with Q-clip to address these issues. Extensive
experiments on typical benchmarks with discrete action space, including Atari
games and a large-scale MOBA game, show the efficacy of our proposed method.
Our code is at:https://github.com/coldsummerday/Revisiting-Discrete-SAC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Haibin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zichuan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Junyou Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1&quot;&gt;Qiang Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_D/0/1/0/all/0/1&quot;&gt;Deheng Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.14905">
<title>RulE: Neural-Symbolic Knowledge Graph Reasoning with Rule Embedding. (arXiv:2210.14905v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2210.14905</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge graph (KG) reasoning is an important problem for knowledge graphs.
In this paper, we propose a novel and principled framework called \textbf{RulE}
(stands for {Rul}e {E}mbedding) to effectively leverage logical rules to
enhance KG reasoning. Unlike knowledge graph embedding (KGE) methods, RulE
learns rule embeddings from existing triplets and first-order {rules} by
jointly representing \textbf{entities}, \textbf{relations} and \textbf{logical
rules} in a unified embedding space. Based on the learned rule embeddings, a
confidence score can be calculated for each rule, reflecting its consistency
with the observed triplets. This allows us to perform logical rule inference in
a soft way, thus alleviating the brittleness of logic. On the other hand, RulE
injects prior logical rule information into the embedding space, enriching and
regularizing the entity/relation embeddings. This makes KGE alone perform
better too. RulE is conceptually simple and empirically effective. We conduct
extensive experiments to verify each component of RulE. Results on multiple
benchmarks reveal that our model outperforms the majority of existing
embedding-based and rule-based approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1&quot;&gt;Xiaojuan Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Song-Chun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yitao Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Muhan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.00241">
<title>Adversarial Policies Beat Superhuman Go AIs. (arXiv:2211.00241v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.00241</link>
<description rdf:parseType="Literal">&lt;p&gt;We attack the state-of-the-art Go-playing AI system KataGo by training
adversarial policies against it, achieving a &amp;gt;97% win rate against KataGo
running at superhuman settings. Our adversaries do not win by playing Go well.
Instead, they trick KataGo into making serious blunders. Our attack transfers
zero-shot to other superhuman Go-playing AIs, and is comprehensible to the
extent that human experts can implement it without algorithmic assistance to
consistently beat superhuman AIs. The core vulnerability uncovered by our
attack persists even in KataGo agents adversarially trained to defend against
our attack. Our results demonstrate that even superhuman AI systems may harbor
surprising failure modes. Example games are available https://goattack.far.ai/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tony T. Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gleave_A/0/1/0/all/0/1&quot;&gt;Adam Gleave&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tseng_T/0/1/0/all/0/1&quot;&gt;Tom Tseng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pelrine_K/0/1/0/all/0/1&quot;&gt;Kellin Pelrine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belrose_N/0/1/0/all/0/1&quot;&gt;Nora Belrose&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miller_J/0/1/0/all/0/1&quot;&gt;Joseph Miller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dennis_M/0/1/0/all/0/1&quot;&gt;Michael D. Dennis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1&quot;&gt;Yawen Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pogrebniak_V/0/1/0/all/0/1&quot;&gt;Viktor Pogrebniak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russell_S/0/1/0/all/0/1&quot;&gt;Stuart Russell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.06407">
<title>Control Transformer: Robot Navigation in Unknown Environments through PRM-Guided Return-Conditioned Sequence Modeling. (arXiv:2211.06407v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2211.06407</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning long-horizon tasks such as navigation has presented difficult
challenges for successfully applying reinforcement learning to robotics. From
another perspective, under known environments, sampling-based planning can
robustly find collision-free paths in environments without learning. In this
work, we propose Control Transformer that models return-conditioned sequences
from low-level policies guided by a sampling-based Probabilistic Roadmap (PRM)
planner. We demonstrate that our framework can solve long-horizon navigation
tasks using only local information. We evaluate our approach on
partially-observed maze navigation with MuJoCo robots, including Ant, Point,
and Humanoid. We show that Control Transformer can successfully navigate
through mazes and transfer to unknown environments. Additionally, we apply our
method to a differential drive robot (Turtlebot3) and show zero-shot sim2real
transfer under noisy observations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lawson_D/0/1/0/all/0/1&quot;&gt;Daniel Lawson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qureshi_A/0/1/0/all/0/1&quot;&gt;Ahmed H. Qureshi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.15944">
<title>The Effectiveness of World Models for Continual Reinforcement Learning. (arXiv:2211.15944v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.15944</link>
<description rdf:parseType="Literal">&lt;p&gt;World models power some of the most efficient reinforcement learning
algorithms. In this work, we showcase that they can be harnessed for continual
learning - a situation when the agent faces changing environments. World models
typically employ a replay buffer for training, which can be naturally extended
to continual learning. We systematically study how different selective
experience replay methods affect performance, forgetting, and transfer. We also
provide recommendations regarding various modeling options for using world
models. The best set of choices is called Continual-Dreamer, it is
task-agnostic and utilizes the world model for continual exploration.
Continual-Dreamer is sample efficient and outperforms state-of-the-art
task-agnostic continual reinforcement learning methods on Minigrid and Minihack
benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kessler_S/0/1/0/all/0/1&quot;&gt;Samuel Kessler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ostaszewski_M/0/1/0/all/0/1&quot;&gt;Mateusz Ostaszewski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bortkiewicz_M/0/1/0/all/0/1&quot;&gt;Micha&amp;#x142; Bortkiewicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zarski_M/0/1/0/all/0/1&quot;&gt;Mateusz &amp;#x17b;arski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolczyk_M/0/1/0/all/0/1&quot;&gt;Maciej Wo&amp;#x142;czyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parker_Holder_J/0/1/0/all/0/1&quot;&gt;Jack Parker-Holder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roberts_S/0/1/0/all/0/1&quot;&gt;Stephen J. Roberts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milos_P/0/1/0/all/0/1&quot;&gt;Piotr Mi&amp;#x142;o&amp;#x15b;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.00679">
<title>Formal Controller Synthesis for Markov Jump Linear Systems with Uncertain Dynamics. (arXiv:2212.00679v3 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2212.00679</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated synthesis of provably correct controllers for cyber-physical
systems is crucial for deployment in safety-critical scenarios. However, hybrid
features and stochastic or unknown behaviours make this problem challenging. We
propose a method for synthesising controllers for Markov jump linear systems
(MJLSs), a class of discrete-time models for cyber-physical systems, so that
they certifiably satisfy probabilistic computation tree logic (PCTL) formulae.
An MJLS consists of a finite set of stochastic linear dynamics and discrete
jumps between these dynamics that are governed by a Markov decision process
(MDP). We consider the cases where the transition probabilities of this MDP are
either known up to an interval or completely unknown. Our approach is based on
a finite-state abstraction that captures both the discrete (mode-jumping) and
continuous (stochastic linear) behaviour of the MJLS. We formalise this
abstraction as an interval MDP (iMDP) for which we compute intervals of
transition probabilities using sampling techniques from the so-called &apos;scenario
approach&apos;, resulting in a probabilistically sound approximation. We apply our
method to multiple realistic benchmark problems, in particular, a temperature
control and an aerial vehicle delivery problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rickard_L/0/1/0/all/0/1&quot;&gt;Luke Rickard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Badings_T/0/1/0/all/0/1&quot;&gt;Thom Badings&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Romao_L/0/1/0/all/0/1&quot;&gt;Licio Romao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Abate_A/0/1/0/all/0/1&quot;&gt;Alessandro Abate&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.03044">
<title>A Survey on Transformers in Reinforcement Learning. (arXiv:2301.03044v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.03044</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer has been considered the dominating neural architecture in NLP and
CV, mostly under supervised settings. Recently, a similar surge of using
Transformers has appeared in the domain of reinforcement learning (RL), but it
is faced with unique design choices and challenges brought by the nature of RL.
However, the evolution of Transformers in RL has not yet been well unraveled.
In this paper, we seek to systematically review motivations and progress on
using Transformers in RL, provide a taxonomy on existing works, discuss each
sub-field, and summarize future prospects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenzhe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1&quot;&gt;Hao Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zichuan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chongjie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zongqing Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_D/0/1/0/all/0/1&quot;&gt;Deheng Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.01802">
<title>FR3D: Three-dimensional Flow Reconstruction and Force Estimation for Unsteady Flows Around Extruded Bluff Bodies via Conformal Mapping Aided Convolutional Autoencoders. (arXiv:2302.01802v2 [physics.flu-dyn] UPDATED)</title>
<link>http://arxiv.org/abs/2302.01802</link>
<description rdf:parseType="Literal">&lt;p&gt;In many practical fluid dynamics experiments, measuring variables such as
velocity and pressure is possible only at a limited number of sensor locations,
\textcolor{black}{for a few two-dimensional planes, or for a small 3D domain in
the flow}. However, knowledge of the full fields is necessary to understand the
dynamics of many flows. Deep learning reconstruction of full flow fields from
sparse measurements has recently garnered significant research interest, as a
way of overcoming this limitation. This task is referred to as the flow
reconstruction (FR) task. In the present study, we propose a convolutional
autoencoder based neural network model, dubbed FR3D, which enables FR to be
carried out for three-dimensional flows around extruded 3D objects with
different cross-sections. An innovative mapping approach, whereby multiple
fluid domains are mapped to an annulus, enables FR3D to generalize its
performance to objects not encountered during training. We conclusively
demonstrate this generalization capability using a dataset composed of 80
training and 20 testing geometries, all randomly generated. We show that the
FR3D model reconstructs pressure and velocity components with a few percentage
points of error. Additionally, using these predictions, we accurately estimate
the Q-criterion fields as well lift and drag forces on the geometries.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ozbay_A/0/1/0/all/0/1&quot;&gt;Ali Girayhan &amp;#xd6;zbay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Laizet_S/0/1/0/all/0/1&quot;&gt;Sylvain Laizet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05222">
<title>FishRecGAN: An End to End GAN Based Network for Fisheye Rectification and Calibration. (arXiv:2305.05222v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05222</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an end-to-end deep learning approach to rectify fisheye images and
simultaneously calibrate camera intrinsic and distortion parameters. Our method
consists of two parts: a Quick Image Rectification Module developed with a
Pix2Pix GAN and Wasserstein GAN (W-Pix2PixGAN), and a Calibration Module with a
CNN architecture. Our Quick Rectification Network performs robust rectification
with good resolution, making it suitable for constant calibration in
camera-based surveillance equipment. To achieve high-quality calibration, we
use the straightened output from the Quick Rectification Module as a
guidance-like semantic feature map for the Calibration Module to learn the
geometric relationship between the straightened feature and the distorted
feature. We train and validate our method with a large synthesized dataset
labeled with well-simulated parameters applied to a perspective image dataset.
Our solution has achieved robust performance in high-resolution with a
significant PSNR value of 22.343.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1&quot;&gt;Xin Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joo_K/0/1/0/all/0/1&quot;&gt;Kyungdon Joo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1&quot;&gt;Jean Oh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.13339">
<title>TrustGuard: GNN-based Robust and Explainable Trust Evaluation with Dynamicity Support. (arXiv:2306.13339v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.13339</link>
<description rdf:parseType="Literal">&lt;p&gt;Trust evaluation assesses trust relationships between entities and
facilitates decision-making. Machine Learning (ML) shows great potential for
trust evaluation owing to its learning capabilities. In recent years, Graph
Neural Networks (GNNs), as a new ML paradigm, have demonstrated superiority in
dealing with graph data. This has motivated researchers to explore their use in
trust evaluation, as trust relationships among entities can be modeled as a
graph. However, current trust evaluation methods that employ GNNs fail to fully
satisfy the dynamic nature of trust, overlook the adverse effects of attacks on
trust evaluation, and cannot provide convincing explanations on evaluation
results. To address these problems, we propose TrustGuard, a GNN-based accurate
trust evaluation model that supports trust dynamicity, is robust against
typical attacks, and provides explanations through visualization. Specifically,
TrustGuard is designed with a layered architecture that contains a snapshot
input layer, a spatial aggregation layer, a temporal aggregation layer, and a
prediction layer. Among them, the spatial aggregation layer adopts a defense
mechanism to robustly aggregate local trust, and the temporal aggregation layer
applies an attention mechanism for effective learning of temporal patterns.
Extensive experiments on two real-world datasets show that TrustGuard
outperforms state-of-the-art GNN-based trust evaluation models with respect to
trust prediction across single-timeslot and multi-timeslot, even in the
presence of attacks. In addition, TrustGuard can explain its evaluation results
by visualizing both spatial and temporal views.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zheng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_J/0/1/0/all/0/1&quot;&gt;Jiahe Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertino_E/0/1/0/all/0/1&quot;&gt;Elisa Bertino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedrycz_W/0/1/0/all/0/1&quot;&gt;Witold Pedrycz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00470">
<title>PatternGPT :A Pattern-Driven Framework for Large Language Model Text Generation. (arXiv:2307.00470v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00470</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models(LLMS) have shown excellent text generation
capabilities,capable of generating fluent responses for many downstream tasks.
However,applying large language models to real-world critical tasks remains
challenging due to their susceptibility to hallucinations and inability to
directly use external knowledge. To address the above challenges,this paper
proposes PatternGPT, a pattern-driven text generation framework for large
language models. First,the framework utilizes the extraction capabilities of
large language models to generate rich and diverse patterns and later draws on
the idea of federated learning. Using multiple agents to achieve sharing to
obtain more diverse patterns. Finally, it searches for high-quality patterns
using judgment criteria and optimization algorithms and uses the searched
patterns to guide the model for generation. This framework has the advantages
of generating diversified patterns, protecting data privacy,combining external
knowledge, and improving the quality of generation, which provides an effective
method to optimize the text generation capability of large language models,and
make it better applied to the field of intelligent dialogue and content
generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1&quot;&gt;Le Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_X/0/1/0/all/0/1&quot;&gt;Xin Shan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01316">
<title>Towards Safe Autonomous Driving Policies using a Neuro-Symbolic Deep Reinforcement Learning Approach. (arXiv:2307.01316v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01316</link>
<description rdf:parseType="Literal">&lt;p&gt;The dynamic nature of driving environments and the presence of diverse road
users pose significant challenges for decision-making in autonomous driving.
Deep reinforcement learning (DRL) has emerged as a popular approach to tackle
this problem. However, the application of existing DRL solutions is mainly
confined to simulated environments due to safety concerns, impeding their
deployment in real-world. To overcome this limitation, this paper introduces a
novel neuro-symbolic model-free DRL approach, called DRL with Symbolic Logics
(DRLSL) that combines the strengths of DRL (learning from experience) and
symbolic first-order logics (knowledge-driven reasoning) to enable safe
learning in real-time interactions of autonomous driving within real
environments. This innovative approach provides a means to learn autonomous
driving policies by actively engaging with the physical environment while
ensuring safety. We have implemented the DRLSL framework in autonomous driving
using the highD dataset and demonstrated that our method successfully avoids
unsafe actions during both the training and testing phases. Furthermore, our
results indicate that DRLSL achieves faster convergence during training and
exhibits better generalizability to new driving scenarios compared to
traditional DRL methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharifi_I/0/1/0/all/0/1&quot;&gt;Iman Sharifi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yildirim_M/0/1/0/all/0/1&quot;&gt;Mustafa Yildirim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fallah_S/0/1/0/all/0/1&quot;&gt;Saber Fallah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03109">
<title>A Survey on Evaluation of Large Language Models. (arXiv:2307.03109v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03109</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) are gaining increasing popularity in both
academia and industry, owing to their unprecedented performance in various
applications. As LLMs continue to play a vital role in both research and daily
use, their evaluation becomes increasingly critical, not only at the task
level, but also at the society level for better understanding of their
potential risks. Over the past years, significant efforts have been made to
examine LLMs from various perspectives. This paper presents a comprehensive
review of these evaluation methods for LLMs, focusing on three key dimensions:
what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide
an overview from the perspective of evaluation tasks, encompassing general
natural language processing tasks, reasoning, medical usage, ethics,
educations, natural and social sciences, agent applications, and other areas.
Secondly, we answer the `where&apos; and `how&apos; questions by diving into the
evaluation methods and benchmarks, which serve as crucial components in
assessing performance of LLMs. Then, we summarize the success and failure cases
of LLMs in different tasks. Finally, we shed light on several future challenges
that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to
researchers in the realm of LLMs evaluation, thereby aiding the development of
more proficient LLMs. Our key point is that evaluation should be treated as an
essential discipline to better assist the development of LLMs. We consistently
maintain the related open-source materials at:
https://github.com/MLGroupJLU/LLM-eval-survey.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1&quot;&gt;Yupeng Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jindong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1&quot;&gt;Kaijie Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Linyi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1&quot;&gt;Xiaoyuan Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Cunxiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yidong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1&quot;&gt;Wei Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yue Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1&quot;&gt;Yi Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1&quot;&gt;Philip S. Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qiang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xing Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03198">
<title>A multilevel framework for AI governance. (arXiv:2307.03198v2 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03198</link>
<description rdf:parseType="Literal">&lt;p&gt;To realize the potential benefits and mitigate potential risks of AI, it is
necessary to develop a framework of governance that conforms to ethics and
fundamental human values. Although several organizations have issued guidelines
and ethical frameworks for trustworthy AI, without a mediating governance
structure, these ethical principles will not translate into practice. In this
paper, we propose a multilevel governance approach that involves three groups
of interdependent stakeholders: governments, corporations, and citizens. We
examine their interrelationships through dimensions of trust, such as
competence, integrity, and benevolence. The levels of governance combined with
the dimensions of trust in AI provide practical insights that can be used to
further enhance user experiences and inform public policy related to AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choung_H/0/1/0/all/0/1&quot;&gt;Hyesun Choung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+David_P/0/1/0/all/0/1&quot;&gt;Prabu David&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seberger_J/0/1/0/all/0/1&quot;&gt;John S. Seberger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03875">
<title>Large Language Models for Supply Chain Optimization. (arXiv:2307.03875v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03875</link>
<description rdf:parseType="Literal">&lt;p&gt;Supply chain operations traditionally involve a variety of complex decision
making problems. Over the last few decades, supply chains greatly benefited
from advances in computation, which allowed the transition from manual
processing to automation and cost-effective optimization. Nonetheless, business
operators still need to spend substantial efforts in explaining and
interpreting the optimization outcomes to stakeholders. Motivated by the recent
advances in Large Language Models (LLMs), we study how this disruptive
technology can help bridge the gap between supply chain automation and human
comprehension and trust thereof. We design OptiGuide -- a framework that
accepts as input queries in plain text, and outputs insights about the
underlying optimization outcomes. Our framework does not forgo the
state-of-the-art combinatorial optimization technology, but rather leverages it
to quantitatively answer what-if scenarios (e.g., how would the cost change if
we used supplier B instead of supplier A for a given demand?). Importantly, our
design does not require sending proprietary data over to LLMs, which can be a
privacy concern in some circumstances. We demonstrate the effectiveness of our
framework on a real server placement scenario within Microsoft&apos;s cloud supply
chain. Along the way, we develop a general evaluation benchmark, which can be
used to evaluate the accuracy of the LLM output in other scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Beibin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mellou_K/0/1/0/all/0/1&quot;&gt;Konstantina Mellou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pathuri_J/0/1/0/all/0/1&quot;&gt;Jeevan Pathuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menache_I/0/1/0/all/0/1&quot;&gt;Ishai Menache&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05766">
<title>Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology Reporting. (arXiv:2307.05766v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05766</link>
<description rdf:parseType="Literal">&lt;p&gt;Radiology reporting is a crucial part of the communication between
radiologists and other medical professionals, but it can be time-consuming and
error-prone. One approach to alleviate this is structured reporting, which
saves time and enables a more accurate evaluation than free-text reports.
However, there is limited research on automating structured reporting, and no
public benchmark is available for evaluating and comparing different methods.
To close this gap, we introduce Rad-ReStruct, a new benchmark dataset that
provides fine-grained, hierarchically ordered annotations in the form of
structured reports for X-Ray images. We model the structured reporting task as
hierarchical visual question answering (VQA) and propose hi-VQA, a novel method
that considers prior context in the form of previously asked questions and
answers for populating a structured radiology report. Our experiments show that
hi-VQA achieves competitive performance to the state-of-the-art on the medical
VQA benchmark VQARad while performing best among methods without
domain-specific vision-language pretraining and provides a strong baseline on
Rad-ReStruct. Our work represents a significant step towards the automated
population of structured radiology reports and provides a valuable first
benchmark for future research in this area. We will make all annotations and
our code for annotation generation, model evaluation, and training publicly
available upon acceptance. Our dataset and code is available at
https://github.com/ChantalMP/Rad-ReStruct.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pellegrini_C/0/1/0/all/0/1&quot;&gt;Chantal Pellegrini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keicher_M/0/1/0/all/0/1&quot;&gt;Matthias Keicher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozsoy_E/0/1/0/all/0/1&quot;&gt;Ege &amp;#xd6;zsoy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1&quot;&gt;Nassir Navab&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05921">
<title>Reading Radiology Imaging Like The Radiologist. (arXiv:2307.05921v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05921</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated radiology report generation aims to generate radiology reports that
contain rich, fine-grained descriptions of radiology imaging. Compared with
image captioning in the natural image domain, medical images are very similar
to each other, with only minor differences in the occurrence of diseases. Given
the importance of these minor differences in the radiology report, it is
crucial to encourage the model to focus more on the subtle regions of disease
occurrence. Secondly, the problem of visual and textual data biases is serious.
Not only do normal cases make up the majority of the dataset, but sentences
describing areas with pathological changes also constitute only a small part of
the paragraph. Lastly, generating medical image reports involves the challenge
of long text generation, which requires more expertise and empirical training
in medical knowledge. As a result, the difficulty of generating such reports is
increased. To address these challenges, we propose a disease-oriented retrieval
framework that utilizes similar reports as prior knowledge references. We
design a factual consistency captioning generator to generate more accurate and
factually consistent disease descriptions. Our framework can find most similar
reports for a given disease from the CXR database by retrieving a
disease-oriented mask consisting of the position and morphological
characteristics. By referencing the disease-oriented similar report and the
visual features, the factual consistency model can generate a more accurate
radiology report.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuhao Wang&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>